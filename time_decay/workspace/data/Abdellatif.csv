Document Title,Authors,Author Affiliations,Publication Title,Date Added To Xplore,Year,Volume,Issue,Start Page,End Page,Abstract,ISSN,ISBNs,DOI,PDF Link,Author Keywords,IEEE Terms,INSPEC Controlled Terms,INSPEC Non-Controlled Terms,MeSH Terms,Article Citation Count,Patent Citation Count,Reference Count,Copyright Year,label,Issue Date,Meeting Date,Publisher,Document Identifier
2D-CHI: a tunable two-dimensional class hierarchy index for object-oriented databases,Jong-Hak Lee; Kyu-Young Whang; Wook-Shin Han; Wan-Sup Cho; Il-Yeol Song,"Dept. of Comput. Sci., Catholic Univ. of Taegu-Hyosung, South Korea",Proceedings 24th Annual International Computer Software and Applications Conference. COMPSAC2000,20020806,2000,,,598,607,"We present a tunable two-dimensional class hierarchy indexing technique (2D-CHI) for object-oriented databases. We use a two-dimensional file organization as the index structure. 2D-CHI deals with the problem of clustering objects in a two-dimensional domain space consisting of the key attribute domain and the class identifier domain. In conventional class indexing techniques using one-dimensional index structures such as the B<sup>+</sup>-tree, the clustering property is exclusively owned by one attribute. These indexing techniques do not efficiently handle the queries that address both the attribute keys and the class identifiers. 2D-CHI enhances query performance by adjusting the degree of clustering between the key value domain and the class identifier domain based on the precollected usage pattern. For performance evaluation, we first compare 2D-CHI with the conventional class indexing techniques using an analytic cost model based on the assumption of uniform object distribution, and then verify the cost model through experiments using the multilevel grid file as the two-dimensional index. We further perform experiments with nonuniform object distributions. Our experiments show that our proposed method does indeed build optimal class index structures regardless of query types and object distributions. We strongly believe that our paper significantly contributes to building a self-tunable database system by supporting automatically tunable index structure",0730-3157;07303157,POD:0-7695-0792-1,10.1109/CMPSAC.2000.884786,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=884786,,Buildings;Computer science;Costs;Indexes;Indexing;Management information systems;Object oriented databases;Object oriented modeling;Performance analysis;Relational databases,database indexing;file organisation;object-oriented databases;query processing;software performance evaluation,2D-CHI;B+ tree;analytic cost model;class identifier;class indexing;experiments;key attribute;multilevel grid file;nonuniform object distribution;object clustering;object-oriented databases;performance evaluation;query performance;tunable 2D class hierarchy index;tunable two-dimensional class hierarchy index;two-dimensional file organization;uniform object distribution,,0,1,14,,no,2000,25 Oct 2000-27 Oct 2000,IEEE,IEEE Conference Publications
A chemical reaction hysteresis model for magnetic materials,A. Nourdine; A. Kedous-Lebouc; G. Meunier; T. Chevalier,"Lab. d'Electrotech. de Grenoble, CNRS, St. Martin d'Heres, France",IEEE Transactions on Magnetics,20020806,2000,36,4,1230,1233,"The authors present a new static hysteresis model for magnetic materials based on physical meaning. The idea is original: electronic transformation of the material is compared to a chemical reaction. Then a simple analytic formulation of the B(H) law for GO iron sheet in the rolling direction is obtained. Results are compared with measurements. The model is as simple as the Potter one, its parameters can be easily determined and its accuracy could compete with the Preisach model. Finally an easy computation by finite element software is gained",0018-9464;00189464,,10.1109/20.877662,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=877662,,Analytical models;Chemicals;Iron;Magnetic analysis;Magnetic domain walls;Magnetic hysteresis;Magnetic materials;Magnetization;Protons;Thermodynamics,finite element analysis;iron;magnetic hysteresis;modelling;reaction kinetics theory;rolling;texture,B(H) law;Fe;GO iron sheet;Goss texture;analytic formulation;chemical reaction hysteresis model;easy computation;electronic transformation;finite element software;magnetic materials;rolling direction;static hysteresis model,,4,,16,,no,Jul-00,,IEEE,IEEE Journals & Magazines
A decision-analytic stopping rule for validation of commercial software systems,T. Chavez,"Rapt Technol. Corp., San Francisco, CA",IEEE Transactions on Software Engineering,20020806,2000,26,9,907,918,"The decision of when to release a software product commercially is not a question of when the software has attained some objectively justifiable degree of correctness. It is, rather, a question of whether the software achieves a reasonable balance among engineering objectives, market demand, customer requirements, and marketing directives of the software organization. We present a rigorous framework for addressing this important decision. Conjugate distributions from statistical decision theory provide an attractive means of modeling the cost and rate of bugs given information acquired during software testing, as well as prior information provided by software engineers about the fidelity of the software before testing begins. In contrast to other methods, the stopping analysis yields a computationally simple rule for deciding when to release a commercial software product based on information revealed to engineers during software testing-complicated numerical procedures are not needed. Our method has the added benefits that it is sequential: it measures explicitly the costs of customer dissatisfaction associated with bugs as well as the costs of declining market position while the testing process continues; and it incorporates a practical framework for cost-criticality assessment that makes sense to professional software developers",0098-5589;00985589,,10.1109/32.877849,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=877849,,Business;Computer bugs;Costs;Decision theory;Design engineering;Engineering management;Programming;Software systems;Software testing;Software tools,formal verification;program debugging;program testing;software development management,bugs;commercial software system validation;conjugate distributions;cost-criticality assessment;customer dissatisfaction;customer requirements;decision-analytic stopping rule;declining market position;engineering objectives;market demand;marketing directives;professional software developers;software organization;software testing;statistical decision theory,,11,,20,,no,Sep-00,,IEEE,IEEE Journals & Magazines
A frequency spectrum model of the intermodulation products generated by paging transmitters,Li Jianyu; Hu Shouming,,"Environmental Electromagnetics, 2000. CEEM 2000. Proceedings. Asia-Pacific Conference on",20020806,2000,,,285,291,"The present paper presents a frequency spectrum model analytic method of the intermodulation products generated by paging transmitters. A frequency spectrum model and its simulation frequency spectrum of nineteen intermodulation products, which are generated by different kinds of paging transmitters, are shown. All these above could be used by the spectrum manager to identify the intermodulation interference. They could also be used to distinguish the interfering signals with the help of computer assisted software of searching interference",,POD:7-5635-0420-6,10.1109/CEEM.2000.853950,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=853950,,Computational modeling;Frequency domain analysis;Frequency shift keying;Interference;Low pass filters;Power amplifiers;Power harmonic filters;Signal analysis;Signal generators;Transmitters,intermodulation;paging communication;radiofrequency interference,EMI;frequency spectrum model;interfering signals;intermodulation interference;intermodulation products;paging transmitters;simulation frequency spectrum;spectrum manager,,0,,3,,no,2000,03 May 2000-07 May 2000,IEEE,IEEE Conference Publications
A method for 3D analysis of upper extremity kinematics applied to a case study of brachial plexus birth palsy,G. Rab; K. Petuskey; A. Bagley,"Motion Anal. Lab., Shriners Hosp. for Children, Sacramento, CA, USA",Pediatric Gait: A New Millennium in Clinical Care and Motion Analysis Technology,20020806,2000,,,202,209,"Kinematic analysis of the upper extremity has been conducted using a wide variety of techniques, philosophies, and analytic methods. The authors propose a simple, surface marker model, using three-dimensional video recording, that borrows concepts from lower extremity kinematic analysis. A sequential order about orthogonal axes is described (Eulerian) to generate sagittal, coronal and transverse plane motion. The technique is applied to a child with brachial plexus birth palsy pre- and postoperatively",,POD:0-7803-6469-4,10.1109/PG.2000.858899,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=858899,,Bismuth;Brachytherapy;Computer aided software engineering;Extremities;Kinematics;Laboratories;Motion analysis;Muscles;Shoulder;Surgery,gait analysis;kinematics;paediatrics;video recording,3D analysis method;brachial plexus birth palsy;child;coronal motion;orthogonal axes;sagittal motion;simple surface marker model;three-dimensional video recording;transverse plane motion;upper extremity kinematics,,0,,17,,no,2000,,IEEE,IEEE Conference Publications
A method for design and performance modeling of client/server systems,D. A. Menasce; H. Gomaa,"Dept. of Comput. Sci., George Mason Univ., Fairfax, VA, USA",IEEE Transactions on Software Engineering,20020806,2000,26,11,1066,1085,"Designing complex distributed client/server applications that meet performance requirements may prove extremely difficult in practice if software developers are not willing or do not have the time to help software performance analysts. The paper advocates the need to integrate both design and performance modeling activities so that one can help the other. We present a method developed and used by the authors in the design of a fairly large and complex client/server application. The method is based on a software performance engineering language developed by one of the authors. Use cases were developed and mapped to a performance modeling specification using the language. A compiler for the language generates an analytic performance model for the system. Service demand parameters at servers, storage boxes, and networks are derived by the compiler from the system specification. A detailed model of DBMS query optimizers allows the compiler to estimate the number of I/Os and CPU time for SQL statements. The paper concludes with some results of the application that prompted the development of the method and language.",0098-5589;00985589,,10.1109/32.881718,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=881718,,Application software;Computer Society;Costs;Databases;Design methodology;Mission critical systems;Network servers;Performance analysis;Software performance;Unified modeling language,client-server systems;formal specification;program compilers;query processing;software performance evaluation,CPU time;DBMS query optimizers;SQL statements;analytic performance model;client/server systems design;compiler;distributed client/server applications;performance modeling;performance modeling activities;performance modeling specification;performance requirements;service demand parameters;software developers;software performance analysts;software performance engineering language;storage boxes;system specification;use cases,,31,1,46,,no,Nov-00,,IEEE,IEEE Journals & Magazines
An analytic approach to pheromone-based coordination,S. A. Brueckner,"ERIM Center for Electron Commerce, Ann Arbor, MI, USA",Proceedings Fourth International Conference on MultiAgent Systems,20020806,2000,,,369,370,"Stigmergetic coordination of large numbers of simple agents in natural agent systems yields complex and stable coordinated system-level behavior. The Pheromone Infrastructure, an enhancement of the runtime environment of software agent systems, supports the use of stigmergetic coordination mechanisms in synthetic ecosystems. This paper discusses how such mechanisms may be tuned and evaluated, employing a formal representation of the processes in the Pheromone Infrastructure",,POD:0-7695-0625-9,10.1109/ICMAS.2000.858479,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=858479,,Difference equations;Ecosystems;Electrical capacitance tomography;Electronic commerce;Emulation;Insects;Multiagent systems;Runtime environment;Software agents,living systems;multi-agent systems,Pheromone Infrastructure;multi agent systems;natural agent systems;pheromone-based coordination;runtime environment;software agent;stigmergetic coordination;synthetic ecosystems,,0,4,4,,no,2000,10 Jul 2000-12 Jul 2000,IEEE,IEEE Conference Publications
"Analytic performance model for speculative, synchronous, discrete-event simulation",B. L. Noble; R. D. Chamberlain,"Dept. of Electr. & Comput. Eng., Southern Illinois Univ., Edwardsville, IL, USA",Proceedings Fourteenth Workshop on Parallel and Distributed Simulation,20020806,2000,,,35,44,"Performance models exist that reliably describe the execution time and efficiency of parallel discrete-event simulations executed in a synchronous iterative fashion. These performance models incorporate the effects of processor heterogeneity, other processor load due to shared computational resources, application workload imbalance, and the use of speculative computation. This includes modeling the effects of predictive optimism, a technique for improving the accuracy of speculative assumptions. We extend these models to incorporate correlated workloads across the set of processors and validate the models with two different applications",,POD:0-7695-0677-1,10.1109/PADS.2000.847142,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=847142,,Computational modeling;Computer applications;Concurrent computing;Discrete event simulation;Iterative algorithms;Logic;Parallel processing;Performance analysis;Predictive models;Very large scale integration,discrete event simulation;parallel programming;software performance evaluation,analytic performance model;application workload imbalance;correlated workloads;parallel discrete-event simulations;predictive optimism;processor heterogeneity;processor load;shared computational resources;speculative synchronous discrete-event simulation;synchronous iteration,,3,,22,,no,2000,28 May 2000-31 May 2000,IEEE,IEEE Conference Publications
Cephalometric Downs' analysis. A mathematical framework,A. Reddy; A. Manju; B. Muthukumaran; C. Kulkarni; A. Nandakumar,"Dept. of Electr. & Electrn. Eng., Sri Venkateswara Coll. of Eng., Pennalur, India",WCC 2000 - ICSP 2000. 2000 5th International Conference on Signal Processing Proceedings. 16th World Computer Congress 2000,20020806,2000,1,,103,106 vol.1,Cephalometric procedure for craniofacial analysis is a very important procedure in orthodontics. Downs' analysis involves tedious geometric and analytic manual calculations to be performed on the X-ray or tracing of the profile of a skull. In this paper we have attempted to emulate this manual operation by modeling the process and creating a software that automatically calculates the required measurements. The orthodontist loads the lateral cephalogram (ceph) and the co-ordinates or landmarks are identified using a mouse interface. The identified landmarks are passed as inputs for the diagnosis module. The standard values are maintained as a table. The calculated values are compared with the standards and appropriate inferences are displayed. The program is also a user friendly one,,POD:0-7803-5747-7,10.1109/ICOSP.2000.894454,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=894454,,Dentistry;Educational institutions;Hospitals;Manuals;Orbital calculations;Orthopedic surgery;Performance analysis;Skull;Software measurement;Teeth,dentistry;diagnostic radiography;medical image processing;orthopaedics,X-ray;cephalometric Downs analysis;co-ordinates identification;craniofacial analysis;diagnosis module;landmarks identification;lateral cephalogram;mouse interface;orthodontics;program;skull profile tracing;software,,2,,7,,no,2000,21 Aug 2000-25 Aug 2000,IEEE,IEEE Conference Publications
Design of discrete-time nonlinear observers,N. Kazantzis; C. Kravaris,"Dept. of Chem. Eng., Texas A&M Univ., College Station, TX, USA",Proceedings of the 2000 American Control Conference. ACC (IEEE Cat. No.00CH36334),20020806,2000,4,,2305,2310 vol.4,"Proposes an approach to the discrete-time nonlinear observer design problem. Based on the early ideas that influenced the development of the linear Luenberger observer, the proposed approach develops a nonlinear analogue. The formulation of the discrete-time nonlinear observer design problem is realized via a system of first-order linear functional equations, and a rather general set of necessary and sufficient conditions for solvability is derived using results from linear functional equation theory. The solution to the above system of linear functional equations can be proven to be locally analytic and this enables the development of a series solution method, that is easily programmable with the aid of a symbolic software package",0743-1619;07431619,POD:0-7803-5519-9,10.1109/ACC.2000.878591,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=878591,,Chemical engineering;Design methodology;Linear systems;Monitoring;Nonlinear equations;Nonlinear systems;Observers;Process control;Software packages;State estimation,discrete time systems;functional equations;matrix algebra;nonlinear systems;observers;series (mathematics);state-space methods,discrete-time nonlinear observers;first-order linear functional equations;linear Luenberger observer;necessary and sufficient conditions;series solution method,,4,,15,,no,2000,28 Jun 2000-30 Jun 2000,IEEE,IEEE Conference Publications
Designing process replication and activation: a quantitative approach,M. Litoiu; J. Rolia; G. Serazzi,"IBM Canada Ltd., Toronto, Ont., Canada",IEEE Transactions on Software Engineering,20020806,2000,26,12,1168,1178,"Distributed application systems are composed of classes of objects with instances that interact to accomplish common goals. Such systems can have many classes of users with many types of requests. Furthermore, the relative load of these classes can shift throughout the day, causing changes to system behavior and bottlenecks. When designing and deploying such systems, it is necessary to determine a process replication and threading policy for the server processes that contain the objects, as well as process activation policies. To avoid bottlenecks, the policy must support all possible workload conditions. Licensing, implementation or resource constraints can limit the number of permitted replicas or threads of a server process. Process activation policies determine whether a server is persistent or should be created and terminated with each call. This paper describes quantitative techniques for choosing process replication or threading levels and process activation policies. Inappropriate policies can lead to unnecessary queuing delays for callers or unnecessarily high consumption of memory resources. The algorithms presented consider all workload conditions, are iterative in nature and are hybrid mathematical programming and analytic performance evaluation methods. An example is given to demonstrate the technique and describe how the results can be applied during software design and deployment",0098-5589;00985589,,10.1109/32.888630,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=888630,,Algorithm design and analysis;Delay;Iterative algorithms;Iterative methods;Licenses;Mathematical programming;Performance analysis;Process design;Software design;Yarn,distributed object management;distributed programming;mathematical programming;multi-threading;queueing theory;software performance evaluation;systems analysis,analytic performance evaluation methods;bottlenecks;closed queuing networks;common goals;distributed application systems;distributed design;implementation constraints;iterative algorithms;licensing constraints;linear programming;mathematical programming;memory resource consumption;nonlinear programming;object classes;performance analysis;performance modeling;process activation design;process replication design;quantitative approach;queuing delays;relative load;resource constraints;server processes;software deployment;software design;system behavior;threading policy;user classes;user requests;workload conditions,,17,,18,,no,Dec-00,,IEEE,IEEE Journals & Magazines
E-representative: a scalability scheme for e-commerce,W. Meira; D. Menasce; V. Almeida; R. Fonseca,"Dept. of Comput. Sci., Univ. Federal de Minas Gerais, Belo Horizonte, Brazil",Proceedings Second International Workshop on Advanced Issues of E-Commerce and Web-Based Information Systems. WECWIS 2000,20020806,2000,,,168,175,"In order to meet the quality of service demanded by a growing number of online customers, e-commerce services need to use scalability techniques. This paper introduces the concept of e-commerce representatives, a means of scaling the performance of e-commerce services. E-representatives are programs that execute on a cache server or at nearby machines. E-representatives can be implemented using redirection, a mechanism available in popular cache servers. Using analytical and simulation models, we show the potential performance gains obtained by e-commerce sites that distribute their services among e-representatives and contribute to reduce bandwidth consumption and network latency",,POD:0-7695-0610-0,10.1109/WECWIS.2000.853872,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=853872,,Analytical models;Bandwidth;Business;Computer science;Delay;Electronic commerce;Network servers;Quality of service;Scalability;Telecommunication traffic,cache storage;electronic commerce;file servers;quality of service;software performance evaluation,analytic models;bandwidth consumption;cache server;distributed services;electronic commerce representatives;network latency;online customers;performance gains;performance scaling;redirection;scalability scheme;service quality;simulation models,,3,4,19,,no,2000,08 Jun 2000-09 Jun 2000,IEEE,IEEE Conference Publications
On the stability of a Tank and Hopfield type neural network in the general case of complex eigenvalues,M. T. Hanna,"Dept. of Eng. Math. & Phys., Cairo Univ., Fayoum, Egypt",IEEE Transactions on Signal Processing,20020806,2000,48,1,289,293,"The stability of a Tank and Hopfield-type neural network is investigated for the general case of practically encountered complex eigenvalues s<sub>D</sub> of the matrix product D<sub>g</sub><sup>T</sup> D<sub>f</sub>, where D<sub>g</sub> and D<sub>f </sub> are approximations of the connection matrix D on the signal and constraint sides of the neural net, respectively. A stability criterion in the form of an analytic expression is derived, thus generalizing the results obtained by Yan (1991) for the special case of purely real eigenvalues",1053-587X;1053587X,,10.1109/78.815505,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=815505,,Circuit stability;Computer aided software engineering;DH-HEMTs;Discrete Fourier transforms;Eigenvalues and eigenfunctions;Hopfield neural networks;Intelligent networks;Neural networks;Operational amplifiers;Stability criteria,Hopfield neural nets;eigenvalues and eigenfunctions;matrix algebra;stability,Tank and Hopfield type neural network;complex eigenvalues;connection matrix;stability criterion,,2,,9,,no,Jan-00,,IEEE,IEEE Journals & Magazines
Parametric design synthesis of distributed embedded systems,Dong-In Kang; R. Gerber; M. Saksena,"Univ. of South Carolina, Arlington, VA, USA",IEEE Transactions on Computers,20020806,2000,49,11,1155,1169,"This paper presents a design synthesis method for distributed embedded systems. In such systems, computations can flow through long pipelines of interacting software components, hosted on a variety of resources, each of which is managed by a local scheduler. Our method automatically calibrates the local resource schedulers to achieve the system's global end-to-end performance requirements. A system is modeled as a set of distributed task chains (or pipelines), where each task represents an activity requiring nonzero load from some CPU or network resource. Task load requirements can vary stochastically due to second-order effects like cache memory behavior, DMA interference, pipeline stalls, bus arbitration delays, transient head-of-line blocking, etc. We aggregate these effects-along with a task's per-service load demand and model them via a single random variable, ranging over an arbitrary discrete probability distribution. Load models can be obtained via profiling tasks in isolation or simply by using an engineer's hypothesis about the system's projected behavior. The end-to-end performance requirements are posited in terms of throughput and delay constraints. Specifically, a pipeline's delay constraint is an upper bound on the total latency a computatation can accumulate, from input to output. The corresponding throughput constraint mandates the pipeline's minimum acceptable output rate-counting only outputs which meet their delay constraints. Since per-component loads can be generally distributed; and since resources host stages from multiple pipelines, meeting all of the system's end-to-end constraints is a nontrivial problem. Our approach involves solving two subproblems in tandem: 1) finding an optimal proportion of load to allocate to each task and channel and 2) deriving the best combination of service intervals over which all load proportions can be guaranteed. The design algorithms use analytic approximations to quickly estimate output rates and propagation delays for candidate solutions. When all parameters are synthesized, the estimated end-to-end performance metrics are rechecked by simulation. The per-component load reservations can then be increased, with the synthesis algorithms rerun to improve performance. At that point, the - system can be configured according to the synthesized scheduling parameters-and then revalidated via on-line profiling. In this paper, we demonstrate our technique on an example system, and compare the estimated performance to its simulated on-line behavior.",0018-9340;00189340,,10.1109/12.895934,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=895934,,Algorithm design and analysis;Cache memory;Delay;Design methodology;Embedded system;Load modeling;Pipelines;Processor scheduling;Resource management;Throughput,delays;distributed processing;embedded systems;performance evaluation;processor scheduling,DMA interference;bus arbitration delays;cache memory behavior;delay constraint;distributed embedded systems;distributed task chains;interacting software components;local resource schedulers;local scheduler;nonzero load;optimal proportion;parametric design synthesis;performance metrics;performance requirements;propagation delays;upper bound,,4,,39,,no,Nov-00,,IEEE,IEEE Journals & Magazines
Performance scalability in multiprocessor systems with resource contention,S. Majumdar,"Dept. of Syst. & Comput. Eng., Carleton Univ., Ottawa, Ont., Canada",2000 IEEE International Symposium on Performance Analysis of Systems and Software. ISPASS (Cat. No.00EX422),20020806,2000,,,129,138,"Multiple processes may contend for shared resources such as variables stored in the shared memory of a multiprocessor system. Mechanisms required to preserve data consistency on such systems often lead do a decrease in system performance. This research focuses on controlling shared resource contention for achieving high capacity and scalability in multiprocessor based applications that include telephone switches and real time databases. Both reengineering of existing code as well as appropriate scheduling of the processes are two viable methods for controlling memory contention. Emphasis is placed on the second approach. Based on analytic models, three different scheduling approaches are compared. The numerical results obtained from the model provide insights into system behavior and highlight the important attributes of each strategy. A hybrid approach that combines the good attributes of a number of these strategies is proposed and analyzed. The results of this research are useful mainly to designers of software for multiprocessor based telecommunication and other embedded systems",,POD:0-7803-6418-X,10.1109/ISPASS.2000.842292,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=842292,,Access protocols;Database systems;Multiprocessing systems;Scalability;Switches;Systems engineering and theory;Telephony;Transaction databases;Writing,multiprocessing systems;performance evaluation;processor scheduling;resource allocation;systems re-engineering,analytic models;data consistency;embedded systems;hybrid approach;memory contention;multiple processes;multiprocessor based applications;multiprocessor based telecommunication;multiprocessor systems;performance scalability;real time databases;reengineering;resource contention;scheduling approaches;shared memory systems;shared resource contention;shared resources;system behavior;system performance;telephone switches,,0,,22,,no,2000,24 Apr 2000-25 Apr 2000,IEEE,IEEE Conference Publications
Phase-locked loop for grid-connected three-phase power conversion systems,S. K. Chung,"Dept. of Control & Instrum. Eng., Gyeongsang Nat. Univ., Kyungnam, South Korea",IEE Proceedings - Electric Power Applications,20020806,2000,147,3,213,219,"Analysis and design of a phase-locked loop (PLL) is presented for the power factor control of grid-connected three-phase power conversion systems. The dynamic characteristics of the closed loop PLL system with a second order are investigated in both continuous and discrete-time domains, and the optimisation method is discussed. In particular, the performance of the PLL in the three-phase system is analysed under the distorted utility conditions such as the phase unbalancing harmonics, and offset caused by nonlinear loads and measurement errors. The PLL technique for the three-phase system is implemented in software of a digital signal processor to verify the analytic results, and the experiments are carried out for various utility conditions. This technique is finally applied to the grid-connected photovoltaic power generation system with the current-controlled PWM inverter as a subpart for generating the current reference of the inverter. The experimental results demonstrate its phase tracking capability in the three-phase grid-connected operation",1350-2352;13502352,,10.1049/ip-epa:20000328,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=848556,,,DC-AC power convertors;PWM invertors;closed loop systems;harmonic distortion;phase locked loops;power conversion harmonics;power factor correction;power system interconnection,closed loop PLL system;continuous-time domain;current-controlled PWM inverter;discrete-time domain;dynamic characteristics;grid-connected three-phase power conversion systems;measurement errors;nonlinear loads;optimisation method;phase tracking capability;phase unbalancing harmonics;phase-locked loop;power factor control,,119,2,,,no,May-00,,IET,IET Journals & Magazines
PhysioNet: a research resource for studies of complex physiologic and biomedical signals,G. B. Moody; R. G. Mark; A. L. Goldberger,"Harvard-MIT Div. of Health Sci. & Technol., Cambridge, MA, USA",Computers in Cardiology 2000. Vol.27 (Cat. 00CH37163),20020806,2000,,,179,182,"PhysioNet (http://www.physionet.org/) is a web-based resource supplying well-characterized physiologic signals and related open-source software to the biomedical research community. Inaugurated in September 1999 under the auspices of the NIH's National Center for Research Resources (NCRR), PhysioNet provides an on-line forum for free dissemination and exchange of research data and software, with facilities for cooperative analysis of data and evaluation of new analytic methods. As of September 2000, PhysioBank, the data archive made available via PhysioNet, contained roughly 35 gigabytes of recorded signals and annotations. PhysioNet is a public service of the Research Resource for Complex Physiologic Signals, a cooperative project initiated by researchers at Boston's Beth Israel Deaconess Medical Center/Harvard Medical School, Boston University, McGill University, and MIT",0276-6547;02766547,POD:0-7803-6557-7,10.1109/CIC.2000.898485,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=898485,,Collaborative software;Data analysis;Error correction;Object oriented databases;Open source software;Rhythm;Roentgenium;Signal analysis;Software algorithms;Software tools,cardiology;electrocardiography;information resources;medical information systems;public domain software;telemedicine,Beth Israel Deaconess Medical Center;Boston University;Harvard Medical School;MIT;McGill University;NCRR;NIH National Center for Research Resources;PhysioBank;PhysioNet;September 1999;September 2000;biomedical research community;complex biomedical signals;complex physiologic signals;cooperative analysis;data archive;free dissemination;http://www.physionet.org/;new analytic methods;on-line forum;open-source software;public service;research data;research resource;web-based resource,,13,,25,,no,2000,24 Sep 2000-27 Sep 2000,IEEE,IEEE Conference Publications
Simulation of hardware support for OpenGL graphics architecture,T. Paltashev; N. Govind; G. Abla,"Dept. of Electr. & Comput. Eng., South Dakota Sch. of Mines & Technol., SD, USA",Proceedings International Conference on Information Technology: Coding and Computing (Cat. No.PR00540),20020806,2000,,,295,300,"The growing number of applications for 3D graphics and imaging systems in the mass market requires the customized approach to the design of high-performance 3D graphics and imaging system architectures. That fact coupled with the strong trends to industrial standardization of graphics API, such as OpenGL, leads to enhanced portability for imaging and graphics applications. The progress on the hardware support of API functionality gives a real possibility to use a top-down approach in the design of custom graphics/imaging systems based on the mixed hardware/software implementation of the OpenGL architecture. Software simulation is a relatively cheap and fast way for initial graphics architecture template development and evaluation to acquire the structure and parameters that can be used later for HDL elaboration and simulation. This paper discusses a 3D graphics and imaging system architecture model implementation based on OpenGL API hardware support simulation. The object-oriented approach has been used for this development of analytic and software models simulating hardware units on different stages of the graphics pipeline. The Interactive Imaging System Architecture Composer has been developed in this project for fast run-time simulation management, data acquisition and interpretation. The expandable set of architecture templates uses a reconfigurable shared library of ‰ÛÏvirtual hardware units‰Ûù simulating the different hardware structures for graphics and image processing",,POD:0-7695-0540-6,10.1109/ITCC.2000.844241,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=844241,,Analytical models;Application software;Computer architecture;Graphics;Hardware design languages;Object oriented modeling;Pipelines;Project management;Runtime;Standardization,application program interfaces;computer architecture;computer graphics;image processing;object-oriented programming;software architecture;software portability;virtual machines,3D graphics;HDL;Interactive Imaging System Architecture Composer;OpenGL graphics architecture;custom graphics systems;data acquisition;graphics API;graphics architecture template development;hardware simulation;image processing;object-oriented approach;reconfigurable shared library;run-time simulation management;software portability;standardization;virtual hardware units,,1,2,14,,no,2000,27 Mar 2000-29 Mar 2000,IEEE,IEEE Conference Publications
Spreading sequence sets with zero and low correlation zone for quasi-synchronous CDMA communication systems,Xinmin Deng; Pingzhi Fan,"Sch. of Comput. & Commun. Eng., Southwest Jiaotong Univ., Chengdu, China",Vehicular Technology Conference Fall 2000. IEEE VTS Fall VTC2000. 52nd Vehicular Technology Conference (Cat. No.00CH37152),20020806,2000,4,,1698,1703 vol.4,"This paper considers the application of sequences with low correlation zone (LCZ) and zero correlation zone (ZCZ) to quasi-synchronous DS/CDMA communication systems. Several classes of sequences with LCZ and ZCZ are described. Numerical results show that the BER performances of systems using ZCZ sequences and LCZ sequences are comparable with the same number of users and almost the same processing gain. In addition, when the timing error is large, the ZCZ sequences perform better than the LCZ sequences. Analytic results are verified with simulations",1090-3038;10903038,POD:0-7803-6507-0,10.1109/VETECF.2000.886114,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=886114,,Application software;Autocorrelation;Bit error rate;Code division multiplexing;Communication systems;Multiaccess communication;Multiple access interference;Performance gain;System performance;Timing,code division multiple access;correlation theory;error statistics;land mobile radio;sequences;spread spectrum communication;synchronisation,BER performances;DS-CDMA;LCZ sequences;ZCZ sequences;low correlation zone sequences;mobile radio;quasi-synchronous CDMA communication systems;spreading sequence sets;timing error;zero correlation zone sequences,,3,,17,,no,2000,24 Sep 2000-28 Sep 2000,IEEE,IEEE Conference Publications
Statistical process control: analyzing space shuttle onboard software process,W. A. Florac; A. D. Carleton; J. R. Barnard,"Software Eng. Inst., Carnegie Mellon Univ., Pittsburgh, PA, USA",IEEE Software,20020806,2000,17,4,97,106,Demand for increased software process efficiency and effectiveness places measurement demands on the software engineering community beyond those traditionally practiced. Statistical- and process-thinking principles lead to the use of statistical process control (SPC) methods to determine the consistency and capability of the processes used to develop software. The authors use data and analysis from a collaborative effort between the Software Engineering Institute (a federally funded research and development center sponsored by the US Department of Defense) and the Space Shuttle Onboard Software Project as a vehicle to illustrate the analytic processes analysts frequently encounter when using SPC,0740-7459;07407459,,10.1109/52.854075,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=854075,,Control charts;Engineering management;Process control;Software development management;Software engineering;Software measurement;Software performance;Space shuttles;Stability;Time measurement,aerospace computing;software development management;software process improvement;space vehicles;statistical process control,SPC;Software Engineering Institute;Space Shuttle Onboard Software Project;analytic processes;collaborative effort;measurement demands;process-thinking principles;software engineering community;software process efficiency;space shuttle onboard software process;statistical process control,,34,,7,,no,Jul/Aug 2000,,IEEE,IEEE Journals & Magazines
The design and implementation of a WWW traffic generator,K. Y. Leung; K. H. Yeung,"Dept. of Electron. Eng., City Univ. of Hong Kong, China",Proceedings Seventh International Conference on Parallel and Distributed Systems (Cat. No.PR00568),20020806,2000,,,509,514,"With the growing importance of the World Wide Web, Web managers are facing the problem of how to accurately measure the performance of their Web servers. This paper addresses this problem by introducing a traffic generator which is developed by using the Java language. The generator generates Web workload by using an analytic model which models the arrivals at the user level. It is both self-scaling and self-configuring, meaning that it can scale the traffic to any intensities and configure the arrival characteristics to conform to those of any specific Web site. The latter is done by analyzing a site's access log for adjusting the model parameters being used during traffic generation. In contrast to other traffic generator being used in common Web benchmark software, the real content of the target Web server is used as the testing file set instead of just using a reduced standard file set. Experimental results on using the generator to test two Web servers are also presented. The results show that the generator is both functional and useful to measure the performance of Web servers under real situations",1521-9097;15219097,POD:0-7695-0568-6,10.1109/ICPADS.2000.857736,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=857736,,Benchmark testing;Engineering management;HTML;Java;Monitoring;Software testing;Traffic control;Web server;Web sites;World Wide Web,Java;information resources;search engines;telecommunication traffic,Java language;WWW traffic generator;Web benchmark software;Web servers;Web site access log;Web workload;analytic model;arrival modelling;performance measurement;testing file set,,0,,9,,no,2000,04 Jul 2000-07 Jul 2000,IEEE,IEEE Conference Publications
The effect of average parallelism and CPU-I/O overlap on application speedup,L. Diaconescu; S. Majumdar,"Dept. of Syst. & Comput. Eng., Carleton Univ., Ottawa, Ont., Canada",Proceedings Seventh International Conference on Parallel and Distributed Systems: Workshops,20020806,2000,,,370,377,Parallelization of I/O using multiple disk drives is warranted for achieving high performance on systems running CPU and I/O intensive applications. This paper is concerned with the characterization of applications with parallel I/O and the derivation of analytic bounds on speedup. New characteristics for capturing parallelism in I/O are introduced. These characteristics are used in conjunction with other characteristics available in the literature for the derivation of the bounds. Both parallelism in CPU and I/O operations in applications as well as the degree of overlap between CPU and I/O are observed to have a strong impact on speedup,,POD:0-7695-0571-6,10.1109/PADSW.2000.884646,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=884646,,Application software;Computer applications;Computer networks;Concurrent computing;Disk drives;Distributed computing;High performance computing;Parallel processing;System performance;Systems engineering and theory,disc drives;input-output programs;parallel processing;performance evaluation,CPU-input output overlap;application speedup;high performance;multiple disk drives;parallel input-output,,0,,9,,no,Oct-00,04 Jul 2000-07 Jul 2000,IEEE,IEEE Conference Publications
Using intelligent agents to identify missing and exploited children,S. G. Romaniuk,"Anger, Fairmont, Wu, USA",IEEE Intelligent Systems and their Applications,20020806,2000,15,2,27,30,"Recent high-profile stings and convictions in the USA have amply demonstrated the Internet's rapid expansion and its increasing use by pedophiles and other sexual predators. Unfortunately, inadequate manpower and fiscal support often mar federal and state investigative operations, and the Internet's ever-expanding growth will only make these efforts more difficult in the future. However, modern computer technology, especially the Internet and agent based software, might help ongoing and future investigations. As part of a National Institute of Justice grant, Analytic Services (Anser) is developing an intelligent-software-agent system that uses the Internet to locate missing and exploited children. We focused our agents in two areas: knowledge discovery and predictive data mining (KD&PDM) for intranet data sources and Web analytics, and intelligent search and discovery (IS&D) for the Internet. Our system is, so far, only an alpha system. We haven't used it with a customer, even though several have expressed interest in its usage after viewing some of its findings and demonstrations. We still need to provide a user interface (the system is completely customizable) and we continue to test the various agents over the Internet, in chat rooms, and in intranets",1094-7167;10947167,,10.1109/5254.850824,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=850824,,Distributed databases;Intelligent agent;Internet;Network servers;Neural networks;Relays;Search engines;User interfaces;Web pages;Web server,Internet;data mining;government data processing;intranets;law administration;software agents,Analytic Services;Anser;Internet;USA;Web analytics;agent based software;alpha system;chat rooms;exploited children;fiscal support;intelligent search and discovery;intelligent software agent system;intranet data sources;knowledge discovery;missing children;modern computer technology;predictive data mining;state investigative operations;user interface,,7,,6,,no,Mar/Apr 2000,,IEEE,IEEE Journals & Magazines
WebSifter: an ontology-based personalizable search agent for the Web,A. Scime; L. Kerschberg,"State Univ. of New York, Brockport, USA",Proceedings 2000 Kyoto International Conference on Digital Libraries: Research and Practice,20020806,2000,,,203,210,"The World Wide Web provides access to a great deal of information on a vast array of subjects. In a typical Web search a vast amount of information is retrieved. The quantity can be overwhelming, and much of the information may be marginally relevant or completely irrelevant to the user's request. We present a methodology, architecture, and proof-of-concept prototype for query construction and results analysis that provides the user with a ranking of choices based on the user's determination of importance. The user initially designs the query with assistance from the user's profile, a thesaurus, and previously constructed queries acting as a taxonomy of the information requirements. After the query has returned its results, decision analytic methods and information source reliability information are used in conjunction with the expanded taxonomy to rank the solution candidates",,POD:0-7695-1022-1,10.1109/DLRP.2000.942176,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=942176,,Information analysis;Ontologies;Prototypes;Search engines;Taxonomy;Thesauri;Web pages;Web search;Web server;Web sites,Internet;information resources;search engines;software agents,Web search;WebSifter;decision analytic methods;domain knowledge;importance;information requirements;information source reliability information;ontology-based personalizable search agent;profile;query construction;ranking;results analysis;thesaurus,,5,6,20,,no,2000,13 Nov 2000-16 Nov 2000,IEEE,IEEE Conference Publications
A cost framework for evaluating integrated restructuring optimizations,B. Chandramouli; J. B. Carter; W. C. Hsieh; S. A. McKee,"Sch. of Comput., Utah Univ., Salt Lake City, UT, USA",Proceedings 2001 International Conference on Parallel Architectures and Compilation Techniques,20020807,2001,,,131,140,"Loop transformations and array restructuring optimizations usually improve performance by increasing the memory locality of applications, but not always. For instance, loop and array restructuring can either complement or compete with one another. Previous research has proposed integrating loop and array restructuring, but there existed no analytic framework for determining how best to combine the optimizations for a given program. Since the choice of which optimizations to apply, alone or in combination, is highly application and input-dependent, a cost framework is needed if integrated restructuring is to be automated by an optimizing compiler. To this end, we develop a cost model that considers standard loop optimizations along with two potential forms of array restructuring: conventional copying-based restructuring and remapping-based restructuring that exploits a smart memory controller. We simulate eight applications on a variety of input sizes and with a variety of hand-applied restructuring optimizations. We find that employing a fixed strategy does not always deliver the best performance. Finally; our cost model accurately predicts the best combination of restructuring optimizations among those we examine, and yields performance within a geometric mean of 5% of the best combination across all benchmarks and input sizes",1089-796X;1089796X,POD:0-7695-1363-8,10.1109/PACT.2001.953294,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=953294,,Application software;Automatic control;Bandwidth;Cost function;Delay;Hardware;Optimizing compilers;Predictive models;Size control;Software performance,data structures;optimising compilers;program control structures;software performance evaluation,array restructuring optimizations;copying-based restructuring;cost framework;loop optimizations;loop transformations;memory locality;optimizing compiler;performance;remapping-based restructuring;smart memory controller,,2,,17,,no,2001,08 Sep 2001-12 Sep 2001,IEEE,IEEE Conference Publications
A weighted distribution of residual failure data using Weibull model,Soo-Jong Lee; Kyeong-Ho Lee,"Lab. of Switching & Transmission Technol., ETRI, Taejon, South Korea",Proceedings 15th International Conference on Information Networking,20020807,2001,,,265,270,"In analyzing the distribution of residual failure data using the Weibull distribution from the first detection of a failure to the completion of repairing failures, we have proposed the method of applying a weight to each failure. To put it concretely, we have suggested the method of applying a weight according to the importance of each failure within the analytic area in order to manage and measure efficiently the failure data found in the course of confirmation, verification, debugging, adding, and repairing functions collectively. The procedure is carried out after constituting a model switching system and installing a synthesis software package on the HANbit ACE ATM switching system under the development phase, which is the main node in building the B-ISDN for multimedia services",,POD:0-7695-0951-7,10.1109/ICOIN.2001.905438,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=905438,,Asynchronous transfer mode;B-ISDN;Bit rate;Failure analysis;Quality assurance;Reliability;Research and development;Switches;Switching systems;Weibull distribution,B-ISDN;Weibull distribution;asynchronous transfer mode;electronic switching systems;failure analysis;multimedia communication;packet switching;software packages;telecommunication network reliability,B-ISDN;HANbit ACE ATM switching system;Weibull model;confirmation;debugging;failure data management;failure data measurement;failure detection;failure importance;failure repair;model switching system;multimedia services;residual failure data analysis;synthesis software package;verification;weighted distribution,,1,,6,,no,2001,31 Jan 2001-02 Feb 2001,IEEE,IEEE Conference Publications
An analytic throughput model for TCP Reno over wireless networks,Zheng Feng; Li Min; Gao Chuanshan,"Dept. of Comput. Sci. & Eng., Fudan Univ., Shanghai, China",Proceedings 2001 International Conference on Computer Networks and Mobile Computing,20020807,2001,,,111,116,"Many wireless network applications are built on TCP, and will continue to be in the foreseeable future. It is important to study TCP performance in wireless networks scenario with high loss rate. In this paper, we develop a simple analytic characterization of the steady state throughput of a bulk transfer TCP Reno flow over wireless networks, as a function of loss rate and round trip time. Our model incorporates many important aspects, such as slow start, congestion avoidance, fast retransmit, fast recovery and timeout mechanism. Our simulation results demonstrate that our model is able to accurately predict TCP throughput over wireless networks and is accurate over a wide range of loss rate",,POD:0-7695-1381-6,10.1109/ICCNMC.2001.962584,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=962584,,Analytical models;Application software;Computational modeling;Computer science;Performance loss;Predictive models;Steady-state;TCPIP;Throughput;Wireless networks,digital simulation;performance evaluation;telecommunication congestion control;transport protocols;wireless LAN,TCP Reno;TCP performance;analytic characterization;analytic throughput model;congestion avoidance;simulation results;timeout mechanism;wireless networks,,2,,7,,no,2001,16 Oct 2001-19 Oct 2001,IEEE,IEEE Conference Publications
An internally replicated quasi-experimental comparison of checklist and perspective based reading of code documents,O. Laitenberger; K. El Emam; T. G. Harbich,"Fraunhofer Inst. for Exp. Software Eng., Kaiserslautern, Germany",IEEE Transactions on Software Engineering,20020807,2001,27,5,387,421,"The basic premise of software inspections is that they detect and remove defects before they propagate to subsequent development phases where their detection and correction cost escalates. To exploit their full potential, software inspections must call for a close and strict examination of the inspected artifact. For this, reading techniques for defect detection may be helpful since these techniques tell inspection participants what to look for and, more importantly, how to scrutinize a software artifact in a systematic manner. Recent research efforts investigated the benefits of scenario-based reading techniques. A major finding has been that these techniques help inspection teams find more defects than existing state-of-the-practice approaches, such as, ad-hoc or checklist-based reading (CBR). We experimentally compare one scenario-based reading technique, namely, perspective-based reading (PBR), for defect detection in code documents with the more traditional CBR approach. The comparison was performed in a series of three studies, as a quasi experiment and two internal replications, with a total of 60 professional software developers at Bosch Telecom GmbH. Meta-analytic techniques were applied to analyze the data",0098-5589;00985589,,10.1109/32.922713,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=922713,,Bioreactors;Computer Society;Costs;Data analysis;Inspection;Phase detection;Software engineering;Software performance;Software quality;Telecommunications,data analysis;program debugging;software engineering,Bosch Telecom;checklist-based reading;code documents;data analysis;defect detection;experiment;perspective based reading;professional software developers;scenario-based reading;software inspection,,38,,90,,no,1-May,,IEEE,IEEE Journals & Magazines
Analytic calculation of leakage inductances of dual frequency concentric windings transformers,J. El Hayek,"Coll. of Eng. & Archit. of Fribourg, Univ. of Appl. Sci. of Western Switzerland, Fribourg, Switzerland","Electrical Machines and Systems, 2001. ICEMS 2001. Proceedings of the Fifth International Conference on",20020806,2001,1,,194,197 vol.1,"Static power electronic converters supplying railways DC or AC motors, require series inductances which can simply be the leakage short-circuit inductances of the traction transformer. This paper describes a one-dimensional analytical method which permits to calculate leakage inductances between primary and secondary windings of dual frequency transformers. A comparison to a two-dimensional analytical method as well as to measurement results on existing units is made. Furthermore, illustration software developed in Visual Basic on Microsoft Excel is presented",,POD:7-5062-5115-9,10.1109/ICEMS.2001.970642,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=970642,,Current density;Educational institutions;Frequency;Inductance;Magnetic circuits;Power engineering and energy;Railway engineering;Transformers;Vectors;Voltage,inductance;magnetic leakage;power convertors;power transformers;railways;traction motors;transformer windings,AC motors;DC motors;Microsoft Excel;dual frequency concentric windings transformers;illustration software;leakage inductances calculation;leakage short-circuit inductances;one-dimensional analytical method;primary windings;railways motors;secondary windings;series inductances;static power electronic converters,,2,,6,,no,2001,18 Aug 2001-20 Aug 2001,IEEE,IEEE Conference Publications
Application of visual specifications for verification of distributed controllers,V. Vyatkin; H. M. Hanisch,"Dept. of Eng. Sci., Martin Luther Univ., Halle, Germany","2001 IEEE International Conference on Systems, Man and Cybernetics. e-Systems and e-Man for Cybernetics in Cyberspace (Cat.No.01CH37236)",20020806,2001,1,,646,651 vol.1,"In a search for an appropriate visual specification language to be applied in control engineering the timing diagram specification language is suggested. It is applied to the verification of distributed controllers following the standard IEC 61499. Specification of inputs and outputs of the controller are given in the graphical form of signal diagrams. The inputs are then converted into finite-state models, while the diagram of outputs is used to build equivalent analytic expressions in extended CTL. These two parts are used in formal verification of the control system",1062-922X;1062922X,POD:0-7803-7087-2,10.1109/ICSMC.2001.969925,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=969925,,Application software;Automatic control;Automation;Control engineering;Control systems;Distributed control;Formal verification;IEC standards;Specification languages;Timing,IEC standards;control system analysis computing;discrete event systems;distributed control;formal verification;industrial control;specification languages;visual languages,IEC 61499;control engineering;discrete-event modeling;distributed controllers;extended CTL;finite-state models;formal verification;signal diagrams;timing diagram specification language;visual specifications,,9,,13,,no,2001,07 Oct 2001-10 Oct 2001,IEEE,IEEE Conference Publications
Automating the analysis of option pricing algorithms through intelligent knowledge acquisition approaches,V. S. Verykios; E. N. Houstis; L. H. Tsoukala; K. N. Pantazopoulos,"Coll. of Inf. Sci. & Technol., Drexel Univ., Philadelphia, PA, USA","IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans",20020806,2001,31,6,573,586,"The traditional approach for estimating the performance of numerical methods is to combine an operation's count with an asymptotic error analysis. This analytic approach gives a general feel of the comparative efficiency of methods, but it rarely leads to very precise results. It is now recognized that accurate performance evaluation can be made only with actual measurements on working software. Given that such an approach requires an enormous amount of performance data related to actual measurements, the development of novel approaches and systems that intelligently and efficiently analyze these data is of great importance to scientists and engineers. The paper presents intelligent knowledge acquisition approaches and an integrated prototype system, which enables the automatic and systematic analysis of performance data. The system analyzes the performance data which is usually stored in a database with statistical, and inductive learning techniques and generates knowledge which can be incorporated in a knowledge base incrementally. We demonstrate the use of the system in the context of a case study, covering the analysis of numerical algorithms for the pricing of American vanilla options in a Black and Scholes modeling framework. We also present a qualitative and quantitative comparison of two techniques used for the automated knowledge acquisition phase",1083-4427;10834427,,10.1109/3468.983414,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=983414,,Algorithm design and analysis;Data analysis;Data engineering;Error analysis;Intelligent systems;Knowledge acquisition;Performance analysis;Pricing;Software measurement;Software performance,data mining;knowledge based systems;object-oriented methods;statistical analysis;stock markets,American vanilla options;Black and Scholes modeling framework;accurate performance evaluation;asymptotic error analysis;comparative efficiency;data mining;decision trees;inductive learning techniques;inductive logic programming;integrated prototype system;intelligent knowledge acquisition approaches;knowledge base;option pricing algorithms;statistical techniques,,2,,33,,no,1-Nov,,IEEE,IEEE Journals & Magazines
Building dependable software for critical applications: multi-version software versus one good version,P. Townend; Jie Xu; M. Munro,"Dept. of Comput. Sci., Durham Univ., UK",Proceedings Sixth International Workshop on Object-Oriented Real-Time Dependable Systems,20020807,2001,,,103,110,"An increasing range of industries have a growing dependence on software based systems, many of which are safety-critical, real-time applications that require extremely high dependability. Multi-version programming has been proposed as a method for increasing the overall dependability of such systems. We describe an experiment to establish whether or not the multi-version method can offer increased dependability over the traditional single-version development approach when given the same level of resources. Three programs were developed independently to control a real-time, safety-critical system, and were put together to form a decentralized multi-version system. Three functionally equivalent single-version systems. were also implemented, each using the same amount of development resources as the combined resources of the multi-version system. The analytic results from this experiment show that 1) a single-version system is much more dependable than any individual version of the multi-version system, and 2) despite the poor quality of individual versions, the multi-version method still results in a safer system than the single-version solution. It is evident that regarding the single-version method as a ""seem-to-be"" safer design decision for critical applications is not generally justifiable. We conclude by describing plans for a follow up study based on our initial findings",1530-1443;15301443,POD:0-7695-1068-X,10.1109/WORDS.2001.945120,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=945120,,Application software;Computer industry;Computer science;Control systems;Costs;Embedded software;Embedded system;Software engineering;Software safety;Software systems,configuration management;real-time systems;safety-critical software;software fault tolerance,critical applications;decentralized multi-version system;dependable software;design decision;development resources;functionally equivalent single-version systems;high dependability;multi-version method;multi-version programming;real-time safety-critical system;safety-critical real-time applications;software based systems,,2,1,25,,no,2001,08 Jan 2001-10 Jan 2001,IEEE,IEEE Conference Publications
Comparison using AHP Web-based learning with classroom learning,H. Murakoshi; T. Kawarasaki; K. Ochimizu,"Schoolof Inf. Sci., Japan Adv. Inst. of Sci. & Technol., Ishikawa, Japan",Proceedings 2001 Symposium on Applications and the Internet Workshops (Cat. No.01PR0945),20020807,2001,,,67,73,"We have developed a Web-base learning system for teaching ""software design methodology"". We also have developed a method using analytic hierarchy process diagrams to evaluate the effectiveness of Web-based learning. Using the method, we evaluated the effectiveness of Web-based learning compared with classroom learning. Based on the results, we are improving our Web-based learning system",,POD:0-7695-0945-2,10.1109/SAINTW.2001.998212,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=998212,,Computer aided instruction;Courseware;Design methodology;Distance learning;Education;Educational institutions;Engineering profession;Information science;Internet;Learning systems,computer aided instruction;educational courses;software engineering,Web-based learning;analytic hierarchy process;classroom learning;software design methodology,,1,,3,,no,2001,08 Jan 2001-12 Jan 2001,IEEE,IEEE Conference Publications
Computation of the electric field around a wet polluted insulator by analytic and numerical techniques,P. Basappa; V. Lakdawala; G. Gerdin,"Norfolk State Univ., VA, USA",2001 Annual Report Conference on Electrical Insulation and Dielectric Phenomena (Cat. No.01CH37225),20020807,2001,,,564,567,"A benchmark problem of a cylindrical insulator with a wet and dry band is formulated. An analytic solution was obtained and numerically evaluated, and compared with the solution obtained from: (1) a finite-differencing technique; and (2) a commercially available finite-element software. The agreement between the solutions obtained from the three methods was found to be within 10 to 20%",,POD:0-7803-7053-8,10.1109/CEIDP.2001.963606,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=963606,,Conductivity;Dielectrics and electrical insulation;Electrodes;Environmentally friendly manufacturing techniques;Finite difference methods;Flashover;Geometry;Pollution;Resistors;Voltage,electric fields;electrical engineering computing;finite difference methods;finite element analysis;flashover;insulator contamination;insulator testing,electric field calculations;finite-differencing technique;finite-element software;polluted outdoor insulation flashover;wet polluted cylindrical insulator,,0,,8,,no,2001,14 Oct 2001-17 Oct 2001,IEEE,IEEE Conference Publications
Denormalization effects on performance of RDBMS,G. L. Sanders; Seungkyoon Shin,"Manage. Sci. & Syst., State Univ. of New York, Buffalo, NY, USA",Proceedings of the 34th Annual Hawaii International Conference on System Sciences,20020807,2001,,,9 pp.,,"Presents a practical view of denormalization, and provides fundamental guidelines for incorporating denormalization. We have suggested using denormalization as an intermediate step between logical and physical modeling, to be used as an analytic procedure for the design of the applications requirements criteria. Relational algebra and query trees are used to examine the effect on the performance of relational database management systems (RDBMS). The guidelines and methodology presented are sufficiently general, and they can be applicable to most databases. It is concluded that denormalization can enhance query performance when it is deployed with a complete understanding of application requirements.",,POD:0-7695-0981-9,10.1109/HICSS.2001.926306,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=926306,,Algebra;Data models;Data warehouses;Guidelines;Information retrieval;Relational databases;Spatial databases;Stability;System performance;Transaction databases,database theory;query processing;relational algebra;relational databases;software performance evaluation,analytic procedure;applications requirements criteria design;denormalization;logical modeling;physical modeling;query performance;query trees;relational DBMS performance;relational algebra,,8,4,24,,no,6-6 Jan. 2001,,IEEE,IEEE Conference Publications
Extended specifications and test data sets for data level comparisons of direct volume rendering algorithms,Kwansik Kim; C. M. Wittenbrink; A. Pang,"Unigraphics Div., Electron. Data Syst. Corp., Cypress, CA, USA",IEEE Transactions on Visualization and Computer Graphics,20020806,2001,7,4,299,317,"Direct volume rendering (DVR) algorithms do not generate intermediate geometry to create a visualization, yet they produce countless variations in the resulting images. Therefore, comparative studies are essential for objective interpretation. Even though image and data level comparison metrics are available, it is still difficult to compare results because of the numerous rendering parameters and algorithm specifications involved. Most of the previous comparison methods use information from the final rendered images only. We overcome limitations of image level comparisons with our data level approach using intermediate rendering information. We provide a list of rendering parameters and algorithm specifications to guide comparison studies. We extend Williams and Uselton's rendering parameter list with algorithm specification items and provide guidance on how to compare algorithms. Real data are often too complex to study algorithm variations with confidence. Most of the analytic test data sets reported are often useful only for a limited feature of DVR algorithms. We provide simple and easily reproducible test data sets, a checkerboard and a ramp, that can make clear differences in a wide range of algorithm variations. With data level metrics, our test data sets make it possible to perform detailed comparison studies. A number of examples illustrate how to use these tools",1077-2626;10772626,,10.1109/2945.965345,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=965345,,Algorithm design and analysis;Computational geometry;Data analysis;Data visualization;Image quality;Performance evaluation;Pipelines;Rendering (computer graphics);Testing;Uncertainty,program testing;rendering (computer graphics);software metrics,Direct Volume Rendering;data level comparisons;data level metrics;image level comparisons;image quality;rendered images;uncertainty visualization,,6,,33,,no,Oct-Dec 2001,,IEEE,IEEE Journals & Magazines
Hypercomplex signals-a novel extension of the analytic signal to the multidimensional case,T. Bulow; G. Sommer,"GRASP Lab., Pennsylvania Univ., Philadelphia, PA, USA",IEEE Transactions on Signal Processing,20020807,2001,49,11,2844,2852,"The construction of Gabor's (1946) complex signal-which is also known as the analytic signal-provides direct access to a real one-dimensional (1-D) signal's local amplitude and phase. The complex signal is built from a real signal by adding its Hilbert transform-which is a phase-shifted version of the signal-as an imaginary part to the signal. Since its introduction, the complex signal has become an important tool in signal processing, with applications, for example, in narrowband communication. Different approaches to an n-D analytic or complex signal have been proposed in the past. We review these approaches and propose the hypercomplex signal as a novel extension of the complex signal to n-D. This extension leads to a new definition of local phase, which reveals information on the intrinsic dimensionality of the signal. The different approaches are unified by expressing all of them as combinations of the signal and its partial and total Hilbert transforms. Examples that clarify how the approaches differ in their definitions of local phase and amplitude are shown. An example is provided for the two-dimensional (2-D) hypercomplex signal, which shows how the novel phase concept can be used in texture segmentation",1053-587X;1053587X,,10.1109/78.960432,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=960432,,Computer aided software engineering;Fourier transforms;Frequency;Gabor filters;Image processing;Multidimensional signal processing;Multidimensional systems;Narrowband;Signal analysis;Signal processing,Hilbert transforms;image segmentation;image texture;multidimensional signal processing,2D hypercomplex signals;Gabor's complex signal;Hilbert transform;analytic signal;complex signal;multidimensional signal;narrowband communication;partial Hilbert transform;real 1D signal local amplitude;real 1D signal local phase;signal processing;texture segmentation;total Hilbert transform,,142,1,33,,no,1-Nov,,IEEE,IEEE Journals & Magazines
"Modeling, design, virtual and physical prototyping, testing, and verification of a multifunctional processor queue for a single-chip multiprocessor architecture",J. R. Heath; A. Tan,"Dept. of Electr. Eng., Kentucky Univ., Lexington, KY, USA",Proceedings 12th International Workshop on Rapid System Prototyping. RSP 2001,20020807,2001,,,128,133,"Critical to run-time processor resource allocation, reconfiguration, and control of a reconfigurable heterogeneous single-chip multiprocessor architecture is a defined multifunctional queue required by each processor of the architecture. The multifunctional queue implements six functions required for control, resource allocation, and reconfiguration within the architecture. In addition to normal queue functionality of First In First Out (FIFO) operation and empty/full indicator, the multifunctional queue implements the additional non-common functions of indicating when queue depth has reached a programmable threshold level, it indicates queue occupancy level at all times, it continually indicates queue input rate over a programmable time interval, it continually indicates queue input rate change over a programmable time interval and it can implement a pseudo-RAM function. An analytic functional model of the queue is first presented then an organization, architecture and design is developed followed by the development of appropriate analytic real-time performance metrics for the queue. Both virtual and Field Programmable Gate array (FPGA) based prototypes of the queue are then developed and used for functional, maximum frequency, and/or performance model testing resulting in verification of desired queue functionality and performance. A contribution of the queue is its functional versatility which would allow its use in computer architectures or processors other than the described target architecture",,POD:0-7695-1206-2,10.1109/IWRSP.2001.933850,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=933850,,Computer architecture;Field programmable gate arrays;Frequency;Measurement;Performance analysis;Queueing analysis;Resource management;Runtime;Testing;Virtual prototyping,computer architecture;field programmable gate arrays;formal verification;microprocessor chips;performance evaluation;queueing theory;resource allocation;software prototyping,FPGA;computer architectures;formal verification;multifunctional processor queue;performance model;physical prototyping;programmable threshold level;pseudo-RAM function;queue functionality;queue occupancy level;reconfiguration;run-time processor resource allocation;single-chip multiprocessor architecture;virtual prototyping,,1,,15,,no,2001,25 Jun 2001-27 Jun 2001,IEEE,IEEE Conference Publications
On the syllogistic structure of object-oriented programming,D. Rayside; K. Kontogiannis,"Dept. of Electr. & Comput. Eng., Waterloo Univ., Ont., Canada",Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001,20020807,2001,,,113,122,"Recent works by J.F. Sowa (2000) and D. Rayside and G.T. Campbell (2000) demonstrate that there is a strong connection between object-oriented programming and the logical formalism of the syllogism, first set down by Aristotle in the Prior Analytics (1928). In this paper, we develop an understanding of polymorphic method invocations in terms of the syllogism, and apply this understanding to the design of a novel editor for object-oriented programs. This editor is able to display a polymorphic call graph, which is a substantially more difficult problem than displaying a non-polymorphic call graph. We also explore the design space of program analyses related to the syllogism, and find that this space includes Unique Name, Class Hierarchy Analysis, Class Hierarchy Slicing, Class Hierarchy Specialization, and Rapid Type Analysis.",0270-5257;02705257,POD:0-7695-1050-7,10.1109/ICSE.2001.919086,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=919086,,Animal structures;Computer displays;Concrete;Humans;Logic programming;Mood;Object oriented programming;Programming profession;Space exploration,object-oriented programming,logical formalism;object-oriented programming;polymorphic call graph;polymorphic method invocations;syllogistic structure,,3,1,20,,no,12-19 May 2001,,IEEE,IEEE Conference Publications
Optimization of quadratic performance indexes for nonlinear control systems,N. Kazantzis; C. Kravaris; R. A. Wright,"Dept. of Chem. Eng., Worcester Polytech. Inst., MA, USA",Proceedings of the 40th IEEE Conference on Decision and Control (Cat. No.01CH37228),20020806,2001,3,,2758,2762 vol.3,"The proposed approach aims at the development of a systematic method to optimally choose the controller tunable parameters in a nonlinear control system, where in addition to the traditional set of closed-loop performance specifications, optimality is also requested with respect to the physically meaningful quadratic performance index. In particular, the value of the performance index can be calculated exactly by solving the Zubov partial differential equation (PDE). It can be shown that the Zubov PDE admits a unique and locally analytic solution that is endowed with the properties of a Lyapunov function for the closed-loop system. Moreover, the analyticity property of the solution of Zubov PDE enables the development of a series solution method that can be easily implemented with the aid of a symbolic software package. It can be shown that the evaluation of the above Lyapunov function at the initial conditions leads to a direct calculation of the value of the performance index which now explicitly depends on the controller parameters. Therefore, the employment of static optimization techniques can provide the optimal values of the finite-set of controller parameters. Finally, it shown that an explicit estimate of the size of the closed-loop stability region can be provided by using results from the Zubov stability theory",,POD:0-7803-7061-9,10.1109/.2001.980690,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=980690,,Chemical engineering;Control nonlinearities;Control systems;Electronic mail;Employment;Linear feedback control systems;Nonlinear control systems;Optimal control;Performance analysis;Stability,Lyapunov methods;closed loop systems;nonlinear control systems;optimal control;parameter estimation;partial differential equations;performance index;stability,Lyapunov function;Zubov partial differential equation;closed-loop systems;initial conditions;nonlinear control systems;optimization;parameter estimation;quadratic performance index;stability region,,0,,,,no,2001,04 Dec 2001-07 Dec 2001,IEEE,IEEE Conference Publications
Performance and scalability analysis on client-server workflow architecture,Kwang-Hoon Kim; Dong-Soo Han,"Dept. of Comput. Sci., Kyonggi Univ., South Korea",Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001,20020807,2001,,,179,186,"We excogitate a performance analytic model and describe its analysis results conceived to be helpful in the understanding of the spectrum of possibilities for large-scale workflow architecture. The analytic model is extended to represent several types of client-server workflow architectures. Especially, we focus on performance estimates of the conventional workflow management systems that are characterized by the client-server workflow architectures. The development of a workflow management system is typically a large and complex task. Decisions need to be made about the hardware and software platforms, the data structures, the algorithms, and the interconnection of various modules utilized by various users and administrators. These design decisions are further complicated by the requirements, such as scalability, flexibility, robustness, speed, and usability. We are particularly concerned about issues of scalability to see how well the client-server workflow architecture is dealing with the large amount of workcases. Finally, we graphically show the comparisons of performance evaluation results for several types of client-server workflow architectures on behalf of the single-server, and the multiple-server workflow systems on the distribution environment",1521-9097;15219097,POD:0-7695-1153-8,10.1109/ICPADS.2001.934817,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=934817,,Computer architecture;Data structures;Hardware;Large-scale systems;Performance analysis;Robustness;Scalability;Software algorithms;Usability;Workflow management software,client-server systems;performance evaluation;workflow management software,client-server system;data structures;performance analysis;performance estimates;scalability;scalability analysis;usability;workflow architecture;workflow management systems,,1,,20,,no,2001,26 Jun 2001-29 Jun 2001,IEEE,IEEE Conference Publications
Predicting with sparse data,M. Shepperd; M. Cartwright,"Sch. of Design, Eng. & Comput., Bournemouth Univ., Poole, UK",IEEE Transactions on Software Engineering,20020806,2001,27,11,987,998,"It is well-known that effective prediction of project cost related factors is an important aspect of software engineering. Unfortunately, despite extensive research over more than 30 years, this remains a significant problem for many practitioners. A major obstacle is the absence of reliable and systematic historic data, yet this is a sine qua non for almost all proposed methods: statistical, machine learning or calibration of existing models. The authors describe our sparse data method (SDM) based upon a pairwise comparison technique and T.L. Saaty's (1980) Analytic Hierarchy Process (AHP). Our minimum data requirement is a single known point. The technique is supported by a software tool known as DataSalvage. We show, for data from two companies, how our approach, based upon expert judgement, adds value to expert judgement by producing significantly more accurate and less biased results. A sensitivity analysis shows that our approach is robust to pairwise comparison errors. We then describe the results of a small usability trial with a practicing project manager. From this empirical work, we conclude that the technique is promising and may help overcome some of the present barriers to effective project prediction",0098-5589;00985589,,10.1109/32.965339,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=965339,,Calibration;Costs;Machine learning;Programming;Project management;Robustness;Sensitivity analysis;Software engineering;Software tools;Usability,data analysis;software cost estimation;software reliability;software tools,AHP;Analytic Hierarchy Process;DataSalvage;SDM;expert judgement;minimum data requirement;pairwise comparison errors;pairwise comparison technique;practicing project manager;project cost related factor prediction;project prediction;sensitivity analysis;single known point;software engineering;software project effort;software tool;sparse data;sparse data method;systematic historic data;usability trial,,39,,32,,no,1-Nov,,IEEE,IEEE Journals & Magazines
Predicting with sparse data,M. Shepperd; M. Cartwright,"Empirical Software Eng. Res. Group, Bournemouth Univ., Poole, UK",Proceedings Seventh International Software Metrics Symposium,20020807,2001,,,28,39,"It is well known that effective prediction of project cost related factors is an important aspect of software engineering. Unfortunately, despite extensive research over more than 30 years, this remains a significant problem for many practitioners. A major obstacle is the absence of reliable and systematic historic data, yet this is a sine qua non for almost all proposed methods: statistical, machine learning or calibration of existing models. The authors describe their sparse data method (SDM) based upon a pairwise comparison technique and T.L. Saaty's (1980) Analytic Hierarchy Process. Our minimum data requirement is a single known point. The technique is supported by a software tool known as DataSalvage. We show, for data from two companies, how our approach, based upon expert judgement, adds value to expert judgement by producing significantly more accurate and less biased results. A sensitivity analysis shows that our approach is robust to pairwise comparison errors. We then describe the results of a small usability trial with a practising project manager. From this empirical work we conclude that the technique is promising and may help overcome some of the present barriers to effective project prediction",1530-1435;15301435,POD:0-7695-1043-4,10.1109/METRIC.2001.915513,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=915513,,Calibration;Costs;Design engineering;Machine learning;Project management;Robustness;Sensitivity analysis;Software engineering;Software tools;Usability,data analysis;expert systems;project management;software cost estimation,Analytic Hierarchy Process;DataSalvage;SDM;expert judgement;minimum data requirement;pairwise comparison errors;pairwise comparison technique;practising project manager;project cost prediction;project cost related factors;sensitivity analysis;small usability trial;software engineering;software tool;sparse data method;systematic historic data,,0,,25,,no,2001,04 Apr 2001-06 Apr 2001,IEEE,IEEE Conference Publications
Proceedings 2001 IEEE International Conference on Data Mining,,,Proceedings 2001 IEEE International Conference on Data Mining,20020807,2001,,,iii,,The following topics are dealt with:data mining; intelligent agents with data mining capabilities; multi-modality interfaces and Internet retrieval systems; data mining for marketing; effective data mining and online analytic mining for efficient use of the Internet; agent technology; and collaborative work,,POD:0-7695-1119-8,10.1109/ICDM.2001.989492,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=989492,,,Internet;data mining;groupware;information retrieval;software agents;very large databases,Internet retrieval systems;agent technology;collaborative work;data mining;intelligent agents;marketing;multi-modality interfaces;online analytic mining,,0,,,,no,Nov. 29 2001-Dec. 2 2001,29 Nov 2001-02 Dec 2001,IEEE,IEEE Conference Publications
Proceedings IEEE International Symposium on Network Computing and Applications. NCA 2001,,,Proceedings IEEE International Symposium on Network Computing and Applications. NCA 2001,20020807,2001,,,iii,,The following topics are dealt with: scalability; system area networking; ad hoc networks; NetSolve; the quadrics network; deadline missing; reconfiguration protocols; free software distribution; firewalls; malicious agreements; application load balancing; multicast; replicated database recovery; an event-based communication framework; the analytic n-burst model; equivalent paths; IP router architectures; high-speed switch fabrics; distributed storage; mobile environment protocols; data logistics; Web cache sharing; IP storage; video transcoding; the Internet; computer clusters; fault-tolerant security; replica divergence; heterogeneous networks; Java cluster management; local scheduling; reconfigurable algorithms; IF environment; mobile Internet; TCP streams; service carrier-grade quality; database state machines; analytic performance; timeout prediction; storage area networks; a rapid I/O-based architecture; mobile agent monitoring; Ethernet; limited scope probing; jitter and delay; wireless MAN; static load balancing; and the PKI trust model,,POD:0-7695-1432-4,10.1109/NCA.2001.962508,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=962508,,,computer networks,Ethernet;IF environment;IP router architectures;IP storage;Java cluster management;NetSolve;PKI trust model;TCP streams;World Wide Web cache sharing;ad hoc networks;analytic multiburst model;analytic n-burst model;analytic performance;application load balancing;computer clusters;computer networks;data logistics;database state machines;deadline missing;delay;distributed storage;equivalent paths;event-based communication framework;fault-tolerant security;firewalls;free software distribution;heterogeneous networks;high-speed switch fabrics;jitter;limited scope probing;local scheduling;malicious agreements;mobile Internet;mobile agent monitoring;mobile environment protocols;multicast;quadrics network;rapid I/O-based architecture;reconfigurable algorithms;reconfiguration protocols;replica divergence;replicated database recovery;scalability;service carrier-grade quality;static load balancing;storage area networks;system area networking;timeout prediction;video transcoding;wireless MAN,,0,,,,no,8-10 Oct. 2001,08 Oct 2001-10 Oct 2001,IEEE,IEEE Conference Publications
Requirements engineering for complex collaborative systems,A. Sutcliffe,"Comput. Dept., Univ. of Manchester Inst. of Sci. & Technol., UK",Proceedings Fifth IEEE International Symposium on Requirements Engineering,20020807,2001,,,110,117,"A method for analysing requirements for complex sociotechnical systems is described. The method builds on the I* family of models by explicitly modelling communication between agents by discourse act types. System (i*) models and use cases are developed which describe the dependencies between human and computer agents in terms of a set of discourse acts that characterise the obligations on agents to respond and act. For human-computer communication, the discourse acts indicate functional requirements to support operators. For human agents, the acts specify their obligation to act and constraints on action. The method provides analytic techniques and heuristics to assess agent workloads in terms of the tasks and communication they have to perform. Scenarios are run against the system model by walking through the chain of operator tasks and communication links to produce time estimates and failure probabilities, where the demands of scenarios impose excessive loads on human operators. The method is illustrated with a case study of a naval command and control system",,POD:0-7695-1125-2,10.1109/ISRE.2001.948550,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=948550,,Collaboration;Collaborative work;Command and control systems;Failure analysis;Human computer interaction;Information analysis;Large-scale systems;Legged locomotion;Performance analysis;Sociotechnical systems,command and control systems;groupware;human factors;naval engineering computing;software agents;systems analysis;user interfaces,CSCW;I* family;agent communication modelling;agent workloads;analytic techniques;case study;communication links;complex collaborative systems;complex socio-technical systems;computer agents;discourse act types;failure probabilities;heuristics;human operators;human-computer communication;naval command and control system;operator tasks;requirements analysis;requirements engineering;system model;time estimates;use cases,,3,,16,,no,2001,27 Aug 2001-31 Aug 2001,IEEE,IEEE Conference Publications
Revisiting adaptively sampled distance fields,L. H. de Figueiredo; L. Velho; J. B. de Oliveira,,Proceedings XIV Brazilian Symposium on Computer Graphics and Image Processing,20020807,2001,,,377,,"Implicit surfaces are a powerful shape description for many applications in computer graphics. An implicit surface is defined by a function f:R<sup>3</sup>‰ ÕR as the set of points satisfying f(p)=0. Implicit representation becomes more effective when f is a signed distance function, i.e., when |f| gives the distance to the closest point on the surface and f is negative inside the object and positive outside the object bounded by the surface. The distance function to an arbitrary surface does not have a simple analytic description, and we must resort to approximations. One simple solution is to use a volumetric representation, constructed by sampling f uniformly, but such models are very large and their resolution is limited by the sampling rate. Frisken et al. (2000) proposed adaptively sampled distance fields (ADFs) as a way to overcome these problems. The authors revisit the ADFs and make two contributions to the original framework. First, we analyse the ADF representation and discuss some possible improvements. Second, we show how to compute ADFs more efficiently",,POD:0-7695-1330-1,10.1109/SIBGRAPI.2001.963083,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=963083,,Application software;Computer graphics;Interpolation;Piecewise linear approximation;Ray tracing;Sampling methods;Shape;Skeleton;Solids;Testing,adaptive systems;computer graphics;sampling methods;surface fitting,ADF representation;adaptively sampled distance fields;arbitrary surface;closest point;computer graphics;implicit representation;implicit surfaces;sampling;shape description;signed distance function;simple analytic description;volumetric representation,,1,7,,,no,1-Oct,15 Oct 2001-18 Oct 2001,IEEE,IEEE Conference Publications
Selecting the best e-commerce product for a retail chain-the analytic hierarchy process model,P. S. Lokachairi; G. Raju; D. D'lima,"Tata Consultancy Services, Minneapolis, MN, USA","Management of Engineering and Technology, 2001. PICMET '01. Portland International Conference on",20020807,2001,1,,145 vol.1,,Retail firms have been in the forefront of the business-to-consumer (B2C) initiatives as they run the whole gamut of transformation from branding and information-centered activities to online sales and innovation-driven marketing. The paradigm shift in business objective from mere selling of products and services to 'owning a share of life of guests' can also be observed from the new networks and alliances between retail firms and other businesses with complementing business goals. A US-based retail chain has been exploring avenues for enhancing its B2C e-commerce capabilities by embarking on a series of initiatives that inter alia includes a comprehensive study on related software products currently available in the market. This paper focuses on evaluation of the e-commerce products in terms of their technical features and services offered as well as exploring their suitability to meet the specific business priorities of the firm. The analytic hierarchy process (AHP) model has been employed for selecting the best product,,POD:1-890843-06-7,10.1109/PICMET.2001.951870,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=951870,,Analytical models;Availability;Business;Content management;Decision making;Isolation technology;Marketing and sales;Performance analysis;Research and development management;Technology management,electronic commerce;management;retail data processing,USA;analytic hierarchy process model;best product selection;branding;business goals;business objective;business priorities;business-to-consumer initiatives;e-commerce product;information-centered activities;innovation-driven marketing;online sales;retail chain,,0,,,,no,2001,29 Jul 2001-02 Aug 2001,IEEE,IEEE Conference Publications
Singular PDES and the assignment of zero dynamics in nonlinear systems,C. Kravaris; M. Niemiec; N. Kazantzis,"Dept. of Chem. Eng., Univ. of Patras, Patras, Greece",2001 European Control Conference (ECC),20150427,2001,,,3729,3734,"The present research work aims at the development of a systematic method to arbitrarily assign the zero dynamics of a nonlinear system by constructing the requisite synthetic output maps. The minimum-phase synthetic output maps constructed can be made statically equivalent to the original output maps, and therefore, they could be directly used for nonminimum-phase compensation purposes. Specifically, the mathematical formulation of the problem is realized via a system of first-order nonlinear singular PDEs and a rather general set of necessary and sufficient conditions for solvability is derived. The solution to the above system of singular PDEs can be proven to be locally analytic and this enables the development of a series solution method that is easily programmable with the aid of a symbolic software package. The minimum-phase synthetic output maps that induce the prescribed zero dynamics for the original nonlinear system can be computed on the basis of the solution of the aforementioned system of singular PDEs. Moreover, statical equivalence to the original output map can be readily established by a simple algebraic construction.",,POD:978-3-9524173-6-2,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7076514,Nonlinear systems;Nonminimum-phase compensation;Singular PDEs;Zero-dynamics,Equations;Europe;Manifolds;Mathematical model;Nonlinear dynamical systems;Vectors,computability;nonlinear control systems;nonlinear differential equations;partial differential equations;zero assignment,first-order nonlinear singular PDEs;minimum-phase synthetic output maps;necessary conditions;nonlinear systems;solvability;statical equivalence;sufficient conditions;symbolic software package;zero dynamics,,0,,,,no,4-7 Sept. 2001,,IEEE,IEEE Conference Publications
A flexible accelerator for Layer 7 networking applications,G. Memik; W. H. Mangione-Smith,"Dept. of Electr. Eng., California Univ., Los Angeles, CA, USA",Proceedings 2002 Design Automation Conference (IEEE Cat. No.02CH37324),20020807,2002,,,646,651,"In this paper, we present a flexible accelerator designed for networking applications. The accelerator can be utilized efficiently by a variety of Network Processor designs. Most Network Processors employ hardware accelerators for implementing key tasks. New applications require new tasks, such as pattern matching, to be performed on the packets in real-time. Using our proposed accelerator, we have implemented several such tasks and measured their performance. Specifically, the accelerator achieves 25-fold improvement on the performance of pattern matching, and 10-fold improvement for tree lookup, over optimized software solutions. Since the accelerator is used for different tasks, the hardware requirements are small compared to an accelerator group that implements the same set of tasks. We also present accurate analytic models to estimate the execution time of these networking tasks.",0738-100X;0738100X,POD:1-58113-461-4,10.1109/DAC.2002.1012704,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1012704,,Algorithm design and analysis;Application software;Application specific integrated circuits;Arithmetic;Hardware;Logic;Pattern matching;Permission;Process design;Software performance,application specific integrated circuits;computer networks;microprocessor chips;pattern matching;table lookup,Layer 7 networking applications;analytic models;application-specific processor;execution time;flexible accelerator;hardware accelerators;network processor designs;pattern matching;tree lookup,,1,1,15,,no,2002,,IEEE,IEEE Conference Publications
An analytic approach to measuring the overall effectiveness of R&D-a case study in the telecom sector,V. Ojanen; M. Tuominen,"Dept. of Ind. Eng. & Manage., Lappeenranta Univ. of Technol., Finland",IEEE International Engineering Management Conference,20021210,2002,2,,667,672 vol.2,R&D managers need to measure the overall performance of R&D activities in order to improve the activities and prove their effectiveness. This paper aims at systematizing the measurement of the overall effectiveness of R&D. An example of the utilization of our analytical approach is presented in the form of a case study in the telecom sector.,,POD:0-7803-7385-5,10.1109/IEMC.2002.1038516,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1038516,,Area measurement;Computer aided software engineering;Diversity reception;Engineering management;Industrial engineering;Performance analysis;Research and development;Research and development management;Technology management;Telecommunications,research and development management;telecommunication,R&D activities effectiveness;R&D activities performance measurement;R&D managers;telecom sector,,1,,13,,no,2002,,IEEE,IEEE Conference Publications
An analytic software testability model,Jin-Cherng Lin; Szu-Wen Lin,"Dept. of Comput. Sci. & Eng., Tatung Univ., Taipei, Taiwan","Test Symposium, 2002. (ATS '02). Proceedings of the 11th Asian",20030228,2002,,,278,283,"Software testability, which has been discussed in the past decade, has been defined as provisions that can be taken into consideration at the early step of software development. This paper gives software testability, previously defined by Voas, a new model and measurement without performing testing with respect to a particular input distribution.",1081-7735;10817735,POD:0-7695-1825-7,10.1109/ATS.2002.1181724,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1181724,,Computer science;Costs;Fault detection;Humans;Performance evaluation;Programming;Software performance;Software quality;Software systems;Software testing,program testing;software reliability,analytic model;software development;software testability;ultra-reliable software,,1,,12,,no,18-20 Nov. 2002,,IEEE,IEEE Conference Publications
An integrated service architecture for managing capital market systems,F. A. Rabhi; B. Benatallah,"Univ. of New South Wales, NSW, Australia",IEEE Network,20020807,2002,16,1,15,19,"This article studies current developments and trends in the area of capital market systems. In particular, it defines the trading lifecycle and the activities associated with it. The article then investigates opportunities for the integration of legacy systems and existing communication protocols through distributed integrated services that correspond to established business processes. These integrated services link to basic services such as an exchange, a settlement, or a registry service. Examples of such integrated services include pre-trade services (e.g., analytics) or post-trade services (e.g., surveillance). The article then presents the various levels of integration in capital market systems and discusses the standards in place. It establishes that most interactions occur at a low level of abstraction such as the network (e.g., TCP/IP), data format (e.g., FIX, XML), and middleware levels (e.g., CORBA). Finally, the article discusses a software development methodology based on the use of design patterns. These design patterns address the essential aspects of managing integrated services in a technology-independent fashion. These aspects are service wrapping, service composition, service contracting, service discovery, and service execution. The objective of the methodology is to facilitate the rapid development of new integrated services that correspond to emerging business opportunities",0890-8044;08908044,,10.1109/65.980540,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=980540,,Business communication;Intserv networks;Middleware;Programming;Protocols;Surveillance;TCPIP;Technology management;Wrapping;XML,Internet;business communication;commerce;computer network management;distributed object management;distributed processing;hypermedia markup languages;software architecture;stock markets;telecommunication standards;transport protocols,CORBA;FIX;Internet;TCP/IP;Web;XML;business processes;capital market systems management;communication protocols;data format;design patterns;distributed integrated services;exchange;integrated service architecture;legacy systems;middleware;post-trade services;pre-trade services;registry service;service composition;service contracting;service discovery;service execution;service wrapping;settlement;software architecture;software development;standards;surveillance;trading lifecycle,,9,,24,,no,Jan/Feb 2002,,IEEE,IEEE Journals & Magazines
Analytic hierarchy process model for global competitiveness of local companies a case for Thai banking business,B. Sirinaovakul,"Dhurakijpundit Univ., Bangkok, Thailand",IEEE International Engineering Management Conference,20021210,2002,2,,842,847 vol.2,"Advances in information technology radically impact all organizations, especially, in providing effective tools to integrate their operations more effectively, respond to market needs more flexibly, and serve their customers globally. This research aims to answer how the local companies survive in the global competitiveness. Using analytic hierarchy process (AHP) method, variables relevant to strategic management and information technology has been developed and with the application-based model, based on the Mc Kinsey framework. Results show that the long term vision of the leader gives the highest contribution to the success of local companies. Appropriate management of leadership and information technology significantly enhances the competitiveness of local companies. Result of evaluation of the proposed model with four local Thai Banks is in accordance with those of international credit rating agencies including Standard & Poors and Moody's Invertors Service.",,POD:0-7803-7385-5,10.1109/IEMC.2002.1038548,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1038548,,Banking;Companies;Computer aided software engineering;Information analysis;Information management;Information technology;Inverters;Strategic planning;Technology management;Technology planning,banking;information technology;management,Mc Kinsey framework;Thai Banks;analytic hierarchy process;application-based model;global competitiveness;information technology;information technology variables;international credit rating agencies;local companies;strategic management variables,,0,,27,,no,2002,,IEEE,IEEE Conference Publications
Application of simulation and mean value analysis to a repair facility model for finding optimal staffing levels,G. Boyer; A. N. Arnason,"Dept. of Comput. Sci., Manitoba Univ., Winnipeg, Man., Canada",Proceedings of the Winter Simulation Conference,20030122,2002,2,,1871,1879 vol.2,"Staffing problems arise in a wide range of applications including job shops, call centres, and hospital emergency departments. They are characterised by the need to allocate shift workers with varying skills to handle an arrival stream of tasks having different sub-task routings and (sub-task) skill requirements. The Manitoba Telecom Service Trouble Diagnosis and Repair System (TDRS) has 3 skill-levels of staff handling multiple types of faults occurring in telephone switching equipment. TDRS is a pure staffing problem having no equipment constraints: the only resource constraint is staff itself. The object of this study is to show how this can be modelled as an open network of queues with feedback and allowing for temporal and fault-class heterogeneity. Analytic mean value analysis then facilitates validation and selecting feasible staffing strategies for closer examination by simulation. The purpose of experiments using simulation is to find effective performance visualisations and ""optimal"" staffing allocations.",,POD:0-7803-7614-5,10.1109/WSC.2002.1166482,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1166482,,Analytical models;Application software;Computational modeling;Costs;Fault diagnosis;Maintenance;Network-on-a-chip;Personnel;Telecommunication network reliability;Telephony,digital simulation;human resource management;maintenance engineering;personnel;telecommunication computing;telecommunication services,Manitoba Telecom Service Trouble Diagnosis and Repair System;experiments;mean value analysis;open network;optimal staffing levels;repair facility model;shift worker allocation;simulation;telephone switching equipment,,3,1,8,,no,8-11 Dec. 2002,,IEEE,IEEE Conference Publications
Automated software robustness testing - static and adaptive test case design methods,M. Dix; H. D. Hofmann,"ABB Corporate Res. Center, Germany",Proceedings. 28th Euromicro Conference,20021210,2002,,,62,66,"Testing is essential in the development of any software system. Testing is required to assess a system's functionality and quality of operation in its final environment. This is especially of importance for systems being assembled from many self-contained software components. In this article, we focus on automated testing of software component robustness, which is a component's ability to handle invalid input data or environmental conditions. We describe how large numbers of test cases can effectively and automatically be generated from small sets of test values. However, there is a great demand on ways to efficiently reduce this mass of test cases as actually executing them on a data processing machine would be too time consuming and expensive. We discuss static analytic methods for test case reduction and some of the disadvantages they bring. Finally a more intelligent and efficient approach is introduced, the Adaptive Test Procedure for Software Robustness Testing developed at ABB Corporate Research in Ladenburg. Along with these discussions the need for intelligent test approaches is illustrated by the Ballista methodology for automated robustness testing of software component interfaces. An object-oriented approach based on parameter data types rather than component functionality essentially eliminates the need for function-specific test scaffolding.",1089-6503;10896503,POD:0-7695-1787-0,10.1109/EURMIC.2002.1046134,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1046134,,Assembly systems;Automatic testing;Computer aided software engineering;Data processing;Design methodology;Robustness;Software systems;Software testing;System testing;Vehicle crash testing,automatic testing;data structures;object-oriented programming;program diagnostics;program testing,ABB Corporate Research;Adaptive Test Procedure for Software Robustness Testing;Ballista methodology;adaptive test case design;automated software robustness testing;object-oriented approach;parameter data types;self-contained software components;software component interfaces;static analytic methods;static test case design;test case reduction,,4,,8,,no,2002,,IEEE,IEEE Conference Publications
Bit-level allocation of multiple-precision specifications,M. C. Molina; J. M. Mendias; R. Hermida,"Dept. Arquitectura de Computadores y Automatica, Univ. Complutense de Madrid, Spain","Proceedings Euromicro Symposium on Digital System Design. Architectures, Methods and Tools",20030106,2002,,,385,392,"This paper proposes an allocation algorithm able to perform the combined resource selection and operation binding of multiple-precision specifications that maximizes the bit-level reuse of hardware resources. Additionally, it presents an analytic method to estimate the amount of area that our approach could save in comparison with traditional allocation algorithms. In order to minimize the cost of the implementations obtained, the proposed algorithm produces circuits only influenced by the maximum number of bits calculated per cycle. This approach contrasts with the cost of implementations designed by traditional algorithms, which also depends on the number and widths of the operations executed in every cycle.",,POD:0-7695-1790-0,10.1109/DSD.2002.1115396,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1115396,,Algorithm design and analysis;Circuits;Computational efficiency;Costs;Delay;Design methodology;Hardware;High level synthesis;High performance computing;Resource management,formal specification;processor scheduling;resource allocation;signal processing,DSP software;bit-level allocation;functional resources;high level synthesis;multiple-precision specifications;operation binding;resource selection;scheduling;storage resources,,0,,12,,no,2002,,IEEE,IEEE Conference Publications
Clinical engineering technology assessment decision support: a case study using the analytic hierarchy process (AHP),E. B. Sloane; M. J. Liberatore; R. L. Nydick; W. Luo; Q. B. Chung,"Dept. of Decision & Inf. Technol., Villanova Univ., PA, USA",Proceedings of the Second Joint 24th Annual Conference and the Annual Fall Meeting of the Biomedical Engineering Society] [Engineering in Medicine and Biology,20030106,2002,3,,1950,1951 vol.3,"This case study showed how the AHP decision support technique can be applied to clinical engineering health technology assessment projects. AHP provides a structured method of organizing and documenting the decision process and takes into consideration the many tradeoffs that exist between alternate choices. When an AHP model is properly designed and implemented, it facilitates interdepartmental and interdisciplinary communication and results in a decision support tool that represents a consensus model. The AHP model can then be used to compare health technology alternatives and delivers a composite score for each alternative that identifies the best choice. AHP produces a clinical engineering decision support tool for the hospital that identifies the best technology alternative for their specific need. Further, the model can be updated or adapted to different medical technologies whenever needed, so the development investment is not wasted.",1094-687X;1094687X,POD:0-7803-7612-9,10.1109/IEMBS.2002.1053110,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1053110,,Business;Clinical diagnosis;Computer aided software engineering;Costs;Educational institutions;Finance;Hospitals;Information technology;Medical services;Pediatrics,biomedical engineering;decision support systems,analytic hierarchy process;clinical engineering technology assessment decision support;decision process documentation;decision support tool;development investment;hospital;interdisciplinary communication;medical technologies;structured method,,3,,7,,no,23-26 Oct. 2002,,IEEE,IEEE Conference Publications
Combination of high resolution analytically computed uncollided flux images with low resolution Monte Carlo computed scattered flux images,J. Tabary; R. Guillemaud; F. Mathy; A. Gliere; P. Hugonnard,"LETI-CEA Recherche Technologique, CEA GRENOBLE, France",2002 IEEE Nuclear Science Symposium Conference Record,20031027,2002,2,,825,829 vol.2,"A new computing model has been implemented in Sindbad, a simulation software dedicated to radiographic systems, to provide quite rapidly realistic radiographs of complex objects, including the scatter component of the photon flux. The new computing model consists of a coupling of two previous models : an analytic approach and a Monte Carlo one. The analytic simulation is used to compute a high resolution uncollided photon flux image whereas the Monte Carlo provides the scattered flux image. This latter image is computed as an extrapolation of a reduced dose Monte Carlo scattered flux image. Extrapolation is possible thanks to the low resolution feature of the scattered flux image. After a detailed description of the method developed to perform the combination, we present a validation on a medical radiograph simulation. In spite of the quite complex geometry of the examined part and the important emitted dose, this new model provides a realistic radiograph with a good estimation of the scattered flux in a reasonable computation time.",,POD:0-7803-7636-6,10.1109/NSSMIC.2002.1239452,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1239452,,Analytical models;Computational modeling;Electromagnetic scattering;Extrapolation;Image analysis;Image resolution;Monte Carlo methods;Optical computing;Particle scattering;Radiography,Monte Carlo methods;diagnostic radiography;medical diagnostic computing,Sindbad;high resolution analytically computed uncollided flux images;low resolution Monte Carlo computed scattered flux images;radiographic systems;simulation software,,2,,11,,no,10-16 Nov. 2002,,IEEE,IEEE Conference Publications
Comparison of analytic and numerical models with commercially available simulation tools for the prediction of semiconductor freeze-out and exhaustion,R. Pieper; S. Michael; D. Reeves,"Dept. of Electr. & Comput. Eng., Naval Postgraduate Sch., Monterey, CA, USA","The 2002 45th Midwest Symposium on Circuits and Systems, 2002. MWSCAS-2002.",20030320,2002,1,,I,40-3 vol.1,"Currently, commercial software packages, such as available through Silvaco International, are well designed to solve the electron/hole transport problem. This type of calculation is usually required to predict the device IV characteristic. Surprisingly, using the same package, to obtain a temperature dependent plot for majority carrier concentration for a uniform semiconductor requires a somewhat complicated procedure. Our paper will present an efficient novel way of obtaining this curve from the Silvaco International software and compare the results with a proposed one dimensional single-equation analytic model and a numerical model that predict the temperature dependence for majority concentration in all regimes.",,POD:0-7803-7523-8,10.1109/MWSCAS.2002.1187148,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1187148,,Analytical models;Charge carrier processes;Computational modeling;Computer simulation;Impurities;Numerical models;Predictive models;Semiconductor device packaging;Software packages;Temperature dependence,carrier density;semiconductor device models,Silvaco International software package;computer simulation;device I-V characteristics;electron transport;hole transport;majority carrier concentration;numerical model;one-dimensional analytical model;semiconductor exhaustion;semiconductor freeze-out;temperature dependence,,1,,9,,no,4-7 Aug. 2002,,IEEE,IEEE Conference Publications
ControlWare: a middleware architecture for feedback control of software performance,Ronghua Zhang; Chenyang Lu; T. F. Abdelzaher; J. A. Stankovic,"Dept. of Comput. Sci., Virginia Univ., Charlottesville, VA, USA",Proceedings 22nd International Conference on Distributed Computing Systems,20021107,2002,,,301,310,"Attainment of software performance assurances in open, largely unpredictable environments has recently become an important focus for real-time research. Unlike closed embedded systems, many contemporary distributed real-time applications operate in environments where offered load and available resources suffer considerable random fluctuations, thereby complicating the performance assurance problem. Feedback control theory has recently been identified as a promising analytic foundation for controlling performance of such unpredictable, poorly modeled software systems, the same way other engineering disciplines have used this theory for physical process control. In this paper we describe the design and implementation of ControlWare, a middleware QoS-control architecture based on control theory, motivated by the needs of performance-assured Internet services. It offers a new type of guarantees we call convergence guarantees that lie between hard and probabilistic guarantees. The efficacy of the architecture in achieving its QoS goals under realistic load conditions is demonstrated in the context of web server and proxy QoS management.",1063-6927;10636927,POD:0-7695-1585-1,10.1109/ICDCS.2002.1022267,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1022267,,Application software;Computer architecture;Control system analysis;Embedded system;Feedback control;Fluctuations;Middleware;Performance analysis;Real time systems;Software performance,client-server systems;distributed object management;quality of service,ControlWare;Internet services;QoS-control;convergence guarantees;distributed real-time applications;middleware architecture;software performance assurances,,38,,30,,no,2002,,IEEE,IEEE Conference Publications
Design of adaptive and reliable mobile agent communication protocols,C. Jiannong; Xinyu Feng; Jian Lu; S. K. Das,"Dept. of Comput., Kong Polytech. Univ., Kowloon, China",Proceedings 22nd International Conference on Distributed Computing Systems,20021107,2002,,,471,472,"This paper presents a mailbox-based scheme for designing flexible and adaptive message delivery protocols in mobile agent (MA) systems. The scheme associates each mobile agent with a mailbox while allowing the decoupling between them, i.e., a mobile agent can migrate to a new site without bringing its mailbox. By separating the concerns of locating the mailbox of a mobile agent and delivering a message to the agent, we obtain a large space of protocol design with flexibility. Using a three-dimensional model based on the scheme, we have developed a taxonomy of MA communication protocols, which not only covers, as special cases, several known MA message delivery protocols, but also allows for the design of new ones well suited for various application requirements. We describe such an efficient and adaptive protocol derived front the model. The protocol guarantees reliable delivery of messages to mobile agents. We analyze the design trade-offs and performance of the protocol, using an analytic model as well as extensive simulation experiments.",1063-6927;10636927,POD:0-7695-1585-1,10.1109/ICDCS.2002.1022295,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1022295,,Algorithm design and analysis;Computer science;Design engineering;Laboratories;Mobile agents;Mobile communication;Performance analysis;Target tracking;Taxonomy;Wireless application protocol,protocols;security of data;software agents,adaptive message delivery protocols;analytic model;mailbox-based scheme;mobile agent systems;reliable mobile agent communication protocols;three-dimensional model,,2,,,,no,2002,,IEEE,IEEE Conference Publications
Image parameter modeling analog traveling-wave phase shifters,G. Bartolucci,"Dept. of Electron. Eng., Tor Vergata Univ., Rome, Italy",IEEE Transactions on Circuits and Systems I: Fundamental Theory and Applications,20021210,2002,49,10,1505,1509,The aim of this work is to present a modeling procedure for the traveling-wave analog phase shifter. The proposed approach is based on the image representation of two-port networks. Design considerations to optimize the electric performance of the component reducing also the size of the structure are discussed. A commercial software package is used for the final electric simulation of the actual phase shifters to check the validity of the analytic model presented.,1057-7122;10577122,,10.1109/TCSI.2002.803364,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1039504,,Capacitance;Design optimization;Frequency;Image representation;Impedance;Microwave devices;Phase shifters;Schottky diodes;Software packages;Transmission lines,equivalent circuits;image representation;microwave phase shifters;millimetre wave phase shifters;modelling;two-port networks,analog traveling-wave phase shifters;commercial software package;electric simulation;image parameter modeling;image parameter representation;insertion losses;microstrip sections;microwave phase shifter;millimeter wave frequencies;two-port networks,,7,,7,,no,2-Oct,,IEEE,IEEE Journals & Magazines
Integrating GSS and AHP: experiences from benchmarking of buyer-supplier relationships,S. Peltola; M. Torkkeli; J. Tuimala,"Lappeenranta Univ. of Technol., Finland",Proceedings of the 35th Annual Hawaii International Conference on System Sciences,20020807,2002,,,514,523,"The increasing overall global competition forces requirements for companies to improve their business performance continuously in every sector. The utilization of external resources, including a supplier network, has become one of the most critical development areas of business, needing a lot of attention. The performance of buyer-supplier relationships will be enhanced by using a benchmarking method in aiding the identification and implementation of development actions required for reaching the world-class level. The benchmarking process will be led by two decision support systems: AHP and GSS, to ensure valuable outcomes of the benchmarking with less resources.",,POD:0-7695-1435-9,10.1109/HICSS.2002.993906,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=993906,,Business;Collaboration;Collaborative software;Companies;Costs;Customer service;Decision making;Decision support systems;Logistics,business data processing;decision support systems;groupware;performance evaluation,AHP;Analytic Hierarchy Process methods;GSS;benchmarking method;benchmarking process;business performance;buyer-supplier relationship benchmarking;critical development areas;decision support systems;development actions;external resources;global competition;group support system;supplier network;valuable outcomes;world-class level,,3,,34,,no,7-10 Jan. 2002,,IEEE,IEEE Conference Publications
New method for dynamic modeling of hybrid stepping motors,C. Kuert; M. Jufer; Y. Perriard,"Tech. Eng. Fac., Swiss Fed. Inst. of Technol., Lausanne, Switzerland",Conference Record of the 2002 IEEE Industry Applications Conference. 37th IAS Annual Meeting (Cat. No.02CH37344),20021210,2002,1,,6,12 vol.1,"To be able to calculate the performances of a hybrid stepping motor or to design it in a short time, it is essential to apply an efficient model, accepting a slight loss of precision. Using software based on finite elements and executing calculations are difficult and time consuming, but normally offers increased precision. Finite elements may be used to better understand some phenomena and help establishing an analytic modeling based on magnetic equivalent scheme. Torque calculation is therefore much faster and permits to easily optimize a given structure. The dynamic modeling permits an estimation of dynamic behavior of the hybrid stepping motor in stationary and transient states.",0197-2618;01972618,POD:0-7803-7420-7,10.1109/IAS.2002.1044060,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1044060,,Actuators;Design engineering;Differential equations;Finite element methods;Laboratories;Nonlinear dynamical systems;Nonlinear equations;State estimation;Torque;Voltage,electric machine analysis computing;equivalent circuits;finite element analysis;machine theory;magnetic circuits;stepping motors,analytic modeling;dynamic behavior estimation;dynamic modeling method;finite element software;hybrid stepping motors;magnetic equivalent scheme;stationary states;torque calculation;transient states,,11,,7,,no,13-18 Oct. 2002,13 Oct 2002-18 Oct 2002,IEEE,IEEE Conference Publications
On the Migration of the Scientific Code Dyana from SMPs to Clusters of PCs and on to the Grid,M. Taufer; T. Stricker; G. Roos; P. Guentert,,"Cluster Computing and the Grid, 2002. 2nd IEEE/ACM International Symposium on",20051121,2002,,,93,93,"Dyana is a molecular biology code used in the study of infectious prion proteins. Like many other scientific codes, Dyana was migrated successfully from vector supercomputers to some more cost-effective cluster of commodity PCs. A further migration to a widely distributed grid computing platform looks very tempting because many of these platforms promise the use of nearly free compute-cycles on the Internet. Not all codes are equally suited for all platforms. Even embarrassingly parallel codes might require a significant re-engineering effort for a migration from one platform to another. A better understanding of the performance characteristics of a code is required before a migration is attempted. To address this problem, we present a systematic method to study the viability of a code migration from one platform to another, before it is actually undertaken. We construct an analytic performance model of the application. We use the previous migration from SMPs to commodity clusters of PCs to validate and calibrate the model. Finally, we extrapolate the performance of Dyana widely distributed computing on the grid and we suggest optimizations in the process of migration. Our general model predicts that Dyana can efficiently use up to 42000 processors with its current workload and is therefore well suited for grid computing on the Internet.",,POD:0-7695-1582-7,10.1109/CCGRID.2002.1017116,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1540445,computation-intensive applications;engineering;grid architectures.;migration of scientific codes;performance evaluation and modeling;software;widely distributed supercomputing,Biology;Computer applications;Distributed computing;Grid computing;High performance computing;Internet;Personal communication networks;Power engineering computing;Supercomputers;Switched-mode power supply,,computation-intensive applications;engineering;grid architectures.;migration of scientific codes;performance evaluation and modeling;software;widely distributed supercomputing,,1,,14,,no,21-24 May 2002,,IEEE,IEEE Conference Publications
Optical character recognition for cursive handwriting,N. Arica; F. T. Yarman-Vural,"Dept. of Comput. Eng., Middle East Tech. Univ., Ankara, Turkey",IEEE Transactions on Pattern Analysis and Machine Intelligence,20020807,2002,24,6,801,813,"A new analytic scheme, which uses a sequence of image segmentation and recognition algorithms, is proposed for the off-line cursive handwriting recognition problem. First, some global parameters, such as slant angle, baselines, stroke width and height, are estimated. Second, a segmentation method finds character segmentation paths by combining gray-scale and binary information. Third, a hidden Markov model (HMM) is employed for shape recognition to label and rank the character candidates. For this purpose, a string of codes is extracted from each segment to represent the character candidates. The estimation of feature space parameters is embedded in the HMM training stage together with the estimation of the HMM model parameters. Finally, information from a lexicon and from the HMM ranks is combined in a graph optimization problem for word-level recognition. This method corrects most of the errors produced by the segmentation and HMM ranking stages by maximizing an information measure in an efficient graph search algorithm. The experiments indicate higher recognition rates compared to the available methods reported in the literature",0162-8828;01628828,,10.1109/TPAMI.2002.1008386,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1008386,,Algorithm design and analysis;Character recognition;Gray-scale;Handwriting recognition;Hidden Markov models;Image analysis;Image recognition;Image segmentation;Image sequence analysis;Optical character recognition software,error correction;feature extraction;graph theory;handwritten character recognition;hidden Markov models;image segmentation;optical character recognition;optimisation;parameter estimation;search problems,HMM model parameters;HMM ranks;HMM training stage;analytic scheme;baselines;binary information;character candidate labelling;character candidate ranking;character segmentation paths;code string extraction;error correction;feature space parameters;global parameter estimation;graph optimization problem;graph search algorithm;gray-scale information;handwritten word recognition;hidden Markov model;image recognition algorithms;image segmentation algorithms;information measure maximization;lexicon matching;offline cursive handwriting recognition;optical character recognition;pre-processing;recognition rates;shape recognition;slant angle;stroke height;stroke width;word-level recognition,,51,3,23,,no,2-Jun,,IEEE,IEEE Journals & Magazines
Performance and implementation of distributed data CPHF and SCF algorithms,Y. Alexeev; M. W. Schmidt; T. L. Windus; M. S. Gordon; R. A. Kendall,"Ames Lab., Iowa State Univ., Ames, IA, USA",Proceedings. IEEE International Conference on Cluster Computing,20030106,2002,,,135,142,"This paper describes a novel distributed data parallel self consistent field (SCF) algorithm and the distributed data coupled perturbed Hartree-Fock (CPHF) step of an analytic Hessian algorithm. The distinguishing features of these algorithms are: (a) columns of density and Fock matrices are distributed among processors, (b) pairwise dynamic load balancing and an efficient static load balancer were developed to achieve a good workload, and (c) network communication time is minimized via careful analysis of data flow in the SCF and CPHF algorithms. By using a shared memory model, novel work load balancers, and improved analytic Hessian steps, we have developed codes that achieve superb performance. The performance of the CPHF code is demonstrated on a large biological system.",,POD:0-7695-1745-5,10.1109/CLUSTR.2002.1137738,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1137738,,Algorithm design and analysis;Chemicals;Chemistry;Clustering algorithms;Concurrent computing;Data models;Distributed computing;Games;Postal services;Wave functions,HF calculations;Hessian matrices;SCF calculations;biology computing;chemistry computing;distributed algorithms;quantum chemistry;resource allocation;shared memory systems;software performance evaluation;workstation clusters,Fock matrices;analytic Hessian algorithm;analytic Hessian steps;distributed data SCF algorithm;distributed data coupled perturbed Hartree-Fock step;efficient static load balancer;large biological system;network communication time;pairwise dynamic load balancing;performance;shared memory model,,0,,19,,no,2002,,IEEE,IEEE Conference Publications
Predicting TCP throughput from non-invasive network sampling,M. Goyal; R. Guerin; R. Rajan,"Dept. of Comput. & Inf. Sci., Ohio State Univ., Columbus, OH, USA",Proceedings.Twenty-First Annual Joint Conference of the IEEE Computer and Communications Societies,20021107,2002,1,,180,189 vol.1,"In this paper, we wish to derive analytic models that predict the performance of TCP flows between specified endpoints using routinely observed network characteristics such as loss and delay. The ultimate goal of our approach is to convert network observables into representative user and application relevant performance metrics. The main contributions of this paper are in studying which network performance data sources are most reflective of session characteristics, and then in thoroughly investigating a new TCP model based on Padhye et al. (2000) that uses non-invasive network samples to predict the throughput of representative TCP flows between given end-points.",0743-166X;0743166X,POD:0-7803-7476-2,10.1109/INFCOM.2002.1019259,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1019259,,Application software;Computer networks;Delay estimation;Monitoring;Performance analysis;Performance loss;Predictive models;Sampling methods;Scalability;Throughput,Internet;performance evaluation;quality of service;sampling methods;telecommunication traffic;transport protocols,Internet;TCP flows;TCP throughput;analytic models;delay;loss;network performance data sources;noninvasive network sampling;performance metrics;performance prediction,,21,,20,,no,2002,,IEEE,IEEE Conference Publications
Real-time autonomous video enhancement system (RAVE),V. Ablavsky; M. Snorrason; C. J. Taylor,"Charles River Analytics, Cambridge, MA, USA",Proceedings. International Conference on Image Processing,20021210,2002,2,,II-317,II-320 vol.2,"The ability to autonomously enhance low-quality or corrupted streaming video data is essential in a number of important civilian and defense scenarios. Applications include visual surveillance, motion picture restoration, and remote control of unmanned aerial vehicles. We have developed a prototype of RAVE: real-time autonomous video enhancement system. It consists of a suite of video artifact detection algorithms and corresponding correction algorithms. The system is autonomously controlled by an intelligent software agent. Our prototype has been successfully validated on several video sequences from different application domains and is being matured into a fully-functional, real-time embedded system.",1522-4880;15224880,POD:0-7803-7622-6,10.1109/ICIP.2002.1039951,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1039951,,Application software;Detection algorithms;Motion control;Motion pictures;Prototypes;Real time systems;Software prototyping;Streaming media;Surveillance;Unmanned aerial vehicles,embedded systems;image enhancement;image sequences;military computing;software agents;video signal processing,RAVE;autonomous control;correction algorithms;corrupted streaming video;intelligent software agent;motion picture restoration;real-time autonomous video enhancement system;real-time embedded system;remote control;unmanned aerial vehicles;video artifact detection algorithms;video sequences;visual surveillance,,0,,7,,no,2002,,IEEE,IEEE Conference Publications
Reliability prediction models to support conceptual design,S. W. Ormon; C. R. Cassady; A. G. Greenwood,"Dept. of Ind. Eng., Mississippi State Univ., MS, USA",IEEE Transactions on Reliability,20020807,2002,51,2,151,157,"During the early stages of conceptual design, the ability to predict reliability is very limited. Without a prototype to test in a lab environment or without field data, component failure rates and system reliability performance are usually unknown. A popular method for early reliability prediction is to develop a computer model for the system. However, most of these models are extremely specific to an individual system or industry. This paper presents three general procedures (using both simulation and analytic solution techniques) for predicting system reliability and average mission cost. The procedures consider both known and unknown failure rates and component-level and subsystem-level analyzes. The estimates are based on the number of series subsystems and redundant (active or stand-by) components for each subsystem. The result is a set of approaches that engineers can use to predict system reliability early in the system-design process. Software was developed (and is discussed in this paper) that facilitates the application of the simulation-based techniques. For the specific type of system and mission addressed in this paper, the analytic approach is superior to the simulation-based prediction models. However, all three approaches are presented for two reasons: (1) to convey the development process involved with building these prediction tools; and (2) the simulation-based approaches are of greater value as the research is extended to consider more complex systems and scenarios",0018-9529;00189529,,10.1109/TR.2002.1011519,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1011519,,Analytical models;Computational modeling;Cost function;Industrial engineering;Predictive models;Prototypes;Redundancy;Reliability engineering;System performance;System testing,costing;design engineering;engineering computing;failure analysis;reliability,average mission cost;component failure rates;conceptual design support;reliability prediction models;simulation software;system reliability performance,,5,,10,,no,2-Jun,,IEEE,IEEE Journals & Magazines
RR interval time series modeling: the PhysioNet/Computers in Cardiology Challenge 2002,G. B. Moody,"Harvard-MIT Div. of Health Sci. & Technol., Cambridge, MA, USA",Computers in Cardiology,20030122,2002,,,125,128,"Cardiac inter-beat (RR) interval time series contain fluctuations at time scales ranging from a few seconds to many hours. Realistic models of these series are potentially useful to researchers, not only as sources of surrogate data with known properties for evaluating novel analytic methods, but also as sources of insight into the diverse mechanisms underlying heart rate variability. PhysioNet and Computers in Cardiology have sponsored an open on-line competition aimed at stimulating the creation and exchange of high-quality models of RR interval variability, the third in an annual series of challenges for the research community. Participants first created software that was used to generate 24-hour synthetic time series, then attempted to identify the synthetic series within an unlabeled data set that included roughly equal amounts of real and synthetic data. All of the software models and the data used in the challenge are available at http://www.physionet.org/challenge/2002/.",0276-6547;02766547,POD:0-7803-7735-4,10.1109/CIC.2002.1166723,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1166723,,Biomedical computing;Biomedical engineering;Cardiology;Heart rate;Heart rate variability;Pathology;Physics;Physiology;Time series analysis,electrocardiography;medical signal processing;time series,Computers in Cardiology;PhysioNet;RR interval time series modeling;RR interval variability;cardiac inter-beat interval time series;heart rate variability;synthetic time series,,3,,2,,no,22-25 Sept. 2002,,IEEE,IEEE Conference Publications
Situation assessment via Bayesian belief networks,S. Das; R. Grey; P. Gonsalves,"Charles River Analytics Inc., Cambridge, MA, USA",Proceedings of the Fifth International Conference on Information Fusion. FUSION 2002. (IEEE Cat.No.02EX5997),20021107,2002,1,,664,671 vol.1,"We present here an approach to battlefield situation assessment based on a level 2 fusion processing of incoming information via probabilistic Bayesian Belief Network technology. A belief network (BN) can be thought of as a graphical program script representing causal relationships among various battlefield concepts represented as nodes to which observed significant events are posted as evidence. In our approach, each BN can be constructed in real-time from a library of smaller component-like BNs to assess a specific high-level situation or address mission-specific high-level intelligence requirements. Furthermore, by distributing components of a BN across a set of networked computers, we enhance inferencing efficiency and allow computation at various levels of abstraction suitable for military hierarchical organizations. We demonstrate them effectiveness of our approach by modeling the situation assessment tasks in the context of a battlefield scenario and implementing on our in-house software engine BNet 2000.",,POD:0-9721844-1-4,10.1109/ICIF.2002.1021218,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1021218,,Bayesian methods;Computer networks;Context modeling;Decision making;Distributed computing;Fusion power generation;Information analysis;Libraries;Military computing;Rivers,belief networks;military computing;sensor fusion,Bayesian belief networks;battlefield situation assessment;causal relationships;graphical program script;high-level situation;level 2 fusion processing;military hierarchical organizations;situation assessment,,17,,16,,no,8-11 July 2002,,IEEE,IEEE Conference Publications
Stability analysis for reconfigurable systems with actuator saturation,A. Bateman; D. Ward; J. Monaco,"Barron Associates Inc., Charlottesville, VA, USA",Proceedings of the 2002 American Control Conference (IEEE Cat. No.CH37301),20021107,2002,6,,4783,4788 vol.6,"Discusses a combined analytic and simulation-based approach to assessing the stability of a control law in a system that may be subject to actuator saturation due to failures and subsequent reconfiguration. The analysis is based on linearized plant dynamics, a linearized state-feedback description of the nonlinear controller dynamics, and a nonlinear actuator model. For systems of this type, a method has previously been developed that provides less conservative estimates of the domain of attraction than other available methods. The domain of attraction estimates are used to guide simulation based stability analysis. The combined analytic and simulation based stability assessment approach is implemented in RASCLE, a software package designed to interface with an arbitrary C, C++, or FORTRAN simulation. Through the combination of analytic stability estimates and automated simulation-based analysis, RASCLE can efficiently provide information about the stability of the full nonlinear system under a wide range of conditions for the purpose of validating a reconfigurable controller.",0743-1619;07431619,POD:0-7803-7298-0,10.1109/ACC.2002.1025415,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1025415,,Analytical models;Automatic control;Failure analysis;Hydraulic actuators;Information analysis;Nonlinear dynamical systems;Nonlinear systems;Software design;Software packages;Stability analysis,actuators;aircraft control;control nonlinearities;control system analysis computing;digital simulation;dynamics;fault tolerance;linearisation techniques;matrix algebra;nonlinear control systems;optimisation;software packages;stability;state feedback,RASCLE;actuator saturation;domain of attraction;linear matrix inequality constraints;linearized plant dynamics;linearized state-feedback description;nonlinear actuator model;nonlinear controller dynamics;optimization problems;reconfigurable systems;simulation-based approach;software package;stability analysis,,3,,5,,no,8-10 May 2002,,IEEE,IEEE Conference Publications
Task structuring a brainstorming group activity with an AHP-based group support system,K. N. Kunene; D. Petkov,"Dept. Inf. Syst., Virginia Commonwealth Univ., Richmond, VA, USA",Proceedings of the 35th Annual Hawaii International Conference on System Sciences,20020807,2002,,,2889,2898,"The use of group support systems has been researched repeatedly, using many task types. For brainstorming tasks, the system process support has been restricted to ensuring member anonymity and allowing the simultaneous entry of ideas. Little work has been done on investigating other process improvements for idea-generating groups. In this paper, we investigate the effect of decomposing a brainstorming task on idea quality and quantity in a South African setting. Using Team Expert Choice, which is an analytic hierarchy process (AHP)-based group support system, we conducted an experiment with two groups. We hypothesized that task decomposition would generate more and better-quality ideas. Our findings showed that task decomposition resulted in 40% more ideas than with no decomposition; the effect on decision quality is statistically significant only when the decision quality is measured as the number of good ideas.",,POD:0-7695-1435-9,10.1109/HICSS.2002.994271,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=994271,,Africa;Collaborative software;Humans;Information systems;Mathematics;Problem-solving;Productivity;Statistics;Testing,group decision support systems;idea processors;task analysis,AHP-based group support system;South Africa;Team Expert Choice;analytic hierarchy process;brainstorming group activity;decision quality;idea numbers;idea quality;idea quantity;idea-generating groups;member anonymity;process improvements;simultaneous ideas entry;statistical significance;system process support;task decomposition;task structuring,,1,,29,,no,7-10 Jan. 2002,,IEEE,IEEE Conference Publications
The 8<sup>th</sup> international conference on communications systems: system level buffer size estimation on QoS demands,V. G. Luo; J. Z. Gu,"Comput. Sci. Dep, East China Normal Univ., Shanghai, China","The 8th International Conference on Communication Systems, 2002. ICCS 2002.",20030228,2002,2,,988,994 vol.2,"Buffer size estimation on QoS demands of a multimedia communication system is an important and challenging issue. Because the QoS is often related with many aspects, it is hard to resolve this problem with an analytic method based on a mathematical model. We propose a simulation method based on an operational Distributed Co-Design Model (DCDM). This method is used in the ROCS, and the operating process is shown by taking the MPEG video transmission over a wireless channel.",,POD:0-7803-7510-6,10.1109/ICCS.2002.1183281,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1183281,,,buffer storage;digital simulation;hardware-software codesign;multimedia communication;quality of service;visual communication,DCDM;MPEG video transmission;QoS demands;ROCS;communications systems;distributed co-design model;mathematical model;multimedia communication system;simulation method;system level buffer size estimation;wireless channel,,0,,5,,no,25-28 Nov. 2002,,IEEE,IEEE Conference Publications
The EFTOS approach to dependability in embedded supercomputing,G. Deconinck; V. De Florio; T. A. Varvarigou; E. Verentziotis,"Dept. Elektrotechniek, Katholieke Univ., Leuven, Belgium",IEEE Transactions on Reliability,20020807,2002,51,1,76,90,"Industrial embedded supercomputing applications benefit from a systematic approach to fault tolerance. The EFTOS (embedded fault-tolerant supercomputing) framework provides a flexible and adaptable set of fault-tolerance tools from which the application developer can choose to make an embedded application on a parallel or distributed system more dependable. A high-level description (recovery language) helps the developer specify the fault-tolerance strategies of the application as a second application layer; this separates functional from fault-tolerance aspects of an application, thus shortening the development cycle and improving maintainability. The framework incorporates a backbone (to hook a set of fault-tolerance tools onto, and to coordinate the fault-tolerance actions) and a presentation layer (to monitor and test the fault tolerance behavior). A practical implementation is described with its performance evaluation, using an industrial case study from the energy-transport area, as well as an analytic deduction of the appropriateness of fault-tolerance techniques for various application profiles",0018-9529;00189529,,10.1109/24.994916,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=994916,,Application software;Costs;Electrical equipment industry;Fault tolerance;Fault tolerant systems;Hardware;Monitoring;Redundancy;Spine;Testing,embedded systems;fault tolerant computing;parallel machines,EFTOS;development cycle;energy-transport area;fault tolerance;fault-tolerant communication;industrial embedded supercomputing applications;maintainability;performance evaluation;presentation layer;software-based fault tolerance;stable memory,,5,3,42,,no,2-Mar,,IEEE,IEEE Journals & Magazines
The fireworks effect: exploring trajectory sets in time,D. Donnelly,"Dept. of Phys., Siena Coll., Loudonville, NY, USA",Computing in Science & Engineering,20020807,2002,4,1,92,97,"Many students in physics and engineering courses often grapple with the mathematics they encounter and struggle to extract meaning from the analytic material they are learning, Much can be done computationally at this level to ask questions about and explore analytic functions or results in a numeric environment. Much can be done to help explain the analytic material that students must master if they gain some facility with software that provides a rich numeric environment, such as Mathcad or MATLAB. Often, approaching the same subject from different angles (here, analytic and numeric) helps the bridge-building process of learning. As an example of such an approach, where the opportunity for exploration is always present, I consider here two basic problems: trajectories under a constant force (where the behavior of a trajectory set illuminates the collective motion of a fireworks display) or under a Hooke's law force. The solutions for these examples are well known, yet the material is sufficiently rich so as to offer new insights when new questions are asked and explored.",1521-9615;15219615,,10.1109/5992.976441,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=976441,,Acceleration;Electrostatics;Kinematics;Physics education,computer aided instruction;physics computing,MATLAB;Mathcad;engineering courses;physics courses;students;trajectories of particles,,0,1,4,,no,Jan.-Feb. 2002,,IEEE,IEEE Journals & Magazines
A new approach to the zero-dynamics assignment problem for nonlinear discrete-time systems using functional equations,N. Kazantzis; M. Niemiec,"Dept. of Chem. Eng., Worcester Polytech. Inst., MA, USA","Proceedings of the 2003 American Control Conference, 2003.",20031110,2003,4,,2954,2960 vol.4,"The present paper work aims at the development of a systematic method to arbitrarily assign the zero dynamics of a nonlinear discrete-time system by constructing the requisite synthetic output maps. The problem under consideration is motivated by the need to address the control problem of nonminimum-phase nonlinear discrete-time systems, since the latter represent a rather broad class of systems due to the well-known effect of sampling on the stability of zero-dynamics. In the proposed approach, the above objective can be attained through: a systematic computation of synthetic output maps that induce minimum-phase behavior, and the subsequent integration into the methodological framework of currently available nonminimum-phase compensation schemes that rely on output redefinition. The mathematical formulation of the zero-dynamics assignment problem is realized via a system of nonlinear functional equations (NFEs), a rather general set of necessary and sufficient conditions for solvability is derived. The solution to the above system of NFEs can be proven to be locally analytic, and this enables the development of a solution method that is easily programmable with the aid of a symbolic software package. The synthetic output maps that induce the prescribed zero dynamics for the original nonlinear discrete-time systems can be explicitly computed on the basis of the solution to the aforementioned system of NFEs.",0743-1619;07431619,POD:0-7803-7896-2,10.1109/ACC.2003.1243774,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1243774,,Chemical engineering;Control systems;Digital control;Nonlinear dynamical systems;Nonlinear equations;Nonlinear systems;Packaging;Sampling methods;Stability;Transfer functions,discrete time systems;functional equations;nonlinear control systems;nonlinear equations;zero assignment,control problem;functional equations;mathematical formulation;methodological framework;minimum phase behavior;nonlinear discrete time systems;nonlinear functional equations;nonminimum phase compensation scheme;output redefinition;requisite synthetic output map;subsequent integration;symbolic software package;systematic computation;zero dynamics assignment problem;zero dynamics stability,,0,,21,,no,4-6 June 2003,,IEEE,IEEE Conference Publications
"Acquisition, representation, query and analysis of spatial data: a demonstration 3D digital library",J. Rowe; A. Razdan; A. Simon,"Arizona State Univ., Tempe, AZ, USA","2003 Joint Conference on Digital Libraries, 2003. Proceedings.",20030620,2003,,,147,158,"The increasing power of techniques to model complex geometry and extract meaning from 3D information create complex data that must be described, stored, and displayed to be useful to researchers. Responding to the limitations of two-dimensional (2D) data representations perceived by discipline scientists, the Partnership for Research in Spatial Modeling (PRISM) project at Arizona State University (ASU) developed modeling and analytic tools that raise the level of abstraction and add semantic value to 3D data. The goals are to improve scientific communication, and to assist in generating new knowledge, particularly for natural objects whose asymmetry limit study using 2D representations. The tools simplify analysis of surface and volume using curvature and topology to help researchers understand and interact with 3D data. The tools produced automatically extract information about features and regions of interest to researchers, calculate quantifiable, replicable metric data, and generate metadata about the object being studied. To help researchers interact with the information, the project developed prototype interactive, sketch-based interfaces that permit researchers to remotely search, identify and interact with the detailed, highly accurate 3D models of the objects. The results support comparative analysis of contextual and spatial information, and extend research about asymmetric man-made and natural objects.",,POD:0-7695-1939-3,10.1109/JCDL.2003.1204855,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1204855,,Data analysis;Data mining;Feature extraction;Information analysis;Information geometry;Prototypes;Software libraries;Solid modeling;Topology;Two dimensional displays,Internet;computational geometry;data acquisition;data analysis;data visualisation;digital libraries;feature extraction;information analysis;meta data;query processing;solid modelling;spatial data structures;user interfaces;visual databases,2D data representation limitation;3D data abstraction;3D data interaction;3D digital library;3D information extraction;WWW application;asymmetric man-made object;asymmetric natural object;contextual information analysis;data querying;data storage;feature extraction;geometric modelling;image database;knowledge generation;metadata;replicable metric data;scientific communication;scientific visualization;shape recognition;sketch-based interface;spatial data acquisition;spatial data analysis;spatial data representation;spatial information analysis,,4,2,29,,no,27-31 May 2003,,IEEE,IEEE Conference Publications
Advanced analysis of dynamic neural control advisories for process optimization and parts maintenance,J. P. Card; W. T. Chan; A. Cao; W. Martin; J. Morgan,"IBEX Process Technol. Inc., Lowell, MA, USA","Advanced Semiconductor Manufacturing Conference and Workshop, 2003 IEEEI/SEMI",20030422,2003,,,315,321,"This paper details an advanced set of analyses designed to drive specific process variable setpoint adjustments or maintenance actions required for cost effective process control using the Dynamic Neural Controller‰ã¢ (DNC) wafer-to-wafer advisories for semiconductor manufacturing advanced process control. The new analytic displays and metrics are illustrated using data obtained on a LAM 4520XL at STMicroelectronics as part of a SEMATECH SPIT beta test evaluation. The DNC represents a comprehensive modeling environment that uses as its input extensive process chamber information and history of the time since maintenance actions occurred. The DNC uses a neural network to predict multiple quality output metrics and a closed-loop risk-based optimization to maximize process quality performance while minimizing overall cost of tool operation and machine downtime. The software responds in an advisory mode on a wafer-to-wafer basis as to the optimal actions to be taken. In this paper, we present three specific instances of patterns arising during wafer processing over time that signal the process or equipment engineer to the need for corrective action: either a process setpoint adjustment or specific maintenance actions. Based on the controller's recommended corrective action set with the overall risk reduction predicted by such actions, a metric of corrective action ""urgency"" can be created. The tracking of this metric over time yields different pattern types that signify a quantified need for a specific type of corrective action. Three basic urgency patterns are found: 1. a pattern in a given maintenance action over time showing increasing urgency or ""risk reduction"" capability for the action; 2. a pattern in a process variable specific to a given recipe indicating a chronic request over time to only adjust the variable setpoint either above or below the current target; 3. a pattern in a process variable existing over all recipes processed through the chamber indicating chronic request to adjust the variable setpoint in either or both directions over time. This pattern is a pointer to the need for a maintenance action that is either corroborated by the urgency graph for that maintenance action, or if no such action has been previously take- n, a guide to the source of the equipment malfunction.",1078-8743;10788743,POD:0-7803-7673-0,10.1109/ASMC.2003.1194514,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1194514,,Costs;Displays;History;Manufacturing processes;Process control;Pulp manufacturing;Risk management;Semiconductor device manufacture;Semiconductor device modeling;Testing,closed loop systems;maintenance engineering;neurocontrollers;process control;semiconductor device manufacture,advanced process control;closed-loop control;dynamic neural controller;parts maintenance;process optimization;risk analysis;semiconductor manufacturing;urgency metric;wafer-to-wafer advisory,,0,3,12,,no,31 March-1 April 2003,,IEEE,IEEE Conference Publications
An agent-based architecture for analyzing business processes of real-time enterprises,Jun-Jang Jeng; J. Schiefer; H. Chang,"IBM T.J. Watson Res. Center, Yorktown Heights, NY, USA","Seventh IEEE International Enterprise Distributed Object Computing Conference, 2003. Proceedings.",20030929,2003,,,86,97,"As the desire for business intelligence capabilities for e-business processes expands, existing workflow management systems and decision support systems are not able to provide continuous, real-time analytics for decision makers. Business intelligence requirements may appear to be different across the various industries, but the underlying requirements are similar nformation that is integrated, current, detailed, and immediately accessible. In this paper we introduce an agent-based architecture that supports a complete business intelligence process to sense, interpret, predict, automate and respond to business processes and aims to decrease the time it takes to make business decisions. In fact, there should be almost zero-latency between the cause and effect of a business decision. Our architecture enables analysis across corporate business processes notifies the business of auctionable recommendations or automatically triggers business operations, effectively closing the gap between business intelligence systems and business processes.",,POD:0-7695-1994-6,10.1109/EDOC.2003.1233840,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1233840,,Decision making;Decision support systems;Humans;Information analysis;Intelligent agent;Intelligent systems;Magnesium compounds;Performance analysis;Real time systems;Workflow management software,decision support systems;electronic commerce;object-oriented programming;real-time systems;software agents;software architecture,agent-based architecture;auctionable recommendations;business decision making;business intelligence process;business operations;business process analysis;corporate business processes;decision makers;decision support systems;e-business processes;intelligence requirements;real-time analytics;real-time enterprises;workflow management systems;zero-latency,,6,3,20,,no,16-19 Sept. 2003,,IEEE,IEEE Conference Publications
Case studies using graphically enhanced computer software to improve technology assessments and enhance clinical decisions,E. B. Sloane; M. Liberatore; R. Nydick,"Dept. of Decision & Inf. Technol., Villanova Univ., PA, USA",Proceedings of the 25th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (IEEE Cat. No.03CH37439),20040405,2003,4,,3636,3639 Vol.4,"This paper describes how clinical engineers and healthcare CIOs, CTOs, and IT/IS specialists can use the Analytic Hierarchy Process (AHP) to improve the quality of diverse and important decisions that hospitals face today. AHP is a versatile and proven decision support tool that allows the user to design a hierarchical structure for decision-making and weighs the trade-offs between decision criteria and alternatives (Saaty 1977, Saaty 1996).",1094-687X;1094687X,POD:0-7803-7789-3,10.1109/IEMBS.2003.1280942,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1280942,,Books;Business;Computer aided software engineering;Decision making;Educational institutions;Finance;Hospitals;Information analysis;Information technology;Medical services,biomedical engineering;computer graphics;decision making;health care;medical computing;software tools;technology management,AHP;Analytic Hierarchy Process;IT/IS specialists;case studies;clinical decisions enhancement;clinical engineers;decision support tool;decision-making;graphically enhanced computer software;healthcare CIO;healthcare CTO;hospitals;technology assessments,,0,,11,,no,17-21 Sept. 2003,,IEEE,IEEE Conference Publications
Challenges in computer architecture evaluation,K. Skadron; M. Martonosi; D. I. August; M. D. Hill; D. J. Lilja; V. S. Pai,"Dept. of Comput. Sci., Virginia Univ., Charlottesville, VA, USA",Computer,20030811,2003,36,8,30,36,"We focus on problems suited to the current evaluation infrastructure. The current limitation and trends in evaluation techniques are troublesome and could noticeably slow the rate of computer system innovation. New research has been recommended to help and make quantitative evaluations of computer systems manageable. We support research in the areas of simulation frameworks, benchmarking methodologies, analytic methods, and validation techniques.",0018-9162;00189162,,10.1109/MC.2003.1220579,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1220579,,Application software;Computational modeling;Computer architecture;Computer simulation;Mathematical model;Multiprocessing systems;Performance evaluation;System-on-a-chip;Technological innovation;Temperature,computer architecture;performance evaluation;program verification;simulation;software metrics,benchmarking methodology;computer architecture evaluation;computer system innovation;current evaluation infrastructure;quantitative evaluation;simulation framework;system analytic method;system validation technique,,33,,12,,no,Aug. 2003,,IEEE,IEEE Journals & Magazines
Data-driven computer graphics,H. Pfister,"Mitsubishi Electr. Res. Lab., Cambridge, MA, USA","11th Pacific Conference onComputer Graphics and Applications, 2003. Proceedings.",20031020,2003,,,38,,"Summary form only given. The field of computer graphics abounds with modeling and simulation problems. Among these are the representation of surface shape, the description of surface reflectance, the probabilistic modeling of small-scale variations, and the application of physics for simulating the dynamics of rigid and elastic materials. During its formative years, computer graphics has focused largely on developing algorithms and systems for performing efficient simulations that transform these analytic representations into images and animations. At present, the simulation framework for computer graphics is very mature. In last ten years, we have also witnessed significant technological developments in the areas of high-quality sensors and measurement devices. However, the data provided from these devices are frequently incompatible with the representations assumed by most computer graphics systems. The author explores new approaches to computer graphics that attempt to bridge the dichotomy between parametric and empirical modeling.",,POD:0-7695-2028-6,10.1109/PCCGA.2003.1238244,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1238244,,Algorithm design and analysis;Analytical models;Application software;Computational modeling;Computer graphics;Computer simulation;Image analysis;Performance analysis;Physics;Reflectivity,computer graphics;digital simulation;image processing;interpolation,analytic representations;data-driven computer graphics;empirical modeling;high-quality sensors;measurement devices;modeling problems;parametric modeling;physics application;probabilistic modeling;simulation framework;simulation problems;small-scale variations;surface reflectance;surface shape;technological developments,,0,,,,no,8-10 Oct. 2003,,IEEE,IEEE Conference Publications
Digitally controlled converter with dynamic change of control law and power throughput,C. Nesgaard; M. A. E. Andersen; N. Nielsen,"Tech. Univ. Denmark, Lyngby, Denmark","Power Electronics Specialist Conference, 2003. PESC '03. 2003 IEEE 34th Annual",20030804,2003,3,,1163,1168 vol.3,"With the continuous development of faster and cheaper microprocessors the field of applications for digital control is constantly expanding. Based on this trend we describe the analysis and implementation of multiple control laws within the same controller. Also, implemented within the control algorithm is a thermal monitoring scheme used for assessment of safe converter power throughput. An added benefit of this thermal monitoring is the possibility of software implemented analytic redundancy, which improves system fault resilience. Finally, reliability issues concerning the substitution of analog controllers with their digital counterparts are considered. The outline of the paper is divided into two segments - the first being an experimental analysis of the timing behavior by means of code optimization - the second being an examination of the dynamics of incorporating two control laws using multiple control parameters.",0275-9306;02759306,POD:0-7803-7754-0,10.1109/PESC.2003.1216613,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1216613,,Circuits;Digital control;Microcontrollers;Microprocessors;Monitoring;Power system dynamics;Pulse width modulation;Redundancy;Switching converters;Throughput,DC-DC power convertors;PWM power convertors;digital control;dynamics;microcontrollers;optimisation;redundancy;reliability,analog controllers;code optimization;control law;digitally controlled converter;dynamic change;microprocessors;multiple control parameters;power throughput;reliability;software implemented analytic redundancy;system fault resilience;thermal monitoring;timing behavior,,2,1,6,,no,15-19 June 2003,,IEEE,IEEE Conference Publications
DWARF: an approach for requirements definition and management of data warehouse systems,F. R. S. Paim; J. F. B. de Castro,"SERPRO, Recife, Brazil","Proceedings. 11th IEEE International Requirements Engineering Conference, 2003.",20030923,2003,,,75,84,"In the novel domain of data warehouse systems, software engineers are required to define a solution that integrates with a number of heterogeneous sources to extract, transform and aggregate data, as well as to offer flexibility to run adhoc queries that retrieve analytic information. Moreover, these activities should be performed based on a concise dimensional schema. This intricate process with its particular multidimensionality claims for a requirements engineering approach to aid the precise definition of data warehouse applications. We adapt the traditional requirements engineering process and propose DWARF, a data warehouse requirements definition method. A case study demonstrates how the method has been successfully applied in the company wise development of a large-scale data warehouse system that stores hundreds of gigabytes of strategic data for the Brazilian Federal Revenue Service.",1090-705X;1090705X,POD:0-7695-1980-6,10.1109/ICRE.2003.1232739,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1232739,,Aggregates;Data engineering;Data mining;Data warehouses;Information analysis;Information retrieval;Large-scale systems;Multidimensional systems;Software systems;Systems engineering and theory,data warehouses;formal specification,Brazilian Federal Revenue Service;DWARF;analytic information retrieval;data warehouse requirements definition;data warehouse system;requirements engineering;software engineers,,10,,26,,no,8-12 Sept. 2003,,IEEE,IEEE Conference Publications
Evaluation of several nonparametric bootstrap methods to estimate confidence intervals for software metrics,Skylar Lei; M. R. Smith,"Gen. Dynamics Canada, Calgary, Alta., Canada",IEEE Transactions on Software Engineering,20031117,2003,29,11,996,1004,"Sample statistics and model parameters can be used to infer the properties, or characteristics, of the underlying population in typical data-analytic situations. Confidence intervals can provide an estimate of the range within which the true value of the statistic lies. A narrow confidence interval implies low variability of the statistic, justifying a strong conclusion made from the analysis. Many statistics used in software metrics analysis do not come with theoretical formulas to allow such accuracy assessment. The Efron bootstrap statistical analysis appears to address this weakness. In this paper, we present an empirical analysis of the reliability of several Efron nonparametric bootstrap methods in assessing the accuracy of sample statistics in the context of software metrics. A brief review on the basic concept of various methods available for the estimation of statistical errors is provided, with the stated advantages of the Efron bootstrap discussed. Validations of several different bootstrap algorithms are performed across basic software metrics in both simulated and industrial software engineering contexts. It was found that the 90 percent confidence intervals for mean, median, and Spearman correlation coefficients were accurately predicted. The 90 percent confidence intervals for the variance and Pearson correlation coefficients were typically underestimated (60-70 percent confidence interval), and those for skewness and kurtosis overestimated (98-100 percent confidence interval). It was found that the Bias-corrected and accelerated bootstrap approach gave the most consistent confidence intervals, but its accuracy depended on the metric examined. A method for correcting the under-/ overestimation of bootstrap confidence intervals for small data sets is suggested, but the success of the approach was found to be inconsistent across the tested metrics.",0098-5589;00985589,,10.1109/TSE.2003.1245301,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1245301,,Computer industry;Context modeling;Estimation error;Footwear industry;Software algorithms;Software engineering;Software metrics;State estimation;Statistical analysis;Statistics,software metrics;software reliability,Efron bootstrap statistical analysis;Spearman correlation coefficients;confidence intervals estimation;industrial software engineering;model parameters;nonparametric bootstrap methods;sample statistics;software metrics,,24,,24,,no,Nov. 2003,,IEEE,IEEE Journals & Magazines
Exact probability of erasure and a decoding algorithm for convolutional codes on the binary erasure channel,B. M. Kurkoski; P. H. Siegel; J. K. Wolf,"Dept. of Electr. & Comput. Eng., California Univ., San Diego, CA, USA","Global Telecommunications Conference, 2003. GLOBECOM '03. IEEE",20040114,2003,3,,1741,1745 vol.3,"Analytic expressions for the exact probability of erasure for systematic, rate- 1/2 convolutional codes used to communicate over the binary erasure channel and decoded using the soft-input, soft-output (SISO) and a posteriori probability (APP) algorithms are given. An alternative forward-backward algorithm which produces the same result as the SISO algorithm is also given. This low-complexity implementation, based upon lookup tables, is of interest for systems which use convolutional codes, such as turbo codes.",,POD:0-7803-7974-8,10.1109/GLOCOM.2003.1258535,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1258535,,Algorithm design and analysis;Application software;Computer networks;Convolutional codes;Decoding;Disk drives;Lifting equipment;Polynomials;Table lookup;Turbo codes,Markov processes;convolutional codes;maximum likelihood estimation;table lookup;telecommunication channels;turbo codes,a posteriori probability algorithm;binary erasure channel;decoding algorithm;erasure algorithm;forward-backward algorithm;lookup table;low-complexity implementation;soft-input soft-output algorithm;systematic rate- 1/2 convolutional code;turbo code,,17,,10,,no,1-5 Dec. 2003,,IEEE,IEEE Conference Publications
Fractional cut: improved recursive bisection placement,A. Agnihotri; M. C. Yildiz; A. Khatkhate; A. Mathur; S. Ono; P. H. Madden,"Dept. of Comput. Sci., SUNY Binghamton, NY, USA",ICCAD-2003. International Conference on Computer Aided Design (IEEE Cat. No.03CH37486),20040107,2003,,,307,310,"In this paper, we present improvements to recursive bisection based placement. In contrast to prior work, our horizontal cut lines are not restricted to row boundaries; this avoids a ""narrow region"" problem. To support these new cut line positions, a dynamic programming based legalization algorithm has been developed. The combination of these has improved the stability and lowered the wire lengths produced by our Feng Shui placement tool. On benchmarks derived from industry partitioning examples, our results are close to those of the annealing based tool Dragon, while taking only a fraction of the run time. On synthetic benchmarks, our wire lengths are nearly 23% better than those of Dragon. For both benchmark suites, our results are substantially better than those of the recursive bisection based tool Capo and the analytic placement tool Kraftwerk.",,POD:1-58113-762-1,10.1109/ICCAD.2003.1257685,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1257685,,Annealing;Computer science;Equations;Explosions;Fabrication;Logic gates;Partitioning algorithms;Permission;Stability;Wire,circuit layout CAD;dynamic programming;simulated annealing;software tools,Capo;Dragon;Feng Shui placement tool;Kraftwerk;analytic placement tool;annealing based tool;bisection based tool;dynamic programming;horizontal cut lines;legalization algorithm;recursive bisection placement;wire lengths,,30,1,18,,no,9-13 Nov. 2003,,IEEE,IEEE Conference Publications
Internet quarantine: requirements for containing self-propagating code,D. Moore; C. Shannon; G. M. Voelker; S. Savage,"California Univ., San Diego, CA, USA",IEEE INFOCOM 2003. Twenty-second Annual Joint Conference of the IEEE Computer and Communications Societies (IEEE Cat. No.03CH37428),20030709,2003,3,,1901,1910 vol.3,"It has been clear since 1988 that self-propagating code can quickly spread across a network by exploiting homogeneous security vulnerabilities. However, the last few years have seen a dramatic increase in the frequency and virulence of such ""worm"" outbreaks. For example, the Code-Red worm epidemics of 2001 infected hundreds of thousands of Internet hosts in a very short period - incurring enormous operational expense to track down, contain, and repair each infected machine. In response to this threat, there is considerable effort focused on developing technical means for detecting and containing worm infections before they can cause such damage. This paper does not propose a particular technology to address this problem, but instead focuses on a more basic question: How well will any such approach contain a worm epidemic on the Internet? We describe the design space of worm containment systems using three key parameters - reaction time, containment strategy and deployment scenario. Using a combination of analytic modeling and simulation, we describe how each of these design factors impacts the dynamics of a worm epidemic and, conversely, the minimum engineering requirements necessary to contain the spread of a given worm. While our analysis cannot provide definitive guidance for engineering defenses against all future threats, we demonstrate the lower bounds that any such system must exceed to be useful today. Unfortunately, our results suggest that there are significant technological and administrative gaps to be bridged before an effective defense can be provided in today's Internet.",0743-166X;0743166X,POD:0-7803-7752-4,10.1109/INFCOM.2003.1209212,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1209212,,Analytical models;Computer worms;Design engineering;Frequency;Internet;Pathogens;Security;Software systems;Space technology;Web server,Internet;computer viruses;telecommunication security,Code-Red worm epidemic;Internet hosts;analytic modeling;containment strategy;definitive guidance;deployment scenario;design factors;homogeneous security vulnerabilities;reaction time;self-propagating code;simulation;wide spread containment mechanisms,,133,81,28,,no,30 March-3 April 2003,,IEEE,IEEE Conference Publications
MACH3: A CHSSI code for computational magnetohydrodynamics of general materials in generalized coordinates,J. MacGillivray; R. E. Peterkin; D. A. Burke; M. H. Frese; S. Frese; J. Neri; J. W. Schumer,"Air Force Res. Lab., Kirtland AFB, NM, USA",2003 User Group Conference. Proceedings,20040108,2003,,,228,235,"We were funded via the Common HPC Software Support Initiative (CHSSI) from 2000-2002 to develop a fully 3D arbitrary-coordinate parallel time-domain magnetohydrodynamic (MHD) simulation code - the CEA-10 project. CEA-10 built upon the single-fluid MHD, arbitrary Lagrangian-Eulerian multiblock, multitemperature, simulation environment called MACH3 (Multibloch Arbitrary Coordinate Hydromagnetics in 3D). The CEA-10 software underwent successful beta testing and review in December 2002. The beta tests consisted of five simulations: 1D magnetic diffusion, 2D Alfvenic shock, 2D flow over a wedge, 3D gas-filled rod pinch, and 3D laser-target interactions, run on various HPC platforms. The first three have analytic solutions and last two have experimental data and qualitative models to which simulated results can be compared. We obtained different scaled efficiency and parallel speedup results on different platforms and for different test problems.",,POD:0-7695-1953-9,10.1109/DODUGC.2003.1253397,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1253397,,Computational modeling;Electric shock;Gas lasers;Lagrangian functions;Laser modes;Magnetic analysis;Magnetic materials;Magnetohydrodynamics;Software testing;Time domain analysis,computational electromagnetics;flow simulation;magnetohydrodynamics;materials science,3D arbitrary-coordinate parallel time-domain;CEA-10 project;Common HPC Software Support Initiative;Lagrangian-Eulerian multiblock;MACH3;Multibloch Arbitrary Coordinate Hydromagnetics in 3D;beta testing;computational magnetohydrodynamics,,1,,11,,no,9-13 June 2003,,IEEE,IEEE Conference Publications
Modeling malware spreading dynamics,M. Garetto; W. Gong; D. Towsley,"Dipt. di Elettronica, Politecnico di Torino, Italy",IEEE INFOCOM 2003. Twenty-second Annual Joint Conference of the IEEE Computer and Communications Societies (IEEE Cat. No.03CH37428),20030709,2003,3,,1869,1879 vol.3,"In this paper we present analytical techniques that can be used to better understand the behavior of malware, a generic term that refers to all kinds of malicious software programs propagating on the Internet, such as e-mail viruses and worms. We develop a modeling methodology based on Interactive Markov Chains that is able to capture many aspects of the problem, especially the impact of the underlying topology on the spreading characteristics of malware. We propose numerical methods to obtain useful bounds and approximations in the case of very large systems, validating our results through simulation. An analytic methodology represents a fundamentally important step in the development of effective countermeasures for future malware activity. Furthermore, we believe our approach can help to understand a wide range of ""dynamic interactions on networks"", such as routing protocols and peer-to-peer applications.",0743-166X;0743166X,POD:0-7803-7752-4,10.1109/INFCOM.2003.1209209,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1209209,,Application software;Computer science;Computer viruses;Computer worms;Electronic mail;Internet;Peer to peer computing;Routing protocols;Topology;Viruses (medical),Internet;Markov processes;computer viruses;network topology;numerical analysis,Interactive Markov Chains;Internet;e-mail viruses;e-mail worms;malicious software program;malware behavior;network dynamic interaction;numerical method;peer-to-peer application;routing protocol;simulation,,50,,16,,no,30 March-3 April 2003,,IEEE,IEEE Conference Publications
Modelling the performance of CORBA using layered queueing networks,T. Verdickt; B. Dhoedt; F. Gielen; P. Demeester,"Dept. of Inf. Technol., Ghent Univ., Belgium",2003 Proceedings 29th Euromicro Conference,20030915,2003,,,117,123,"One-of the typical features of distributed systems is the heterogeneity of its components (e.g. geographical spreading and different platform architectures), leading to interoperability issues. Many of these are handled by generic middleware-based solutions. We present an analytic model of the impact of using such middleware on the overall system performance. Specifically, a layered queueing network is described that models a client/server system using CORBA as a middleware system offering location transparency. The response times estimated from the model are compared to the measured response times for a growing number of clients, in order to assess the accuracy of the model and the values of the parameters in the model. This model can then be used for designing a distributed application, before the entire system is installed or even fully implemented.",1089-6503;10896503,POD:0-7695-1996-2,10.1109/EURMIC.2003.1231576,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1231576,,Queuing analysis;Software performance,distributed object management;middleware;queueing theory;software performance evaluation,CORBA;client-server system;distributed system;interoperability;layered queueing network;middleware;system performance modelling;system response time,,2,,15,,no,1-6 Sept. 2003,,IEEE,IEEE Conference Publications
Multi-version attack recovery for workflow systems,Meng Yu; Peng Liu; Wanyu Zang,"Sch. of Inf. Sci. & Technol., Pennsylvania State Univ., University Park, PA, USA","19th Annual Computer Security Applications Conference, 2003. Proceedings.",20040108,2003,,,142,150,"Workflow systems are popular in daily business processing. Since vulnerabilities cannot be totally removed from a system, recovery from successful attacks is unavoidable. We focus on attacks that inject malicious tasks into workflow management systems. We introduce practical techniques for on-line attack recovery, which include rules for locating damage and rules for execution order. In our system, an independent intrusion detection system reports identified malicious tasks periodically. The recovery system detects all damage caused by the malicious tasks and automatically repairs the damage according to dependency relations. Without multiple versions of data objects, recovery tasks may be corrupted by executing normal tasks when we try to run damage analysis and normal tasks concurrently. We address the problem by introducing multiversion data objects to reduce unnecessary blocking of normal task execution and improve the performance of the whole system. We analyze the integrity level and performance of our system. The analytic results demonstrate guidelines for designing such kinds of systems.",,POD:0-7695-2041-3,10.1109/CSAC.2003.1254319,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1254319,,Access control;Application software;Computer crashes;Computer security;Guidelines;Intrusion detection;Performance analysis;Transaction databases;Workflow management software,business data processing;security of data;system recovery;workflow management software,data object;intrusion detection system;multiversion attack recovery;on-line attack recovery;workflow management system,,2,,15,,no,8-12 Dec. 2003,,IEEE,IEEE Conference Publications
On the accuracy of a cellular location system based on RSS measurements,A. J. Weiss,"Dept. of Electr. Eng. Syst., Tel-Aviv Univ., Israel",IEEE Transactions on Vehicular Technology,20031124,2003,52,6,1508,1518,"We discuss the performance of a cellular location system based on received signal strength (RSS) measurements. Each mobile station (MS) collects RSS measurements of the downlink control channels transmitted by the surrounding base stations. It is assumed that there is one-to-one mapping between the RSS and the MS location. Hence, these measurements can be used to obtain the MS location. We examine the accuracy of this method by deriving the Cramer-Rao bound, the concentration ellipse, and the circular error probability (CEP) of this method. In addition, we obtain an analytic expression that predicts the point at which accuracy deviates significantly from the bound (the threshold point). The accuracy of the method does not meet the FCC E911 requirement, but it is an attractive solution for less-demanding location-based services.",0018-9545;00189545,,10.1109/TVT.2003.819613,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1247811,,Base stations;Costs;Downlink;FCC;Hardware;Measurement standards;Radio frequency;Signal processing;Software measurement;Telephone sets,cellular radio;field strength measurement;probability;telecommunication control,ALI;Cramer-Rao bound;RSS measurements;accuracy;automatic location identification;cellular location system;circular error probability;concentration ellipse;downlink control channels;mobile assisted handoff;mobile station;performance;received signal strength;threshold point,,105,,20,,no,Nov. 2003,,IEEE,IEEE Journals & Magazines
Rate distortion optimization in the scalable video coding,Zhijie Yang; Feng Wu; Shipeng Li,"Inst. of Software, Acad. Sinica, Beijing, China","Circuits and Systems, 2003. ISCAS '03. Proceedings of the 2003 International Symposium on",20030625,2003,2,,II-884,II-887 vol.2,"This paper discusses the rate distortion optimization (RDO) issue in the scalable video coding. It is different from that in the non-scalable video coding because a scalable codec usually presents the decoded video within a certain range of the quality, frame rate and resolution. Therefore, this paper first analyzes the problem from a general model and gives a common description by the Lagrangian formula. Secondly, an algorithm is proposed to solve the above optimization problem at a specific scalable codec: the H.26L-based PFGS. At the same time, two analytic formulae are derived to ascertain the parameters ëÈ in this algorithm. The experimental results show that the proposed algorithm significantly improves the coding efficiency of the H.26L-based PFGS. Further studies on this problem will be quite significant and challenging.",,POD:0-7803-7761-3,10.1109/ISCAS.2003.1206116,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1206116,,Algorithm design and analysis;Asia;Bit rate;Codecs;Decoding;Lagrangian functions;MPEG 4 Standard;Motion compensation;Rate-distortion;Video coding,optimisation;video codecs;video coding,H.26L PFGS codec;Lagrangian model;rate distortion optimization algorithm;scalable video coding,,2,,15,,no,25-28 May 2003,,IEEE,IEEE Conference Publications
Risk analysis in project of software development,Xiangnan Lu; Yali Ge,"Sch. of Manage., Zhejiang Univ., Hangzhou, China","Engineering Management Conference, 2003. IEMC '03. Managing Technologically Driven Organizations: The Human Side of Innovation and Change",20040107,2003,,,72,75,"The Project Management in the area of IT is becoming hot topic now. The risk problems in IT Projects, especially in software projects become more and more concentration in software project management in China. This paper analyze systematically the characteristics of project management for software itself, then the paper analyses the risk of software development project in following two aspects: one for owners another for contractors. To owners the paper identify and analyze the risk in software development projects according to life cycle of a project. To contractors the paper identifies and assess the risk of project management in software development, and on the bases of investigating of software development project in the IT enterprises of China some conclusions are obtained with the method of AHP.",,POD:0-7803-8150-5,10.1109/IEMC.2003.1252234,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1252234,,Costs;Financial management;Knowledge management;Manufacturing processes;Programming;Project management;Risk analysis;Risk management;Scheduling;Software development management,contracts;information technology;project management;risk analysis;software development management,AHP method;Chinas ITenterprise;IT projects;analytic hierarchy process;contractors;information technology;owners;projects life cycle;risk analysis;risk assessment;risk identification;software development project;software project management,,1,,6,,no,2-4 Nov. 2003,,IEEE,IEEE Conference Publications
"Semantically reliable multicast: definition, implementation, and performance evaluation",J. Pereira; L. Rodrigues; R. Oliveira,,IEEE Transactions on Computers,20030219,2003,52,2,150,165,"Semantic reliability is a novel correctness criterion for multicast protocols based on the concept of message obsolescence: A message becomes obsolete when its content or purpose is superseded by a subsequent message. By exploiting obsolescence, a reliable multicast protocol may drop irrelevant messages to find additional buffer space for new messages. This makes the multicast protocol more resilient to transient performance perturbations of group members, thus improving throughput stability. This paper describes our experience in developing a suite of semantically reliable protocols. It summarizes the motivation, definition, and algorithmic issues and presents performance figures obtained with a running implementation. The data obtained experimentally is compared with analytic and simulation models. This comparison allows us to confirm the validity of these models and the usefulness of the approach. Finally, the paper reports the application of our prototype to distributed multiplayer games.",0018-9340;00189340,,10.1109/TC.2003.1176983,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1176983,,Analytical models;Computational modeling;Context;Context-aware services;Multicast algorithms;Multicast protocols;Prototypes;Stability;Throughput;Virtual prototyping,multicast protocols;software performance evaluation;software reliability,buffer space;correctness criterion;distributed multiplayer games;group members;irrelevant message abandonment;irrelevant message dropping;message obsolescence;multicast protocols;performance evaluation;semantically reliable multicast;throughput stability;transient performance perturbations,,10,,37,,no,Feb. 2003,,IEEE,IEEE Journals & Magazines
Simulation and analytical calculation of the noise figure in HEMT gate mixers,F. Amrouche; R. Allam; J. M. Paillot,"Lab. d'Automatique et d'Informatique Industrielle, Univ. de Poitiers, France",33rd European Microwave Conference Proceedings (IEEE Cat. No.03EX723C),20040206,2003,1,,351,354 Vol.1,"A simplified analytic expression is developed to predict the noise performance of HEMT gate mixers. A nonlinear model of noise was given for theoretical study and implanted in ADS simulator. A contribution of each noise source was presented in this paper. This study is applied to a millimeter-wave HEMT gate mixer. The LO, RF and IF frequencies chosen for this test were 24.5, 28.5 and 4 GHz respectively. Good agreement is obtained between analytical calculation, simulation and experimental noise figure in single side band NF<sub>SSB</sub>,.",,POD:1-58053-834-7,10.1109/EUMC.2003.1262291,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1262291,,1f noise;Analytical models;Circuit noise;Equivalent circuits;HEMTs;Millimeter wave technology;Noise figure;Noise generators;Radio frequency;Voltage,HEMT integrated circuits;MMIC mixers;circuit simulation;equivalent circuits;field effect MMIC;integrated circuit noise,24.5 GHz;28.5 GHz;4 GHz;ADS simulator;HEMT gate mixers;equivalent circuit;harmonic balance software;noise figure;nonlinear model;simplified analytic expression;single side band,,0,,12,,no,7-9 Oct. 2003,,IEEE,IEEE Conference Publications
Smooth and efficient zooming and panning,J. J. van Wijk; W. A. A. Nuij,"Dept. of Math. & Comput. Sci., Technische Universiteit Eindvohen, Netherlands",IEEE Symposium on Information Visualization 2003 (IEEE Cat. No.03TH8714),20090304,2003,,,15,23,"Large 2D information spaces, such as maps, images, or abstract visualizations, require views at various level of detail: close ups to inspect details, overviews to maintain (literally) an overview. Users often switch between these views. We discuss how smooth animations from one view to another can be defined. To this end, a metric on the effect of simultaneous zooming and panning is defined, based on an estimate of the perceived velocity. Optimal is defined as smooth and efficient. Given the metric, these terms can be translated into a computational model, which is used to calculate an analytic solution for optimal animations. The model has two free parameters: animation speed and zoom/pan trade off. A user experiment to find good values for these is described.",,POD:0-7803-8154-8,10.1109/INFVIS.2003.1249004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1249004,,Animation;Chromium;Cities and towns;Computational modeling;Computer graphics;Data visualization;Navigation;Software engineering;Switches;Uninterruptible power systems,computational geometry;computer animation;data visualisation;graphical user interfaces;image processing,2D information spaces;abstract visualization;animation speed;computational model;image visualization;inspection techniques;map visualization;navigation;optimal animations;panning;perceived velocity;scale space;scrolling;smooth animations;software engineering;user interfaces;zooming,,17,4,11,,no,21-21 Oct. 2003,,IEEE,IEEE Conference Publications
Social scheduler: a proposal of collaborative personal task management,I. Ohmukai; H. Takeda,"Graduate Univ. for Adv. Studies, Tokyo, Japan",Proceedings IEEE/WIC International Conference on Web Intelligence (WI 2003),20031027,2003,,,666,670,"We propose a collaborative approach for personal task management which is modeled as an integration of alliance and human-in-the-loop model. Alliance model is based on information sharing and collaboration of several persons. They disclose their task condition and maintain to be updatable by their friends. To avoid privacy issues we propose emergent group discovery algorithm to control the level of disclosure. Human-in-the-loop model consists of three subsystems to support decision-making activities. Visualizer indicates the attributes associated with each task such as the deadline, the subjective priority, and the workload, which are determined by the user. Optimizer generates executable schedules from these tasks by active scheduler and multiobjective genetic algorithm. Recommender evaluates these alternatives by analytic hierarchy process. We implement client/server system called social scheduler on cell-phones environment. We remark the advantages of our approach with an experiment.",,POD:0-7695-1932-6,10.1109/WI.2003.1241292,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1241292,,Collaboration;Collaborative software;Collaborative work;Decision making;Environmental management;Humans;Proposals;Resource management;Scheduling;Visualization,client-server systems;decision making;decision support systems;genetic algorithms;groupware;mobile handsets,alliance model;analytic hierarchy process;cell-phones environment;client-server system;collaborative personal task management;decision-making activities;emergent group discovery algorithm;human-in-the-loop model;information sharing;multiobjective genetic algorithm;optimizer;privacy issues;recommender;social scheduler;visualizer,,0,,10,,no,13-17 Oct. 2003,,IEEE,IEEE Conference Publications
Stationary and adaptive replication approach to data availability in structured peer-to-peer overlay networks,Xuezheng Liu; Guangwen Yang; DingXing Wang,"Dept. of Comput. Sci. & Technol., Tsinghua Univ., Beijing, China","The 11th IEEE International Conference on Networks, 2003. ICON2003.",20040226,2003,,,265,270,"Structured peer-to-peer overlay networks offer a novel infrastructure for large-scale applications. However, replications aimed for high data availability often have too much maintenance cost. In this paper we propose a simple and efficient replication strategy over low available peers, which has stationary replica locations and variation-tolerant data recovery mechanism, so that it can greatly reduce the costs for maintaining replicas, esp. in the circumstance of poor peer availabilities. We also present an analytic model for data availability and replication maintenance costs. Based on our model, we make adaptive mechanism for replicas checking, which can further reduce the cost and guarantee data availability in variable circumstance and conditions for living replicas. The simulation results proved the contribution of our approach and model.",1531-2216;15312216,POD:0-7803-7788-5,10.1109/ICON.2003.1266201,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1266201,,Application software;Availability;Collaborative work;Computer science;Costs;Data analysis;Electrostatic precipitators;Intelligent networks;Large-scale systems;Peer to peer computing,computer maintenance;data communication,adaptive replication;data availability;peer-to-peer overlay networks;replicas checking;replication maintenance costs;variation-tolerant data recovery mechanism,,0,,10,,no,28 Sept.-1 Oct. 2003,,IEEE,IEEE Conference Publications
Technology Management for Reshaping the World. PICMET'03. Portland International Conference on Management of Engineering and Technology (IEEE Cat. No.03CH37455),,,"Management of Engineering and Technology, 2003. PICMET '03. Technology Management for Reshaping the World. Portland International Conference on",20030818,2003,,,,,"The following topics are dealt with: strategic and policy issues in technology management; R&D and innovation management; knowledge management; information and communication management; project and product management; software process management; technological entrepreneurship; technology roadmapping; and technology transfer, commercialization and marketing.",,POD:1-890843-08-3,10.1109/PICMET.2003.1222771,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1222771,,Conference management;Engineering management;IEEE catalog;Isolation technology;Libraries;Permission;Read only memory;Technology management,innovation management;knowledge based systems;knowledge management;management information systems;marketing;project management;software management;technology management;technology transfer;telecommunication network management,PRISM;R&D management;adaptation strategy;analytic Delphi process;cellular phone display;commercialization;communication management;communication technology industry;computer servers;e-commerce technology;economic growth;electronic cooling technology;fuel cells;high tech marketing;high technology manufacturing;information management;information technology industry;information theory;innovation management;integrated strategy development;knowledge management;knowledge-based economy;life-cycle perspective;location-based services;m-commerce;macro approach;mobile Internet;multiple perspective approach;policy issues;portfolio alignment;price sensitivity measurement;process development;product development;product management;project commitment;project management;project strategy;regional technological innovation ecosystem;software engineering process;software process management;strategic issues;technological entrepreneurship;technology development envelope;technology incubator program;technology management;technology roadmapping;technology transfer;uncertain network structures,,0,,,,no,24-24 July 2003,,IEEE,IEEE Conference Publications
Ultrasound simulation for 3D-axisymmetric models,J. J. Kaufman; Gangming Luo; R. S. Siffert,"Dept. of Orthopaedics, Mount Sinai Sch. of Medicine, New York, NY, USA","IEEE Symposium on Ultrasonics, 2003",20040504,2003,2,,2065,2068 Vol.2,"Development of ultrasound nondestructive evaluation testing (NDT) techniques has involved a combination of both analytic and experimental methods. In contrast, relatively little software exists for ultrasound simulation in NDT, particularly in comparison to that existing for stress-strain and electromagnetic areas. This paper describes new software for simulating the full (longitudinal and shear) solution to the three-dimensional (3D) axisymmetric wave equation. The simulation software is able to model both liquids and solids, and also accounts for losses using a classic viscoelastic model. The program computes the solution using a finite difference time domain algorithm, and evaluates the displacement vector at each (discrete grid) point of the object. Sources and receivers may be placed anywhere in or on the object, which is assumed to be cylindrical. A comparison of the on-axis diffraction pattern results obtained with the simulation software show excellent agreement with analytic results. Results using a cladded rod are also presented. This software should help to broaden the use of computational methods in ultrasonic NDT and in ultrasonics in general.",,POD:0-7803-7922-5,10.1109/ULTSYM.2003.1293325,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1293325,,Computational modeling;Elasticity;Finite difference methods;Grid computing;Liquids;Nondestructive testing;Partial differential equations;Solid modeling;Ultrasonic imaging;Viscosity,digital simulation;finite difference time-domain analysis;modelling;software packages;ultrasonic materials testing;ultrasonics;wave equations,3D-axisymmetric models;classic viscoelastic model;displacement vector;finite difference time domain algorithm;three-dimensional axisymmetric wave equation;ultrasonic NDT;ultrasound nondestructive evaluation testing;ultrasound simulation,,1,,5,,no,5-8 Oct. 2003,,IEEE,IEEE Conference Publications
"Usage of OLAP-means of the system ""analytics"" for the problem of health protection",Y. V. Dudina; P. R. Ishenin,"Institute of Calculus Mathematics, Krasnoyarsk, Russia","Proceedings of the 9th International Scientific and Practical Conference of Students, Post-graduates Modern Techniques and Technologies, 2003. MTT 2003.",20050613,2003,,,150,152,"The health care sector has been experiencing some difficulty as a result of the implementation of some new principles in planning and management and the continuous accumulation of electronic information. To help ease the situation, the use of an analytical modeling software, called OLAP (online analytical processing), is suggested. With the aid of OLAP, the capability of business analysis is significantly improved in terms of increased operation speed, presentation of the results and operative design of analytical reports.",,POD:0-7803-7669-2,10.1109/SPCMTT.2003.1438173,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1438173,,Analytical models;Calculus;Data analysis;Databases;Electrons;Information analysis;Insurance;Mathematics;Protection;State estimation,data mining;health care;information systems,Analytics system;OLAP;electron information;health protection management;health protection problems;information systems;online analytical processing,,0,2,3,,no,7-11 April 2003,,IEEE,IEEE Conference Publications
Web tool opens up power system visualization,Fangxing Li,,IEEE Power and Energy Magazine,20031219,2003,1,4,37,41,"This article presents a flexible architecture of a Web-based tool that serves as a platform for power system visualization with open data structures. Different from regular applications, this platform may be viewed as a semicompleted application. It implements common features of a power system application such as GIS-like drawing and visualization, built-in topology processor, and so on. Meanwhile, the platform defines open data structures for system components with the consideration of efficiency and flexibility. It also provides a mechanism to link itself with external engines, instead of implementing the engines directly. This architecture may achieve considerable flexibility. For instance, users may extend and customize data structures of power system components. Users may also develop their own analytic engines based on their specific needs and link them with the platform. To maximize the benefits to users, the proposed platform is Web-enabled with Java client-side technology. It is universally accessible, instantaneously upgradable, and operating-system independent.",1540-7977;15407977,,10.1109/MPAE.2003.1213525,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1213525,,Application software;Data structures;Data visualization;Engines;Graphics;Logic;Performance analysis;Power system analysis computing;Power systems;User interfaces,Java;client-server systems;data structures;data visualisation;graphical user interfaces;information resources;power system analysis computing,GIS-like drawing;Java client-side technology;Web-based tool;built-in topology processor;external engines link;open data structures;power system visualization,,2,,5,,no,Jul-Aug 2003,,IEEE,IEEE Journals & Magazines
Worst cases and lattice reduction,D. Stehle; L. Lefevre; P. Zimmermann,"Ecole Normale Superieure, Paris, France",Proceedings 2003 16th IEEE Symposium on Computer Arithmetic,20030709,2003,,,142,147,"We propose a new algorithm to find worst cases for correct rounding of an analytic function. We first reduce this problem to the real small value problem - i.e. for polynomials with real coefficients. Then we show that this second problem can be solved efficiently, by extending Coppersmith's work on the integer small value problem - for polynomials with integer coefficients - using lattice reduction (D. Coppersmith, 1996; 2001). For floating-point numbers with a mantissa less than N, and a polynomial approximation of degree d, our algorithm finds all worst cases at distance < N<sup>-d2</sup>/(2d+1) from a machine number in time O(N<sup>((d+1)</sup>(2d+1))+ëµ/). For d=2, this improves on the O(N<sup>2</sup>(3+ëµ)/) complexity from Lefevre's algorithm (V. Lefevre, 2000; V. Lefevre et al., 2001) to O(N<sup>3</sup>(5+ëµ)/). We exhibit some new worst cases found using our algorithm, for double-extended and quadruple precision. For larger d, our algorithm can be used to check that there exist no worst cases at distance < N<sup>-k</sup> in time O(N<sup>(1</sup>2)+O(1/k)/).",1063-6889;10636889,POD:0-7695-1894-X,10.1109/ARITH.2003.1207672,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1207672,,Algorithm design and analysis;Approximation algorithms;Computer aided software engineering;Floating-point arithmetic;Lattices;Libraries;Polynomials;Proposals;Search methods;Standards Board,computational complexity;floating point arithmetic;functional analysis;polynomial approximation;roundoff errors,analytic function;double-extended precision;floating-point number;integer small value problem;lattice reduction;polynomial approximation;quadruple precision;real small value problem;worst case detection,,6,,22,,no,15-18 June 2003,,IEEE,IEEE Conference Publications
A case study of organizational effects in a distributed sensor network,B. Horling; R. Mailler; V. Lesser,"Multiagent Syst. Lab., Massachusetts Univ., Amherst, MA, USA","Proceedings. IEEE/WIC/ACM International Conference on Intelligent Agent Technology, 2004. (IAT 2004).",20041018,2004,,,51,57,"We describe how a system employing different types of organizational techniques addresses the challenges posed by a large-scale distributed sensor network environment. The high-level multiagent architecture of real-world system is given in detail, and empirical and analytic results are provided showing the various effects that organizational characteristics have on the system's performance. We show how partitioning of the environment can lead to better locality and more constrained communication, as well as disproportionate load on individuals or increased load on the population as a whole. The presence of such tradeoffs motivates the need for a better understanding of organizational effects.",,POD:0-7695-2101-0,10.1109/IAT.2004.1342923,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1342923,,Computer aided software engineering;Government;Intelligent networks;Large-scale systems;Multiagent systems;Particle measurements;Radar tracking;Sensor systems;Target tracking;Time measurement,distributed sensors;multi-agent systems;organisational aspects,high-level multiagent architecture;large-scale distributed sensor network;organizational effects;organizational techniques,,8,,6,,no,24-24 Sept. 2004,,IEEE,IEEE Conference Publications
A new software tool to model measured RF-data with optimum circuit topology,B. Siddik Yarman; A. Kilinc; A. Aksen,"Dept. of Electron. Eng., Isik Univ., Istanbul, Turkey",2004 IEEE International Symposium on Circuits and Systems (IEEE Cat. No.04CH37512),20040903,2004,1,,I,980-3 Vol.1,"In this paper a new S/W tool is presented to model measured RF data employing immitance interpolation techniques. This S/W tool also employs a recently developed sub-routine, which generates circuit models with least number of circuit elements by means of a numerical approach. Furthermore, an analytic procedure is introduced and implemented within the new S/W package to select proper sample-points subject to interpolation error in the ""near min-max sense"" or the so-called ""Chebysev sense"". Hence optimum circuit topology for the given data is constructed. An example is presented to exhibit the utilization of the new tool. This new S/W package may be utilized as the front end to the commercially available design and analysis package such as ANSOFT, EAGLEWARE etc.",,POD:0-7803-8251-X,10.1109/ISCAS.2004.1328361,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1328361,,Circuit topology;Curve fitting;Error correction;Filtering theory;Interpolation;Packaging;Polynomials;Radio frequency;Software measurement;Software tools,circuit analysis computing;electric immittance;integrated circuit modelling;interpolation;software tools,ANSOFT;Chebysev sense;EAGLEWARE;RF-data;circuit elements;circuit models;circuit topology;immitance interpolation;interpolation error;near min-max case;numerical approach;sample-points subject;software tool,,0,,6,,no,23-26 May 2004,,IEEE,IEEE Conference Publications
Active performance management in supply chains,Changrui Ren; Yueting Chai; Yi Liu,"Dept. of Autom., Tsinghua Univ., Beijing, China","2004 IEEE International Conference on Systems, Man and Cybernetics (IEEE Cat. No.04CH37583)",20050307,2004,7,,6036,6041 vol.7,"The importance of performance management in supply chains has long been recognized from a variety of functional disciplines. But much of the work has focused on designing performance measures with less concern for the other stages of the entire performance management process. In this paper, an integrated, efficient and effective performance management system, ""active performance management system"", is presented. The system covers the entire performance management process including measures design, analysis, and dynamic update. The analysis of performance measures using causal loop diagrams, qualitative inference and analytic network process is mainly discussed in this paper. A real world case study is carried out throughout the paper to explain how the framework works. A software tool, supply chain performance analyzer, is also introduced. Some related topics and further research directions are discussed as concluding remarks.",1062-922X;1062922X,POD:0-7803-8566-7,10.1109/ICSMC.2004.1401345,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1401345,,Automation;Collaborative work;Continuous improvement;Decision making;Measurement;Performance analysis;Process design;Software tools;Supply chain management;Supply chains,common-sense reasoning;supply chain management,active performance management system;analytic network process;causal loop diagrams;performance measures;qualitative inference;real world case study;software tool;supply chain performance analyzer;supply chains,,1,,16,,no,10-13 Oct. 2004,,IEEE,IEEE Conference Publications
"An analysis on the process from ""play"" to ""learning""",T. Kotani; M. Kaburagi; M. Kang; H. Murao,"Graduate Sch. of Sci. & Technol., Kobe Univ., Japan","Information Technology Based Proceedings of the FIfth International Conference onHigher Education and Training, 2004. ITHET 2004.",20041122,2004,,,231,234,"It has been recognized especially in preschool that ""play"" is very useful for educating children since early times. Infants play just for enjoyment and learn many things such as scientific concept, communication and physical function through play. Many schoolteachers and educators had tried to make great efforts to apply ""play"" to schooling. To date many educational practices have been reported. But the mechanism of the transition process from ""play"" to ""learning"" as well as important factors for it has not been made apparent yet. We introduce an agent-model with several parameters, based on the probabilistic game theory and obtain analytic solution. The result shows that our model exhibits the existence of the mode transition from play-mode to learning-mode in a certain parameter regime.",,POD:0-7803-8596-9,10.1109/ITHET.2004.1358169,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1358169,,Educational institutions;Game theory;Pediatrics;Physics;Psychology,education;game theory;software agents,agent-model;analytic solution;children;educational practices;playing-learning mode transition;probabilistic game theory;transition process,,2,,12,,no,31 May-2 June 2004,,IEEE,IEEE Conference Publications
An evaluation of various analytic reconstruction algorithms and implementations for 2D and 3D PET,K. Dinelle; K. Thielemans; C. Tsoumpas; T. J. Spinks,"Hammersmith Imanet, London",IEEE Symposium Conference Record Nuclear Science 2004.,20050801,2004,7,,4043,4047,"Effects of the application, and implementation, of various reconstruction algorithms to 2D and 3D PET data were studied. Results were compared based on the performance of each method when subjected to the NEMA performance tests for resolution, scatter correction accuracy, and uniformity. An independent measurement of the coefficient of variation for a uniform cylinder was also made. Resulting data was reconstructed using each of the ECAT 7.1, STIR, and ECAT 7.2 software packages. Two implementations of the STIR software were investigated, STIR-INT1 which backprojects the original sinograms, and STIR-INT2 which first interpolates the sinograms to have equal bin and voxel sizes. Behaviour of the spatial resolution and CV in terms of varying pixel size was attributed to both the method of backprojector implementation and the reconstruction algorithm (FORE vs. 3DRP). An understanding of the method for reconstruction is important in understanding image quality, and selecting a pixel size for clinical use",1082-3654;10823654,Electronic:0-7803-8701-5; POD:0-7803-8700-7,10.1109/NSSMIC.2004.1466782,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1466782,,Algorithm design and analysis;Image quality;Image reconstruction;Pixel;Positron emission tomography;Reconstruction algorithms;Scattering;Software packages;Spatial resolution;Testing,image reconstruction;medical image processing;positron emission tomography;software packages,2D PET;3D PET;ECAT 7.1 software packages;ECAT 7.2 software packages;NEMA performance tests;STIR-INT1 software packages;STIR-INT2 software packages;backprojector implementation;clinical use;image quality;pixel size;reconstruction algorithms;sinograms;spatial resolution,,1,,8,,no,16-22 Oct. 2004,,IEEE,IEEE Conference Publications
An improved AHP method in performance assessment,Yidan Bao; Yanping Wu; Yong He; Xiaofeng Ge,"Coll. of Biosyst. Eng. & Food Sci., Zhejiang Univ., Hangzhou, China",Fifth World Congress on Intelligent Control and Automation (IEEE Cat. No.04EX788),20041018,2004,1,,177,180 Vol.1,"In order to reduce subjective errors in the traditional analytic hierarchy process (AHP), an improved AHP method integrated with orthogonal experimental design was presented in this paper. The new method combined both AHP and orthogonal design principles to make decisions, objectives in assessment system and values computed through mapping and satisfying function respectively as the corresponding factors, levels and results in orthogonal experimental design. With software SAS analysis, priorities on performance were to be ranked out accordingly. Application has showed that the method can improve the accuracy of AHP in performance assessment.",,POD:0-7803-8273-0,10.1109/WCICA.2004.1340551,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1340551,,Analysis of variance;Decision making;Design engineering;Design for experiments;Educational institutions;Helium;Performance analysis;Software performance;Synthetic aperture sonar;Testing,decision making;decision theory;design of experiments;operations research,AHP method;SAS software analysis;analytic hierarchy process;decision making;error reduction;orthogonal design principles;orthogonal experimental design;performance assessment system,,1,,6,,no,15-19 June 2004,,IEEE,IEEE Conference Publications
BRIX: meeting the requirements for online second language learning,M. Sawatpanit; D. Suthers; S. Fleming,"eSpherical Inc., Washington, DC, USA","37th Annual Hawaii International Conference on System Sciences, 2004. Proceedings of the",20040226,2004,,,10 pp.,,"This paper describes the design and evaluation of BRIX, an environment for online learning of second languages. A needs analysis identified specific requirements of online language learning. Commercial course management systems were determined to be inadequate with respect to these requirements. BRIX was developed to address the need for a generic language learning environment that fulfils language educators' requirements focusing on reading, writing, and listening activities and can easily be customized for different language courses. The design of BRIX is based on pedagogic approaches and theories of teaching and learning second languages and on the results of analytic and empirical evaluation of test versions of the software. In this paper, we describe the needs analysis and the design of BRIX, and present an evaluation that compares the use and usability of a Chinese course in BRIX to a previous handcrafted version of the same course.",,POD:0-7695-2056-1,10.1109/HICSS.2004.1265047,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1265047,,Distance learning;Diversity reception;Education;Internet;Natural languages;Programming profession;Software testing;Usability;World Wide Web;Writing,distance learning;educational courses,BRIX;Chinese course;course management;language courses;language learning environment;online learning;second language learning,,1,,20,,no,5-8 Jan. 2004,,IEEE,IEEE Conference Publications
Browsers to support awareness and social interaction,A. Lee; A. Girgensohn; J. Zhang,"IBM Thomas J. Watson Res. Center, NY, USA",IEEE Computer Graphics and Applications,20041004,2004,24,5,66,75,"Information sharing and social interaction are the Web's main features that have enabled online communities to abound and flourish. However, the Web is lacking cues and browsing mechanisms for the online social spaces. The challenge of creating social browsing tools to access such social information and patterns is of interest as a visual analytic problem for two reasons. Browsers that combine social visualizations and tools let newcomers and visitors explore information and patterns. Here we present social browsers for two Web communities. In addition to the novel visualizations and representation of two facets of a group's identity, our work has three other notable contributions.",0272-1716;02721716,,10.1109/MCG.2004.24,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1333629,,Animation;Collaborative work;Computerized monitoring;Context awareness;Data visualization;Distributed computing;Ecosystems;Humans;Visual analytics;Web page design,Web design;data visualisation;online front-ends;social aspects of automation;user interfaces,Web browsers;information sharing;social awareness;social interaction;social visualizations;visual analytic problem,"Awareness;Computer Graphics;Database Management Systems;Databases, Factual;Information Storage and Retrieval;Internet;Interpersonal Relations;Online Systems;Software;User-Computer Interface",6,9,20,,no,Sept.-Oct. 2004,,IEEE,IEEE Journals & Magazines
Can CEM software ever be validated?,C. S. Biddlecombe; C. P. Riley; C. R. I. Emson,"Vector Fields Ltd., Oxford, UK",IEE Seminar on Validation of Computational Electromagnetics,20051003,2004,,,1,4,"Validation of computational electromagnetics software has been a key topic for code developers for many years. The question arises: can software ever be validated meaningfully? Two cases are examined. Case 1: an electromagnetic code has been written to solve Maxwell's equations using the finite element method, including effects of eddy current losses and displacement currents (i.e. the full set of equations). The software is compared against a number of test problems, each having an analytic solution. It is shown that any required level of accuracy can be achieved by refining the mesh. Is this software therefore fully tested? Case 2: an electromagnetic code has been written to solve a subset of Maxwell's equations, neglecting displacement currents. It is used to model an induction heater and comparison is made with measured results. The results are found to be close to measurement but there are some differences. Is this software therefore in error? This paper discusses these two approaches to software validation in more detail and makes some recommendations.",0537-9989;05379989,,10.1049/ic:20040102,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1514247,,,Maxwell equations;computational electromagnetics;eddy current losses;formal verification;induction heating;mesh generation,CEM software;Maxwell equations;accuracy;computational electromagnetics;displacement currents;eddy current losses;electromagnetic code;finite element method;induction heater;mesh refinement;validation,,0,,,,no,30-Mar-04,,IET,IET Conference Publications
COTS acquisition process: incorporating business factors in COTS vendor evaluation taxonomy,H. C. Yeoh; J. Miller,"Electr. & Comput. Eng., Alberta Univ., Edmonton, Alta., Canada","10th International Symposium on Software Metrics, 2004. Proceedings.",20041122,2004,,,84,95,"The increasingly prevalent use of COTS components has attracted a huge capital pool to the industry. The result is an industry that is characterized by strong change forces and weak resistance. Under such environment, weaker players are constantly replaced by stronger players, and older technologies are constantly replaced by emerging technologies. This phenomenon has brought about a new class of risk to the COTS acquirers. These risk factors include the vendor's financial stability and technology capability. However, the existing COTS vendor evaluation taxonomies remain product centric, focusing only on product functionality and costs. We extend the taxonomies to incorporate business factors in the vendor evaluation process, and the resulting process is called VERPRO. The VERPRO decision making tool, which is based on the analytic hierarchy process, allows the acquirers to incorporate vendor business factors into the selection criteria.",1530-1435;15301435,POD:0-7695-2129-0,10.1109/METRIC.2004.1357893,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357893,,Application software;Business;Computer industry;Cost function;Decision making;Electric resistance;Hardware;Software performance;Stability;Taxonomy,DP industry;decision making;risk management;software cost estimation;software development management;software packages,COTS;analytic hierarchy process;business process;commercial-off-the-shelf;decision making;risk management;vendor evaluation process,,0,,33,,no,14-16 Sept. 2004,,IEEE,IEEE Conference Publications
Detecting flaws and intruders with visual data analysis,Soon Tee Teoh; Kwan-Liu Ma; S. F. Wu; T. J. Jankun-Kelly,"California Univ., Davis, CA, USA",IEEE Computer Graphics and Applications,20041004,2004,24,5,27,35,"The task of sifting through large amounts of data to find useful information spawned the field of data mining. Most data mining approaches are based on machine-learning techniques, numerical analysis, or statistical modeling. They use human interaction and visualization only minimally. Such automatic methods can miss some important features of the data. Incorporating human perception into the data mining process through interactive visualization can help us better understand the complex behaviors of computer network systems. This article describes visual-analytics-based solutions and outlines a visual exploration process for log analysis. Three log-file analysis applications demonstrate our approach's effectiveness in discovering flaws and intruders in network systems.",0272-1716;02721716,,10.1109/MCG.2004.26,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1333625,Internet routing stability;information visualization;intrusion detection;network visualization;visual data mining,Application software;Computer networks;Data analysis;Data mining;Data security;Data visualization;Humans;Intrusion detection;Performance analysis;Visual analytics,data analysis;data mining;data visualisation;security of data,data mining;flaws detection;human interaction;intruders;log-file analysis;machine-learning;visual data analysis,"Computer Communication Networks;Computer Graphics;Computer Security;Database Management Systems;Databases, Factual;Information Storage and Retrieval;Online Systems;Software;User-Computer Interface",29,,22,,no,Sept.-Oct. 2004,,IEEE,IEEE Journals & Magazines
Exact analysis of a class of GI/G/1-type performability models,A. Riska; E. Smirni; G. Ciardo,"Seagate Res., Pittsburgh, PA, USA",IEEE Transactions on Reliability,20040628,2004,53,2,238,249,"We present an exact decomposition algorithm for the analysis of Markov chains with a GI/G/1-type repetitive structure. Such processes exhibit both M/G/1-type & GI/M/1-type patterns, and cannot be solved using existing techniques. Markov chains with a GI/G/1 pattern result when modeling open systems which accept jobs from multiple exogenous sources, and are subject to failures & repairs; a single failure can empty the system of jobs, while a single batch arrival can add many jobs to the system. Our method provides exact computation of the stationary probabilities, which can then be used to obtain performance measures such as the average queue length or any of its higher moments, as well as the probability of the system being in various failure states, thus performability measures. We formulate the conditions under which our approach is applicable, and illustrate it via the performability analysis of a parallel computer system.",0018-9529;00189529,,10.1109/TR.2004.829134,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1308668,GI/G/1;GI/M/1;M/G/1;Markov chains;matrix analytic techniques;performability;stochastic complementation,Algorithm design and analysis;Computer science;Hardware;High performance computing;Length measurement;Open systems;Performance analysis;Performance evaluation;Risk analysis;Software quality,Markov processes;computer network reliability;matrix decomposition;open systems;parallel processing;queueing theory,GI/G/1;GI/M/1;M/G/1;Markov chain;average queue length;exact decomposition algorithm;matrix analytic techniques;multiple exogenous source;open system;parallel computer system;performability;single failure;stationary probability;stochastic complementation,,3,,24,,no,4-Jun,,IEEE,IEEE Journals & Magazines
Finding trading patterns in stock market data,K. V. Nesbitt; S. Barrass,"Charles Sturt Univ., Bathurst, NSW, Australia",IEEE Computer Graphics and Applications,20041004,2004,24,5,45,55,"This article describes our design and evaluation of a multisensory human perceptual tool for the real-world task domain of stock market trading. The tool is complementary in that it displays different information to different senses - our design incorporates both a 3D visual and a 2D sound display. The results of evaluating the tool in a formal experiment are complex. The data mined in this case study is bid-and-ask data - also called depth-of-market data - from the Australian Stock Exchange. Our visual-auditory display is the bid-ask-land-scape, which we developed over much iteration with the close collaboration of an expert in the stock market domain. From this domain's perspective, the project's principal goal was to develop a tool to help traders uncover new trading patterns in depth-of-market data. In this article, we not only describe the design of the bid-ask-landscape but also report on a formal evaluation of this visual-auditory display. We tested nonexperts on their ability to use the tool to predict the future direction of stock prices.",0272-1716;02721716,,10.1109/MCG.2004.28,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1333627,data mining;multisensory;sonification;stock market;visualization,Auditory displays;Bandwidth;Computer displays;Data mining;Humans;Stock markets;Testing;Three dimensional displays;Two dimensional displays;Visual analytics,auditory displays;data mining;electronic trading;share prices;stock markets,Australian Stock Exchange;bid-ask-landscape;formal evaluation;multisensory human perceptual tool;stock market data;trading patterns;visual-auditory display,"Australia;Commerce;Computer Graphics;Database Management Systems;Databases, Factual;Information Storage and Retrieval;Multimedia;Numerical Analysis, Computer-Assisted;Online Systems;Pattern Recognition, Automated;Software;User-Computer Interface",11,,12,,no,Sept.-Oct. 2004,,IEEE,IEEE Journals & Magazines
IEEE Computer Graphics and Applications,,,IEEE Computer Graphics and Applications,20040802,2004,24,1,3,3,,0272-1716;02721716,,10.1109/MCG.2004.1317926,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1317926,,Application software;Computer displays;Computer graphics;Data visualization;Haptic interfaces;Intelligent robots;Rendering (computer graphics);Robot sensing systems;Technological innovation;Visual analytics,,,,0,,,,no,Jan.-Feb. 2004,,IEEE,IEEE Journals & Magazines
Information retrieval with distributed databases: analytic models of performance,R. M. Losee; L. Church,"Sch. of Inf. & Libr. Sci., North Carolina Univ., Chapel Hill, NC, USA",IEEE Transactions on Parallel and Distributed Systems,20040219,2004,15,1,18,27,"The major emphasis is on analytical techniques for predicting the performance of various collection fusion scenarios. Knowledge of analytical models of information retrieval system performance, both with single processors and with multiple processors, increases our understanding of the parameters (e.g., number of documents, ranking algorithms, stemming algorithms, stop word lists, etc.) affecting system behavior. While there is a growing literature on the implementation of distributed information retrieval systems and digital libraries, little research has focused on analytic models of performance. We analytically describe the performance for single and multiple processors, both when different processors have the same parameter values and when they have different values. The use of different ranking algorithms and parameter values at different sites is examined.",1045-9219;10459219,,10.1109/TPDS.2004.1264782,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1264782,,Analytical models;Data analysis;Distributed databases;Information analysis;Information retrieval;Metasearch;Performance analysis;Predictive models;Search engines;Software libraries,digital libraries;distributed databases;information retrieval system evaluation;search engines,analytical performance model;collection fusion;digital libraries;distributed databases;distributed information retrieval system;metasearch engine,,4,,55,,no,Jan. 2004,,IEEE,IEEE Journals & Magazines
"More ""normal"" than normal: scaling distributions and complex systems",W. Willinger; D. Alderson; J. C. Doyle; L. Li,"Inf. & Software Syst. Res. Center, AT&T Labs.-Res., Florham Park, NJ, USA","Proceedings of the 2004 Winter Simulation Conference, 2004.",20050103,2004,1,,,141,"One feature of many naturally occurring or engineered complex systems is tremendous variability in event sizes. To account for it, the behavior of these systems is often described using power law relationships or scaling distributions, which tend to be viewed as ""exotic"" because of their unusual properties (e.g., infinite moments). An alternate view is based on mathematical, statistical, and data-analytic arguments and suggests that scaling distributions should be viewed as ""more normal than normal"". In support of this latter view that has been advocated by Mandelbrot for the last 40 years, we review in this paper some relevant results from probability theory and illustrate a powerful statistical approach for deciding whether the variability associated with observed event sizes is consistent with an underlying Gaussian-type (finite variance) or scaling-type (infinite variance) distribution. We contrast this approach with traditional model fitting techniques and discuss its implications for future modeling of complex systems.",,POD:0-7803-8786-4,10.1109/WSC.2004.1371310,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1371310,,Control systems;Frequency;Gaussian distribution;IP networks;Inspection;Microorganisms;Power engineering and energy;Power system modeling;Probability;Systems engineering and theory,large-scale systems;modelling;probability;statistical analysis;telecommunication traffic,Gaussian-type distribution;complex systems;data-analytic arguments;model fitting techniques;power law relationships;probability theory;scaling distributions;scaling-type distribution;statistical arguments,,7,,36,,no,5-8 Dec. 2004,,IEEE,IEEE Conference Publications
Object dependency of resolution in reconstruction algorithms with interiteration filtering applied to PET data,S. Mustafovic; K. Thielemans,"Hammersmith Hosp., Hammersmith Imanet Ltd., London, UK",IEEE Transactions on Medical Imaging,20040405,2004,23,4,433,446,"In this paper, we study the resolution properties of those algorithms where a filtering step is applied after every iteration. As concrete examples we take filtered preconditioned gradient descent algorithms for the Poisson log likelihood for PET emission data. For nonlinear estimators, resolution can be characterized in terms of the linearized local impulse response (LLIR). We provide analytic approximations for the LLIR for the class of algorithms mentioned above. Our expressions clearly show that when interiteration filtering (with linear filters) is used, the resolution properties are, in most cases, spatially varying, object dependent and asymmetric. These nonuniformities are solely due to the interaction between the filtering step and the Poisson noise model. This situation is similar to penalized likelihood reconstructions as studied previously in the literature. In contrast, nonregularized and postfiltered maximum-likelihood expectation maximization (MLEM) produce images with nearly ""perfect"" uniform resolution when convergence is reached. We use the analytic expressions for the LLIR to propose three different approaches to obtain nearly object independent and uniform resolution. Two of them are based on calculating filter coefficients on a pixel basis, whereas the third one chooses an appropriate preconditioner. These three approaches are tested on simulated data for the filtered MLEM algorithm or the filtered separable paraboloidal surrogates algorithm. The evaluation confirms that images obtained using our proposed regularization methods have nearly object independent and uniform resolution.",0278-0062;02780062,,10.1109/TMI.2004.824225,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1281997,,Algorithm design and analysis;Concrete;Filtering algorithms;Image reconstruction;Image resolution;Maximum likelihood estimation;Nonlinear filters;Positron emission tomography;Reconstruction algorithms;Spatial resolution,adaptive filters;filtering theory;image reconstruction;image resolution;iterative methods;medical image processing;positron emission tomography;transient response,PET;Poisson log likelihood;Poisson noise model;convergence;filtered preconditioned gradient descent algorithms;image resolution;interiteration filtering;linear filters;linearized local impulse response;maximum-likelihood expectation maximization;object dependency;penalized likelihood reconstructions;positron emission tomography;reconstruction algorithms;regularization methods;spatially adaptive filter,"Algorithms;Computer Simulation;Feedback;Image Enhancement;Image Interpretation, Computer-Assisted;Models, Biological;Reproducibility of Results;Sensitivity and Specificity;Signal Processing, Computer-Assisted;Software Validation;Tomography, Emission-Computed",11,2,27,,no,4-Apr,,IEEE,IEEE Journals & Magazines
Optimal object state transfer - recovery policies for fault tolerant distributed systems,P. Katsaros; C. Lazos,"Dept. of Informatics, Aristotle Univ. of Thessaloniki, Greece","International Conference on Dependable Systems and Networks, 2004",20040726,2004,,,762,771,"Recent developments in the field of object-based fault tolerance and the advent of the first OMG FT-CORBA compliant middleware raise new requirements for the design process of distributed fault-tolerant systems. In this work, we introduce a simulation-based design approach based on the optimum effectiveness of the compared fault tolerance schemes. Each scheme is defined as a set of fault tolerance properties for the objects that compose the system. Its optimum effectiveness is determined by the tightest effective checkpoint intervals, for the passively replicated objects. Our approach allows mixing miscellaneous fault tolerance policies, as opposed to the published analytic models, which are best suited in the evaluation of single-server process replication schemes. Special emphasis has been given to the accuracy of the generated estimates using an appropriate simulation output analysis procedure. We provide showcase results and compare two characteristic warm passive replication schemes: one with periodic and another one with load-dependent object state checkpoints. Finally, a trade-off analysis is applied, for determining appropriate checkpoint properties, in respect to a specified design goal.",,POD:0-7695-2052-9,10.1109/DSN.2004.1311947,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1311947,,Analytical models;Application software;Delay;Fault tolerance;Fault tolerant systems;Informatics;Middleware;Process design;Robustness;Software systems,checkpointing;distributed object management;middleware;software fault tolerance,OMG FT-CORBA;checkpointing;fault tolerant distributed systems;middleware;object replication;object state checkpoints;object-based fault tolerance;optimal object state transfer;single-server process replication;trade-off analysis,,1,,20,,no,28 June-1 July 2004,,IEEE,IEEE Conference Publications
Performance evaluation and failure rate prediction for the soft implemented error detection technique,B. Nicolescu; Y. Savaria; R. Velazco,"Ecole Polytech. de Montreal, Que., Canada",Proceedings. 10th IEEE International On-Line Testing Symposium,20040809,2004,,,233,238,"This paper presents two error models to evaluate safety of a software error detection method. The proposed models analyze the impact on program overhead in terms of memory code area and increased execution time when the studied error detection technique is applied. For faults affecting the processor's registers, analytic formulas are derived to estimate the failure rate before program execution. These formulas are based on probabilistic methods and use statistics of the program, which are collected during compilation. The studied error detection technique was applied to several benchmark programs and then program overhead and failure rate was estimated. Experimental results validate the estimated performances and show the effectiveness of the proposed evaluation formulas.",,POD:0-7695-2180-0,10.1109/OLT.2004.1319693,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1319693,,Algorithm design and analysis;Application software;Electromagnetic transients;Failure analysis;Fault detection;Performance analysis;Performance evaluation;Predictive models;Single event transient;Software safety,avionics;benchmark testing;error detection;logic testing;radiation hardening (electronics);redundancy;safety-critical software;software fault tolerance;software performance evaluation;space vehicle electronics,benchmark programs;dependability;error models;failure rate prediction;fault tolerance;hardening function;increased execution time;memory code area;performance evaluation;probabilistic methods;program complexity;program overhead;redundancy;safety-critical systems;signature analysis;single event upsets;soft implemented error detection,,2,,10,,no,12-14 July 2004,,IEEE,IEEE Conference Publications
Power laws for monkeys typing randomly: the case of unequal probabilities,B. Conrad; M. Mitzenmacher,"Dept. of Math., Univ. of Michigan, Ann Arbor, MI, USA",IEEE Transactions on Information Theory,20040621,2004,50,7,1403,1414,"An early result in the history of power laws, due to Miller, concerned the following experiment. A monkey types randomly on a keyboard with N letters (N>1) and a space bar, where a space separates words. A space is hit with probability p; all other letters are hit with equal probability (1-p)/N. Miller proved that in this experiment, the rank-frequency distribution of words follows a power law. The case where letters are hit with unequal probability has been the subject of recent confusion, with some suggesting that in this case the rank-frequency distribution follows a lognormal distribution. We prove that the rank-frequency distribution follows a power law for assignments of probabilities that have rational log-ratios for any pair of keys, and we present an argument of Montgomery that settles the remaining cases, also yielding a power law. The key to both arguments is the use of complex analysis. The method of proof produces simple explicit formulas for the coefficient in the power law in cases with rational log-ratios for the assigned probabilities of keys. Our formula in these cases suggests an exact asymptotic formula in the cases with an irrational log-ratio, and this formula is exactly what was proved by Montgomery.",0018-9448;00189448,,10.1109/TIT.2004.830752,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1306541,,Computer aided software engineering;Frequency;History;Information analysis;Information theory;Internet;Keyboards;Mathematics;Natural languages;Psychology,information theory;number theory;probability;rational functions,analytic information theory;analytic number theory;monkeys typing randomly;power laws;rank-frequency distribution;rational log-ratios;unequal probability,,7,,16,,no,4-Jul,,IEEE,IEEE Journals & Magazines
Reliability improvement of Web-based software applications,L. Davila-Nicanor; P. Mejia-Alvarez,"Seccion de Computacion, CINVESTAV-IPN, Zacatenco, Mexico","Fourth International Conference onQuality Software, 2004. QSIC 2004. Proceedings.",20041122,2004,,,180,188,"In diverse industrial and academic environments, the quality of the software has been evaluated (validated) using different analytic studies. It is a common practice on these environments to use statistical models for the assurance, control and evaluation of the quality of a software product or process. A number of industries in the safety-critical sector are forced nowadays to use such processes by industry-specific standards (e.g., the DO-178B standard for airborne software systems). The contribution of the present work is focused on the development of a methodology for the evaluation and analysis of the reliability of Web-based software applications. We tested our methodology in a Web-based software system and used statistical modeling theory for the analysis and evaluation of the reliability. The behavior of the system under ideal conditions was evaluated and compared against the operation of the system executing under real conditions. The personal software process (PSP) was introduced in our methodology for improving the quality of the process and the product. The evaluation + improvement (Ei) process is performed in our methodology to evaluate and improve the quality of the software system. The results obtained demonstrated the effectiveness and applicability of our methodology.",,POD:0-7695-2207-6,10.1109/QSIC.2004.1357959,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357959,,Application software;Computer industry;Electrical equipment industry;Performance evaluation;Reliability theory;Software quality;Software standards;Software systems;Software testing;System testing,Internet;software performance evaluation;software process improvement;software quality;software reliability;statistical analysis,Web-based software applications;Web-based software reliability;industry-specific standards;personal software process;reliability analysis;reliability evaluation;reliability improvement;safety-critical sector;software quality assurance;software quality control;software quality evaluation;software system quality;statistical modeling theory,,0,,17,,no,8-9 Sept. 2004,,IEEE,IEEE Conference Publications
Route throughput analysis for mobile multi-rate wireless ad hoc networks,Yu-Chee Tseng; Weikuo Chu; Lien-Wu Chen; Chih-Min Yu,"Dept. of Comput. Sci. & Inf. Eng., Nat. Chiao Tung Univ., Hsinchu, Taiwan",First International Conference on Broadband Networks,20041213,2004,,,469,475,"The mobile ad hoc networks (MANETs) have received a lot of attention recently. While many routing protocols have been proposed for MANETs based on different criteria, few have considered the impact of multi-rate communication capability that is supported by many current WLAN products. Given a routing path, this paper provides an analytic tool to evaluate the expected throughput of the route, assuming that hosts move following the discrete-time, random-walk model. The derived result can be added as another metric for route selection. Simulation results are also presented.",,POD:0-7695-2221-1,10.1109/BROADNETS.2004.74,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1363835,,Ad hoc networks;Communication system software;Computer science;Embedded software;Handheld computers;Mobile ad hoc networks;Mobile communication;Routing protocols;Throughput;Wireless LAN,ad hoc networks;mobile radio;routing protocols,discrete-time model;mobile multirate wireless ad hoc network;multirate communication;random-walk model;route throughput analysis;routing protocol,,4,,13,,no,25-29 Oct. 2004,,IEEE,IEEE Conference Publications
Software rejuvenation policies for cluster systems under varying workload,W. Xie; Y. Hong; K. S. Trivedi,"Dept. of Electr. & Comput. Eng., Duke Univ., Durham, NC, USA","10th IEEE Pacific Rim International Symposium on Dependable Computing, 2004. Proceedings.",20040330,2004,,,122,129,"We analyze two software rejuvenation policies of cluster server systems under varying workload, called fixed rejuvenation and delayed rejuvenation. In order to achieve a higher average throughput, we propose the delayed rejuvenation policy, which postpones the rejuvenation of individual nodes until off-peak hours. Analytic models using the well known paradigm of Markov chains are used. Since the size of the Markov model is nontrivial, automated specification generation, and the solution via stochastic Petri nets is utilized. Deterministic time to trigger rejuvenation is approximated by a 20-stage Erlangian distribution. Based on the numerical solutions of the models, we find that under the given context, although the fixed rejuvenation occasionally yields a higher throughput, the delayed rejuvenation policy seems to outperform fixed rejuvenation policy by up to 11%. We also compare the steady-state system availabilities of these two rejuvenation policies.",,POD:0-7695-2076-6,10.1109/PRDC.2004.1276563,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1276563,,Aging;Availability;Communication system software;Delay;Network servers;Petri nets;Quality of service;Software maintenance;Stochastic processes;Throughput,Markov processes;Petri nets;software fault tolerance;systems re-engineering,Erlangian distribution;Markov chains;automated specification generation;cluster server system;delayed rejuvenation policy;software rejuvenation policy;steady-state system;stochastic Petri nets,,3,,19,,no,3-5 March 2004,,IEEE,IEEE Conference Publications
The impact of free-riding on peer-to-peer networks,R. Krishnan; M. D. Smith; Zhulei Tang; R. Telang,"H. John Heinz III Sch. of Public Policy & Manage., Carnegie Mellon Univ., Pittsburgh, PA, USA","37th Annual Hawaii International Conference on System Sciences, 2004. Proceedings of the",20040226,2004,,,10 pp.,,"Peer-to-peer networking is gaining popularity as a architecture for sharing information goods and other computing resources. However, these networks suffer from a high level of free-riding, whereby some users consume network resources without providing any network resources. The high levels of free-riding observed by several recent studies have led some to suggest the imminent collapse of these communities as a viable information sharing mechanism. Our research develops analytic models to analyze the behavior of P2P networks in the presence of free-riding. In contrast to previous predictions, we find that P2P networks can operate effectively in the presence of significant free-riding. However, we also show that without external incentives, the level of free-riding in P2P networks is higher than socially optimal. Our research also explores the implications of these findings for entrepreneurs, network designers, and copyright holders.",,POD:0-7695-2056-1,10.1109/HICSS.2004.1265472,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1265472,,Application software;Cities and towns;Computer architecture;Computer industry;Computer network management;Computer networks;Economic forecasting;Peer to peer computing;Public policy;Resource management,information resources;peer-to-peer computing,P2P network;free-riding;information sharing;network resources;peer-to-peer network,,22,,43,,no,5-8 Jan. 2004,,IEEE,IEEE Conference Publications
The multi-agent rendezvous problem - the asynchronous case,J. Lin; A. S. Morse; B. D. O. Anderson,"Yale Univ., New Haven, CT, USA",2004 43rd IEEE Conference on Decision and Control (CDC) (IEEE Cat. No.04CH37601),20050516,2004,2,,1926,1931 Vol.2,"This paper is concerned with the collective behavior of a group of n > 1 mobile autonomous agents, labelled 1 through n, which can all move in the plane. Each agent is able to continuously track the positions of all other agents currently within its ""sensing region"" where by an agent's sensing region is meant a closed disk of positive radius r centered at the agent's current position. The multi-agent rendezvous problem is to devise ""local"" control strategies, one for each agent, which without any active communication between agents, cause all members of the group to eventually rendezvous at single unspecified location. This paper describes a family of asynchronously functioning strategies for solving the problem. Correctness is established appealing to the concept of ""analytic synchronization"".",0191-2216;01912216,POD:0-7803-8682-5,10.1109/CDC.2004.1430329,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1430329,,Australia Council;Autonomous agents;Communication system control;Computer aided software engineering;Distributed control;Vehicles,distributed control;mobile robots;multi-agent systems,active agent communication;analytic synchronization;collective group behavior;cooperative control;distributed control;local control strategies;mobile autonomous agents;multiagent rendezvous problem;position tracking;sensing region,,35,,3,,no,14-17 Dec. 2004,,IEEE,IEEE Conference Publications
The operation of the ITS architecture - a case study of public bus service in Taiwan,Jin-Yuan Wang; Chih-Kang Lin,"Transp. Res. Center, Nat. Chiao Tung Univ., Hsinchu, Taiwan","IEEE International Conference on Networking, Sensing and Control, 2004",20040927,2004,2,,1137,1142 Vol.2,"The paper presents an analytic approach to generate the market packages for the operation via the intelligent transportation system (ITS) architecture. This analytic approach uses the concepts of the ""use case diagram"" and ""sequence diagram"" defined by the unified modeling language (UML). By following the steps, we take Taiwan's public bus service as an example and generate the market package effectually, public bus monitoring and public bus scheduling and dispatching, which is corresponding to the real world situation. We also point out the academic models in market packages that could be the patent products. Consequently, driving the establishment of ITS market package not only conduces to ITS development but also promotes the knowledge economic in our society.",1810-7869;18107869,POD:0-7803-8193-9,10.1109/ICNSC.2004.1297107,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1297107,,Computer aided software engineering;Dispatching;Government;Intelligent systems;Intelligent transportation systems;Monitoring;Packaging;Road transportation;Technology planning;Unified modeling language,dispatching;monitoring;road traffic;scheduling;specification languages;traffic engineering computing;transportation;travel industry,ITS;UML;intelligent transportation system;knowledge economic;market package;public bus dispatching;public bus monitoring;public bus scheduling;public bus service;sequence diagram;unified modeling language;use case diagram,,0,,8,,no,2004,,IEEE,IEEE Conference Publications
Visual analytics in the pharmaceutical industry,J. D. Saffer; V. L. Burnett; Guang Chen; P. van der Spek,,IEEE Computer Graphics and Applications,20041004,2004,24,5,10,15,"With the flood of data across all aspects of the pharmaceutical industry, information visualization is emerging as a critical component of discovery, development, and business. But it's a new class of visualizations that is having the greatest impact. Higher-level summaries that can provide a framework for understanding the immense volumes of data and that reveal unexpected relationships have come to the forefront. And, the ability to use these visualizations to cross-domains and data types provides the ability to integrate analyses and support fast, effective decisions.",0272-1716;02721716,,10.1109/MCG.2004.40,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1333621,,Application software;Data visualization;Diseases;Gene expression;Information analysis;Navigation;Performance analysis;Pharmaceuticals;Proteins;Visual analytics,data visualisation;genetics;pharmaceutical industry,gene expression;information visualization;pharmaceutical industry;visual analytics,"Artificial Intelligence;Combinatorial Chemistry Techniques;Computer Graphics;Database Management Systems;Databases, Factual;Drug Delivery Systems;Drug Design;Drug Industry;Gene Expression Profiling;Information Storage and Retrieval;Online Systems;Protein Interaction Mapping;Toxicology;User-Computer Interface",7,,5,,no,Sept.-Oct. 2004,,IEEE,IEEE Journals & Magazines
A generic toolkit for multivariate fitting designed with template metaprogramming,F. Fabozzi; L. Lista,"INFNInst. Nazionale di Fisica Nucl.e, Univ. della Basilicata, Potenza, Italy",IEEE Transactions on Nuclear Science,20051205,2005,52,5,1654,1658,We present a toolkit developed to perform Unbinned Maximum Likelihood and Chi-square fits for parameters and yields estimates. The design of the toolkit is based on template metaprogramming in order to provide users with a generic interface to minimization tools. The default implementation is based on ROOT wrapper of MINUIT. Commonly used Probability Density Functions (PDFs) are provided with the toolkit and users may add custom PDFs for their own analyses. Description of fit models can be done using a package for symbolic manipulation of analytic functions.,0018-9499;00189499,,10.1109/TNS.2005.856753,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1546479,Object oriented programming;parameter estimation;software packages,Data analysis;Libraries;Maximum likelihood estimation;Monte Carlo methods;Packaging;Parameter estimation;Probability density function;Random number generation;Software packages;Yield estimation,mathematics computing;maximum likelihood estimation;object-oriented programming;parameter estimation;probability;software packages;statistical testing,Chi-square fits;ROOT wrapper;generic interface;generic toolkit;maximum likelihood estimation;object oriented programming;parameter estimation;probability density functions;software packages;template metaprogramming,,0,,8,,no,Oct. 2005,,IEEE,IEEE Journals & Magazines
A model-based framework: an approach for profit-driven optimization,M. Zhao; B. R. Childers; M. L. Soffa,"Pittsburgh Univ., PA, USA",International Symposium on Code Generation and Optimization,20050404,2005,,,317,327,"Although optimizations have been applied for a number of years to improve the performance of software, problems that have been long-standing remain, which include knowing what optimizations to apply and how to apply them. To systematically tackle these problems, we need to understand the properties of optimizations. In our current research, we are investigating the profitability property, which is useful for determining the benefit of applying an optimization. Due to the high cost of applying optimizations and then experimentally evaluating their profitability, we use an analytic model framework for predicting the profitability of optimizations. In this paper, we target scalar optimizations, and in particular, describe framework instances for partial redundancy elimination (PRE) and loop invariant code motion (LICM). We implemented the framework for both optimizations and compare profit-driven PRE and LICM with a heuristic-driven approach. Our experiments demonstrate that a model-based approach is effective and efficient in that it can accurately predict the profitability of optimizations with low overhead. By predicting the profitability using models, we can selectively apply optimizations. The model-based approach does not require tuning of parameters used in heuristic approaches and works well across different code contexts and optimizations.",,POD:0-7695-2298-X,10.1109/CGO.2005.2,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1402098,,Computer architecture;Context modeling;Cost function;Hardware;Optimizing compilers;Predictive models;Profitability;Software performance;Technological innovation;Tiles,optimising compilers;partial evaluation (compilers);program control structures;redundancy;software performance evaluation,analytic model framework;heuristic-driven approach;loop invariant code motion;model-based framework;partial redundancy elimination;profit-driven optimization;profitability property;software performance,,5,1,36,,no,20-23 March 2005,,IEEE,IEEE Conference Publications
A quantitative software quality evaluation model for the artifacts of component based development,Kilsup Lee; Sung Jong Lee,"Korea Nat. Defense Univ., South Korea","Sixth International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing and First ACIS International Workshop on Self-Assembling Wireless Network",20050606,2005,,,20,25,"Recently, software quality evaluation based on ISO/IEC 9126 and ISO/IEC 14598 has been used widely. However, these standards for software quality don't provide practical guidelines to apply the quality model and the evaluation process to real projects. Hence, this paper presents a quantitative software quality evaluation model for the artifacts of the component based development (CBD) methodology which is developed by the Ministry of National Defense of the Republic of Korea, Particularly, our model adopts the weights of quality characteristics which are obtained by carefully selected questionnaires for the stakeholders and analytic hierarchical process (AHP) technique. We also present the evaluation process using checklists and the result of a trial evaluation for validation of our model. As a result, we believe that the proposed model helps to acquire high quality software.",,POD:0-7695-2294-7,10.1109/SNPD-SAWN.2005.7,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1434862,,Costs;Guidelines;IEC standards;ISO standards;Proposals;Quality management;Software quality;Software standards;Standards publication;Testing,IEC standards;ISO standards;object-oriented programming;software performance evaluation;software quality;software standards,ISO-IEC 14598;ISO-IEC 9126;analytic hierarchical process;component-based development;software quality evaluation model;software standards,,4,,11,,no,23-25 May 2005,,IEEE,IEEE Conference Publications
A time-dependent Hamilton-Jacobi formulation of reachable sets for continuous dynamic games,I. M. Mitchell; A. M. Bayen; C. J. Tomlin,"Dept. of Comput. Sci., Univ. of British Columbia, Vancouver, BC, Canada",IEEE Transactions on Automatic Control,20050711,2005,50,7,947,957,"We describe and implement an algorithm for computing the set of reachable states of a continuous dynamic game. The algorithm is based on a proof that the reachable set is the zero sublevel set of the viscosity solution of a particular time-dependent Hamilton-Jacobi-Isaacs partial differential equation. While alternative techniques for computing the reachable set have been proposed, the differential game formulation allows treatment of nonlinear systems with inputs and uncertain parameters. Because the time-dependent equation's solution is continuous and defined throughout the state space, methods from the level set literature can be used to generate more accurate approximations than are possible for formulations with potentially discontinuous solutions. A numerical implementation of our formulation is described and has been released on the web. Its correctness is verified through a two vehicle, three dimensional collision avoidance example for which an analytic solution is available.",0018-9286;00189286,,10.1109/TAC.2005.851439,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1463302,Differential games;Hamilton‰ÛÒJacobi equations;reachability;verification,Aircraft;Collaborative software;Collision avoidance;Computational modeling;Nonlinear equations;Nonlinear systems;Partial differential equations;Trajectory;Vehicle dynamics;Viscosity,approximation theory;continuous time systems;game theory;nonlinear control systems;partial differential equations;reachability analysis;state-space methods,approximations;continuous dynamic games;differential game formulation;nonlinear systems;partial differential equation;reachable sets;state space;time-dependent Hamilton-Jacobi formulation;two-vehicle three-dimensional collision avoidance;uncertain parameters,,218,1,51,,no,5-Jul,,IEEE,IEEE Journals & Magazines
"Advanced system modeling for fast, iterative, fully 3D positron-emission-tomography reconstruction",J. Scheins; F. Boschen; H. Herzog,"Medicine Inst., Juelich, Germany","The Fourth International Workshop on Multidimensional Systems, 2005. NDS 2005.",20050919,2005,,,182,186,"Iterative algorithms for PET reconstruction, e.g. maximum-likelihood-expectation-maximization (MLEM), are well established and often yield superior image quality compared to conventional analytic reconstruction methods. Especially, the incorporation of detector blurring within the modeling of the scanner response can significantly improve the image quality. However, the blurring modeling is accompanied by a reduction of the rate of convergence. Therefore, the statistical accuracy of the acquired data limits the improvements depending on the specific system sampling. We have developed a fully 3D MLEM-reconstruction software for a Siemens/CTI HR+ PET scanner including an appropriate modeling. Here, the true parameters of the detector resolution appear not adequate for the modeling, because strong artefacts in the images are observed. Therefore, we have adapted the parameters of the detector resolution model so that the artefacts disappeared, but that the image resolution was still better than without using the detector resolution model.",,POD:3-9810299-8-4,10.1109/NDS.2005.195351,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1507853,,Algorithm design and analysis;Detectors;Image analysis;Image quality;Image reconstruction;Image resolution;Iterative algorithms;Modeling;Positron emission tomography;Reconstruction algorithms,image reconstruction;maximum likelihood estimation;positron emission tomography;stereo image processing,PET reconstruction;Siemens/CTI HR+ PET scanner;advanced system modeling;detector resolution model;fast iterative fully 3D positron-emission-tomography reconstruction;maximum-likelihood-expectation-maximization,,0,,4,,no,10-13 July 2005,,IEEE,IEEE Conference Publications
An LQG Optimal Linear Controller for Control Systems with Packet Losses,B. Sinopoli; L. Schenato; M. Franceschetti; K. Poolla; S. Sastry,"Department of Electrical Engineering, UC Berkeley, Berkeley, CA, USA. sinopoli@eecs.berkeley.edu",Proceedings of the 44th IEEE Conference on Decision and Control,20060130,2005,,,458,463,"Motivated by control applications over lossy packet networks, this paper considers the Linear Quadratic Gaussian (LQG) optimal control problem in the discrete time setting and when packet losses may occur between the sensors and the estimation-control unit and between the latter and the actuation points. Previous work [1] shows that, for protocols where packets are acknowledged at the receiver (e.g. TCP- like protocols), the separation principle holds. Moreover, in this case the optimal LQG control is a linear function of the estimated state and there exist critical probabilities for the successful delivery of both observation and control packets, below which the optimal controller fails to stabilize the system. The existence of such critical values is determined by providing analytic upper and lower bounds on the cost functional, and stochastically characterizing their convergence properties in the infinite horizon. Finally, it turns out that when there is no feedback on whether a control packet has been delivered or not (e.g. UDP-like protocols), the LQG optimal controller is in general nonlinear, as shown in [2]. There exists a special case, i.e. the observation matrix C is invertible and there is no output noise. In this case this paper shows that the optimal control is linear and critical values for arrival probabilities exist and can be computed analytically.",0191-2216;01912216,POD:0-7803-9567-0,10.1109/CDC.2005.1582198,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1582198,LQG control;distributed control;networked control;optimal stochastic control;sensor networks,Application software;Communication channels;Communication system control;Control systems;Cost function;Distributed control;Optimal control;Protocols;State estimation;Wireless sensor networks,,LQG control;distributed control;networked control;optimal stochastic control;sensor networks,,26,,19,,no,12-15 Dec. 2005,,IEEE,IEEE Conference Publications
Analytic approximations for multilayer substrate coplanar-plate capacitors,S. Gevorgian; S. Abadei,"Dept. of Microtechnology & Nanoscience, Chalmers Univ. of Technol., Gothenburg, Sweden","IEEE MTT-S International Microwave Symposium Digest, 2005.",20051031,2005,,,4 pp.,,"Closed form analytic approximations are proposed for complex impedance (capacitance and Q-factor) of capacitors formed by two coplanar rectangular conducting patches sandwiched between dielectric layers. The computations using proposed formulas are at least an order of magnitude faster in comparison with the commercially available software's, and can be used in optimisation procedures. The formulas are reversible, i.e. from measured impedance one can compute permittivity and losses of one of the dielectric layers.",0149-645X;0149645X,POD:0-7803-8845-3,10.1109/MWSYM.2005.1516957,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1516957,,Capacitance;Capacitors;Dielectric loss measurement;Dielectric measurements;Dielectric substrates;Impedance measurement;Loss measurement;Nonhomogeneous media;Permittivity measurement;Q factor,Q-factor;approximation theory;capacitance;conformal mapping;coplanar waveguides;dielectric materials;multilayers;rectangular waveguides,Q-factor;capacitance;closed form analytic approximations;conformal mapping;coplanar rectangular conducting patches;dielectric layers;multilayer substrate coplanar-plate capacitors,,3,,7,,no,12-17 June 2005,,IEEE,IEEE Conference Publications
Boundary Case of Pulse Propagation Analytic Solution in the Presence of Interference and Higher Order Dispersion,A. S. Panajotovic; D. M. Milovic; A. M. Mitic,"Faculty of Electronic Engineering, Aleksandra Medvedeva 14, 18000 Nis, Serbia and Montenegro, E-mail: alexa@elfak.ni.ac.yu","TELSIKS 2005 - 2005 uth International Conference on Telecommunication in ModernSatellite, Cable and Broadcasting Services",20060110,2005,2,,545,550,"Higher order dispersion is one of the most important limiting factors in high-speed optical transmission systems. In this paper the pulse propagation problem under the influence of higher order dispersion is considered by treating the fiber as a linear medium. The influence of interference is also considered and analytic solution of pulse propagation in the presence of interference and higher order dispersion is given. Interference is time and phase shifted in regard to useful signal. We consider the worst case, i.e. case when phase shift is pi",,POD:0-7803-9164-0,10.1109/TELSKS.2005.1572172,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1572172,Higher order chromatic dispersion;impulsive response;intersymbol interference,Computer aided software engineering;Interference,high-speed optical techniques;intersymbol interference;optical fibre communication;optical fibre dispersion,high-speed optical transmission systems;higher order dispersion;interference;pulse propagation analytic solution,,1,,11,,no,28-30 Sept. 2005,,IEEE,IEEE Conference Publications
Comparison of Software Reliability Assessment Methods for Open Source Software,Y. Tamura; S. Yamada,"Department of Information Systems Faculty of Environmental and Information Studies, Tottori University of Environmental Studies",11th International Conference on Parallel and Distributed Systems (ICPADS'05),20051121,2005,2,,488,492,"IT (information technology) advanced with steady steps from 1970's is essential in our daily life. As the results of the advances in high-speed data-transfer network technology, software development environment has been changing into new development paradigm. In this paper, we propose software reliability assessment methods for concurrent distributed system development by using the analytic hierarchy process. Also, we make a comparison between the inflection S-shaped software reliability growth model and the other models based on a nonhomogeneous Poisson process applied to reliability assessment of the entire system composed of several software components. Moreover, we analyze actual software fault count data to show numerical examples of software reliability assessment for the open source project. Furthermore, we investigate an efficient software reliability assessment method for the actual open source system development",1521-9097;15219097,POD:0-7695-2281-5,10.1109/ICPADS.2005.111,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1524356,,Collaborative software;Information systems;Information technology;Internet;Object oriented modeling;Open source software;Programming;Reliability engineering;Software reliability;Systems engineering and theory,distributed programming;public domain software;software reliability;stochastic processes,S-shaped software reliability growth model;concurrent distributed system;hierarchy process;high-speed data-transfer network technology;information technology;nonhomogeneous Poisson process;open source software;open source system development;software component;software development environment;software fault count data;software reliability assessment method,,11,,6,,no,22-22 July 2005,,IEEE,IEEE Conference Publications
Design comparison of two rotating electrical machines for 42 V electric power steering,G. Aroquiadassou; H. Henao; V. Lanfranchi; F. Betin; B. Nahidmobarakeh; G. A. Capolino; M. Biedinger; G. Friedrich,"Dept. of Electr. Eng., Picardie Jules Verne Univ., Amiens, France","IEEE International Conference on Electric Machines and Drives, 2005.",20051205,2005,,,431,436,"The proposed paper presents two design procedures of rotating electrical machines for 42 V embedded application. Particularly, for an electrical power steering, a three-phase interior permanent magnet synchronous machine (PMSM) fed by a switch redundant power converter and a six-phase induction machine (IM6) fed by a new type of six switches converter are designed for future 42 V DC system. For the PMSM, the magnetic circuit has been fully designed using the optimization from analytic and finite-element based software. For the IM6, a classical magnetic circuit coming from a traditional three-phase squirrel-cage low power induction machine has been used. The final design results are compared on the basis of the power-to-weight ratio",,Electronic:0-7803-8988-3; POD:0-7803-8987-5,10.1109/IEMDC.2005.195759,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1531377,,Design optimization;Finite element methods;Induction machines;Magnetic analysis;Magnetic circuits;Magnetic switching;Permanent magnet machines;Power steering;Switches;Switching converters,design engineering;electric machine analysis computing;finite element analysis;magnetic circuits;optimisation;permanent magnet machines;squirrel cage motors;switching convertors;synchronous machines,electric power steering;finite-element based software;magnetic circuit;power-to-weight ratio;rotating electrical machines;squirrel-cage low power induction machine;switch redundant power converter;three-phase interior permanent magnet synchronous machine,,3,,14,,no,15-15 May 2005,,IEEE,IEEE Conference Publications
Earth return path impedances of underground cables for the two-layer earth case,D. A. Tsiamitros; G. K. Papagiannis; D. P. Labridis; P. S. Dokopoulos,"Dept. of Electr. & Comput. Eng., Aristotle Univ. of Thessaloniki, Greece",IEEE Transactions on Power Delivery,20050627,2005,20,3,2174,2181,"The influence of earth stratification on underground power cable impedances is investigated in this paper. A rigorous solution of the electromagnetic field for the case of underground conductors and a two-layer earth is presented. Analytic expressions for the self and mutual impedances of the cable are derived. The involved semi-infinite integrals are calculated by a novel, numerically stable, and efficient integration scheme. Typical single-core cable arrangements are examined for a combination of layer depths and earth resistivities, based on actual measurements. The accuracy of the results over a wide frequency range is justified by a proper finite-element method formulation. The differences in cable impedances due to earth stratification are presented. Finally, a simple switching transient simulation is examined to evaluate the influence of the earth stratification on transient voltages and currents.",0885-8977;08858977,,10.1109/TPWRD.2005.848737,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1458895,Electromagnetic transient analysis;finite-element method (FEM);nonhomogeneous earth;power cable modeling,Computer aided software engineering;Conductivity;Conductors;Earth;Equations;Finite element methods;Impedance;Power system transients;Underground power cables;Voltage,electric impedance;electromagnetic fields;finite element analysis;transients;underground cables,earth return path impedances;earth stratification;electromagnetic field;finite element method;switching transient simulation;transient current;transient voltage;two-layer earth case;underground power cables,,17,,17,,no,5-Jul,,IEEE,IEEE Journals & Magazines
Enabling proteomics discovery through visual analysis,S. L. Havre; M. Singhal; D. A. Payne; M. S. Weir Lipton; B. J. M. Webb-Robertson,"Pacific Northwest Nat. Lab., Richland, WA, USA",IEEE Engineering in Medicine and Biology Magazine,20050606,2005,24,3,50,57,"This article presents the motivation for developing visual analysis tools for proteomic data and demonstrates their application to proteomics research with a visualization tool named Peptide Permutation and Protein Prediction, or PQuad, a functioning visual analytic tool for the study of systems biology, is in operation at the Pacific Northwest National Laboratory (PNNL). PQuad supports the exploration of proteins identified by proteomic techniques in the context of supplemental biological information. In particular, PQuad supports differential proteomics by simplifying the comparison of peptide sets from different experimental conditions as well as different proteins identification or confidence scoring techniques. Finally, PQuad supports data validation and quality control by providing a variety of resolutions for huge amounts of data to reveal errors undetected by other methods.",0739-5175;07395175,,10.1109/MEMB.2005.1436460,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1436460,,Amino acids;Bioinformatics;Databases;Genomics;Mass spectroscopy;Organisms;Peptides;Proteins;Proteomics;Sequences,biology computing;data visualisation;molecular biophysics;proteins,PQuad;confidence scoring techniques;data validation;peptide permutation;protein prediction;proteins identification;proteomics;quality control;supplemental biological information;systems biology;visual analysis tools,"Algorithms;Computer Graphics;Gene Expression Profiling;Mass Spectrometry;Peptides;Proteins;Proteomics;Sequence Analysis, Protein;Software;Systems Biology;User-Computer Interface",5,,8,,no,May-June 2005,,IEEE,IEEE Journals & Magazines
Facing scalability issues in requirements prioritization with machine learning techniques,P. Avesani; C. Bazzanella; A. Perini; A. Susi,"ITC-IRST, Trento, Italy",13th IEEE International Conference on Requirements Engineering (RE'05),20051114,2005,,,297,305,"Case-based driven approaches to requirements prioritization proved to be much more effective than first-principle methods in being tailored to a specific problem, that is they take advantage of the implicit knowledge that is available, given a problem representation. In these approaches, first-principle prioritization criteria are replaced by a pairwise preference elicitation process. Nevertheless case-based approaches, using the analytic hierarchy process (AHP) technique, become impractical when the size of the collection of requirements is greater than about twenty since the elicitation effort grows as the square of the number of requirements. We adopt a case-based framework for requirements prioritization, called case-based ranking, which exploits machine learning techniques to overcome the scalability problem. This method reduces the acquisition effort by combining human preference elicitation and automatic preference approximation. Our goal in this paper is to describe the framework in details and to present empirical evaluations which aim at showing its effectiveness in overcoming the scalability problem. The results prove that on average our approach outperforms AHP with respect to the trade-off between expert elicitation effort and the requirement prioritization accuracy.",1090-705X;1090705X,POD:0-7695-2425-7,10.1109/RE.2005.30,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1531050,,Costs;Customer satisfaction;Decision making;Humans;Machine learning;Programming;Project management;Scalability;Software systems;Time factors,case-based reasoning;learning (artificial intelligence);systems analysis,automatic preference approximation;case-based ranking;human preference elicitation;machine learning;requirements prioritization;scalability issues,,13,,26,,no,29 Aug.-2 Sept. 2005,,IEEE,IEEE Conference Publications
Field Modeling for the CESR-C Superconducting Wiggler Magnets,J. A. Crittenden; A. Mikhailichenko; E. Smith; K. Smolenski; A. Temnykh,"LEPP, Cornell University, Ithaca, NY 14853-8001, critten@lepp.cornell.edu",Proceedings of the 2005 Particle Accelerator Conference,20060213,2005,,,2336,2338,"Superconducting wiggler magnets for operation of the CESR electron-storage ring at energies as low as 1.5 GeV have been designed, built and installed in the years 2000 to 2004. Finite-element models of field quality have been developed and various sources of field errors have been investigated and compared to field measurements. Minimization algorithms providing accurate analytic representations of the wiggler fields have been established. We present quantitative descriptions of field modeling, of measured field quality and of the accuracy achieved in the analytic functions of the field.",1944-4680;19444680,POD:0-7803-8859-3,10.1109/PAC.2005.1591102,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1591102,,Coils;Finite element methods;Gaussian processes;Lattices;Magnetic analysis;Magnetic field measurement;Production;Software packages;Superconducting magnets;Undulators,,,,1,,12,,no,16-20 May 2005,,IEEE,IEEE Conference Publications
Generalized preview and delayed H<sup>‰ö_</sup> control: output feedback case,A. Kojima,"Human Mechatronics Systems Course, Faculty of System Design, Tokyo Metropolitan University, Asahigaoka 6-6, Hino-city, Tokyo 191-0065, Japan akojima@cc.tmit.ac.jp",Proceedings of the 44th IEEE Conference on Decision and Control,20060130,2005,,,5770,5775,"A generalized H<sup>‰ö_</sup>control problem, which covers preview and delayed control strategies, is discussed in the output feedback setting. By introducing analytic solutions to the corresponding control/filtering operator Riccati equations, the solvability is clarified based on the fundamental solutions to ordinary differential equations. The solutions obtained here enable us to deal with important class of H<sup>‰ö_</sup>control problems, which include multiple input/output delays and the preview tracking strategies.",0191-2216;01912216,POD:0-7803-9567-0,10.1109/CDC.2005.1583083,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1583083,,Computer aided software engineering;Control systems;Delay effects;Delay systems;Differential equations;Filtering theory;Humans;Hydrogen;Output feedback;Riccati equations,,,,2,,11,,no,15-15 Dec. 2005,,IEEE,IEEE Conference Publications
Image up-sampling using total-variation regularization with a new observation model,H. A. Aly; E. Dubois,"Minist. of Defence, Cairo, Egypt",IEEE Transactions on Image Processing,20050919,2005,14,10,1647,1659,"This paper presents a new formulation of the regularized image up-sampling problem that incorporates models of the image acquisition and display processes. We give a new analytic perspective that justifies the use of total-variation regularization from a signal processing perspective, based on an analysis that specifies the requirements of edge-directed filtering. This approach leads to a new data fidelity term that has been coupled with a total-variation regularizer to yield our objective function. This objective function is minimized using a level-sets motion that is based on the level-set method, with two types of motion that interact simultaneously. A new choice of these motions leads to a stable solution scheme that has a unique minimum. One aspect of the human visual system, perceptual uniformity, is treated in accordance with the linear nature of the data fidelity term. The method was implemented and has been verified to provide improved results, yielding crisp edges without introducing ringing or other artifacts.",1057-7149;10577149,,10.1109/TIP.2005.851684,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1510697,Data fidelity;gamma correction;image up-sampling;interpolation;level-sets motion (LSM);observation model;regularization;total variation,Application software;Cameras;Digital filters;Filtering;Image resolution;Image sampling;Interpolation;Lattices;Optical filters;Signal analysis,filtering theory;image motion analysis;image sampling;interpolation,data fidelity;display process;edge-directed filtering;gamma correction;human visual system;image acquisition;interpolation;level-sets motion;regularized image up-sampling problem,"Algorithms;Artificial Intelligence;Computer Simulation;Image Enhancement;Image Interpretation, Computer-Assisted;Information Storage and Retrieval;Models, Statistical;Pattern Recognition, Automated;Sample Size",92,4,54,,no,Oct. 2005,,IEEE,IEEE Journals & Magazines
Implementing Large-Scale Autonomic Server Monitoring Using Process Query Systems,C. Roblee; V. Berk; G. Cybenko,Institute for Security Technology Studies,Second International Conference on Autonomic Computing (ICAC'05),20050906,2005,,,123,133,"In this paper we present a new server monitoring method based on a new and powerful approach to dynamic data analysis: process query systems (PQS). PQS enables user-space monitoring of servers and, by using advanced behavioral models, makes accurate and fast decisions regarding server and service state. Data to support state estimation come from multiple sensor feeds located within a server network. By post-processing a system's state estimates, it becomes possible to identify, isolate and/or restart anomalous systems, thus avoiding cross-infection or prolonging performance degradation. The PQS system we use is a generic process detection software platform. It builds on the wide variety of system-level information that past autonomic computing research has studied by implementing a highly flexible, scalable and efficient process-based analytic engine for turning raw system information into actionable system and service state estimates",,POD:0-7965-2276-9,10.1109/ICAC.2005.34,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1498058,,Data analysis;Degradation;Engines;Feeds;Information analysis;Large-scale systems;Monitoring;Network servers;Power system modeling;State estimation,file servers;query processing;state estimation;system monitoring,actionable system;advanced behavioral models;autonomic computing;cross-infection;dynamic data analysis;large-scale autonomic server monitoring;performance degradation;process detection software;process query systems;process-based analytic engine;server network;server state;service state estimation;system turning;system-level information;user-space monitoring,,6,2,14,,no,13-16 June 2005,,IEEE,IEEE Conference Publications
Load Balancing using Grid-based Peer-to-Peer Parallel I/O,Y. Wang; D. Kaeli,"Department of Electrical and Computer Engineering, Northeastern University, Boston, MA 02115. yiwang@ece.neu.edu",2005 IEEE International Conference on Cluster Computing,20070416,2005,,,1,10,"In the area of grid computing, there is a growing need to process large amounts of data. To support this trend, we need to develop efficient parallel storage systems that can provide for high performance for data-intensive applications. In order to overcome I/O bottlenecks and to increase I/O parallelism, data streams need to be parallelized at both the application level and the storage device level. In this paper, we propose a novel peer-to-peer (P2P) storage architecture for MPI applications on grid systems. We first present an analytic model of our P2P storage architecture. Next, we describe a profile-guided data allocation algorithm that can increase the degree of I/O parallelism present in the system, as well as to balance I/O in a heterogeneous system. We present results on an actual implementation. Our experimental results show that by partitioning data across all available storage devices and carefully tuning I/O workloads in the grid system, our peer-to-peer scheme can deliver scalable high performance I/O that can address I/O-intensive workloads",1552-5244;15525244,CD-ROM:0-7803-9486-0; POD:0-7803-9485-2,10.1109/CLUSTR.2005.347040,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4154083,,Application software;Bandwidth;File servers;Grid computing;Load management;Parallel processing;Partitioning algorithms;Peer to peer computing;Supercomputers;Throughput,digital storage;grid computing;message passing;parallel processing;peer-to-peer computing;resource allocation,MPI applications;data allocation;data streams;data-intensive applications;grid systems;grid-based peer-to-peer parallel I/O;heterogeneous system;load balancing;parallel storage systems;peer-to-peer storage architecture,,3,1,20,,no,Sept. 2005,,IEEE,IEEE Conference Publications
Measurement-driven dashboards enable leading indicators for requirements and design of large-scale systems,R. W. Selby,"Northrop Grumman Space Technol., Redondo Beach, CA, USA",11th IEEE International Software Metrics Symposium (METRICS'05),20051024,2005,,,10 pp.,22,"Measurement-driven dashboards provide a unifying mechanism for understanding, evaluating, and predicting the development, management, and economics of large-scale systems and processes. Dashboards enable interactive graphical displays of complex information and support flexible analytic capabilities for user customizability and extensibility. Dashboards commonly include software requirements and design metrics because they provide leading indicators for project size, growth, and stability. This paper focuses on dashboards that have been used on actual large-scale projects as well as example empirical relationships revealed by the dashboards. The empirical results focus on leading indicators for requirements and design of large-scale systems. In the first set of 14 projects focusing on requirements metrics, the ratio of software requirements to source-lines-of code averaged 1:46. Projects that far exceeded the 1:46 requirements-to-code ratio tended to be more effort-intensive and fault-prone during verification. In the second set of 16 projects focusing on design metrics, the components in the top quartile of the number of component internal states had 6.2 times more faults on average than did the components in the bottom quartile, after normalization by size. The components in the top quartile of the number of component interactions had 4.3 times more faults on average than did the components in the bottom quartile, after normalization by size. When the number of component internal states was in the bottom quartile, the component fault-proneness was low even when the number of component interactions was in the upper quartiles, regardless of size normalization. Measurement-driven dashboards reveal insights that increase visibility into large-scale systems and provide feedback to organizations and projects",1530-1435;15301435,POD:0-7695-2371-4,10.1109/METRICS.2005.31,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1509300,,Data analysis;Displays;Economic forecasting;Extraterrestrial measurements;Feedback;Information analysis;Large-scale systems;Project management;Space technology;Technology management,formal specification;formal verification;large-scale systems;object-oriented programming;software metrics;visual programming,component interaction;design metrics;interactive graphical display;large-scale systems;measurement-driven dashboards;software requirements;system design;system development;system economics;system evaluation;system management;system requirements;system understanding;system verification,,0,,27,,no,1-1 Sept. 2005,,IEEE,IEEE Conference Publications
Modelling of SACK TCP and application to the HTTP file transfer environment,D. Phillips; Jiankun Hu,"Sch. of Comput. Sci. & Inf. Technol., RMIT Univ., Melbourne, Vic., Australia",The IEEE Conference on Local Computer Networks 30th Anniversary (LCN'05)l,20051212,2005,,,2 pp.,516,"It is known that analytic modelling for TCP latency is a non trivial task. Recently, some significant progress has been made, such as the comprehensive result by Sikdar et al. However, models similar to these often rely on trial and error methods such as ""data fitting"". This can lead to a very limited scope for the resulting model and also large estimation error. In this paper, we propose improvements to Sikdar's SACK TCP model. A new delayed acknowledgement slow start model is developed that is analytically derived from the slow start algorithm which provides a novel mechanism to model the relationship between RTT and the delayed acknowledgement timer. We introduce a simple mechanism to include time taken to send an HTTP get request to broaden the scope of our SACK TCP model to Website file transfer. Simulation and live Internet experimentation has validated our scheme",0742-1303;07421303,POD:0-7695-2421-4,10.1109/LCN.2005.91,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1550913,,Algorithm design and analysis;Application software;Australia;Computer science;Delay;Estimation error;Information analysis;Information technology;Internet;Predictive models,Internet;file organisation;hypermedia;transport protocols,HTTP file transfer;HTTP get request;SACK TCP;Website file transfer;data fitting;large estimation error,,0,,5,,no,17-17 Nov. 2005,,IEEE,IEEE Conference Publications
Optimal control with unreliable communication: the TCP case,B. Sinopoli; L. Schenato; M. Franceschetti; K. Poolla; S. S. Sastry,"Dept. of Electr. Eng. & Comput. Sci., California Univ., Berkeley, CA, USA","Proceedings of the 2005, American Control Conference, 2005.",20050801,2005,,,3354,3359 vol. 5,"The paper considers the linear quadratic Gaussian (LQG) optimal control problem in the discrete time setting and when data loss may occur between the sensors and the estimation-control unit and between the latter and the actuation points. We consider the case where the arrival of the control packet is acknowledged at the receiving actuator, as it happens with the common transfer control protocol (TCP). We start by showing that the separation principle holds. Additionally, we can prove that the optimal LQG control is a linear function of the state. Finally, building upon our previous results on estimation with unreliable communication, the paper shows the existence of critical arrival probabilities below which the optimal controller fails to stabilize the system. This is done by providing analytic upper and lower bounds on the cost functional.",0743-1619;07431619,Electronic:0-7803-9099-7; POD:0-7803-9098-9,10.1109/ACC.2005.1470488,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1470488,,Actuators;Communication channels;Communication system control;Computer aided software engineering;Control systems;Delay estimation;Navigation;Optimal control;Protocols;Wireless sensor networks,linear quadratic Gaussian control;optimal control;probability;stability;telecommunication control;transport protocols,TCP;critical arrival probabilities;discrete time setting;linear function;linear quadratic Gaussian optimal control;optimal LQG control;receiving actuator;transfer control protocol;unreliable communication,,32,,12,,no,8-10 June 2005,,IEEE,IEEE Conference Publications
Paint deposition modeling for trajectory planning on automotive surfaces,D. C. Conner; A. Greenfield; P. N. Atkar; A. A. Rizzi; H. Choset,"Robotics Inst., Carnegie Mellon Univ., Pittsburgh, PA, USA",IEEE Transactions on Automation Science and Engineering,20051003,2005,2,4,381,392,"This research is focused on developing trajectory planning tools for the automotive painting industry. The geometric complexity of automotive surfaces and the complexity of the spray patterns produced by modern paint atomizers combine to make this a challenging and interesting problem. This paper documents our efforts to develop computationally tractable analytic deposition models for electrostatic rotating bell (ESRB) atomizers, which have recently become widely used in the automotive painting industry. The models presented in this paper account for both the effects of surface curvature as well as the deposition pattern of ESRB atomizers in a computationally tractable form, enabling the development of automated trajectory generation tools. We present experimental results used to develop and validate the models, and verify the interaction between the deposition pattern, the atomizer trajectory, and the surface curvature. Limitations of the deposition model with respect to predictions of paint deposition on highly curved surfaces are discussed. Note to Practitioners-The empirical paint deposition models developed herein, which are fit to experimental data, offer a significant improvement over models that are typically used in industrial robot simulations. The improved simulation results come without the computational cost and complexity of finite element methods. The models could be incorporated, as is, into existing industrial simulation tools, provided the users are cognizant of the model limitations with respect to highly curved surfaces. Although the models are based on readily available information, incorporating the models into existing robot simulation software would likely require support from the software vendor.",1545-5955;15455955,,10.1109/TASE.2005.851631,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1514457,Automotive painting;coverage;trajectory planning,Atomic layer deposition;Automotive engineering;Computational modeling;Electrostatic analysis;Painting;Paints;Service robots;Spraying;Surface fitting;Trajectory,automobile industry;industrial manipulators;painting;path planning;position control;spray coating techniques;spray coatings,analytic deposition models;atomizer trajectory;automated trajectory generation tools;automotive painting industry;automotive surfaces;deposition pattern;electrostatic rotating bell atomizers;geometric complexity;paint atomizers;paint deposition modeling;spray pattern complexity;surface curvature;trajectory planning tools,,25,1,20,,no,Oct. 2005,,IEEE,IEEE Journals & Magazines
Performance analysis of binary code protection,D. M. Nicol; H. Okhravi,"Dept. of Electr. & Comput. Eng., Illinois Univ., Urbana, IL, USA","Proceedings of the Winter Simulation Conference, 2005.",20060123,2005,,,10 pp.,,"Software protection technology seeks to prevent unauthorized observation or use of applications. Cryptography can be used to provide such protection, but imposes a potentially significant additional computation load. This paper examines the performance impact of two software protection techniques. We develop an analytic model and validate it using a detailed discrete-event simulator applied to memory reference traces of well-known benchmark programs. We find that even though the added workload may be large, that impact is often dominated by inherent costs of disk activity.",0891-7736;08917736,POD:0-7803-9519-0,10.1109/WSC.2005.1574300,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1574300,,Analytical models;Application software;Binary codes;Computational modeling;Computer security;Costs;Cryptography;Delay;Performance analysis;Software protection,security of data,binary code protection;discrete-event simulation;memory reference traces;performance analysis;software protection,,0,2,14,,no,4-7 Dec. 2005,,IEEE,IEEE Conference Publications
Practical Performance Model for Optimizing Dynamic Load Balancing of Adaptive Applications,K. Barker; N. Chrisochoides,"Los Alamos Nat. Lab., NM, USA",19th IEEE International Parallel and Distributed Processing Symposium,20050418,2005,,,28b,28b,"Optimizing the performance of dynamic load balancing toolkits and applications requires the adjustment of several runtime parameters; however, determining sufficiently good values for these parameters through repeated experimentation can be an expensive and prohibitive process. We describe an analytic modeling method which allows developers to study and optimize adaptive application performance in the presence of dynamic load balancing. To aid tractibility, we first derive a ""bi-modal"" step function which simplifies and approximates task execution behavior. This allows for the creation of an analytic modeling function which captures the dynamic behavior of adaptive and asynchronous applications, enabling accurate predictions of runtime performance. We validate our technique using synthetic microbenchmarks and a parallel mesh generation application and demonstrate that this technique, when used in conjunction with the PREMA runtime toolkit, can offer users significant performance improvements over several well-known load balancing tools used in practice today.",1530-2075;15302075,POD:0-7695-2312-9,10.1109/IPDPS.2005.352,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1419848,,Application software;Computer science;Informatics;Load management;Mobile computing;Performance analysis;Predictive models;Quantum computing;Runtime environment;Software tools,mesh generation;parallel processing;resource allocation,PREMA runtime toolkit;analytic modeling method;asynchronous application;dynamic load balancing toolkit;parallel mesh generation application,,0,,25,,no,04-08 April 2005,,IEEE,IEEE Conference Publications
PULSATINGSTORE: An Analytic Framework for Automated Storage Management,Lin Qiao; D. Agrawal; A. E. Abbadi; B. R. Iyer,"University of California, Santa Barbara",21st International Conference on Data Engineering Workshops (ICDEW'05),20060705,2005,,,1213,1213,"Self-management of large information technology components, such as DBMSs, has emerged as one important problem in the area of autonomic computing. In particular, automated storage management is critical for most data-intensive applications. The reason is that the storage maintenance cost manifests one of the biggest factors in the overall operational cost. At the same time, due to the interactive nature of most applications, users typically pose the QoS constraints on IO access performance. Hence it is crucial to ensure that the applications are not underprovisioned (giving rise to the risk of QoS violation) or over-provisioned (resulting in high operational costs). Such issue gets further complicated when the application workload keeps changing. In this paper, we present a novel analytic framework, PULSATINGSTORE, for autonomically managing the storage to balance the cost and performance in an online manner. In particular, given the workload characteristics of an application and storage QoS requirement, our PULSATINGSTORE framework is capable of scheduling the up-migration (in the case of under-provisioning) or down-migration (in the case of over-provisioning) with the optimal or near-optimal cost while still maintaining the QoS constraint.",,POD:0-7695-2657-8,10.1109/ICDE.2005.271,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1647830,,Access protocols;Application software;Cost function;Information technology;Performance analysis;Processor scheduling;Silicon;Standardization;Storage automation;Technology management,,,,2,1,22,,no,05-08 April 2005,,IEEE,IEEE Conference Publications
QoS-based dynamic scheduling for manufacturing grid workflow,Yiping Yuan; Tao Yu; Feng Xiong; Minlun Fang,"CIMS & Robot Center, Shanghai Univ., China","Proceedings of the Ninth International Conference on Computer Supported Cooperative Work in Design, 2005.",20050906,2005,2,,1123,1128 Vol. 2,"Manufacturing grid (MG) workflow can be defined as the composition of manufacturing activities which execute on heterogeneous and distributed manufacturing resources in virtual organization to accomplish a specific goal. Uncertainties within MG environment pose new challenges for MG workflow. Dynamic scheduling is one of the most critical components in MG workflow. Through analyzing the QoS properties in MG, a scheduling architecture based on QoS for MG workflow is presented. Workflow engine can dynamically schedule the resources according the activity requirements and resource QoS capabilities. If multiple resources that meet the QoS requirements are available to an activity at a time, workflow scheduling can select the 'best match' resource according to scheduling algorithms based on AHP (analytic hierarchy process).",,POD:1-84600-002-5,10.1109/CSCWD.2005.194347,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1504254,,Application software;Automatic control;Collaborative work;Computer aided manufacturing;Computer integrated manufacturing;Dynamic scheduling;Job shop scheduling;Machine tools;Quality of service;Resource management,dynamic scheduling;grid computing;manufacturing systems;quality of service;resource allocation;virtual enterprises;workflow management software,QoS-based dynamic scheduling;analytic hierarchy process;manufacturing grid workflow;resource scheduling;virtual organization;workflow engine;workflow scheduling,,1,,12,,no,24-26 May 2005,,IEEE,IEEE Conference Publications
"Quasi-static solutions of multilayer elliptical, cylindrical coplanar striplines and multilayer coplanar striplines with finite dielectric dimensions - asymmetrical case",V. Akan; E. Yazgan,"Dept. of Electr. & Electron. Eng., Hacettepe Univ., Ankara, Turkey",IEEE Transactions on Microwave Theory and Techniques,20051205,2005,53,12,3681,3686,"In this paper, fast, accurate, and simple analytic closed-form expressions are presented in order to calculate the quasi-TEM parameters of multilayer elliptical coplanar stripline (CPS), multilayer cylindrical CPS, and multilayer CPS, with finite dielectric dimensions by using conformal mapping techniques. The obtained computer-aided-design-oriented expressions are quite accurate and easy to apply in designing microwave integrated circuits and antenna applications. The results have also been compared with the ones of another conformal mapping method available in the literature and also confirmed analytically.",0018-9480;00189480,,10.1109/TMTT.2005.856080,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1550016,Computer-aided design (CAD)-oriented formulas;conformal mapping;coplanar stripline (CPS);multilayer cylindrical coplanar stripline (MC CPS);multilayer elliptical coplanar stripline (ME CPS),Application software;Capacitance;Closed-form solution;Conformal mapping;Coplanar transmission lines;Dielectric constant;Dielectric substrates;Microwave integrated circuits;Nonhomogeneous media;Stripline,circuit CAD;circular waveguides;conformal mapping;coplanar waveguides;dielectric materials;microwave antennas;microwave integrated circuits;strip lines,CAD-oriented formula;antenna application;closed-form expressions;computer-aided-design-oriented expression;conformal mapping;cylindrical coplanar stripline;finite dielectric dimension;microwave integrated circuit;multilayer coplanar stripline;multilayer elliptical coplanar stripline;quasiTEM parameter;quasistatic solution,,5,,11,,no,Dec. 2005,,IEEE,IEEE Journals & Magazines
Reliability evaluation of Web-based software applications,L. Davila-Nicanor; P. Mejia-Alvarez,"Sect. de Comput., CINVESTAV-IPN, Zacatenco, Mexico",Sixth Mexican International Conference on Computer Science (ENC'05),20060213,2005,,,106,112,"In diverse industrial and academic environments, the quality of software has been evaluated (validated) using different analytic studies. It is common practice in these environments to use statistical models for the assurance, control and evaluation of the quality of a software product or process. A number of industries in the safety-critical sector are forced nowadays to use such processes by industry-specific standards (e.g., the DO-178B standard for airborne software systems). The contribution of the present work is focused on the development of a methodology for the evaluation and analysis of the reliability of Web-based software applications. We tested our methodology in a Web-based software system and used statistical modeling theory for the analysis and evaluation of the reliability. The behavior of the system under ideal conditions was evaluated and compared against the operation of the system executing under real conditions. The results obtained demonstrated the effectiveness and applicability of our methodology.",1550-4069;15504069,POD:0-7695-2454-0,10.1109/ENC.2005.36,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1592207,,Application software;Computer industry;Electrical equipment industry;Internet;Quality assurance;Software quality;Software standards;Software systems;Testing;Web server,Internet;software performance evaluation;software quality;software reliability;statistical analysis,Web-based software application;academic environment;industrial environment;industry-specific standard;reliability evaluation;safety-critical sector;software process;software product;software quality;statistical modeling theory,,1,,15,,no,26-30 Sept. 2005,,IEEE,IEEE Conference Publications
Requirements on worm mitigation technologies in MANETS,R. G. Cole; N. Phamdo; M. A. Rajab; A. Terzis,"Appl. Phys. Lab., Johns Hopkins Univ., MD, USA",Workshop on Principles of Advanced and Distributed Simulation (PADS'05),20050620,2005,,,207,214,"This study presents an analysis of the impact of mitigation on computer worm propagation in mobile ad-hoc networks (MANETS). According to the recent DARPA BAA - defense against cyber attacks on MANETS (A.K. Ghosh 2004), ""one of the most severe cyber threats is expected to be worms with arbitrary payload that can infect and saturate MANET-based networks on the order of seconds"". Critical to the design of effective worm counter measures in MANET environments is an understanding of the propagation mechanisms and the performance of the mitigation technologies. This work aims to advance the security of these critical systems through increased knowledge of propagation mechanisms, performance and the effect of mitigation technologies. We present both analytic and simulation analysis of mitigation effectiveness. The ultimate goal of these studies is to develop an accurate set of performance requirements on mitigation techniques to minimize worm propagation in tactical, battlefield MANETS.",1087-4097;10874097,POD:0-7695-2383-8,10.1109/PADS.2005.21,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1443326,,Ad hoc networks;Analytical models;Computer worms;Intelligent networks;Laboratories;Mobile ad hoc networks;Mobile computing;Payloads;Physics;Predictive models,ad hoc networks;digital simulation;invasive software;mobile computing,MANETS;computer worm mitigation technology;cyber attack;cyber threat;mobile ad-hoc network;worm propagation,,7,6,16,,no,1-3 June 2005,,IEEE,IEEE Conference Publications
Resource Allocation for Autonomic Data Centers using Analytic Performance Models,M. N. Bennani; D. A. Menasce,George Mason University,Second International Conference on Autonomic Computing (ICAC'05),20050906,2005,,,229,240,"Large data centers host several application environments (AEs) that are subject to workloads whose intensity varies widely and unpredictably. Therefore, the servers of the data center may need to be dynamically redeployed among the various AEs in order to optimize some global utility function. Previous approaches to solving this problem suffer from scalability limitations and cannot easily address the fact that there may be multiple classes of workloads executing on the same AE. This paper presents a solution that addresses these limitations. This solution is based on the use of analytic queuing network models combined with combinatorial search techniques. The paper demonstrates the effectiveness of the approach through simulation experiments. Both online and batch workloads are considered",,POD:0-7965-2276-9,10.1109/ICAC.2005.50,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1498067,,Application software;Computational modeling;Computer science;Data analysis;Delay;Humans;Performance analysis;Queueing analysis;Resource management;Scalability,batch processing (computers);combinatorial mathematics;queueing theory;resource allocation;search problems,analytic performance models;application environments;autonomic data centers;batch workload;combinatorial search;online workload;queuing network models;resource allocation;scalability;utility function,,110,11,13,,no,13-16 June 2005,,IEEE,IEEE Conference Publications
Search-based amorphous slicing,D. Fatiregun; M. Harman; R. M. Hierons,"Kings Coll. London, UK",12th Working Conference on Reverse Engineering (WCRE'05),20060103,2005,,,10 pp.,12,"Amorphous slicing is an automated source code extraction technique with applications in many areas of software engineering, including comprehension, reuse, testing and reverse engineering. Algorithms for syntax-preserving slicing are well established, but amorphous slicing is harder because it requires arbitrary transformation; finding good general purpose amorphous slicing algorithms therefore remains as hard as general program transformation. In this paper we show how amorphous slices can be computed using search techniques. The paper presents results from a set of experiments designed to explore the application of genetic algorithms, hill climbing, random search and systematic search to a set of six subject programs. As a benchmark, the results are compared to those from an existing analytical algorithm for amorphous slicing, which was written specifically to perform well with the sorts of program under consideration. The results, while tentative at this stage, do give grounds for optimism. The search techniques proved able to reduce the size of the programs under consideration in all cases, sometimes equaling the performance of the specifically-tailored analytic algorithm. In one case, the search techniques performed better, highlighting a fault in the existing algorithm",1095-1350;10951350,POD:0-7695-2474-5,10.1109/WCRE.2005.28,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1566141,search based software engineering;slicing;transformation,Algorithm design and analysis;Amorphous materials;Application software;Automatic testing;Educational institutions;Genetic algorithms;Performance analysis;Reverse engineering;Software engineering;Space exploration,genetic algorithms;graph theory;program slicing;reverse engineering;search problems,analytical algorithm;general program transformation;genetic algorithms;hill climbing;random search;reverse engineering;search-based amorphous slicing;software comprehension;software engineering;software reusability;software testing;source code extraction;syntax-preserving slicing;systematic search,,7,,34,,no,11-11 Nov. 2005,,IEEE,IEEE Conference Publications
Sentiment mining in WebFountain,J. Yi; W. Niblack,"IBM Almaden Res. Center, San Jose, CA, USA",21st International Conference on Data Engineering (ICDE'05),20050418,2005,,,1073,1083,"WebFountain is a platform for very large-scale text analytics applications that allows uniform access to a wide variety of sources. It enables the deployment of a variety of document-level and corpus-level miners in a scalable manner, and feeds information that drives end-user applications through a set of hosted Web services. Sentiment (or opinion) mining is one of the most useful analyses for various end-user applications, such as reputation management. Instead of classifying the sentiment of an entire document about a subject, our sentiment miner determines sentiment of each subject reference using natural language processing techniques. In this paper, we describe the fully functional system environment and the algorithms, and report the performance of the sentiment miner. The performance of the algorithms was verified on online product review articles, and more general documents including Web pages and news articles.",1063-6382;10636382,POD:0-7695-2285-8,10.1109/ICDE.2005.132,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1410217,,Algorithm design and analysis;Application software;Data mining;Feeds;Information analysis;Large-scale systems;Natural language processing;Performance analysis;Web pages;Web services,Internet;data mining;information retrieval;natural languages;text analysis,Web service;WebFountain;corpus-level miner;document-level miner;end-user application;natural language processing;online product review article;reputation management;sentiment mining;text analytics application,,4,1,32,,no,5-8 April 2005,,IEEE,IEEE Conference Publications
Sequential correction of perspective warp in camera-based documents,C. Monnier; V. Ablavsky; S. Holden; M. Snorrason,"Charles River Analytics Inc., Cambridge, MA, USA",Eighth International Conference on Document Analysis and Recognition (ICDAR'05),20060116,2005,,,394,398 Vol. 1,"Documents captured with hand-held devices, such as digital cameras often exhibit perspective warp artifacts. These artifacts pose problems for OCR systems which at best can only handle in-plane rotation. We propose a method for recovering the planar appearance of an input document image by examining the vertical rate of change in scale of features in the document. Our method makes fewer assumptions about the document structure than do previously published algorithms.",1520-5363;15205363,POD:0-7695-2420-6,10.1109/ICDAR.2005.216,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1575576,,Computer science;Digital cameras;Image analysis;Image edge detection;Information analysis;Nearest neighbor searches;Optical character recognition software;Optical distortion;Rivers;Text analysis,cameras;document image processing;feature extraction;optical character recognition,OCR system;camera-based document;digital camera;document image processing;document structure;hand-held device;perspective warp artifact;sequential correction,,5,2,9,,no,29 Aug.-1 Sept. 2005,,IEEE,IEEE Conference Publications
Smooth adaptive fitting of 3D models using hierarchical triangular splines,A. Yvart; S. Hahmann; G. P. Bonneau,"LMC, IMAG, Grenoble, France",International Conference on Shape Modeling and Applications 2005 (SMI' 05),20051227,2005,,,13,22,"The recent ability to measure quickly and inexpensively dense sets of points on physical objects has deeply influenced the way engineers represent shapes in CAD systems, animation software or in the game industry. Many researchers advocated to completely bypass smooth surface representations, and to stick to a dense mesh model throughout the design process. Yet smooth analytic representations are still required in standard CAD systems and animation software, for reasons of compactness, control, appearance and manufacturability. In this paper we present a new method for fitting a smooth adoptively refinable triangular spline surface of arbitrary topology to an arbitrary dense triangular mesh. The key ingredient in our solution is that adaptive fitting is achieved by 4-splitting triangular surface patches locally therefore no particular attention has to be paid the validity of an underlying subdivided mesh. Furthermore, the final surface is composed of low-degree polynomial patches that always join with Gl-continuity. The ability to adoptively refine the model allows to achieve a given approximation error with a minimal number of patches.",,POD:0-7695-2379-X,10.1109/SMI.2005.43,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1563206,,Animation;Computer industry;Design automation;Electrical equipment industry;Manufacturing industries;Shape measurement;Software measurement;Software systems;Surface fitting;Toy industry,CAD;computational geometry;mesh generation;solid modelling;splines (mathematics);surface fitting,3D model;4-splitting triangular surface patch;CAD system;animation software;arbitrary dense triangular mesh;arbitrary topology;design process;game industry;hierarchical triangular spline;low-degree polynomial patch;manufacturability;mesh model;smooth adaptive fitting;smooth surface representation,,1,,39,,no,13-17 June 2005,,IEEE,IEEE Conference Publications
The design and development of a sales force automation tool using business process management software,C. Baysan; A. Bertman; R. Maynigo; G. Norville; N. Osborne; T. Taylor,"Dept. of Syst. & Inf. Eng., Virginia Univ., Charlottesville, VA, USA","2005 IEEE Design Symposium, Systems and Information Engineering",20050822,2005,,,318,327,"A sales force automation (SFA) tool is a computerized system that provides sales team members and managers with the functionality to track sales leads, manage contacts, control customer relations, monitor sales processes, schedule meetings, forecast sales and analyze employee performance. SFA tools aim to increase the efficiency and effectiveness of a sales team; however many commercially available SFA tools are generically structured solutions that do not accommodate the specific needs of a company. However, because of recent interest and developments in business process management (BPM) software, an emerging technology capable of modeling and automating business processes, it is possible for individual firms to custom-design their own sales force automation tools. In general, BPM software tools provide automated support for tracking tasks across multiple departments as they are completed by different employees. This paper discusses the design and development of an SFA tool using BPM software, including the identification of a detailed sales process, an analysis of the reporting capabilities, a model for determining the probabilistic outcomes of the sales process, and a decision-analytic model for optimizing sales force resource allocation.",,POD:0-9744559-4-6,10.1109/SIEDS.2005.193274,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1497167,,Automatic control;Business;Computerized monitoring;Control systems;Design automation;Force control;Marketing and sales;Processor scheduling;Software development management;Software tools,business data processing;corporate modelling;sales management,BPM software tools;business process automation;business process management software;business process modeling;computerized system;contact management;customer relation control;decision-analytic model;employee performance;meeting scheduling;probabilistic outcomes;sale process monitoring;sales force automation tool;sales force resource allocation optimization;sales forecasting;sales leads;sales process,,1,6,8,,no,29-Apr-05,,IEEE,IEEE Conference Publications
The essential components of software architecture design and analysis,R. Kazman,"Software Eng. Inst., Carnegie Mellon Univ., Pittsburgh, PA, USA",12th Asia-Pacific Software Engineering Conference (APSEC'05),20060320,2005,,,1 pp.,,"Summary form only given. Architecture analysis and design methods such as ATAM, QAW, ADD and CBAM have enjoyed modest success in recent years and are being adopted by many companies as part of their standard software development processes. They are used in the software lifecycle, as a means of understanding business goals and stakeholder concerns, mapping these onto an architectural representation, and assessing the risks associated with this mapping. These methods have evolved a set of shared component techniques. In this article the author describes how these techniques can be combined in countless ways to create needs-specific methods. The author demonstrates the generality of these techniques by describing a new architecture improvement method called APTIA (analytic principles and tools for the improvement of architectures). APTIA almost entirely reuses pre-existing techniques but in a new combination, with new goals and results. Lastly, the author exemplifies APTIA's use in improving the architecture of a commercial information system.",1530-1362;15301362,POD:0-7695-2465-6,10.1109/APSEC.2005.103,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1607129,,Business;Companies;Computer architecture;Design methodology;Information systems;Programming;Software architecture;Software design;Software standards;Standards development,information systems;software architecture;software process improvement;software reusability,commercial information system;software architecture design;software development process;software lifecycle;software process improvement;software reuse;stakeholder,,1,,,,no,15-17 Dec. 2005,,IEEE,IEEE Conference Publications
Wordlength determination algorithms for hardware implementation of linear time invariant systems with prescribed output accuracy,S. C. Chan; K. M. Tsui,"Dept. of Electr. & Electron. Eng., Hong Kong Univ., China",2005 IEEE International Symposium on Circuits and Systems,20050725,2005,,,2607,2610 Vol. 3,"This paper proposes two novel algorithms for optimizing the hardware resources in finite wordlength implementation of linear time invariant systems. The hardware complexity is measured by the exact internal wordlength used for each intermediate data. The first algorithm formulates the design problem as a constrained optimization, from which an analytic closed-form solution of the internal wordlengths subject to a prescribed output accuracy can be determined by the Lagrange multiplier method. The second algorithm is based on a discrete optimization method called the marginal analysis method, and it yields the desired wordlengths in integer values. Both approaches are found to be very effective and they are well-suited to large scale systems such as software radio receivers. Design examples show that the proposed algorithms offer better results and a lower design complexity than conventional methods.",0271-4302;02714302,POD:0-7803-8834-8,10.1109/ISCAS.2005.1465160,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1465160,,Algorithm design and analysis;Closed-form solution;Constraint optimization;Design optimization;Hardware;Lagrangian functions;Large-scale systems;Optimization methods;Software radio;Time invariant systems,IIR filters;optimisation;radio receivers;software radio,Lagrange multiplier method;analytic closed-form solution;constrained optimization;discrete optimization;finite wordlength implementation;hardware complexity;hardware implementation;linear time invariant systems;marginal analysis method;prescribed output accuracy;software radio receivers;wordlength determination algorithms,,6,,10,,no,23-26 May 2005,,IEEE,IEEE Conference Publications
"""GeoAnalytics"" - Exploring spatio-temporal and multivariate data",M. Jern; J. Franzen,"Linkoping University, Sweden",Tenth International Conference on Information Visualisation (IV'06),20060724,2006,,,25,31,"The voluminous nature of social scientific, spatial-temporal statistical databases calls for high interactive performance and creative integrated information and geo-visualization tools. A solution to this challenge can be found in the emerging visual analytics (VA), a science of analytical reasoning facilitated by interactive visual interfaces and innovative visualization and is now actively pursued by research groups worldwide. In this paper, we present a tool called ""GeoAnalytics"", based on the principles behind VA. Our objective is to define new suitable approaches and tools for exploring time variant and multivariate attributes simultaneous including a spatial dimension. We introduce parallel coordinates integrated with time series and trend graph that serves as the visual control panel for the application. Multivariate attribute dynamic queries can express simultaneously queries involving time varying spatial data. VA encourages the need to build a bridge between the advantages of both human perception and computer science technologies. The sense of immediacy and speed-of-thought interaction is achieved in our dynamically linked components and maximum allocation of screen area for visual displays that helps users stay focused on their work and shortens their time to enlightenment",1550-6037;15506037,POD:0-7695-2602-0,10.1109/IV.2006.1,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1648237,,Application software;Data visualization;Displays;Internet;Local government;Performance analysis;Spatial databases;Statistics;Visual analytics;Visual databases,data visualisation;geophysics computing;graph theory;graphical user interfaces;time series,GeoAnalytics;analytical reasoning;geo-visualization tools;interactive visual interfaces;multivariate attribute dynamic queries;multivariate data;social scientific database;spatial-temporal statistical database;spatio-temporal data;time series;time varying spatial data;trend graph;visual analytics;visual displays,,9,,14,,no,5-7 July 2006,,IEEE,IEEE Conference Publications
A Longitudinal Study of the Use of a Collaboration Tool: A Breadth and Depth Analysis,J. Scholtz; E. Morse; M. P. Steves,"National Institute of Standards and Technology Gaithersburg, MD",International Symposium on Collaborative Technologies and Systems (CTS'06),20060626,2006,,,1,11,"In this paper we present both a broad and deep look at the use of a collaboration tool in the intelligence community. Through an experimental program, intelligence analysts are given the opportunity to explore and use tools to determine if the tools provide sufficient value to be certified and moved into the analytic work environment. The goal of this program is to bring advanced technologies to the intelligence community through research and experimentation. New tools are evaluated using a metrics based assessment. Tools that successfully pass these evaluations are then introduced on an experimental network. Analysts employed by the experimental program work along side analysts in the intelligence community and look for opportunities where the experimental tools could be useful in current analytic processes. These uses are also evaluated to determine the value of the tools in the analytic environment.",,POD:0-9785699-0-3,10.1109/CTS.2006.8,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1644110,analysis;collaboration;evaluation;intelligence;metrics,Application software;Collaboration;Collaborative tools;Collaborative work;Employment;Laboratories;NIST;Procurement;Springs,,analysis;collaboration;evaluation;intelligence;metrics,,0,,11,,no,14-17 May 2006,,IEEE,IEEE Conference Publications
A Matlab-Based Simulation of System Stability In Frequency-Field Analysis,Zhigang Xu; Changsheng Xu,"Wuhan University of Technology, China","First International Conference on Innovative Computing, Information and Control - Volume I (ICICIC'06)",20061016,2006,1,,529,532,"The frequency-field analysis to a control system says the system's steady-state response when it has a sine signal input. Use this analytic method, control system's specification can be found directly and the method is very simple. Under this we can solve many questions such as avoiding resonance, restraining molest, improving system stability and so on. Matlab (Matrix Laboratory) is software designed for researching of matrix theory, linear algebra and numerical value accounting by Ph.D Clever Moler, the doyen scientist of MathWorks. Now Matlab software has developed into an edition of 7.1 and has been adopted widely by more and more researchers and engineers for its wide account ability and OOP GUI. The application of system stability simulation in frequency-field analysis to a linear system is described. We share our experiences and intuitions gained and encourage other control process to develop simulation-based system analysis",,POD:0-7695-2616-0,10.1109/ICICIC.2006.9,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1691854,,Analytical models;Computer languages;Control system analysis;Control systems;Frequency;MATLAB;Resonance;Signal analysis;Stability analysis;Steady-state,control system analysis computing;frequency response;linear systems;matrix algebra,Matlab-based simulation;control system;frequency-field analysis;linear algebra;linear system;matrix theory;numerical method;steady-state response;system stability,,0,,7,,no,Aug. 30 2006-Sept. 1 2006,,IEEE,IEEE Conference Publications
A New System for Frequency Monitoring and Fault Analysis,L. Jiang; H. Yang; L. Li,"College of Electrical & Information Engineering, Changsha University of Science & Technology. Changsha, Hunan, China. e-mail: JLP736200@163.com",2006 International Conference on Power System Technology,20070226,2006,,,1,4,"The new system studied in this paper is made of late-model single chip microcomputer and personal computer. It not only can monitor networks' frequency in real-time but also can calculate frequency variable then distinguish and record frequency fault course automatically. The whole recording time can come to 30 minutes. It is also equipped with data analytic software under WINDOWS circumstance. The new system can be used in power plants, substations and all levels dispatching station. It will play an important role in monitoring and recording networks' frequency.The recorded messages will be used in analyzing system frequency characteristic and under- frequency load shedding devices' performance. It has been used in power system and the effect is very good.",,CD-ROM:1-4244-0111-9; POD:1-4244-0110-0,10.1109/ICPST.2006.321708,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4116176,Cycle measure method;Frequency;Frequency measure method;Microcomputer;Monitor;Multi-cycle measure method;Power Network,Circuit faults;Computerized monitoring;Employee welfare;Frequency measurement;Hardware;Microcomputers;Power measurement;Power supplies;Power system analysis computing;Voltage,computerised monitoring;load shedding;power engineering computing;power supply quality;power system faults;power system measurement,Windows;fault analysis;frequency fault;frequency monitoring;late-model single chip microcomputer;networks frequency recording;personal computer;under frequency load shedding,,0,,9,,no,22-26 Oct. 2006,,IEEE,IEEE Conference Publications
A Process Diagnosis Technique for the Problems of Business Process,Xi Huang; Renzhong Tang; Hongtao Tang,"Institute of Production Engineering, Zhejiang Univ, Hangzhou 310027, China. E-mail: hhhhyhh@sohu.com",2006 6th World Congress on Intelligent Control and Automation,20061023,2006,2,,5633,5637,"One process diagnosis technique is proposed to solve the problems in business process. Firstly, on the base of objective view and analytic hierarchy process (AHP), a process diagnosis model is founded and the limits of the causes are given by using process business objective view. Then, objective view is converted into a hierarchical structure of AHP to quantitatively calculate the effects of these causes on the problems. Finally, a software package to support the proposed technique was developed and, as a case study, applied to a process diagnosis in a manufacturing enterprise. The operations of the system in the enterprise have shown that the performance is fine and have got better economic profit and social benefit",,POD:1-4244-0332-4,10.1109/WCICA.2006.1714153,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1714153,AHP;business process;objective view;process diagnosis,Automation;Computer integrated manufacturing;Educational institutions;Intelligent control;Manufacturing processes;Production engineering;Software packages,business data processing;fault diagnosis;operations research;process planning;software packages,analytic hierarchy process;business process diagnosis;manufacturing enterprise;objective view;software package,,0,,10,,no,0-0 0,,IEEE,IEEE Conference Publications
A Quality Evaluation Technique of RFID Middleware in Ubiquitous Computing,G. O. Oh; D. Y. Kim; S. I. Kim; S. Y. Rhew,"Kangwon Tourism College, Korea",2006 International Conference on Hybrid Information Technology,20111212,2006,2,,730,735,"With ubiquitous computing system, users can access information through a computer network at any time and in any place. The basic infrastructure of ubiquitous computing system is wireless network environment, and a RFID (Radio Frequency Identification) system is composed tags, readers, middleware, application services, etc. and uses networks. RFID middleware is system software that collects a large volume of raw data generated in RFID environment, filters the data, summarizes them into meaningful information and delivers the information to application services. RFID middleware links hardware to conventional middleware. Previous researches on RFID middleware have covered middleware from SUN, SAP, IBM, Microsoft, Oracle, etc. These products attach importance to different quality characteristics, and there have been few researches on the quality properties of RFID middleware. The present study examined functionality, reliability, usability, efficiency and portability among the quality characteristics of software in international standard ISO/IEC 9126 as well as the quality elements of standard RFID middleware of EPC Global, and based on them we extracted and analyzed items for evaluating the quality of RFID middleware in ubiquitous computing systems. Using the AHP (Analytic hierarchy process) that enables rational decision making by simplifying complicated problems, we evaluated the subjective characteristics of stakeholders in an objective way and proposed a selection method that evaluates quality using quality evaluation criteria. The proposed evaluation selection method is useful for developers who are going to develop RFID middleware in areas such as distribution and logistics to select RFID middleware suitable for their environment.",,POD:0-7695-2674-8,10.1109/ICHIT.2006.253690,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021295,,Application software;Computer networks;IEC standards;ISO standards;Middleware;Radiofrequency identification;Software standards;System software;Ubiquitous computing;Wireless networks,,,,9,,15,,no,9-11 Nov. 2006,,IEEE,IEEE Conference Publications
A Visualization System for Space-Time and Multivariate Patterns (VIS-STAMP),Diansheng Guo; Jin Chen; A. M. MacEachren; Ke Liao,IEEE,IEEE Transactions on Visualization and Computer Graphics,20060918,2006,12,6,1461,1474,"The research reported here integrates computational, visual and cartographic methods to develop a geovisual analytic approach for exploring and understanding spatio-temporal and multivariate patterns. The developed methodology and tools can help analysts investigate complex patterns across multivariate, spatial and temporal dimensions via clustering, sorting and visualization. Specifically, the approach involves a self-organizing map, a parallel coordinate plot, several forms of reorderable matrices (including several ordering methods), a geographic small multiple display and a 2-dimensional cartographic color design method. The coupling among these methods leverages their independent strengths and facilitates a visual exploration of patterns that are difficult to discover otherwise. The visualization system we developed supports overview of complex patterns and through a variety of interactions, enables users to focus on specific patterns and examine detailed views. We demonstrate the system with an application to the IEEE InfoVis 2005 contest data set, which contains time-varying, geographically referenced and multivariate data for technology companies in the US",1077-2626;10772626,,10.1109/TVCG.2006.84,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1703367,Information visualization;geovisualization;multivariate and spatio-temporal data;ordering;self-organizing map (SOM);small multiples.;visual analytics,Companies;Data visualization;Design methodology;Displays;Geography;Humans;Pattern analysis;Sorting;Spatiotemporal phenomena;Time varying systems,cartography;data visualisation;matrix algebra;pattern clustering;self-organising feature maps;sorting,cartographic method;geovisual analytic approach;multivariate pattern;parallel coordinate plot;pattern clustering;reorderable matrix;self-organizing map;sorting;space-time pattern;visual exploration;visualization system,"Algorithms;Computer Graphics;Computer Simulation;Data Interpretation, Statistical;Geographic Information Systems;Information Storage and Retrieval;Multivariate Analysis;Software;User-Computer Interface",103,,57,,no,Nov.-Dec. 2006,,IEEE,IEEE Journals & Magazines
About the Time Complexity of EAs Based on Finite Search Space,L. Ding; Y. Bi,"State Key Lab of Software Engineering, Wuhan University, Wuhan 430072, China. lxding@whu.edu.cn",2006 International Conference on Computational Intelligence and Security,20070129,2006,1,,308,311,"Some results about the computation time of evolutionary algorithms are obtained in this paper. First, some exact analytic expressions of the mean first hitting times of evolutionary algorithms infinite search spaces are acquired theoretically by using the properties of Markov chain associated with evolutionary algorithms considered here. Then, by introducing drift analysis and applying Dynkin's formula, the general upper and lower bounds of the mean first hitting times of evolutionary algorithms are estimated",,CD-ROM:1-4244-0605-6; POD:1-4244-0604-8,10.1109/ICCIAS.2006.294144,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4072097,,Algorithm design and analysis;Evolutionary computation;Genetics;Information technology;Software engineering;Space technology;State-space methods;Stochastic processes;Testing;Upper bound,Markov processes;computational complexity;evolutionary computation,Dynkin formula;Markov chain;drift analysis;evolutionary algorithm;finite search space;time complexity,,0,,21,,no,Nov. 2006,,IEEE,IEEE Conference Publications
Activity based high level modeling of dynamic switching currents in digital IC modules,A. Gstottner; T. Steinecke; M. Huemer,"Erlangen-Nurnberg Univ., Erlangen, Germany",2006 17th International Zurich Symposium on Electromagnetic Compatibility,20060515,2006,,,598,601,"Electromagnetic compatibility (EMC) becomes an increasingly important subject within the IC design process, because more and more market segments demand for low electromagnetic emission (EME) of integrated circuits. Therefore automatic emission model generation tools need to become part of the design flow. In this paper we present an automatic generation procedure of equivalent current sources (ECS) from chip netlists, based on an analytic approach. The ECSs describe the dynamic switching currents of digital function blocks of complex ICs. Worst case or typical current profiles are not generated by pattern simulation, but by pre-characterized logic cells and a set of configuration parameters like switching activity",,POD:3-9522990-3-0,10.1109/EMCZUR.2006.215005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1629695,,Circuit simulation;Circuit testing;Digital integrated circuits;Electromagnetic compatibility;Integrated circuit interconnections;Integrated circuit modeling;Logic;Semiconductor device modeling;Software libraries;Switching circuits,digital integrated circuits;electric current;electromagnetic compatibility,EMC;chip netlists;digital IC modules;dynamic switching currents;electromagnetic compatibility;electromagnetic emission;equivalent current sources;high level modeling;integrated circuits,,5,,4,,no,Feb. 27 2006-March 3 2006,,IEEE,IEEE Conference Publications
Adaptive multivariate rational data fitting with applications in electromagnetics,A. Cuyt; R. B. Lenin; S. Becuwe; B. Verdonk,"Dept. of Math., Univ. Antwerpen, Belgium",IEEE Transactions on Microwave Theory and Techniques,20060508,2006,54,5,2265,2274,"The behavior of certain electromagnetic devices or components can be simulated with great detail in software. A drawback of these simulation models is that they are very time consuming. Since the accuracy required for the computational electromagnetic analysis is usually only 2-3 significant digits, an approximate analytic model is sometimes used instead, as noted by Lehmensiek and Meyer in 2001. The most complex model we consider here is a multivariate rational function, which interpolates a number of simulation data. The interpolating rational function is constructed in such a way that it minimizes both the truncation error and the number of simulation data since each evaluation of the simulation model is computationally costly.",0018-9480;00189480,,10.1109/TMTT.2006.873637,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1629071,Electromagnetics;meta-modeling;multivariate;rational function,Application software;Computational electromagnetics;Computational modeling;Electromagnetic analysis;Electromagnetic devices;Electromagnetic modeling;Finite wordlength effects;Interpolation;Linear systems;Polynomials,computational electromagnetics;electromagnetic devices;interpolation;rational functions,adaptive multivariate rational data fitting;computational electromagnetic analysis;electromagnetic devices;interpolating rational function;metamodeling;truncation error,,11,3,23,,no,6-May,,IEEE,IEEE Journals & Magazines
An Analytic Solution of a Linear Camera Self-Calibration,Yang Guo; Zheng Fang; Xinhe Xu,"Department of Mathematics, Northeastern University, ShenYang, Liaoning Province, China. gy",2006 6th World Congress on Intelligent Control and Automation,20061023,2006,2,,9930,9934,"We propose an analytic solution for the self-calibration of five-parameter linear camera from three pairs of views of a scene, of four-parameter linear camera from two pairs of views and of two-parameter linear camera from one pairs of views based on Kruppa equations, respectively. The approach does not need any initial value, and can calculate accurate solution. The analysis was performed first by writing a suitable set of equations under introducing unknown scale factors from Kruppa's equations. Then, by an elimination scheme and symbolic computation, only one unknown scale factor was retained, so the equation set was reduced to a 30th order univariate polynomial equation for five-parameter or four-parameter camera, and a 3rd order univariate polynomial equation for two-parameter camera. After scale factors were attained, the camera intrinsic parameters were computed by a linear method. A numerical example was reported that confirms the theoretical results by ""Mathematica"" software",,POD:1-4244-0332-4,10.1109/WCICA.2006.1713938,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1713938,Analytic solution;Camera self-calibration;Kruppa&#8217;s equations;Sylvester&#8217;s dialytic method,Artificial intelligence;Calibration;Intelligent robots;Layout;Mathematics;Nonlinear equations;Optimization methods;Polynomials;Robot vision systems;Smart cameras,calibration;cameras;computer vision;matrix algebra;polynomials;symbol manipulation,Kruppa equations;Mathematica;Sylvester dialytic method;linear camera self-calibration;symbolic computation;univariate polynomial equation,,1,,15,,no,0-0 0,,IEEE,IEEE Conference Publications
An Approach for Intersubject Analysis of 3D Brain Images Based on Conformal Geometry,G. Zou; J. Hua; X. Gu; O. Muzik,"Department of Computer Science, Wayne State University",2006 International Conference on Image Processing,20070220,2006,,,1193,1196,"Recent advances in imaging technologies, such as magnetic resonance imaging (MRI), positron emission tomography (PET) and diffusion tensor imaging (DTI) have accelerated brain research in many aspects. In order to better understand the synergy of the many processes involved in normal brain function, integrated modeling and analysis of MRI, PET, and DTI across subjects is highly desirable. The current state-of-art computational tools fall short in offering an analytic approach for intersubject brain registration and analysis. In this paper we present an approach which is based on landmark constrained conformal parameterization of a brain surface from high-resolution structural MRI data to a canonical spherical domain. This model allows natural integration of information from co-registered PET as well as DTI data and lays a foundation for the quantitative analysis of the relationship among diverse datasets across subjects. Consequently, the approach can be extended to provide a software environment able to facilitate detection of abnormal functional brain patterns in patients with neurological disorder.",1522-4880;15224880,CD-ROM:1-4244-0481-9; POD:1-4244-0480-0,10.1109/ICIP.2006.312697,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4106749,Brain Image Analysis;Conformal Mapping;Information Integration;Registration,Brain mapping;Brain modeling;Computer science;Conformal mapping;Diffusion tensor imaging;Geometry;Image analysis;Magnetic analysis;Magnetic resonance imaging;Positron emission tomography,biomedical MRI;brain;computational geometry;image registration;image resolution;medical image processing;neurophysiology;positron emission tomography;statistical analysis,3D brain images;PET;abnormal functional brain pattern detection;canonical spherical domain;conformal geometry;diffusion tensor imaging;high-resolution structural MRI;image co-registration;intersubject analysis;landmark constrained conformal parameterization;magnetic resonance imaging;neurological disorder;positron emission tomography;quantitative analysis;statistical analysis,,5,,15,,no,8-11 Oct. 2006,,IEEE,IEEE Conference Publications
An Audio Information Hiding Algorithm Based on ICA,Rangding Wang; Qian Li; Diqun Yan,"CKC software lab, Ningbo University, Ningbo 315211. E-mail: wangrangding@nbu.edu.cn",2006 6th World Congress on Intelligent Control and Automation,20061023,2006,1,,4175,4179,"A novel audio information hiding technology combined with ICA (independent component analysis) and QIM (quantized index modulation) is proposed. ICA method is applied to audio signal processing to obtain the statistical independent sources as hiding channels. As ICA processing is sensitive to the only dividing matrix, security is guaranteed. A piece of meaningful speech signal is hidden in the carrier audio. Through analytic comparisons and experimental results, this algorithm is found to have a larger hiding capacity and a lower distortion to carrier audio, and also robust against a variety of common signal processing manipulations",,POD:1-4244-0332-4,10.1109/WCICA.2006.1713161,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1713161,ICA;QIM;information hiding;mutual information;nongaussianity,Algorithm design and analysis;Discrete cosine transforms;Discrete wavelet transforms;Distortion;Independent component analysis;Information security;Robustness;Signal analysis;Signal processing algorithms;Speech,audio signal processing;data encapsulation;independent component analysis;modulation;security of data,audio information hiding algorithm;audio signal processing;dividing matrix;independent component analysis;quantized index modulation;security,,0,,9,,no,0-0 0,,IEEE,IEEE Conference Publications
An infrastructure for automating information sharing in analytic collaboration,G. A. Mack; D. Fado; M. B. Blake; D. Widdows,"Adv. Syst. & Concepts, SAIC, Arlington, VA, USA",2006 IEEE Aerospace Conference,20060724,2006,,,11 pp.,,"In many parts of the intelligence community data gathers at rates that preclude the ability of analysts and policy makers to ingest, let alone comprehend, the complex information confronting them without working together in teams. These teams often form ad-hoc, mission-oriented communities of practice, and sometimes larger multi-mission communities of interest. These communities can be distributed across the globe. Within these communities analysts try to share their insights but often end up simply passing around partially processed information. This paper reports on developments toward a next generation collaboration supporting small groups of analysts (or policy makers) engaged in joint analysis and problem solving. The work reported in this paper ""automates what ought to be automated,"" differentiating between what machines do well and what humans do well. It describes an infrastructure that automates much of the information exchange between analysts while at the same time creating an environment that facilitates insight sharing among analysts in a community of interest",1095-323X;1095323X,POD:0-7803-9545-X,10.1109/AERO.2006.1656048,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1656048,,Buildings;Collaboration;Collaborative software;Collaborative work;Computer science;Drives;Humans;Information analysis;Intelligent agent;Problem-solving,government data processing;groupware;software agents,analytic collaboration;information exchange;information sharing;intelligence community;joint analysis;partially processed information;policy makers;problem solving;software agents,,0,,18,,no,0-0 0,,IEEE,IEEE Conference Publications
An Insight-Based Longitudinal Study of Visual Analytics,P. Saraiya; C. North; Vy Lam; K. A. Duca,"Dept. of Comput. Sci., Virginia Tech, Blacksburg, VA",IEEE Transactions on Visualization and Computer Graphics,20060918,2006,12,6,1511,1522,"Visualization tools are typically evaluated in controlled studies that observe the short-term usage of these tools by participants on preselected data sets and benchmark tasks. Though such studies provide useful suggestions, they miss the long-term usage of the tools. A longitudinal study of a bioinformatics data set analysis is reported here. The main focus of this work is to capture the entire analysts process that an analyst goes through from a raw data set to the insights sought from the data. The study provides interesting observations about the use of visual representations and interaction mechanisms provided by the tools, and also about the process of insight generation in general. This deepens our understanding of visual analytics, guides visualization developers in creating more effective visualization tools in terms of user requirements, and guides evaluators in designing future studies that are more representative of insights sought by users from their data sets",1077-2626;10772626,,10.1109/TVCG.2006.85,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1703371,Evaluation/methodology;Graphical User Interface (GUI);information visualization;visualization and methodologies.;visualization systems and software,Bioinformatics;Data analysis;Data visualization;Failure analysis;Graphical user interfaces;Particle measurements;Performance analysis;Software systems;Time measurement;Visual analytics,biology computing;data analysis;data visualisation;graphical user interfaces,bioinformatics data set analysis;insight-based longitudinal study;visual analytics;visualization tools,Algorithms;Computer Graphics;Humans;Information Storage and Retrieval;Longitudinal Studies;Software;Task Performance and Analysis;User-Computer Interface;Visual Perception,34,,47,,no,Nov.-Dec. 2006,,IEEE,IEEE Journals & Magazines
Analyses of elliptical coplanar coupled waveguides and coplanar coupled waveguides with finite ground width,M. Duyar; V. Akan; E. Yazgan; M. Bayrak,"Nigde Ind. & Trade Governer Dept., Turkey",IEEE Transactions on Microwave Theory and Techniques,20060410,2006,54,4,1388,1395,"In this paper, the quasi-TEM characteristic parameters of elliptical coplanar coupled waveguides and coupled coplanar waveguide with a finite ground width configuration are presented and analyzed. Computer-aided design (CAD)-oriented fast, simple, and accurate analytic formulas are derived by using conformal mapping techniques, which provide satisfactory accuracy at microwave frequencies and lead to closed-form analytical solutions suitable for CAD software packages. The results for the odd- and even-mode characteristic impedance, effective dielectric constant, and coupling coefficient have been computed by these formulas. Good agreement between the present results and published results is observed. For the planar case, simulations have also been undertaken with Sonnet electromagnetic-circuit solver software. The computed results agree well with those of the simulation ones.",0018-9480;00189480,,10.1109/TMTT.2006.871354,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1618555,Computer-aided design (CAD) oriented;conformal mapping;coplanar coupled waveguides (CCPWs);elliptical CCPWs,Computational modeling;Conformal mapping;Coordinate measuring machines;Coplanar transmission lines;Coplanar waveguides;Coupling circuits;Design automation;Distributed parameter circuits;Electromagnetic waveguides;Planar transmission lines,conformal mapping;coplanar waveguides;electronic design automation,Sonnet electromagnetic-circuit solver;characteristic impedance;closed-form analytical solutions;computer-aided design;conformal mapping;coupling coefficient;effective dielectric constant;elliptical coplanar coupled waveguides;finite ground width;quasiTEM characteristic parameters,,4,,14,,no,6-Jun,,IEEE,IEEE Journals & Magazines
Analyzing Actors and Their Discussion Topics by Semantic Social Network Analysis,P. A. Gloor; Yan Zhao,"MIT & Center for Digital Strategies at Tuck at Dartmouth,iQuest Analytics",Tenth International Conference on Information Visualisation (IV'06),20060724,2006,,,130,135,"iQuest is a novel software system to improve understanding of organizational phenomena with greater precision, clarity, and granularity than has previously been possible. It permits to gain new insights into organizational behavior, addressing issues such as tracking information while respecting privacy, comparing different interaction channels, network membership, and correlating organizational performance and creativity. It extends automatic visualization of social networks by mining communication archives such as e-mail and blogs through including analysis of the contents of those archives",1550-6037;15506037,POD:0-7695-2602-0,10.1109/IV.2006.23,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1648252,SNA;automatic communication analysis;semantic social network analysis;surface;temporal social;vector space information retrieval,Blogs;Data visualization;Electronic mail;Functional analysis;Information analysis;Joining processes;Neural networks;Privacy;Social network services;Software systems,data mining;data privacy;data visualisation;electronic mail;information networks;information retrieval;organisational aspects;semantic networks,actors analysis;automatic communication analysis;blogs;communication archive mining;e-mail;iQuest;organizational behavior;organizational phenomena;semantic social network analysis;social networks visualization;software system;temporal social surface;vector space information retrieval,,17,,11,,no,5-7 July 2006,,IEEE,IEEE Conference Publications
Applying Multiobjective Compromise Approach to Exploring the Strategic Plan of Fuel Cell Technology,H. k. Chiou; G. h. Tzeng; C. c. Wan,"Department of Statistics, National Defense University, Chunghe, 235, Taiwan",2006 Technology Management for the Global Future - PICMET 2006 Conference,20070129,2006,2,,593,600,"The fuel cell is one of the most important energy products in the 21st century, and most industrial-advanced countries in the world are placing high expectation on its development. The applications of fuel cells in Taiwan has spread broadly in many products and it takes high global market share, such as notebook computers, PDAs and digital cameras, along with items like automobiles, motorcycles and power generation equipment. In this study we introduce a multi-objective compromise optimization to solve the optimal strategic plans of fuel cell industry, which is emerging industry in Taiwan. In the first step, we introduce popular analytic hierarchy process with fuzzy set theory to establish the hierarchy system to determine the relative importance of evaluated criteria. Secondly, fuzzy geometric mean method was utilized to aggregate the performance score by individual judgment of evaluator, which scores express the measurement of strategic plan proposed by participated experts. Thirdly, the synthetic value of each strategic plan was integrated by criteria weights with performance score, and positive ideal solution (PIS) and negative ideal solution (NIS) were defined by Minkowski's distance metric function. Fourthly, the optimal compromise solution will be derived which should have the shortest distance from the positive ideal solution as well as the farthest distance from the negative ideal solution. Along with the results of this research, we successfully demonstrate that the multi-objective compromise optimization is a good alternative for evaluation of multiple criteria decision making problems",2159-5100;21595100,POD:1-890843-14-8,10.1109/PICMET.2006.296658,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4077435,,Application software;Automobiles;Digital cameras;Fuel cells;Fuel processing industries;Fuzzy set theory;Globalization;Motorcycles;Personal digital assistants;Power generation,decision making;fuel cells;fuzzy set theory;globalisation;industrial economics;optimisation;research and development;strategic planning,Minkowski distance metric function;PDAs;Taiwan;analytic hierarchy process;automobiles;decision making;digital cameras;energy product;fuel cell technology;fuzzy geometric mean method;fuzzy set theory;global market share;industrial-advanced countries;motorcycles;negative ideal solution;notebook computers;optimization;positive ideal solution;power generation equipment;strategic planning,,0,,17,,no,6-Jul,,IEEE,IEEE Conference Publications
Autonomic Replication of Management Data Evaluation of a Market-based Approach,R. Brennan; G. O'Gorman; C. Doherty; N. Hurley; C. McArdle,"NMRC, Ericsson Ireland R&D, Dublin, Ireland",2006 IEEE/IFIP Network Operations and Management Symposium NOMS 2006,20061023,2006,,,1,4,"This paper describes work in progress whereby a dynamic data replication scheme, under market-based control is applied to a proposed autonomic distributed data layer for managing configuration management data. The scope of the proposed autonomic system is described and also some experimental work are presented. Analytic approximations of the performance achieved for management requests under various static data replication schemes are compared with event-based simulations of the same system under dynamic market-based replication control. The purpose of this comparison is to evaluate the performance and suitability of a market-based control approach for such autonomic replication systems",1542-1201;15421201,POD:1-4244-0142-9,10.1109/NOMS.2006.1687631,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1687631,Autonomic;Control;Modelling;Replication;Simulation,Analytical models;Application software;Control systems;Discrete event simulation;Engineering management;Optimal control;Research and development management;Resource management;Telecommunication control;Telecommunication network management,configuration management;telecommunication control;telecommunication network management,analytic approximations;autonomic distributed data layer;autonomic replication;configuration management data;dynamic data replication scheme;dynamic market-based replication control;event-based simulations;management requests;static data replication schemes,,1,,15,,no,3-7 April 2006,,IEEE,IEEE Conference Publications
Avian Flu Case Study with nSpace and GeoTime,P. Proulx; S. Tandon; A. Bodnar; D. Schroh; R. Harper; W. Wright,"Oculus Info Inc., email: pascale.proulx@oculusinfo.com",2006 IEEE Symposium On Visual Analytics Science And Technology,20061226,2006,,,27,34,"GeoTime and nSpace are new analysis tools that provide innovative visual analytic capabilities. This paper uses an epidemiology analysis scenario to illustrate and discuss these new investigative methods and techniques. In addition, this case study is an exploration and demonstration of the analytical synergy achieved by combining GeoTime's geo-temporal analysis capabilities, with the rapid information triage, scanning and sense-making provided by nSpace. A fictional analyst works through the scenario from the initial brainstorming through to a final collaboration and report. With the efficient knowledge acquisition and insights into large amounts of documents, there is more time for the analyst to reason about the problem and imagine ways to mitigate threats. The use of both nSpace and GeoTime initiated a synergistic exchange of ideas, where hypotheses generated in either software tool could be cross-referenced, refuted, and supported by the other tool",,CD-ROM:1-4244-0592-0; POD:1-4244-0591-2,10.1109/VAST.2006.261427,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4035744,geo-spatial information systems;human information interaction;information visualization;sense making;temporal analysis;user centered design;visual analytics,Collaborative work;Graphical user interfaces;Influenza;Information analysis;Information retrieval;NIST;Performance analysis;Time series analysis;Visual analytics;Visualization,data visualisation;diseases;graphical user interfaces,GeoTime analysis tool;analytical synergy;avian flu case study;epidemiology analysis;geo-temporal analysis;information triage;knowledge acquisition;nSpace analysis tool;visual analytics,,7,,22,,no,Oct. 31 2006-Nov. 2 2006,,IEEE,IEEE Conference Publications
Broadening Our Collaboration with Design,B. Watson,"North Carolina State Univ., Raleigh, NC",IEEE Computer Graphics and Applications,20060828,2006,26,5,18,21,"Computer graphics researchers have been collaborating successfully with engineers, architects, and artists for decades, focusing on better tools for model and image creation. Graphics researchers have already developed a wide range of procedural (automatic) modeling techniques, but with few exceptions, these focus on modeling natural objects, such as plants, terrains, and water. The next generation of tools must automate modeling of the most common and complex elements of digital content: manmade artifacts such as cities, buildings, vehicles, and furniture. Creating these tools require a new and close collaboration with architects as well as urban and industrial designers",0272-1716;02721716,,10.1109/MCG.2006.99,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1683688,architecture;art;graphic design;industrial design;information visualization;procedural modeling;urban design;visual analytics;visual design,Biomedical imaging;Collaboration;Collaborative tools;Collaborative work;Computer graphics;Data visualization;Design automation;Displays;Internet;World Wide Web,CAD;data visualisation;groupware;software tools;solid modelling,architectural design;automatic modeling tools;computer graphics modeling tools;data visualization;design collaboration;image creation tool;industrial design;urban design,,1,,9,,no,Sept.-Oct. 2006,,IEEE,IEEE Journals & Magazines
Case Retrieving Based on Grey Incidence Degree in CBR,Chuanmin Mi; Sifeng Liu; Y. Dang; Zhigeng Fang,"Nanjing University of Aeronautics and Astronautics, College of Economics and Management, Nanjing 210016. E-mail: michuanmin@163.com",2006 6th World Congress on Intelligent Control and Automation,20061023,2006,1,,4387,4391,"Based on the characteristic of grey system theory, which is an effective method to solve uncertain problems, we used grey incidence analytic method into case retrieving of CBR. We took grey incidence degree as a tool for measuring the similarity degree of case. Grey predominance analysis was applied for choosing the most suitable case. If there was no strict suitable case, AHP (analytic hierarchy process) method was used to calculate weight of different indices, then we got synthetical grey incidence degree, ascertain synthetical most similar case. A CBR framework based on grey incidence degree was built based on grey incidence analytic method. Finally, an example was presented. The results show that the method is simple and practical; the model provides an effective method to case retrieving",,POD:1-4244-0332-4,10.1109/WCICA.2006.1713206,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1713206,CBR;case retrieving;grey incidence degree,Automation;Computer aided software engineering;Extraterrestrial measurements;Intelligent control,case-based reasoning;grey systems;information retrieval,analytic hierarchy process;case based reasoning;case retrieval;grey incidence analytic method;grey incidence degree;grey predominance analysis;grey system theory,,0,,17,,no,0-0 0,,IEEE,IEEE Conference Publications
Challenges in Visual Data Analysis,D. A. Keim; F. Mansmann; J. Schneidewind; H. Ziegler,"University of Konstanz, Germany",Tenth International Conference on Information Visualisation (IV'06),20060724,2006,,,9,16,"In today's applications data is produced at unprecedented rates. While the capacity to collect and store new data grows rapidly, the ability to analyze these data volumes increases at much lower pace. This gap leads to new challenges in the analysis process, since analysts, decision makers, engineers, or emergency response teams depend on information ""concealed"" in the data. The emerging field of visual analytics focuses on handling massive, heterogenous, and dynamic volumes of information through integration of human judgement by means of visual representations and interaction techniques in the analysis process. Furthermore, it is the combination of related research areas including visualization, data mining, and statistics that turns visual analytics into a promising field of research. This paper aims at providing an overview of visual analytics, its scope and concepts, and details the most important technical research challenges in the field",1550-6037;15506037,POD:0-7695-2602-0,10.1109/IV.2006.31,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1648235,,Application software;Data analysis;Data engineering;Data mining;Data visualization;Humans;Information analysis;Space exploration;Statistical analysis;Visual analytics,data analysis;data mining;data visualisation;statistics,data mining;statistics;visual analytics;visual data analysis;visual representations,,45,,15,,no,5-7 July 2006,,IEEE,IEEE Conference Publications
Checkpoint Placement Algorithms for Mobile Agent System,J. Yang; J. Cao; W. Wu,"Hong Kong Polytechnic University, Hong Kong",2006 Fifth International Conference on Grid and Cooperative Computing (GCC'06),20061219,2006,,,339,346,"Checkpointing is a fault tolerance technique widely used in various types of computer systems. In checkpointing, an important issue is how to achieve a good trade-off between the recovery cost and the system performance. Excessive checkpointing would result in the performance degradation due to the high costly I/O operations during checkpointing. Equidistant and equicost are two well-known checkpointing strategies for addressing this issue. However, there is no study on these strategies catering for a mobile agent (MA) system, which has different characteristics with conventional systems. In this paper, based on an analysis of the behaviours of an MA system, we find that it can be modelled as a homogeneous discrete-parameter Markov chain, which is different from the models used in conventional systems. Therefore, the analytic methods and corresponding results for conventional systems cannot be adopted directly for an MA system. Based on our proposed model, we study the equidistant and equicost checkpointing strategies and propose checkpoint placement algorithms for MA systems. Through simulations we evaluate the performance of our proposed algorithms and the result shows that the equicost strategy based algorithm is most suitable for an MA system",2160-4908;21604908,POD:0-7695-2694-2,10.1109/GCC.2006.35,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4031479,,Checkpointing;Computational modeling;Costs;Degradation;Distributed computing;Fault tolerant systems;Internet;Mobile agents;Mobile computing;System performance,Markov processes;checkpointing;mobile agents;software fault tolerance,checkpoint placement;discrete-parameter Markov chain;equicost checkpointing;equidistant checkpointing;fault tolerance;mobile agent system;recovery cost;system performance,,0,,20,,no,Oct. 2006,,IEEE,IEEE Conference Publications
Connectionless Quality-of-Service Routing Framework,Y. Zhao; T. Zhang; Y. Cui,"Department of Computer Science and Technology, Tsinghua University, Beijing China. E-mail: zhaoyj@csnet1.cs.tsinghua.edu.cn","The Proceedings of the Multiconference on Computational Engineering in Systems Applications""""",20070730,2006,1,,945,952,"Next generation networks call for novel network architectures in order to support QoS-oriented applications. However, existing QoS architectures mostly suffer from the unscalability problem. In this paper we propose a novel connectionless QoS routing framework for high-speed core networks. Unlike traditional QoS architectures, neither resource reservation nor per-flow management exists in the proposed routing framework, so it has good scalability. In this framework, QoS constraints are carried by every packet and routing decisions are made hop by hop at the packet level. We also establish an analytic QoS model and a simulation model to acquire performance evaluation for the connectionless QoS routing framework. Both the analysis and simulations show that this framework achieves satisfying performance for any underlying type of QoS-oriented services",,CD-ROM:7-900718-14-1; POD:7-302-13922-9,10.1109/CESA.2006.4281786,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4281786,Connectionless;Framework;QoSR;Routing,Analytical models;Application software;Computer architecture;Diffserv networks;Performance analysis;Quality of service;Resource management;Routing;Scalability;Web and internet services,quality of service;telecommunication network routing,connectionless quality-of-service routing framework;network architecture;next generation networks;unscalability problem,,0,,29,,no,4-6 Oct. 2006,,IEEE,IEEE Conference Publications
Container Terminal Operation Emulation system and application in Optimizing of Terminal Plane Layout Scheme,Y. Wuyuan,"College of Logistics Engineering, Wuhan University of Technology, Wuhan 430063. E-mail: yanwuyuan@yahoo.com",2006 Chinese Control Conference,20070115,2006,,,1760,1764,"Aimed at the complicated system engineering of plane layout scheme design of container terminal, and based on the eM-plant distribute software have set up container terminal operating emulation modules, such as berth module, gate entrance module, storage module and statistic module. Have set up the comprehensive evaluation system of plane layout, and the right of every index was confirmed by analytic hierarchy process method and expert consult method synthetically. By analyses the level of every index value that got from the emulation results, can get the comprehensive valuation result. Have carried on intelligence optimized research by using VR-UC GA algorithm on the overall arrangement scheme, and applied to a certain container terminal program project, have gotten relatively satisfactory result, offered strong support for decision of container terminal's plane layout scheme.",1934-1768;19341768,CD-ROM:7-900669-88-4; POD:7-81077-802-1,10.1109/CHICC.2006.280849,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4060397,Comprehensive Evaluation;Container Terminal;Optimizing;Plane Layout Scheme;eM-plant Emulation,Containers;Cranes;Design engineering;Emulation;Land vehicles;Marine vehicles;Road vehicles;Space vehicles;Statistics;Systems engineering and theory,containers;decision support systems;production engineering computing,analytic hierarchy process;berth module;certain container terminal program project;complicated system engineering;comprehensive valuation result;container terminal operation emulation system;container terminal plane layout;decision support;eM-plant distribute software;expert consult method;gate entrance module;intelligence optimized research;plane layout scheme design;statistic module;storage module;terminal plane layout scheme;using VR-UC GA algorithm,,1,,3,,no,7-11 Aug. 2006,,IEEE,IEEE Conference Publications
Controlling Quality of Service in Multi-Tier Web Applications,Yixin Diao; J. L. Hellerstein; S. Parekh; H. Shaikh; M. Surendra,"IBM Thomas J. Watson Research Center, NY",26th IEEE International Conference on Distributed Computing Systems (ICDCS'06),20060724,2006,,,25,25,"The need for service differentiation in Internet services has motivated interest in controlling multi-tier web applications. This paper describes a tier-to-tier (T2T) management architecture that supports decentralized actuator management in multi-tier systems, and a testbed implementation of this architecture using commercial software products. Based on testbed experiments and analytic models, we gain insight into the value of coordinated exploitation of actuators on multiple tiers, especially considerations for control efficiency and control granularity. For control efficiency, we show that more effective utilization of tiers can be achieved by using actuators on the bottleneck tier rather than only using actuators on the entry tier. For granularity of control (the ability to achieve a wide range of service level objectives) we show that a fine granularity of control can be achieved through a coordinated, cross-tier exploitation of coarse grained actuators (e.g., multiprogramming level), an approach that can greatly reduce controllerinduced variability.",1063-6927;10636927,POD:0-7695-2540-7,10.1109/ICDCS.2006.23,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1648812,,Actuators;Computer architecture;Databases;Delay;Network servers;Quality of service;Routing;Web and internet services;Web server;Yarn,,,,11,,10,,no,2006,,IEEE,IEEE Conference Publications
Designing a Genetic Algorithm for Function Approximation for Embedded and ASIC Applications,J. W. Hauser; C. N. Purdy,"Department of Computer Science, Northern Kentucky University, Highland Heights, KY 41099 USA. hauserj@nku.edu",2006 49th IEEE International Midwest Symposium on Circuits and Systems,20070709,2006,2,,555,559,"In embedded systems and application specific integrated circuits (ASICs) that typically do not have a floating-point processor, measured data or function-sampled data is commonly described by an analytic function derived using standard numerical methods. The resultant errors are not caused by rounding but by translating a real solution to a restricted fixed-point environment. We have previously described a genetic algorithm that discovers a superior piece-wise polynomial approximation with coefficients restricted to the integer target space. In this paper we discuss details of the genetic algorithm implementation.",1548-3746;15483746,CD-ROM:1-4244-0173-9; POD:1-4244-0172-0,10.1109/MWSCAS.2006.381790,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4267414,,Algorithm design and analysis;Application software;Application specific integrated circuits;Circuit analysis computing;Computer science;Embedded computing;Function approximation;Genetic algorithms;Neural networks;Polynomials,application specific integrated circuits;embedded systems;floating point arithmetic;function approximation;genetic algorithms;integrated circuit design;piecewise polynomial techniques,ASIC;analytic function;application specific integrated circuits;embedded systems;fixed-point environment;function approximation;genetic algorithm;integer target space;optimal integer polynomial coefficients;piece-wise polynomial approximation;standard numerical methods,,0,,15,,no,6-9 Aug. 2006,,IEEE,IEEE Conference Publications
Dynamic Test Composition In Hierarchical Software Testing,B. Bergelson; I. Exman,"Software Engineering Dept., Jerusalem College of Engineering, Jerusalem &#191; POB 3566 - 91035 - Israel. bennyber@jce.ac.il",2006 IEEE 24th Convention of Electrical & Electronics Engineers in Israel,20070226,2006,,,37,41,"Testing of large software systems is essential to assure the desired system reliability, but it involves large amounts of computer resources. It turns out that working in mid-range test granularity obtains very regular hierarchical test graphs, implying significant savings of testing-time, by dynamic generation of the strictly necessary tests. We formulate general analytic expressions for time savings and obtain the important result that savings increases with system size. The approach was demonstrated by actually implementing a testing system for large bundles of content files.",,CD-ROM:1-4244-0230-1; POD:1-4244-0229-8,10.1109/EEEI.2006.321078,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4115241,dynamic tests;hierarchical testing;mid-range granularity;software composition;subset graphs,Computer languages;Educational institutions;Electronic equipment testing;Reliability engineering;Software design;Software engineering;Software standards;Software systems;Software testing;System testing,,,,0,,9,,no,15-17 Nov. 2006,,IEEE,IEEE Conference Publications
Efficient continuous collision detection for bounding boxes under rational motion,Dan Albocher; U. Sarel; Yi-King Choi; G. Elber; Wenping Wang,"Dept. of Comput. Sci., Technion-Israel Inst. of Technol., Haifa","Proceedings 2006 IEEE International Conference on Robotics and Automation, 2006. ICRA 2006.",20060626,2006,,,3017,3022,"This paper presents a simple yet precise and efficient algorithm for collision prediction of two oriented bounding boxes under univariate (piecewise) rational motion. We present an analytic solution to the problem of finding the time of collision and the feature involved, or declaring that no collision should occur. Our solution can be applied to boxes of any size, under arbitrary rational rigid motion. The algorithm is based on the efficient examination of the Minkowski sum (MS) of the two boxes, using a spherical Gauss map dual representation, and a precise extraction of the collision time, if any, as a solution to a set of rational equations that are automatically derived",1050-4729;10504729,POD:0-7803-9505-0,10.1109/ROBOT.2006.1642160,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1642160,,Algorithm design and analysis;Application software;Charge coupled devices;Computer science;Equations;Gaussian processes;Motion analysis;Motion detection;Prediction algorithms;Robot motion,collision avoidance;mobile robots;robot vision,Minkowski sum;bounding boxes;continuous collision detection;mobile robots;rational motion;robot vision;spherical Gauss map dual representation,,4,,26,,no,15-19 May 2006,,IEEE,IEEE Conference Publications
Efficient Data Analysis with Modern Analytical System Developed for Use in Photovoltaic (PV) Monitoring System,W. Kolodenny,"SolarLab, Faculty of Microsystems Electronics and Photonics, Wroclaw University of Technology, Wroclaw, Poland. wlodzimierz.kolodenny@pwr.wroc.pl",2006 International Students and Young Scientists Workshop - Photonics and Microsystems,20070410,2006,,,26,29,"Outdoor data acquisition systems (DAS) designed for long term monitoring of photovoltaic (PV) modules provide wide range of measurement conditions. Measured data contain modules' electrical parameters extracted from I-V curves and meteorological conditions occurring during measurements. Every year measurement volume of data systematically increases making analysis more complex and difficult. To cope with such large amount of data efficient database together with analytical tools is essential. This paper presents a set of analytic tools implemented with The SAS Institute Software (USA) as online analytical processing (OLAP) website. Data are available for analysis via SolarLab's website for authorized users. Software provides multidimensional data filtering, data visualization and different types of regression. Additionally, summarized data, like periodical energy gain or insolation, can be viewed and drilled down to single measurements. Efficient data integration and data warehouse tools enable efficient analysis of data measured in different laboratories. Internet web browser interface simplifies multilaboratory cooperation in data analysis and does not require any additional software to be installed",1939-4381;19394381,CD-ROM:1-4244-0393-6; POD:1-4244-0392-8,10.1109/STYSW.2006.343663,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4149602,,Condition monitoring;Data acquisition;Data analysis;Data mining;Electric variables measurement;Meteorology;Photovoltaic systems;Solar power generation;Visual databases;Volume measurement,Internet;computerised monitoring;data acquisition;data analysis;data visualisation;data warehouses;photovoltaic power systems;power engineering computing;power system measurement;user interfaces,I-V curves;Internet web browser interface;SAS Institute Software;SolarLab website;analytic tools;data analysis;data integration;data visualization;data warehouse tools;electrical parameters;meteorological conditions;multidimensional data filtering;online analytical processing website;outdoor data acquisition systems;photovoltaic monitoring system;regression types,,1,,3,,no,June 30 2006-July 2 2006,,IEEE,IEEE Conference Publications
"Electronics at Nanoscale: Fundamental and Practical Challenges, and Emerging Directions",S. Tiwari; A. Kumar; C. C. Liu; H. Lin; S. K. Kim; H. Silva,"School of Electrical and Computer Engineering, Cornell University, Ithaca, NY 14853",2006 IEEE Conference on Emerging Technologies - Nanoelectronics,20060327,2006,,,481,486,"In electronics, i.e. when using charge transport and change of electromagnetic fields in devices and systems, non-linearity, collective effects, and a hierarchy of design across length and time scales is central to efficient information processing through manipulation and transmission of bits. Silicon-based electronics brings together a systematic interdependent framework that connects software and hardware to reproducibility, speed, power, noise margin, reliability, signal restoration and communication, low defect count, and an ability to do predictive design across the scales. In the limits of nanometer scale, the dominant practical constraints arise from power dissipation in ever smaller volumes and of efficient signal interconnectivity commensurate with the large density of devices. These limitations are tied to the physical basis in charge transport and changes of fields, and equally apply to other materials ‰ÛÒ hard, soft or molecular. At the largest scale, the limitations arise from partitioning and hierarchical apportionment for system performance, ease of design and manufacturing. Power management, behavioral encapsulation, fault tolerance, congestion avoidance, timing, placement, routing, electromagnetic cross-talk, etc. all need to be addressed from the perspective of centimeter scale. We take a hierarchical view of the underlying fundamental and practical challenges of the conventional and unconventional approaches using the analytic framework appropriate to the length scale to distinguish between fact and fantasy, and to point to practical emerging directions with a system-scale perspective.",2159-3523;21593523,POD:0-7803-9357-0,10.1109/NANOEL.2006.1609776,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1609776,,,,,,5,,24,,no,10-13 Jan. 2006,,IEEE,IEEE Conference Publications
Evaluation Models for E-Learning Platform: an AHP approach,F. Colace; M. D. Santo; A. Pietrosanto,"Dipt. di Ingegneria dell''Informazione e Ingegneria Elettrica, Univ. degli Studi di Salerno, Fisciano",Proceedings. Frontiers in Education. 36th Annual Conference,20070305,2006,,,1,6,"Our ""information-oriented"" society shows an increasing exigency of life-long learning. In such framework, the e-learning approach is becoming an important tool to allow the flexibility and quality requested by such a kind of learning process. In the recent past, a great number of online platforms have been introduced on the market showing different characteristics and services. With a plethora of e-learning providers and solutions available in the market, there is a new kind of problem faced by organizations consisting in the selection of the most suitable e-learning suite. This paper proposes a model for describing, characterizing and selecting e-learning platform. The e-learning solution selection is a multiple criteria decision-making problem that needs to be addressed objectively taking into consideration the relative weights of the criteria for any organization. We formulate the quoted multi criteria problem as a decision hierarchy to be solved using the analytic hierarchy process (AHP). In this paper we will show the general evaluation strategy and some obtained results using our model to evaluate some existing commercial platforms",0190-5848;01905848,CD-ROM:1-4244-0257-3; POD:1-4244-0256-5,10.1109/FIE.2006.322312,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4116906,E-Learning;E-Learning Platform;Multiple Criteria Decision Making Problem,Context modeling;Decision making;Education;Electronic learning;Globalization;Information technology;Internet;Software packages;Standards development;Usability,computer aided instruction,analytic hierarchy process;decision hierarchy;e-learning;evaluation model;life-long learning;multiple criteria decision-making,,5,,15,,no,27-31 Oct. 2006,,IEEE,IEEE Conference Publications
Games Theory and Software Defined Radios,S. J. Silverman,"Raytheon, Network Centric Systems, 1801 W Hughes Drive, Fullerton, CA 92834. steven",MILCOM 2006 - 2006 IEEE Military Communications conference,20070212,2006,,,1,7,"The use of game theory in software designed radio networks is studied using OPNET. In order to model the spectrum usage, radio frequency interference avoidance, and distributed radio resource management, the behavior of software defined radios are predicted using game theory in an analytic mathematical framework. The wireless networks consist of devices that dynamically reconfigure themselves to respond to any air-interface or data format. Interfering radios however affect the adaptation schemes used on a link by link basis. Cognitive radio is an enhancement on traditional software radio design that can help establish a design model. Cognitive radios employ a cognition cycle to alter their actions in response to changes in the environment through the use of state machines. A smart network presents particularly difficult challenges to the analysis of radio resource management, as changes that one node makes may influence the decisions that other nodes make, so network planning remains a difficult task. The radios are the players in the game; the strategies in the game are based on the transmitter choosing the value of the adaptive link characteristic that maximizes the spectral efficiency, while meeting a BER constraint. The goal of the game is to maximize your winnings. Here the winnings would be for a radio to accrue the most bandwidth, or in our case, to achieve a particular performance target. Additionally, these approaches rest upon the assumption of higher-order rationality, i.e. the ability of a node to independently and recursively analyze a best response to other network nodes OPNET's pipeline processing can provide a solution path unavailable with other tools. We use OPNET to model changes in waveform appearance due to the interfering effects of neighboring communication links. In order to successfully model these networks, it will be necessary to determine if the network will eventually reach a steady state. With a game theoretic analysis, these network s- - teady states can be identified from the Nash Equilibriums of its associated game. In other traditional approaches this is not possible and demonstrates the more rapid convergence of the game results to modeling and simulation",2155-7578;21557578,CD-ROM:1-4244-0618-8; POD:1-4244-0617-X,10.1109/MILCOM.2006.302490,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4086382,Games Theory;Prisoners Dilemma;Software Defined Radios,Cognitive radio;Game theory;Mathematical model;Predictive models;Radio network;Radiofrequency interference;Resource management;Software design;Software radio;Wireless networks,game theory;interference suppression;radio spectrum management;radiofrequency interference;software radio,OPNET pipeline processing;communication link;distributed radio resource management;games theory;radio frequency interference avoidance;software defined radio,,0,,6,,no,23-25 Oct. 2006,,IEEE,IEEE Conference Publications
Have Green - A Visual Analytics Framework for Large Semantic Graphs,P. C. Wong; G. Chin; H. Foote; P. Mackey; J. Thomas,"Pacific Northwest National Laboratory, Email: pak.wong@pnl.gov",2006 IEEE Symposium On Visual Analytics Science And Technology,20061226,2006,,,67,74,"A semantic graph is a network of heterogeneous nodes and links annotated with a domain ontology. In intelligence analysis, investigators use semantic graphs to organize concepts and relationships as graph nodes and links in hopes of discovering key trends, patterns, and insights. However, as new information continues to arrive from a multitude of sources, the size and complexity of the semantic graphs will soon overwhelm an investigator's cognitive capacity to carry out significant analyses. We introduce a powerful visual analytics framework designed to enhance investigators' natural analytical capabilities to comprehend and analyze large semantic graphs. The paper describes the overall framework design, presents major development accomplishments to date, and discusses future directions of a new visual analytics system known as Have Green",,CD-ROM:1-4244-0592-0; POD:1-4244-0591-2,10.1109/VAST.2006.261432,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4035749,"1.6.9 [Visualization] - Information Visualization, Visualization Systems and Software, Visualization Techniques Methodologies;Graph and Network Visualization;Information Analytics;Information Visualization;Visual Analytics",Computer displays;Fuses;Information analysis;Laboratories;Ontologies;Pattern analysis;Performance analysis;Software systems;Visual analytics;Visualization,data visualisation;semantic networks,Have Green;graph visualization;information analytics;information visualization;network visualization;semantic graph;visual analytics framework,,7,1,42,,no,Oct. 31 2006-Nov. 2 2006,,IEEE,IEEE Conference Publications
Identification of Software Specifications through Quality Function Deployment,R. C. Vlad; L. Benyoucef; S. Vlad,"Technical University of Cluj-Napoca, Radu.Constantin.Vlad@mis.utcluj.ro","2006 IEEE International Conference on Automation, Quality and Testing, Robotics",20061211,2006,2,,74,79,"The present paper describes the procedure that has been used to determine the software specifications for a simulation component of an information platform. The procedure has been designed and implemented within the GRailChem project. The initiative was funded under a French-German scheme of cooperation and aimed to determine whether the intended information platform could actually improve the effectiveness and the efficiency of the freight transportation between Germany and France. The suggested procedure follows the general steps of the quality function deployment to translate user requirements into system specifications, and later on, into simulation specifications. It also uses the analytic hierarchy process to decrease the subjectivity of the decisions made during the systems analysis process",,CD-ROM:1-4244-0361-8; POD:1-4244-0360-X,10.1109/AQTR.2006.254603,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4022926,,Application software;Chemical elements;Chemical hazards;Computer architecture;Europe;Information analysis;Manufacturing industries;Quality function deployment;Rail transportation;Software quality,chemical industry;formal specification;freight handling;quality function deployment;transportation,GRailChem project;analytic hierarchy process;freight transportation;quality function deployment;simulation component;simulation specifications;software specifications;system specifications,,1,,10,,no,25-28 May 2006,,IEEE,IEEE Conference Publications
Information seeking in academic learning environments: an exploratory factor analytic approach to understanding design features,D. H. l. Goh; S. S. b. Foo; Y. l. Theng; S. s. Lee,"Nanyang Technological University, Singapore",Proceedings of the 6th ACM/IEEE-CS Joint Conference on Digital Libraries (JCDL '06),20070305,2006,,,335,335,"Traditional information retrieval (IR) systems perform retrieval by ""closely matching"" a query to a set of documents objectively without considering users' contexts. We aim to enhance objective relevance and address its limitations by taking a quantitative, subjective relevance (SR) approach. SR provides suitable theoretical underpinnings as it focuses on a document's relevance for users' needs. Our present work builds on an initial study where features supporting students' evaluations of subjective relevance of documents were elicited. Using digital libraries as an example of IR systems, we designed a survey form based on elicited SR features and conducted a quantitative, pilot study with 465 university students. The pilot study was exploratory and aimed to understand students' preferences for features as they completed a task in a digital library. Exploratory factor analysis (EFA) was used to analyze the data as it removed redundant features and identified relationships so that groups of important features supporting students' IR interactions could be discovered to provide rationale for designing IR interfaces supporting SR",,POD:1-59593-354-9,10.1145/1141753.1141831,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4119151,exploratory factor analysis;information retrieval;interface design;subjective relevance,Data mining;Feature extraction;Human factors;Information analysis;Information management;Information retrieval;Software libraries;Strontium;User centered design;User interfaces,academic libraries;digital libraries;information retrieval systems;relevance feedback;user interface management systems,academic learning environment;digital library;exploratory factor analytic approach;information retrieval system;information seeking;subjective relevance;user interface design,,0,,4,,no,11-15 June 2006,,IEEE,IEEE Conference Publications
InterJoin: Exploiting Indexes and Materialized Views in XPath Evaluation,D. Phillips; Ning Zhang; I. F. Ilyas; M. T. Ozsu,"University of Waterloo, Canada",18th International Conference on Scientific and Statistical Database Management (SSDBM'06),20060724,2006,,,13,22,"XML has become the standard for data exchange for a wide variety of applications, particularly in the scientific community. In order to efficiently process queries on XML representations of scientific data, we require specialized techniques for evaluating XPath expressions. Exploiting materialized views in query processing significantly enhances query processing performance. We propose a novel view definition that allows for intermediate (structural) join results to be stored and reused in XML query evaluation. Unlike current XML view proposals, our views do not require navigation in the original document or path-based pattern matching. Hence, they are evaluated significantly faster and are easily costed as part of a query plan. In general, current structural joins cannot exploit views efficiently when the view definition is not a prefix (or a suffix) of the XPath query. To increase the applicability of our proposed view definition, we propose a novel physical structural join operator called InterJoin. The InterJoin operator allows for joining interleaving XPath expressions, e.g., joining //A//C with //B to evaluate //A//B//C. InterJoin allows for more join alternatives in XML query plans. We propose several physical implementations for InterJoin, including a technique to exploit spatial indexes on the inputs. We give analytic cost models for the implementations so they can be costed in an existing XML query optimizer. Experiments on real and synthetic XML data show significant speed-ups of up to 200% using InterJoin, and speed-ups of up to 400% using our materialized views",1551-6393;15516393,POD:0-7695-2590-3,10.1109/SSDBM.2006.31,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1644293,,Application software;Books;Computer science;Interleaved codes;Navigation;Pattern matching;Proposals;Query processing;Spatial indexes;XML,XML;database indexing;query processing;scientific information systems;tree data structures,InterJoin;XML query evaluation;XML query optimizer;XPath evaluation;analytic cost models;data exchange;join operator;materialized views;path-based pattern matching;query processing;scientific community;spatial indexes,,2,,19,,no,0-0 0,,IEEE,IEEE Conference Publications
Kinematic scheme design based on design catalogue,D. Peisong; D. Ruicheng; Y. Shandong,"Shandong Univ. of Technol., Jinan",2006 7th International Conference on Computer-Aided Industrial Design and Conceptual Design,20070326,2006,,,1,4,"First of all the essential procedure of design catalogue is expatriated on. And the methods of function coding and mechanism coding are defined according to the features of function and mechanism. On this base, the function table and mechanism table including evaluation criteria are constructed. Then because different users have different interests in the importance of various evaluation criteria of function, a user's function table is set up to describe the user's function on one hand and the different authority of each evaluation criteria for different functions is acquired with AHP(analysis of hierarchy process) on the other hand. In addition the user's functions may be searched in mechanism solution table and the set of feasible solutions is obtained. At last the optimal solution is achieved with a fuzzy evaluation method. In terms of idea expressed above the software system kinematic based design catalogue is developed for kinematic design. Accordingly, a software system kinematic based on design catalogue is developed",,CD-ROM:1-4244-0684-6; POD:1-4244-0683-8,10.1109/CAIDCD.2006.329425,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4127054,,Agricultural engineering;Concrete;Design methodology;Job design;Kinematics;Man machine systems;Mechanical systems;Protection;Software systems;Turning,CAD;decision making;mechanical engineering computing,analytic hierarchy process;function coding;function table;fuzzy evaluation method;kinematic scheme design catalogue;mechanism coding;mechanism table,,0,,5,,no,17-19 Nov. 2006,,IEEE,IEEE Conference Publications
Knowledge discovery in high-dimensional data: case studies and a user survey for the rank-by-feature framework,Jinwook Seo; B. Shneiderman,"Children's Res. Inst., Washington, DC, USA",IEEE Transactions on Visualization and Computer Graphics,20060320,2006,12,3,311,322,"Knowledge discovery in high-dimensional data is a challenging enterprise, but new visual analytic tools appear to offer users remarkable powers if they are ready to learn new concepts and interfaces. Our three-year effort to develop versions of the hierarchical clustering explorer (HCE) began with building an interactive tool for exploring clustering results. It expanded, based on user needs, to include other potent analytic and visualization tools for multivariate data, especially the rank-by-feature framework. Our own successes using HCE provided some testimonial evidence of its utility, but we felt it necessary to get beyond our subjective impressions. This paper presents an evaluation of the hierarchical clustering explorer (HCE) using three case studies and an e-mail user survey (n=57) to focus on skill acquisition with the novel concepts and interface for the rank-by-feature framework. Knowledgeable and motivated users in diverse fields provided multiple perspectives that refined our understanding of strengths and weaknesses. A user survey confirmed the benefits of HCE, but gave less guidance about improvements. Both evaluations suggested improved training methods",1077-2626;10772626,,10.1109/TVCG.2006.50,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1608018,Information visualization evaluation;case study;hierarchical clustering explorer.;rank-by-feature framework;user survey,Computer Society;Computer aided software engineering;Data analysis;Data visualization;Genomics;Histograms;Scattering;Testing;Visual analytics,data mining;data visualisation;database management systems;interactive systems;user interfaces,e-mail user survey;hierarchical clustering explorer;high-dimensional data;interactive tool;knowledge discovery;multivariate data;rank-by-feature framework;user interface;visual analytic tools;visualization tools,"Algorithms;Artificial Intelligence;Cluster Analysis;Computer Graphics;Computer Simulation;Imaging, Three-Dimensional;Models, Theoretical;Oligonucleotide Array Sequence Analysis;User-Computer Interface",26,,27,,no,May-June 2006,,IEEE,IEEE Journals & Magazines
Low-cost static performance prediction of parallel stochastic task compositions,H. Gautama; A. J. C. van Gemund,"Stat. Inf. Syst., Stat. Indonesia, Jakarta, Indonesia",IEEE Transactions on Parallel and Distributed Systems,20051205,2006,17,1,78,91,"Current analytic solutions to the execution time distribution of a parallel composition of tasks having stochastic execution times are computationally complex, except for a limited number of distributions. In this paper, we present an analytical solution based on approximating execution time distributions in terms of the first four statistical moments. This low-cost approach allows the parallel execution time distribution to be approximated at ultra-low solution complexity for a wide range of execution time distributions. The accuracy of our method is experimentally evaluated for synthetic distributions as well as for task execution time distributions found in real parallel programs and kernels (NAS-EP, SSSP, APSP, Splash2-Barnes, PSRS, and WATOR). Our experiments show that the prediction error of the mean value of the parallel execution time for N-ary parallel composition is in the order of percents, provided the task execution time distributions are sufficiently independent and unimodal.",1045-9219;10459219,,10.1109/TPDS.2006.13,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1549817,Performance prediction;stochastic graphs;workload distribution.,Bandwidth;Communication system control;Computer Society;Concurrent computing;Distributed computing;Information analysis;Kernel;Performance analysis;Processor scheduling;Stochastic processes,computational complexity;parallel programming;software performance evaluation;statistical distributions;stochastic processes,N-ary parallel composition;low-cost static performance prediction;parallel execution time distribution;parallel programs;parallel stochastic task composition;statistical moments;task execution time distributions,,6,,41,,no,Jan. 2006,,IEEE,IEEE Journals & Magazines
Low-power buffer management using hybrid control,J. Ridenour; Jianghai Hu; Yung-Hsiang Lu,"Sch. of Electr. & Comput. Eng., Purdue Univ., West Lafayette, IN, USA",2006 American Control Conference,20060724,2006,,,6 pp.,,"The power management problem is studied for a pipeline of streaming data consisting of multiple stages with buffer memories inserted between adjacent stages. The objective is to find the optimal strategy for dynamically changing the power states of the stages so that the power consumption of the overall system is minimized. By modeling the pipeline as a hybrid system, we derive various necessary conditions for the optimal solutions. In particular, an operation called folding is introduced that can improve the performance of an existing strategy. Analytic conditions are derived for determining whether folding can save power and the optimal number of folds",0743-1619;07431619,Electronic:1-4244-0210-7; POD:1-4244-0209-3,10.1109/ACC.2006.1656626,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1656626,,Application software;Buffer storage;Computer networks;Energy consumption;Energy management;Information retrieval;Pipelines;Power system management;Power system modeling;Turning,buffer storage,buffer memory;hybrid control;hybrid system;low-power buffer management;optimal strategy;pipeline modeling;power consumption minimization;power management;streaming data pipeline,,1,,15,,no,14-16 June 2006,,IEEE,IEEE Conference Publications
Manufacturing Execution Systems (MES) assessment and investment decision study,L. Chao; Li Qing,"Department of Automation, Tsinghua University, Beijing, China. Telephone: 13811645065; e-mail: liang-c04@mails.tsinghua.edu.cn","2006 IEEE International Conference on Systems, Man and Cybernetics",20070716,2006,6,,5285,5290,"Manufacturing execution systems (MES), a category of industrial software for the manufacturing environment, is grasping increasing attention, due to its unique ability to improve manufacturing performance, and then to have a positive impact on the financial performance of the company. For many executives, whether to implement MES to improve manufacturing and financial performance is an issue they must encounter. Since what MES brings about are not only benefits and potential opportunities, but also costs and potential risks, a comprehensive and systematic assessment is absolutely necessary before making such a critical investment decision. This paper proposes a decision methodology for MES investment decision making. In addition, a general decision model with respect to Benefits, Opportunities, Costs and Risks merits is provided. The decision methodology and decision model are validated by an undershirt manufacturer in China, by applying analytic network process (ANP) method.",1062-922X;1062922X,CD-ROM:1-4244-0100-3; POD:1-4244-0099-6,10.1109/ICSMC.2006.385148,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4274757,ANP;BOCR;Manufacture Execution systems;assessment;decision,Chaos;Computer industry;Costs;Decision making;Investments;Manufacturing automation;Manufacturing industries;Personnel;Software performance;Virtual manufacturing,decision making;investment;manufacturing systems,analytic network process method;comprehensive assessment;critical investment decision;decision methodology;industrial software;investment decision making;manufacturing execution systems assessment;systematic assessment,,1,,26,,no,8-11 Oct. 2006,,IEEE,IEEE Conference Publications
Modelling and Improving Group Communication in Server Operating Systems,M. Kwok; T. Brecht; M. Karsten; Jialin Song,"University of Waterloo, Canada","14th IEEE International Symposium on Modeling, Analysis, and Simulation",20061016,2006,,,207,217,"virtual environment (DVE) have become increasingly popular. Many DVE implementations use a client-server architecture that requires the server to send the same data to all members of a collaborating or interacting group. This type of group communication operation is often implemented by sending data from the server to each recipient in a unicast fashion. The problem with this approach is that the cost of communication at the server does not scale very well with the number of participants because the application requires significant interaction with the operating system, network stack and drivers for each individual send. In this paper, we first propose a general analytic framework for predicting how group communication performance impacts DVE server capacity. We then conduct an experimental evaluation to determine the extent to which using a kernel-based group communication mechanism reduces the cost of group send operations. Lastly, we use the measurements obtained from these experiments to demonstrate how to apply the analytic framework by determining the extent to which the kernel-based group communication mechanism permits example applications to scale to more users.",1526-7539;15267539,POD:0-7695-2573-3,10.1109/MASCOTS.2006.32,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1698552,,Application software;Computer architecture;Computer science;Costs;Internet;Network servers;Operating systems;Performance analysis;Videoconference;Virtual environment,,,,0,,28,,no,11-14 Sept. 2006,,IEEE,IEEE Conference Publications
NetLens: Iterative Exploration of Content-Actor Network Data,H. Kang; C. Plaisant; B. Lee; B. B. Bederson,"University of Maryland Institute for Advanced Computer Studies, kang@cs.umd.edu",2006 IEEE Symposium On Visual Analytics Science And Technology,20061226,2006,,,91,98,"Networks have remained a challenge for information retrieval and visualization because of the rich set of tasks that users want to accomplish. This paper offers an abstract content-actor network data model, a classification of tasks, and a tool to support them. The NetLens interface was designed around the abstract content-actor network data model to allow users to pose a series of elementary queries and iteratively refine visual overviews and sorted lists. This enables the support of complex queries that are traditionally hard to specify. NetLens is general and scalable in that it applies to any dataset that can be represented with our abstract data model. This paper describes NetLens applying a subset of the ACM Digital Library consisting of about 4,000 papers from the CM I conference written by about 6,000 authors. In addition, we are now working on a collection of half a million emails, and a dataset of legal cases",,CD-ROM:1-4244-0592-0; POD:1-4244-0591-2,10.1109/VAST.2006.261426,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4035752,Human-Computer Interaction;content-actor network data;digital library;incremental data exploration;information visualization;iterative query refinement;network visualization;piccolo;user interfaces,Data models;Data visualization;Displays;Graphical user interfaces;Laboratories;Law;Legal factors;Software libraries;User interfaces;Visual analytics,data models;data visualisation;digital libraries;graphical user interfaces,NetLens interface;abstract data model;content-actor network data;digital library;human-computer interaction;incremental data exploration;information retrieval;information visualization;iterative query refinement;network visualization;piccolo,,8,,29,,no,Oct. 31 2006-Nov. 2 2006,,IEEE,IEEE Conference Publications
Numerical Studies of Curved-walled Micro Nozzle/Diffuser,Y. T. Chen; C. C. Hsu; M. C. Chang; S. W. Kang,"Department of Mechanical Engineering, De Lin Institute of Technology, Taipei, Taiwan",2006 1st IEEE International Conference on Nano/Micro Engineered and Molecular Systems,20070319,2006,,,841,845,"In this study, commercially available software CFD was adopted for analyzing the performance of straight-walled and curved-walled micro nozzle/diffuser. Such nozzle/diffuser was used in valve-less micro-pumps. This model tested different types of nozzle/diffuser and the results showed that the pressure loss coefficient for nozzle/diffuser decreases with the increase of Reynolds number. At the same Reynolds number, the pressure loss coefficient for nozzle is higher than that of the diffuser. At a given volumetric flow rate, the pressure loss coefficient and ratio of pressure loss coefficient for curved-walled nozzle/diffuser are slightly higher than that of the straight-walled nozzle/diffuser. In this study, the numerical data was found good agreement with previous analytic solution and experimental results.",,CD-ROM:1-4244-0140-2; POD:1-4244-0139-9,10.1109/NEMS.2006.334908,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4135081,Diffuser;Fluent;Nozzle;Pressure loss coefficient;Reynolds number,Computational fluid dynamics;Differential equations;Fabrication;Micropumps;Performance analysis;Performance loss;Pumps;Software performance;Systems engineering and theory;Testing,computational fluid dynamics;diffusion;flow;nozzles,CFD software;Reynolds number;curved-walled micro nozzle;diffuser;pressure loss coefficient;valve-less micro-pumps,,0,,6,,no,18-21 Jan. 2006,,IEEE,IEEE Conference Publications
On discretizing linear passive controllers,R. Costa-Castello; E. Fossas,"Inst. d'Organitzacio i Control de Sistemes industrials, Univ. Politecnica de Catalunya, Barcelona, Spain",2006 IEEE International Symposium on Circuits and Systems,20060911,2006,,,4 pp.,838,"In this work a new methodology which allows to discretize linear continuous-time passivity based controller is presented. This methodology is based on choosing a proper output, which preserves the passivity structure, while keeping the continuous-time energy function. Analytic formulation and a numerical example are provided in the paper",0271-4302;02714302,POD:0-7803-9389-9,10.1109/ISCAS.2006.1692715,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1692715,Digital Control;Passivity;Preserving Passivity,Application software;Control systems;Damping;Data systems;Digital control;Linear systems;Open loop systems;Sampling methods;Stability,continuous time systems;controllers;discrete time systems,continuous-time energy function;continuous-time passivity based controller;digital control;linear passive controller;passivity structure,,1,,10,,no,21-24 May 2006,,IEEE,IEEE Conference Publications
On the asymptotic performance analysis of subspace DOA estimation in the presence of modeling errors: case of MUSIC,A. Ferreol; P. Larzabal; M. Viberg,"THALES Commun., Colombes, France",IEEE Transactions on Signal Processing,20060221,2006,54,3,907,920,"This paper provides a new analytic expression of the bias and RMS error (root mean square) error of the estimated direction of arrival (DOA) in the presence of modeling errors. In , first-order approximations of the RMS error are derived, which are accurate for small enough perturbations. However, the previously available expressions are not able to capture the behavior of the estimation algorithm into the threshold region. In order to fill this gap, we provide a second-order performance analysis, which is valid in a larger interval of modeling errors. To this end, it is shown that the DOA estimation error for each signal source can be expressed as a ratio of Hermitian forms, with a stochastic vector containing the modeling error. Then, an analytic expression for the moments of such a Hermitian forms ratio is provided. Finally, a closed-form expression for the performance (bias and RMS error) is derived. Simulation results indicate that the new result is accurate into the region where the algorithm breaks down.",1053-587X;1053587X,,10.1109/TSP.2005.861798,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1597557,Calibration;MUSIC;direction of arrival (DOA) estimation;modeling errors;performance analysis,Closed-form solution;Computer aided software engineering;Direction of arrival estimation;Error analysis;Estimation error;Multiple signal classification;Performance analysis;Random variables;Root mean square;Stochastic processes,Hermitian matrices;direction-of-arrival estimation;mean square error methods,Hermitian forms;MUSIC;asymptotic performance analysis;chaotic vector;direction of arrival;root mean square error;second-order performance analysis;subspace DOA estimation,,42,,21,,no,6-Mar,,IEEE,IEEE Journals & Magazines
Partner Selection and Evaluation in Agile Virtual Enterprise Based upon TFN-AHP Model,J. Cao; G. Zhou,"Institute of Information Intelligence & Decision-making Optimization, Zhejiang University of Technology, Hangzhou, P.R. China; Institute of Industrial Process Control, Zhejiang University, Hangzhou, P.R. China. jcao@iipc.zju.edu.cn",2006 10th International Conference on Computer Supported Cooperative Work in Design,20061204,2006,,,1,6,"To solve some problems caused by the analytic hierarchy process (AHP) or other fuzzy-AHP approaches to partner selection and evaluation in the formation of agile virtual enterprises, a model based on the AHP and basic theory of the trigonometrical fuzzy number (TFN) was given and corresponding evaluation process was described. Compared with the AHP, the proposed model combines subjective analysis with quantitative analysis more reasonably, synthesizes group opinions more adequately. The model derives priorities from TFN-based judgment matrices effectively, regardless of their inconsistency. In addition, the model can be programmed easily due to its simple, normative arithmetic formulas. The results of a case study on selecting suppliers in an agile virtual enterprise indicate that, by applying the model-based software system, not only fair and reasonable conclusions, but also the consistent degree of opinions provided by different experts can be acquired clearly",,CD-ROM:1-4244-0165-8; POD:1-4244-0164-X,10.1109/CSCWD.2006.253009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4019045,Agile Virtual Enterprise;Analytic Hierarchy Process;Logarithmic Least Squares Method;Partner Selection;Trigonometrical Fuzzy Number,Agile manufacturing;Collaborative work;Computer industry;Decision making;Design optimization;Fuzzy control;Industrial control;Least squares methods;Process control;Virtual enterprises,agile manufacturing;distributed decision making;fuzzy set theory;matrix algebra;supply chain management;virtual enterprises,TFN-AHP model;TFN-based judgment matrices;agile virtual enterprise;fuzzy analytic hierarchy process;group decision making;model-based software system;normative arithmetic formula;partner selection;trigonometrical fuzzy number,,2,,13,,no,3-5 May 2006,,IEEE,IEEE Conference Publications
Performance analysis of the FastICA algorithm and Crameå«r-rao bounds for linear independent component analysis,P. Tichavsky; Z. Koldovsky; E. Oja,"Inst. of Inf. Theor. & Autom., Acad. of Sci. of the Czech Republic, Prague, Czech Republic",IEEE Transactions on Signal Processing,20060320,2006,54,4,1189,1203,"The FastICA or fixed-point algorithm is one of the most successful algorithms for linear independent component analysis (ICA) in terms of accuracy and computational complexity. Two versions of the algorithm are available in literature and software: a one-unit (deflation) algorithm and a symmetric algorithm. The main result of this paper are analytic closed-form expressions that characterize the separating ability of both versions of the algorithm in a local sense, assuming a ""good"" initialization of the algorithms and long data records. Based on the analysis, it is possible to combine the advantages of the symmetric and one-unit version algorithms and predict their performance. To validate the analysis, a simple check of saddle points of the cost function is proposed that allows to find a global minimum of the cost function in almost 100% simulation runs. Second, the Crameå«r-Rao lower bound for linear ICA is derived as an algorithm independent limit of the achievable separation quality. The FastICA algorithm is shown to approach this limit in certain scenarios. Extensive computer simulations supporting the theoretical findings are included.",1053-587X;1053587X,,10.1109/TSP.2006.870561,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1608537,Blind source separation;CramÌär‰ÛÒRao lower bound;independent component analysis (ICA),Algorithm design and analysis;Analytical models;Closed-form solution;Computational complexity;Computational modeling;Cost function;Independent component analysis;Performance analysis;Prediction algorithms;Software algorithms,blind source separation;independent component analysis,Cramer-Rao bounds;cost function;deflation algorithm;fastICA algorithm;fixed-point algorithm;linear independent component analysis;one-unit version algorithm;symmetric algorithm,,62,,33,,no,6-Apr,,IEEE,IEEE Journals & Magazines
Performance evaluation and modeling in Grid Resource Management,Y. Liu; Y. Jian,"Chongqing University of Posts and Telecommunications, China; UEST of China, China",2006 Fifth International Conference on Grid and Cooperative Computing (GCC'06),20061219,2006,,,335,338,"Grid resource management is regarded as an important component of a grid computing system. The resources in the grid are geographically distributed, heterogeneous in nature, owned by different individuals/organizations each having their own resource management policies and different access-and-cost models. Based on abstract owner model, this article proposes a new resource storage model called resource pool which couples geographically distributed resource together. Meanwhile, this paper uses queuing and global optimization theory as tools to emphasize on quantitative analysis for resource pool capacity. Consequently, some new analytic formula for practical computation is obtained. Furthermore, an algorithm for analytic formulas is introduced into global optimization to evaluate the performance",2160-4908;21604908,POD:0-7695-2694-2,10.1109/GCC.2006.71,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4031478,,Algorithm design and analysis;Computer networks;Computer science;Distributed computing;Grid computing;High performance computing;IP networks;Performance analysis;Queueing analysis;Resource management,grid computing;optimisation;queueing theory;resource allocation;software performance evaluation,abstract owner model;analytic formula algorithm;distributed resources;global optimization theory;grid computing;grid resource management;performance evaluation;queuing theory;resource pool;resource storage model,,2,,7,,no,Oct. 2006,,IEEE,IEEE Conference Publications
Quantitative system design,M. K. Vernon,"Wisconsin Univ., USA",2006 IEEE International Symposium on Performance Analysis of Systems and Software,20060418,2006,,,130,,"Summary form only given. This talk provides a 20-year perspective on the use of analytic models to design of a wide range of commercially important architectures and systems with complex behavior. These systems include resources with highly bursty and/or correlated packet arrivals, communication protocols with complex routing and blocking of messages, resources that are configured for a very high probability (e.g., 0.9999) of providing immediate service to each arriving client, and complex large-scale grid/Internet applications. The examples illustrate some guiding principles for model development, and show that the models can be relatively easy to develop. More importantly, the models can be highly accurate - often more accurate than simulation, and sometimes more accurate than the system implementation! The examples also illustrate that the models can provide unique insight into system design as well as significant new system functionality. In other words, analytic models are a key tool for competitive systems engineering. Time permitting, the talk includes some important observations about workload models, and some ways to avoid key pitfalls in simulation.",,POD:1-4244-0186-0,10.1109/ISPASS.2006.1620797,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1620797,,,computer networks;message passing;packet switching;protocols,analytic models;client large-scale Internet applications;commercially important architectures;commercially important systems;communication protocols;competitive systems engineering;complex behavior;complex large-scale grid applications;complex message routing;correlated packet arrivals;highly bursty packet arrivals;message blocking;quantitative system design;workload models,,0,,,,no,19-21 March 2006,,IEEE,IEEE Conference Publications
Repetitive Controller Design for Track Following of Hard Disk Drive Servo System,W. Yilei; D. Mingzhong; O. KianKeong; D. Chenyang,"Seagate Technology International, 63, The Fleming, Science Park Drive, Singapore 118249. Email: Yilei.Wu@Seagate.com","2006 9th International Conference on Control, Automation, Robotics and Vision",20070716,2006,,,1,4,"Recently repetitive controller has been applied in servo systems of hard disk drives (HDD) to remove the repeatable errors caused by rotating mechanism of spindle motors. However, most results of published articles are too complex to be implemented in industrial applications. In this paper, we present an improved design strategy for repetitive control of HDD by minimizing the power spectral density (PSD) of PES. An analytic solution is given with tolerable computation requirements and can be evaluated by software, even assembly languages. Simulations of track following of HDD servo are carried out based upon the theoretical derivations. The result shows that compared to single PID control, the proposed controller improves the performance of steady state response",,CD-ROM:1-4214-042-1; POD:1-4244-0341-3,10.1109/ICARCV.2006.345434,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4150381,hard disk drive;optimization;repetitive controller,Application software;Assembly;Computational modeling;Control systems;Electrical equipment industry;Error correction;Hard disks;Servomechanisms;Servomotors;Three-term control,control system synthesis;disc drives;hard discs;minimisation;position control;stability,hard disk drive servo system;power spectral density minimization;repetitive controller design;rotating mechanism;spindle motors;track following,,0,,9,,no,5-8 Dec. 2006,,IEEE,IEEE Conference Publications
Requirements elicitation through model-driven evaluation of software components,L. Chung; Weimin Ma; K. Cooper,"Dept. of Comput. Sci., Texas Univ., Dallas, TX, USA",Fifth International Conference on Commercial-off-the-Shelf (COTS)-Based Software Systems (ICCBSS'05),20060306,2006,,,10 pp.,,"The use of software components is perceived to significantly shorten development time and cost, while improving quality, in developing a large, complex software system. A key premise to this perception seems to be the ability to effectively search, match, rank, and select software components, during the requirements engineering process. In this paper, we present a technique for eliciting requirements by using model-driven evaluation of software components, where the evaluation revolves around ""models"" of software components and ""models"" of the component-based application (CBA). As part of our ongoing project, component-aware requirements engineering (CARE), this model-driven evaluation technique is intended to match the models of the stakeholders' needs for the component-based application against the models of the capabilities of the set of components that are currently available. More specifically, this technique allows for an integrated use of several searching/matching techniques, such as keyword-based search, case-based reasoning (CBR) and analytic hierarchy process (AHP), in evaluating models of components' requirements against models of the requirements of the stakeholders of the CBA being elicited incrementally. The model-driven evaluation technique is illustrated using a home appliance control system (HACS) example.",,POD:0-7695-2515-6,10.1109/ICCBSS.2006.24,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1595762,,Application software;Computer architecture;Computer science;Control system synthesis;Costs;Embedded software;Home appliances;Software quality;Software systems;Testing,formal specification;formal verification;object-oriented programming;software performance evaluation;software quality,analytic hierarchy process;case-based reasoning;component aware requirements engineering;component-based application;home appliance control system;keyword-based search;matching technique;model-driven evaluation;searching technique;software component,,1,,26,,no,13-16 Feb. 2006,,IEEE,IEEE Conference Publications
Resource management with stateful support for analytic applications,L. L. Fong; C. H. Crawford; H. Shaikh,"IBM T. J. Watson Res. Center, Yorktown Heights, NY, USA",Proceedings 20th IEEE International Parallel & Distributed Processing Symposium,20060626,2006,,,8 pp.,,"Analytic applications from various industrial sectors have specific attributes and requirements including relatively long processing time, parallelization, multiple interactive invocations, Web services, and expected quality of service objectives. Current parallel resource management systems for batch-oriented jobs lack the effective support for multiple interactive invocations with consideration in quality of service objectives, while transaction processing systems do not support dynamic creation of parallel application instances. To better serve the analytic applications, a set of additional resource management services, defined as stateful support, introduces the concept of service instance and service instance management. This set of stateful support services can be implemented as extension to existing parallel resource management to serve these analytic applications that rapidly increase in the demand of computing power",1530-2075;15302075,POD:1-4244-0054-6,10.1109/IPDPS.2006.1639695,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1639695,,Application software;Cost function;Grid computing;Parallel processing;Portfolios;Power system management;Quality of service;Resource management;Vehicle dynamics;Web services,parallel processing;resource allocation,analytic application;batch-oriented jobs;parallel application;parallel resource management system;quality of service;service instance management;stateful support services,,1,,23,,no,25-29 April 2006,,IEEE,IEEE Conference Publications
Self-Motion Analysis on A Redundant Robot with A Parallel/Series Hybrid Configuration,Z. Jianwen; S. Lining; D. Zhijiang; Z. Bo,"Robotics Institute, Harbin Institute of Technology, Harbin, Heilongjiang Province, China 150001. Email: zhaojianwen@hit.edu.cn","2006 9th International Conference on Control, Automation, Robotics and Vision",20070716,2006,,,1,6,"A redundant robot configured with a parallel base and a series manipulator is proposed in this paper. The position and orientation of end manipulator are considered as the result of the joints' ordinal motion according to the equivalence principle, so the close analytic solution for self-motion of this redundant robot is obtained via geometrical method. The Jacobian matrices of the parallel base and the whole robot system are solved respectively. Based on this, the solution for self-motion in two classes of singular configuration is also obtained. Finally a self-motion simulation of the redundant robot is performed via software MATLAB and ADAMS to validate the algorithm",,CD-ROM:1-4214-042-1; POD:1-4244-0341-3,10.1109/ICARCV.2006.345173,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4150083,Kinematics analysis;Planar parallel manipulator;Redundant robot;Self-motion,Equations;Fasteners;Jacobian matrices;Kinematics;Manipulators;Orbital robotics;Parallel robots;Shoulder;Sun;Wrist,Jacobian matrices;gravitation;motion control;position control;redundant manipulators,ADAMS;Jacobian matrices;MATLAB;end manipulator;equivalence principle;kinematics analysis;parallel hybrid configuration;planar parallel manipulator;redundant robot;self-motion analysis;self-motion simulation;series hybrid configuration;series manipulator;singular configuration,,0,,7,,no,5-8 Dec. 2006,,IEEE,IEEE Conference Publications
Simulation Study of TCP/IP Communication Based on Networked Control Systems,An Wei; Yuqiang Chen; Jinhua Wu,"The Key Laboratory of Complex Systems and Intelligence Sciences, Institute of Automation, Chinese Academy of Sciences, Beijing 100080, China. an.wei@mail.ia.ac.cn",2006 6th World Congress on Intelligent Control and Automation,20061023,2006,1,,4479,4483,"This paper studies the TCP/IP communication issue of data stream in networked control systems (NCS). The character of the NCS is analyzed and the data stream type is discussed. Analytic model of TCP/IP is presented according to slow start theory. The simulation model is established based on OPNET software. The performance result of different type of data stream TCP/IP transmission is presented. Finally, the simulation result is analyzed, and defects and improved method of TCP/IP protocol is discussed",,POD:1-4244-0332-4,10.1109/WCICA.2006.1713226,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1713226,Model;Networked Control Systems;OPNET;Simulation;TCP/IP,Communication networks;Communication system control;Control systems;Data communication;Ethernet networks;IP networks;Networked control systems;Protocols;Remote monitoring;TCPIP,local area networks;transport protocols,Ethernet;OPNET software;TCP/IP communication;TCP/IP protocol;data stream type;networked control systems;simulation model;slow start theory,,3,,10,,no,0-0 0,,IEEE,IEEE Conference Publications
Software Reliability Metrics Selecting Method Based on Analytic Hierarchy Process,H. Li; M. Lu; Q. Li,"BeiHang University, China",2006 Sixth International Conference on Quality Software (QSIC'06),20061219,2006,,,337,346,"It is very important to select and use appropriate software reliability metrics in software reliability engineering. This paper proposes a framework for selecting software reliability metrics based on analytic hierarchy process (AHP) and expert judgment. Selecting criteria and the metrics for selection are identified. In each development phase, the grading of metrics according to every criterion are given by experts qualitatively, and then analyzed synthetically to calculate the weights of metrics using AHP. A preliminary application is practised, and the metrics whose weights are top-ranked are recommended and analyzed. Sensitivity and consistency of this method are also analyzed. Compared with general selecting criteria, the method studied in this paper can be used to select appropriate metrics correctly, stably and systemically. Furthermore, the final selection results are accordant with engineering experience, and using the metrics recommended will make software reliability evaluation more reliable and effective",1550-6002;15506002,POD:0-7695-2718-3,10.1109/QSIC.2006.59,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4032303,,Application software;Costs;Decision making;Reliability engineering;Software engineering;Software quality;Software reliability;Software safety;Software testing;Systems engineering and theory,program testing;software metrics;software quality;software reliability,analytic hierarchy process;consistency analysis;sensitivity analysis;software engineering;software reliability metrics,,0,,11,,no,27-28 Oct. 2006,,IEEE,IEEE Conference Publications
Study of Measurement Approach of Loop Gain of Converter,W. Zhang; Y. Chen; Y. Liu; D. Zhang; Z. Meng,"North China University of Technology/Lab of Green Power & Energy System, Beijing, China. Email: zwp@ncut.edu.cn",2006 CES/IEEE 5th International Power Electronics and Motion Control Conference,20090210,2006,1,,1,6,"In this paper, an approach by using the Agilent 4395A to measure the loop gain of the power supply has been deeply developed. The main issues have been investigated as the following: (1). Measurement techniques about the parasitic parameters of the filtering components will be studied. One is how to test of the electrolyte capacitor by Agilent 4395A; another is how to test dynamic inductance of filter inductor by Tektronix TDS5052 with TDSPWR3 software; (2). A measurement approach for loop gain has been put forward; (3). A prototype has been made up and its loop gain has been tested to verify the proposed approaches; (4). The experimental result has first revealed that the biggest analytic error of the loop gain occurs nearby about the resonance frequency of low pass filter. It can be explained that the error results from the equivalent resistor of the power diode and MOSFET; (5). Based on considering the equivalent resistor, a novel accurate small-signal model of buck converter has been proposed to reduce the theoretic error. The proposed approaches, model and other results have provided a powerful tool to analyze and design a power supply",,POD:1-4244-0448-7,10.1109/IPEMC.2006.4777983,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4777983,loop gain;model;power components stability,Capacitors;Filtering;Filters;Gain measurement;Inductance;Measurement techniques;Power measurement;Power supplies;Resistors;Software testing,gain measurement;power convertors,Agilent 4395A;MOSFET;TDSPWR3 software;Tektronix TDS5052;buck converter;electrolyte capacitor;filter inductor;loop gain;power diode;power supply,,0,,5,,no,14-16 Aug. 2006,,IEEE,IEEE Conference Publications
Supporting Distributed Collaborative Work with Multi-versioning,M. A. Orgun; L. Xue; Z. Han,"Department of Computing, Macquarie University, Sydney, NSW 2109 Australia. mehmet@comp.mq.edu.au",2006 10th International Conference on Computer Supported Cooperative Work in Design,20061204,2006,,,1,6,"The multi-version approach is useful in both synchronous and asynchronous groupware systems. This paper discusses the implementation of a real-time group editor that embodies our approaches and algorithms based on multi-versioning, which can preserve individual users' concurrent conflicting intentions in a consistent way. To highlight the distinct contributions of our work, we also present a detailed description of some novel features of the system",,CD-ROM:1-4244-0165-8; POD:1-4244-0164-X,10.1109/CSCWD.2006.253140,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4019176,Coordination;Groupware;Multi-version,Australia;Collaboration;Collaborative software;Collaborative work;Database systems;Delay effects;Distributed computing;Humans;IP networks;Internet,configuration management;distributed processing;groupware;text editing,asynchronous groupware system;distributed collaborative work;multiversion approach;real-time group editor;synchronous groupware system,,1,,14,,no,3-5 May 2006,,IEEE,IEEE Conference Publications
Systematic Development of Requirements Documentation for General Purpose Scientific Computing Software,S. Smith,McMaster University,14th IEEE International Requirements Engineering Conference (RE'06),20061016,2006,,,209,218,"This paper presents a methodology for developing the requirements for general purpose scientific computing software. The first step in the methodology is to determine the general purpose scientific software of interest. The second step consists of a commonality analysis on this identified family of general purpose tools to document the terminology, commonalities and variabilities. The commonality analysis is then refined in the third step into a family of specific requirements documents. Besides fixing the variabilities and their binding times, each specific requirements document also shows the relative importance of the different nonfunctional requirements, for instance using the analytic hierarchy process (AHP). The new methodology addresses the challenge of writing validatable requirements by including solution validation strategies as part of the requirements documentation. To illustrate the methodology an example is shown of a solver for a linear system of equations",1090-705X;1090705X,POD:0-7695-2555-5,10.1109/RE.2006.61,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1704064,,,formal specification;formal verification;system documentation,nonfunctional requirements;requirements documentation;scientific computing software;systematic development,,4,,21,,no,11-15 Sept. 2006,,IEEE,IEEE Conference Publications
Temporal Data Mining in Dynamic Feature Spaces,B. Wenerstrom; C. Giraud-Carrier,"Sharp Analytics, USA",Sixth International Conference on Data Mining (ICDM'06),20070108,2006,,,1141,1145,"Many interesting real-world applications for temporal data mining are hindered by concept drift. One particular form of concept drift is characterized by changes to the underlying feature space. Seemingly little has been done in this area. This paper presents FAE, an incremental ensemble approach to mining data subject to such concept drift. Empirical results on large data streams demonstrate promise.",1550-4786;15504786,POD:0-7695-2701-7,10.1109/ICDM.2006.157,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4053168,,Application software;Cities and towns;Computer science;Data mining;Decision trees;Degradation;Marketing and sales;Niobium;Predictive models;Testing,data mining;feature extraction;learning (artificial intelligence),concept drift;dynamic feature space;feature adaptive ensemble;incremental ensemble approach;temporal data mining,,7,1,20,,no,18-22 Dec. 2006,,IEEE,IEEE Conference Publications
The Application of the System Parameter Fusion Principle to Assessing University Electronic Library Performance,Y. Li; H. Pu; Q. Pu,"Library, University of Science and Technology of Suzhou, Jiangsu, China",2006 5th IEEE International Conference on Cognitive Informatics,20070910,2006,2,,929,934,"Modern technology provides a great amount of information. But for computer monitoring systems or computer control systems, in order to have the situation in hand, we need to reduce the number of variables to one or two parameters, which express the quality and/or security of the whole system. In this paper, the authors introduce the system parameter fusion principle put forward by the third author and present how to apply it to assessing university electronic library performance combining with the Delphi technique and AHP",,POD:1-4244-0475-4,10.1109/COGINF.2006.365617,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4216535,,Application software;Computer security;Computerized monitoring;Condition monitoring;Control systems;Home computing;Information security;Software libraries;Software performance;Workstations,digital libraries;research libraries,Delphi technique;analytic hierarchy process;system parameter fusion principle;university electronic library,,0,,7,,no,17-19 July 2006,,IEEE,IEEE Conference Publications
Time Synchronization Simulator and Its Application,C. Xu; L. Zhao; Y. Xu; X. Li,"Advanced Test Technology Lab, Institute of Computing Technology, Chinese Academy of Sciences, 100080; Department of computer science, Hefei University of Technology, Hefei PR China, 230009. Email: xu",2006 1ST IEEE Conference on Industrial Electronics and Applications,20061211,2006,,,1,6,"Time synchronization is a critical middleware service of wireless sensor networks. Since the performance of time synchronization algorithm is greatly influenced by many factors, benchmark for evaluating time synchronization algorithm is not only difficult but also urgently needed. Software simulation is a good solution especially in the comparison between similar algorithms. In this paper, we presented a time synchronization simulator, Simsync, for wireless sensor networks. Simsync models the distribution of packet delay and the frequency of crystal oscillator as Gaussian. Based on it, we realized reference broadcast synchronization algorithm (RBS) and broadcast time synchronization algorithm (BTS) in 3 different scenarios. Simulated results are compared with the analytic results to advocate its effectiveness",,CD-ROM:0-7803-9514-X; POD:0-7803-9513-1,10.1109/ICIEA.2006.257223,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4026017,,Analytical models;Broadcasting;Computational modeling;Computer simulation;Delay;Frequency synchronization;Middleware;Oscillators;Testing;Wireless sensor networks,delays;synchronisation;telecommunication services;wireless sensor networks,Simsync;broadcast time synchronization algorithm;critical middleware service;crystal oscillator;frequency distribution;packet delay distribution;reference broadcast synchronization algorithm;time synchronization simulator;wireless sensor networks,,2,,20,,no,24-26 May 2006,,IEEE,IEEE Conference Publications
TreePlus: Interactive Exploration of Networks with Enhanced Tree Layouts,Bongshin Lee; C. S. Parr; C. Plaisant; B. B. Bederson; V. D. Veksler; W. D. Gray; C. Kotfila,"Dept. of Comput. Sci., Maryland Univ., College Park, MD",IEEE Transactions on Visualization and Computer Graphics,20060918,2006,12,6,1414,1426,"Despite extensive research, it is still difficult to produce effective interactive layouts for large graphs. Dense layout and occlusion make food Webs, ontologies and social networks difficult to understand and interact with. We propose a new interactive visual analytics component called TreePlus that is based on a tree-style layout. TreePlus reveals the missing graph structure with visualization and interaction while maintaining good readability. To support exploration of the local structure of the graph and gathering of information from the extensive reading of labels, we use a guiding metaphor of ""plant a seed and watch it grow."" It allows users to start with a node and expand the graph as needed, which complements the classic overview techniques that can be effective at (but often limited to) revealing clusters. We describe our design goals, describe the interface and report on a controlled user study with 28 participants comparing TreePlus with a traditional graph interface for six tasks. In general, the advantage of TreePlus over the traditional interface increased as the density of the displayed data increased. Participants also reported higher levels of confidence in their answers with TreePlus and most of them preferred TreePlus",1077-2626;10772626,,10.1109/TVCG.2006.106,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1703363,Graph visualization;Piccolo Zoomable User Interface (ZUI) Toolkit.;evaluation/methodology;graphical user interfaces;information visualization;interaction techniques;navigation techniques,Data visualization;Graphical user interfaces;Navigation;Ontologies;Social network services;Sociology;Tree graphs;User interfaces;Visual analytics;Watches,data visualisation;graphical user interfaces;hidden feature removal;interactive systems;trees (mathematics),data visualization;graph structure;graphical user interface;interactive visual analytics;occlusion;tree-style layout,"Algorithms;Computer Graphics;Computer Simulation;Information Storage and Retrieval;Models, Biological;Pattern Recognition, Automated;Software;User-Computer Interface",35,1,41,,no,Nov.-Dec. 2006,,IEEE,IEEE Journals & Magazines
Understanding representational sensitivity in the iterated prisoner's dilemma with fingerprints,D. Ashlock; Eun-Youn Kim; N. Leahy,"Dept. of Math. & Stat., Univ. of Guelph, Ont., Canada","IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)",20060619,2006,36,4,464,475,"The iterated prisoner's dilemma is a widely used computational model of cooperation and conflict. Many studies report emergent cooperation in populations of agents trained to play prisoner's dilemma with an evolutionary algorithm. This study varies the representation of the evolving agents resulting in levels of emergent cooperation ranging from 0% to over 90%. The representations used in this study are directly encoded finite-state machines, cellularly encoded finite-state machines, feedforward neural networks, if-skip-action lists, parse trees storing two types of Boolean functions, lookup tables, Boolean function stacks, and Markov chains. An analytic tool for rapidly identifying agent strategies and comparing across representations called a fingerprint is used to compare the more complex representations. Fingerprints provide functional signatures of an agent's strategy in a manner that is independent of the agent's representation. This study demonstrates conclusively that choice of a representation dominates agent behavior in evolutionary prisoner's dilemma. This in turn suggests that any soft computing system intended to simulate behavior must be concerned with the representation issue",1094-6977;10946977,,10.1109/TSMCC.2006.875423,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1643837,Game theory;genetic algorithm;knowledge representation,Biological system modeling;Boolean functions;Cellular neural networks;Computational modeling;Evolutionary computation;Feedforward neural networks;Fingerprint recognition;Mathematics;Neural networks;Table lookup,Boolean functions;Markov processes;cellular automata;feedforward neural nets;finite state machines;game theory;genetic algorithms;grammars;knowledge representation;software agents;table lookup;tree data structures,Boolean function stacks;Boolean functions;Markov chains;agent cooperation;agent representation;cellularly encoded finite-state machines;data structures;directly encoded finite-state machines;evolutionary algorithm;feedforward neural networks;fingerprints;game theory;genetic algorithm;if-skip-action lists;iterated prisoner dilemma;knowledge representation;lookup tables;parse trees;soft computing system,,52,,27,,no,6-Jul,,IEEE,IEEE Journals & Magazines
Usage of the Analytic Hierarchy Process for Production Optimization,V. Dubrovin; N. Mironova,"Zaporizhzhya National Technical University, Software Engineering Department, 64, Zhukovskogo Street, Zaporizhzhya, 69063, Ukraine","2006 International Conference - Modern Problems of Radio Engineering, Telecommunications, and Computer Science",20071217,2006,,,576,577,The issue of Analytic Hierarchy Process for group decision making problems is the long-standing question under study. The multicriterion optimization task of the manufacturing process based on Analytic Hierarchy Process method is solved in the article.,,POD:966-553-507-2,10.1109/TCSET.2006.4404638,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4404638,Analytic Hierarchy Process;group decision making process;multicriteria decision making;pairwise comparisons,Chromium;Costs;Decision making;Manufacturing processes;Paper technology;Production;Pulp manufacturing;Robustness;Sensitivity analysis;Uncertainty,decision making;operations research;production management,analytic hierarchy process;group decision making problem;manufacturing process;multicriterion optimization task;production optimization,,0,,11,,no,Feb. 28 2006-March 4 2006,,IEEE,IEEE Conference Publications
Visual Analysis of Historic Hotel Visitation Patterns,C. Weaver; D. Fyfe; A. Robinson; D. Holdsworth; D. Peuquet; A. M. MacEachren,"The GeoVISTA Center and Department of Geography, The Pennsylvania State University, e-mail: cweaver@psu.edu",2006 IEEE Symposium On Visual Analytics Science And Technology,20061226,2006,,,35,42,"Understanding the space and time characteristics of human interaction in complex social networks is a critical component of visual tools for intelligence analysis, consumer behavior analysis, and human geography. Visual identification and comparison of patterns of recurring events is an essential feature of such tools. In this paper, we describe a tool for exploring hotel visitation patterns in and around Rebersburg, Pennsylvania from 1898-1900. The tool uses a wrapping spreadsheet technique, called reruns, to display cyclic patterns of geographic events in multiple overlapping natural and artificial calendars. Implemented as an improvise visualization, the tool is in active development through a iterative process of data collection, hypothesis, design, discovery, and evaluation in close collaboration with historical geographers. Several discoveries have inspired ongoing data collection and plans to expand exploration to include historic weather records and railroad schedules. Distributed online evaluations of usability and usefulness have resulted in numerous feature and design recommendations",,CD-ROM:1-4244-0592-0; POD:1-4244-0591-2,10.1109/VAST.2006.261428,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4035745,D.2.2 [Software Engineering]: Design Tools and TechniquesÌâåÀUser Interfaces;Geovisualization;H.5.2 [Information Systems]: Information Interfaces and PresentationÌâåÀUser Interfaces;coordinated multiple views;exploratory visualization;historical geography;travel pattern analysis,Calendars;Consumer behavior;Data visualization;Displays;Geography;Humans;Intelligent networks;Pattern analysis;Social network services;Wrapping,behavioural sciences computing;data visualisation;hotel industry;social sciences computing;spreadsheet programs,complex social network;consumer behavior analysis;display cyclic pattern;distributed online evaluation;geographic event;historic hotel visitation pattern;human geography;improvise visualization;intelligence analysis;iterative data collection;visual identification analysis;wrapping spreadsheet technique,,4,,18,,no,Oct. 31 2006-Nov. 2 2006,,IEEE,IEEE Conference Publications
Visual Analytics Education,J. Foley; S. Card; D. Ebert; A. MacEachren; B. Ribarsky,"Georgia Tech, foley@cc.gatech.edu",2006 IEEE Symposium On Visual Analytics Science And Technology,20061226,2006,,,209,211,"Visual analytics is a newly evolving field that spans across several more established disciplines. This panel discusses how VA system developers and researchers are best educated at the MS and PhD levels. This paper describes several ways in which VA can be characterized - with the goal of using these characterizations to identify knowledge domains that can be used to define VA curricula. Also, a digital library of VA educational resources is described",,CD-ROM:1-4244-0592-0; POD:1-4244-0591-2,10.1109/VAST.2006.261419,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4035767,Visual Analytics Curricula;Visual Analytics Education,Computer science education;Data visualization;Financial management;Information analysis;Knowledge management;Software development management;Software libraries;Taxonomy;Time series analysis;Visual analytics,computer science education;data analysis;data visualisation;digital libraries,digital library;educational resources;knowledge domain identification;visual analytics education,,0,,2,,no,Oct. 31 2006-Nov. 2 2006,,IEEE,IEEE Conference Publications
Visualizing the Performance of Computational Linguistics Algorithms,S. G. Eick; J. Mauger; A. Ratner,"SSS Research, Inc. eick@sss-research.com",2006 IEEE Symposium On Visual Analytics Science And Technology,20061226,2006,,,151,157,"We have built a visualization system and analysis portal for evaluating the performance of computational linguistics algorithms. Our system focuses on algorithms that classify and cluster documents by assigning weights to words and scoring each document against high dimensional reference concept vectors. The visualization and algorithm analysis techniques include confusion matrices, ROC curves, document visualizations showing word importance, and interactive reports. One of the unique aspects of our system is that the visualizations are thin-client Web-based components built using SVG visualization components",,CD-ROM:1-4244-0592-0; POD:1-4244-0591-2,10.1109/VAST.2006.261417,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4035760,AJAX;ROC curves;SVG;confusion matrices;document categorization;thin-client,Algorithm design and analysis;Clustering algorithms;Computational linguistics;Content based retrieval;Information retrieval;Performance analysis;Portals;Storage automation;Text analysis;Visualization,computational linguistics;document handling;portals;program visualisation;software performance evaluation,AJAX;ROC curves;SVG visualization components;analysis portal;computational linguistics algorithm performance visualization;confusion matrices;document classification;document clustering;document visualizations;interactive reports;scalable vector graphics;thin-client Web-based components,,1,1,6,,no,Oct. 31 2006-Nov. 2 2006,,IEEE,IEEE Conference Publications
A Decision model for Selecting an Offshore Outsourcing Location: Using a Multicriteria Method,Z. K. Lin; J. J. Wang; Y. Y. Qin,"School of Computer Science & Technology, Dalian Maritime University, Dalian Liaoning, 116026, PR. China","2007 IEEE International Conference on Service Operations and Logistics, and Informatics",20071119,2007,,,1,5,"As the offshoring becomes more widespread, business practitioners outsource their activities to overseas countries. Although there is a wealth of academic literature examining outsourcing and offshoring, there is little academic literature that addresses the current outsourcing decision most firms facing, which is where to outsource. Given multi-attribute nature of offshore location selection, this paper argues that five factors should be considered for decisions, and proposes the use of analytic hierarchy process (AHP) and PROMETHEE as aids in making offshore location selection decisions. AHP is used to analyze the structure of the location selection problem and determine weights of the criteria, and PROMETHEE method is used for final ranking, together with changing weights for a sensitivity analysis. It shows by means of an application that the hybrid method is very well suited as a decision-making tool for the offshore location selection decision. Finally, potential issues for future research are presented.",,CD-ROM:978-1-4244-1118-4; POD:978-1-4244-1117-7,10.1109/SOLI.2007.4383936,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4383936,,Application software;Companies;Computer science;Decision making;Educational institutions;Fuel economy;Information technology;Outsourcing;Sensitivity analysis;Technology management,,,,1,,19,,no,27-29 Aug. 2007,,IEEE,IEEE Conference Publications
A Discrete Differential Operator for Direction-based Surface Morphometry,M. Boucher; O. Lyttelton; S. Whitesides; A. Evans,"School of Computer Science, McGill University, Montr&#233;al, Canada. boucher@bic.mni.mcgill.ca",2007 IEEE 11th International Conference on Computer Vision,20071226,2007,,,1,8,"This paper presents a novel directional morphometry method for surfaces using first order derivatives. Non-directional surface morphometry has been previously used to detect regions of cortical atrophy using brain MRI data. However, evaluating directional changes on surfaces requires computing gradients to obtain a full metric tensor. Non-directionality reduces the sensitivity of deformation-based morphometry to area-preserving deformations. By proposing a method to compute directional derivatives, this paper enables analysis of directional deformations on surfaces. Moreover, the proposed method exhibits improved numerical accuracy when evaluating mean curvature, compared to the so-called cotangent formula. The directional deformation of folding patterns was measured in two groups of surfaces and the proposed methodology allowed to defect morphological differences that were not detected using previous non-directional morphometry. The methodology uses a closed-form analytic formalism rather than numerical approximation and is readily generalizable to any application involving surface deformation.",1550-5499;15505499,CD-ROM:978-1-4244-1631-8; POD:978-1-4244-1630-1,10.1109/ICCV.2007.4408886,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4408886,,Application software;Brain;Computer graphics;Computer science;Equations;Magnetic resonance imaging;Shape measurement;Surface morphology;Tensile stress;Turing machines,biomedical MRI;computational geometry;differential geometry;function approximation;mathematical operators;medical image processing;neurophysiology;surface fitting,brain MRI data;closed-form analytic formalism;differential geometry;direction-based surface morphometry;discrete differential operator;discrete linear approximation;first order derivatives,,1,,26,,no,14-21 Oct. 2007,,IEEE,IEEE Conference Publications
"A distributed-memory, parallel MLFMA for the solution of radiation and scattering problems in complex environments",A. J. Hesford; W. C. Chew,"Center for Computational Electromagnetics and Electromagnetics, Laboratory, Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, 61801-2991, USA",2007 IEEE Antennas and Propagation Society International Symposium,20071206,2007,,,3448,3451,"A distributed-memory parallel MLFMA has been designed to handle simulation of electromagnetic radiation and scattering in the presence of complex objects. The code has been validated against previously-validated software, analytic solutions and physical optics approximations. For more complex problems, where reference data is not available, the resulting solutions conform to expectations based on physical reasoning.",1522-3965;15223965,CD-ROM:978-1-4244-0878-8; POD:978-1-4244-0877-1,10.1109/APS.2007.4396279,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4396279,,Acceleration;Computational electromagnetics;Computational modeling;Computer interfaces;Computer simulation;Concurrent computing;Distributed computing;Electromagnetic radiation;Electromagnetic scattering;Libraries,approximation theory;electromagnetic wave scattering;physical optics,complex environments;distributed-memory parallel MLFMA;electromagnetic radiation;electromagnetic scattering;multilevel fast multipole algorithm;physical optics approximations,,2,,6,,no,9-15 June 2007,,IEEE,IEEE Conference Publications
A function modulation method for digital communications,S. Das; A. Singh; N. Mohanty,"CCSI, West Hills, California, USA",2007 Wireless Telecommunications Symposium,20080715,2007,,,1,8,"We present a novel modulation method for digital communication systems. The method uses orthogonal and/or non-orthogonal analytic functions as symbols. It is shown that, in the case of orthogonal functions, to transmit M bits using one symbol, the new receiver needs to search over only M functions as opposed to 2 symbols. A numerical method for generating the analytic symbols, called Constrained Gram Schmidt method, is presented. Experimental results demonstrating the proof of concept of the approach are also discussed.",1934-5070;19345070,CD-ROM:978-1-4244-0697-5; POD:978-1-4244-0696-8,10.1109/WTS.2007.4563308,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4563308,,Amplitude shift keying;Design methodology;Digital communication;Digital modulation;Frequency modulation;Frequency shift keying;Phase shift keying;Software radio;Transmitters;Voltage-controlled oscillators,digital communication;modulation;numerical analysis,analytic symbols;constrained Gram Schmidt method;digital communication systems;function modulation method;nonorthogonal analytic functions;numerical method;orthogonal analytic functions,,0,,10,,no,26-28 April 2007,,IEEE,IEEE Conference Publications
A Fuzzy AHP approach to evaluating e-commerce websites,Y. w. Liu; Y. j. Kwon; B. d. Kang,"Daegu University, Korea","5th ACIS International Conference on Software Engineering Research, Management & Applications (SERA 2007)",20070904,2007,,,114,124,"As e-commerce is playing a more and more important role in our daily life, there is no wonder that significant attention is being focused on the evaluation of e-commerce Web sites in recent years. In this paper, a fuzzy analytic hierarchy process (FAHP) approach is used to evaluating e-commerce Web sites, which can tolerate vagueness and uncertainty of judgment. Therefore, the insufficiency and imprecision problems associated with the conventional AHP can be solved. Hence, Web sites can be evaluated more reasonably. In the end, a case study is presented to make this approach more understandable for a decision-maker(s).",,POD:978-0-7695-2867-0,10.1109/SERA.2007.16,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4296926,E-commerce websites;FAHP;conventional AHP;evaluation,Application software;Conference management;Costs;Engineering management;Internet;Mirrors;Quality management;Software engineering;Uncertainty;Uniform resource locators,Web sites;electronic commerce;fuzzy set theory,Web site;e-commerce;fuzzy analytic hierarchy process,,3,,42,,no,20-22 Aug. 2007,,IEEE,IEEE Conference Publications
A Fuzzy Model for Selecting Software,H. S. Lee; M. H. Wang,"National Taiwan Ocean University, Taiwan",Fourth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD 2007),20071218,2007,3,,411,415,"A fuzzy AHP model is proposed in this paper for the selection of software. Since the priorities of selection criteria obtained by traditional fuzzy AHP are dependent, in stead of normalizing priorities and aggregating the performance of software by simple weighted average, in this paper we propose a nonadditive aggregation method based on fuzzy integral to aggregate the performance of software.",,POD:0-7695-2874-0,10.1109/FSKD.2007.36,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4406271,,Aggregates;Application software;Documentation;Fuzzy systems;Management training;Marine transportation;Oceans;Software packages;Software performance;Technology management,fuzzy set theory;software performance evaluation;software selection,analytic hierarchy process;fuzzy AHP model;fuzzy integral;nonadditive aggregation method;simple weighted average;software performance;software selection criteria,,0,,14,,no,24-27 Aug. 2007,,IEEE,IEEE Conference Publications
A Lightweight Value-based Software Architecture Evaluation,C. K. Kim; D. H. Lee; I. Y. Ko; J. Baik,"ETRI, Korea","Eighth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing (SNPD 2007)",20070813,2007,2,,646,649,"Current software engineering practice is focused on value-neutral processes. Value-based architecting, one of value-based software engineering agendas, involves the further consideration of the system objectives associated with different stakeholder values in selecting an optimal architectural alternative. There are several value-based architectural evaluation techniques and cost benefit analysis method (CBAM) is a widely used, established technique based on return on investment (ROI). The weaknesses of the existing techniques are uncertainties from several subjective errors and the heavyweight process, which requires many steps and participation of stakeholders. This paper proposes a lightweight value-based architecture evaluation technique, called LiVASAE, using analytic hierarchy process (AHP), which can support a multi-criteria decision-making process. The proposed technique can help overcome the major weakness of the existing techniques such as the uncertainties caused by subjective decision making and heavy-weight process for architecture evaluations. LiVASAE provides a way to measure the uncertainty level using AHP's consistency rate (CR) and It also provides three simplified evaluation steps. In addition, the LiVASAE presents a framework for decision makers to make technical decisions associated with business goals (or values) such as cost, time-to-market, and integration with legacy system.",,POD:0-7695-2909-7,10.1109/SNPD.2007.507,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4287763,,Artificial intelligence;Computer architecture;Cost benefit analysis;Decision making;Distributed computing;Investments;Software architecture;Software engineering;Time to market;Uncertainty,cost-benefit analysis;software architecture;software performance evaluation,LiVASAE;analytic hierarchy process;cost benefit analysis;lightweight value-based software architecture evaluation;multicriteria decision making;return on investment;software engineering,,1,,4,,no,July 30 2007-Aug. 1 2007,,IEEE,IEEE Conference Publications
A Manager Framework of Supply Chain Operational Reference Model,Y. Tan; Z. Y. Cai; W. X. Mei,"Sch. of Manage., Huazhong Univ. of Sci. & Technol., Wuhan","2007 International Conference on Wireless Communications, Networking and Mobile Computing",20071008,2007,,,4767,4771,"It is very necessary for supply chain operational managers to adopt robust and practicable design and evaluation tools. Many researchers focused on high level strategic aspects of supply chain design whose results are usually generic guidelines for business executives rather than specific tools for operational managers. In this paper, a decision-based framework for supply chain operational managers is presented and can be used to select suppliers, and so on. The methodology combines the techniques of analytic hierarchy process and preemptive goal programming. The performance metrics based on supply chain operational reference model level I are incorporated into the proposed model as the decision criteria. Additionally, a set of performance metrics is developed to evaluate the overall supply chain effectiveness, allowing direct comparison of different supply chain designs for operational managers.",2161-9646;21619646,CD-ROM:1-4244-1312-5; POD:1-4244-1311-7,10.1109/WICOM.2007.1170,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4340945,,Computer aided instruction;Design engineering;Guidelines;Inventory management;Management information systems;Manufacturing;Measurement standards;Software standards;Supply chain management;Supply chains,mathematical programming;supply chain management,analytic hierarchy process;business executives;decision-based framework;preemptive goal programming;supply chain design;supply chain operational managers;supply chain operational reference model,,0,,9,,no,21-25 Sept. 2007,,IEEE,IEEE Conference Publications
A New Modulation Method for Digital Communication,S. Das; N. Mohanty; A. Singh,"CCSI, 24300 Abbeywood Drive, West Hills, California 91307, USA, ccsi@ccsi-ca.com",2007 12th IEEE Symposium on Computers and Communications,20071112,2007,,,7,12,"We present the design of a transmitter and a receiver for digital communication system that uses function modulation (fm) method. The design covers both orthogonal and non-orthogonal analytic functions. The design is based on the concept of software radio on a general purpose Digital Signal Processor (DSP). A numerical method for generating the waveforms required for the fm system is presented. Experimental results on real environment, demonstrating the feasibility of the concept, are also discussed.",1530-1346;15301346,CD-ROM:978-1-4244-1521-2; POD:978-1-4244-1520-5,10.1109/ISCC.2007.4381583,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4381583,,Design methodology;Digital communication;Digital modulation;Digital signal processing;Frequency modulation;Frequency shift keying;Radio transmitters;Receivers;Software radio;Voltage-controlled oscillators,digital radio;modulation;radio receivers;radio transmitters;software radio,digital communication modulation method;digital signal processor;function modulation;nonorthogonal analytic function;software radio,,1,,9,,no,1-4 July 2007,,IEEE,IEEE Conference Publications
A New Performance Evaluation Model and AHP-Based Analysis Method in Service-Oriented Workflow,B. Liu; Y. Fan,"Tsinghua University, Beijing",Sixth International Conference on Grid and Cooperative Computing (GCC 2007),20070827,2007,,,685,692,"In service-oriented architecture, services and workflows are closely related so that the research on service-oriented workflow attracts the attention of academia. Because of the loosely-coupled, autonomic and dynamic nature of service, the operation and performance evaluation of workflow meet some challenges, such as how to judge the quality of service (QoS) and what is the relation between QoS and workflow performance. In this paper we are going to address these challenges. First the definition of service is proposed, and the characteristics and operation mechanism of service-oriented workflow are presented. Then a service-oriented workflow performance evaluation model is described which combines the performance of the business system and IT system. The key performance indicators (KPI) are also depicted with their formal representation. Finally the improved Analytic Hierarchy Process is brought forward to analyze the correlation between different KPIs and select services.",2160-4908;21604908,POD:0-7695-2871-6,10.1109/GCC.2007.20,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4293848,,Automation;Grid computing;Manufacturing processes;Performance analysis;Petri nets;Quality of service;Resource management;Service oriented architecture;Stochastic processes;Web services,object-oriented methods;performance evaluation;quality of service;workflow management software,AHP based analysis method;IT system;QoS;analytic hierarchy process;business system;key performance indicators;quality of service;service-oriented architecture;service-oriented workflow performance evaluation model,,2,1,13,,no,16-18 Aug. 2007,,IEEE,IEEE Conference Publications
A one-dimensional semi-analytic model of the microwave heating effect in verification of numerical hybrid modeling software,P. Kopyt,"Institute of Radioelectronics, Warsaw University of Technology, Warsaw, Poland, e-mail: pkopyt@elka.pw.edu.pl","EUROCON 2007 - The International Conference on Computer as a Tool""""",20071226,2007,,,65,72,In this paper a semi-analytic model of the microwave heating effect has been briefly presented. The paper illustrates also an application of the model in verification of the computational accuracy of a commercially available electromagnetic simulator coupled with an FDTD-based thermal solver prepared by the author. In a series of simulations the behavior of the numerical model has been verified against the semi-analytic model for media of various losses as well as different relationships between temperature and the medium properties. The obtained results confirm the expectations based on the known properties of the computational methods employed in the numerical model.,,CD-ROM:978-1-4244-0813-9; POD:978-1-4244-0812-2,10.1109/EURCON.2007.4400464,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4400464,FDTD;microwave heating;multiphysics;verification,Computational modeling;Computer simulation;Electromagnetic coupling;Electromagnetic heating;Finite difference methods;Microwave ovens;Numerical models;Predictive models;Time domain analysis;Water heating,electrical engineering computing;finite difference time-domain analysis;microwave heating,FDTD-based thermal solver;microwave heating effect;numerical hybrid modeling software verification;one-dimensional semi-analytic model,,2,,22,,no,9-12 Sept. 2007,,IEEE,IEEE Conference Publications
A Service-oriented Architecture for Business Intelligence,L. Wu; G. Barash; C. Bartolini,HP Software,IEEE International Conference on Service-Oriented Computing and Applications (SOCA '07),20070716,2007,,,279,285,"Business intelligence is a business management term used to describe applications and technologies which are used to gather, provide access to and analyze data and information about the organization, to help make better business decisions. In other words, the purpose of business intelligence is to provide actionable insight Business intelligence technologies include traditional data warehousing technologies such as reporting, ad-hoc querying, online analytical processing (OLAP). More advanced business intelligence tools - such as HP Openview DecisionCenter - also include data-mining, predictive analysis using rule-based simulations, Web services and advanced visualization capabilities. In this paper we describe a service-oriented architecture for business intelligence that makes possible a seamless integration of technologies into a coherent business intelligence environment, thus enabling simplified data delivery and low-latency analytics. We compare our service-oriented approach with traditional BI architectures, illustrate the advantages of the service oriented paradigm and share our experience and the lessons learned in architecting and implementing the framework.",2163-2871;21632871,POD:0-7695-2861-9,10.1109/SOCA.2007.6,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4273437,,Analytical models;Bismuth;Data analysis;Data visualization;Information analysis;Predictive models;Service oriented architecture;Technology management;Warehousing;Web services,competitive intelligence,Web services;business decisions;business intelligence;data mining;data warehousing;predictive analysis;rule-based simulations;service-oriented architecture,,19,1,10,,no,19-20 June 2007,,IEEE,IEEE Conference Publications
A Study on Performance Measurement of a Plastic Packaging Organization's Manufacturing System by AHP Modeling,G. T. Temur; B. Emeksizoglu; S. Gozlu,"Istanbul Technical University, Faculty of Management, Istanbul, TURKEY",PICMET '07 - 2007 Portland International Conference on Management of Engineering & Technology,20071015,2007,,,1256,1263,"By the effect of globalization, products, services, capital, technology, and people began to circulate more freely in the world. As a conclusion, in order to achieve and gain an advantage against competitors, manufacturing firms had to adopt themselves to changing conditions and evaluate their critical performance criteria. In this study, the aim is to determine general performance criteria and their characteristics and classifications from previous studies and evaluate performance criteria for a plastic packaging organization by utilizing analytic hierarchy process (AHP) modeling. A specific manufacturing organization, operating in the Turkish plastic packaging sector has been selected and the manufacturing performance criteria have been determined for that specific organization. Finally, the selected criteria have been assessed according to their relative importance by utilizing AHP approach and expert choice (EC) software program. As a result of this study, operating managers chose cost, quality, customer satisfaction and time factors as criteria for this organization. As the findings of the study indicate, the manufacturing organization operating in the plastic packaging sector, overviews its operations and measures its manufacturing performance basically on those four criteria and their sub criteria. Finally, relative importance of those main measures and their sub criteria are determined in consideration to plastic packaging sector.",2159-5100;21595100,CD-ROM:978-1-8908-4315-1,10.1109/PICMET.2007.4349449,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4349449,,Costs;Customer satisfaction;Globalization;Manufacturing systems;Measurement;Performance analysis;Performance gain;Plastic packaging;Quality management;Time factors,customer satisfaction;manufacturing systems;organisational aspects;plastic packaging;production engineering computing;statistical analysis,Turkish plastic packaging sector;analytic hierarchy process modeling;customer satisfaction;expert choice software program;globalization;manufacturing firms;manufacturing organization;manufacturing performance criteria;manufacturing system;plastic packaging organization,,0,,23,,no,5-9 Aug. 2007,,IEEE,IEEE Conference Publications
Abstract Syntax Trees - and their Role in Model Driven Software Development,G. Fischer; J. Lusiardi; J. Wolff von Gudenberg,University of Wurzburg,International Conference on Software Engineering Advances (ICSEA 2007),20070910,2007,,,38,38,Abstract syntax trees (ASTs) are known from compiler construction where they build the intermediate data format which is passed from the analytic front-end to the synthetic back-end. In model driven software development ASTs are used as a model of the source code. The object management group (OMG) has issued a request for proposals for AST models. Various levels of abstraction can be introduced. ASTs can be used for program analysis and for program transformation. In this paper we present an eclipse based representation framework for ASTs.,,CD-ROM:978-0-7695-2937-0; POD:0-7695-2937-2,10.1109/ICSEA.2007.12,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4299919,,Aggregates;Cloning;Concrete;Data analysis;Informatics;Performance analysis;Program processors;Programming;Proposals;Unified modeling language,abstract data types;computational linguistics;object-oriented programming;program compilers;program diagnostics;tree data structures,abstract syntax trees;intermediate data format;model driven software development;object management group;program analysis,,1,,18,,no,25-31 Aug. 2007,,IEEE,IEEE Conference Publications
Activity Analysis Using Spatio-Temporal Trajectory Volumes in Surveillance Applications,F. Janoos; S. Singh; O. Irfanoglu; R. Machiraju; R. Parent,"Dept. of Computer Science and Engineering, Ohio State University. e-mail: janoos@cse.ohio-state.edu",2007 IEEE Symposium on Visual Analytics Science and Technology,20071127,2007,,,3,10,"In this paper, we present a system to analyze activities and detect anomalies in a surveillance application, which exploits the intuition and experience of security and surveillance experts through an easy- to-use visual feedback loop. The multi-scale and location specific nature of behavior patterns in space and time is captured using a wavelet-based feature descriptor. The system learns the fundamental descriptions of the behavior patterns in a semi-supervised fashion by the higher order singular value decomposition of the space described by the training data. This training process is guided and refined by the users in an intuitive fashion. Anomalies are detected by projecting the test data into this multi-linear space and are visualized by the system to direct the attention of the user to potential problem spots. We tested our system on real-world surveillance data, and it satisfied the security concerns of the environment.",,POD:978-1-4244-1659-2,10.1109/VAST.2007.4388990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4388990,HOSVD;anomaly detection;surveillance;trajectory;wavelets,Application software;Cameras;Collaboration;Computer vision;Data security;Feeds;Humans;Surveillance;System testing;Trajectory,computer vision;feature extraction;learning (artificial intelligence);security of data;singular value decomposition;video surveillance;wavelet transforms,activity analysis;anomaly detection;computer vision;semisupervised learning;singular value decomposition;spatio-temporal trajectory volume;surveillance application;visual feedback loop;wavelet-based feature descriptor,,6,,29,,no,Oct. 30 2007-Nov. 1 2007,,IEEE,IEEE Conference Publications
Admission control in data transfers over lightpaths,W. Golab; R. Boutaba,"Dept. of Comput. Sci., Univ. of Toronto, Toronto, Canada",IEEE Journal on Selected Areas in Communications,20070808,2007,25,6,102,110,"The availability of optical network infrastructure and appropriate user control software has recently made it possible for scientists to establish end-to-end circuits across multiple management domains in support of large data transfers. These high-performance data paths are typically provisioned over 10 Gigabit optical links, and accessed using ethernet encapsulation at Gigabit and 10 Gigabit rates. The resulting mixture of circuit sizes gives rise to resource conflicts whereby requests to allocate bandwidth partitions are blocked despite vast underutilization of the optical link. In an attempt to remedy this problem, we investigate intelligent admission control policies that consider the long-term effects of admission decisions. Using analytic techniques we show that the greedy policy, which accepts requests to allocate bandwidth partitions whenever sufficient bandwidth exists, is suboptimal in a pertinent scenario. We then consider dynamic online computation of the optimal admission control policy and show that the acceptance ratio of requests to establish end-to-end circuits can be improved by up to 19% on a fifteen-node network where the behaviour of each link is governed by a local optimization effort.",0733-8716;07338716,,10.1109/JSAC-OCN.2007.024806,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4285335,,Admission control;Bandwidth;Circuits;Encapsulation;Ethernet networks;Intelligent control;Optical control;Optical fiber communication;Optical fiber networks;Resource management,bandwidth allocation;intelligent control;optical fibre LAN;optical links;telecommunication congestion control,Ethernet;bandwidth allocation;data transfers;intelligent admission control;multiple management domains;optical links;optical network infrastructure;user control software;user-controlled networks,,3,,17,,no,7-Aug,,IEEE,IEEE Journals & Magazines
Adoption of e-Commerce: A decision theoretic framework and an illustrative application,M. Quaddus; Jun Xu,"Curtin University of Technology, Perth, Australia",2007 10th international conference on computer and information technology,20080725,2007,,,1,6,"Electronic commerce (e-Commerce or EC), in some form or other, is changing the way organizations do their business. Many organizations (banks etc) are forcing their customers to adopt e-Commerce. Others are adopting e-Commerce for competitive necessity. This raises the obvious question: How can organizations adopt appropriate e-Commerce model judiciously? This paper addresses the above research question. We use a decision theoretic framework based on multiple attributes of e-Commerce. Extensive literature review revealed a number of factors or attributes that either act as drivers or barriers of e-Commerce success. A well known decision theoretic approach based on multiple attribute, called analytic hierarchy process (AHP), is used to develop a comprehensive model of e-Commerce adoption. Our framework can be used as a guide to select the appropriate e-Commerce model. Real world data, from EC consultants, have been collected for a hypothetical SME which is embarking on adopting an e-Commerce model. The paper presents the application of the framework for this SME.",,CD-ROM:978-1-4244-1551-9; POD:978-1-4244-1550-2,10.1109/ICCITECHN.2007.4579406,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4579406,Decision theoretic approach;E-Commerce adoption;SME,Application software;Australia;Business communication;Consumer electronics;Costs;Customer service;Electronic commerce;IP networks;Internet;Portals,decision theory;electronic commerce;small-to-medium enterprises,SME;analytic hierarchy process;decision theoretic framework;e-commerce;electronic commerce,,0,,19,,no,27-29 Dec. 2007,,IEEE,IEEE Conference Publications
Agent-based Human-computer-interaction for Real-time Monitoring Systems in the Trucking Industry,E. I. Krauth; J. V. Hillegersberg; S. L. Van De Velde,RSM Erasmus University,"System Sciences, 2007. HICSS 2007. 40th Annual Hawaii International Conference on",20070129,2007,,,27,27,"Auto ID systems can replace time-consuming, costly and error-prone processes of human data entry and produce detailed real time information. However, they would add value only to the extent that data is presented in a user-friendly manner. As model-based decision support is not always adequate, an agent-based approach is often chosen. Real life entities such as orders and trucks are represented by agents, which negotiate in order to solve planning problems. For the respective data representation at least two forms can be distinguished, focusing either on (1) resources (account-based) or (2) orders (order-centric). Applying cognitive fit theory we describe how the different interfaces affect decision making. The hypotheses would be tested in a laboratory experiment. The intended contribution should support that order-centric interfaces have higher user-friendliness and are especially beneficial to low-analytics and planners working under time pressure",1530-1605;15301605,Electronic:0-7695-2755-8,10.1109/HICSS.2007.52,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4076424,,Decision making;Decision support systems;Humans;Laboratories;Logistics;Monitoring;Radiofrequency identification;Real time systems;Testing;Transportation,data structures;decision making;human computer interaction;human factors;industrial engineering;monitoring;planning (artificial intelligence);real-time systems;software agents;vehicles,agent-based human-computer-interaction;auto ID systems;cognitive fit theory;data representation;decision making;model-based decision support;planning problems;real-time monitoring systems;truck industry,,0,,35,,no,Jan. 2007,,IEEE,IEEE Conference Publications
AHP-based Master Device Selection Scheme for Pervasive Personal Network,H. Zheng; Z. lei; L. Wei; Y. Yaoyao; S. Juwei,"School of Telecommunications Engineering, Beijing University of Posts and Telecommunications (BUPT), 100876. Email: huzheng.red@gmail.com",2007 Second International Conference on Communications and Networking in China,20080307,2007,,,909,913,"The advancement of mobile communications and small computers bring pervasive personal network closer to reality. To reduce user intervention in the pervasive environment, we proposed a framework to compose a virtual terminal over distributed devices among the pervasive personal network, called UST Since Master Device performs the framework functionality in UST, selecting proper master device plays a vital role in ensuring quality of service especially. In this article we develop a master device selection scheme for UST system based on the Analytic Hierarchy Process (AHP). It also describes the state transitions and message sequences for the scheme. Experimental results show that the proposed scheme can effectively choose the optimum master device under diverse pervasive personal network conditions.",,CD-ROM:978-1-4244-1009-5; POD:978-1-4244-1008-8,10.1109/CHINACOM.2007.4469531,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4469531,,Application software;Computer architecture;Context;Costs;Educational institutions;Mobile communication;Network servers;Network topology;Pervasive computing;Wireless sensor networks,mobile communication;mobile computing;personal area networks;quality of service,AHP-based master device selection scheme;analytic hierarchy process;distributed devices;framework functionality;master device;message sequences;mobile communications;pervasive personal network;quality of service;user intervention;virtual terminal,,0,,11,,no,22-24 Aug. 2007,,IEEE,IEEE Conference Publications
An SMSE Implementation of CDMA with Partial Band Interference Suppression,M. L. Roberts; M. A. Temple; R. F. Mills; R. A. Raines,"Air Force Inst. of Technol., Wright-Patterson AFB",IEEE GLOBECOM 2007 - IEEE Global Telecommunications Conference,20071226,2007,,,4424,4428,"A spectrally modulated, spectrally encoded (SMSE) framework is adopted for implementing code division multiple access (CDMA) with partial band interference suppression. SMSE signals are constructed within an architecture governed by cognitive radio (CR) principles and supported by software defined radio (SDR) implementation, a union referred to as CR- based SDR. Orthogonal frequency division multiplexing (OFDM) signals, a foundational part of future 4G systems, are collectively classified as SMSE because data modulation and encoding are applied in the spectral domain. Framework applicability was demonstrated for realistic 4G signals by illustrating consistency between resultant analytic SMSE expressions and published results. Framework implementability and flexibility is further demonstrated herein using more complex CDMA signal structures. Modeling and simulation results are presented for a form of multi-carrier CDMA using polyphase codes with adaptive spectral notching (interference avoidance). Collectively, the SMSE implementation of this system correlates very well with theoretical predictions for multiple access scenarios.",1930-529X;1930529X,CD-ROM:978-1-4244-1043-9; POD:978-1-4244-1042-2,10.1109/GLOCOM.2007.841,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4411751,,Chromium;Cognitive radio;Computer architecture;Encoding;Interference suppression;Modulation coding;Multiaccess communication;OFDM modulation;Signal analysis;Software radio,4G mobile communication;OFDM modulation;code division multiple access;codes;cognitive radio;interference suppression;modulation coding;software radio,4G systems;adaptive spectral notching;code division multiple access;cognitive radio;data encoding;data modulation;interference avoidance;orthogonal frequency division multiplexing signals;partial band interference suppression;polyphase codes;software defined radio;spectral domain;spectrally modulated spectrally encoded framework,,0,,12,,no,26-30 Nov. 2007,,IEEE,IEEE Conference Publications
Analytic Hierarchy Process Based on Fuzzy Logic,N. Mironova; K. Hafizova,"Zaporizhzhya National Technical University, Software Engineering Department, 64, Zhukovskogo Street, Zaporizhzhya, 69063, Ukraine, E-mail: mironovanata@ukr.net",2007 9th International Conference - The Experience of Designing and Applications of CAD Systems in Microelectronics,20070904,2007,,,186,187,The issue of analytic hierarchy process for decision making problems is the long-standing question under study. The improved analytic hierarchy process based on fuzzy logic is considered.,,POD:966-533-587-0,10.1109/CADSM.2007.4297520,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4297520,Analytic Hierarchy Process;aggregative membership function;consistency index;consistency ratio;fuzzy logic;fuzzy set;fuzzy triangular number;maximal eigenvalue;maximum value;pairwise comparisons,Algorithm design and analysis;Chromium;Decision making;Eigenvalues and eigenfunctions;Fuzzy logic;Fuzzy sets;Manufacturing;Problem-solving;Resource management;Strategic planning,decision making;eigenvalues and eigenfunctions;fuzzy logic;fuzzy set theory;hierarchical systems,aggregative membership function;analytic hierarchy process;decision making problems;fuzzy logic;fuzzy set;fuzzy triangular number;maximal eigenvalue;pairwise comparisons,,0,,2,,no,19-24 Feb. 2007,,IEEE,IEEE Conference Publications
Analytic Hierarchy Process for Technology Policy: Case Study the Costa Rican Digital Divide,A. M. A. Baez; D. F. Kocaoglu,"Dept. of Engineering and Technology Management, Portland State University, USA",PICMET '07 - 2007 Portland International Conference on Management of Engineering & Technology,20071015,2007,,,1195,1208,"This article consists of presentation slides on the following subject. This paper presents an application of the analytic hierarchy process (AHP) for technology policy. Developing countries face the great challenge to bridge their internal digital divide. However, most studies seeking to bridge this gap offer only recommendations at the policy level. With the use of AHP as a methodology we can provide appropriate information about which technologies will have the greatest impact on bridging this gap.",2159-5100;21595100,CD-ROM:978-1-8908-4315-1,10.1109/PICMET.2007.4349443,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4349443,,Application software;Appropriate technology;Bridges;Decision making;Paper technology;Petrochemicals;Project management;Research and development;Technological innovation;Technology management,technology management,AHP;Costa Rican digital divide;analytic hierarchy process;technology policy,,0,,40,,no,5-9 Aug. 2007,,IEEE,IEEE Conference Publications
Analytic Model for Web Anomalies Classification,N. Alaeddine; J. Tian,"Southern Methodist Univ., Dallas","High Assurance Systems Engineering Symposium, 2007. HASE '07. 10th IEEE",20071217,2007,,,395,396,"In this paper, an analytic technique is proposed to improve the dynamic Web application quality and reliability. The technique integrates orthogonal defect classification (ODC), and Markov chain to classify as well as analyze the collective view of Web errors. The error collective view will be built from access logs and defect data. This classification technique will enable viewing the Web errors in page, path, and application contexts. This technique will help in developing reliable Web applications that benefit from the understanding of Web anomalies and past issues. The preliminary results of applying this approach to a case study from telecommunications industry are included to demonstrate its' viability.",1530-2059;15302059,POD:0-7695-3043-5,10.1109/HASE.2007.13,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4404772,,Application software;Computer science;Data mining;Error analysis;Performance analysis;Reliability engineering;Systems engineering and theory;USA Councils;Web pages;Web server,Internet;Markov processes;pattern classification;software reliability,Markov chain;Web anomalies classification;orthogonal defect classification,,0,,18,,no,14-16 Nov. 2007,,IEEE,IEEE Conference Publications
Analytics-driven solutions for customer targeting and sales-force allocation,R. Lawrence; C. Perlich; S. Rosset; J. Arroyo; M. Callahan; J. M. Collins; A. Ershov; S. Feinzig; I. Khabibrakhmanov; S. Mahatma; M. Niemaszyk; S. M. Weiss,"IBM Research Division, Thomas J. Watson Research Center, P.O. Box 218, Yorktown Heights, New York 10598, USA",IBM Systems Journal,20100406,2007,46,4,797,816,"Sales professionals need to identify new sales prospects, and sales executives need to deploy the sales force against the sales accounts with the best potential for future revenue. We describe two analytics-based solutions developed within IBM to address these related issues. The Web-based tool OnTARGET provides a set of analytical models to identify new sales opportunities at existing client accounts and noncustomer companies. The models estimate the probability of purchase at the product-brand level. They use training examples drawn from historical transactions and extract explanatory features from transactional data joined with company firmographic data (e.g., revenue and number of employees). The second initiative, the Market Alignment Program, supports sales-force allocation based on field-validated analytical estimates of future revenue opportunity in each operational market segment. Revenue opportunity estimates are generated by defining the opportunity as a high percentile of a conditional distribution of the customer's spending, that is, what we could realistically hope to sell to this customer. We describe the development of both sets of analytical models, the underlying data models, and the Web sites used to deliver the overall solution. We conclude with a discussion of the business impact of both initiatives.",0018-8670;00188670,,10.1147/sj.464.0797,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5386583,,,,,,4,,,,no,2007,,IBM,IBM Journals & Magazines
Analyzing Large-Scale News Video Databases to Support Knowledge Visualization and Intuitive Retrieval,H. Luo; J. Fan; J. Yang; W. Ribarsky; S. Satoh,"Software Engineering Institute, East China Normal University, Shanghai, China. e-mail: memcache@gmail.com",2007 IEEE Symposium on Visual Analytics Science and Technology,20071127,2007,,,107,114,"In this paper, we have developed a novel framework to enable more effective investigation of large-scale news video database via knowledge visualization. To relieve users from the burdensome exploration of well-known and uninteresting knowledge of news reports, a novel interestingness measurement for video news reports is presented to enable users to find news stories of interest at first glance and capture the relevant knowledge in large-scale video news databases efficiently. Our framework takes advantage of both automatic semantic video analysis and human intelligence by integrating with visualization techniques on semantic video retrieval systems. Our techniques on intelligent news video analysis and knowledge discovery have the capacity to enable more effective visualization and exploration of large-scale news video collections. In addition, news video visualization and exploration can provide valuable feedback to improve our techniques for intelligent news video analysis and knowledge discovery.",,POD:978-1-4244-1659-2,10.1109/VAST.2007.4389003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4389003,Knowledge Discovery;Knowledge Visualization;Semantic Video Classification,Artificial intelligence;Broadcasting;Data mining;Displays;Information retrieval;Large-scale systems;Multimedia communication;USA Councils;Visual databases;Visualization,content-based retrieval;data mining;semantic networks;video databases;video retrieval,automatic semantic video analysis;human intelligence;intuitive retrieval;knowledge discovery;knowledge visualization;large-scale news video databases;semantic video retrieval systems;valuable feedback,,7,,16,,no,Oct. 30 2007-Nov. 1 2007,,IEEE,IEEE Conference Publications
Appliance-Based Autonomic Provisioning Framework for Virtualized Outsourcing Data Center,X. Wang; D. Lan; G. Wang; X. Fang; M. Ye; Y. Chen; Q. Wang,"Tsinghua University, China",Fourth International Conference on Autonomic Computing (ICAC'07),20070716,2007,,,29,29,"As outsourcing data centers emerge to host applications or services from many different organizations and companies, it is critical for data center owners to isolate different applications while dynamically and optimally allocate resources among them. To address this problem, we propose a virtual-appliance-based autonomic resource provisioning framework for large virtualized data centers. Firstly, we present the architecture of the data center with enriched autonomic features. Secondly, we define a non-linear constrained optimization model for dynamic resource provisioning and present its novel analytic solution. Key factors including virtualization overhead and reconfiguration delay are incorporated into the model. Experimental results based on a prototype system demonstrate that system-level performance has been greatly improved by taking advantage of fine-grained server consolidation. Experiments with the impact of switching delay also show the efficiency of the framework through significantly reducing provisioning time.",,CD-ROM:0-7695-2779-5,10.1109/ICAC.2007.6,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4273123,,Application software;Constraint optimization;Delay effects;Delay estimation;Home appliances;Isolation technology;Outsourcing;Prototypes;Resource management;Resource virtualization,computer centres;optimisation;outsourcing;resource allocation,appliance-based autonomic provisioning framework;nonlinear constrained optimization model;resource allocation;virtualized outsourcing data center,,23,1,32,,no,11-15 June 2007,,IEEE,IEEE Conference Publications
Applying Incomplete Linguistic Preference Relations to a Selection of ERP System Suppliers,Y. C. Chiang; T. C. Wang; S. C. Hsu,"Center of Gen. Educ., I-Shou Univ., Kaohsiung","2007 International Conference on Wireless Communications, Networking and Mobile Computing",20071008,2007,,,6087,6090,"This study provides a method to solve the incomplete linguistic preference relations under multi-criteria decision making. The method uses simple calculation and can speed up the process of comparison and selection of alternative. Decision-making experts obtain the matrix by choosing a finite and fixed set of alternatives and set a pairwise comparison based on their different preferences and knowledge. The method considers only n-1 judgments, whereas the traditional analytic hierarchy approach takes n(n -1)/2 judgments in a preference matrix with n elements. By using the method described above, this study evaluates the ERP system suppliers.",2161-9646;21619646,CD-ROM:1-4244-1312-5; POD:1-4244-1311-7,10.1109/WICOM.2007.1493,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4341268,,Companies;Costs;Decision making;Enterprise resource planning;Government;Hearing aids;Information management;Performance analysis;Proposals;Protection,DP industry;decision making;enterprise resource planning;operations research;software management,ERP system suppliers;analytic hierarchy approach;incomplete linguistic preference relations;multi-criteria decision making,,0,,17,,no,21-25 Sept. 2007,,IEEE,IEEE Conference Publications
Applying Systematic Reviews to Diverse Study Types: An Experience Report,T. Dyba; T. Dingsoyr; G. K. Hanssen,"SINTEF ICT, Norway",First International Symposium on Empirical Software Engineering and Measurement (ESEM 2007),20071015,2007,,,225,234,"Systematic reviews are one of the key building blocks of evidence-based software engineering. Current guidelines for such reviews are, for a large part, based on standard meta-analytic techniques. However, such quantitative techniques have only limited applicability to software engineering research. In this paper, therefore, we describe our experience with an approach to combine diverse study types in a systematic review of empirical research of agile software development.",1949-3770;19493770,CD-ROM:0-7695-2886-4,10.1109/ESEM.2007.59,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343750,,Appraisal;Databases;Decision making;Guidelines;Medical services;Programming;Software engineering;Software maintenance;Software measurement;Strontium,software engineering,agile software development;evidence-based software engineering;standard meta-analytic technique;systematic review,,31,,32,,no,20-21 Sept. 2007,,IEEE,IEEE Conference Publications
Atmospheric Composition Processing System (ACPS): Evolution from instrument-based to measurement-based processing,C. Tilmes; A. J. Fleig; M. Linda,"GSFC Code 614.5 NASA Goddard Space Flight Center Greenbelt, MD 20771, USA",2007 IEEE International Geoscience and Remote Sensing Symposium,20080107,2007,,,1382,1386,"NASA's ACCESS Program is sponsoring an evolution of a mission-based ozone data processing system into a measurement-based Atmospheric Composition Processing System (ACPS). Services currently available only to mission science team members will be extended to members of the Atmospheric Composition Community. Data and information technologies developed for the BUV, TOMS, OMI, and OMPS science teams will be made available to a wider base of science users. Community scientists will have access to current processing algorithms and resources. They will be able to run modified forms of existing algorithms or develop and run new algorithms on the ACPS. Development costs will be minimized by utilizing elements of existing and proven systems. Implementation costs for external scientists will also be minimized by utilizing Linux-based commodity processors, open standards, and open source software. This presentation introduces the architecture of the ACPS and describes how external scientists will be able to interact with it.",2153-6996;21536996,CD-ROM:978-1-4244-1212-9; POD:978-1-4244-1211-2,10.1109/IGARSS.2007.4423064,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4423064,,Atmospheric measurements;Costs;Data processing;Extraterrestrial measurements;Information technology;Instruments;MODIS;NASA;Open source software;Space vehicles,atmospheric composition;atmospheric techniques;ozone,Linux-based commodity processors;NASA ACCESS Program;atmospheric composition processing system;instrument-based processing;measurement-based processing;open source software;open standards;ozone data processing,,0,,3,,no,23-28 July 2007,,IEEE,IEEE Conference Publications
Canny Operator Based Level Set Segmentation Algorithm for Medical Images,X. Qin; J. Jiang; W. Wang; F. Zhang,"Coll. of Software, Zhejiang Univ. of Technol., Hangzhou",2007 1st International Conference on Bioinformatics and Biomedical Engineering,20070716,2007,,,892,895,"The segmentation and extraction of tissues and organs are fundamental work in 3D reconstruction and visualization of medical images. Considering the features of virtual human images, a Canny operator based level set algorithm and the analytic expression of Level set equation is deduced. The algorithm is implemented using narrow band method numerically. The algorithm combines the advantages of Canny operator which can orient the boundary accurately and the idea that Level set method continuously evolves the boundary in image space. Experiments show that the algorithm calculates the result precisely with good anti-jamming property and fast computing speed. Fine results can be obtained by using this algorithm.",2151-7614;21517614,POD:1-4244-1120-3,10.1109/ICBBE.2007.232,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4272716,,Algorithm design and analysis;Biomedical imaging;Equations;Humans;Image analysis;Image reconstruction;Image segmentation;Level set;Narrowband;Visualization,biological organs;biological tissues;image segmentation;medical image processing,3D virtual human images;Canny operator;Level set equation;Level set segmentation algorithm;biological organs;medical images;tissues,,4,,12,,no,6-8 July 2007,,IEEE,IEEE Conference Publications
Classifying facial expressions using point-based analytic face model and Support Vector Machines,A. S. Md. Sohail; P. Bhattacharya,"Dept. of Computer Science and Software Engineering, Concordia University, 1515 St. Catherine West, Montreal, Quebec H3G 2W1, CANADA","2007 IEEE International Conference on Systems, Man and Cybernetics",20080102,2007,,,1008,1013,"This paper describes a fully automated method of classifying facial expressions using support vector machines (SVM). Facial expressions are communicated by subtle changes in one or more discrete features such as tightening the lips, raising the eyebrows, opening and closing of the eyes or certain combination of them, which can be identified through monitoring the changes in muscles movement located near about the regions of mouth, eyes and eyebrows. In this work, we have applied an analytic face model using eleven feature points that represent and identify the principle muscle actions as well as provide measurements of the discrete features responsible for each of the six basic human emotions. A multi-detector approach of facial feature point localization has been utilized for identifying these points of interest from the contours of facial components such as eyes, eyebrows and mouth. Feature vectors composed of eleven features are then obtained by calculating the degree of displacement of these eleven feature points from a non-changeable rigid point. Finally, the obtained feature sets are used to train a SVM classifier so that it can classify facial expressions when given to it in the form of a feature set. The method has been tested on two different publicly available facial expression databases and on average, 89.44% and 84.86% of successful recognition rates have been achieved.",1062-922X;1062922X,CD-ROM:978-1-4244-0991-4; POD:978-1-4244-0990-7,10.1109/ICSMC.2007.4413713,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4413713,,Eyebrows;Eyes;Face;Humans;Lips;Monitoring;Mouth;Muscles;Support vector machine classification;Support vector machines,emotion recognition;face recognition;feature extraction;image classification;support vector machines,SVM classifier;discrete features;facial components;facial expression classification;facial expression databases;facial feature point localization;feature points;feature vectors;fully automated method;human emotions;point-based analytic face model;principle muscle actions;support vector machines,,1,4,26,,no,7-10 Oct. 2007,,IEEE,IEEE Conference Publications
"Communication Waveform Design Using an Adaptive Spectrally Modulated, Spectrally Encoded (SMSE) Framework",M. L. Roberts; M. A. Temple; R. A. Raines; R. F. Mills; M. E. Oxley,"US Air Force Inst. of Technol., Wright-Patterson AFB, OH",IEEE Journal of Selected Topics in Signal Processing,20070515,2007,1,1,203,213,"Fourth-generation (4G) communication systems will likely support multiple capabilities while providing universal, high-speed access. One potential enabler for these capabilities is software-defined radio (SDR). When controlled by cognitive radio (CR) principles, the required waveform diversity is achieved through a synergistic union called CR-based SDR. This paper introduces a general framework for analyzing, characterizing, and implementing spectrally modulated, spectrally encoded (SMSE) signals within CR-based SDR architectures. Given orthogonal frequency division multiplexing (OFDM) is one 4G candidate signal, OFDM-based signals are collectively classified as SMSE since data modulation and encoding are applied in the spectral domain. The proposed framework provides analytic commonality and unification of multiple SMSE signals. Framework applicability and flexibility is demonstrated for candidate 4G signals by: 1) showing that resultant analytic expressions are consistent with published results and 2) presenting representative modeling and simulation results to reinforce practical utility",1932-4553;19324553,,10.1109/JSTSP.2007.897061,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4200710,Wireless communication;cognitive radio (CR);multicarrier communication;orthogonal frequency-division multiplexing (OFDM);software-defined radio (SDR),Cellular networks;Chromium;Cognitive radio;Communication system control;Firewire;Mobile communication;OFDM modulation;Signal analysis;Signal design;Ultra wideband technology,4G mobile communication;OFDM modulation;adaptive modulation;modulation coding;software radio,OFDM;SDR;adaptive spectrally modulated spectrally encoded framework;cognitive radio;communication waveform design;data modulation;encoding;fourth-generation communication systems;orthogonal frequency division multiplexing;software-defined radio,,16,,46,,no,7-Jun,,IEEE,IEEE Journals & Magazines
Data Burst Statistics and Performance Analysis of Optical Burst Switching Networks with Self-Similar Traffic,W. Ruyan; W. Dapeng; G. Fang,"Chongqing University of Posts and Telecommunications, China","Eighth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing (SNPD 2007)",20070813,2007,3,,974,978,"The self-similar model can reflect the Internet traffic property more precisely than Poisson model, the theoretical and simulation results of burst length distribution based on the time threshold assembly mechanism are shown in this paper. Moreover, the simulation result of OBS schedule algorithm under self-similar traffic model is reported. Both the analytic and simulation results show that burst length distribution is related to the length of assembly period; secondly, the assembly mechanism based on time period can reduce the self-similarity of traffic effectively; at last, the schedule algorithms in OBS have different performance based on the self-similar traffic.",,POD:0-7695-2909-7,10.1109/SNPD.2007.450,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4287990,,Assembly;Internet;Mechanical factors;Optical burst switching;Performance analysis;Scheduling algorithm;Statistical analysis;Statistical distributions;Telecommunication traffic;Traffic control,Internet;Poisson distribution;optical burst switching;optical fibre networks;telecommunication traffic,OBS schedule algorithm;Poisson model;burst length distribution;data burst statistics;optical burst switching network;performance analysis;self-similar Internet traffic model;time threshold assembly mechanism,,3,,5,,no,July 30 2007-Aug. 1 2007,,IEEE,IEEE Conference Publications
Deciding on the Ideal Channel Coefficients in Multi-Channel Manufacturing,F. Ozcelik,"Eskisehir Osmangazi University, Department of Industrial Engineering, Bademlik, 26030, Eskisehir, Turkey. fdurmaz@ogu.edu.tr",2007 IEEE Symposium on Computational Intelligence in Multi-Criteria Decision-Making,20070604,2007,,,115,121,"This paper provides a methodology to determine ideal channel coefficients in multi-channel manufacturing (MCM). MCM enhances the advantages of cellular manufacturing by expanding the capabilities of the cells to handle multiple products. The ideal channel coefficients are needed as input for MCM design techniques. While determining ideal channel coefficients (so channel coefficients), we want to assign more profitable parts to more channels. In some cases, this may require additional investment (as extra machines) for some of the channels. These two conflicted goals must be compromised. To do this, the analytic network process (ANP) approach, which is one of the systematic decision-aid tools, is used. The developed model is solved by Super Decisions software. Results showed that ANP is a powerful methodology to determine ideal channel coefficients in MCM design.",,POD:1-4244-0702-8,10.1109/MCDM.2007.369425,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4222991,,Cellular manufacturing;Computational intelligence;Decision making;Industrial engineering;Investments;Manufactured products;Manufacturing systems;Pareto analysis;Power system modeling;Pulp manufacturing,cellular manufacturing;decision making,Super Decisions software;analytic network process;cellular manufacturing;ideal channel coefficients;multichannel manufacturing,,0,,9,,no,1-5 April 2007,,IEEE,IEEE Conference Publications
Design Considerations for Collaborative Visual Analytics,J. Heer; M. Agrawala,"University of California, Soda Hall, UC Berkeley, Berkeley, CA 94720-1776. E-Mail: jheer@cs.berkeley.edu",2007 IEEE Symposium on Visual Analytics Science and Technology,20071127,2007,,,171,178,"Information visualization leverages the human visual system to support the process of sensemaking, in which information is collected, organized, and analyzed to generate knowledge and inform action. Though most research to date assumes a single-user focus on perceptual and cognitive processes, in practice, sensemaking is often a social process involving parallelization of effort, discussion, and consensus building. This suggests that to fully support sensemaking, interactive visualization should also support social interaction. However, the most appropriate collaboration mechanisms for supporting this interaction are not immediately clear. In this article, we present design considerations for asynchronous collaboration in visual analysis environments, highlighting issues of work parallelization, communication, and social organization. These considerations provide a guide for the design and evaluation of collaborative visualization systems.",,POD:978-1-4244-1659-2,10.1109/VAST.2007.4389011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4389011,analysis;collaboration;computer-supported cooperative work;design;visualization,Collaboration;Collaborative software;Collaborative work;Data visualization;Decision making;Displays;Humans;Information analysis;Visual analytics;Visual system,cognition;data visualisation;groupware;human computer interaction;human factors;interactive systems;visual perception,asynchronous collaborative visual analytics design;cognitive process;collaborative visualization system;human sensemaking;human visual perception system;interactive information visualization;social interaction,,7,,53,,no,Oct. 30 2007-Nov. 1 2007,,IEEE,IEEE Conference Publications
Determining the Proper Number and Price of Software Licenses,M. Murtojarvi; J. Jarvinen; M. Johnsson; T. Leipala; O. S. Nevalainen,"Dept. of Inf. Technol., Turku Univ.",IEEE Transactions on Software Engineering,20070423,2007,33,5,305,315,"Software houses sell their products by transferring usage licenses of various software components to the customers. Depending on the kind of software, there are several different license types that allow controlled access of services. The two most popular types are the fixed license, which gives access rights for an identified workstation, and the floating license, which restricts the number of simultaneous users to a certain bound. The latter of these types is advantageous when the users do not demand full-time services and occasional lack of access is bearable. The problem of deciding the number of floating licenses is studied in the present paper. Based on the expected usage profile of the software, we calculate the minimal number of licenses that guarantees that the customers get service better than a given lower bound. The problem is studied by using certain queuing models, known as the Erlang toss system, the Erlang delay system, and the Engset model. None of these analytic models consider, however, the transient period that we analyze by means of simulation and by the so-called modified offered load approximation. We also give simple formulas presenting how the number of software licenses needed to keep the probability of nonaccess below a given blocking level grows as a function of the offered load, which is the proportion of the time used in the case that all requests were successful. Results of the study may be used for setting license prices and for determining the proper number of licenses.",0098-5589;00985589,,10.1109/TSE.2007.1003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4160969,Software release management and delivery;mathematical modeling;queuing theory;simulation.,Analytical models;Delay systems;Licenses;Mathematical model;Packaging;Permission;Queueing analysis;Software packages;Transient analysis;Workstations,contracts;industrial property;queueing theory;software houses;software management,Engset model;Erlang delay system;Erlang toss system;queuing model;software license;software release management,,7,1,16,,no,7-May,,IEEE,IEEE Journals & Magazines
Discrete event simulation modeling of resource planning and service order execution for service businesses,Young M. Lee; Lianjun An; S. Bagchi; D. Connors; S. Kapoor; K. Katircioglu; Wei Wang; Jing Xu,"IBM T. J. Watson Research Center 1101 Kitchawan Road Yorktown Heights, NY 10598, U.S.A.",2007 Winter Simulation Conference,20080104,2007,,,2227,2233,"In this paper, we present a framework for developing discrete-event simulation models for resource-intensive service businesses. The models simulate interactions of activities of demand planning of service engagements, supply planning of human resources, attrition of resources, termination of resources and execution of service orders to estimate business performance of resource-intensive service businesses. The models estimate serviceability, costs, revenue, profit and quality of service businesses. The models are also used in evaluating effectiveness of various resource management analytics and policies. The framework is aided by an information meta-model, which componentizes modeling objects of service businesses and allows effective integration of the components.",0891-7736;08917736,CD-ROM:978-1-4244-1306-5; POD:978-1-4244-1305-8,10.1109/WSC.2007.4419858,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4419858,,Analytical models;Availability;Capacity planning;Discrete event simulation;Humans;Manufacturing;Performance analysis;Predictive models;Resource management;Supply chains,business data processing;discrete event simulation;human resource management,discrete event simulation modeling software;human resource supply planning;information meta-model;resource planning;resource-intensive service business performance estimation;service engagement demand planning;service order execution,,1,,8,,no,9-12 Dec. 2007,,IEEE,IEEE Conference Publications
Drop Dynamic Responses and Modal Analysis for Board Level TFBGA,P. Kai-lin; Z. Bin; Y. Yi-lin,"Institute of Mechatronics Engineering, Guilin University of Electronic Technology, No.1 Jinji Road, Guilin, China. 541004. Email: pankl@guet.edu.cn",2007 International Symposium on High Density packaging and Microsystem Integration,20070808,2007,,,1,5,"Along with more and more use of thin-profile fine-pitch ball grid array (TFBGA) in portable electronic products, such as mobile phones and personal digital assistant (PDA), etc., the drop impact reliability of solder joints for TFBGA becomes a critical concern. This problem is more serious with the application of lead free solder because lead free solder alloy have higher rigidity and lower ductibility compared with the traditional SnPb solder alloy. Whereas the drop test is high cost and long cycle, and the traditional board level drop simulation poses severe challenge to the computer resource, the theory analysis and modal superposition method are combined in this paper. Firstly, the drop vibration differential equation of simplified PCB assembly is deduced and the analytic solution is solved based on the given boundary condition according to the dynamics theory. Secondly, the ANSYS/LS-DYNA software is employed, a quarter 3D model with 15 representatives lead free TFBGA mounted on PCB is developed, and the dynamic responses of solder joint is analyzed under the drop impact. The critical solder joint with the maximal stress caused by deflection of PCB is identified by the model. Finally, the relationship between the number of screws which support the PCB and the PCB deflection under drop impact is studied through the modal superposition, and the influence of every stage mode to the PCB bending deflection is discussed. The results show that the outermost corner solder joint called critical solder joint is aptest to failure. More deflection of PCB causes larger stress of solder joint. The first bend mode is the main factor which influences the bending deflection of PCB. Adopting more screws to support PCB as well as reducing the influence of the first mode is good to improve the drop impact performance of solder joint. Compared with time history explicit dynamic analysis, modal superposition harmonic response analysis can save the solving time greatly.",,CD-ROM:1-4244-1253-6; POD:1-4244-1252-8,10.1109/HDP.2007.4283578,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4283578,Dynamic response;Modal superposition;Thin-profile fine-pitch ball grid array;Vibration equation,Electronics packaging;Environmentally friendly manufacturing techniques;Fasteners;Harmonic analysis;Lead;Mobile handsets;Modal analysis;Personal digital assistants;Soldering;Stress,ball grid arrays;dynamic response;electronic products;modal analysis;solders,PDA;TFBGA;drop dynamic response;electronic products;modal superposition harmonic response analysis;personal digital assistant;solder alloy;thin-profile fine-pitch ball grid array,,1,,5,,no,26-28 June 2007,,IEEE,IEEE Conference Publications
Evaluation on the Industrialization Potential of Emerging Technologies Using the Analytic Network Process,W. Jiwu; H. Lucheng; Lu Wenguang; Li Jian,"School of Economics and Management, Harbin Engineering University, Harbin 150001, China",PICMET '07 - 2007 Portland International Conference on Management of Engineering & Technology,20071015,2007,,,1209,1219,"Due to uncertainties and complexities give birth to the future of emerging technologies, conventional methods for technology evaluation are subjected to many drawbacks and limitations. Even though we can divide a complex system into subsystems, the relative weights of the subsystems are also a crucial problem, because of these subsystems usually exist interdependence and feedback. This paper discusses the industrialization potential evaluation system based on Delphi survey, and evaluates the industrialization potential of emerging technologies using the analytic network process (ANP). A complete ANP model and pairwise comparisons are generated in this paper though the Super Decision software, and supermatrix calculation and sensitivity analysis are discussed at last. Benefits ofthe approach are detailed through illustrative example.",2159-5100;21595100,CD-ROM:978-1-8908-4315-1,10.1109/PICMET.2007.4349444,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4349444,,Computer industry;Engineering management;Feedback;Industrial economics;Industrial relations;Large-scale systems;Manufacturing industries;Sensitivity analysis;Social implications of technology;Technology management,decision making;decision theory;sensitivity analysis;technology management,Delphi survey;Super Decision software;analytic network process;industrialization potential evaluation system;sensitivity analysis;supermatrix calculation,,1,,13,,no,5-9 Aug. 2007,,IEEE,IEEE Conference Publications
ExView: A Real-time Collaboration Environment for Multi-ship Experiments,A. R. Maffei; S. Lerner; J. Lynch; A. Newhall; K. Fall; C. Sellers; S. Glenn,"Woods Hole Oceanographic Institution, Woods Hole, MA 02543 USA. phone: 508-289-2764; fax: 508-457-2174; e-mail: amaffei@whoi.edu",OCEANS 2007 - Europe,20070917,2007,,,1,6,"New challenges in the area of experimental logistics, data visualization and data fusion are encountered in oceanographic research when the need to keep track of the location of multiple ships, moorings, gliders, drifters, and other platforms is combined with assimilating supporting data gathered off the Internet and inserted into the experimental framework. Showing that this can be done well is a start towards our being able to think of scientific expeditions on research vessels as deployable ocean observatories. Researchers at the Woods Hole Oceanographic Institution recently collaborated with the Rutgers University Coastal Ocean Observing Lab (COOL) and other members of the Shallow Water '06 experiment (sponsored by the US Office of Naval Research) in the creation of a new software tool called ExView. This experiment viewer software is a Web-based application that runs on ships and on shore. It enables coordinated, real-time collaboration between researchers employing a number of different research platforms involved in a large-scale experiment. During the SW06 experiment, logistics information and scientific reports associated with twenty-five principal investigators, six ships, eight gliders, three REMUS class AUVs, sixty-two moorings, two aircraft, and four drifting moorings were all made available to researchers in near-real-time over a three month time-period during the summer of 2006. A primarily wireless communications network comprising of HiSeasNet (satellite), SWAP (shipboard WiFi), SeaNet (INMARSAT-B), and the Global Internet was used to synchronize Websites (5 on ships, 1 on shore) so that all participants of the experiment could contribute and monitor platform locations, ship tracks, glider tracks, aircraft tracks, daily reports, weather information, CODAR imagery, satellite imagery, and ocean model results. A dynamic website was mirrored between all of the ships involved in the SW06 experiment. A map at the center of the web display showed location- and tracks of all platforms (ships, moorings, planes, gliders, etc.) and the logistics-related information available from each of them. As ships wandered in and out of wireless range of each other, they updated each others' Websites (even though Internet access might not have been available to the ship at that time). A shore-based website was also updated regularly by ships that had satellite connections back to the Internet. Participants on shore (and on each ship) were able to use the website to browse back in time to see the location and status of mobile platforms, science reports that were submitted each day, and data from a number of standard data sensors collected by each of the ships throughout the experiment. A shore-based team at the Rutgers University Cool Lab provided daily reports with graphics such as water temperature profiles, hurricane reports, satellite imagery, weather reports, wind speed profiles, etc. They also provided an analytic analysis of these elements and how they related to the current experiment plans. In addition, Rutger's staff used the ExView application to monitor the locations of their fleet of gliders and steer them to avoid moorings and other fixed and mobile assets in the area. An emerging technology called delay-tolerant networking (DTN) is being examined for inclusion in the ExView software suite. The current DTN design promises tighter integration of wireless technologies and the development of new algorithms capable of routing data via mobile platforms based on available bandwidth, remaining battery power, platform location, data priority, etc. These characteristics will be incorporated into new optional routing algorithms that will be developed in the future as part of DTN. The successful 3-month use of ExView shows how a novel, near-real-time, Web-based application can be used to improve access to logistics information about a collection of ships, other research platforms, investigators, data sensors, and related data so",,CD-ROM:978-1-4244-0635-7; POD:978-1-4244-0634-0,10.1109/OCEANSE.2007.4302345,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4302345,collaboration;data integration;ships;wireless,Aircraft;Application software;Artificial satellites;Collaboration;Disruption tolerant networking;Internet;Logistics;Marine vehicles;Monitoring;Oceans,Internet;data assimilation;data visualisation;geophysics computing;oceanographic techniques;sensor fusion;ships,Delay-Tolerant Networking;ExView;Experiment Viewer software;Global Internet;HiSeasNet;REMUS class AUV;Rutgers University Coastal Ocean Observing Lab Shallow Water '06 experiment;SWAP;SeaNet;Web-based application;Woods Hole Oceanographic Institution;data assimilation;data fusion;data visualization;drifters;gliders;hurricane reports;moorings;oceanographic research;real-time collaboration environment;satellite imagery;ships;water temperature profile;weather reports;wind speed profiles;wireless communications network,,4,,5,,no,18-21 June 2007,,IEEE,IEEE Conference Publications
Food for Thought: Improving the Market for Assurance,C. E. Landwehr,,IEEE Security & Privacy,20070604,2007,5,3,3,4,"Better information can improve a marketplace. An evaluation/certification process that leveraged modern programming languages and analytic tools could accelerate both the development and the adoption of less vulnerable and more effective programming practices, products, and systems. *This article also includes a letter to the editor, regarding ""Alien vs. Quine"" by Vanessa Gratzer and David Naccache from the March/April 2007 issue.",1540-7993;15407993,,10.1109/MSP.2007.60,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4218542,Common Criteria;assurance,Acceleration;Certification;Computer languages;Computer security;Dairy products;Humans;Privacy;Software measurement;Software systems;Writing,,,,0,,,,no,May-June 2007,,IEEE,IEEE Journals & Magazines
Forecasting Final/Class Yield Based on Fabrication Process E-Test and Sort Data,W. Yip; K. Law; W. Lee,"Software Developer, Analytic Solutions Development, Intel Malaysia. phone: 604-253-35742; fax: 604-253-2273; e-mail: wai.kuan.yip@intel.com",2007 IEEE International Conference on Automation Science and Engineering,20071008,2007,,,478,483,"This paper presents an application of data mining using gradient boosting trees to predict class test yield performance at high volume manufacturing (HVM) based on e-test and sort ancestry parameters. The paper also presents a framework for the predictive capability system and highlights some of the techniques and implementation details. Modeling at wafer level was found to give the best accuracy and the analytic model provides wafer level yield accuracy at 97% within plusmn2% accuracy level for Intel chipset products. Certain functional bins that correlate to e-test and sort can also be modeled for identification of possible high fallouts. In addition, the modeling process also produced Pareto analysis reports that lists dominant influencers and the dependency plots, enabling Assembly and Test (ATM) engineers to feedback to upstream operation engineers. The overall predictive capability has set a new standard for proactive yield monitoring and excursion management at Intel ATM factories and it is useful to various functions including the yield engineering, product engineering, manufacturing and planning.",2161-8070;21618070,CD-ROM:978-1-4244-1154-2; POD:978-1-4244-1153-5,10.1109/COASE.2007.4341700,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4341700,Classification and Regression Trees;Gradient Boosting Trees;Yield Prediction,Assembly;Boosting;Data mining;Fabrication;Feedback;Monitoring;Pareto analysis;Pulp manufacturing;Semiconductor device modeling;Testing,Pareto analysis;data mining;semiconductor device manufacture,HVM;Intel ATM factories;Intel chipset products;Pareto analysis;data mining;fabrication process e-test;gradient boosting trees;high volume manufacturing;predictive capability system;product engineering;semiconductor manufacturing;sort ancestry parameters;wafer level yield accuracy;yield engineering,,4,,9,,no,22-25 Sept. 2007,,IEEE,IEEE Conference Publications
Hydrodynamic transport parameters of wurtzite ZnO from analytic- and full-band Monte Carlo Simulation,E. Furno; F. Bertazzi; M. Goano; G. Ghione; E. Bellotti,"Dipartimento di Elettronica, Politecnico di Torino, Italy",2007 International Semiconductor Device Research Symposium,20080107,2007,,,1,2,"Zinc oxide (ZnO) has seen practical applications much earlier than most wide band gap semiconductors, but only recently it has received renewed attention for electronic and optoelectronic applications because of its potential advantages over III-nitrides, including commercial availability of bulk single crystals, amenability to wet chemical etching, a larger exciton binding energy, and excellent radiation-hard characteristics (Ozgur et al., 2005).",,CD-ROM:978-1-4244-1892-3; POD:978-1-4244-1891-6,10.1109/ISDRS.2007.4422373,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4422373,,Application software;Brillouin scattering;Chemical analysis;Distribution functions;Educational institutions;Electrons;Hydrodynamics;Monte Carlo methods;Temperature distribution;Zinc oxide,II-VI semiconductors;Monte Carlo methods;etching;hydrodynamics;wide band gap semiconductors;zinc compounds,Monte Carlo simulation;Wurtzite ZnO;ZnO;bulk single crystals;exciton binding energy;hydrodynamic transport parameters;radiation-hard characteristics;wet chemical etching;zinc oxide,,0,,4,,no,12-14 Dec. 2007,,IEEE,IEEE Conference Publications
Intelligent Web Services Selection based on AHP and Wiki,C. Wu; E. Chang,"Curtin Univ. of Technol., Perth","Web Intelligence, IEEE/WIC/ACM International Conference on",20080107,2007,,,767,770,"Web service selection is an essential element in service-oriented computing. How to wisely select appropriate Web services for the benefits of service consumers is a key issue in service discovery. In this paper, we approach QoS-based service selection using a decision making model - the analytic hierarchy process (AHP). In our solution, both subjective and objective criteria are supported by the AHP engine in a context-specific manner. We also provide a flexible Wiki platform to collaboratively form the initial QoS model within a service community. The software prototype is evaluated against the system scalability.",,POD:0-7695-3026-5,10.1109/WI.2007.15,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4427186,,Collaboration;Decision making;Ecosystems;Engines;Fasteners;Quality of service;Runtime;Scalability;Software prototyping;Web services,Web services;Web sites;decision making;knowledge based systems;quality of service,AHP engine;QoS-based service selection;Wiki;analytic hierarchy process;decision making model;intelligent Web services selection;service consumers;service discovery;service-oriented computing;software prototype;system scalability,,2,,15,,no,2-5 Nov. 2007,,IEEE,IEEE Conference Publications
Knowledge management based on SOA in aerospace industry,Li Wang; Jiaguyue Xu; Lu Liu,"School of Economic and Management, Beihang University, Beijing, 100083, China",2007 IEEE International Conference on Grey Systems and Intelligent Services,20080128,2007,,,1233,1236,"The paper discusses the characteristics of the knowledge management in Chinese aerospace industry. With a study on the process that the standard converts from the self-dependent intellectual property to the industry, national, international ones, the paper further studies on the design and construction of the SOA-based enterprise knowledge management in the aerospace industry. The main research of the design is on the capability of platform-free and interoperability of the heterogeneous system with the integration model of the agent analytic system. The system is supposed to be with an open and unified environment.",2166-9430;21669430,CD-ROM:978-1-4244-1294-5; POD:978-1-4244-1293-8,10.1109/GSIS.2007.4443469,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4443469,,Aerospace engineering;Aerospace industry;IEC standards;ISO standards;Knowledge management;Semiconductor optical amplifiers;Service oriented architecture;Standardization;Technological innovation;Technology management,Web services;aerospace industry;industrial property;knowledge management;open systems;software architecture,Chinese aerospace industry;SOA-based enterprise knowledge management;agent analytic system;heterogeneous system;interoperability;self-dependent intellectual property,,0,,7,,no,18-20 Nov. 2007,,IEEE,IEEE Conference Publications
Leveraging Resource Prediction for Anticipatory Dynamic Configuration,V. Poladian; D. Garlan; M. Shaw; M. Satyanarayanan; B. Schmerl; J. Sousa,Carnegie Mellon University,First International Conference on Self-Adaptive and Self-Organizing Systems (SASO 2007),20070723,2007,,,214,223,"Self-adapting systems based on multiple concurrent applications must decide how to allocate scarce resources to applications and how to set the quality parameters of each application to best satisfy the user. Past work has made those decisions with analytic models that used current resource availability information: they react to recent changes in resource availability as they occur, rather than anticipating future availability. These reactive techniques may model each local decision optimally, but the accumulation of decisions over time nearly always becomes less than optimal. In this paper, we propose an approach to self- adaptation, called anticipatory configuration that leverages predictions of future resource availability to improve utility for the user over the duration of the task. The approach solves the following technical challenges: (1) how to express resource availability prediction, (2) how to combine prediction from multiple sources, and (3) how to leverage predictions continuously while improving utility to the user. Our experiments show that when certain adaptation operations are costly, anticipatory configuration provides better utility to the user than reactive configuration, while being comparable in resource demand.",1949-3673;19493673,POD:0-7695-2906-2,10.1109/SASO.2007.35,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4274905,,Analytical models;Application software;Availability;Batteries;Pervasive computing;Predictive models;Resource management;Telecommunication traffic;Traffic control;Uncertainty,concurrency control;resource allocation,anticipatory dynamic configuration;multiple concurrent application;resource allocation;resource availability prediction;self-adapting system,,7,,18,,no,9-11 July 2007,,IEEE,IEEE Conference Publications
Linear Current Division Principles,H. Fei; R. Geiger,"Electrical and Computer Engineering, Iowa State University, Ames, IA 50011 U.S.A. haibofei@iastate.edu",2007 IEEE International Symposium on Circuits and Systems,20070625,2007,,,2830,2833,"This paper presents the study of a well established linear current division circuit, which has been referenced many times in published works as a basic building block in variety applications up to date. Understanding the advantages and, even more important, the limitations of this technique would benefit the further exploiting of potential applications. Analytic study with close form expressions shows highly agreements with the original statement that this structure is inherently linear and the linearity is independent of the electrical variables and only relies on the transistors geometrical match performance. However, the more accurate study with simulation results suggests that there are limitations for using this technique in terms of high linearity in the most popular semiconductor processes. Thereby more general circuit principles behind reported high linearity is of particular interested.",0271-4302;02714302,CD-ROM:1-4244-0921-7; POD:1-4244-0920-9,10.1109/ISCAS.2007.378761,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4253267,,Application software;Attenuation;Circuit simulation;Digital filters;Geometry;Linearity;MOSFETs;Nonlinear filters;Performance analysis;Resistors,linear network analysis;transistors,close form expressions;general circuit principles;high linearity;linear current division circuit;semiconductor processes;transistors geometrical match performance,,0,,7,,no,27-30 May 2007,,IEEE,IEEE Conference Publications
Literature Fingerprinting: A New Method for Visual Literary Analysis,D. A. Keim; D. Oelke,University of Konstanz. e-mail: keim@inf.uni-konstanz.de,2007 IEEE Symposium on Visual Analytics Science and Technology,20071127,2007,,,115,122,"In computer-based literary analysis different types of features are used to characterize a text. Usually, only a single feature value or vector is calculated for the whole text. In this paper, we combine automatic literature analysis methods with an effective visualization technique to analyze the behavior of the feature values across the text. For an interactive visual analysis, we calculate a sequence of feature values per text and present them to the user as a characteristic fingerprint. The feature values may be calculated on different hierarchy levels, allowing the analysis to be done on different resolution levels. A case study shows several successful applications of our new method to known literature problems and demonstrates the advantage of our new visual literature fingerprinting.",,POD:978-1-4244-1659-2,10.1109/VAST.2007.4389004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4389004,Visual literature analysis;literature fingerprinting;visual analytics,Application software;Art;Computer applications;Fingerprint recognition;Information analysis;Iron;Lifting equipment;Visual analytics;Visualization;Vocabulary,computer aided instruction;data visualisation,automatic literature analysis;characteristic fingerprint;computer-based literary analysis;interactive visual analysis;literature fingerprinting;visual literary analysis;visualization technique,,19,,13,,no,Oct. 30 2007-Nov. 1 2007,,IEEE,IEEE Conference Publications
Low order radiation forces by analytic interpolation with degree constraint,G. Fanizza; K. Unneland,"Division of Optimization and System Theory, Royal Institute of Technology, SE-100 44 Stockholm, Sweden",2007 46th IEEE Conference on Decision and Control,20080121,2007,,,2405,2410,"The positive real modeling of a floating body is considered, whereas the main focus is on the radiation forces and moments. The radiation forces and moments describe the interaction of a floating body with the surrounding fluid. This type of mathematical model is of interest in among control and simulation of dynamical positioned vessels (i.e. ships and offshore platforms) and wave power plants. It has been proven that the radiation forces are passive, but very little attention have been drawn towards low order passive identification of these forces. Traditionally high order models have been obtained, and subsequently model order reduction have been applied to obtain low order models. Here, a direct approach for obtaining low order passive models using analytic interpolation with a degree constraint is applied. A case study involving a 3 degrees of freedom surface vessel is shown to illustrate the features of the proposed approach.",0191-2216;01912216,CD-ROM:978-1-4244-1498-7; POD:978-1-4244-1497-0,10.1109/CDC.2007.4434656,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4434656,,Constraint theory;Equations;Force control;Interpolation;Marine technology;Marine vehicles;Mathematical model;Power generation;Software testing;USA Councils,hydrodynamics;interpolation;marine systems;motion control;position control,analytic interpolation;dynamical positioned vessels;floating body;low order radiation forces;radiation moments;wave power plants,,0,,32,,no,12-14 Dec. 2007,,IEEE,IEEE Conference Publications
Mapping Applications to Tiled Multiprocessor Embedded Systems,L. Thiele; I. Bacivarov; W. Haid; K. Huang,"Swiss Federal Institute of Technology Zurich, Switzerland",Seventh International Conference on Application of Concurrency to System Design (ACSD 2007),20070723,2007,,,29,40,"Modern multiprocessor embedded systems execute a large number of tasks on shared processors and handle their complex communications on shared communication networks. Traditional methods from the HW /SW codesign or general purpose computing domain cannot be applied any more to cope with this new class of complex systems. To overcome this problem, a framework called Distributed Operation Layer (DOL) is proposed that enables the efficient execution of parallel applications on multiprocessor platforms. Two main services are offered by the DOL: systemlevel performance analysis and multi-objective algorithmarchitecture mapping. This paper presents the basic principles of the DOL, the specification mechanisms for applications, platform and mapping as well as its internal analytic performance evaluation framework. To illustrate the presented concepts, an MPEG -2 decoder case study is presented.",1550-4808;15504808,POD:0-7695-2902-X,10.1109/ACSD.2007.53,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4276262,,Algorithm design and analysis;Application software;Computer architecture;Design methodology;Embedded system;Hardware;Multiprocessing systems;Performance analysis;Resource management;System analysis and design,hardware-software codesign;multiprocessing systems;video coding,HW/SW codesign;MPEG-2 decoder;distributed operation layer;multiobjective algorithm- architecture mapping;shared communication networks;specification mechanisms;system- level performance analysis;tiled multiprocessor embedded systems,,57,,27,,no,10-13 July 2007,,IEEE,IEEE Conference Publications
Middleware and Performance Issues for Computational Finance Applications on Blue Gene/L,T. Phan; R. Natarajan; S. Mitsumori; H. Yu,IBM Almaden Research Center,2007 IEEE International Parallel and Distributed Processing Symposium,20070611,2007,,,1,8,"We discuss real-world case studies involving the implementation of a Web services middleware tier for the IBM Blue Gene/L supercomputer to support financial business applications. These programs that are representative of a class of modern financial analytics that take part in distributed business workflows and are heavily database-centric with input and output data stored in external SQL data warehouses. We describe the design issues related to the development of our middleware tier that provides a number of core features, including an automated SQL data extraction and staging gateway, a standardized high-level job specification schema, a well-defined Web services (SOAP) API for interoperability with other applications, and a secure HTML/JSP Web-based interface suitable for general users. Further, we provide observations on performance optimizations to support the relevant data movement requirements.",1530-2075;15302075,CD-ROM:1-4244-0910-1; POD:1-4244-0909-8,10.1109/IPDPS.2007.370561,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4228289,,Computer applications;Data mining;Data warehouses;Distributed databases;Finance;Job design;Middleware;Spatial databases;Supercomputers;Web services,Web services;application program interfaces;data warehouses;financial data processing;middleware;open systems;parallel machines;relational databases;workflow management software,IBM Blue Gene/L supercomputer;SOAP API;SQL data warehouses;Web services middleware tier;automated SQL data extraction;computational finance applications;distributed business workflows;interoperability;performance optimizations;secure HTML-JSP Web-based interface;staging gateway;standardized high-level job specification schema,,0,,22,,no,26-30 March 2007,,IEEE,IEEE Conference Publications
NEDAT - A Network Engineering Design Analytic Toolset to Design and Analyze Large Scale MANETs,K. Chang; P. Gopalakrishnan; L. Kant; K. Krishnan; K. Manousakis; A. McAuley; E. Van den Berg; K. Young; C. Graff; D. Yee; T. Cook,"Telcordia Technologies, One Telcordia Drive, Piscataway, NJ 08854, USA",MILCOM 2007 - IEEE Military Communications Conference,20080222,2007,,,1,8,"Future battlefield networks such as FCS and WIN-T will rely on mobile ad hoc networks (MANETs) to satisfy their communications requirements. Thus there is a critical need for systematic techniques based on formal methodologies to design MANETs to meet mission requirements. However, given the complexity of MANETs, the variety of applications they need to support, and the associated performance measures, it is extremely challenging to perform such an engineering design of MANETs. The challenges stem both from the large number of design choices that can be made for any given mission, and, more importantly, from the lack of a systematic body of principles that a network designer can rely upon to guide these choices. To address this critical need, a joint effort has been initiated between Telcordia and CERDEC to formulate a Network Engineering Design Analytic Toolset (NEDAT) that applies formal network science-based approaches to systematically and accurately design, analyze and predict the performance of MANETs. In this paper, we present the results from the first phase of this joint effort.",2155-7578;21557578,CD-ROM:978-1-4244-1513-7; POD:978-1-4244-1512-0,10.1109/MILCOM.2007.4454956,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4454956,,Design engineering;Design methodology;Design optimization;Government;Large-scale systems;Mobile ad hoc networks;Mobile communication;Performance analysis;Routing;Software prototyping,,,,11,,7,,no,29-31 Oct. 2007,,IEEE,IEEE Conference Publications
News: A Structure for Unstructured Data Search,G. Goth,,IEEE Distributed Systems Online,20070122,2007,8,1,3,3,"The Unstructured Information Management Architecture is a software development framework developed at IBM to help realize the value of unstructured data search. IBM made UIMA open source in mid-2005 to encourage development of domain analytics. In early December 2006, IBM joined some of the technology industry's other leading governmental and academic sector players to form the UIMA Technical Committee at the Organization for the Advancement of Structured Information Standards. The UIMA framework was transferred to the Apache Foundation's incubator concurrent with the formation of the OASIS committee.",1541-4922;15414922,,10.1109/MDSO.2007.5,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4069218,information management;open source software;search technology,Algorithm design and analysis;Business;Concrete;Consumer electronics;Engines;Information analysis;Information management;Law;Legal factors;Performance analysis,,,,3,,1,,no,Jan. 2007,,IEEE,IEEE Journals & Magazines
Notice of Retraction<BR>A Digital Diagnosis Instrument of Hess Screen for Paralytic Strabismus,L. p. Wang; D. Yu; F. Qiu; J. Shen,"Inst. of Inf. Intell. & Decision Optimization, Zhejiang Univ. of Technol., Hangzhou",2007 1st International Conference on Bioinformatics and Biomedical Engineering,20070716,2007,,,1234,1237,"Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR>The diagnose instrument of Hess screen is an important instrument for ophthalmic diagnoses. It is mainly used for the measurement of paralytic strabismus. This article is about a digital diagnosis instrument of Hess screen for paralytic strabismus, using image manipulation technology and software technology, based on method of making a traditional Hess screen, medical theory of Hess screen diagnosis and oculomotor muscle constitution. This instrument is composed by diagnosis system of Hess screen for strabismus, LCD screen, computer, and red/blue goggles. The diagnosis system of Hess screen for strabismus is consist of automated Hess screen check up and assistant Hess screen check up, the management system of medical record and the result analytic system. It provides integration function of diagnosis of strabismus, data record and analyze. The instrument uses 20 in LCD screen to display and JAVA to implement the function. The clinical test in some hospitals shows: this instrument is convenience, nicety, quick, no hurt, and provides contrast diagnose medical record and after operation medical record. In the end of the article shows analysis of instrument's error based on the clinical data.",2151-7614;21517614,POD:1-4244-1120-3,10.1109/ICBBE.2007.318,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4272802,,Biomedical imaging;Constitution;Data analysis;Eye protection;Health information management;Instruments;Java;Liquid crystal displays;Medical diagnostic imaging;Muscles,Java;biomedical equipment;digital instrumentation;diseases;eye;liquid crystal displays;medical computing;medical information systems;muscle;patient diagnosis,Hess screen diagnosis;JAVA;LCD screen;digital diagnosis instrument;image manipulation technology;medical record management system;medical theory;oculomotor muscle constitution;ophthalmic diagnoses;paralytic strabismus;red/blue goggles;software technology,,0,,9,,no,6-8 July 2007,,IEEE,IEEE Conference Publications
Observations on new developments in composability and multi-resolution modeling,P. K. Davis; A. Tolk,"RAND Corporation, Santa Monica, CA 90407, U.S.A.",2007 Winter Simulation Conference,20080104,2007,,,859,870,"MRM (MRM) and Composability are two of the most challenging topics in M&S. They are also related. In this paper, which was written to set the stage for conference discussion of related papers, we discuss how addressing the MRM challenge is sometimes a necessary - although not sufficient - step towards solving the composability challenge. This paper summarizes recent developments in theory drawing distinctions among issues of syntax, semantics, pragmatics, assumptions, and validity. The paper then discusses how technology for ontology development may be useful in improving both composability and MRM. Two examples illustrate how some of the issues arise. One involves a large analytic war gaming system from the past; the other involves current counter-terrorism modeling in which many of the complications are due to the social- science nature of the problem area.",0891-7736;08917736,CD-ROM:978-1-4244-1306-5; POD:978-1-4244-1305-8,10.1109/WSC.2007.4419682,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4419682,,Application software;Collaboration;Command and control systems;Independent component analysis;Modeling;Ontologies;Research and development management;Service oriented architecture;Systems engineering and theory;Tree graphs,digital simulation;ontologies (artificial intelligence);software architecture,analytic war gaming system;composability;counter-terrorism modeling;multiresolution modeling;ontology development;software architecture,,13,,40,,no,9-12 Dec. 2007,,IEEE,IEEE Conference Publications
On Determinism in Event-Triggered Distributed Systems with Time Synchronization,E. A. Lee; S. Matic,"Electrical Engineering and Computer Sciences, University of California, Berkeley. eal@eecs.berkeley.edu","2007 IEEE International Symposium on Precision Clock Synchronization for Measurement, Control and Communication",20071119,2007,,,56,63,"We study event processing in locally distributed realtime systems. The objective is to use event-triggered communication together with a time-synchronization protocol, in particular. IEEE 1588 over Ethernet, to achieve the similar level of determinism as in statically scheduled time-triggered systems. Given a distributed application with component properties and input event rate characterization, we discuss an analytic procedure that bounds performance parameters. These parameters are also necessary for deterministic implementation of the application. The procedure is experimentally evaluated on a setup with standard software and networking components.",1949-0305;19490305,CD-ROM:978-1-4244-1064-4; POD:978-1-4244-1063-7,10.1109/ISPCS.2007.4383774,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4383774,,Clocks;Communication system control;Delay;Distributed computing;Ethernet networks;Job shop scheduling;Protocols;Real time systems;Switches;Synchronization,local area networks;protocols;real-time systems;synchronisation,Ethernet;IEEE 1588;event-triggered distributed systems;locally distributed realtime systems;networking components;standard software;time-synchronization protocol,,2,,14,,no,1-3 Oct. 2007,,IEEE,IEEE Conference Publications
On Metrics-Driven Software Process,Y. Cao; Z. Qin-xin,"Univ. of Electron. Sci. & Technol. of China, Chengdu",Second International Multi-Symposiums on Computer and Computational Sciences (IMSCCS 2007),20071204,2007,,,543,549,"Metrics can drive software processing, because they found the base of its quantizing management. There are two fundamental requirements in engineering: formal modeling and quantitative modeling. We must emphasize that metrics for software engineering is insufficient in quantification now. In order to improve software and software processing, the factors that affect schedule, cost and quality of software development should be measured. Metrics produces adjustable and iterative motions in software processing. Based on Jaynes' maximum entropy principle, this paper establishes a model to quantify the factors and introduces distance to compare the metric indicators. The authors propose that the metric estimation tree can be used and the nodes that stand for the software attributes in the tree can be marked with their corresponding evaluation values. Dynamic feedback in the software processing will be combined with AHP (analytic hierarchy process), and the project and process of development will be learned and analyzed entirely, concentratedly and dynamically.",,POD:0-7695-3039-7,10.1109/IMSCCS.2007.45,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4392660,,Costs;Entropy;Feedback;Multidimensional systems;Programming;Project management;Software engineering;Software measurement;Software metrics;Software quality,formal specification;maximum entropy methods;software metrics;software process improvement,analytic hierarchy process;formal modeling;maximum entropy principle;metric estimation tree;metrics-driven software process;quantitative modeling;software attributes;software development;software engineering;software improvement,,2,,10,,no,13-15 Aug. 2007,,IEEE,IEEE Conference Publications
On the relative value of local scheduling versus routing in parallel server systems,R. Wu; D. G. Down,"343 Thornall Street, Edison, NJ, 08837 USA",2007 International Conference on Parallel and Distributed Systems,20091215,2007,2,,1,9,"We consider a system with a dispatcher and several identical servers in parallel. Task processing times are known upon arrival. We first study the impact of the local scheduling policy at a server. To this end, we study random routing followed by a priority scheme at each server. Our numerical results show that the performance (mean waiting time) of such a policy could be significantly better than the best known suggested policies that use FCFS at each server. We then propose to use multi-layered round robin routing, which is shown to further improve system performance. Our analysis involves a combination of comparing analytic models, heavy traffic asymptotic and numerical work.",1521-9097;15219097,CD-ROM:978-1-4244-1890-9; POD:978-1-4244-1889-3,10.1109/ICPADS.2007.4447751,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4447751,,,network routing;queueing theory;scheduling,first come first served;local scheduling;multilayered round robin routing;parallel server system;priority scheme;task processing,,1,,15,,no,5-7 Dec. 2007,,IEEE,IEEE Conference Publications
Open Architecture Software Design for Online Spindle Health Monitoring,Li Zhang; R. Yan; R. X. Gao; K. B. Lee,"Department of Mechanical and Industrial Engineering, University of Massachusetts, Amherst, MA 01003",2007 IEEE Instrumentation & Measurement Technology Conference IMTC 2007,20070625,2007,,,1,6,"This paper presents an open systems architecture-based software design for an online spindle health monitoring system. The software is implemented using the graphical programming language of LabVIEW, and presents the spindle health status in two types of windows: simplified spindle condition display and warning window for standard machine operators (operator window) and advanced diagnosis window for machine experts (expert window). The capability of effective and efficient spindle defect detection and localization has been realized using the analytic wavelet-based envelope spectrum algorithm. The software provides a user-friendly human-machine interface and contributes directly to the development of a new generation of smart machine tools.",1091-5281;10915281,CD-ROM:1-4244-1080-0; POD:1-4244-0588-2,10.1109/IMTC.2007.379290,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4258268,Analytic Wavelet;Open Systems Architecture;Smart Machining Systems;Software Design;Spindle Health Monitoring,Algorithm design and analysis;Computer architecture;Computer languages;Displays;Envelope detectors;Monitoring;Open systems;Software design;Software standards;Software systems,computerised monitoring;machine tool spindles;maintenance engineering;production engineering computing;user interfaces,LabVIEW;advanced diagnosis window;analytic wavelet envelope spectrum algorithm;graphical programming language;human-machine interface;machine expert;online spindle health monitoring;open architecture software design;operator window;smart machine tools;spindle condition display;spindle defect detection;standard machine operator;warning window,,1,,11,,no,1-3 May 2007,,IEEE,IEEE Conference Publications
Parameters Optimization of VSC-HVDC Control System Based on Simplex Algorithm,C. Zhao; X. Lu; G. Li,"Member, IEEE, Key Laboratory of Power System Protection and Dynamic Security Monitoring and Control under Ministry of Education, North China Electric Power University, Baoding, 071003, Hebei Provinece, China. e-mail: chengyongzhao@ncepu.edu.cn",2007 IEEE Power Engineering Society General Meeting,20070723,2007,,,1,7,"The performance of VSC-HVDC system depends on the parameters of control system, and usually PI controllers have been used to adjust system to fulfill desired objectives. However, the optimization methods for PI controllers' parameters of VSC-HVDC system are very few up to now. A control strategy based on direct analytic expression for VSC- HVDC is presented and the corresponding control system is designed. Simplex algorithm and system objective function are adopted to optimize the PI parameters for single- and multi- objective VSC-HVDC system on the basis of the control strategy. Simulation results in PSCAD/EMTDC software testify the performance of VSC-HVDC control system with optimized PI parameters and show that the controllers with the optimized PI parameters can effectively control VSC-HVDC system. Advantages of the control system with optimized parameters, such as precise control, quickly responding time and strong robustness have been testified by step response.",1932-5517;19325517,CD-ROM:1-4244-1298-6; POD:1-4244-1296-X,10.1109/PES.2007.386085,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4275851,Simplex;VSC-HVDC;direct analytic expression;parameters optimization,Control system synthesis;Control systems;EMTDC;HVDC transmission;Optimization methods;PSCAD;Robust control;Software performance;Software testing;System testing,HVDC power convertors;HVDC power transmission;PI control;power engineering computing,PI controllers parameters;PSCAD-EMTDC software;VSC-HVDC control system;high voltage DC transmission system;parameters optimization;system objective function;voltage source convertors,,9,,16,,no,24-28 June 2007,,IEEE,IEEE Conference Publications
Partial Discharge Pulse Propagation in Shielded Power Cable and Implications for Detection Sensitivity,N. Oussalah; Y. Zebboudj; S. A. Boggs,,IEEE Electrical Insulation Magazine,20071127,2007,23,6,5,10,"Boggs and Stone (1982) defined the fundamental limits to the electrical detection of corona and partial discharge (PD), i.e., wideband detection of a PD-induced pulse in the presence of thermal noise. This paper treated the effect of frequency-dependent attenuation in shielded power cable in that context. However, most of the plots in that paper were the result of numerical computations. In the same year, Stone and Boggs set out a theory for high-frequency attenuation of shielded power cable. They showed good agreement between attenuation predicted from measured material properties and measured, high-frequency attenuation of shielded power cable. Since 1982, measurements of high-frequency cable attenuation have been reported by a number of authors for a variety of cables. In addition, software tools have become available that facilitate an analytic solution for the parameters of interest. This article summarizes the theory for PD propagation in shielded power cable for both symmetric (Gaussian) and asymmetric PD-pulse waveforms, based on the assumption that the attenuation constant (dB/m or Nepers/m) of the cable is proportional to frequency. This appears to be the most complete possible analytic exposition of PD attenuation in shielded-power cable, which has obvious applications to field PD measurements of such cable.",0883-7554;08837554,,10.1109/MEI.2007.4389974,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4389974,,Attenuation measurement;Cable shielding;Corona;Frequency;Material properties;Partial discharges;Power cables;Power measurement;Software tools;Wideband,attenuation measurement;partial discharge measurement;power cables;thermal noise,asymmetric pulse waveforms;cable attenuation;detection sensitivity;electrical detection;frequency-dependent attenuation;high-frequency attenuation;partial discharge pulse propagation;shielded power cable;software tools;thermal noise,,29,1,9,,no,Nov.-Dec. 2007,,IEEE,IEEE Journals & Magazines
Pattern Matching in Constrained Sequences,Y. Choi; W. Szpankowski,"Department of Computer Science, Purdue University, W. Lafayette, IN 47907 U.S.A. Email: ywchoi@purdue.edu",2007 IEEE International Symposium on Information Theory,20080709,2007,,,2606,2610,"Constrained sequences find applications in communication, magnetic recording, and biology. In this paper, we restrict our attention to the so-called (d, k) constrained binary sequences in which any run of zeros must be of length at least d and at most k, where 0lesd<k. In some applications one needs to know the number of occurrences of a given pattern w in such sequences, for which we coin the term constrained pattern matching. For a given word w or a set of words W, we estimate the (conditional) probability of the number of occurrences of w in a (d, k) sequence generated by a memoryless source. As a by-product, we enumerate asymptotically the number of (d, k) sequences with exactly r occurrences of a given word w, and compute Shannon entropy of (d, k) sequences with a given number of occurrences of w. Throughout this paper we use techniques of analytic information theory such as combinatorial calculus, generating functions, and complex asymptotics.",2157-8095;21578095,CD-ROM:978-1-4244-1429-1; POD:978-1-4244-1397-3,10.1109/ISIT.2007.4557611,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4557611,,Application software;Binary sequences;Biodiversity;Biological system modeling;Computer science;Constraint theory;Digital communication;Neurons;Pattern analysis;Pattern matching,binary sequences;entropy;pattern matching;probability,Shannon entropy;combinatorial calculus;complex asymptotics;conditional probability;constrained binary sequences;constrained pattern matching;generating function;information theory,,3,,20,,no,24-29 June 2007,,IEEE,IEEE Conference Publications
Performability Models for Multi-Server Systems with High-Variance Repair Durations,H. P. Schwefel; I. Antonios,"Aalborg University, Denmark",37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07),20070716,2007,,,770,779,"We consider cluster systems with multiple nodes where each server is prone to run tasks at a degraded level of service due to some software or hardware fault. The cluster serves tasks generated by remote clients, which are potentially queued at a dispatcher. We present an analytic queueing model of such systems, represented as an M/MMPP/1 queue, and derive and analyze exact numerical solutions for the mean and tail-probabilities of the queue-length distribution. The analysis shows that the distribution of the repair time is critical for these performability metrics. Additionally, in the case of high-variance repair times, the model reveals so-called blow-up points, at which the performance characteristics change dramatically. Since this blowup behavior is sensitive to a change in model parameters, it is critical for system designers to be aware of the conditions under which it occurs. Finally, we present simulation results that demonstrate the robustness of this qualitative blow-up behavior towards several model variations.",1530-0889;15300889,POD:0-7695-2855-4,10.1109/DSN.2007.73,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4273028,,Analytical models;Computer crashes;Computer science;Degradation;Electric breakdown;Failure analysis;Hardware;Queueing analysis;Robustness;Software performance,multiprocessing systems;workstation clusters,analytic queueing model;cluster systems;high-variance repair durations;multiserver systems;remote clients;repair time distribution,,1,,22,,no,25-28 June 2007,,IEEE,IEEE Conference Publications
Performance Modeling of Integrated Mobile Prepaid Services,W. Z. Yang; F. S. Lu; M. F. Chang,"Dept. of Comput. Sci. & Inf. Eng., Asia Univ., WuFeng",IEEE Transactions on Vehicular Technology,20070326,2007,56,2,899,906,"Prepaid personal communication service users have outnumbered postpaid users. This paper studies the charging issues of an integrated Global System for Mobile Communications (GSM) and General Packet Radio Service (GPRS) prepaid service, where a single prepaid account provides a user both voice and data services. The call setup and charging procedures for GSM and GPRS are presented using the Customized Applications for Mobile network Enhanced Logic network architecture. To reduce the probability of terminating both ongoing voice and data calls, we suggest that no new call be admitted when the user credit is below a threshold. An analytic model has been developed to evaluate the performance of the approach. Computer simulations have also been used to verify the results. The numeric results indicate that the force-termination probability can be significantly reduced by choosing an appropriate threshold of the user credit",0018-9545;00189545,,10.1109/TVT.2007.891473,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4138052,Customized Applications for Mobile network Enhanced Logic (CAMEL);General Packet Radio Service (GPRS);prepaid services,Application software;Asia;Computer science;Europe;GSM;Ground penetrating radar;Intelligent networks;Logic;Packet radio networks;Personal communication networks,cellular radio;integrated voice/data communication;packet radio networks;personal communication networks,GPRS;GSM;Global System for Mobile Communications;data services;force-termination probability;general packet radio service;integrated mobile prepaid services;mobile network enhanced logic network architecture;prepaid personal communication service,,4,,15,,no,7-Mar,,IEEE,IEEE Journals & Magazines
r-AnalytiCA: Requirements Analytics for Certification & Accreditation,S. W. Lee; R. A. Gandhi; S. J. Wagle; A. B. Murty,"Univ. of North Carolina, Charlotte",15th IEEE International Requirements Engineering Conference (RE 2007),20071119,2007,,,383,384,Numerous interdependent quality requirements imposed by regulatory Certification and Accreditation (C&A) processes enable a rich context to gather compliance evidences for promoting software assurance. The goal of the r-AnalytiCA workbench is to make sense out of the large collection of available evidences for a complex software system though multidimensional requirements-driven problem domain analysis. The requirements analytics employed in the workbench support C&A activities by leveraging the expressiveness of ontologies used to model C&A requirements and their interdependencies.,1090-705X;1090705X,POD:0-7695-2935-6,10.1109/RE.2007.34,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4384213,,Accreditation;Certification;Documentation;Information analysis;Information systems;Multidimensional systems;Ontologies;Software quality;Software systems;USA Councils,accreditation;certification;formal specification;quality assurance;software quality,C&A quality requirements analytics;r-AnalytiCA workbench;software assurance,,2,,38,,no,15-19 Oct. 2007,,IEEE,IEEE Conference Publications
Research of AHP-SOM Model and Its Application,Y. Peng; Z. Hu,"College of Information Engineering, Capital Normal University, Beijing 100037, China; Ahead Software post-Doctor Scientific Workstation, Beijing 100041, China. pengyanpy@163.com",2007 2nd International Conference on Pervasive Computing and Applications,20071029,2007,,,90,94,"Decision-making in the field of information systems has become more complex due to a larger number of alternatives, multiple and sometimes conflicting goals. In this paper we explore the appropriateness of Analytic Hierarchy Process (AHP) to support construction project bid decision-making. Since AHP can be applied if the decision problem includes multiple objectives, conflicting criteria, incommensurable units, and aims at selecting an alternative from a known set of alternatives. And Self-organizing feature map network (SOM) network can classify input models automatically by learning rules. That is, under the circumstance of no teachers, through repeated studies, it can capture all features of input data and organize them automatically. This paper attempts to establish a link between these two approaches. We combine AHP with SOM approach, explore and establish a new kind of evaluation model. The proposed model is applied to construction project bid system and a proof - of - concept prototype has been developed to demonstrate our model.",,CD-ROM:978-1-4244-0971-6; POD:978-1-4244-0970-9,10.1109/ICPCA.2007.4365418,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4365418,Analytic hierarchy process;Self-Organizing Feature Maps;evaluation model,Application software;Data mining;Decision making;Educational institutions;Information systems;Neural networks;Pattern recognition;Power generation economics;Prototypes;Workstations,decision making;self-organising feature maps,AHP-SOM model;analytic hierarchy process;project bid decision-making;self-organizing feature map network,,1,,11,,no,26-27 July 2007,,IEEE,IEEE Conference Publications
Research of Project Evaluation Decision Model Based on AHP-SOM,Y. Peng; L. K. Zhuang,"College of Information Engineering, Capital Normal University, Beijing 100037, China; Ahead Software post-Doctor Scientific Workstation, Beijing 100041, China. E-MAIL: pengyanpy@163.com",2007 International Conference on Machine Learning and Cybernetics,20071029,2007,7,,3980,3984,"Firstly, to construct project bidding decision-making of complex indexes, simplifying indexes system and preprocessing of sample data are performed through analytic hierarchy process to normal data. Secondly, a decision-making model is constructed using SOFM and trained by the sample data. The preliminary experimental result demonstrates that the AHP-SOFM model obtains better visual results and the accuracy attains up to 84.6%.",2160-133X;2160133X,CD-ROM:978-1-4244-0973-0; POD:978-1-4244-0972-3,10.1109/ICMLC.2007.4370842,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4370842,Analytic hierarchy process;Artificial neural network;Bidding decision-making;Feature maps;Self-Organizing,Artificial neural networks;Cybernetics;Data engineering;Decision making;Educational institutions;Machine learning;Neural networks;Software performance;Visual effects;Workstations,decision making;decision support systems;self-organising feature maps,analytic hierarchy process;decision-making model;self-organizing feature maps,,1,,7,,no,19-22 Aug. 2007,,IEEE,IEEE Conference Publications
Research of Software User Interface Evaluation Method based on Subjective Expectation,Y. Shengyuan; Y. Xiaoyang; Z. Hongguo; Z. Zhijian; P. Minjun; W. Shanling,"Instrument Science and Technology Postdoctoral Workstation, Harbin University of Science and Technology; College of Mechanical and Electrical Engineering, Harbin Engineering University, 145 Nantong Street, Harbin, 150001, China. yanshycncn@yahoo.com.cn",2007 International Conference on Mechatronics and Automation,20070924,2007,,,3190,3195,"The artificial neural network based subjective expectation evaluation method on software user interface is proposed. The analytic hierarchy process and radial basis functions neural network based subjective expectation evaluation models are established. The two kinds of models are used to evaluate the control system software user interface of power station. Evaluation results show that the radial basis function neural network based evaluation method can be used to evaluate the software user interface. Because the proposed method has the advantages of deciding the subjective expectation evaluation weights automatically, so it can observably enhance the objectivity of subjective expectation evaluation of software user interface.",2152-7431;21527431,CD-ROM:978-1-4244-0828-3; POD:978-1-4244-0827-6,10.1109/ICMA.2007.4304072,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4304072,Analytic hierarchy process;Radial basis function neural network;Software user interface;Subjective expectation evaluation,Artificial neural networks;Automatic control;Coherence;Educational institutions;Ergonomics;Power system modeling;Radial basis function networks;Software design;Usability;User interfaces,control engineering computing;radial basis function networks;software performance evaluation;user interfaces,analytic hierarchy process;artificial neural network;control system software user interface;power station;radial basis function neural network;subjective expectation evaluation,,0,,12,,no,5-8 Aug. 2007,,IEEE,IEEE Conference Publications
Research on project evaluation system based on ‰ÛÏblack box‰Ûù technology,Tianbiao Yu; Xingyu Jiang; Jianrong Wang; Yingying Su; Ge Yu; Wanshan Wang,"School of Mechanical Engineering & Automation, Northeastern University, Shenyang, Liaoning Province, China",2007 IEEE International Conference on Robotics and Biomimetics (ROBIO),20080516,2007,,,2250,2255,"To assure justice and science of scientific and technological project evaluation, avoiding the corrupt transaction in the process of project evaluation, scientific and technological project evaluation management model based on ""black box"" technology is presented, and the architecture of evaluation ""black box"" is established based on artificial intelligence, experts' selection based on knowledge reasoning is analyzed, system architecture and work workflow are studied, and evaluation model of experts' performance based on analytic hierarchy process and fuzzy comprehensive evaluation is established. Based on these a prototype system is developed, results of the system running prove the correctness of theory study and feasibility of technology research. The study works provides a scientific and reliable method of scientific and technological project evaluation.",,CD-ROM:978-1-4244-1758-2; POD:978-1-4244-1761-2,10.1109/ROBIO.2007.4522520,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4522520,Artificial intelligence;Black box;Export performance evaluation;project evaluation,Airplanes;Appraisal;Artificial intelligence;Concrete;Intelligent agent;Multiagent systems;Performance analysis;Project management;Prototypes;Robotics and automation,artificial intelligence;expert systems;fuzzy set theory;inference mechanisms;project management;software architecture;workflow management software,analytic hierarchy process;artificial intelligence;black box technology;expert selection;fuzzy comprehensive evaluation;knowledge reasoning;project evaluation system;scientific project evaluation management;system architecture;technological project evaluation management;workflow,,0,,8,,no,15-18 Dec. 2007,,IEEE,IEEE Conference Publications
Scalable visual reasoning: Supporting collaboration through distributed analysis,W. A. Pike; R. May; B. Baddeley; R. Riensche; J. Bruce; K. Younkin,"Pacific Northwest National Laboratory, USA",2007 International Symposium on Collaborative Technologies and Systems,20080905,2007,,,24,32,"We present a visualization environment called the scalable reasoning system (SRS) that provides a suite of tools for the collection, analysis, and dissemination of reasoning products. This environment is designed to function across multiple platforms, bringing the display of visual information and the capture of reasoning associated with that information to both mobile and desktop clients. The service-oriented architecture of SRS facilitates collaboration and interaction between users regardless of their location or platform. Visualization services allow data processing to be centralized and analysis results to be collected from distributed clients in real time. We use the concept of ldquoreasoning artifactsrdquo to capture the analytic value attached to individual pieces of information and collections thereof, helping to fuse the foraging and sense-making loops in information analysis. Reasoning structures composed of these artifacts can be shared across platforms while maintaining references to the analytic activity (such as interactive visualization) that produced them.",,CD-ROM:978-0-9785699-1-4,10.1109/CTS.2007.4621734,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4621734,analytical reasoning;knowledge management;mobile applications;service-oriented architecture;visual analytics,Analytical models;Cognition;Collaboration;Construction industry;Data visualization;Mobile communication;Visualization,data visualisation;groupware;information analysis;knowledge management;software architecture;software tools,desktop clients;distributed clients;information analysis;mobile clients;scalable reasoning system;service-oriented architecture;visualization services,,8,,25,,no,25-25 May 2007,,IEEE,IEEE Conference Publications
Self-consistent analytic scattering theory for apertureless THz near-field microscope,J. Kim; H. Park; K. Lee; H. Han; I. Park; H. Lim,"Pohang University of Science and Techanology, 790-784 Korea",2007 Joint 32nd International Conference on Infrared and Millimeter Waves and the 15th International Conference on Terahertz Electronics,20080512,2007,,,496,497,"We propose a self-consistent analytic scattering theory for apertureless THz near-field microscope (NFM) which is based on an exact image theory. In this new scattering theory, a quasi-electrostatic image theory is adopted to include the effects of the specular reflection of the incident field and the exact image dipoles induced in the probe tip and the sample substrate. The analytic results from our self-consistent theory are in good agreement with the simulation results obtained by using HFSS, a commercial software based on finite element method.",2162-2027;21622027,CD-ROM:978-1-4244-1439-0; POD:978-1-4244-1438-3,10.1109/ICIMW.2007.4516599,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4516599,Apertureless near-field microscope;Image theory;Scattering theory;Terahertz wave,Atomic force microscopy;Dielectric substrates;Image analysis;Optical imaging;Optical microscopy;Optical reflection;Optical scattering;Particle scattering;Polarization;Probes,finite element analysis;microscopes;near-field scanning optical microscopy;scattering,HFSS;apertureless THz near-field microscope;finite element method;image dipoles;incident field;quasielectrostatic image theory;sample substrate;self-consistent analytic scattering theory;self-consistent theory;specular reflection,,1,,8,,no,2-9 Sept. 2007,,IEEE,IEEE Conference Publications
Sense-and-respond supply chain using model-driven techniques,S. Kapoor; B. Binney; S. Buckley; H. Chang; T. Chao; M. Ettl; E. N. Luddy; R. K. Ravi; J. Yang,"IBM Research Division, Thomas J. Watson Research Center, P.O. Box 218, Yorktown Heights, NY 10598, USA",IBM Systems Journal,20100406,2007,46,4,685,702,"The results of an effort to build a sense-and-respond solution for a supply chain by using a model-driven development framework are described in this paper. One of the components of the framework is the IBM Research-developed model-driven business-transformation (MDBT) toolkit, a set of formal models, methods, and tools. The inventory optimization analytics used to improve supply chain performance are also described. This approach is illustrated through a case study involving the IBM System x‰ã¢ supply chain.",0018-8670;00188670,,10.1147/sj.464.0685,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5386582,,,,,,0,1,,,no,2007,,IBM,IBM Journals & Magazines
SIFT - A Component-Based Integration Architecture for Enterprise Analytics,D. Thurman; J. Almquist; I. Gorton; A. Wynne; J. Chatterton,"Pacific Northwest National Laboratory, Richland WA",2007 Sixth International IEEE Conference on Commercial-off-the-Shelf (COTS)-Based Software Systems (ICCBSS'07),20070312,2007,,,82,92,"Architectures and technologies for enterprise application integration are relatively mature, resulting in a range of standards-based and proprietary COTS middleware technologies. However, in the domain of complex analytical applications, integration architectures are not so well understood. Analytical applications such as those used in scientific discovery and financial and intelligence analysis exert unique demands on their underlying architectures. These demands make existing COTS integration middleware less suitable for use in enterprise analytics environments. In this paper we describe SIFT (Scalable Information Fusion and Triage), an application architecture designed for integrating the various components that comprise enterprise analytics applications. SIFT exploits a common pattern for composing analytical components, and extends an existing messaging platform with dynamic configuration mechanisms and scaling capabilities. We demonstrate the use of SIFT to create a decision support platform for quality control based on large volumes of incoming delivery data. The strengths and weaknesses of the SIFT solution are discussed, and we conclude by describing where further work is required to create a complete solution applicable to a wide range of analytical application domains",,POD:0-7695-2785-X,10.1109/ICCBSS.2007.35,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4127303,,Application software;Concurrent computing;Disaster management;Information analysis;Laboratories;Lifting equipment;Middleware;Pattern analysis;Pipelines;Quality control,business data processing;middleware;object-oriented programming;software architecture;software packages;software quality;software standards,COTS integration middleware;COTS middleware technology;SIFT;application architecture;component-based integration architecture;decision support platform;enterprise analytics;enterprise application integration;integration architectures;intelligence analysis;quality control;scalable information fusion and triage,,0,,11,,no,Feb. 26 2007-March 2 2007,,IEEE,IEEE Conference Publications
Simulation 101 software: Workshop and beyond,B. Lawson; L. Leemis,"Department of Mathematics and Computer Science University of Richmond, VA 23173-0001, U.S.A.",2007 Winter Simulation Conference,20080104,2007,,,233,236,"The C source code associated with the Simulation 101 pre-conference workshop (offered at the 2006 and 2007 Winter Simulation Conferences) is presented here. This paper begins with general instructions for downloading, compiling, and executing the software. This is followed by sections on four groups of the software, categorized by functionality: libraries, Monte Carlo simulations, discrete-event simulations, and utilities. The libraries contain code to generate random numbers, code to generate random variates, and code to evaluate probability density functions, cumulative distribution functions, and inverse distribution functions. The Monte Carlo simulations consist of six programs that estimate various probabilities associated with simple probability problems, some with known analytic solutions and others without analytic solutions. The discrete-event simulations consist of various applications from queueing and inventory systems. Finally, the utilities are used to calculate various point and interval estimators from data sets.",0891-7736;08917736,CD-ROM:978-1-4244-1306-5; POD:978-1-4244-1305-8,10.1109/WSC.2007.4419606,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4419606,,,C language;Monte Carlo methods;discrete event simulation;probability;program compilers;software libraries;system monitoring;utility programs,C source code;Monte Carlo simulation;cumulative distribution function;discrete-event simulation;inverse distribution function;probability density function;simulation 101 software;software compiling;software downloading;software execution;software library;utility software,,0,,4,,no,9-12 Dec. 2007,,IEEE,IEEE Conference Publications
Software Performance Estimation in MPSoC Design,M. Oyamada; F. R. Wagner; M. Bonaciu; W. Cesario; A. Jerraya,"UFRGS, Instituto de Inform&#225;tica, Porto Alegre, RS, Brazil; TIMA Laboratory, SLS Group, Grenoble, France. marcio@inf.ufrgs.br",2007 Asia and South Pacific Design Automation Conference,20070507,2007,,,38,43,"Estimation tools are a key component of system-level methodologies, enabling a fast design space exploration. Estimation of software performance is essential in current software-dominated embedded systems. This work proposes an integrated methodology for system design and performance analysis. An analytic approach based on neural networks is used for high-level software performance estimation. At the functional level, this analytic tool enables a fast evaluation of the performance to be obtained with selected processors, which is an essential task for the definition of a ""golden"" architecture. From this architectural definition, a tool that refines hardware and software interfaces produces a bus-functional model. A virtual prototype is then generated from the bus-functional model, providing a global, cycle-accurate simulation model and offering several features for design validation and detailed performance analysis. Our work thus combines an analytic approach at functional level and a simulation-based approach at bus functional level. This provides an adequate trade-off between estimation time and precision. A multiprocessor platform implementing an MPEG4 encoder is used as case study, and the analytic estimation results in errors only up to 17% when compared to the virtual platform simulation. On the other hand, the analytic estimation takes only 17 seconds, against 10 minutes using the cycle-accurate simulation model.",2153-6961;21536961,CD-ROM:1-4244-0630-7; POD:1-4244-0629-3,10.1109/ASPDAC.2007.357789,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4195993,,Analytical models;Computer architecture;Embedded system;Hardware;Neural networks;Performance analysis;Refining;Software performance;Space exploration;System analysis and design,circuit CAD;coprocessors;formal verification;image coding;integrated software;multiprocessing systems;neural nets;software performance evaluation;system-on-chip,MPEG4 encoder;MPSoC design;bus-functional model;cycle-accurate simulation model;design space exploration;design validation;integrated methodology;multiprocessor platform;neural networks;performance analysis;software performance estimation;software-dominated embedded systems,,2,3,16,,no,23-26 Jan. 2007,,IEEE,IEEE Conference Publications
Solving Constraints on the Intermediate Result of Decimal Floating-Point Operations,M. Aharoni; R. Maharik; A. Ziv,IBM Research Lab in Haifa,18th IEEE Symposium on Computer Arithmetic (ARITH '07),20070716,2007,,,38,45,"The draft revision of the IEEE Standard for Floating- Point Arithmetic (IEEE P754) includes a definition for decimal floating-point (FP) in addition to the widely used binary FP specification. The decimal standard raises new concerns with regard to the verification of hardware- and software-based designs. The verification process normally emphasizes intricate corner cases and uncommon events. The decimal format introduces several new classes of such events in addition to those characteristic of binary FP. Our work addresses the following problem: Given a decimal floating-point operation, a constraint on the intermediate result, and a constraint on the representation selected for the result, find random inputs for the operation that yield an intermediate result compatible with these specifications. The paper supplies efficient analytic solutions for addition and for some cases of multiplication and division. We provide probabilistic algorithms for the remaining cases. These algorithms prove to be efficient in the actual implementation.",1063-6889;10636889,POD:0-7695-2854-6,10.1109/ARITH.2007.33,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4272849,,Application software;Digital arithmetic;Floating-point arithmetic;Hardware;Natural languages;Software debugging;Software standards;Software tools;System testing,floating point arithmetic;hardware-software codesign;probability,decimal floating-point operations;floating-point arithmetic IEEE standard;hardware-software-based designs;probabilistic algorithms,,5,,19,,no,25-27 June 2007,,IEEE,IEEE Conference Publications
Stochastic Analysis and Improvement of the Reliability of DHT-Based Multicast,G. Tan; S. A. Jarvis,"Warwick Univ., Coventry",IEEE INFOCOM 2007 - 26th IEEE International Conference on Computer Communications,20070529,2007,,,2198,2206,"This paper investigates the reliability of application-level multicast based on a distributed hash table (DHT) in a highly dynamic network. Using a node residual lifetime model, we derive the stationary end-to-end delivery ratio of data streaming between a pair of nodes in the worst case, and show through numerical examples that in a practical DHT network, this ratio can be very low (e.g., less than 50%). Leveraging the property of heavy-tailed lifetime distribution, we then consider three optimizing techniques, namely senior member overlay (SMO), longer-lived neighbor selection (LNS), and reliable route selection (RRS), and present quantitative analysis of data delivery reliability under these schemes. In particular, we discuss the tradeoff between delivery ratio and the load imbalance among nodes. Simulation experiments are also used to evaluate the multicast performance under practical settings. Our model and analytic results provide useful tools for reliability analysis for other overlay-based applications (e.g., those involving persistent data transfers).",0743-166X;0743166X,POD:1-4244-1047-9,10.1109/INFCOM.2007.254,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4215836,,Application software;Communications Society;Computer network reliability;Computer science;Multicast protocols;Peer to peer computing;Senior members;Speech analysis;Stochastic processes;Telecommunication network reliability,multicast communication;stochastic processes;telecommunication network reliability;telecommunication network routing,DHT-based multicast reliability;data delivery reliability;data streaming;distributed hash table;heavy-tailed lifetime distribution;highly dynamic network;longer-lived neighbor selection;node residual lifetime model;reliable route selection;senior member overlay;stationary end-to-end delivery ratio;stochastic analysis,,12,,31,,no,6-12 May 2007,,IEEE,IEEE Conference Publications
Study on Performance of Flexure Hinge Mechanism Based on the EBM,S. Jingying,"Jiaxing University, Jiaxing 314001 China",2007 8th International Conference on Electronic Measurement and Instruments,20071022,2007,,,4-561,4-564,"The basic principle of the equivalent beam methodology (EBM) is introduced and the performance of a double parallel four-bar flexure hinge mechanism is discussed. First, the displacement of this mechanism is theoretically analyzed. Then the displacements of mechanism are studied with the EBM and the conventional finite element method (CFEM) in software ANSYS. Compared with the analytic result of the CFEM, the EBM has the advantage of high efficiency with a low number of elements, which is very important for the simulations of micro-positioning stage with complex flexure hinge mechanism.",,CD-ROM:978-1-4244-1136-8; POD:978-1-4244-1135-1,10.1109/ICEMI.2007.4351206,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4351206,Equivalent beam methodology;Flexure hinge mechanism,Analytical models;Computational modeling;Computer simulation;Fasteners;Finite element methods;Instruments;Manufacturing;Performance analysis;Stress;Structural beams,fasteners;finite element analysis;micropositioning,ANSYS;double parallel four-bar hinge;equivalent beam methodology;finite element method;flexure hinge;hinge displacement;micropositioning simulation,,1,,5,,no,Aug. 16 2007-July 18 2007,,IEEE,IEEE Conference Publications
Study on The Analytic Calculation Method of Throttle Slice Deformation,C. Zhou; B. Kang; L. Gu,"School of Transport and Vehicle Engineering, Shandong University of Technology, Zibo, Shandong Province, China. greatwall@sdut.edu.cn",2007 International Conference on Mechatronics and Automation,20070924,2007,,,2688,2692,"In this paper, the governing differential equation for deformation of a single throttle-slice is introduced first, and based on its solution satisfying required boundary conditions, the formula and coefficient for deformation of a single slice is obtained through equivalency transformation. The G<sub>r</sub> and it's physics meaning at different radius are studied, the deformation at any radius r is researched. At last, compared with other methods, such as machine design and ANSYS FEA software, the compared results show that G<sub>r</sub> method is the most precise computation method way of throttle-slice deformation, and has very important meaning to the throttle-slice design, analysis, and verification, at the same time, it provides an effective method for mathematics theory better used in practical application.",2152-7431;21527431,CD-ROM:978-1-4244-0828-3; POD:978-1-4244-0827-6,10.1109/ICMA.2007.4303982,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4303982,Analytic Calculation;Computation method;Deformatiom;Shock absorber;Throttle-slice,Automation;Boundary conditions;Damping;Deformable models;Differential equations;Elasticity;Mechatronics;Performance analysis;Shock absorbers;Vehicles,differential equations;shock absorbers,analytic calculation;damper;differential equation;equivalency transformation;mathematics theory;shock absorber;throttle slice deformation;throttle-slice design,,0,,7,,no,5-8 Aug. 2007,,IEEE,IEEE Conference Publications
Sunfall: A Collaborative Visual Analytics System for Astrophysics,C. R. Aragon; S. J. Bailey; S. Poon; K. J. Runge; R. C. Thomas,Lawrence Berkeley National Laboratory. E-Mail: aragon@hpcrd.lbl.gov,2007 IEEE Symposium on Visual Analytics Science and Technology,20071127,2007,,,219,220,"Computational and experimental sciences produce and collect ever- larger and complex datasets, often in large-scale, multi-institution projects. The inability to gain insight into complex scientific phenomena using current software tools is a bottleneck facing virtually all endeavors of science. In this paper, we introduce Sunfall, a collaborative visual analytics system developed for the Nearby Supernova Factory, an international astrophysics experiment and the largest data volume supernova search currently in operation. Sunfall utilizes novel interactive visualization and analysis techniques to facilitate deeper scientific insight into complex, noisy, high-dimensional, high-volume, time-critical data. The system combines novel image processing algorithms, statistical analysis, and machine learning with highly interactive visual interfaces to enable collaborative, user-driven scientific exploration of supernova image and spectral data. Sunfall is currently in operation at the Nearby Supernova Factory; it is the first visual analytics system in production use at a major astrophysics project.",,POD:978-1-4244-1659-2,10.1109/VAST.2007.4389026,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4389026,Data and knowledge visualization;astrophysics;scientific visualization;visual analytics;visual exploration,Astrophysics;Data visualization;Image processing;International collaboration;Large-scale systems;Machine learning algorithms;Production facilities;Software tools;Time factors;Visual analytics,astronomical image processing;data analysis;data visualisation;graphical user interfaces;learning (artificial intelligence);statistical analysis,Nearby Supernova Factory;Sunfall;astrophysics;collaborative visual analytics system;data analysis;image processing algorithms;interactive data visualization;machine learning;software tools;spectral data;statistical analysis;user-driven scientific exploration;visual interfaces,,1,,4,,no,Oct. 30 2007-Nov. 1 2007,,IEEE,IEEE Conference Publications
The Current State of Business Intelligence,H. J. Watson; B. H. Wixom,University of Georgia,Computer,20070917,2007,40,9,96,99,"Business intelligence (BI) is now widely used, especially in the world of practice, to describe analytic applications. BI is currently the top-most priority of many chief information officers. BI has become a strategic initiative and is now recognized by CIOs and business leaders as instrumental in driving business effectiveness and innovation. BI is a process that includes two primary activities: getting data in and getting data out. Getting data in, traditionally referred to as data warehousing, involves moving data from a set of source systems into an integrated data warehouse. Getting data in delivers limited value to an enterprise; only when users and applications access the data and use it to make decisions does the organization realize the full value from its data warehouse. Thus, getting data out receives most attention from organizations. This second activity, which is commonly referred to as BI, consists of business users and applications accessing data from the data warehouse to perform enterprise reporting, OLAP, querying, and predictive analytics.",0018-9162;00189162,,10.1109/MC.2007.331,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4302625,IT systems perspectives;business intelligence,Application software;Bismuth;Costs;Data mining;Data warehouses;Hardware;Performance analysis;Publishing;Time measurement;Warehousing,competitive intelligence;data mining;data warehouses;decision making;query processing,OLAP;business intelligence;data warehousing;enterprise reporting;predictive analytics;query processing,,34,,,,no,Sept. 2007,,IEEE,IEEE Journals & Magazines
The History of Applications of Analytic Signals in Electrical and Radio Engineering,S. L. Hahn,"Institute of Radioelectronics, Warsaw University of Technology, Warsaw, Poland, e-mail: hahn@ire.pw.edu.pl","EUROCON 2007 - The International Conference on Computer as a Tool""""",20071226,2007,,,2627,2631,"Analytic signals have been originally described in 1946 by Dennis Gabor. The paper describes briefly the mathematical background created in the past by prominent mathematicians, physicists and engineers, especially the complex notation of functions, the theory of analytic functions and the Fourier spectral analysis and the theory of distributions. Due to the personal experience of the author, the extension of the Gabor's notion of the analytic signal for N-dimensional signals is briefly described As well, problems of analytic spectra of causal signals with extension for N dimensions and 2N-dimensional Wigner distributions are shortly presented.",,CD-ROM:978-1-4244-0813-9; POD:978-1-4244-0812-2,10.1109/EURCON.2007.4400463,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4400463,analytic functions;analytic signals;historical data,Application software;Data analysis;Equations;Fourier series;Fourier transforms;History;Quaternions;Signal analysis;Spectral analysis;Wikipedia,electrical engineering education;functional equations;history,Fourier spectral analysis;Gabor notion;N-dimensional signals;Wigner distributions;complex function notation;electrical engineering;history;radio engineering,,1,,29,,no,9-12 Sept. 2007,,IEEE,IEEE Conference Publications
The selection of CRM systems in financial institutes using the analytic hierarchy,Taeho Hong; Eunmi Kim,"Pusan National University, 30 Jangjeon-dong Geumjeong-gu, Busan 609-735, Korea",2007 2nd International Conference on Digital Information Management,20080131,2007,1,,399,404,"Recently, by changing business circumstances, financial institutes have become interested in customer relationship management (CRM) in order to obtain continuous profit and to keep long term relationships with customers using information technology. CRM in the Korean financial market has more important role for the competitive advantage of financial institutes as the financial service sector has grown into the key industry in korean economy. Thus, selection criteria and evaluation for CRM systems are a necessity when financial institutes introduce the CRM systems . In this paper, we presented the selection of CRM systems by the suggested criteria and relative weights using the analytic hierarchy process (AHP). First of all. selection criteria state, make a model for CRM packages, and survey three groups; researcher group, developer group, and user group. In this paper, weights of selection criteria were confirmed through various experts and a necessity element was suggested for financial institutes with weight of element.",,CD-ROM:978-1-4244-1476-5; POD:978-1-4244-1475-8,10.1109/ICDIM.2007.4444256,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4444256,,Business;Cost function;Customer relationship management;Industrial relations;Information analysis;Information management;Information technology;Packaging;Software packages;Software quality,customer relationship management;finance,AHP;CRM systems;Korean financial market;analytic hierarchy process;customer relationship management;developer group;financial institutes;researcher group;selection criteria weights;user group,,0,,24,,no,28-31 Oct. 2007,,IEEE,IEEE Conference Publications
Thin Client Visualization,S. G. Eick; M. A. Eick; J. Fugitt; B. Horst; M. Khailo; R. A. Lankenau,"SSS Research, Inc, 600 S. Washington, Suite 100, Naperville, IL 60540. eick@sss-research.com",2007 IEEE Symposium on Visual Analytics Science and Technology,20071127,2007,,,51,58,"We have developed a Web 2.0 thin client visualization framework called GeoBoosttrade. Our framework focuses on geospatial visualization and using scalable vector graphics (SVG), AJAX, RSS and GeoRSS we have built a complete thin client component set. Our component set provides a rich user experience that is completely browser based. It includes maps, standard business charts, graphs, and time-oriented components. The components are live, interactive, linked, and support real time collaboration.",,POD:978-1-4244-1659-2,10.1109/VAST.2007.4388996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4388996,JavaScript;linked view visual analytics;scalable vector graphics;visualization components;web 2.0,Application software;Collaboration;Displays;Graphics;Java;Portals;Shape;Streaming media;Visual analytics;Visualization,Internet;Java;XML;data visualisation;geographic information systems;groupware;network computers,AJAX;RSS;Web 2.0;XML;asynchronous JavaScript;geospatial visualization;scalable vector graphics;thin client collaborative visualization framework,,11,5,9,,no,Oct. 30 2007-Nov. 1 2007,,IEEE,IEEE Conference Publications
Topological Landscapes: A Terrain Metaphor for Scientific Data,G. Weber; P. T. Bremer; V. Pascucci,IEEE Computer Society,IEEE Transactions on Visualization and Computer Graphics,20071105,2007,13,6,1416,1423,"Scientific visualization and illustration tools are designed to help people understand the structure and complexity of scientific data with images that are as informative and intuitive as possible. In this context the use of metaphors plays an important role since they make complex information easily accessible by using commonly known concepts. In this paper we propose a new metaphor, called ""topological landscapes,"" which facilitates understanding the topological structure of scalar functions. The basic idea is to construct a terrain with the same topology as a given dataset and to display the terrain as an easily understood representation of the actual input data. In this projection from an n-dimensional scalar function to a two-dimensional (2D) model we preserve function values of critical points, the persistence (function span) of topological features, and one possible additional metric property (in our examples volume). By displaying this topologically equivalent landscape together with the original data we harness the natural human proficiency in understanding terrain topography and make complex topological information easily accessible.",1077-2626;10772626,,10.1109/TVCG.2007.70601,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376169,Contour Tree;Feature Detection (primary keyword);SOAR;Terrain;Topology;User Interfaces;Visual Analytics,Computer vision;Data visualization;Histograms;Humans;Isosurfaces;Laboratories;Surfaces;Topology;Two dimensional displays;User interfaces,data visualisation;software tools;user interfaces,feature detection;illustration tools;scientific data visualization;terrain metaphor;terrain topography;topological landscapes;user interfaces;visual analytics,,33,,49,,no,Nov.-Dec. 2007,,IEEE,IEEE Journals & Magazines
Toward an Integration of the Fuzzy Logic and MCDA to GIS: Application to the Project of the Localization of a Site for the Implantation of Chemical Products Factory,T. Agouti; A. Tikniouine; M. Eladnani; A. Elfazziki,"Department of Computer Science, Faculty of the Sciences Semlalia, University Cadi Ayyad Bd. Prince My Abdellah, B.P. 2390, Marrakech Morocco. t.agouti@ucam.ac.ma",2007 International Symposium on Computational Intelligence and Intelligent Informatics,20070604,2007,,,79,83,"The implementation of a new urban infrastructure causes more and more conflicts mainly because of the correlation of actors coming often from opposed different domains, such as the political, the social, the economic, and the spatial one. To help the decision makers in the territory management, several actors have shown the adequacy of the association of the geographical information systems (GIS) and the multicriterion decision aid (MCDA) methods. This association not only permits to manage the spatial reference information but also to apply new analysis methods permitting to have the most pertinent and most profitable information. However, most problems of MCDA take into account not only the traditional quantitative criteria, but also the qualitative and imprecise criteria, which make their use difficult for the decision and analysis. We propose in this paper, a hybrid model of MCDA for the localization of a site for the implantation of a chemical products factory. This model enables us to combine the use of the multi-representation geographical information systems (MRGIS) and the MCDA methods, such as AHP (analytic hierarchy process). Then, to facilitate the interpretation of the qualitative data in our model, we propose an approach of data modelling based on the fuzzy set theory, which makes it possible to interpret the qualitative values by precise values rather than by classical intervals.",,CD-ROM:1-4244-1158-0; POD:1-4244-1157-2,10.1109/ISCIII.2007.367366,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4218399,AHP;Fuzzy logic;GIS;MCDA;MRGIS;Qualitative criteria,Application software;Chemical products;Computer science;Fuzzy logic;Fuzzy set theory;Geographic Information Systems;Information analysis;Management information systems;Production facilities;Quality management,chemical industry;fuzzy logic;fuzzy set theory;geographic information systems,GIS;MCDA;chemical products factory;data modelling;fuzzy logic;fuzzy set theory;multicriterion decision aid;multirepresentation geographical information systems analytic hierarchy process;territory management;urban infrastructure,,0,,7,,no,28-30 March 2007,,IEEE,IEEE Conference Publications
Toward the Realization of Policy-Oriented Enterprise Management,M. Kaiser,SAP Research Palo Alto,Computer,20071119,2007,40,11,57,63,"A goal-driven approach to business process composition uses generic, logic-based strategies, descriptions of Web services, and formalized business policies to generate business processes that satisfy the stated business goals. The approach is based on an enterprise physics metaphor, in which business objects are analogous to physical objects and policies are analogous to physical laws. Medium and large businesses use traditional enterprise software systems to manage diverse operations in modules that are part of a unified software reflection (that is, how the software represents the enterprise's organizational structure). Each of these modules typically provides support for an entire business department. Examples of such modules include 1) a business intelligence suite (information warehouse and analytics), 2) customer relationship management, 3) supply chain management, and 4) enterprise resource planning.",0018-9162;00189162,,10.1109/MC.2007.406,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4385258,POEM;SAP enterprise SOA;Web services;enterprise services;service orientation,Application software;Companies;Computer architecture;Logic;Packaging;Process design;Service oriented architecture;Software development management;Software systems;Web services,Web services;business data processing;organisational aspects;software architecture,Web service description;business intelligence suite;business process composition;customer relationship management;enterprise organizational structure;enterprise resource planning;enterprise software system;goal-driven approach;information warehouse;logic-based strategy;policy-oriented enterprise management;service-oriented architecture;supply chain management,,6,,12,,no,Nov. 2007,,IEEE,IEEE Journals & Magazines
Towards a conceptual framework for visual analytics of time and time-oriented data,W. Aigner; A. Bertone; S. Miksch; C. Tominski; H. Schumann,"Department of Information and Knowledge Engineering, Dr.-Karl-Dorrek-Strasse 30, Danube University Krems, A-3500, Austria",2007 Winter Simulation Conference,20080104,2007,,,721,729,"Time is an important data dimension with distinct characteristics that is common across many application domains. This demands specialized methods in order to support proper analysis and visualization to explore trends, patterns, and relationships in different kinds of time-oriented data. The human perceptual system is highly sophisticated and specifically suited to spot visual patterns. For this reason, visualization is successfully applied in aiding these tasks. But facing the huge volumes of data to be analyzed today, applying purely visual techniques is often not sufficient. Visual analytics systems aim to bridge this gap by combining both, interactive visualization and computational analysis. In this paper, we introduce a concept for designing visual analytics frameworks and tailored visual analytics systems for time and time-oriented data. We present a number of relevant design choices and illustrate our concept by example.",0891-7736;08917736,CD-ROM:978-1-4244-1306-5; POD:978-1-4244-1305-8,10.1109/WSC.2007.4419666,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4419666,,Application software;Biological system modeling;Computational modeling;Computer simulation;Data analysis;Data visualization;Humans;Medical simulation;Space technology;Visual analytics,data analysis;data visualisation;interactive systems;visual perception,data analysis;data visualization;human perceptual system;interactive visualization;tailored visual analytics system;time-oriented data dimension;visual analytic conceptual framework,,8,,22,,no,9-12 Dec. 2007,,IEEE,IEEE Conference Publications
Twelve Years of Visualization Research at Microsoft,G. Robertson,"Microsoft Research, USA",IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC 2007),20071022,2007,,,12,12,"Summary form only given. Microsoft Research has been involved in a variety of visualization research efforts over the last twelve years. In this talk, the author summarizes the various threads of research, which include task management, personal information management, software visualization, business visualization, community visualization, graph and tree visualization, and visual analytics for homeland security. The author also gives demonstrations of key prototypes that have been built. One of the key challenges throughout this work has been developing effective means of evaluation of visualization techniques. The author summarizes what we have learned about evaluation methods. Finally, the author summarizes some basic lessons learned about what visualization techniques are most effective across all of these research efforts.",1943-6092;19436092,POD:0-7695-2987-9,10.1109/VLHCC.2007.60,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4351320,,Information management;Prototypes;Software prototyping;Terrorism;Tree graphs;USA Councils;Visual analytics;Visualization;Yarn,data visualisation,Microsoft Research;business visualization;community visualization;graph-tree visualization;homeland security;personal information management;software visualization;task management;visual analytics;visualization technique,,0,,,,no,23-27 Sept. 2007,,IEEE,IEEE Conference Publications
Unified Volumes for Light Shaft and Shadow with Scattering,S. Li; G. Wang; E. Wu,"Key Laboratory of High Confidence Software Technologies (Peking University), Ministry of Education, China; HCI & Multimedia Lab, School of EECS, Peking University, Beijing, 100871, China. Email: lisheng@graphics.pku.edu.cn",2007 10th IEEE International Conference on Computer-Aided Design and Computer Graphics,20071226,2007,,,161,166,"It is a challenge work to render natural lighting phenomena in real-time. A major reason is due to high computational expense to simulate the physical model of atmosphere scattering. Another is due to the lack of power and programmability in the graphic hardware. In this paper, we propose unified volumes representation for light shaft and shadow, which is an efficient method of simulating natural light shafts and shadows with atmospheric scattering effect. We give the analytic formula of light shaft without numerical integration and then make use of the current graphic hardware to implement the integral computation on each volume surface for scattering. Our approach can not only simulate the lighting effect with single light source but also multiple parallel light sources according to the physical model of skylight and sunlight. With acceleration of the GPU, we can generate realistic appearance with high frame rate satisfying real time application. It can possibly be used in current commercial game or other virtual reality systems.",,CD-ROM:978-1-4244-1579-3; POD:978-1-4244-1578-6,10.1109/CADCG.2007.4407874,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4407874,,Atmosphere;Atmospheric modeling;Computational modeling;Graphics;Hardware;Light scattering;Light sources;Physics computing;Rendering (computer graphics);Shafts,lighting;natural scenes;virtual reality,GPU;atmosphere scattering;commercial game;graphic hardware;integral computation;light shadow;light shaft;multiple parallel light sources;natural lighting;skylight;sunlight;unified volumes representation;virtual reality systems,,2,1,16,,no,15-18 Oct. 2007,,IEEE,IEEE Conference Publications
Using genetic algorithms for Spectrally Modulated Spectrally Encoded waveform design,T. W. Beard; M. A. Temple; J. O. Miller; R. F. Mills,"Department of Electrical and Computer Engineering, Air Force Institute of Technology, Wright-Patterson AFB, OH 45433 USA",2007 International Waveform Diversity and Design Conference,20071008,2007,,,265,269,"A genetic algorithm (GA) is used to design Spectrally Modulated, Spectrally Encoded (SMSE) waveforms while characterizing the impact of parametric variation on coexistence. As recently proposed, the SMSE framework supports cognition-based, software defined radio (SDR) applications and is well-suited for coexistence analysis. For initial proof-of-concept, two SMSE waveform parameters (number of carriers and carrier bandwidth) are optimized in a coexistent scenario to characterize SMSE impact on Direct Sequence Spread Spectrum (DSSS) bit error performance. Given optimization via GA techniques have been successfully applied in many engineering fields, as well as operations research, they are viable candidates for robust SMSE waveform design. As demonstrated, the analytic SMSE framework is well-suited for parametric optimization via GA techniques.",2150-4652;21504652,CD-ROM:978-1-4244-1276-1; POD:978-1-4244-1275-4,10.1109/WDDC.2007.4339423,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4339423,,Algorithm design and analysis;Application software;Bandwidth;Design engineering;Design optimization;Genetic algorithms;Operations research;Robustness;Software radio;Spread spectrum communication,genetic algorithms;software radio,direct sequence spread spectrum bit error performance;genetic algorithm;parametric optimization;software defined radio;spectrally encoded waveform design;spectrally modulated waveform design;waveform parameter,,1,,8,,no,4-8 June 2007,,IEEE,IEEE Conference Publications
Utility-based QoS Brokering in Service Oriented Architectures,D. A. Menasce; V. Dubey,George Mason University,IEEE International Conference on Web Services (ICWS 2007),20070730,2007,,,422,430,"Quality of service (QoS) is an important consideration in the dynamic service selection in the context of service oriented architectures. This paper extends previous work on QoS brokering for SOAs by designing, implementing, and experimentally evaluating a service selection QoS broker that maximizes a utility function for service consumers. Utility functions allow stakeholders to ascribe a value to the usefulness of a system as a function of several attributes such as response time, throughput, and availability. This work assumes that consumers of services provide to a QoS broker their utility functions and their cost constraints on the requested services. Service providers register with the broker by providing service demands for each of the resources used by the services provided and cost functions for each of the services. Consumers request services from the QoS broker, which selects a service provider that maximizes the consumer's utility function subject to its cost constraint. The QoS broker uses analytic queuing models to predict the QoS values of the various services that could be selected under varying workload conditions. The broker and services were implemented using a J2EE/Weblogic platform and experiments were conducted to evaluate the broker's efficacy. Results showed that the broker adequately adapts its selection of service providers according to cost constraints.",,POD:0-7695-2924-0,10.1109/ICWS.2007.186,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4279627,,Availability;Computer science;Constraint optimization;Cost function;Performance analysis;Predictive models;Quality of service;Queueing analysis;Runtime;Service oriented architecture,Java;Web services;quality of service;queueing theory;software architecture,J2EE;Weblogic;analytic queuing model;cost constraint;dynamic service selection;quality of service;service consumer;service demand;service oriented architecture;utility function;utility-based QoS brokering,,23,,16,,no,9-13 July 2007,,IEEE,IEEE Conference Publications
VAST 2007 Contest Interactive Poster: Data Analysis Using NdCore and REGGAE,L. Schwendiman; J. McLean; J. Larson,"ATS Intelligent Discovery, 3505 NW Anderson Hill Road, Suite 200, Silverdale, WA 98383. lynn.schwendiman@atsid.com",2007 IEEE Symposium on Visual Analytics Science and Technology,20071127,2007,,,199,200,"ATS intelligent discovery analyzed the VAST 2007 contest data set using two of its proprietary applications, NdCore and REGGAE (relationship generating graph analysis engine). The paper describes these tools and how they were used to discover the contest's scenarios of wildlife law enforcement, endangered species issues, and ecoterrorism.",,POD:978-1-4244-1659-2,10.1109/VAST.2007.4389016,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4389016,data discovery;text analysis;visual analytics;visualization,Application software;Data analysis;Engines;Multidimensional systems;Performance analysis;Prototypes;Relational databases;Text analysis;User interfaces;Wildlife,data analysis;data mining;data visualisation;legislation;relational databases;text analysis,ATS intelligent discovery;NdCore tool;REGGAE tool;VAST contest scenario;data analysis;ecoterrorism;endangered species issue;relational database;relationship generating graph analysis engine;text document;visual analytics;wildlife law enforcement,,0,,,,no,Oct. 30 2007-Nov. 1 2007,,IEEE,IEEE Conference Publications
Video verification of point of sale transactions,P. L. Venetianer; Z. Zhang; A. Scanlon; Y. Hu; A. J. Lipton,"ObjectVideo. Inc., Reston, VA, USA",2007 IEEE Conference on Advanced Video and Signal Based Surveillance,20080107,2007,,,411,416,"Loss prevention is a significant challenge in retail enterprises. A significant percentage of this loss occurs at point of sale (POS) terminals. POS data mining tools known collectively as exception based reporting (EBR) are helping retailers, but they have limitations as they can only work statistically on trends and anomalies in digital POS data. By applying video analytics techniques to POS transactions, it is possible to detect fraudulent or anomalous activity at the level of individual transactions. Very specific fraudulent behaviors that cannot be detected via POS data alone become clear when combined with video-derived data. ObjectVideo, a provider of intelligent video software, has produced a system called RetailWatch that combines POS information with video data to create a unique loss prevention tool. This paper describes the system architecture, algorithmic approach, and capabilities of the system, together with a customer case-study illustrating the results and effectiveness of the system.",,CD-ROM:978-1-4244-1696-7; POD:978-1-4244-1695-0,10.1109/AVSS.2007.4425346,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4425346,,Computer architecture;Data mining;Data security;Employee rights;Event detection;Management training;Marketing and sales;Nose;Software tools;Video surveillance,data mining;video surveillance,algorithmic approach;data mining tools;exception based reporting;intelligent video software;loss prevention;loss prevention tool;point of sale terminals;system architecture;video verification,,1,3,16,,no,5-7 Sept. 2007,,IEEE,IEEE Conference Publications
Visual Analytics for Requirements-driven Risk Assessment,R. A. Gandhi; S. W. Lee,"Univ. of North Carolina at Charlotte, Charlotte",Second International Workshop on Requirements Engineering Visualization (REV 2007),20080321,2007,,,6,6,"Risk assessment is a complex decision making process during certification and accreditation (C&A) activities. It requires to understand the multidimensional correlations among numerous C&A requirements to reason about their collective and adequate behavior to minimize risks to a software system. Also, diverse stakeholders in the organizational hierarchy should be able to comprehend and utilize the risk assessment artifacts to agree upon an acceptable level of risks and justify the criticality and cost of mitigation strategies related to C&A requirements. We believe requirements visualization plays an important role in providing rich contextual information for understanding and analyzing risk assessment artifacts and present our initial experiences in using intuitive visual metaphors and their explanations for requirements-driven risk assessment.",,POD:0-7695-3248-9,10.1109/REV.2007.6,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4473006,,Accreditation;Certification;Costs;Decision making;Information analysis;Multidimensional systems;Risk management;Software systems;Visual analytics;Visualization,accreditation;certification;decision making;risk management;security of data;software engineering;software standards;systems analysis,accreditation;certification;decision making process;organizational hierarchy;requirements visualization;requirements-driven risk assessment;software system,,6,,19,,no,15-19 Oct. 2007,,IEEE,IEEE Conference Publications
What Use is Verified Software?,J. Rushby,"SRI International, USA",12th IEEE International Conference on Engineering Complex Computer Systems (ICECCS 2007),20070723,2007,,,270,276,"The world at large cares little for verified software; what it cares about are trustworthy and cost-effective systems that do their jobs well. We examine the value of verified software and of verification technology in the systems context from two perspectives, one analytic, the other synthetic. We propose some research opportunities that could enhance the contribution of the verified software initiative to the practices of systems engineering and assurance.",,POD:0-7695-2895-3,10.1109/ICECCS.2007.47,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4276323,,Algorithm design and analysis;Computer science;Formal verification;Laboratories;Phase frequency detector;Runtime;Software algorithms;Software safety;Subcontracting;Systems engineering and theory,formal verification,cost-effective systems;systems context;systems engineering;verification technology;verified software,,1,,24,,no,11-14 July 2007,,IEEE,IEEE Conference Publications
Wireless Temporal-Spatial Human Mobility Analysis Using Real-Time Three Dimensional Acceleration Data,A. Kalpaxis,"PhoenixNGT, Inc., USA","Computing in the Global Information Technology, 2007. ICCGI 2007. International Multi-Conference on",20070326,2007,,,1,1,"It is estimated that by the year 2010, the number of people over 65 years of age will reach 39 million. By 2030, that number is expected to over 70 million individuals in the 85 and older age group making them the fastest growing group of older individuals. Their growth rate is three times more than all of the 65 and older age groups put together. Falls and mobility issues are very common among older individuals and can have severe consequences. In older individuals, falls and mobility issues can occur as a result of normal age related changes such as changes in vision, gait, strength, disease progression, and medication. We introduce wireless microcontroller hardware and software that leverages micro electro mechanical system (MEMS) transducers which communicate with a server-based heuristics analytics system. The real-time heuristics analytics system performs correlation analysis which will allow for measuring and detecting of mobility related events correlated with, for example, specific disease progression and/or changes in medication dosing and scheduling. If there is excessive inactivity detected within a selected time period, notification will be sent by the heuristics analytics system to a response center.",,POD:0-7695-2798-1,10.1109/ICCGI.2007.65,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4137056,,Acceleration;Biomedical transducers;Communication system software;Diseases;Hardware;Humans;Mechanical systems;Microcontrollers;Micromechanical devices;Performance analysis,geriatrics;medical control systems;microcontrollers;micromechanical devices,correlation analysis;microelectro mechanical system transducers;real-time three dimensional acceleration data;wireless microcontroller hardware;wireless temporal-spatial human mobility analysis,,2,16,4,,no,4-9 March 2007,,IEEE,IEEE Conference Publications
<?Pub Dtl?>A Consideration of the Use of Plagiarism Tools for Automated Student Assessment,O. H. Graven; L. M. MacKinnon,"Dept. of Technol., Buskerud Univ. Coll., Kongsberg",IEEE Transactions on Education,20080507,2008,51,2,212,219,"In this paper, the authors evaluate the flexibility and richness of two well-established text analysis plagiarism tools, through a consideration of the use of plagiarism detection software as a mechanism for the automated assessment of student-created narrative in a virtual learning environment (VLE). The authors are currently engaged in a project creating a prototype VLE, using technologies for multilevel and multiplayer games, based on the inherent support such an environment would provide for constructivist learning, engagement, and contextual socialization. Progress between levels in the VLE will be based on the creation, by the student, of a narrative linking together a number of conceptual elements obtained through game-play at that level. Support for the narrative creation process will help the student to contextualize the conceptual elements, providing the necessary linking elements or themes to enable the student to produce a coherent description of their understanding of the concepts. A particular challenge in such environments is the need for fast, real-time feedback to students to maintain the level of engagement and to support the game-play metaphor. Additionally, the student must be able to make as many attempts to progress as they need and it will be their decision when and how often to submit for assessment. Since the student narrative will be in a textual form and can therefore be related to a sample solution narrative, generated by the author of the level within the learning environment, the idea of using plagiarism detection software as the mechanism for automated comparison and assessment was considered appropriate for investigation. While the limitation of such tools would appear to be that they are seeking direct copies of text elements, the authors wanted to investigate whether they offered sufficient richness and fuzziness to detect common conceptually-linked texts. The initial decision was to experiment with text-analytic tools, since they a- re both widely used and readily available. The tools chosen were TurnItIn, a commercial tool provided to the U.K. higher education community by the U.K. Joint Information Systems Committee (JISC), and VALT/VAST, a set of tools created at the Centre for Interactive Systems Engineering at London South Bank University, London, U.K., the workings of which are based on recognized and well-published research. An experiment using a small group of students in a traditional assessment situation was carried out, and is described in detail. The rationale for this approach was that there is not yet a fully working prototype of the VLE in which to carry out such an experiment, but that the conditions necessary to test the hypothesis that plagiarism tools could be utilized for such a purpose could be replicated sufficiently to make such an experiment viable. The results of the experiment demonstrated neither a correlation between the sample solution and student solutions, nor any correlation between the individual student solutions, proving the null hypothesis. This result demonstrates that these tools are not useful for the development of automated assessment within the VLE, and the authors are now giving consideration to the use of lexical analysis/tokenizer and other tools. However, it also suggests that these text-analytic plagiarism tools are too firmly focused on direct copy, which does raise the question of whether or not they offer enough richness and fuzziness to detect a sophisticated plagiarism attempt using, for example, text replacement tools. An ongoing close relationship between research in automated assessment and plagiarism detection is also proposed, to achieve mutual benefit.",0018-9359;00189359,,10.1109/TE.2007.914940,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4472096,Automated assessment;formative and summative assessment;games-based virtual learning environment;plagiarism detection tools;student-created-narrative-based assessment,Feedback;Information systems;Interactive systems;Joining processes;Plagiarism;Prototypes;Software prototyping;Software tools;Systems engineering and theory;Text analysis,computer aided instruction;computer games;copyright;educational administrative data processing;groupware;text analysis,TurnItIn tools;automated student assessment;multilevel games;multiplayer games;narrative creation process;plagiarism detection software;text analysis plagiarism tools;virtual learning environment,,7,,22,,no,8-May,,IEEE,IEEE Journals & Magazines
A Case Study of Hardware/Software Partitioning of Traffic Simulation on the Cray XD1,J. L. Tripp; M. B. Gokhale; A. ?. Hansson,"Los Alamos Nat. Lab., Los Alamos",IEEE Transactions on Very Large Scale Integration (VLSI) Systems,20071218,2008,16,1,66,74,"Scientific application kernels mapped to reconfigurable hardware have been reported to have 10times to 100times speedup over equivalent software. These promising results suggest that reconfigurable logic might offer significant speedup on applications in science and engineering. To accurately assess the benefit of hardware acceleration on scientific applications, however, it is necessary to consider the entire application including software components as well as the accelerated kernels. Aspects to be considered include alternative methods of hardware/software partitioning, communications costs, and opportunities for concurrent computation between software and hardware. Analysis of these factors is beyond the scope of current automatic parallelizing compilers. In this paper, a case study is presented in which a simulation of metropolitan road traffic networks is mapped onto a reconfigurable supercomputer, the Cray XD1. Five different methods are presented for mapping the application onto the combined hardware/software system. An approach for approximating the performance of each method is derived through analytic equations. Our results, both analytically and empirically, show that key predictors of performance (which are often not considered in reported speedup of kernel operations) are not necessarily maximum parallelism, but must account for the fraction of the problem that runs on the reconfigurable logic and the amount data flow between software and hardware.",1063-8210;10638210,,10.1109/TVLSI.2007.912126,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4407544,Hardware/software codesign;simulation;system integration,Acceleration;Application software;Computational modeling;Concurrent computing;Costs;Hardware;Kernel;Performance analysis;Reconfigurable logic;Traffic control,data flow analysis;hardware-software codesign;parallel machines;reconfigurable architectures;traffic engineering computing,Cray XD1 supercomputer;application speedup;data flow;hardware acceleration;hardware-software partitioning;hardware-software system;kernel operations;metropolitan road traffic network;reconfigurable hardware;reconfigurable logic;scientific application kernels;software components;traffic simulation,,3,,20,,no,Jan. 2008,,IEEE,IEEE Journals & Magazines
A Description of a Highly Modular System for the Emergent Flood Prediction,I. Vondrak; J. Martinovic; J. Kozusznik; S. Stolfa; T. Kozubek; P. Kubicek; V. Vondrak; J. Unucka,"Fac. of Electr. Eng. & Comput. Sci., VSB - Technick Univ. Ostrava, Ostrava",2008 7th Computer Information Systems and Industrial Management Applications,20080709,2008,,,219,224,"The main goal of our system is to provide the end user with information about an approaching disaster. The concept is to ensure information access to adequate data for all potential users, including citizens, local mayors, governments, and specialists, within one system. It is obvious that there is a knowledge gap between the lay user and specialist. Therefore, the system must be able to provide this information in a simple format for the less informed user while providing more complete information with computation adjustment and parameterization options to more qualified users. Important feature is the open structure and modular architecture that enables the usage of different modules. Modules can contain different functions, alternative simulations or additional features. Since the architectural structure is open, modules can be combined in any way to achieve any desired function in the system. One of many important modules is our own analytic solution to the flood waves for a small basin to our system.",,POD:978-0-7695-3184-7,10.1109/CISIM.2008.22,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4557866,,Application software;Computer industry;Computer science;Engineering management;Floods;Geology;Local government;Management information systems;Predictive models;Visualization,disasters;floods;geography;weather forecasting,computation adjustment;emergent flood prediction;flood waves;highly modular system;information access;parameterization options,,2,,20,,no,26-28 June 2008,,IEEE,IEEE Conference Publications
A Framework to Evaluate and Predict Performances in Virtual Machines Environment,D. Ye; Q. He; H. Chen; J. Che,"Coll. of Comput. Sci., Zhejiang Univ., Hangzhou",2008 IEEE/IFIP International Conference on Embedded and Ubiquitous Computing,20090120,2008,2,,375,380,"Virtualization technology becomes more and more important in area of compute science, such as data center and server consolidation. A large number of hypervisors are available to manage the virtualization either on bare hardware or on host operating systems. One of the important task for the designer is to measure and compare the performance overhead of given virtual machines. In this paper, we provide an analytic framework for the performance analyzing either without running a system or in a runnable real system. Meanwhile, analytic performance models that are based on the queue network theory are developed to study the designs of virtual machines. At the end, a case study of the mathematical models is given to illustrate the performance evaluation.",,POD:978-0-7695-3492-3,10.1109/EUC.2008.42,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4755255,Performance evaluation;queue networks;virtual machines,Application software;Computer science;Condition monitoring;Hardware;Operating systems;Performance analysis;Performance evaluation;Queueing analysis;Virtual machine monitors;Virtual machining,queueing theory;virtual machines,data center;mathematical model;performance evaluation;queue network theory;server consolidation;virtual machine environment;virtualization technology,,5,,21,,no,17-20 Dec. 2008,,IEEE,IEEE Conference Publications
A Hilbert warping method for camera-based finger-writing recognition,Hiroyuki Ishida; Tomokazu Takahashi; Ichiro Ide; H. Murase,"Nagoya University, Furo-cho, Chikusa-ku, Aichi, 464-8601 Japan",2008 19th International Conference on Pattern Recognition,20090123,2008,,,1,5,"We propose a time-warping algorithm for recognizing finger actions by a camera. In the proposed method, an input image sequence is aligned to the reference sequences by phase-synchronization of the analytic signals, and then classified by comparing the cumulative distances. A major benefit of this method is that over-fitting to sequences of incorrect categories is restricted. The proposed method exhibited high recognition accuracy in finger-writing character recognition.",1051-4651;10514651,CD-ROM:978-1-4244-2175-6; POD:978-1-4244-2174-9,10.1109/ICPR.2008.4761380,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4761380,,Application software;Cameras;Character recognition;Fingers;Humans;Image analysis;Image sequence analysis;Image sequences;Man machine systems;Signal analysis,cameras;handwritten character recognition;image classification;image sequences;synchronisation,Hilbert warping method;analytic signal;camera-based finger-writing character recognition;finger action recognization;image classification;image reference sequence;phase-synchronization;time-warping algorithm,,0,,11,,no,8-11 Dec. 2008,,IEEE,IEEE Conference Publications
A Parallel Algorithm for Block Tridiagonal Systems,H. Zhang; W. Zhang; X. H. Sun,"Sch. of Comput. Eng. & Sci., Shanghai Univ., Shanghai","2008 Ninth International Conference on Parallel and Distributed Computing, Applications and Technologies",20081212,2008,,,62,65,"A parallel algorithm, namely parallel block diagonal dominant (PBDD) algorithm, is proposed to solve block tridiagonal linear systems on multi-computers. This algorithm is based on divided-and-conquer idea of the PDD method. When the systems is strictly block diagonal dominant, the PBDD is highly parallel and provides approximate solutions that equals to the exact solutions within machine accuracy. The PBDD method has been implemented on a 64-node multi-computer. The analytic results match closely with the results measured from the numerical experiments.",2379-5352;23795352,POD:978-0-7695-3443-5,10.1109/PDCAT.2008.21,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4710962,Block tridiagonal systems;block LU decomposition;block diagonal dominant;divided-and-conquer,Algorithm design and analysis;Application software;Bismuth;Concurrent computing;Distributed computing;Educational institutions;Linear systems;Mathematics;Parallel algorithms;Partitioning algorithms,divide and conquer methods;multiprocessing systems;parallel algorithms,64-node multicomputer;block tridiagonal linear systems;block tridiagonal systems;divide-and-conquer method;parallel algorithm;parallel block diagonal dominant algorithm;strictly block diagonal dominant systems,,2,,5,,no,1-4 Dec. 2008,,IEEE,IEEE Conference Publications
A scheme of telemetering and preventing electric larceny system based on GPRS communication system,Guilin Zheng; Hongyan Zong; Zhifu Zhang,"Department of Automation, Faculty of Power and Mechanics, Wuhan University, China",2008 7th World Congress on Intelligent Control and Automation,20080808,2008,,,5148,5151,"For the power supply department, the key problem is hard to find a suitable and cheap way to not only collect huge number of the vastly distributed meters, but also calculate the balance of electric quantity and detect the nontechnical loss (such as electric larceny) automatically. This paper analyzes the structure and principle of the telemetering and preventing electric larceny system based on the public mobile data communication network GPRS. This system realizes the real time and accuracy of reading meter. Meanwhile, for the unique addressing mode, the monitoring software of this system can monitor the process of power provision and consumption, compare and analyze the data in real-time to find the nontechnical loss and measure the power or water supply system weak points; The power supply department can calculate the line loss according to the real-time data; The strong monitoring software also provides some analytic information about the real-time measurements of electrical energy quantities and alarms of electric larceny. The application shows that this system has the character of cheapness, high efficiency, practicability and so on.",,POD:978-1-4244-2113-8,10.1109/WCICA.2008.4593767,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4593767,GPRS;electric larceny preventing;telemetering,Electric variables measurement;Energy measurement;Ground penetrating radar;Information analysis;Loss measurement;Monitoring;Power measurement;Power supplies;Real time systems;Software measurement,mobile radio;packet radio networks;telemetry,GPRS communication system;electric larceny system;electrical energy quantities;monitoring software;nontechnical loss;power supply department;public mobile data communication;telemetering system,,0,,9,,no,25-27 June 2008,,IEEE,IEEE Conference Publications
Activity Recognition Using a Web 3.0 Database,J. Aasman,,2008 IEEE International Conference on Semantic Computing,20080812,2008,,,488,489,"Web 3.0 envisages software agents that know how to reason over activities, events, locations, people, companies, and their inter-relationships. Learning more about customers through behavioral and activity recognition is here today through currently available Semantic Technologies and is a showcase for how these technologies will evolve. The demonstration shows real world examples of activity recognition using a combination of industry standard RDF and OWL, reasoning with basic Geotemporal primitives and some well-known Social Network Analytics.",,CD-ROM:978-0-7695-3279-0,10.1109/ICSC.2008.33,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4597233,activity recognition;database;geospatial;rdf;semantic web;web 3.0,Data warehouses;Logic;OWL;Ontologies;Resource description framework;Semantic Web;Social network services;Software agents;Transaction databases;Wikipedia,behavioural sciences computing;knowledge representation languages;ontologies (artificial intelligence);semantic Web;social sciences computing;software agents;temporal reasoning,OWL;RDF;Web 3.0 database;activity recognition;behavioral recognition;geotemporal primitives;reasoning;semantic Web;social network analytics;software agents,,0,,14,,no,4-7 Aug. 2008,,IEEE,IEEE Conference Publications
Aeroengine Health Assessment Using a Web-Based Grey Analytic Hierarchy Process,J. Wang; D. Huang; Y. Su; W. Wang,"Sch. of Mech. Eng. & Autom., Northeastern Univ., Shenyang",2008 International Conference on Computer Science and Software Engineering,20081222,2008,1,,411,414,"This paper proposed a Web-based evaluation method for aeroengine health assessment which combined the advantages of the analytic hierarchy process (AHP) and a grey clustering method to guarantee the accuracy and objectivity of weight coefficients. The Web-based framework is convenient for data collection, distributed management and processing. After constructing an index system of aeroengine health assessment based on correlated factors involved in aeroengine health status, we confirmed the weight of every index with AHP and gave a general health assessment by means of a grey clustering method. A case study was conducted in a fleet with ten aeroengines to validate the design of the underlying grey analytic hierarchy process (GAHP) model. Running results show the feasibility and reliability of the model, which will be helpful to realize the quantitative analysis in aeroengine health assessment and provide a decision support tool for decision makers.",,POD:978-0-7695-3336-0,10.1109/CSSE.2008.1236,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721774,aeroengine health assessment;grey analytic hierarchy process;index system,Aerospace safety;Aircraft;Automation;Clustering methods;Decision making;Engines;Information analysis;Maintenance;Mechanical engineering;Temperature,Internet;aerospace engines;condition monitoring;decision making;grey systems;matrix algebra;pattern clustering,Web-based grey analytic hierarchy process;aeroengine health condition assessment;correlated factor;data collection;distributed management;distributed processing;evaluation matrix;grey clustering method;index system;weight coefficient,,2,,14,,no,12-14 Dec. 2008,,IEEE,IEEE Conference Publications
An adaptive planning model based on multi-agent,Qing-Min Zhang; Heng-Xin Xue; Cheng Chen; Chun-Mei Wu,"School of Economy and Management, Nanjing University of Science and Technology, 210094, China",2008 International Conference on Machine Learning and Cybernetics,20080905,2008,2,,1121,1126,"Quantitative factors are usually taken into account, but qualitative factors are neglected when a retailer in distribution chain decides to order. For this reason, a planning model based on multi-agent is supposed to solve adaptive problems and then analyze the problems by using quantitative and qualitative factors to improve the retailerspsila ordering efficiency. Using AHP approach, the multi-attribute analysis method is developed to evaluate the influence factors for ordering quantities. The weight for each retailer is calculated via Super Decisions software. In addition, According to the weights and qualitative data in ERP, a multi-objective decision-making method is given to determine the ordering quantities. The simulation results show that the proposed model based on multi-agent increases ordering flexibility and offers a good planning method with cooperation between retailers and a DC.",2160-133X;2160133X,POD:978-1-4244-2095-7,10.1109/ICMLC.2008.4620572,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4620572,AHP;Distribution chain;Multi-agent;Multi-objective optimization;Planning,Artificial intelligence;Cybernetics;Decision making;Enterprise resource planning;Machine learning;Marketing and sales;Quality management;Real time systems;Technology management;Technology planning,decision making;enterprise resource planning;multi-agent systems;retail data processing;supply chain management,Super Decisions software;adaptive planning;analytic hierarchy process;distribution chain;enterprise resource planning;multiagent system;multiattribute analysis;multiobjective decision making;ordering quantity;qualitative factor;quantitative factor;retailer ordering efficiency,,0,,10,,no,12-15 July 2008,,IEEE,IEEE Conference Publications
An AHP-Based Evaluation Index System of Coding Standards,W. Yan-qing; X. Xiao-fei; L. Li; W. Jian-zheng,"Sch. of Software, Harbin Inst. of Technol., Harbin, China",2008 International Conference on Computer Science and Software Engineering,20081222,2008,5,,620,623,"Compliance with coding standards is becoming an important evaluating index of a software engineerÌâåÀs teamwork capability and international cooperation capability. Unreadable and non-reusable program code, high defect density, and expensive maintenance cost are changing the mindset of managers working in software companies. The best place to solve this problem should be in an educational environment such as college where students begin to write their introductory programs. In order to propose an effective and quantitative evaluation index system, and to gradually improve studentsÌâåÀ ability to comply with coding standards, a simplified version of coding standards was presented. By utilizing questionnaire surveys and analytical hierarchy process (AHP), weights of the indices in the evaluation system were determined. This evaluation index system was proved quite practical in the teaching process of PSP course.",,POD:978-0-7695-3336-0,10.1109/CSSE.2008.119,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722978,analytic hierarchy process (AHP);coding standards;evaluation index system;quantitative evaluation;questionnaire survey,Computer industry;Computer science;Education;Educational institutions;Outsourcing;Programming profession;Software engineering;Software standards;Standards development;Switches,software maintenance;software reusability,AHP-based evaluation index system;analytical hierarchy process;coding standards;evaluation index system;international cooperation capability;maintenance cost;software companies;software engineering,,1,,14,,no,12-14 Dec. 2008,,IEEE,IEEE Conference Publications
An analytic model and optimization technique based methods for fault diagnosis in power systems,Zhiwei Liao; Fushuan Wen; Wenxin Guo; Xiangzhen He; Wei Jiang; Taifu Dong; Junhui Liang; Binghua Xu,"Department of Electrical Engineering, South China University of Technology, Guangzhou 510641, China",2008 Third International Conference on Electric Utility Deregulation and Restructuring and Power Technologies,20080516,2008,,,1388,1393,"An analytic model for fault diagnosis of power system using optimization technique is expressed as unconstrained 0-1 integer programming problem, and consequently faulty equipment identification can be solved by refined mathematical operation. Considering the configuration of automatic devices in modern power systems, such as protective relays and reclosing relays, an improved analytic model and optimization technique-based method for fault diagnosis of power system is proposed in this paper. The evaluation criteria of the presented model is improved considering the relationship of multiple main protective relays, backup protective relays, malfunctioning protective relays and reclosing relays. Improvements of analytic model for fault diagnosis of electric power system based on optimization techniques are presented firstly. A brief description about the modulars and functions of the online fault diagnosis software which is developed by the authors for Jiangsu Provincial Power Company is given. The adopted EMS data acquisition method and simulated online test results for the power system of Jiangsu Power Company are described.",,CD-ROM:978-7-900714-13-8,10.1109/DRPT.2008.4523623,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4523623,analytic model;fault diagnosis;optimization technique;power system,Fault diagnosis;Mathematical model;Optimization methods;Power system analysis computing;Power system faults;Power system modeling;Power system protection;Power system relaying;Power system simulation;Protective relaying,fault diagnosis;integer programming;power system analysis computing;power system faults;power system protection;power system relaying;relay protection,EMS data acquisition method;Jiangsu Provincial Power Company;fault diagnosis;faulty equipment identification;optimization technique;power systems;protective relays;reclosing relays;unconstrained integer programming problem,,2,,8,,no,6-9 April 2008,,IEEE,IEEE Conference Publications
An Enhanced Autonomic Multiclass Multithreaded Web Server: A Performance Model Approach,A. Doostmohammadi,"Dept. of Comput. Eng., IUST, Tehran",2008 32nd Annual IEEE International Computer Software and Applications Conference,20080808,2008,,,1128,1133,"Computer systems are becoming more complex continuously and managing them is quite challenging even to the most skilled IT professionals. A promising way to address obstacles to efficient management and re-organization of computer systems relies on the automation of these tasks. An approach has been introduced that in which analytic performance models are combined with combinatorial search techniques to design a QoS controller that runs periodically to determine the best possible configuration for the system given its workload. The results of applying this approach have been shown to QoS control of many systems. In this paper we considered a multithreaded web server with multiple classes of request and proposed that the QoS controller re-configure it in each controller interval distinguishing different workload classes. We illustrate that this mechanism enhances the self-optimizing and self-managing capability of the system, and improves global QoS value of it using simulated multithreaded web server. In addition, we show that applying this mechanism make QoS controller to become fairer and more flexible.",0730-3157;07303157,CD-ROM:978-0-7695-3262-2,10.1109/COMPSAC.2008.138,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4591735,analytical performance model;autonomic multithreaded web server;self-managing systems,Application software;Computer applications;Web server,Internet;fault tolerant computing;file servers;multi-threading;quality of service;resource allocation;search problems;software performance evaluation,QoS controller;autonomic multiclass multithreaded Web server;combinatorial search technique;performance model approach;self-managing capability;self-optimizing capability;workload class,,0,,13,,no,July 28 2008-Aug. 1 2008,,IEEE,IEEE Conference Publications
An Improved Particle Filtering Algorithm Based on Consensus Fusion Sampling,C. Yunzhi; J. Yong; L. Jie,"Coll. of Comput. & Inf. Eng., Henan Univ., Kaifeng, China",2008 International Conference on Computer Science and Software Engineering,20081222,2008,5,,1337,1340,"Particle filtering is briefly introduced first. Because the depletion of particle diversity resulted from re-sampling causes the decline of filtering precision, an improved particle filtering algorithm based on consensus fusion sampling is proposed. After the re-sampling process, the new algorithm extracts candidate particles based on Markov Chain Monte Carlo (MCMC) principle and combines the re-sampling particles to construct a candidate particle set. Then according to the principle of analytic hierarchy process (AHP), consensus matrix is established, and the complementary and redundancy information of the candidate particles is fully used. Finally, the optimal selection of particles is realized by calculating consensus matrix. Simulation results show the method can effectively reduce the phenomenon of particle impoverishment and improve the state estimation precision.",,POD:978-0-7695-3336-0,10.1109/CSSE.2008.122,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4723157,AHP;MCMC;Particle filtering;consensus fusion;style,Computer science;Data mining;Diversity reception;Educational institutions;Filtering algorithms;Information filtering;Information filters;Sampling methods;Software engineering;State estimation,Markov processes;Monte Carlo methods;particle filtering (numerical methods),Markov Chain;Monte Carlo principle;analytic hierarchy process;consensus fusion sampling;particle filtering algorithm,,0,,9,,no,12-14 Dec. 2008,,IEEE,IEEE Conference Publications
An Ontology-Driven Framework for Deploying JADE Agent Systems,C. I. Nyulas; M. J. O'Connor; S. W. Tu; D. L. Buckeridge; A. Okhmatovskaia; M. A. Musen,,2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology,20090106,2008,2,,573,577,"Multi-agent systems have proven to be a powerful technology for building distributed applications. However, the process of designing, configuring and deploying agent-based applications is still primarily a manual one. There is a need for mechanisms and tools to help automate the many development steps required when building these applications. Using the Semantic Web ontology language OWL and the JADE platform we have developed a number of models and software tools that provide an end-to-end solution for designing and deploying agent-based systems. This solution supports the construction of detailed models of agent behavior and the automatic deployment of agents from those models. We illustrate its use in the construction of a multi-agent system that supports the configuration, deployment, and evaluation of analytic methods for detecting disease outbreaks.",,POD:978-0-7695-3496-1,10.1109/WIIAT.2008.25,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4740686,JADE;OWL;intelligent agents;ontologies;surveillance,Application software;Biomedical informatics;Buildings;Diseases;Intelligent agent;Multiagent systems;OWL;Ontologies;Power system modeling;Problem-solving,diseases;medical computing;multi-agent systems;ontologies (artificial intelligence);semantic Web,BioSTORM;JADE agent system;Web ontology language;disease detection;distributed applications;multi-agent system;ontology-driven framework;semantic OWL;software tools,,6,,16,,no,9-12 Dec. 2008,,IEEE,IEEE Conference Publications
An Open Community Approach to Emergency Information Services during a Disaster,Z. Wang; M. HÌ_mÌ_lÌ_inen; Z. Lin,"Sch. of Economic Inf. Eng., Southwestern Univ. of Finance & Econ., Chengdu",2008 International Symposium on Information Science and Engineering,20081230,2008,1,,649,654,"This paper discusses the application of open community for the emergency information services during a disaster with the lessons learned from the 2008 Sichuan earthquake. An open community approach enables open participation for information collection and sharing. The suggested ldquodual modelrdquo allows a two-way-traffic from citizens to officials and from officials to citizens. The latest Web services based mashup technology can play a critical role in implementing an OC with the dual model in a cellphone network. This paper further addresses the issues of information sharing, security, and service quality for an open community.",2160-1283;21601283,POD:978-0-7695-3494-7,10.1109/ISISE.2008.166,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4732299,disaster;mashup;mobile network;open community;user-driven services;web 2.0,,Web services;disasters;earthquakes;emergency services;information resources,Web services;cellphone network;disaster;earthquake;emergency information services;information sharing;open community,,1,,14,,no,20-22 Dec. 2008,,IEEE,IEEE Conference Publications
Analysis and design of novel directional ultra wide-band antennas,Zhong Lingling; Qiu Jinghui; Zhang Ning; Sun Bo,"Dept. Electronics and Communication Engineering, Harbin Institute of Technology, 150001, China",2008 International Conference on Microwave and Millimeter Wave Technology,20080613,2008,4,,1658,1661,"Ultra wide-band is one of the hottest issues in the current researches on antennas. The circular monopole antenna is a kind of ultra wide-band antennas with simple structure. Firstly a novel antenna was researched. Four circular disc monopoles were placed across each other. They were considered as the feedings, and the reflection plane was planar. It could achieve dual polarization characteristics. On this basis, further study was applied to two other novel antennas. The first was changing the planar reflector to parabolic one, and the purpose was to realize wide beam, high gain and directed radiation characteristic. The other was changing the circular disc shape, and the purpose was to realize wider lower frequency bandwidth. The novel antennas were theoretic analyzed, and the S- parameter and radiation patterns of the antennas were discussed by electromagnetism simulation software. The analytic results verify that the characteristic of the traditional monopole antenna is effective improved by the novel antennas. Dual-polarization, high gain, wide beam and directed radiation characteristics of the antenna are realized. The novel antennas are suitable for the satellite and mobile communication systems with good prospects.",,POD:978-1-4244-1879-4,10.1109/ICMMT.2008.4540785,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4540785,Circular monopole antenna;Directional antenna;Dual polarization;Ultra wide-band antenna;Wide-beam,Antenna theory;Bandwidth;Directive antennas;Electromagnetic analysis;Frequency;Optical reflection;Polarization;Shape;Ultra wideband antennas;Ultra wideband technology,antenna feeds;antenna radiation patterns;directive antennas;electromagnetic wave polarisation;monopole antennas;planar antennas;reflector antennas;ultra wideband antennas,antenna feeds;antennas radiation patterns;circular disc monopoles;circular monopole antenna;directed radiation characteristics;electromagnetism simulation software;lower frequency bandwidth;novel directional ultra wideband antennas;parabolic planar reflector;planar reflection plane,,2,,11,,no,21-24 April 2008,,IEEE,IEEE Conference Publications
Analysis of the Virtual Enterprise Partner Selection Based on Multi-agent System,Z. Yang; H. Lin,"Bus. Sch., Tianjin Univ. of Finance & Econ., Tianjin",2008 International Conference on Computer Science and Software Engineering,20081222,2008,2,,516,519,"Todaypsilas world, virtual enterprise is regarded as the most competitive management model of enterprises that face the resource of the globe. How to select the best partner enterprise is a key section of the whole process of virtual enterprise creation. This paper proposed an instructor of remote manufacturing system supporting dynamic alliance for virtual enterprises with the multi-agent technology to solve the problem of partner enterprise selections.",,POD:978-0-7695-3336-0,10.1109/CSSE.2008.1613,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722105,Analytic Hierarchy Process (AHP);Multi-Agent System;Partner Selection;Virtual Enterprise (VE),Business;Computer science;Conference management;Engineering management;Finance;Financial management;Multiagent systems;Resource management;Software engineering;Virtual enterprises,manufacturing systems;multi-agent systems;virtual enterprises,competitive management model;dynamic alliance;multiagent system;partner enterprise selections;remote manufacturing system;virtual enterprise partner selection,,1,,10,,no,12-14 Dec. 2008,,IEEE,IEEE Conference Publications
Analytic performance models for bounded queueing systems,P. Krishnamurthy; R. D. Chamberlain,"Dept. of Computer Science and Engineering, Washington University in St. Louis, USA",2008 IEEE International Symposium on Parallel and Distributed Processing,20080603,2008,,,1,8,"Pipelined computing applications often have their performance modeled using queueing techniques. While networks with infinite capacity queues have well understood properties, networks with finite capacity queues and blocking between servers have resisted closed-form solutions and are typically analyzed with approximate solutions. It is this latter case that more closely represents the circumstances present for pipelined computation. In this paper, we extend an existing approximate solution technique and, more importantly, provide guidance as to when the approximate solutions work well and when they fail.",1530-2075;15302075,POD:978-1-4244-1693-6,10.1109/IPDPS.2008.4536123,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4536123,,Application software;Closed-form solution;Computer applications;Computer science;Network servers;Performance analysis;Pipeline processing;Predictive models;Queueing analysis;Throughput,pipeline processing;queueing theory,analytic performance models;bounded queueing systems;pipelined computing,,0,,8,,no,14-18 April 2008,,IEEE,IEEE Conference Publications
Analytic Programming Powered by Distributed Self-Organizing Migrating Algorithm Application,P. Varacha; I. Zelinka,"Dept. of Appl. Inf., Tomas Bata Univ. in Zlin, Zlin",2008 7th Computer Information Systems and Industrial Management Applications,20080709,2008,,,99,100,"This paper presents an idea of new algorithm combining advantages of evolutionary algorithm and simple distributed computing to perform tasks which required many re-runs of the same program. Computing time is shorted due to elementary distribution within a number of common computers via the Internet. Progressive .NET framework technology allowing this algorithm to run effectively and examples of possible usage are also described. The algorithm deals with a problem of synthesis of the artificial neural networks using the evolutional scanning method. The basic task to be solved is to create a symbolic regression algorithm on principles of analytic programming, which will be capable of performing a convenient neural network synthesis. The main motivation here is the computerization of such synthesis and discovering so far unknown solutions.",,POD:978-0-7695-3184-7,10.1109/CISIM.2008.50,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4557842,analytic programming;distributed computing;evolutionary algorithms;evolutionary scanning;evolutionary searching;neural network;symbolic regression,Algorithm design and analysis;Application software;Artificial neural networks;Distributed computing;Evolutionary computation;Functional programming;Genetic programming;Internet;Network synthesis;Performance analysis,distributed algorithms;evolutionary computation;network operating systems;neural nets,.NET framework technology;Internet;analytic programming;artificial neural network;distributed computing;distributed self-organizing migrating algorithm;evolutional scanning;evolutionary algorithm;neural network synthesis;program reruns;symbolic regression,,9,,9,,no,26-28 June 2008,,IEEE,IEEE Conference Publications
ANSIG‰ÛÓAn analytic signature for permutation-invariant two-dimensional shape representation,J. J. Rodrigues; P. M. Q. Aguiar; J. M. F. Xavier,"Institute for Systems and Robotics / IST, Lisboa, Portugal",2008 IEEE Conference on Computer Vision and Pattern Recognition,20080805,2008,,,1,8,"Many applications require a computer representation of 2D shape, usually described by a set of 2D points. The challenge of this representation is that it must not only capture the characteristics of the shape but also be invariant to relevant transformations. Invariance to geometric transformations, such as translation, rotation and scale, has received attention in the past, usually under the assumption that the points are previously labeled, i.e., that the shape is characterized by an ordered set of landmarks. However, in many practical scenarios the landmarks are obtained from an automatic process, e.g., edge/corner detection, thus without natural ordering. In this paper, we represent 2D shapes in a way that is invariant to the permutation of the landmarks. Within our framework, a shape is mapped to an analytic function on the complex plane, leading to what we call its analytic signature (ANSIG). We show that different shapes lead to different ANSIGs but that shapes that differ by a permutation of the landmarks lead to the same ANSIG, i.e., that our representation is a maximal invariant with respect to the permutation group. To store an ANSIG, it suffices to sample it along a closed contour in the complex plane. We further show how easy it is to factor out geometric transformations when comparing shapes using the ANSIG representation. We illustrate the ANSIG capabilities in shape-based image classification.",1063-6919;10636919,POD:978-1-4244-2242-5,10.1109/CVPR.2008.4587612,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4587612,,Application software;Computer applications;Image classification;Image edge detection;Iterative algorithms;Iterative closest point algorithm;Registers;Robots;Shape;Two dimensional displays,image classification;image representation,analytic signature;geometric transformations;permutation-invariant 2D shape representation;shape-based image classification,,10,,25,,no,23-28 June 2008,,IEEE,IEEE Conference Publications
Application of analytic network process in knowledge management performance evaluation,L. B. Liu,"Molex Interconnect Co. Ltd., Tianjin, China","2008 IEEE International Conference on Service Operations and Logistics, and Informatics",20081125,2008,1,,816,819,"Knowledge management (KM) has become an effective strategic tool to improve the organizational competition capacity in the age of knowledge economy. Knowledge management performance evaluation is an important way to know the knowledge management level of an enterprise. In this study, a knowledge management performance evaluation indices system was established based on knowledge management theory and analytic network process (ANP) performance evaluation methodology. Theoretical foundations and application process of ANP were discussed; dependence and feedback among indices were analyzed. Super Decisions software was used to obtain the result of ANP model for the indices system. The case study in this paper demonstrated that the ANP method can solve problems with dependent indices effectively.",,CD-ROM:978-1-4244-2013-1; POD:978-1-4244-2012-4,10.1109/SOLI.2008.4686511,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4686511,Knowledge management (KM);analytic network process (ANP);performance evaluation,Application software;Artificial neural networks;Decision making;Feedback;Globalization;Information analysis;Information technology;Knowledge management;Performance analysis;Power generation economics,knowledge management;performance evaluation,Super Decisions software;analytic network process;knowledge economy;knowledge management;organizational competition capacity;performance evaluation,,0,,15,,no,12-15 Oct. 2008,,IEEE,IEEE Conference Publications
Application of Schwarz-Christoffel Mapping to Permanent-Magnet Linear Motor Analysis,D. C. J. Krop; E. A. Lomonova; A. J. A. Vandenput,"Eindhoven Univ. of Technol., Eindhoven",IEEE Transactions on Magnetics,20080222,2008,44,3,352,359,"Several well-known analytical techniques exist for the force profile analysis of permanent-magnet linear synchronous motors. These techniques, however, make significant simplifications in order to obtain the magnetic field distribution in the air gap. From the field distribution, the force profile can be found. These widely used techniques provide a reasonable approximation for force profile analysis, but fail to give really accurate results in the sense of the exact shape of the force profile caused by effects that due to simplification are not fully included. To obtain the exact shape for the force profile in these cases, the computationally expensive finite-element method (FEM) is often applied. In this paper, an elegant semianalytical approach is presented to acquire the force profile. First, the magnetic field distribution in the air gap is determined by means of Schwarz-Christoffel (SC) mapping. The SC mapping allows a slotted structure of the machine to be mapped to a geometrically simpler domain for which analytic solutions are available. Subsequently, the field solution in the slotted structure can be determined by applying the mapping function to the field distribution in the simplified domain. From the resulting field distribution, the force profile is calculated by means of the Maxwell stress tensor. The results are compared with those from the commonly used equivalent magnetic circuit modeling and 2-D FEM software to demonstrate the accuracy which can be reached by application of the SC method.",0018-9464;00189464,,10.1109/TMAG.2007.914513,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4455703,Analytical model;Schwarz-Christoffel mapping;conformal mapping;end effects;magnetic equivalent circuit;permanent-magnet linear synchronous machine;slotted structure,,linear synchronous motors;permanent magnet motors,Maxwell stress tensor;Schwarz-Christoffel mapping;field distribution;force profile analysis;magnetic field distribution;permanent-magnet linear motor analysis;slotted structure,,26,,16,,no,8-Mar,,IEEE,IEEE Journals & Magazines
Applied visual analytics for economic decision-making,A. Savikhin; R. Maciejewski; D. S. Ebert,"Purdue University Department of Economics, USA",2008 IEEE Symposium on Visual Analytics Science and Technology,20081118,2008,,,107,114,"This paper introduces the application of visual analytics techniques as a novel approach for improving economic decision making. Particularly, we focus on two known problems where subjectspsila behavior consistently deviates from the optimal, the Winnerpsilas and Loserpsilas Curse. According to economists, subjects fail to recognize the profit-maximizing decision strategy in both the Winnerpsilas and Loserpsilas curse because they are unable to properly consider all the available information. As such, we have created a visual analytics tool to aid subjects in decision making under the Acquiring a Company framework common in many economic experiments. We demonstrate the added value of visual analytics in the decision making process through a series of user studies comparing standard visualization methods with interactive visual analytics techniques. Our work presents not only a basis for development and evaluation of economic visual analytic research, but also empirical evidence demonstrating the added value of applying visual analytics to general decision making tasks.",,POD:978-1-4244-2935-6,10.1109/VAST.2008.4677363,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4677363,,Application software;Decision making;Helium;Statistics;Uncertainty;Visual analytics;Visualization,data visualisation;decision making;econometrics;profitability,acquiring-a-company framework;data visualization method;economic decision-making;empirical evidence;interactive visual analytics technique;profit-maximization decision strategy;visual analytics tool,,8,,18,,no,19-24 Oct. 2008,,IEEE,IEEE Conference Publications
Approximate Joins for Data-Centric XML,N. Augsten; M. Bohlen; C. Dyreson; J. Gamper,"Faculty of Computer Science, Free University of Bozen-Bolzano, Dominikanerplatz 3, Bozen, Italy. augsten@inf.unibz.it",2008 IEEE 24th International Conference on Data Engineering,20080425,2008,,,814,823,"In data integration applications, a join matches elements that are common to two data sources. Often, however, elements are represented slightly different in each source, so an approximate join must be used. For XML data, most approximate join strategies are based on some ordered tree matching technique. But in data-centric XML the order is irrelevant: two elements should match even if their subelement order varies. In this paper we give a solution for the approximate join of unordered trees. Our solution is based on windowed pq-grams. We develop an efficient technique to systematically generate windowed pq-grams in a three-step process: sorting the unordered tree, extending the sorted tree with dummy nodes, and computing the windowed pq-grams on the extended tree. The windowed pq-gram distance between two sorted trees approximates the tree edit distance between the respective unordered trees. The approximate join algorithm based on windowed pq-grams is implemented as an equality join on strings which avoids the costly computation of the distance between every pair of input trees. Our experiments with synthetic and real world data confirm the analytic results and suggest that our technique is both useful and scalable.",1063-6382;10636382,CD-ROM:978-1-4244-1837-4; POD:978-1-4244-1836-7,10.1109/ICDE.2008.4497490,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4497490,,Application software;Computer science;Data analysis;Internet;Partitioning algorithms;Polynomials;Shape;Sorting;XML,XML;tree data structures,XML data;approximate join strategies;data integration applications;data sources;data-centric XML;join matches elements;ordered tree matching;windowed pq-grams,,6,1,24,,no,7-12 April 2008,,IEEE,IEEE Conference Publications
Appscio: A Software Environment for Semantic Multimedia Analysis,G. Friedland; E. Hensley; R. Jain; J. Schumacher,"Appscio Inc., Freedom, CA",2008 IEEE International Conference on Semantic Computing,20080812,2008,,,456,459,"The goal of the Appscio(tm) software platform is to ease the creation of multimedia content analysis applications that consist of components provided from multiple sources, in different programming languages, and for various operating systems. Appscio provides a unified approach that standardizes the entire process of development, deployment, and integration of components into productive applications. In addition, the aim is to facilitate the integration of analytic approaches with traditional sensor output. Therefore the framework allows the combination of multimedia analytics with any other event generating sources, as used for observational systems. A basic concept of the platform is to allow mainstream application developers to create semantic-rich Web applications that integrate components previously only accessible to scientists.",,CD-ROM:978-0-7695-3279-0,10.1109/ICSC.2008.56,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4597225,Analysis;Audio;Framework;Multimedia;Video,Algorithm design and analysis;Application software;Computer science;Information analysis;Multimedia computing;Multimedia systems;Operating systems;Signal processing algorithms;Software engineering;Streaming media,content management;multimedia computing;semantic Web,Appscio software environment;operating system;programming language;semantic multimedia content analysis;semantic-rich Web application,,0,1,,,no,4-7 Aug. 2008,,IEEE,IEEE Conference Publications
Asset Reliability Modeling and Simulation,T. T. Tower,"Kimberly-Clark Corporation, Central Research & Engineering, 2100 Winchester Rd., Neenah, WI 54956, USA",2008 Winter Simulation Conference,20081230,2008,,,2934,2934,"Traditionally in manufacturing discrete event simulation, delay occurrence and duration are represented by static distributions. For many problems and industries, this assumption may be appropriate, rendering them essentially independent. The manufacturing of soft, disposable consumer products, however, may involve the assembly of multi-component products with flexible materials that pose significant challenges to process reliability and thus make the independence assumption invalid. In fact, these non-linear interactions between coupled-delays can result in substantial financial opportunities from seemingly minor contributors. We describe an asset reliability modeling and simulation (ARMS) framework developed at Kimberly-Clark that uses discrete event simulation and dynamic reliability modeling of assetsÌâåÀ process event databases. Models are essentially built ÌâåÀon the flyÌâåÀ and survival simulations are validated against asset process history. Analytics and reports provide a means for identifying the biggest, overall improvement opportunities in performance metrics including production, delays, uptime, and waste.",0891-7736;08917736,CD-ROM:978-1-4244-2708-6; POD:978-1-4244-2707-9,10.1109/WSC.2008.4736424,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4736424,,Arm;Assembly;Consumer products;Delay;Discrete event simulation;Flexible manufacturing systems;Manufacturing industries;Manufacturing processes;Materials reliability;Virtual manufacturing,database management systems;discrete event simulation;software reliability,asset reliability modeling;asset reliability modeling and simulation;delay occurrence;discrete event simulation;multicomponent products;nonlinear interactions;performance metrics;process event databases;process reliability;product manufacturing;static distributions,,0,,,,no,7-10 Dec. 2008,,IEEE,IEEE Conference Publications
Award: Efficient toolkit integration solving the cell phone calls challenge with the Prajna Project,E. Swing,"Vision Systems & Technology, Inc., USA",2008 IEEE Symposium on Visual Analytics Science and Technology,20081118,2008,,,,,"The Prajna Project is a Java toolkit designed to provide various capabilities for visualization, knowledge representation, geographic displays, semantic reasoning, and data fusion. Rather than attempt to recreate the significant capabilities provided in other tools, Prajna instead provides software bridges to incorporate other toolkits where appropriate. This challenge required the development of a custom application for visual analysis. By applying the utilities within the Prajna project, I developed a robust and diverse set of capabilities to solve the analytical challenge.",,POD:978-1-4244-2935-6,10.1109/VAST.2008.4677396,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4677396,D.2.11 [Software Engineering]: Software Architectures - Domain-specific architectures;Information Visualization;Knowledge Representation;Software Toolkit;Toolkit Integration;[Computer Graphics]: Methodology and Techniques - Interaction Techniques,Visual analytics,Java;cellular radio;data visualisation;inference mechanisms;knowledge representation;sensor fusion;social sciences computing;telecommunication computing,Java toolkit integration;Prajna project;cell phone call;data fusion;data visualization;geographic display;knowledge representation;semantic reasoning;social network display;software bridge;visual analytics application,,2,,5,,no,19-24 Oct. 2008,,IEEE,IEEE Conference Publications
Bifurcationanalysis of an LC-tank VCO including the variable capacitance,J. K. Bremer; C. Zorn; W. Mathis,"Leibniz University Hanover, GERMANY",2008 15th International Conference on Mixed Design of Integrated Circuits and Systems,20081017,2008,,,389,394,"An alternative approach for designing fully integrated LC oscillators based on the Andronov-Hopf bifurcation analysis including a nonlinear overall-model for MOS transistors (EKV model) is introduced. In the presented procedure the capacitance of the VCO is described in terms of geometric transistor dimensions. This leads to an extended bifurcation analysis which considers only the widths and lengths of the used transistors. For the description of the MOS capacitance a basic-charge-model, as it was introduced in the work by Enz and Vittoz, was used in combination of an explicit analytic approximation of the surface potential. Our procedure enables the consideration of the oscillator amplitude as an additional design parameter in the VCO design process. Furthermore it allows the designer to receive a more exact estimation of the varactor dimensions in advance. The procedure has been implemented in a software toolbox in Matlab which employs a graphical optimization process that facilitates the analytical determination of the required design parameters.",,POD:978-83-922632-7-2,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4600942,Bifurcation analysis;EKV model;LC-Tank VCO;Nonlinear dynamic system;Oscillator design;Surface potential;Varactor design,Bifurcation;Capacitance;Circuits;Design optimization;Mathematical model;Nonlinear dynamical systems;Phase noise;Process design;Varactors;Voltage-controlled oscillators,,,,0,,12,,no,19-21 June 2008,,IET,IET Conference Publications
Business 2.0: A novel model for delivery of business services,Poh-Hean Yap; Kok-Leong Ong; Xungai Wang,"Deakin University, Australia",2008 International Conference on Service Systems and Service Management,20080812,2008,,,1,5,"Web 2.0, regardless of the exact definition, has proven to bring about significant changes to the way the Internet was used. Evident by key innovations such as Wikipedia, FaceBook, YouTube, and Blog sites, these community-based Website in which contents are generated and consumed by the same group of users are changing the way businesses operate. Advertisements are no longer dasiaforcedpsila upon the viewers but are instead dasiaintelligentlypsila targeted based on the contents of interest. In this paper, we investigate the concept of Web 2.0 in the context of business entities. We asked if Web 2.0 concepts could potentially lead to a change of paradigm or the way businesses operate today. We conclude with a discussion of a Web 2.0 application we recently developed that we think is an indication that businesses will ultimately be affected by these community-based technologies; thus bringing about Business 2.0 - a paradigm for businesses to cooperate with one another to deliver improved products and services to their own customers.",2161-1890;21611890,CD-ROM:978-1-4244-1672-1; POD:978-1-4244-1671-4,10.1109/ICSSSM.2008.4598499,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4598499,B2B;Web 2.0;business analytics;data analysis,Application software;Blogs;Cultural differences;Data analysis;Facebook;Internet;Software tools;User-generated content;Wikipedia;Writing,Web services;Web sites;business data processing;customer services,Business 2.0;FaceBook;Internet;Web 2.0;Web sites;Wikipedia;YouTube;blog sites;business services;customer service,,0,,16,,no,June 30 2008-July 2 2008,,IEEE,IEEE Conference Publications
Cell phone mini challenge award: Social network accuracy‰ÛÓ exploring temporal communication in mobile call graphs,Qi Ye; Tian Zhu; Deyong Hu; Bin Wu; Nan Du; Bai Wang,"Beijing University of Posts and Telecommunications, Beijing Key Lab of Intelligent Telecommunications Software and Multimedia, China",2008 IEEE Symposium on Visual Analytics Science and Technology,20081118,2008,,,,,"In the mobile call mini challenge of VAST 2008 contest, we explored the temporal communication patterns of Catalano/Vidro social network which is reflected in the mobile call data. We focus on detecting the hierarchy of the social network and try to get the important actors in it. We present our tools and methods in this summary. By using the visual analytic approaches, we can find out not only the temporal communication patterns in the social network but also the hierarchy of it.",,POD:978-1-4244-2935-6,10.1109/VAST.2008.4677389,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4677389,D.2.2 [Software Engineering]: Design Tools and Techniques‰ÛÓUser interfaces;H.2.8 [Database Management]: Database Applications‰ÛÓData mining,Visual analytics,cellular radio;graph theory,Catalano-Vidro social network;cell phone mini challenge award;mobile call graphs;social network accuracy;temporal communication patterns;visual analytic approaches,,3,,3,,no,19-24 Oct. 2008,,IEEE,IEEE Conference Publications
Challenges of mapping financial analytics to many-core architecture,M. Smelyanskiy,"Corporate Technology Group, Intel, USA",2008 Workshop on High Performance Computational Finance,20090109,2008,,,1,1,"Summary form only given. In the past 20 years there has been an explosive growth of the variety of traded financial instruments, from European and American options to a more complex, alas ill-fated, credit derivatives. The rapid increase in computational power coupled with the use of mathematical tools for valuing these instruments and estimating the risk has given rise to the discipline of computational finance. Multi- and many-core architectures present significant potential for performance gains in financial applications. Recent years have seen an emergence of a variety of such designs, from general-purpose multi-cores, to GPGPU-, ASIC- and FPGA- style accelerates. To efficiently utilize hundreds of gigaflops offered by such systems requires serious optimization and parallelization effort from an application programmer. This can be a significant deterrent to a quant, traditionally focused on development, validation and deployment speed of a given pricing model, rather than optimizing model implementation for higher performance. In this talk I will describe several recent parallel hardware and software platforms for accelerating financial analytics. I will also discuss the impact that many-core era will have on financial industry and implications for both quants as well as parallel platform designers.",,CD-ROM:978-1-4244-3311-7; POD:978-1-4244-2911-0,10.1109/WHPCF.2008.4745397,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4745397,,Acceleration;Application software;Computer architecture;Explosives;Finance;Hardware;Instruments;Performance gain;Pricing;Programming profession,financial data processing;financial management;parallel architectures;parallel programming;performance evaluation;pricing,American options;European options;financial analysis;many-core architecture;parallel hardware;parallel software;pricing model,,0,,,,no,16-16 Nov. 2008,,IEEE,IEEE Conference Publications
ChargeView: An integrated tool for implementing chargeback in IT systems,S. Agarwala; R. Routray; S. Uttamchandani,"IBM Almaden Research Center, San Jose, CA 95120, USA",NOMS 2008 - 2008 IEEE Network Operations and Management Symposium,20080826,2008,,,371,378,"Most organizations are becoming increasingly reliant on IT product and services to manage their daily operations. The total cost of ownership (TCO), which includes the hardware and software purchase cost, management cost, etc., has significantly increased and forms one of the major portions of the total expenditure of the company. CIOs have been struggling to justify the increased costs and at the same time fulfill the IT needs of their organizations. For businesses to be successful, these costs need to be carefully accounted and attributed to specific processes or user groups/departments responsible for the consumption of IT resources. This process is called IT chargeback and although desirable, is hard to implement because of the increased consolidation of IT resources via technologies like virtualization. Current IT chargeback methods are either too complex or too adhoc, and often a times lead to unnecessary tensions between IT and business departments and fail to achieve the goal for which chargeback was implemented. This paper presents a new tool called ChargeView that automates the process of IT costing and chargeback. First, it provides a flexible hierarchical framework that encapsulates the cost of IT operations at different level of granularity. Second, it provides an easy way to account for different kind of hardware and management costs. Third, it permits implementation of multiple chargeback policies that fit the organization goals and establishes relationship between the cost and the usage by different users and departments within an organization. Finally, its advanced analytics functions can keep track of usage and cost trends, measure unused resources and aid in determining service pricing. We discuss the prototype implementation of ChargeView and show how it has been used for managing complex systems and storage networks.",1542-1201;15421201,POD:978-1-4244-2065-0,10.1109/NOMS.2008.4575157,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4575157,,Automation;Availability;Companies;Cost function;Costing;Hardware;Pricing;Prototypes;Resource virtualization;Storage area networks,DP management;costing,ChargeView;IT chargeback;IT costing;IT systems;organization goals;service pricing;virtualization,,2,2,26,,no,7-11 April 2008,,IEEE,IEEE Conference Publications
Combining LISREL and Bayesian network to predict tourism loyalty,Chi-I Hsu; Meng-Long Shih; Biing-Wen Huan; Bing-Yi Li; Chun-Nan Lin,"Kainan University, Taiwan, ROC",2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence),20080926,2008,,,3000,3004,"This study proposes an analytic approach that combines LISREL and Bayesian networks (BN) to examine factors influencing tourism loyalty and predict a touristpsilas loyalty level. LISREL is used to verify the hypothesized relationships proposed in the research model. Subsequently, the supported relationships are used as the BN network structure for prediction. 452 valid samples were collected from tourists with the tour experience of the Toyugi hot spring resort, Taiwan. Compared with other prediction methods, our approach yielded better results than those of back-propagation neural networks (BPN) or classification and regression trees (CART) for 10-fold cross-validation.",2161-4393;21614393,POD:978-1-4244-1820-6,10.1109/IJCNN.2008.4634220,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4634220,,Bayesian methods;Classification tree analysis;Neural networks;Prediction methods;Regression tree analysis;Springs,belief networks;mathematics computing;statistical analysis;travel industry,Bayesian network;LISREL statistical software package;Toyugi hot spring resort;back-propagation neural network;classification method;regression tree;tourism loyalty prediction,,0,,29,,no,1-8 June 2008,,IEEE,IEEE Conference Publications
Combining the Power of Taverna and caGrid: Scientific Workflows that Enable Web-Scale Collaboration,W. Tan; I. Foster; R. Madduri,University of Chicago and Argonne National Laboratory,IEEE Internet Computing,20081111,2008,12,6,61,68,"Service-oriented architecture represents a promising approach to integrating data and software across different institutional and disciplinary sources, thus facilitating Web-scale collaboration while avoiding the need to convert different data and software to common formats. The US National Cancer Institute's Biomedical Information Grid program seeks to create both a service-oriented infrastructure (caGrid) and a suite of data and analytic services. Workflow tools in caGrid facilitate both the use and creation of services by accelerating service discovery, composition, and orchestration tasks. The authors present caGrid's workflow requirements and explain how they met these requirements by adopting and extending the Taverna system.",1089-7801;10897801,,10.1109/MIC.2008.120,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4670120,bioinformatics grid;scientific workflow;service oriented science,Acceleration;Cancer;Cities and towns;Collaboration;Collaborative software;Collaborative work;Data analysis;Ecosystems;Information analysis;Service oriented architecture,Internet;grid computing;groupware;medical computing;software architecture;software tools;workflow management software,Taverna software tool;US National Cancer Institute Biomedical Information Grid program;Web-scale collaboration;caGrid workflow requirement;orchestration service discovery;scientific workflow;service-oriented architecture,,15,,10,,no,Nov.-Dec. 2008,,IEEE,IEEE Journals & Magazines
Comparison with Parametric Optimization in Credit Card Fraud Detection,M. F. A. Gadi; X. Wang; A. P. d. Lago,"Grupo Santander, Santander Analytics, Milton Keynes, UK",2008 Seventh International Conference on Machine Learning and Applications,20081222,2008,,,279,285,"We apply five classification methods, neural nets (NN), Bayesian nets (BN), naive Bayes (NB), artificial immune systems (AIS) and decision trees (DT), to credit card fraud detection. For a fair comparison, we fine adjust the parameters for each method either through exhaustive search, or through genetic algorithm (GA). Furthermore, we compare these classification methods in two training modes: a cost sensitive training mode where different costs for false positives and false negatives are considered in the training phase; and a plain training mode. The exploration of possible cost-sensitive metaheuristics to be applied is not in the scope of this work and all executions are run using Weka, a publicly available software. Although NN is claimed to be widely used in the market today, the evaluated implementation of NN in plain training leads to quite poor results. Our experiments are consistent with the early result of Maes et al. (2002) which concludes that BN is better than NN. Cost sensitive training substantially improves the performance of all classification methods apart from NB and, independently of the training mode, DT and AIS with, optimized parameters, are the best methods in our experiments.",,POD:978-0-7695-3495-4,10.1109/ICMLA.2008.59,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724987,Artificial Immune Systems;Bayesian Nets;Comparison;Credit card;Fraud Detection;Neural Nets,Artificial immune systems;Artificial neural networks;Bayesian methods;Classification tree analysis;Costs;Credit cards;Decision trees;Genetic algorithms;Neural networks;Niobium,Bayes methods;artificial immune systems;belief networks;credit transactions;decision trees;fraud;genetic algorithms;neural nets,Bayesian nets;artificial immune system;cost sensitive training;cost-sensitive metaheuristics;credit card fraud detection;decision trees;genetic algorithm;naive Bayes;neural net;parametric optimization,,2,,20,,no,11-13 Dec. 2008,,IEEE,IEEE Conference Publications
Comprehensive Assessment Model of Network Vulnerability Based upon Refined Mealy Automata,C. Fu; L. Fu,"Inf. & network Manage. Center, Chongqing Univ., Chongqing",2008 International Conference on Computer Science and Software Engineering,20081222,2008,3,,595,600,"To evaluate the vulnerability of a network of hosts comprehensively, a security analyst must take into account the effects of interactions between these vulnerabilities. This paper proposes Comprehensive Assessment Model (CAM) to illustrate the multi-stage attack action and evaluate network vulnerability in context. We make use of first-order predicate calculus to formally refine the Mealy Automata based CAM model. And then we in detail demonstrate the application of this model, in which we have applied the Analytic Hierarchy Process (AHP) to the measurement of vulnerability degree and conclude the security solution.",,POD:978-0-7695-3336-0,10.1109/CSSE.2008.779,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722414,Attack model;First-order predicate;Mealy automata;Network vulnerability assessment,Automata;CADCAM;Collaboration;Computer aided manufacturing;Computer network management;Computer science;Conference management;Information management;Software engineering;Tree graphs,automata theory;calculus of communicating systems;computer networks;telecommunication security,Mealy automata;comprehensive assessment model;first-order predicate calculus;multistage attack action;network security analyst;network vulnerability,,0,,14,,no,12-14 Dec. 2008,,IEEE,IEEE Conference Publications
Decision Support for User Interface Design: Usability Diagnosis by Time Analysis of the User Activity,A. Harel; R. S. Kenett; F. Ruggeri,"Ergolight Ltd., Haifa",2008 32nd Annual IEEE International Computer Software and Applications Conference,20080808,2008,,,836,840,"This paper presents a methodology for setting up a decision support system for user interface design (DSUID). We first motivate the role and contributions of DSUID and then demonstrate its implementation in the case of usability diagnosis of Web pages, based on time analysis of clickstream data. The resulting DSUID diagnostic reports enable website managers to learn about possible sources of usability barriers. The proposed DSUID analytic method is based on the integration of stochastic Bayesian and Markov models with models for estimating and analyzing the visitors' mental activities during their interaction with a Website. Based on this approach, a seven-layer model for data analysis is suggested and an example of a log analyzer that implements this model is presented. We demonstrate the approach with an example of a Bayesian network applied to clickstream data and conclude with general observations on the generic role of DSUID and the implementation framework we propose.",0730-3157;07303157,CD-ROM:978-0-7695-3262-2,10.1109/COMPSAC.2008.61,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4591677,Bayesian Networks;DSUID;Markov Models;Seven Layers;Usability,Bayesian methods;Data analysis;Decision support systems;File servers;Humans;Navigation;Testing;Usability;User interfaces;Web page design,Bayes methods;Internet;Markov processes;decision support systems;stochastic processes;user interfaces,Markov models;Web pages;Website;clickstream data;decision support system for user interface design;log analyzer;stochastic Bayesian models;usability diagnosis;user activity,,2,,8,,no,July 28 2008-Aug. 1 2008,,IEEE,IEEE Conference Publications
Design and implementation of honeypot systems based on open-source software,Chao-Hsi Yeh; Chung-Huang Yang,"Graduate Institute of Information and Computer Education, National Kaohsiung Normal University, Taiwan, R.O.C.",2008 IEEE International Conference on Intelligence and Security Informatics,20080715,2008,,,265,266,"A honeypot is a type of information system that is used to obtain information on intruders in a network. When a honeypot is deployed in front of a firewall, it can serve as an early warning system. When deployed behind the firewall, it can serve as part of a defense-in-depth system and can be used to detect attackers who bypass the firewall and the intrusion detection system (IDS) or threats from insiders. Honeyd is an open-source honeypot; however, it uses a command-line interface and its configuration is difficult for beginners. The purpose of this study is to use the open-source tool to construct a graphic user interface (GUI) for honeyd. For the sake of portability and easy deployment, the whole system will be installed in a live USB stick. The end user can create a honeyd template by using the GUI or the result of the Nmap scan of a target computer. Moreover, the system will provide a log-review interface and real-time SMS functionality. Finally, we deployed the designed system in a campus network and presented an analytic result of a 60-day period with a Web-based data analysis system.",,CD-ROM:978-1-4244-2415-3; POD:978-1-4244-2414-6,10.1109/ISI.2008.4565077,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4565077,,Alarm systems;Computer graphics;Data analysis;Graphical user interfaces;Information systems;Intrusion detection;Open source software;Real time systems;Universal Serial Bus;User interfaces,authorisation;computer networks;graphical user interfaces;public domain software,Nmap scan;command-line interface;defense-in-depth system;firewall;graphic user interface;honeyd;honeypot system;information system;intrusion detection system;network intruder;open-source software,,1,,9,,no,17-20 June 2008,,IEEE,IEEE Conference Publications
Design Method of UI of AV Remote Controller Based on AHP,M. Matsuda; Y. Uesugi; T. Nonaka; T. Hase,"Mitsubishi Electric Microcomputer Application Software Co., Ltd.",2008 Digest of Technical Papers - International Conference on Consumer Electronics,20080805,2008,,,1,2,"This paper proposes a new model for designing the user interface of an audio-visual remote controller based on an analytic hierarchy process. This model has the goal of switching the UI of the controller to one optimized for each user. The following four criteria were defined; demographic, geographic, psychographic and activity, sensor and I/O, Visual, Functional and Interaction interfaces were selected as the alternatives. Next, the method of selecting the most suitable UI for each user by using the proposed model based on AHP is described and shown by an example. Finally, the implementation of the proposal into a verification system for an AV remote controller using an embedded microprocessor for consumer use is described.",2158-3994;21583994,CD-ROM:978-1-4244-1459-8; POD:978-1-4244-1458-1,10.1109/ICCE.2008.4587863,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4587863,,Application software;Audio-visual systems;Communication system control;Control systems;Demography;Design methodology;Microcomputers;Proposals;Psychology;User interfaces,audio-visual systems;consumer electronics;microprocessor chips;telecontrol;user interfaces,analytic hierarchy process;audio-visual remote controller;consumer use;embedded microprocessor;user interface;verification system,,3,,5,,no,9-13 Jan. 2008,,IEEE,IEEE Conference Publications
Draw-lines algorithm in 2D for iris image,S. S. Islas; R. R. Herrera; G. de Jesus Lopez Ruiz,"Escuela Superior de Computo, IPN, Departamento de Sistemas Electr&#243;nicos, UPLM - Zacatenco, Av. Batiz y Mendiz&#225;bal s/n Col. Lindavista 07738 M&#233;xico, D. F., Mexico","2008 7th International Caribbean Conference on Devices, Circuits and Systems",20080613,2008,,,1,5,"Personal computers have become in a powerful tool to produce image, to interpret information or to improve the quality of the visualization of the same ones in a quick and a economic form. The algorithms used for graphics by computer, are used to create one or more images. In this work we are going to approach on the application of an algorithm for drawing lines in 2-D on images of iris in computers, which is based on concepts of analytic geometry. The implemented method uses the basic definitions of the polar coordinate equations and its transformation to the rectangular coordinates. In essence what is shown is the operation of the algorithm and that the drawing of lines in 2-D is exact and more efficient that the one developed by Bresenham to draw lines in 2-D on images of the iris. The proposed algorithm allows to make segmentation of the human eye with high accuracy; This is of much aid for the individual analysis of the eye, reason why it is an important tool in the case of surgical operations of the eye.",2165-3542;21653542,CD-ROM:978-1-4244-1957-9; POD:978-1-4244-1956-2,10.1109/ICCDCS.2008.4542666,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4542666,Angle;line screen;pending;pixel;point;radius,Algorithm design and analysis;Application software;Computational geometry;Computer graphics;Equations;Image analysis;Iris;Microcomputers;Power generation economics;Visualization,computer graphics;eye;medical image processing;surgery,computer graphics;draw-lines algorithm;eye;iris image;surgical operations,,0,,7,,no,28-30 April 2008,,IEEE,IEEE Conference Publications
Effect of automotive headlamp modeling on automotive aerodynamic drag,Lanfang Jiang; Hong Liu; Guozhong Chai; Guangnan Jiang; Weiming Lin,"The MOE Key Laboratory of Mechanical Manufacture and Automation, Zhejiang University of Technology, Hangzhou, China",2008 9th International Conference on Computer-Aided Industrial Design and Conceptual Design,20081230,2008,,,588,593,"Automotive headlamp design, combining science with art, is essential in automotive modeling design. Headlamp modeling design should consider harmonizing with automotive modeling design as well as meeting the national standards of structural design and lighting property. The research aims to present an approach for headlamp modeling design considering automotive aerodynamic drag. The effect of different headlamp modeling design on aerodynamic drag and accordingly three sorts of parameters of headlamp modeling design combining with aesthetic demand are studied. An analytic model of headlamp on automotive aerodynamic drag is established. Numerical simulation of 3-D flow field around the automobile with CFD software is performed. The change of drag coefficient with parameters is analyzed. Finally the design rules of headlamp aerodynamic modeling optimization are concluded. The research effort will enrich the design means of automotive modeling and automotive headlamp, so as to improve the technical property.",,CD-ROM:978-1-4244-3291-2; POD:978-1-4244-3290-5,10.1109/CAIDCD.2008.4730637,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4730637,Aerodynamic Drag;Automotive Modeling Design;Headlamp,Aerodynamics;Automobiles;Automotive engineering;Fuel economy;Laboratories;Lamps;Manufacturing automation;Shape;Testing;Vehicle dynamics,aerodynamics;automotive components;computational fluid dynamics;drag;flow simulation;lamps;production engineering computing,3-D flow field;CFD software;automotive aerodynamic drag;automotive headlamp modeling;drag coefficient,,1,,12,,no,22-25 Nov. 2008,,IEEE,IEEE Conference Publications
Effect of resonance-mode order on mass-sensing resolution of microcantilever sensors,Xiaoyuan Xia; Ping Zhou; Xinxin Li,"State Key Lab of Transducer Technology, Shanghai Institute of Microsystem and Information Technology, Chinese Academy of Sciences, China",2008 IEEE Sensors,20081216,2008,,,577,580,"The paper reports a systematic study of the resonance-mode effect on mass-sensing performance of resonant microcantilevers by both analytic and experimental methods. Firstly the research reveals that the in-air mass-sensing resolution of resonant microcantilevers is dominated by air-drag loss limited Q-factor. Using theoretic analysis and software simulation, we conclude that a higher order mode is with the resolution superior to a lower order mode and a torsion-mode is generally better than a flexure-mode. 4 types of cantilevers resonating in 4 practically realizable resonance-modes, i.e. the 1st, 2nd flexural modes and the 1st, 2nd torsional modes, are designed with identical length, thickness and the main-stem width. Micromachining techniques are used to fabricate the 4 silicon cantilevers with the actuating and signal-readout elements integrated. Biotin-avidin detecting experiment is performed, with the results well verifying the analytic conclusion for sensor optimal design guidance.",1930-0395;19300395,CD-ROM:978-1-4244-2581-5; POD:978-1-4244-2580-8,10.1109/ICSENS.2008.4716505,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4716505,,Analytical models;Damping;Drag;Energy resolution;Force sensors;Micromachining;Performance analysis;Q factor;Resonance;Sensor systems,Q-factor;cantilevers;finite element analysis;mass measurement;microsensors;resonance,Biotin-avidin detection;air-drag loss limited Q-factor;in-air mass-sensing resolution;microcantilever sensors;micromachining techniques;resonance-mode order;resonant microcantilevers;sensor optimal design guidance,,0,,8,,no,26-29 Oct. 2008,,IEEE,IEEE Conference Publications
EIGER‰ã¢ development and application to an IR frequency-selective surface,W. A. Johnson; L. I. Basilio; D. R. Wilton; N. J. Champagne; J. D. Kotulski; A. A. Cruz-Cabrera; D. W. Peters,"University of Houston, Tx, 77204-4005, USA",2008 IEEE Antennas and Propagation Society International Symposium,20080909,2008,,,1,3,"As presented in (Crruz-Cabera et al., 2008), our experimental counterparts have recently obtained exciting results for a frequency-selective surface (FSS) operating in the mid-wave to long-wave infrared frequency range. While that paper emphasizes fabrication and experimental results, it also includes a numerical validation check based on the EIGER electromagnetics simulation tool (the comparison shows favorable agreement). EIGER is a general purpose frequency-domain integral equation code that supports a variety of Greenpsilas functions (GFs), including 2D and 3D free space GFs, symmetry-plane GFs, periodic GFs, and layered media GFs. While the choice of integral equations as a modeling tool may first appear to be the most complex choice, the strength of this method lies in the fact that code generality is realized on the development of the corresponding Greenpsilas functions. In other words, the capability of integral-equation-based code to handle a wide variety of problems is obtained by incorporating more Greenpsilas functions into the software and furthermore, to complement the generality, code speedup can be obtained by taking advantage of the amenability of GFs to analytic techniques. For example, applications of EIGER at Sandia have addressed EMC and EMI problems including thin-slot coupling, periodic diffraction gratings for a petawatt laser, photonic band-gap structures, and electro- and magnetostatic problems for pulsed power and micromachine designs.",1522-3965;15223965,CD-ROM:978-1-4244-2042-1; POD:978-1-4244-2041-4,10.1109/APS.2008.4619723,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4619723,,Application software;Diffraction;Electromagnetic compatibility;Electromagnetic interference;Fabrication;Frequency selective surfaces;Integral equations;Magnetic analysis;Nonhomogeneous media;Optical coupling,antenna accessories;electromagnetic compatibility;electromagnetic interference;frequency selective surfaces;photonic band gap,2D free space;3D free space;EIGER electromagnetics simulation tool;EMC;EMI;Green functions;IR frequency-selective surface;frequency-domain integral equation code;integral-equation-based code;layered media;long-wave infrared frequency range;micromachine designs;mid-wave infrared frequency range;periodic diffraction gratings;petawatt laser;photonic band-gap structures;pulsed power designs;symmetry-plane;thin-slot coupling,,0,,10,,no,5-11 July 2008,,IEEE,IEEE Conference Publications
Enabling Streaming Remoting on Embedded Dual-Core Processors,K. Y. Hsieh; Y. C. Liu; P. W. Wu; S. W. Chang; J. K. Lee,"Dept. of Comput. Sci., Nat. Tsing-Hua Univ., Hsinchu",2008 37th International Conference on Parallel Processing,20080916,2008,,,35,42,"Dual-core processors (and, to an extent, multicore processors) have been adopted in recent years to provide platforms that satisfy the performance requirements of popular multimedia applications. This architecture comprises groups of processing units connected by various interprocess communication mechanisms such as shared memory, memory mapping interrupts, mailboxes, and channel-based protocols. The associated challenges include how to provide programming models and environments for developing streaming applications for such platforms. In this paper, we present middleware called streaming RPC for supporting a streaming-function remoting mechanism on asymmetric dual-core architectures. This middleware has been implemented both on an experimental platform known as the PAC dual-core platform and in TI OMAP dual-core environments. We also present an analytic model of streaming equations to optimize the internal handshaking for our proposed streaming RPC. The usage and efficiency of the proposed methodology are demonstrated in a JPEG decoder, MP3 decoder, and QCIF H.264 decoder. The experimental results show that our approach improves the performance of the decoders of JPEG, MP3, and H.264 by 24%, 38%, and 32% on PAC, respectively. The communication load of internal handshaking has also been reduced compared to the naive use of RPC over embedded dual-core systems. The experiments also show that the performance improvement can also be achieved on OMAP dual-core platforms.",0190-3918;01903918,CD-ROM:978-0-7695-3374-2,10.1109/ICPP.2008.32,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4625830,,Application software;Computer science;Decoding;Digital audio players;Java;Middleware;Multicore processing;Parallel processing;Protocols;Streaming media,data handling;interrupts;microprocessor chips;middleware;shared memory systems,channel-based protocols;embedded dual-core processors;mailboxes;memory mapping interrupts;middleware;shared memory;streaming remoting,,10,1,19,,no,9-12 Sept. 2008,,IEEE,IEEE Conference Publications
Entity-based collaboration tools for intelligence analysis,E. A. Bier; S. K. Card; J. W. Bodnar,"Palo Alto Research Center, Inc., 3333 Coyote Hill Road, California, 94304, USA",2008 IEEE Symposium on Visual Analytics Science and Technology,20081118,2008,,,99,106,"Software tools that make it easier for analysts to collaborate as a natural part of their work will lead to better analysis that is informed by more perspectives. We are interested to know if software tools can be designed that support collaboration even as they allow analysts to find documents and organize information (including evidence, schemas, and hypotheses). We have modified the Entity Workspace system, described previously, to test such designs. We have evaluated the resulting design in both a laboratory study and a study where it is situated with an analysis team. In both cases, effects on collaboration appear to be positive. Key aspects of the design include an evidence notebook optimized for organizing entities (rather than text characters), information structures that can be collapsed and expanded, visualization of evidence that emphasizes events and documents (rather than emphasizing the entity graph), and a notification system that finds entities of mutual interest to multiple analysts.",,POD:978-1-4244-2935-6,10.1109/VAST.2008.4677362,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4677362,"H.3.3 [Information Search and Retrieval]: Information filtering;H.4 [Information Systems Applications]: H.4.m Miscellaneous;H.5.2 [User Interfaces]: Graphical user interfaces (GUI);H.5.3 [Group and Organization Interfaces]: Collaborative computing, Computer-supported cooperative work, Web-based interaction;argumentation marshalling;collaboration;collective intelligence;entity-based;exploratory search;information foraging;information workspace;intelligence analysis;semantic notebook;sensemaking;visual analytics;visualization",Collaborative software;Collaborative tools;Collaborative work;Design optimization;Information analysis;Laboratories;Organizing;Software tools;System testing;Text analysis,entity-relationship modelling;groupware;software tools,entity workspace system;entity-based collaboration tools;information structures;intelligence analysis;software tools,,14,,17,,no,19-24 Oct. 2008,,IEEE,IEEE Conference Publications
Estimating the energy cost of communication on portable wireless devices,R. Palit; K. Naik; A. Singh,"Department of Electrical and Computer Engineering, University of Waterloo, ON, Canada",2008 1st IFIP Wireless Days,20090410,2008,,,1,5,"Software applications running on portable wireless devices communicate with the rest of the network over a wireless link. In these portable devices, the communication cost is a large fraction of the total energy consumption. The amount of energy consumed by the communication component of a portable device mostly depends on different parameters such as packet size and packet rate (or, bit rate). In this paper, we present the results of our investigation of the impacts of these communication parameters on energy consumption. First we build a simple analytic model to estimate the energy consumption due to receiving and transmitting data packets, and then we validate our model by conducting experiments. Results show that the analytical model is effective and gives accurate results. By varying data packet lengths, a communication device consumes different levels of energy to achieve the same data rate. When the packet size is very small compared to the maximum transmission unit (MTU), the device consumes more energy. However, large packets do not necessarily save energy. They rather add some other types of overheads, such as segmentation, recombination, and packet drop. Thus, for a given set of network parameters, an application can choose a suitable data packet length to minimize energy consumption. We also present the impact of data rate and packet delays on energy consumption. These results help us in understanding the energy consumption behavior of a communication device. They also facilitate us in optimizing the energy cost while designing a wireless application.",2156-9711;21569711,CD-ROM:978-1-4244-2829-8; POD:978-1-4244-2828-1,10.1109/WD.2008.4812890,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4812890,Portable wireless devices;energy estimation model;wireless networks,Application software;Bandwidth;Base stations;Batteries;Costs;Energy consumption;Hardware;Mobile communication;Web and internet services;Wireless communication,delays;radio links,communication cost;data packet lengths;data packets transmission;energy consumption;energy cost estimation;maximum transmission unit;network parameters;packet delays;packet rate;portable wireless devices;wireless application;wireless link,,2,,16,,no,24-27 Nov. 2008,,IEEE,IEEE Conference Publications
Evacuation traces mini challenge: User testing to obtain consensus discovering the terrorist,A. L. Simeone; B. Paolo,"University of Bari, Italy",2008 IEEE Symposium on Visual Analytics Science and Technology,20081118,2008,,,,,"The adoption of visual analytics methodologies in security applications is an approach that could lead to interesting results. Usually, the data that has to be analyzed finds in a graphical representation its preferred nature, such as spatial or temporal relationships. Due to the nature of these applications, it is very important that key-details are made easy to identify. In the context of the VAST 2008 Challenge, we developed a visualization tool that graphically displays the movement of 82 employees of the Miami Department of Health (USA). We also asked 13 users to identify potential suspects and observe what happened during an evacuation of the building caused by an explosion. In this paper we explain the results of the user testing we conducted and how the users interpreted the event taken into account.",,POD:978-1-4244-2935-6,10.1109/VAST.2008.4677390,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4677390,H.1.2 [Models and principles]: User/Machine Systems‰ÛÓHuman information processing;H.1.2 [Models and principles]: User/Machine Systems‰ÛÓSoftware psychology;casualties detection;evacuation;user testing;visual analytics,Animation;Data security;Data visualization;Discrete event simulation;Information processing;Psychology;System testing;Visual analytics,data visualisation;emergency services;security;terrorism;user interfaces,evacuation traces mini challenge;graphical representation;security applications;terrorist;user testing;visual analytics,,0,,4,,no,19-24 Oct. 2008,,IEEE,IEEE Conference Publications
Expanding the Criteria for Evaluating Socio-Technical Software,B. Whitworth; V. Banuls; C. Sylla; E. Mahinda,"Inst. of Inf. & Math. Sci., Massey Univ., Auckland","IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans",20080617,2008,38,4,777,790,"This paper compares two evaluation criterion frameworks for sociotechnical software. Research on the technology acceptance model (TAM) confirms that perceived usefulness and perceived ease of use are relevant criteria for users evaluating organizational software. However, information technology has changed considerably since TAM's 1989 inception, so an upgraded evaluation framework may apply. The web of system performance (WOSP) model suggests eight evaluation criteria, based on a systems theory definition of performance. This paper compares WOSP and TAM criterion frameworks in a performance evaluation experiment using the analytic hierarchy process method. Subjects who used both TAM and WOSP criteria preferred the WOSP criteria, were more satisfied with its decision outcomes, and found the WOSP evaluation more accurate and complete. As sociotechnical software becomes more complex, users may need (or prefer) more comprehensive evaluation criterion frameworks.",1083-4427;10834427,,10.1109/TSMCA.2008.923038,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4544888,Sociotechnical;software requirements;system performance;technology assessment,Costs;Decision making;Information technology;Investments;Management information systems;Performance analysis;Software performance;Software systems;System performance;Technology management,software performance evaluation,Web of system performance model;evaluation criteria;organizational software evaluation;socio-technical software evaluation;technology acceptance model,,3,,90,,no,8-Jul,,IEEE,IEEE Journals & Magazines
Experimental validation of grid algorithms: A comparison of methodologies,E. Jeannot,"INRIA Nancy Grand-Est, LORIA, Nancy University, CNRS, France",2008 IEEE International Symposium on Parallel and Distributed Processing,20080603,2008,,,1,8,"The increasing complexity of available infrastructures with specific features (caches, hyper- threading, dual core, etc.) or with complex architectures (hierarchical, parallel, distributed, etc.) makes models either extremely difficult to build or intractable. Hence, it raises the question: how to validate algorithms if a realistic analytic analysis is not possible any longer? As for some other sciences (physics, chemistry, biology, etc.), the answer partly falls in experimental validation. Nevertheless, experiment in computer science is a difficult subject that opens many questions: what an experiment is able to validate? What is a ""good experiments""? How to build an experimental environment that allows for ""good experiments""? etc. In this paper we will provide some hints on this subject and show how some tools can help in performing ""good experiments"". More precisely we will focus on three main experimental methodologies, namely real-scale experiments (with an emphasis on PlanetLab and Grid'5000), Emulation (with an emphasis on Wrekavoc: http://wrekavoc.gforge.inria.fr) and simulation (with an emphasis on SimGRID and Grid-Sim). We will provide a comparison of these tools and methodologies from a quantitative but also qualitative point of view.",1530-2075;15302075,POD:978-1-4244-1693-6,10.1109/IPDPS.2008.4536210,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4536210,,Algorithm design and analysis;Biological system modeling;Biology computing;Chemistry;Computer science;Emulation;Grid computing;Information analysis;Operating systems;Physics,grid computing;program verification;software architecture,Grid'5000;PlanetLab;complex architectures;experimental validation;grid algorithms,,0,,19,,no,14-18 April 2008,,IEEE,IEEE Conference Publications
Extending Always Best Connected Paradigm for Voice Communications in Next Generation Wireless Network,T. Y. Chung; F. C. Yuan; Y. M. Chen; B. J. Liu; C. C. Hsu,"Yuan Ze Univ., Taoyuan",22nd International Conference on Advanced Information Networking and Applications (aina 2008),20080403,2008,,,803,810,"Selecting transparently a proper network connection for voice communication will be a fundamental requirement in future multi-mode heterogeneous wireless network. This paper extends always best connected (ABC) to a fine-grain paradigm called always best network connection (ABNC) to address this issue. Instead of selecting a best access network as in conventional ABC, ABNC enable users to select a best network connection, which consists of source and destination access network pair, to satisfy quality constraint and users' preference. To support ABNC, we develop a user profile to specify network connection priority. Meanwhile we extend SIP and propose a network selection information service (NSIS) based on MIH (media independent handover) to collect information of both source and destination access networks for decision making. Finally, analytic hierarchy process (AHP) is used to recommend a network connection with assistance of user profile and NSIS. An example is illustrated to show that AHP can successfully select a good network connection that fulfill the requirement of users.",1550-445X;1550445X,POD:978-0-7695-3095-6,10.1109/AINA.2008.22,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4482788,802.21;ABNC;AHP;Always Best Connected;MIH;Multiple Criteria Decision Making;network selection,Access protocols;Application software;Computer science;Costs;Decision making;Mobile communication;Next generation networking;Switches;Telephone sets;Wireless networks,decision making;subscriber loops;voice communication;wireless channels,access network;always best connected;always best network connection;analytic hierarchy process;decision making;fine-grain paradigm;media independent handover;multimode heterogeneous wireless network;network selection information service;next generation wireless network;voice communications,,2,,15,,no,25-28 March 2008,,IEEE,IEEE Conference Publications
Finite element analysis for local stability of thin-walled box section,Ji Aimin; Liu Yongming; Shen Libin; Huang Quansheng,"College of Mechanical & Electrical Engineering, Hohai University, Changzhou, Jiangsu Province, China",2008 Asia Simulation Conference - 7th International Conference on System Simulation and Scientific Computing,20081117,2008,,,451,455,"In view of the problem how to calculate the local stability on the structure of thin-walled box section, the finite element analysis was carried out with ANSYS package as tool in this paper. Considering the cross-section as a whole, a parametric finite element model and the parametric optimization model for the local stability calculation of the box section were established. Taking a rectangular cross-section as an example, its critical stress was calculated, and the result coincides with that from the analytic method. The increase degree in the critical stress that the rectangular cross-section is transformed into the hexagonal cross-section was obtained by another calculation. The local stability of a dodecagonal cross-section structure was also analyzed. The method given by this paper can applied to the calculation of the local stability on the many sorts of cross-section shape, which has practical reference for the design of the thin-walled box section structure.",,CD-ROM:978-1-4244-1787-2; POD:978-1-4244-1786-5,10.1109/ASC-ICSC.2008.4675403,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4675403,ANSYS software;Critical stress;Finite element method;Local stability;Thin-walled box section,Bars;Cranes;Educational institutions;Eigenvalues and eigenfunctions;Finite element methods;Packaging;Shape;Stability analysis;Stress;Thin wall structures,finite element analysis;mathematics computing;stability;stress-strain relations;structural engineering computing;thin wall structures,ANSYS package;dodecagonal cross-section structure;hexagonal cross-section structure;parametric finite element analysis;parametric optimization model;rectangular cross-section structure stress;thin plate structure;thin-walled box section structure stability design,,0,,13,,no,10-12 Oct. 2008,,IEEE,IEEE Conference Publications
Global Exponential Stability of Fuzzy Cellular Neural Networks with Mixed Delays,R. Wu; L. Chen,"Sch. of Math., Anhui Univ., Hefei",2008 International Conference on Computer Science and Software Engineering,20081222,2008,4,,867,870,"In this paper, a class of fuzzy cellular neural networks with mixed delays is studied. By using the fixed point theorem, M-matrix theory and some analytic techniques, sufficient conditions for the existence and global exponential stability of the unique equilibrium point are obtained. For illustration, an example is given to show the effectiveness of the obtained results.",,POD:978-0-7695-3336-0,10.1109/CSSE.2008.707,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722756,,Cellular neural networks;Delay;Feeds;Fuzzy logic;Fuzzy neural networks;Mathematics;Neurofeedback;Stability analysis;State feedback;Sufficient conditions,asymptotic stability;cellular neural nets;delays;fuzzy neural nets;matrix algebra,M-matrix theory;fixed point theorem;fuzzy cellular neural network;global exponential stability;mixed delay,,0,,12,,no,12-14 Dec. 2008,,IEEE,IEEE Conference Publications
Hermitian and Skew-Hermitian splitting methods for streamline upwind Petrov-Galerkin approximations of a grid-aligned flow problem,M. S. Sunhaloo; J. Narsoo; A. Gopaul; M. Bhuruth,"Department of Applied Mathematical Sciences, University of Technology, Mauritius",2008 International Conference on Innovations in Information Technology,20090210,2008,,,29,33,"In this paper, we study the convergence of two-step iterative methods based on Hermitian and skew-Hermitian splitting of the coefficient matrix for solving the linear systems obtained from the bilinear finite element discretisation of a model two-dimensional convection-diffusion problem. Analytic expressions for the optimal convergence factors are derived. The inexact and preconditioned versions of the methods have been analyzed via an extensive set of computational experiments.",,CD-ROM:978-1-4244-3397-1; POD:978-1-4244-3396-4,10.1109/INNOVATIONS.2008.4781713,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4781713,convection-diffusion problem;convergence factor;two-step iterative method,Boundary conditions;Convergence;Electronic mail;Finite element methods;Informatics;Iterative methods;Linear systems;Mathematical model;Mathematics;Symmetric matrices,Galerkin method;convection;diffusion;finite element analysis;iterative methods,Skew-Hermitian splitting methods;bilinear finite element discretisation;grid-aligned flow problem;linear systems;streamline upwind Petrov-Galerkin approximations;two-dimensional convection-diffusion problem;two-step iterative methods,,0,,12,,no,16-18 Dec. 2008,,IEEE,IEEE Conference Publications
How to tackle security issues in large existing/legacy systems while maintaining development priorities,D. Campara; N. Mansourov,"CEO, KDM Analytics, 3730 Richmond Rd, Ottawa, ON. Ph: (613) 867-7918, Fax: (646) 421-2091, djenana@kdmanalytics.com",2008 IEEE Conference on Technologies for Homeland Security,20080530,2008,,,167,172,"Legacy software systems represent a large base of software assets that hold significant corporate intellectual properties, along with carrying large opportunity costs and operational risks. There is a growing need to prolong their lifespan through maintenance efforts and enhance them to accommodate new and changing market requirements and governmental regulations. The majority of these systems were developed at a time when security requirements were more relaxed and not well understood, and at a time when being netted did not have the same consequences as exist today. There is a real need for retrofitting security into legacy software systems so they can operate in the current environments. However, over time, as legacy systems became larger and more complex, their design structure eroded which hinders system comprehension, compromises architectural integrity and decreases maintenance productivity. This makes the task of retrofitting security difficult and risky. Since legacy systems are a large part of our nations' critical infrastructure we must retrofit-in security in such way that the level of confidence related to security is substantially increased. This paper will discuss a standards based approach to achieving this goal.",,CD-ROM:978-1-4244-1978-4; POD:978-1-4244-1977-7,10.1109/THS.2008.4534443,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4534443,,Computer architecture;Costs;Information security;Intellectual property;Risk analysis;Risk management;Software maintenance;Software standards;Software systems;Software tools,industrial property;risk management;software development management;software maintenance,corporate intellectual properties;legacy software systems;operational risks;retrofitting security;software assets,,0,,5,,no,12-13 May 2008,,IEEE,IEEE Conference Publications
IEEE Visualization and Graphics Technical Committee (VGTC),,,2008 IEEE Symposium on Visual Analytics Science and Technology,20081118,2008,,,viii,viii,Provides a listing of current committee members.,,POD:978-1-4244-2935-6,10.1109/VAST.2008.4677342,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4677342,,Application software;Augmented reality;Computer graphics;Computer science;Conferences;Data visualization;Ray tracing;Technical Activities Board;Technical activities;Virtual reality,,,,0,,,,no,19-24 Oct. 2008,,IEEE,IEEE Conference Publications
iFAO: Spatial Decision Support Services for Facility Network Transformation,W. Yin; X. Bai; M. Zhu; M. Xie; J. Dong,"IBM China Res. Lab., Beijing",2008 IEEE International Conference on Services Computing,20080725,2008,2,,451,458,"Facility network transformation (FNT) is a strategic approach involving assessing and optimizing the industrial facility networks such as new site selection, demand forecasting, performance evaluation in banking, retailing, etc. In practice, FNT requirements are often diverse, dynamic and industry specific, it's often difficult to implement a generic FNT service fully integrated with legacy systems. The heterogeneity of spatial information further calls for a loosely coupled architecture. An innovative spatial decision support system, iFAO (intelligent facility network analytics and optimization), is therefore developed based on service oriented architecture for FNT problems. In this paper, key FNT service patterns are identified and modeled to develop an industrial independent solution, and an SOA-based framework for iFAO is proposed correspondingly. Implementation of iFAO services is presented with a model-driven approach. With a real case in banking, it's illustrated how the SOA based iFAO services are integrated to solve the real industrial problems, especially for quick decisions on business strategy in the competitive and ever-changing marketplaces.",,POD:978-0-7695-3283-7,10.1109/SCC.2008.42,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4578554,,Banking;Computer networks;Decision support systems;Demand forecasting;Geographic Information Systems;Industrial plants;Intelligent systems;Laboratories;Resource management;Service oriented architecture,Web services;bank data processing;decision support systems;software architecture,business strategy;demand forecasting;facility network transformation;industrial facility networks;industrial independent solution;intelligent facility network analytics and optimization;model-driven approach;performance evaluation;service oriented architecture;site selection;spatial decision support services,,3,,24,,no,7-11 July 2008,,IEEE,IEEE Conference Publications
Implementing a quantitative-based methodology for project risk assessment DSS,Guo Jianyi; Zhang Li; Liu Xusheng; Huang Yuejuan; Yu Zhengtao,"Department of Information Engineering and Automation, Kunming University of Science and Technology, Yunnan 650051, China",2008 27th Chinese Control Conference,20080822,2008,,,730,734,"Project risk assessment is a critical activity adopted in project risk management process to prevent risks and to enhance the success rate of projects. But so far it is a big challenge for project managers and experts to combine their expertise with intelligent technology to evaluate project risks due to insufficient risk related data. Based on this, a novel attempt to integrate analytic hierarchy process (AHP) and support vector regression (SVR) is proposed to build the assessment decision model. A prototype system called project risk assessment decision support system (PRADSS) which consists of risk index system, AHP evaluation model, intelligent evaluation system, knowledge base and man-machine interaction is brought forward to reduce negative risk factors and assist in decision making. Software project risk assessment is conducted to show the efficiency and feasibility of the prototype system.",1934-1768;19341768,CD-ROM:978-7-900719-70-6,10.1109/CHICC.2008.4605452,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4605452,Decision support system;Project risk assessment;Quantitative prediction;Support vector regression,Automation;Decision making;Decision support systems;Electronic mail;Humans;Project management;Risk analysis;Risk management;Software prototyping;Software quality,decision making;decision support systems;production engineering computing;project management;regression analysis;risk management;support vector machines,analytic hierarchy process;assessment decision model;intelligent evaluation system;intelligent technology;knowledge base;man-machine interaction;project risk assessment decision support system;quantitative-based methodology;risk index system;support vector regression,,1,1,13,,no,16-18 July 2008,,IEEE,IEEE Conference Publications
Improving maritime anomaly detection and situation awareness through interactive visualization,M. Riveiro; G. Falkman; T. Ziemke,"School of Humanities and Informatics, University of Sk&#246;vde, 541 28, Sweden",2008 11th International Conference on Information Fusion,20080926,2008,,,1,8,"Surveillance of large land, air or sea areas with a multitude of sensor and sensor types typically generates huge amounts of data. Human operators trying to establish individual or collective maritime situation awareness are often overloaded by this information. In order to help them cope with this information overload, we have developed a combined methodology of data visualization, interaction and mining techniques that allows filtering out anomalous vessels, by building a model over normal behavior from which the user can detect deviations. The methodology includes a set of interactive visual representations that support the insertion of the userpsilas knowledge and experience in the creation, validation and continuous update of the normal model. Additionally, this paper presents a software prototype that implements the suggested methodology.",,POD:978-3-8007-3092-6,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4632191,anomaly detection;interaction;situation awareness;visual analytics;visual data mining;visualization,Buildings;Data mining;Data models;Data visualization;Humans;Training data;Visualization,data visualisation;graphical user interfaces;interactive systems;marine systems;surveillance,interactive visual representations;interactive visualization;maritime anomaly detection;situation awareness;surveillance,,4,,39,,no,June 30 2008-July 3 2008,,IEEE,IEEE Conference Publications
Index Weight Technology in Threat Evaluation Based on Improved Grey Theory,Y. Li; K. Wang,"Dept. of Comput. & Inf. Engineeringr, Heilongjiang Inst. of Sci. & Technol., Harbin",2008 International Symposium on Intelligent Information Technology Application Workshops,20081230,2008,,,307,310,"A scientific evaluation of network's threat degree is an important part of network risk assessment. It's main basis to perform active defense for supervisory systems. And itpsilas an important function of network risk evaluation system. It's necessary to create a credible and general evaluating index model in network threat assessment. Many indices affect the evaluation of network threat and generally they also affect each other. Thus it can not reflect the real situation of network threat with index weights determined by single method. According to the network indicespsila effect, this paper proposed an algorithm which combines grey correlation degree in grey theory with traditional analytic hierarchy process (AHP) to determine the indices. It considered both objective and subjective factors. Experiments showed that this method had a strict mathematical theory basis, a definite meaning and a high practical value. It improved the evaluation methods' credibility of network threat assessment.",,POD:978-0-7695-3505-0,10.1109/IITA.Workshops.2008.104,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4731939,AHP;Grey relation analysis;Threat evaluation,Algorithm design and analysis;Application software;Computer networks;Computer science;Educational institutions;Entropy;Information technology;Intelligent networks;Principal component analysis;Risk management,computer network reliability;grey systems;performance evaluation;telecommunication security,analytic hierarchy process;grey correlation degree;improved grey theory;index model;index weight technology;mathematical theory basis;network risk assessment;network risk evaluation system;network threat assessment;network threat degree;scientific evaluation;supervisory system;threat evaluation,,0,,10,,no,21-22 Dec. 2008,,IEEE,IEEE Conference Publications
Information as a Service in a Data Analytics Scenario - A Case Study,V. Dwivedi; N. Kulkarni,"SETLabs, Infosys Technol. Ltd., Bangalore",2008 IEEE International Conference on Web Services,20081111,2008,,,615,620,"In this work we present a case study of a SOA realization exercise at a business information provider firm, which deals with disparate sources of data in-order to provide reliable reports to its clients. Unlike typical enterprise scenarios, where applications are required to be service enabled, the key requirement here was to service enable its data acquisition, quality check, reporting and other processes which are either mostly manual or ETL based workflows. This paper also addresses how shared services, business processes, rules, and semantics are used to provide quality and agility to the internal processes many of which are entirely dependent on the type of data received. The case and the scenario are chosen specifically to emphasize the fact, that mere web-services implementation does not lead to service oriented architecture, but it is the appropriate usage of them.",,CD-ROM:978-0-7695-3310-0,10.1109/ICWS.2008.119,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4670228,ETL;Extract Tranform Load;SOA Adoption;Workflow,Data acquisition;Data analysis;Data mining;Feeds;Globalization;Information analysis;Service oriented architecture;Technology management;Web services;XML,Web services;data acquisition;software architecture,ETL based workflows;SOA realization;Web-services;business information provider firm;business processes;data acquisition;data analytics;quality check;reporting;service oriented architecture;shared services,,2,,8,,no,23-26 Sept. 2008,,IEEE,IEEE Conference Publications
Information Security Risk Assessment Method Based on CORAS Frame,Q. Yong; X. Long; L. Qianmu,"Sch. of Comput. Applic., Nanjing Univ. of Sci. & Technol., Nanjing",2008 International Conference on Computer Science and Software Engineering,20081222,2008,3,,571,574,"This paper first carry out the summary to the information security risk assessment's present situation and the correlation criterion, then introduced in detail to the risk which possibly exists carry out the quantification based on the CORAS frame's information security risk assessment method and using the analytic hierarchy process, finally uses on-line Electronic bank system's example, proved this method to be possible very well suitable in the information security risk assessment.",,POD:978-0-7695-3336-0,10.1109/CSSE.2008.1001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722408,CORAS;Information security;risk assessment,Appraisal;Information analysis;Information security;Libraries;Mechanical factors;Packaging;Risk analysis;Risk management;Terminology;Unified modeling language,banking;decision making;risk management;security of data,CORAS frame;Information security risk assessment method;analytic hierarchy process;correlation criterion;online electronic bank system,,1,,6,,no,12-14 Dec. 2008,,IEEE,IEEE Conference Publications
Integrated Evaluation Model for Software Process Modeling Methods,R. Zhang; B. Zhang,"Coll. of Comput. Sci. & Technol., Harbin Eng. Univ., Harbin",2008 International Conference on Internet Computing in Science and Engineering,20080624,2008,,,355,358,"In order to help developers choose suitable modeling method according to specific modeling environment and requirement for achieving the best modeling effect, it is important to make reasonable assessment for software process modeling methods. An evaluation system for software process modeling methods is presented, and an evaluation model for evaluating modeling methods is established using method that combines uncertain analytic hierarchy process (AHP) with fuzzy integrated evaluation technology. Further, by an example it proves that using the model can evaluate modeling methods soundly, so it has practical value in software project development.",2330-9857;23309857,CD-ROM:978-0-7695-3112-0,10.1109/ICICSE.2008.59,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4548290,,Computer science;Economics;Educational institutions;Fuzzy systems;Human factors;Internet;Mathematical model;Predictive models;Software quality;Software systems,fuzzy set theory;project management;software management;software performance evaluation,analytic hierarchy process;fuzzy integrated evaluation technology;software process modeling method;software project development,,0,,7,,no,28-29 Jan. 2008,,IEEE,IEEE Conference Publications
Intelligence Database Creation and Analysis: Network-Based Text Analysis versus Human Cognition,J. M. Graham; K. M. Carley; D. Cukor,"United States Mil. Acad., West Point",Proceedings of the 41st Annual Hawaii International Conference on System Sciences (HICSS 2008),20080122,2008,,,76,76,"The 9/11 Commission Report and the National Intelligence Reform Act both state that the development of terrorist network database collection processes is an immediate and pressing requirement. This paper is a study and comparison of two complementary approaches to developing a terror network dataset: Automap, a network text analysis (NTA) tool; and Intelligence Analyst coding, a human process. NTA tools are an emerging branch of software that supports the analysis of quantitative characteristics of large-scale textual data as well as the extraction of meaning from texts. Intelligence Analyst coding is the traditional method that requires a human to read and cognitively process each raw field report. In this study, both approaches were applied to the same one hundred eighty-three open source texts on the Al Qaeda organization. Each approach's process, dataset product, and analytics are compared qualitatively and quantitatively. In terms of process, the Automap-assisted system required less manpower and time resources. In terms of dataset product, both approaches identified unique nodes and relationships that the other missed. Lastly, the differences in the datasets significantly impacted threat analytics and potential course of action selection. These results suggest an integrated human-centered automation support approach to intelligence dataset development.",1530-1605;15301605,,10.1109/HICSS.2008.213,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4438779,,Cognition;Data analysis;Data mining;Deductive databases;Humans;Intelligent networks;Large-scale systems;Pressing;Software tools;Text analysis,cognition;deductive databases;government data processing;military computing;terrorism;text analysis,9/11 Commission Report and the National Intelligence Reform Act;Automap;Intelligence Analyst coding;human cognition;intelligence database creation;network text analysis tool;terror network dataset;terrorist network database collection process,,1,,28,,no,7-10 Jan. 2008,,IEEE,IEEE Conference Publications
Intelligent Video Surveillance Networks: Data Protection Challenges,F. Coudert; J. Dumortier,"Int. Center for Law & ICT (ICRI), Katholieke Univ. Leuven, Leuven","2008 Third International Conference on Availability, Reliability and Security",20080523,2008,,,975,981,"Video surveillance techniques are evolving from static and passive cameras documenting events to dynamic and preventive networks. Two trends lead this change: the shift towards wireless IP systems and the emergence of video analytics. The former allows for flexible networks, massive customization whereas the later comes to solve the problem of increase network complexity. This evolution brings however new threats for individual freedoms, challenging in particular the application of data protection safeguards. This paper takes the prototype developed by DYVINE project as example for the next generation of video surveillance networks and analyses its potential threats from a data protection standpoint.",,POD:978-0-7695-3102-1,10.1109/ARES.2008.143,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4529449,behaviour analysis;data protection;image recognition software;interconnection of video surveillance networks;video analytics.;video surveillance,Availability;Cameras;Crisis management;Data security;Humans;Intelligent networks;Monitoring;Protection;Prototypes;Video surveillance,IP networks;data privacy;video surveillance,data protection;intelligent video surveillance networks;network complexity;video analytics;wireless IP systems,,0,,26,,no,4-7 March 2008,,IEEE,IEEE Conference Publications
Introduction to the smoothed particle hydrodynamics method in electromagnetics,G. H. Park; K. Krohne; E. P. Li,"Engineering Software and Applications, Institute of High Performance Computing, Science Park II, Singapore 117528",2008 Asia-Pacific Symposium on Electromagnetic Compatibility and 19th International Zurich Symposium on Electromagnetic Compatibility,20080715,2008,,,582,585,"The smoothed particle hydrodynamics (SPH) method is a mesh-free method that is characterized by an easy adaptability and an ability to track moving particles. In this paper, the fundamentals of SPH and its corrective smoothed particle method (CSPM) are introduced in detail and those two are compared in terms of consistency. The CSPM formulation for electromagnetics in time domain (SPEM) is presented and an investigation of the effect of particle distribution on the accuracy of SPEM is carried out. The results are compared to those of a finite difference time domain solution and analytic one. It is found that SPEM has a high potential in computational electromagnetics, and that consistency restoring is required for irregular particle distributions.",,CD-ROM:978-981-08-0629-3,10.1109/APEMC.2008.4559942,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4559942,,Computational electromagnetics;Erbium;Finite difference methods;Hydrodynamics;Interpolation;Kernel;Numerical simulation;Special issues and sections;Time domain analysis;Transient analysis,computational electromagnetics;finite difference time-domain analysis,computational electromagnetics;corrective smoothed particle method;finite difference time domain solution;mesh-free method;smoothed particle hydrodynamics,,0,,12,,no,19-23 May 2008,,IEEE,IEEE Conference Publications
Keynote address Practical applications of visual analytics: On the cusp of widespread adoption,C. Chabot,"Tableau Software, USA",2008 IEEE Symposium on Visual Analytics Science and Technology,20081118,2008,,,xi,xi,"Summary form only given. As practitioners and educators in the field of Visual Analytics Science and Technology, youpsilave seen the power of visual analytics for scientific and technical applications including Homeland Security. But visual analytics is spreading to the general business population solving unexpected problems and challenges. In this talk, Tableau Software CEO Christian Chabot will highlight the areas of opportunity for visual analytics and demonstrate real examples of practical problems being solved by visual analytics. Hepsilall share his vision for the future of this industry - how everyday people can and are using visual analytics to solve some of businesspsilas and societypsilas most challenging issues. Hepsilall also identify whatpsilas needed to bring visual analytics to the forefront of main-stream data analysis and how the industry is helping to deliver on those needs.",,POD:978-1-4244-2935-6,10.1109/VAST.2008.4677347,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4677347,,,business data processing;data visualisation,Tableau Software;data analysis;homeland security;visual analytics,,1,,,,no,19-24 Oct. 2008,,IEEE,IEEE Conference Publications
Knowledge management performance evaluation based on ANP,Jin-Yu Wei; Ran Bi,"Department of Management, Tianjin University of Technology, 300384, China",2008 International Conference on Machine Learning and Cybernetics,20080905,2008,1,,257,261,"Knowledge management is considered as a critical process in organizations. With the scope of this paper, 4 first-level indices and 16 second-level indices were chosen to construct a knowledge management evaluation index system on the basis of research both domestic and international on knowledge management. A new appraisal method - analytic network process (ANP) has been adopted to model a multi-criteria knowledge management performance evaluation feedback system. Theoretical foundations and application processes of ANP were discussed and dependence and feedback among indices were analyzed. The result of ANP model for the index system was obtained with super decisions software solution. ANP can solve problems with dependent indices effectively rather than analytic hierarchy process (AHP).",2160-133X;2160133X,POD:978-1-4244-2095-7,10.1109/ICMLC.2008.4620414,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4620414,Analytic network process (ANP);Index system;Knowledge management;Performance evaluation,Bismuth;Conference management;Cybernetics;Data envelopment analysis;Feedback;Knowledge management;Machine learning;Performance analysis;Radio access networks;Technology management,knowledge management,ANP;analytic hierarchy process;analytic network process;knowledge management evaluation index system;knowledge management performance evaluation;multi-criteria knowledge management performance evaluation feedback system,,1,,29,,no,12-15 July 2008,,IEEE,IEEE Conference Publications
Large deviations for constrained pattern matching,Yongwook Choi; W. Szpankowski,"Department of Computer Science, Purdue University, W. Lafayette, IN 47907 U.S.A.",2008 IEEE International Symposium on Information Theory,20080808,2008,,,2141,2145,"In the constrained pattern matching one searches for a given pattern in a constrained sequence, which finds applications in communication, magnetic recording, and biology. We concentrate on the so-called (d, k) constrained binary sequences in which any run of zeros must be of length at least d and at most k, where 0 les d Lt k. In our previous paper [2] we established the central limit theorem (CLT) for the number of occurrences of a given pattern in such sequences. Here, we present precise large deviations results, often used in diverse applications. In particular, we apply our results to detect under- and over-represented patterns in neuronal data (spike trains), which satisfy structural constraints that match the framework of (d, k) binary sequences. Among others, we obtain justifiably accurate statistical inferences about their biological properties and functions. Throughout, we use techniques of analytic information theory such as combinatorial calculus, generating functions, and complex asymptotics.",2157-8095;21578095,CD-ROM:978-1-4244-2257-9; POD:978-1-4244-2256-2,10.1109/ISIT.2008.4595368,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4595368,,Application software;Binary sequences;Biological information theory;Computer science;Information analysis;Information theory;Magnetic analysis;Magnetic recording;Neurons;Pattern matching,binary sequences;diversity reception;pattern matching;statistical analysis,analytic information theory;central limit theorem;constrained binary sequences;constrained pattern matching;diverse applications;magnetic recording;precise large deviations;statistical inferences,,0,,16,,no,6-11 July 2008,,IEEE,IEEE Conference Publications
Linking Business Intelligence into Your Business,S. Viaene,"K.U.Leuven, Belgium",IT Professional,20090109,2008,10,6,28,34,IT departments are under pressure to serve their enterprises by professionalizing their business intelligence (BI) operation. Companies can only be effective when their systematic and structured approach to BI is linked into the business itself.,1520-9202;15209202,,10.1109/MITP.2008.128,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4747653,Computing and information systems management;IT as a profession;IT professional;business intelligence,Application software;Bismuth;Costs;Electrical capacitance tomography;Information systems;Investments;Joining processes;Lenses;Spine;Warehousing,business data processing;competitive intelligence;investment,analytics competitors;business intelligence;continuous data feeds;enterprise-class Bl capability;investments;massive data feeds,,4,,6,,no,Nov.-Dec. 2008,,IEEE,IEEE Journals & Magazines
Maintaining interactivity while exploring massive time series,Sye-Min Chan; Ling Xiao; J. Gerth; P. Hanrahan,"Stanford University, USA",2008 IEEE Symposium on Visual Analytics Science and Technology,20081118,2008,,,59,66,"The speed of data retrieval qualitatively affects how analysts visually explore and analyze their data. To ensure smooth interactions in massive time series datasets, one needs to address the challenges of computing <i>ad</i> <i>hoc</i> queries, distributing query load, and hiding system latency. In this paper, we present ATLAS, a visualization tool for temporal data that addresses these issues using a combination of high performance database technology, predictive caching, and level of detail management. We demonstrate ATLAS using commodity hardware on a network traffic dataset of more than a billion records.",,POD:978-1-4244-2935-6,10.1109/VAST.2008.4677357,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4677357,D.2.11 [Software Engineering]: Software Architectures‰ÛÓDomain-specific architectures;H.5.2 [Information Interfaces And Presentation]: User Interface‰ÛÓGraphical user interfaces (GUI);K.4.0 [Information Systems Applications]: General,Costs;Data analysis;Data visualization;Delay;Hardware;Information retrieval;Telecommunication traffic;Time series analysis;Visual analytics;Visual databases,cache storage;data visualisation;query processing;temporal databases;time series;very large databases,ATLAS;ad hoc query;data analysis;data retrieval;distributed query load;hidden system latency;high performance database technology;massive time series dataset;predictive caching;temporal data visualization tool,,5,,29,,no,19-24 Oct. 2008,,IEEE,IEEE Conference Publications
Methodology of the Correct Plate-Making to Keep Consistence of Tone Reproduction,Z. Xuliang; Z. Yuanhong; L. Zhihong,"Sch. of Media & Commun., Shenzhen Polytech., Shenzhen, China",2008 International Conference on Computer Science and Software Engineering,20081222,2008,6,,385,388,"The process of offset plate copy is controlled through the correct reproduction at the highest and the lowest tone. We propose that we should control the whole tone, and we need the correct control method. Based on the offset control strap, this paper proposes a new detected method of plate resolution and analytic method of the correct copy range of plate. Using these methods and through the experiment, we analyze the resolution and the correct copy range of three types of offset plates. By working in the correct copy range, we can guarantee the consistence of tone reproduction of different types of plates. This is the precondition for color consistence in color printing and convenient to adjust the press.",,POD:978-0-7695-3336-0,10.1109/CSSE.2008.870,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4723278,,Computer science;Debugging;Glass;Manufacturing;Materials testing;Printing;Production facilities;Software engineering;Temperature distribution,image colour analysis;image resolution;printing;printing machinery;quality control,color consistence;color printing;offset plate copy;plate-making;resolution;tone reproduction,,0,,5,,no,12-14 Dec. 2008,,IEEE,IEEE Conference Publications
Migrant boat mini challenge award: Analysis summary a geo-temporal analysis of the migrant boat dataset,B. Holland; L. Kuchy; J. Dalton,"SPADAC, Inc., USA",2008 IEEE Symposium on Visual Analytics Science and Technology,20081118,2008,,,,,"The SPADAC team used various visual analytics tools and methods to find geo-temporal patterns of migration from a Caribbean island from 2005-2007. In this paper, we describe the tools and methods used in the analysis. These methods included generating temporal variograms, dendrograms, and proportionally weighted migration maps, using tools such as the R statistical software package and Signature Analysttrade. We found that there is a significant positive space-time correlation with the boat encounters (especially the landings), with a migratory shift further away from the point of departure over time.",,POD:978-1-4244-2935-6,10.1109/VAST.2008.4677394,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4677394,G.1.1 [Numerical Analysis]: Interpolation‰ÛÓInterpolation-Formulas;G.1.2 [Numerical Analysis]: ApproximationNonlinear Approximation;H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval‰ÛÓClustering;I.2.1 [Artificial Intelligence]: Applications and Expert Systems‰ÛÓCartography;geo-temporal analysis;visual analytics,Artificial intelligence;Information retrieval;Numerical analysis;Pattern analysis;Predictive models;Software packages;Visual analytics,cartography;data visualisation;software packages,Caribbean island;dendrograms;geo-temporal analysis;geo-temporal patterns;migrant boat dataset;migrant boat mini challenge award;proportionally weighted migration maps;statistical software package;temporal variograms,,0,,3,,no,19-24 Oct. 2008,,IEEE,IEEE Conference Publications
Migrant boat mini challenge award: Simple and effective integrated display geo-temporal analysis of migrant boats,R. Miklin; T. Lipic; Z. Konyha; M. Beric; W. Freiler; K. Matkovic; D. Gracanin,"Dept. of Telecom.&#191;FER, Univ. of Zagreb, Croatia",2008 IEEE Symposium on Visual Analytics Science and Technology,20081118,2008,,,203,204,"We provide a description of the tools and techniques used in our analysis of the VAST 2008 Challenge dealing with mass movement of persons departing Isla Del Sue.no on boats for the United States during 2005-2007. We used visual analytics to explore migration patterns, characterize the choice and evolution of landing sites, characterize the geographical patterns of interdictions and determine the successful landing rate. Our ComVis tool, in connection with some helper applications and Google Earth, allowed us to explore geo-temporal characteristics of the data set and answer the challenge questions. The ComVis project file captures the visual analysis context and facilitates better collaboration among team members.",,POD:978-1-4244-2935-6,10.1109/VAST.2008.4677387,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4677387,I.3.0 [Computer Graphics]: General‰ÛÓ;I.3.6 [Computer Graphics]: Methodology and Techniques‰ÛÓ(Interaction techniques);J.4.1 [Social and Behavioral Sciences]: Sociology‰ÛÓ;Visual analytics;geo-temporal data,Application software;Collaboration;Computer graphics;Histograms;Telecommunications;Visual analytics,computer graphics;data visualisation,ComVis tool;Google Earth;integrated display geo-temporal analysis;landing sites;mass movement;migrant boat;migration patterns,,0,,3,,no,19-24 Oct. 2008,,IEEE,IEEE Conference Publications
MIS Implementing Approaches Choose Based on Analytic Hierarchy Process,X. Hu; H. Xia,"Sch. of Manage., Huazhong Univ. of Sci. & Technol., Wuhan, China",2008 International Symposium on Computer Science and Computational Technology,20081230,2008,2,,500,504,"Choose of suitable approaches for implementing management information system is a complex problem involves many intangibles that need to be trades off. Based on analytic hierarchy process, this paper structures the problem of choose as a hierarchy. There are four alternatives and four criteria for the decision. The alternatives are end-user development, joint development, outsourcing development and purchasing software packages. The criteria are performance and quality of the system, and costs and benefits of the implementation. Each criterion is broken down into subcriteria to be more comprehensible and measurable. The elements are compared one by one to construct a set of pairwise comparison, and then obtain the local and global priority of each element. The final priorities of alternatives can derive a better decision. The case study demonstrated that an optimal implementing approach of MIS could be effectively selected based on AHP.",,POD:978-0-7695-3498-5,10.1109/ISCSCT.2008.196,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4731673,analytic hierarchy process;development approaches;evaluation;information system,Control systems;Costs;Engineering management;Information systems;Information technology;Management information systems;Outsourcing;Packaging;Software packages;Technology management,management information systems;software engineering,analytic hierarchy process;joint development;management information system;outsourcing development;software packages purchasing;user development,,0,,6,,no,20-22 Dec. 2008,,IEEE,IEEE Conference Publications
Modelling quality of service in IEEE 802.16 networks,G. Iazeolla; P. Kritzinger; P. Pileggi,"Software Engineering and System Performance Modelling Group, University of Roma `Tor Vergata&#191;, Italy","2008 16th International Conference on Software, Telecommunications and Computer Networks",20081107,2008,,,130,134,"While only relatively recently standardized, IEEE 802.16 orWiMAX networks are receiving a great deal of attention in both industry and research. This is so because with the increased emphasis on multimedia data, apart from the general advantage of wireless, 802.16 promises wider bandwidth and QoS as part of the standard. As a back haul network for other networks, in particular the 802.11a/b/g/e or WiFi networks, it is well suited. As for any new technology, there are many open questions of which Transmission Scheduling and Connection Admission Control (CAC) are the most prominent. The standard intentionally makes no statement about either function. Different from other performance models we have seen, we consider an analytical framework which takes into account the close relationship between the CAC algorithms and the Scheduler algorithms and is applicable to each mode of operation and admission control paradigm specified by the standard. The long term objective of this work is to present a hybrid analytic and simulation model, based on the proposed framework, for modelling QoS metrics in 802.16 networks.",,CD-ROM:978-953-290-009-5; POD:978-953-6114-97-9,10.1109/SOFTCOM.2008.4669466,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4669466,WiMAX;call admission;scheduling,Admission control;Algorithm design and analysis;Bandwidth;Base stations;Job shop scheduling;Portable media players;Quality of service;Scheduling algorithm;Traffic control;WiMAX,IEEE standards;WiMax;quality of service;telecommunication congestion control;wireless LAN,IEEE 802.16 networks;WiFi networks;WiMAX networks;connection admission control;quality of service;transmission scheduling,,1,,10,,no,25-27 Sept. 2008,,IEEE,IEEE Conference Publications
"Multi-criteria, context-enabled B2B partner selection",P. S. Tan; E. W. Lee; K. Mous; S. S. G. Lee; A. E. S. Goh,"Planning & Operations Management, Singapore Institute of Manufacturing Technology (SIMTech), Singapore","2008 IEEE International Conference on Systems, Man and Cybernetics",20090407,2008,,,1699,1706,"B2B collaborations today are typically short-term and dynamic, so it is imperative that business processes be quickly and accurately forged. This paper details research that combines both the context-aware (CA) approach and analytic hierarchy process (AHP) technique to select and rank potential partners in context-enabled B2B collaborations. The approach was validated by a case study. A review of context-aware applications and the criteria for the selection of potential partners are also presented.",1062-922X;1062922X,CD-ROM:978-1-4244-2384-2; POD:978-1-4244-2383-5,10.1109/ICSMC.2008.4811533,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4811533,AHP;B2B;SOA;collaboration;context-aware;decision support;services,Aerodynamics;Application software;Collaboration;Collaborative work;Companies;Computer applications;Context modeling;Context-aware services;Pervasive computing;Service oriented architecture,commerce;ubiquitous computing,B2B collaborations;analytic hierarchy process technique;business processes;business-to-business collaborations;context-aware approach;multicriteria context-enabled B2B partner selection,,2,,22,,no,12-15 Oct. 2008,,IEEE,IEEE Conference Publications
Multidimensional visual analysis using cross-filtered views,C. Weaver,"The GeoVISTA Center and Department of Geography, The Pennsylvania State University, USA",2008 IEEE Symposium on Visual Analytics Science and Technology,20081118,2008,,,163,170,"Analysis of multidimensional data often requires careful examination of relationships across dimensions. Coordinated multiple view approaches have become commonplace in visual analysis tools because they directly support expression of complex multidimensional queries using simple interactions. However, generating such tools remains difficult because of the need to map domain-specific data structures and semantics into the idiosyncratic combinations of interdependent data and visual abstractions needed to reveal particular patterns and distributions in cross-dimensional relationships. This paper describes: (1) a method for interactively expressing sequences of multidimensional set queries by cross-filtering data values across pairs of views, and (2) design strategies for constructing coordinated multiple view interfaces for cross-filtered visual analysis of multidimensional data sets. Using examples of cross-filtered visualizations of data from several different domains, we describe how cross-filtering can be modularized and reused across designs, flexibly customized with respect to data types across multiple dimensions, and incorporated into more wide-ranging multiple view designs. The demonstrated analytic utility of these examples suggest that cross-filtering is a suitable design pattern for instantiation in a wide variety of visual analysis tools.",,POD:978-1-4244-2935-6,10.1109/VAST.2008.4677370,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4677370,D.2.2 [Software Engineering]: Design Tools and Techniques‰ÛÓ[User Interfaces];H.2.3 [Information Systems]: Database Management‰ÛÓ[Languages];H.5.2 [Information Systems]: Information Interfaces and Presentation‰ÛÓ[User Interfaces],Data analysis;Data structures;Data visualization;Geography;Management information systems;Multidimensional systems;Pattern analysis;Prototypes;Software engineering;Usability,data analysis;data structures;data visualisation;query processing;visual databases,complex multidimensional set query expression;cross-filtered coordinated multiple view interface;data semantics;data visualization;design strategy;domain-specific data structure mapping;idiosyncratic combination;multidimensional visual data analysis tool;visual abstraction,,8,,27,,no,19-24 Oct. 2008,,IEEE,IEEE Conference Publications
nAble Adaptive Scaffolding Agent å_ Intelligent Support for Novices,J. MacInnes; S. Santosa; N. Kronenfeld; J. McCuaig; W. Wright,"Oculus Info Inc., Toronto, ON",2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology,20090106,2008,3,,365,368,"Scaffolding techniques allow human instructors to support novice learners in critical early stages, and to remove that support as expertise grows. This paper describes nAble, an adaptive scaffolding agent designed to guide new users through the use of an analytic software tool in the 'nSpace Sandbox' for visual sense-making. nAble adapts the interface and instructional content based on user expertise, learning style and subtask. Bayesian Networks and Hidden Markov task models provide the agent reasoning engine. An experiment was conducted in which participants were provided with one of: an adaptive scaffold, an indexed help file or a human guide. Users of the adaptive scaffold outperformed users of the indexed help and more quickly converged with the performance of users with the human guide.",,POD:978-0-7695-3496-1,10.1109/WIIAT.2008.264,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4740799,Adaptive Interfaces;Intelligent agents;Scaffolding,Adaptive systems;Bayesian methods;Cognition;Engines;Hidden Markov models;Humans;Intelligent agent;Software tools;Testing;User interfaces,belief networks;hidden Markov models;inference mechanisms;intelligent tutoring systems;multi-agent systems;user interfaces,Bayesian network;agent reasoning engine;hidden Markov task model;human guide;indexed help file;intelligent support;nAble adaptive scaffolding agent;nSpace Sandbox analytic software tool;novice learning;user interface;visual sense-making,,0,,10,,no,9-12 Dec. 2008,,IEEE,IEEE Conference Publications
Narratives: A visualization to track narrative events as they develop,D. Fisher; A. Hoff; G. Robertson; M. Hurst,"Microsoft Research, USA",2008 IEEE Symposium on Visual Analytics Science and Technology,20081118,2008,,,115,122,"Analyzing unstructured text streams can be challenging. One popular approach is to isolate specific themes in the text, and to visualize the connections between them. Some existing systems, like ThemeRiver, provide a temporal view of changes in themes; other systems, like In-Spire, use clustering techniques to help an analyst identify the themes at a single point in time. Narratives combines both of these techniques; it uses a temporal axis to visualize ways that concepts have changed over time, and introduces several methods to explore how those concepts relate to each other. Narratives is designed to help the user place news stories in their historical and social context by understanding how the major topics associated with them have changed over time. Users can relate articles through time by examining the topical keywords that summarize a specific news event. By tracking the attention to a news article in the form of references in social media (such as weblogs), a user discovers both important events and measures the social relevance of these stories.",,POD:978-1-4244-2935-6,10.1109/VAST.2008.4677364,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4677364,I.3.8 [Computer Graphics]: Applications;I.7.m [Document and Text Processing]: Miscellaneous;blogs;events;time series;topic detection and tracking;trends,Application software;Blogs;Computer graphics;Data mining;Data visualization;Displays;Event detection;Reflection;Text processing;Visual analytics,Web sites;data visualisation;information retrieval;pattern clustering;text analysis;user interfaces,In-Spire system;Narratives interface;ThemeRiver system;Web log;clustering technique;narrative news event tracking;social media;temporal axis;text visualization;topical keyword retrieval;unstructured text stream analysis,,14,1,16,,no,19-24 Oct. 2008,,IEEE,IEEE Conference Publications
Networking concepts comparison for avionics architecture,T. Schuster; D. Verma,"L-3 Communications - Integrated Systems, Greenville, Texas, USA",2008 IEEE/AIAA 27th Digital Avionics Systems Conference,20081209,2008,,,1.D.1-1,1.D.1-11,"The purpose of this paper is to evaluate alternative networking concepts (standards and protocols), with a particular emphasis on comparing ARINC 664 network standard with legacy avionics networks. The conclusions of this comparison are reinforced with an example network solution for the avionics architecture using the Avionics Full- Duplex Switched Ethernet (AFDX) protocol. The networking of modules (hardware and software) and applications on an aircraft is crucial, and often new designs and upgrades rely on legacy network architectures. In the past, such systems in defense applications have been successfully integrated around the 1553B bus architecture, while commercial applications have satisfied FAA requirements with systems integrated around the ARINC 429 bus architecture. The new applications and capabilities being requested by stakeholders for Avionics systems of the future require increased bandwidth and latency requirements that suggest likely inadequacies in legacy bus architectures. There is continuing pressure by pilots for more information displayed on increasingly more intuitive graphical displays and interfaces; while the ground control and logistics teams want additional and more timely airplane status data - this data is consolidated from a multitude of sub-systems and sensors on board, including event logs and trend data. These new demands may require us to leverage new technologies to keep pace with stakeholder expectations today and in the future. A recent advancement in networking technology is ARINC 664, which defines a deterministic version of an Ethernet network. Boeing and Airbus have adopted Avionic Full- Duplex Switched Ethernet for their newer airplanes [1-3], and NASA is considering ARINC 664 for the new Crew Exploration Vehicle [4]. The B787 is slated to accommodate 100 applications in part due to the availability of larger network bandwidth [5]. NASA hopes to benefit from commercial-off-the-shelf (COTS) Ethernet components which includ- - e reduced overall costs, faster system development and less-costly maintenance for the system network. Both found ARINC 664 to be the best fit in ARINC 653 based systems. Integrating an Avionics Full-Duplex Switched Ethernet may benefit the defense industry avionics customer by lowering life cycle cost and accommodating increasing requirements. This paper addresses the speed, reliability, and flexibility of modern network protocols and explores new options for avionics architectures. This applies to either a complete redesign or a phased avionics upgrade in legacy airplanes. For a historical context, this paper also summarizes the value and utility of legacy networking protocols, together with their downsides. Networking standards and protocols are evaluated as a function of critical requirements pertaining to performance, certification, security, reliability, evolvability, cost, and flexibility to meet changing requirements. Methods such as Quality Function Deployment (QFD) and Analytic Hierarchy Process (AHP) are used to evaluate the three network architectures. The impacts on security and reliability are explored, and additional aspects are highlighted for future research.",2155-7195;21557195,CD-ROM:978-1-4244-2208-1; POD:978-1-4244-2207-4,10.1109/DASC.2008.4702761,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4702761,,Aerospace electronics;Airplanes;Application software;Bandwidth;Computer architecture;Costs;Ethernet networks;NASA;Protocols;Space technology,aircraft computers;data communication;local area networks;protocols,AFDX;ARINC 664 network standard;alternative networking;avionics architecture;full-duplex switched Ethernet protocol;legacy avionics networks;networking concepts;networking protocols,,9,2,14,,no,26-30 Oct. 2008,,IEEE,IEEE Conference Publications
New direction in project management success: Base on smart methodology selection,I. Attarzadeh; S. H. Ow,"Department of Software Engineering, Faculty of Computer Science & Information Technology, University of Malaya, 50603 Kuala Lumpur, MALAYSIA",2008 International Symposium on Information Technology,20080926,2008,1,,1,9,"Modern project management is a well-understood discipline that can produce predictable, repeatable results. The methodologies of modern project management are highly analytic, usually requiring automated tools to support them on large projects. Like most other disciplines, it is learned through both practice and study. Project management encompasses many different skills, such as understanding the interdependencies among people, technologies, budgets, and expectations; planning the project to maximize productivity; motivating others to execute the plan; analyzing the actual results; and reworking and tuning the plan to deal with the realities of what really happens as the project is executed. In order to manage a project and bring it to a successful completion, its project manager must have a complete understanding of the methodologies being used for the management of different parts of the project. Managers prefer specific project methodology, while resist and face difficulties for an opportunity to manage another project with different methodology as they don‰Ûªt know how much commonality exists between the preferred and the new required methodology.",2155-8973;21558973,CD-ROM:978-1-4244-2328-6; POD:978-1-4244-2327-9,10.1109/ITSIM.2008.4631556,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4631556,,,,,,0,,11,,no,26-28 Aug. 2008,,IEEE,IEEE Conference Publications
Numerical simulation and experimental study of liquid-solid two-phase flow in nozzle of DIA Jet,Guihua Hu; Wenhua Zhu; Tao Yu; Jin Yuan,"CIMS & Robot Center of Shanghai University, 200072, China",2008 6th IEEE International Conference on Industrial Informatics,20080903,2008,,,1700,1705,The velocity of abrasive particles at the nozzle exit of Direct Injection Abrasive (DIA) Jet is a key factor affecting cutting capacity of jet. The powerful Computational Fluid Dynamics (CFD) analysis software Fluent is applied to numerical simulation of liquid-solid two-phase flow in the hard alloy nozzle of different cylindrical section length under a certain conditions. The optimum ratio of diameter to length is obtained when the particle velocities are the largest at the nozzle exit. The rule of velocity distribution of liquid-solid two-phase flow of the optimum nozzle is analyzed. The numerical control cutting machine tool of DIA Jet is adopted to finish cutting experiments on different variety of materials. The analytic results of experiments verify the results of numerical simulation.,1935-4576;19354576,CD-ROM:978-1-4244-2171-8; POD:978-1-4244-2170-1,10.1109/INDIN.2008.4618377,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4618377,,Abrasives;Acceleration;Computational fluid dynamics;Computer integrated manufacturing;Fluid flow;Kinetic energy;Numerical simulation;Shape;Solids;Water jet cutting,computational fluid dynamics;flow simulation;nozzles;numerical analysis;production engineering computing;two-phase flow;water jet cutting,CFD analysis software;Fluent;abrasive particles;computational fluid dynamics;direct injection abrasive jet;liquid-solid two-phase flow;nozzle exit;numerical simulation,,0,,16,,no,13-16 July 2008,,IEEE,IEEE Conference Publications
Ontology Refactoring,D. A. Ostrowski,"Syst. Analytics & Environ. Sci., Res. & Adv. Eng. Ford Motor Co., Dearborn, MI",2008 IEEE International Conference on Semantic Computing,20080812,2008,,,476,479,"This paper presents a rule based approach to ontology refactoring. Our method supports large scale instance relationships for support of translation to an improved, functionally equivalent design. By generating new ontology versions on-the-fly we can verify potential updates to our requirements. An example of this technique is presented utilizing a subset of the OWL-DL specification through the implementation of the Jena API. Advantages of this approach include rapid prototyping, versioning support, querying and tool development to support the automated engineering of instance data.",,CD-ROM:978-0-7695-3279-0,10.1109/ICSC.2008.38,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4597228,Ontology;Semantic Web;Software Engineering,Data engineering;Design engineering;Employment;Knowledge based systems;Large-scale systems;Logic;Ontologies;Prototypes;Semantic Web;Testing,ontologies (artificial intelligence);software maintenance,Jena API;OWL-DL specification;functionally equivalent design;large scale instance relationship;ontology refactoring;rule based approach,,2,,24,,no,4-7 Aug. 2008,,IEEE,IEEE Conference Publications
Orchestrating caGrid Services in Taverna,W. Tan; R. Madduri; K. Keshav; B. E. Suzek; S. Oster; I. Foster,"Argonne Nat. Lab., Univ. of Chicago, Chicago, IL",2008 IEEE International Conference on Web Services,20081111,2008,,,14,20,"caBIGtrade (the cancer Biomedical Informatics Gridtrade) is an open-source, open-access information network enabling cancer researchers to share tools, data, applications, and technologies. caGrid is the underlying service-based grid software infrastructure for caBIG, integrating distributed data and analytic resources into a virtual collaborative platform for cancer research. Within caGrid, many cancer-related data analysis and aggregation tasks can make use of ""canned"" sets of service invocations, or workflows. As a result, there is a need to orchestrate the invocation of caGrid services through the use of both a workflow language and tooling. In this paper, we first explain why we select Taverna as a candidate for workflow authoring and invocation. We then review the development of Taverna plug-ins in general, and describe how we extend Taverna to use caGrid services. We then detail a real-world example and the lessons learned from our research. Finally we conclude with a summary and a description of potential next steps.",,CD-ROM:978-0-7695-3310-0,10.1109/ICWS.2008.56,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4670154,caBIG;grid;service orchestration;workflow,Application software;Biomedical informatics;Cancer;Collaborative software;Collaborative tools;Collaborative work;Data analysis;Information analysis;Open source software;Software tools,cancer;grid computing;medical information systems;open systems,analytic resources;caBIGtrade;caGrid services;cancer Biomedical Informatics Grid;distributed data;open-source open-access information network;service-based grid software;tooling;workflow language,,1,,23,,no,23-26 Sept. 2008,,IEEE,IEEE Conference Publications
Partner Selection for car industry Logistics Alliance in China,Zhang bixi; Li Jing; Song jing; Liu hongwei,"School of Economics and Management, Guangdong University of Technology, Guangzhou 510520, China",2008 IEEE International Conference on Automation and Logistics,20080930,2008,,,2069,2072,"The objectives to form car logistics alliance are confirmed and the factors affecting the partner selection for Chinese car industry logistics alliance have been analyzed. Considering the interdependence and feedback between the factors, we build an ANP model of partner selection for car industry logistics alliance. The elements priorities of the model are obtained by applying the super decisions system software. The preferable partners can be selected by fuzzy evaluating method. The numeric computation proves that the partner selection for car logistics alliance can be solved efficiently with the method.",2161-8151;21618151,POD:978-1-4244-2502-0,10.1109/ICAL.2008.4636504,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4636504,Analytic Network Process;Partner Selection;car Logistics Alliance,Automation;Conference management;Cost benefit analysis;Feedback;Industrial economics;Logistics;Risk analysis;Risk management;System software;Technology management,automobile industry;fuzzy set theory;logistics;organisational aspects,Chinese car industry logistics alliance;analytic network process;feedback;fuzzy evaluating method;partner selection;super decisions system software,,0,,8,,no,1-3 Sept. 2008,,IEEE,IEEE Conference Publications
Poverty Grade Evaluation Model Based on Multilevel Fuzzy System,E. Hu; Y. Liu,"Sch. of Software, Beijing Jiaotong Univ., Beijing","2008 International Conference on Information Management, Innovation Management and Industrial Engineering",20090106,2008,1,,317,320,"Poverty, a permanent problem in the society, is listed top-class global problem of social development by the United Nations. Grading the needy situation of impoverished families helps the government establish better policies, distribute resources more reasonably, and therefore provide aid more effectively. However, the traditional single-factor mode is not adequate, for poverty grade evaluation involves various factors of different weights, and some factors cannot be analyzed by classical algorithm. To overcome such problems, in this paper we establish a model applying the theories and methods of fuzzy mathematics and comprehensive evaluation. Based on fuzzy inference, we perform evaluations which are both qualitative and quantitative, and include exact and inexact factors. We determine the indexes of poverty grade according to maximum membership degree, and assign their weight using Analytic Hierarchy Process (AHP) -- in this way we quantify the qualitative problems. Finally, we verify our model with instances; the test result indicated that this technologically-advanced model provides a higher reliability to poverty grade evaluation, and is practically applicable.",2155-1456;21551456,POD:978-0-7695-3435-0,10.1109/ICIII.2008.150,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4737553,Analytic Hierarchy Process;Fuzzy comprehensive evaluation;grade evaluation;indicator system;membership degree,Algorithm design and analysis;Fuzzy systems;Government;Industrial engineering;Inference algorithms;Information management;Innovation management;Mathematical model;Mathematics;Performance evaluation,fuzzy set theory;social sciences;statistical analysis,analytic hierarchy process;fuzzy inference;fuzzy mathematics;multilevel fuzzy system;poverty grade evaluation model;social development problem,,0,,5,,no,19-21 Dec. 2008,,IEEE,IEEE Conference Publications
Preliminary Considerations on ADC Standard Harmonization,S. Rapuano,"Sannio Univ., Benevento",IEEE Transactions on Instrumentation and Measurement,20080107,2008,57,2,386,394,"In this paper, an analytic comparison of the dynamic parameters that are used for qualifying analog-to-digital converters (ADCs) in the frequency domain reported in the most diffused standards is provided. This could be the first step toward their harmonization.",0018-9456;00189456,,10.1109/TIM.2007.909426,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4427397,‰ÛÏMethods and draft standards for the DYNamic characterization and testing of Analog to Digital converters‰Ûù (DYNAD);Analog-to-digital converter (ADC);IEC 60748;IEC 62008;IEEE Standard 1057;IEEE Standard 1241;SIgnal-to-Noise And Distortion ratio (SINAD);effective number of bits (ENOB);signal-to-noise ratio (SNR);spurious-free dynamic range (SFDR);total harmonic distortion (THD),Analog-digital conversion;Calibration;Data acquisition;IEC standards;Manufacturing;Measurement standards;Software standards;Standardization;Testing;Total harmonic distortion,analogue-digital conversion;frequency-domain analysis,ADC standard harmonization;analog-digital converter;frequency domain analysis,,8,,10,,no,Feb. 2008,,IEEE,IEEE Journals & Magazines
QoS-Aware Service Selection Using QDG for B2B Collaboration,C. Lv; W. Dou; J. Chen,"Nat. Key Lab. on Novel Software Technol., Nanjing Univ., Nanjing, China",2008 14th IEEE International Conference on Parallel and Distributed Systems,20081222,2008,,,336,343,"Collaboration among enterprises through Web service has become a hot topic. Before the collaboration, how to select the most appropriate enterprise to collaborate with, from a set of enterprise candidates that provide similar functions, is an important issue. Existing work focus on proposing evaluation rules, and aggregating these rules to evaluate a service, where subjectiveness is usually involved. In this paper, we propose to utilize ÌâåÀserve, be servedÌâåÀ relationship to evaluate the quality of services. In more detail, we use quality dependency graph (QDG) method model the relationship among enterprises, and then, by traveling the built QDG, an analytic hierarchy process (AHP) model is used to calculate the evaluation result of each candidate organization. Our method provides a more objective way for collaboration on enterprise level.",1521-9097;15219097,POD:978-0-7695-3434-3,10.1109/ICPADS.2008.10,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724337,AHP;B2B;QoS;Quality Dependency Graph;Service Composition;service dependency,Acceleration;Australia;Chaos;Collaborative software;Collaborative work;Costs;International collaboration;Laboratories;Quality of service;Web services,Web services;business data processing;decision making;graph theory;quality of service,AHP;B2B collaboration;QDG;QoS-aware Web service selection;analytic hierarchy process;quality dependency graph;quality of service,,3,,21,,no,8-10 Dec. 2008,,IEEE,IEEE Conference Publications
Research on the Performance of ITS Information Publishing Measures,Q. Li; B. t. Dong; C. x. Ji; B. He,"Sch. of Traffic & Transp., Beijing Jiaotong Univ., Beijing",2008 International Conference on Intelligent Computation Technology and Automation (ICICTA),20081028,2008,2,,278,281,"This paper explores the performance of traffic information publishing measures (TIPM). By applying the model and theory of the Analytic Network Process (ANP), a comprehensive multiple-attribute evaluation framework was proposed with the consideration of both the relationships of feedback and dependence among the criterion in five dimensions, such as cost, technique, capability, quality and charger. Weights were calculated to emphasize the interdependent relationships by using special software Super Decision and used in evaluating the six main intelligent transportation system (ITS) information publishing measures in Beijing. Finally, the evaluation results were ranked based on the total scores in descending order. It can be used as a guide for TIPMs proprietors to review, improve, and enhance service qualities in the future.",,POD:978-0-7695-3357-5,10.1109/ICICTA.2008.382,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4659767,ANP;ITS;Performance;TIPM,Automation;Costs;Decision making;Displays;Feedback;Information analysis;Intelligent transportation systems;Publishing;Telecommunication traffic;Traffic control,feedback;traffic information systems,Beijing;ITS;analytic network process;comprehensive multiple-attribute evaluation framework;feedback;intelligent transportation system;interdependent relationships;special software super decision;traffic information publishing measures,,0,,12,,no,20-22 Oct. 2008,,IEEE,IEEE Conference Publications
Research on Web services selection model based on AHP,Meiyun Zuo; Shijuan Wang; Bei Wu,"Information School and Key Laboratory of Data Engineering and Knowledge Engineering, MOE Renmin University of China, Beijing, China","2008 IEEE International Conference on Service Operations and Logistics, and Informatics",20081125,2008,2,,2763,2768,"This paper focuses on the problem that how to select the optimal service among many Web services which all meet the functional needs, establishes an index system for Web services products selection from four aspects, namely the supply side, the user, product and environment. Based on this, we collect the views of 30 experts by Analytic Hierarchy Process (AHP) method and calculate the weight of each index at all levels based on the data collected from questionnaire survey. In the overall sample data analysis, we put two types of sample data namely business operation experts and academics for comparative analysis. The Web services selection model proposed in this article can provide the reference to Web services managers when they selecting Web services, and also contributes to in-depth research on the adoption of Web services based information system.",,CD-ROM:978-1-4244-2013-1; POD:978-1-4244-2012-4,10.1109/SOLI.2008.4683004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4683004,analytic hierarchy process;information system;model;selection;web services,,Web services;decision making;information systems;software architecture,AHP method;SOA based information system;Web service index system;Web service product selection model;analytic hierarchy process;service-oriented architecture,,1,,17,,no,12-15 Oct. 2008,,IEEE,IEEE Conference Publications
RFM Value and Grey Relation Based Customer Segmentation Model in the Logistics Market Segmentation,X. Weiwen; C. Liang; Z. Zhiyong; Q. Zhuqiang,"Dept. of Logistics Eng., South China Univ. of Technol., Guangzhou, China",2008 International Conference on Computer Science and Software Engineering,20081222,2008,5,,1298,1301,"In CRM (customer relationship management), the importance of a segmentation method for identifying good customers has been increasing. The paper recalls the development of domestic and foreign markets subdivision process and trends. This study presents a novel approach that combines customer targeting and customer segmentation for marketing strategies. This investigation identifies customer behavior using a recency, frequency and monetary (RFM) model and grey correlation model to evaluate proposed segmented customers. Models have taken into account the customerÌâåÀs value for enterprises and logistics services that the customers are concerned about. The AHP (Analytic Hierarchy Process) algorithm is used to computer the weights of indicators. To demonstrate the efficiency of the proposed method, this work performs an empirical study of a logistics enterprise to segment 10 customers. The result shows the way to segment customer is effective. So it is easy to find high added customers based for enterprises to develop effective marketing strategies.",,POD:978-0-7695-3336-0,10.1109/CSSE.2008.79,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4723147,AHP;RFM value model;customer segmentation;grey relation,Algorithm design and analysis;Business;Computer science;Customer relationship management;Demography;Frequency estimation;Logistics;Marketing and sales;Psychology;Software engineering,customer relationship management;grey systems;logistics,analytic hierarchy process;customer relationship management;customer segmentation;grey relation;logistics enterprise;logistics market segmentation;value relation,,0,,10,,no,12-14 Dec. 2008,,IEEE,IEEE Conference Publications
Robust reconfigurable filter design using analytic variability quantification techniques,A. Nieuwoudt; J. Kawa; Y. Massoud,"Department of Electrical and Computer Engineering, Rice University, Houston, Texas, USA",2008 IEEE/ACM International Conference on Computer-Aided Design,20081118,2008,,,765,770,"In this paper, we develop a variability-aware design methodology for reconfigurable filters used in multi-standard wireless systems. To model the impact of statistical circuit component variations on the predicted manufacturing yield, we implement several different analytic variability quantification techniques based on a double-sided implementation of the first and second order reliability methods (FORM and SORM), which provide several orders of magnitude improvement in computational complexity over statistical sampling methods. Leveraging these efficient analytic variability quantification techniques, we employ an optimization approach using Sequential Quadratic Programming to simultaneously determine the fixed and tunable/switchable circuit element values in an arbitrary-order canonical filter to improve the overall robustness of the filter design when statistical variations are present. The results indicate that reconfigurable filters and impedance matching networks designed using the proposed methodology meet the specified performance requirements with a 26% average absolute yield improvement over circuits designed using deterministic techniques.",1092-3152;10923152,POD:978-1-4244-2819-9,10.1109/ICCAD.2008.4681662,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4681662,,Circuits;Computational complexity;Design methodology;Design optimization;Filters;Predictive models;Quadratic programming;Robustness;Sampling methods;Virtual manufacturing,band-pass filters;circuit reliability;impedance matching;passive filters;software radio,analytic variability quantification techniques;double sided implementation;first order reliability methods;impedance matching networks;reconfigurable filters;robust reconfigurable filter design;second order reliability methods;variability aware design,,4,,26,,no,10-13 Nov. 2008,,IEEE,IEEE Conference Publications
Semi-blind channel estimation schemes based on a cooperative form of the cross relation criterion,C. Mavrokefalidis; K. Berberidis; A. A. Rontogiannis,"Dept. of Computer Engineering and Informatics & CTI-R&D, University of Patras, 26504 Rio, Greece",2008 3rd International Symposium on Wireless Pervasive Computing,20080702,2008,,,651,655,"In this paper, two channel estimation schemes are derived for specific cooperative scenarios. Both schemes are based on the cross-relation criterion that has been extensively studied in the (semi-) blind literature. As shown in the paper, in a cooperative system, the channel estimator can be constructed in a natural way either by fractionally-spaced or symbol-spaced samples. We investigate the performance of these two schemes using semi-analytic arguments accompanied by corresponding experimental results.",,CD-ROM:978-1-4244-1653-0; POD:978-1-4244-1652-3,10.1109/ISWPC.2008.4556290,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556290,,Application software;Channel estimation;Chromium;Cooperative systems;Costs;Informatics;Observatories;Protocols;Relays;Remote sensing,channel estimation;cooperative systems,cooperative system;cross relation criterion;fractionally spaced samples;semi analytic arguments;semi blind channel estimation schemes;symbol spaced samples,,1,,5,,no,7-9 May 2008,,IEEE,IEEE Conference Publications
Service analytics framework for web-delivered services,Chunhua Tian; Rongzeng Cao; Hao Zhang; Feng Li; Wei Ding; B. Ray,"IBM China Research Lab, Beijing 100193, China","2008 IEEE International Conference on Service Operations and Logistics, and Informatics",20081125,2008,1,,635,640,"Web-delivered service is an emerging approach for IT service by leveraging the partnership and Web technology to reduce IT service cost and improve delivery efficiency. To make it more effective, business design is also very important besides technology innovation. This paper proposes a three-tier analytical framework to improve business design based on the centralized information from the platform. At business model design tier, a role-based service ecosystem modeling method is presented to contain the duplication and evolvement of participants' functionalities within the same model. Through the model, we can analyze the impact of business model on the business performance. At service analytics tier, we summarize the available data and categorize the operational analysis by roles. At self-transform tier, several potential functionalities are listed to help end client to do business transformation by themselves. Based on such framework, a case study in mini-ERP services is presented to illustrate how the analytics technology can help in business design of Web-delivered service.",,CD-ROM:978-1-4244-2013-1; POD:978-1-4244-2012-4,10.1109/SOLI.2008.4686475,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4686475,business model;service ecosystem;web-delivered service,Collaboration;Collaborative software;Costs;Data analysis;Data mining;Ecosystems;Game theory;Information analysis;Performance analysis;Technological innovation,Web services;corporate modelling;software architecture,IT service;Web technology;Web-delivered services;business design;mini-ERP services;role-based service ecosystem modeling;service analytics framework,,0,,12,,no,12-15 Oct. 2008,,IEEE,IEEE Conference Publications
Simulation Analysis for the Nonlinear Seismic Response of Seismically Isolated Continuous Bridge,K. Zhang; E. Zhu; Y. Li; K. Gao,"Sch. of Civil Eng., Beijing Jiao Tong Univ., Beijing","2008 International Workshop on Modelling, Simulation and Optimization",20090120,2008,,,364,367,"According to the behavior of seismically isolated continuous bridges, the bidirectional nonlinear characteristics of lead rubber bearing are taken into account by using two orthogonal nonlinear level spring elements. Based on the FEA software, the analysis models of seismically isolated and non-isolated continuous bridges are established. And the nonlinear seismic response for these analysis models is carried out under the function of the reasonably chosen seismic motion. The analytic results indicate that the natural period of seismically isolated bridge can be prolonged to avoid the principal period of ground and the seismic energy of structure can be efficiently consumed by the hysteretic energy dissipation of lead rubber bearing. So the response of bridge structure can be reduced to make sure most of the components work in the elastic phase and the structure can be well protected.",,POD:978-0-7695-3484-8,10.1109/WMSO.2008.56,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4757026,bridge;lead rubber bearing;nonlinear analysis;seismic response;spring elements,Acceleration;Analytical models;Bridges;Civil engineering;Earthquakes;Equations;Isolation technology;Motion analysis;Rubber;Springs,bridges (structures);earthquake engineering;earthquakes;finite element analysis;seismology;springs (mechanical),FEA software;bidirectional nonlinear characteristics;hysteretic energy dissipation;lead rubber bearing;nonlinear seismic response;orthogonal nonlinear level spring elements;seismic energy;seismically isolated continuous bridge,,0,,5,,no,27-28 Dec. 2008,,IEEE,IEEE Conference Publications
SMILE Visualization with Flash Technologies,A. Tungkasthan; P. Poompuang; W. Premchaiswadi,"Grad. Sch. of Inf. Technol. in Bus., Siam Univ., Siam","2008 Ninth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing",20080903,2008,,,551,556,"The principles of decision-analytic decision support, implemented in GeNIe (Graphical Network Interface) and SMILE (Structural Modeling, Inference, and Learning Engine) can be applied in practical decision support systems (DSSs). GeNIe plays the role of a development environment and SMILE plays the role of a reasoning engine. A decision support system based on SMILE can be equipped with a customized user interface. GeNIe's name and its uncommon capitalization originate from the name Graphical Network Interface, given to the original simple interface to SMILE which is a library of functions for graphical probabilistic and decision-theoretic models. GeNIe only runs under one of the most popular computing platform,: the Windows operating systems, which makes it not easily portable. GeNIe is therefore limited in its graphical representation across multiple system platforms. This paper is composed of two parts. The first part discusses a development environment for building graphical decision-theoretic models, an influence diagram, on a website by using an newly developed engine called ""SMILE"". The second part of the paper discusses the visualization of SMILE decision-theoretic models on a website using Flash technologies.",,POD:978-0-7695-3263-9,10.1109/SNPD.2008.77,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4617430,,Decision support systems;Engines;Libraries;Network interfaces;Operating systems;Paper technology;Portable computers;Spread spectrum communication;User interfaces;Visualization,data visualisation;decision support systems;decision theory;inference mechanisms;learning (artificial intelligence),"GeNIe;SMILE visualization;Structural Modeling, Inference, and Learning Engine;Windows operating systems;decision support systems;decision-analytic decision support;decision-theoretic model;development environment;flash technology;graphical network interface;graphical probabilistic model",,1,,13,,no,6-8 Aug. 2008,,IEEE,IEEE Conference Publications
Statistical Machine Learning in Natural Language Understanding: Object Constraint Language Translator for Business Process,Li Zhao; F. Li,"Department of Mechanism, ChangChun University, ChangChun, P.R. China. zhaolcn@126.com",2008 IEEE International Symposium on Knowledge Acquisition and Modeling Workshop,20090403,2008,,,1056,1059,"Natural language is used to represent human thoughts and human actions. Business rules described by natural language are very hard for machine to understand. In order to let machine know the business rules, parts of business process, we need to translate them into a language which machine can understand. Object constraint language is one of those languages. In this paper we present a statistical machine learning method to understand the natural business rules and then translate them into object constraint language. Subsequently a translation algorithm for business process modeling is also provided. A real case, air cargo load planning process is proposed to illustrate the efficiency and effective of the method and the algorithm. The result has shown that this method and algorithm enrich business process modeling technology and enhance the efficiency of software developers in business process modeling.",,CD-ROM:978-1-4244-3531-9; POD:978-1-4244-3530-2,10.1109/KAMW.2008.4810674,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4810674,Business Process;Object Constraint Language;Statistical Machine Translation,Constraint optimization;Humans;Laboratories;Learning systems;Machine learning;Machine learning algorithms;Natural languages;Power system modeling;Programming;Unified modeling language,business process re-engineering;language translation;learning (artificial intelligence);natural languages;statistical analysis,business process modeling technology;natural business rules;natural language understanding;object constraint language translator;statistical machine learning,,0,,14,,no,21-22 Dec. 2008,,IEEE,IEEE Conference Publications
Study of Index Weight in Network Threat Evaluation Based on Improved Grey Theory,J. Si; K. Wang; W. Wang; D. Man; W. Yang,"Inf. Security Res. Center, Harbin Eng. Univ., Harbin",2008 IEEE Pacific-Asia Workshop on Computational Intelligence and Industrial Application,20090120,2008,2,,9,13,"It's necessary to create a credible and general evaluating index model in network threat assessment. According to the network indices' effect, this paper proposed an algorithm which combines grey correlation degree in grey theory with traditional analytic hierarchy process (AHP) to determine the indices. It considered both objective and subjective factors. Experiments showed that this method had a strict mathematical theory basis, a definite meaning and a high practical value. It improved the evaluation methods' credibility of network threat assessment.",,POD:978-0-7695-3490-9,10.1109/PACIIA.2008.134,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4756724,,Algorithm design and analysis;Application software;Computational intelligence;Computer industry;Computer networks;Conferences;Entropy;Independent component analysis;Information security;Principal component analysis,computer networks;grey systems;telecommunication security,analytic hierarchy process;grey theory;index weight;network threat assessment;network threat evaluation,,0,,10,,no,19-20 Dec. 2008,,IEEE,IEEE Conference Publications
Study of the Method of Evaluating Color Digital Image,T. Ma,"Dept. of Printing & Packaging, Wuhan Univ., Wuhan, China",2008 International Conference on Computer Science and Software Engineering,20081222,2008,6,,225,228,"This paper makes a lot of investigation, analysis and research about the image quality control and AHP modeling theory. Based on the theory and principle of color reproduction in color image reproductive process, a index system evaluating the quality of color image reproduction is presented. Referring to the national and industry standards, the relative evaluation standards of various indicators is given. Using the analytic hierarchy process in mathematics, it establishes a mathematics modeling of synthetical evaluation of color image reproductive quality, and designs a set of mathematics methods of synthetical evaluating image processing and color reproduction. Finally,using VB to design panels and programmes, it basically realizes the objective of simulated evaluating color images reproduction quality in computer.",,POD:978-0-7695-3336-0,10.1109/CSSE.2008.1535,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4723237,analytic hierarchy process;color image reproduction;mathematical model;quality evaluation,Computational modeling;Computer simulation;Digital images;Electrical equipment industry;Image analysis;Image color analysis;Image processing;Image quality;Mathematical model;Mathematics,image colour analysis;quality control;statistical analysis,analytic hierarchy process;color digital image;color image reproductive process;color image reproductive quality;color reproduction;image processing;image quality control,,0,,5,,no,12-14 Dec. 2008,,IEEE,IEEE Conference Publications
Study on Selection Model of CPC System Partner in Execution and Education,Z. Zhu; S. Wei,"Huazhong Univ. of Sci. & Technol., China",2008 International Conference on Computer Science and Software Engineering,20081222,2008,5,,545,548,"Collaborative Product Commerce (CPC) is the outcome of the latest development of E-commerce. This paper addresses a CPC partner (in execution and education) selection model by applying AHP (analytic hierarchy process) method and fuzzy system theory. This model is practical, effective and useful for enterprises when they select CPC partners, and it enhances the chance of success in the application of CPC system.",,POD:978-0-7695-3336-0,10.1109/CSSE.2008.300,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722960,AHP;CPC;Comprehensive Evaluation;Fuzzy System and Hierarchy.,Collaboration;Collaborative work;Computer science;Costs;Educational products;Educational technology;Fuzzy systems;Manufacturing;Mass production;Technology management,electronic commerce;groupware,analytic hierarchy process method;collaborative product commerce;e-commerce;fuzzy system theory,,0,,19,,no,12-14 Dec. 2008,,IEEE,IEEE Conference Publications
Teaching digital test with BIST analyzer,A. Jutman; A. Tsertov; A. Tsepurov; I. Aleksejev; R. Ubar; H. D. Wuttke,"Department of Computer Engineering, Tallinn University of Technology, Raja 15, 12618, Estonia",2008 19th EAEEIE Annual Conference,20080829,2008,,,123,128,"This paper describes a new software tool for high quality training/learning in the field of digital microelectronics. Its main purpose is to give insight into reliability and quality assurance technologies based on linear feedback shift registers (LFSR) and other pseudo-random pattern generators (PRPG). Various PRPG types are becoming the mainstream test generation solution used in built-in self-test (BIST) structures. Taking into account complex theoretical concepts behind the microelectronics self-testing (including data coding and compression, cryptography, field theory, linear programming) it is important to effectively educate engineers in this field. The software tool we present in this paper is aimed at facilitating this goal. Unlike other similar systems, this tool facilitates study of various test optimization problems, allows fault coverage analysis for different circuits and with different LFSR parameters. The main didactic aim of the tool is presenting complicated concepts in a comprehensive graphical and analytical way. The multi-platform JAVA runtime environment allows for easy usage of the tool both in the classroom and at home. The BIST Analyzer represents an integrated simulation, training, and research environment that supports both analytic and synthetic way of learning. Due to the above mentioned facts the tool provides a unique training platform to use in courses on electronic testing and design for testability.",,CD-ROM:978-1-4244-2009-4; POD:978-1-4244-2008-7,10.1109/EAEEIE.2008.4610171,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4610171,,Automatic testing;Built-in self-test;Circuit testing;Cryptography;Education;Linear feedback shift registers;Linear programming;Microelectronics;Quality assurance;Software tools,built-in self test;learning systems;shift registers;software tools;training,BIST analyzer;built-in self-test;digital microelectronics;digital test;linear feedback shift registers;pseudo-random pattern generators;software tool,,2,,8,,no,June 29 2008-July 2 2008,,IEEE,IEEE Conference Publications
Teaching Model of Coding Standards Based on Evaluation Index System and Evaluating Platform,Y. Wang; L. Lei; C. Zhao; Z. Huang,"Sch. of Software, Harbin Inst. of Technol., Harbin",2008 International Conference on Computer Science and Software Engineering,20081222,2008,2,,635,638,"In order to increase students' capacity of complying with coding standard and strengthen their competitiveness in software industry, an AHP-based evaluation index system and Web-based evaluating platform were established. A student-centered teaching model was discussed, in which both teachers and evaluating platform support the students. After implementing the preliminary teaching model in one academic year, some problems were found, so that the improved teaching model was proposed.",,POD:978-0-7695-3336-0,10.1109/CSSE.2008.1139,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722131,coding standards;evaluating platform;evaluation index system;international competitiveness;teaching model,Code standards;Computer industry;Computer science;Education;Maintenance engineering;Programming profession;Software engineering;Software maintenance;Software quality;Software standards,Internet;computer aided instruction;teaching,AHP-based evaluation index system;Web-based evaluating platform;analytic hierarchy process;coding standard;student-centered teaching model,,2,,8,,no,12-14 Dec. 2008,,IEEE,IEEE Conference Publications
The Evaluation of B2C E-Commerce Web Sites Based on Fuzzy AHP,J. Fei; L. Yu,"Sch. of Manage., Huazhong Normal Univ., Wuhan, China",2008 International Symposium on Computer Science and Computational Technology,20081230,2008,2,,792,795,"E-commerce is one of the most important developments in Internet application. To be successful in the e-commerce marketplace organizations will need to provide high quality web sites that attract and retain users. Hence, evaluation methods for the effectiveness of e-commerce Web sites are a critical issue in both practice and research. The evaluation process involves human subjectivity and it is a multiple-criteria decision making (MCDM) problem in the presence of many quantitative and qualitative attributes. This paper presents fuzzy analytic hierarchy process model to measure the e-commerce web sitesÌâåÀ performance. The study has investigated three web sites with the proposed method.",,POD:978-0-7695-3498-5,10.1109/ISCSCT.2008.320,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4731740,defuzzication;e-commerce;fuzzy AHP;triangular fuzzy numbers;web sites evaluation,Application software;Computer science;Electronic commerce;Electronic mail;Fuzzy set theory;Fuzzy sets;Humans;Internet;Performance analysis;Technology management,Web sites;decision making;electronic commerce;fuzzy set theory,B2C e-commerce Web sites;Internet application;analytic hierarchy process;fuzzy AHP;multiple-criteria decision making problem,,2,,6,,no,20-22 Dec. 2008,,IEEE,IEEE Conference Publications
The kinematics analysis on single cross universal joint,Guo Yanying; Sun Zhonghui; Sun Zhonghong,"JiLin University/College of Automobile Engineering, Changchun, China",2008 IEEE Vehicle Power and Propulsion Conference,20081118,2008,,,1,3,"A new method of space analytic geometry is presented, by which the kinematics characteristics of single cross universal joint is analyzed. Compared with traditional method such as descriptive geometry, three element complex and multi-body system dynamics, space analytic geometry has characteristics of brachylogy and understandability. It is a more convenient way to analyze the kinematics characteristics of single cross universal joint and suitable for project design. The Matlab software is used for simulation to analyze kinematics characteristics of rotate speed and angular acceleration of cross universal joint driven fork shaft.",1938-8756;19388756,CD-ROM:978-1-4244-1849-7; POD:978-1-4244-1848-0,10.1109/VPPC.2008.4677771,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4677771,Kinematics analysis;Single cross universal joint;Spatial analytic geometry,Acceleration;Educational institutions;Geometry;Kinematics;Mathematical model;Propulsion;Shafts;Space technology;Sun;Vehicle dynamics,couplings;kinematics;shafts,Matlab software;angular acceleration;descriptive geometry;fork shaft;kinematics analysis;multibody system dynamics;rotate speed;single cross universal joint;space analytic geometry;three element complex dynamics,,0,,5,,no,3-5 Sept. 2008,,IEEE,IEEE Conference Publications
The online prediction of the faults for integrated maintenance and reliability,C. Popa; M. Fetoiu; G. Vladut; A. Craciun,"IPA CIFATT, Romania","2008 IEEE International Conference on Automation, Quality and Testing, Robotics",20080805,2008,1,,200,203,"The objective of this paper is to realize analytic studies and applications in oriented engineering, especially in maintenance, reliability and security of the big technical installations, particularly for the nuclear domain. Software applications were developed to permit the online automatic computation of the reliability, maintenance, availability and predictive maintenance parameters. This paper presents a task of an integrated application that accomplishes structural and dynamic analysis computation, geometric simulation, software for monitoring on-line predictive maintenance for the installation meant to tritium elimination, all based on scientific methods.",,CD-ROM:978-1-4244-2577-8; POD:978-1-4244-2576-1,10.1109/AQTR.2008.4588734,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4588734,,Analytical models;Application software;Availability;Computational modeling;Predictive maintenance;Predictive models;Reliability engineering;Security;Software maintenance;Solid modeling,computerised monitoring;digital simulation;fault diagnosis;installation;nuclear engineering computing;preventive maintenance;reliability,dynamic analysis computation;geometric simulation;installation online predictive maintenance monitoring;integrated maintenance-reliability application;nuclear domain;online fault prediction;reliability;software applications;structural analysis computation;tritium elimination,,0,,5,,no,22-25 May 2008,,IEEE,IEEE Conference Publications
The Research on Affecting Factors of E-learning Training Effect,Y. Qin; Q. Zhang,"Sch. of Manage., Wuhan Univ. of Technol., Wuhan",2008 International Conference on Computer Science and Software Engineering,20081222,2008,5,,271,277,"This research found the key affecting factors on e-learning training effect with the use of survey method and analytic hierarchy process (AHP). Through literature research and survey method, we got the factors that affect e-learning training effect from three aspects: organizational factors, E-learning training factors and personal characteristics factors. Then we designed a questionnaire on importance of these factors and took trainees of XX Company as the sample. We analysis the data with AHP software YAAHP0.4.1 and finally got the key factors on E-learning training effects. Managers should pay attention to these factors and control them effectively in order to improve E-learning training effects. So this research can provide guidance and reference value for government department and enterprises in their E-learning training process.",,POD:978-0-7695-3336-0,10.1109/CSSE.2008.141,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722895,E-learning;affecting factors;training effect,Computer networks;Computer science education;Conference management;Electronic learning;Engineering management;Government;Internet;Learning systems;Management training;Technology management,computer based training;decision making;organisational aspects,analytic hierarchy process;e-learning training effect;organizational factor;personal characteristics factor,,1,,5,,no,12-14 Dec. 2008,,IEEE,IEEE Conference Publications
The Research on Building Enterprise Knowledge Management Performance Evaluating Indicator System,Tianyilin,"Zhejiang Educ. Inst., Hangzhou","2008 International Workshop on Modelling, Simulation and Optimization",20090120,2008,,,25,29,"The enterprise's knowledge management performance evaluating indicator system is an important basis for the evaluation of enterprise knowledge management performance. Only if we establish a set of scientific and feasible enterprise knowledge management performance evaluating indicator system, the enterprise's knowledge management level could be reflected, and the gap and shortage could be found and corrected, so that the enterprise could adapt to the complex and changeable circumstances in the age of knowledge-based economy. The thesis proposes the principle for building enterprise knowledge management performance evaluating indicator system. Taking into account of the practical condition of the enterprise, the thesis select the enterprise knowledge management performance evaluating indicator set by using Delphi method. Then the evaluating indicator system hierarchy chart is established by AHP method. The thesis quotes Saaty 1<sub>~</sub>9 scaling procedure and average random coincidence indicator RI value, proceeds the quantifying and weighting calculation for the evaluating indicator, and establishes enterprise knowledge management performance evaluating indicator system model by coincidence test and other integrated methods for judgment matrix. Through the simulated experiment, the scientificalness and feasibility of the enterprise knowledge management performance evaluating indicator system model is proved.",,POD:978-0-7695-3484-8,10.1109/WMSO.2008.21,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4756949,,Books;Computer Society;Computer science;Conference management;Engineering management;Knowledge management;Portals;Publishing;Software engineering;Technology management,knowledge management;matrix algebra;random processes,Delphi method;Saaty scaling procedure;analytic hierarchy process method;average random coincidence indicator;enterprise knowledge management performance evaluating indicator system;judgment matrix;knowledge-based economy,,0,,4,,no,27-28 Dec. 2008,,IEEE,IEEE Conference Publications
The Role of Blackboard-Based Reasoning and Visual Analytics in RESIN's Predictive Analysis,D. Liu; J. Yue; X. Wang; A. Raja; W. Ribarsky,"Dept. of Software & Inf. Syst., Univ. of North Carolina at Charlotte, Charlotte, NC",2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology,20090106,2008,2,,508,511,"Knowledge gathering and investigative tasks in open environments can be very complex because the problem-solving context is constantly evolving, and the data may be incomplete, unreliable and/or conflicting. This paper significantly extends our previous work on a mixed-initiative agent by making it capable of assisting humans in foraging task analysis using AI blackboard-based reasoning, visualizations and a mix-initiative user interface. The agent is equipped with the ability to adapt its processing to available resources, deadlines and its current problem-solving context.",,POD:978-0-7695-3496-1,10.1109/WIIAT.2008.307,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4740675,Blackboard System;Markov Decision Process;Mixed-initiative Agent;Predictive Analysis;Visual Analytics,Artificial intelligence;Data visualization;Decision making;Information analysis;Intelligent agent;Performance analysis;Problem-solving;Resins;User interfaces;Visual analytics,data visualisation;inference mechanisms;information resources;user interfaces,AI blackboard-based reasoning;foraging task analysis;knowledge gathering;knowledge investigative tasks;predictive analysis;problem-solving context;visual analytics,,2,,10,,no,9-12 Dec. 2008,,IEEE,IEEE Conference Publications
The technical risk assessment for electronic commerce,Ze-Hong Zhang; Dong-Mei Zhao,"School of Computer, Beijing Information Science & Technology University, 100101, China",2008 International Conference on Machine Learning and Cybernetics,20080905,2008,3,,1515,1520,"A technical risk assessment method for electronic commerce is proposed. On the basis of the analysis of the technical risk of electronic commerce, the method, which combines fuzzy comprehensive judgment method and AHP method, is applied to evaluate risks by qualitative analysis and calculation. The study of the case shows that the method can be easily used to the technical risk assessment for electronic commerce and the results are in accord with the reality.",2160-133X;2160133X,POD:978-1-4244-2095-7,10.1109/ICMLC.2008.4620646,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4620646,AHP method;Fuzzy comprehensive judgment method;Risk assessment;Technology risk of E-commerce,Computer hacking;Cybernetics;Electronic commerce;Information technology;Internet;Machine learning;Network servers;Risk analysis;Risk management;Software safety,electronic commerce;fuzzy set theory;risk management,AHP method;analytic hierarchy process;electronic commerce;fuzzy comprehensive judgment method;qualitative analysis;technical risk assessment,,1,,5,,no,12-15 July 2008,,IEEE,IEEE Conference Publications
Towards Analysing Information Management Requirements in New Zealand Genetic Services,Y. Gu; J. Warren,"Dept. of Comput. Sci., Univ. of Auckland, Auckland, New Zealand",2008 15th Asia-Pacific Software Engineering Conference,20081222,2008,,,255,262,"The development of genetic services within healthcare systems is a global phenomenon that raises challenges for managing genetic information. This paper describes an ongoing qualitative study to collect stakeholder perspectives of New Zealand (NZ) genetic services concerning genetic information management. We are conducting semi-structured interviews to build an understanding of their experiences, expectations, and concerns. The data analysis takes a general inductive approach with an analytic comparison strategy and evaluation research techniques. This study draws on past social and health science theories, on our experience in the NZ genetics context, and on emerging issues in the domain. The study result will provide deeper insights on how the genetic service system works and where it should go. The project deliverables will include a structured synthesis of stakeholder requirements, NZ genetic information management principles and goals.",1530-1362;15301362,POD:978-0-7695-3446-6,10.1109/APSEC.2008.65,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724555,genetic services;genetic testing;information management requirement;knowledge management,Genetics;Health information management;Information analysis;Information management;Investments;Medical diagnostic imaging;Medical services;Pediatrics;Risk management;Testing,data analysis;genetics;health care;medical information systems,New Zealand genetic service;data analysis;healthcare system;information management requirement;stakeholder perspective,,0,,63,,no,3-5 Dec. 2008,,IEEE,IEEE Conference Publications
Uncertainty Boundaries for Complex Objects in Augmented Reality,J. Chen; B. MacIntyre,"School of Interactive Computing, Georgia Institute of Technology, johnchen@cc.gatech.edu",2008 IEEE Virtual Reality Conference,20080404,2008,,,247,248,"Registration errors between the physical world and computer- generated objects are a central problem in Augmented Reality (AR) systems. Some existing AR systems have demonstrated how to dynamically estimate registration errors based on estimates of spatial errors in the system. Using these error estimates, these systems also demonstrated a number of ways of ameliorating the effects of registration error. One central part of this previous work was the creation and use of error regions around objects; unfortunately, the analytic methods used only created accurate regions for simple convex objects. In this paper, we present a simple and stable algorithm for generating the uncertainty regions for complex objects, including non-convex objects and objects with interior holes. We demonstrate how our approach can be used to create a set of more accurate error-based highlights in the presence of registration error, and also be used as a general highlighting mechanism.",1087-8270;10878270,CD-ROM:978-1-4244-1972-2; POD:978-1-4244-1971-5,10.1109/VR.2008.4480784,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4480784,AR;I.3.3 [Computer Graphics]: Picture/Image Generation - Viewing Algorithms;I.3.6 [Computer Graphics]: Methodology and Techniques - Interaction Techniques;non-convex objects;registration error;spatial uncertainty,Application software;Augmented reality;Computer errors;Computer graphics;Delay;Image generation;Physics computing;Sensor systems;Shape;Uncertainty,augmented reality;uncertainty handling,augmented reality;complex objects;computer-generated objects;registration errors;uncertainty boundaries,,2,,6,,no,8-12 March 2008,,IEEE,IEEE Conference Publications
Using Web 2.0 applications to deliver innovative services on the internet,N. J. Koris; A. P. Hoddinott; Kok-Leong Ong,"Deakin University, Australia",2008 International Conference on Service Systems and Service Management,20080812,2008,,,1,4,"The emergence of Web 2.0 has brought about new Web applications being developed. Represented chiefly by Web applications such as YouTube, MySpace, blogs and Google applications, these community-based technologies are changing the way we use the Internet. One interesting result of these innovations is the extensibility of these applications. For example, YouTubepsilas content can be displayed on other Websites and hence, are popularly dasiaextendedpsila to be displayed on individual blogs and other organization Websites. In this paper, we discussed two applications that were a result of extending Google Earth and Google Maps. These two applications illustrate how new solutions can be quickly built from these extensible applications thus suggesting the future of application development, one that is built upon applications rather than object-oriented components.",2161-1890;21611890,CD-ROM:978-1-4244-1672-1; POD:978-1-4244-1671-4,10.1109/ICSSSM.2008.4598500,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4598500,B2B;Google Earth;Google Maps;Web 2.0;business analytics;data analysis;data visualization,Application software;Blogs;DVD;Earth;MySpace;Spatial databases;Technological innovation;Visual databases;Web and internet services;YouTube,Internet;geographic information systems;object-oriented programming,Google Earth;Google Maps;Internet;Web 2.0 application;community-based technology;innovative service delivery;object-oriented component,,2,,8,,no,June 30 2008-July 2 2008,,IEEE,IEEE Conference Publications
Versatile models of systems using map queueing networks,G. Casale; N. Mi; E. Smirni,"College of William and Mary, Department of Computer Science, Williamsburg, VA, USA",2008 IEEE International Symposium on Parallel and Distributed Processing,20080603,2008,,,1,5,"Analyzing the performance impact of temporal dependent workloads on hardware and software systems is a challenging task that yet must be addressed to enhance performance of real applications. For instance, existing matrix-analytic queueing models can capture temporal dependence only in systems that can be described by one or two queues, but the capacity planning of real multi-tier architectures requires larger models with arbitrary topology. To address the lack of a proper modeling technique for systems subject to temporal dependent workloads, we introduce a class of closed queueing networks where service times can have non-exponential distribution and accurately approximate temporal dependent features such as short or long range dependence. We describe these service processes using Markovian arrival processes (MAPs), which include the popular Markov-modulated Poisson processes (MMPPs) as special cases. Using a linear programming approach, we obtain for MAP closed networks tight upper and lower bounds for arbitrary performance indexes (e.g., throughput, response time, utilization). Numerical experiments indicate that our bounds achieve a mean accuracy error of 2% and promote our modeling approach for the accurate performance analysis of real multi-tier architectures.",1530-2075;15302075,POD:978-1-4244-1693-6,10.1109/IPDPS.2008.4536387,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4536387,,Application software;Capacity planning;Computer architecture;Delay;Hardware;Linear programming;Network topology;Performance analysis;Software systems;Throughput,Markov processes;linear programming;queueing theory,MAP closed network;Markov-modulated Poisson process;Markovian arrival process;linear programming;multitier architecture;queueing network,,0,,7,,no,14-18 April 2008,,IEEE,IEEE Conference Publications
Visual Analysis of a Co-authorship Network and Its Underlying Structure,Q. Ye; B. Wu; B. Wang,"Beijing Key Lab. of Intell. Telecommun. Software & Multimedia, Beijing Univ. of Posts & Telecommun., Beijing",2008 Fifth International Conference on Fuzzy Systems and Knowledge Discovery,20081105,2008,4,,689,693,"An interesting property of network is that the information is not only contained in the entities, but also in the links between them. As the structure of the co-authorship network can greatly influence its function and reflect how the internal information is exchanged. We attempt to get deep insight of the features in a co-authorship network at a university. This is done by the following two steps. First, we will explore the basic statistical properties of the co-authorship network and try to present these properties by different graph drawing techniques. Second, to gain more insight of the co-authorship network, we will use a community detecting algorithm to find the clusters of the network. By filtering the unstable links and detecting the clusters, we find there are many stable links in these clusters and many unstable links take the role of ""bridges""' between these clusters. By labeling these communities with different department names, we can get an overview of the main research fields of the university and their relations.",,POD:978-0-7695-3305-6,10.1109/FSKD.2008.436,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4666472,Complex Networks;Data Mining;Social Network Analysis;Visual Analytics,Clustering algorithms;Data mining;Fuzzy systems;Information analysis;Intelligent networks;Intelligent structures;Labeling;Laboratories;Social network services;Visualization,social sciences computing,co-authorship network;community detecting algorithm;graph drawing techniques;internal information exchange;statistical properties,,2,,23,,no,18-20 Oct. 2008,,IEEE,IEEE Conference Publications
Visual Analytics on the Financial Market: Pixel-based Analysis and Comparison of Long-Term Investments,H. Ziegler; T. Nietzschmann; D. A. Keim,"Univ. of Konstanz, Konstanz",2008 12th International Conference Information Visualisation,20080725,2008,,,287,295,"In this paper, we describe solutions how pixel-based visualization techniques can support the decision making process for investors on the financial market. We especially focus on explorative interactive techniques where analysts try to analyze large amounts of financial data for long-term investments, and show how visualization can effectively support an investor to gain insight into large amounts of financial time series data. After presenting methods for improving the traditional performance/risk computation in order to take user-specific regions of interest into account, we present a novel visualization approach that demonstrates how changes in these regions of interest affect the ranking of assets in a long-term investment strategy.",1550-6037;15506037,POD:978-0-7695-3268-4,10.1109/IV.2008.80,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4577961,Financial Data Analysis;Visual Data Mining,Application software;Data analysis;Data mining;Data visualization;Decision making;Information analysis;Investments;Self organizing feature maps;Time series analysis;Visual analytics,data analysis;data mining;data visualisation;decision making;financial data processing;interactive systems;investment;time series,decision making process;explorative interactive technique;financial market visual analytics;financial time series data analysis;knowledge discovery;long-term investment;pixel-based visualization technique,,8,,20,,no,9-11 July 2008,,IEEE,IEEE Conference Publications
Visualizing Time-Dependent Data in Multivariate Hierarchic Plots - Design and Evaluation of an Economic Application,T. Tekusova; T. Schreck,"Fraunhofer Comput. Graphics Inst., Darmstadt",2008 12th International Conference Information Visualisation,20080725,2008,,,143,150,"For successfully competing in a modern economy, large amounts of hierarchic time-dependent data need to be analyzed. As an example, one could consider the geographic composition of inflation in the European Union, or the revenue by product (sub) categories of a firm in the last month. Analysts wish to interpret the structure of the data not only at a single point in time, but examine the changes in the data categories through time. The analysts may need to consider additional dimensions to composition and time, such as the growth rate or profit rate. To reflect such analytic requirements, we have developed an interactive visualization of multi-dimensional, structured data taking the time dimension into account. The data are displayed in a three dimensional hierarchic circular or column plot. The time dimension of the data is represented by animation. Our system provides interactive tools for the visual data analysis and variable set-up of the data display. For better orientation in the data space, we have enhanced the visualization with smooth transitions between different data selections in case of 3D hierarchic plots. The techniques presented can be applied to various data domains. A user study using European inflation data has shown the usefulness for effective economic analysis.",1550-6037;15506037,POD:978-0-7695-3268-4,10.1109/IV.2008.51,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4577939,Data visualization;applications;economic and financial data;hierarchic data visualization;time-dependent data,Aggregates;Animation;Application software;Computer graphics;Data analysis;Data visualization;Displays;Fuel economy;Information analysis;Visual databases,computer animation;data analysis;data visualisation;financial data processing;inflation (monetary);interactive systems,European inflation data;computer animation;economic analysis;interactive tools;multivariate hierarchic plots;time-dependent data visualization;visual data analysis,,3,1,22,,no,9-11 July 2008,,IEEE,IEEE Conference Publications
Viz-A-Vis: Toward Visualizing Video through Computer Vision,M. Romero; J. Summet; J. Stasko; G. Abowd,Georgia Tech,IEEE Transactions on Visualization and Computer Graphics,20081024,2008,14,6,1261,1268,"In the established procedural model of information visualization, the first operation is to transform raw data into data tables. The transforms typically include abstractions that aggregate and segment relevant data and are usually defined by a human, user or programmer. The theme of this paper is that for video, data transforms should be supported by low level computer vision. High level reasoning still resides in the human analyst, while part of the low level perception is handled by the computer. To illustrate this approach, we present Viz-A-Vis, an overhead video capture and access system for activity analysis in natural settings over variable periods of time. Overhead video provides rich opportunities for long-term behavioral and occupancy analysis, but it poses considerable challenges. We present initial steps addressing two challenges. First, overhead video generates overwhelmingly large volumes of video impractical to analyze manually. Second, automatic video analysis remains an open problem for computer vision.",1077-2626;10772626,,10.1109/TVCG.2008.185,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658139,Index Terms&#8212;Spatiotemporal visualization;image/video analytics;sensor analytics;time series data;video visualization,Computer vision;Visualization,computer vision;data visualisation;video signal processing,Viz-A-Vis video visualization;automatic video analysis;computer vision;data aggregation;data segmentation;data table;information visualization procedural model;raw data transform,"Algorithms;Artificial Intelligence;Computer Graphics;Image Enhancement;Image Interpretation, Computer-Assisted;Software;User-Computer Interface;Video Recording",11,,28,,no,Nov.-Dec. 2008,,IEEE,IEEE Journals & Magazines
A computational method for the determination of attraction regions,W. F. Guerrero-SÌÁnchez; J. F. Guerrero-Castellanos; V. V. Alexandrov,Facultad de Ciencias F&#237;sico-Matem&#225;ticas,"2009 6th International Conference on Electrical Engineering, Computing Science and Automatic Control (CCE)",20100119,2009,,,1,7,"The region of attraction of nonlinear dynamical system can be considered using an analytical R-function that can be written like an infinite series where each term of the series has the homogeneous form of degree n åÀåÀ 2 this function allows to determine and to come near to the region of attraction of a nonlinear system around the point of equilibrium located in the origin. The analytical function and the sequence of this Taylor polynomials are constructed by a recurrence formula using the coefficients of the power series expansion of f at 0. This paper describes a novel computational method using the Software MATHEMATICA for obtaining a solution to this problem, which was proposed by the Russian mathematician, V. I. Zubov. In order to evaluate the method, two examples are treated in which the exact attraction region is found in analytic closed form. Since the construction procedure requires the solution of a linear partial differential equation, there are many cases for which an exact analytic solution is not possible. In some of these cases, however, it is possible to construct an approximate series solution which is always at least as good approximation of the usual quadratic form of Lyapunov functions. The ""trajectory reversing method"" is presented as a powerful numerical technique for low order systems. Then an analytical procedure based on the same topological approach is developed, and a comparison is made with the classical Zubov method.",,Electronic:978-1-4244-4689-6; POD:978-1-4244-4688-9,10.1109/ICEEE.2009.5393394,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5393394,Domain of attraction;Lyapunov function;Trajectory reversing method;Zubov method,Differential equations;Lyapunov method;Nonlinear dynamical systems;Nonlinear systems;Partial differential equations;Polynomials;Stability;Steady-state;US Department of Transportation,Lyapunov methods;nonlinear dynamical systems;partial differential equations;polynomials;series (mathematics),Lyapunov function;MATHEMATICA;Taylor polynomials;Zubov method;analytic closed form;analytical R-function;attraction region;computational method;linear partial differential equation;low order systems;nonlinear dynamical system;nonlinear system;power series expansion;recurrence formula;trajectory reversing method,,0,,15,,no,10-13 Jan. 2009,,IEEE,IEEE Conference Publications
A computer based strategy design for automobile spare part logistics network optimization,G. Yi; L. Xinda; W. Dong,"Software School, Shanghai Jiao tong University, China","2009 International Conference for Internet Technology and Secured Transactions, (ICITST)",20100129,2009,,,1,6,"This paper discusses the problem of the automobile spare part logistics warehouse network strategy, a quite important problem in logistics. On the basis of the integration of quantitative technology and qualitative technology, a mathematic model and algorithm of automobile spare part logistics network optimization are presented. In the quantitative analysis part we propose a novel hybrid approach through crossing over the PSO and GA, called hybrid PSO-GA based algorithm. Among the mixed algorithm, GA is embedded to solve the difficulties of updating the particles in the binary code system; the roulette algorithm is embedded to eliminate worse particles; SA is embed to control convergence of particles. Then in the qualitative part, we propose a selection model along with the AHP methodology that project selection would be easier and more accurate than before. In the end we apply the above approach in an automobile spare part logistics company. Computational simulation is carried out to evaluate the performance of the algorithm and the results show that this approach can indeed find effective solutions for the automobile spare part logistics optimization problem.",,Electronic:978-1-4244-5648-2; POD:978-1-4244-5647-5,10.1109/ICITST.2009.5402534,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5402534,,Algorithm design and analysis;Automobiles;Binary codes;Computer networks;Control systems;Convergence;Design optimization;Logistics;Mathematical model;Mathematics,automobile industry;decision making;genetic algorithms;logistics;particle swarm optimisation,analytic hierarchy processing;automobile spare parts;genetic algorithm;hybrid PSO-GA based algorithm;logistics warehouse network strategy;particle swarm optimization;roulette algorithm,,0,,8,,no,9-12 Nov. 2009,,IEEE,IEEE Conference Publications
A faster parallel algorithm and efficient multithreaded implementations for evaluating betweenness centrality on massive datasets,K. Madduri; D. Ediger; K. Jiang; D. A. Bader; D. Chavarria-Miranda,"Computational Research Division, Lawrence Berkeley National Laboratory, CA, USA",2009 IEEE International Symposium on Parallel & Distributed Processing,20090710,2009,,,1,8,"We present a new lock-free parallel algorithm for computing betweenness centrality of massive complex networks that achieves better spatial locality compared with previous approaches. Betweenness centrality is a key kernel in analyzing the importance of vertices (or edges) in applications ranging from social networks, to power grids, to the influence of jazz musicians, and is also incorporated into the DARPA HPCS SSCA#2, a benchmark extensively used to evaluate the performance of emerging high-performance computing architectures for graph analytics. We design an optimized implementation of betweenness centrality for the massively multithreaded Cray XMT system with the Thread-storm processor. For a small-world network of 268 million vertices and 2.147 billion edges, the 16-processor XMT system achieves a TEPS rate (an algorithmic performance count for the number of edges traversed per second) of 160 million per second, which corresponds to more than a 2times performance improvement over the previous parallel implementation. We demonstrate the applicability of our implementation to analyze massive real-world datasets by computing approximate betweenness centrality for the large IMDb movie-actor network.",1530-2075;15302075,POD:978-1-4244-3751-1,10.1109/IPDPS.2009.5161100,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5161100,,Complex networks;Computer networks;Concurrent computing;Grid computing;High performance computing;Kernel;Parallel algorithms;Performance analysis;Power grids;Social network services,graph theory;multi-threading;parallel algorithms;software architecture;software performance evaluation,XMT system;betweenness centrality;graph analytics;high-performance computing architectures;massive datasets;multithreaded implementations;parallel algorithm;performance evaluation;vertices,,29,,17,,no,23-29 May 2009,,IEEE,IEEE Conference Publications
A framework f or calculating fundamental DVR performance limits,S. H. Russ; R. Nallur,"University of South Alabama, USA",2009 Digest of Technical Papers International Conference on Consumer Electronics,20090529,2009,,,1,2,"This describes an analytic framework that can be used to estimate the performance of any DVR system in terms of both megabits per second and number of simultaneous video streams. The framework also highlights the extent to which the maximum performance is constrained by the disk, by the host hardware, or by the host software.",2158-3994;21583994,POD:978-1-4244-2558-7,10.1109/ICCE.2009.5012365,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5012365,,Acceleration;Aggregates;Bandwidth;Bit rate;Delay estimation;Hardware;Performance analysis;Software performance;Streaming media;Time factors,digital versatile discs;video recording;video streaming,DVR performance estimation;digital video recorder;video stream,,3,,5,,no,10-14 Jan. 2009,,IEEE,IEEE Conference Publications
A framework for calculating fundamental DVR performance limits,S. H. Russ; R. Nallur,"University of South Alabama, Mobile, AL 36688 USA",IEEE Transactions on Consumer Electronics,20090417,2009,55,1,132,138,"New systems have emerged that deliver hundreds of megabits per second of video to the home. This, coupled with deeper penetration of in-home video networking, will create demand for digital video recorder (DVR) performance to scale up dramatically as well. This paper outlines a simple analytic framework that can be used to estimate the performance of any DVR system, using either hard-disk drives or solid-state disk drives, in terms of both megabits per second and number of video streams. The framework also highlights the extent to which the maximum performance is constrained by the disk, by the host hardware, or by the host software.",0098-3063;00983063,,10.1109/TCE.2009.4814425,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4814425,Digital Video Recording;Disk Performance;Hard Disk Drive;Personal Video Recording,Aggregates;Bandwidth;Hard disks;High definition video;Performance analysis;Satellite broadcasting;Solid state circuits;Streaming media;TV;Video recording,disc drives;hard discs;video recording;video streaming,digital video recorder;hard-disk drives;in-home video networking;performance estimation;performance limits;personal video recording;solid-state disk drives,,4,,5,,no,9-Feb,,IEEE,IEEE Journals & Magazines
A Fuzzy AHP and BSC Approach for Evaluating Performance of a Software Company Based on Knowledge Management,Y. Wang; Q. Xia,"Coll. of Economic & Manage., Hebei Univ. of Sci. & Technol., ShiJiaZhuang, China",2009 First International Conference on Information Science and Engineering,20100426,2009,,,2242,2245,"The objective of this study is to construct an balanced scorecard (BSC) based on knowledge management and the fuzzy analytic hierarchy process (FAHP) for evaluating an software companies. The BSC concept is applied to define the hierarchy with four major perspectives (i.e. financial, customer, internal business process, and learning and growth), and performance indicators are selected for each perspective. A fuzzy AHP (FAHP) approach is then proposed in order to determine weight. The results provide guidance to software companies regarding strategies for improving department performance.",2160-1283;21601283,Electronic:978-1-4244-5728-1; POD:978-1-4244-4909-5,10.1109/ICISE.2009.38,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5455616,,Advertising;Communication system control;Companies;Costs;Educational institutions;Knowledge management;Knowledge transfer;Marketing and sales;Software performance;Technology management,DP industry;fuzzy set theory;knowledge management;software houses,balanced scorecard;department performance;fuzzy analytic hierarchy process;knowledge management;performance evaluation;performance indicators;software company,,1,,5,,no,26-28 Dec. 2009,,IEEE,IEEE Conference Publications
A Hardware Accelerated Semi Analytic Approach for Fault Trees with Repairable Components,C. Kara-Zaitri; E. Ever,"Sch. of Eng., Design & Technol., Univ. of Bradford Bradford, Bradford",2009 11th International Conference on Computer Modelling and Simulation,20090526,2009,,,146,151,"Fault tree analysis of complex systems with repairable components can easily be quite complicated and usually requires significant computer time and power despite significant simplifications. Invariably, software-based solutions, particularly those involving Monte Carlo simulation methods, have been used in practice to compute the top event probability. However, these methods require significant computer power and time. In this paper, a hardware-based solution is presented for solving fault trees. The methodology developed uses a new semi analytic approach embedded in a Field Programmable Gate Array (FPGA) using accelerators. Unlike previous attempts, the methodology developed properly handles repairable components in fault trees. Results from a specially written software-based simulation program confirm the accuracy and validate the efficacy of the hardware-oriented approach.",,POD:978-1-4244-3771-9,10.1109/UKSIM.2009.83,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4809753,FPGAs;Fault Trees;Monte Carlo Simulation,Acceleration;Circuit faults;Computational modeling;Electronic design automation and methodology;Fault trees;Field programmable gate arrays;Hardware;Integrated circuit modeling;Monte Carlo methods;Software tools,Monte Carlo methods;digital simulation;fault trees;field programmable gate arrays,Monte Carlo simulation;fault trees;field programmable gate array;hardware accelerated semianalytic approach;software-based simulation program,,0,,16,,no,25-27 March 2009,,IEEE,IEEE Conference Publications
A high performance pair trading application,Jieren Wang; C. Rostoker; A. Wagner,"Department of Mathematics, University of British Columbia, Vancouver, Canada",2009 IEEE International Symposium on Parallel & Distributed Processing,20090710,2009,,,1,8,"This paper describes a high-frequency pair trading strategy that exploits the power of MarketMiner, a high-performance analytics platform that enables a real-time, market-wide search for short-term correlation breakdowns across multiple markets and asset classes. The main theme of this paper is to discuss the computational requirements of model formulation and back-testing, and how a scalable solution built using a modular, MPI-based infrastructure can assist quantitative model and strategy developers by increasing the scale of their experiments or decreasing the time it takes to thoroughly test different parameters. We describe our work to date which is the design of a canonical pair trading algorithm, illustrating how fast and efficient backtesting can be performed using MarketMiner. Preliminary results are given based on a small set of stocks, parameter sets and correlation measures.",1530-2075;15302075,POD:978-1-4244-3751-1,10.1109/IPDPS.2009.5161147,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5161147,,Algorithm design and analysis;Computational modeling;Computer science;Data analysis;Electric breakdown;Mathematical model;Mathematics;Open source software;Performance analysis;Testing,commerce;data mining;market research;marketing data processing,MPI-based infrastructure;MarketMiner;asset classes;backtesting;computational requirements;correlation measures;high frequency pair trading;high performance analytics platform;high performance pair trading;market-wide search;model formulation;real-time search;short-term correlation breakdowns,,2,,21,,no,23-29 May 2009,,IEEE,IEEE Conference Publications
A Highway Express Passenger Transportation Company Evaluation System Based on the Evidential Reasoning Approach,M. Zhou; X. B. Liu; H. Qu; F. Pei,"Sch. of Manage., Hefei Univ. of Technol., Hefei, China",2009 Second International Symposium on Electronic Commerce and Security,20091023,2009,2,,329,332,"The existing researches on highway express passenger transportation (HEPT) have been studied, and an improved evaluation attribute system of the assessment to HEPT company is constructed. Considering the disagreement in group of experts and the majority of uncertainties in the process of assessment to HEPT company, an evidential reasoning (ER) approach based HEPT company assessment model is proposed. In this assessment model, the weight of each attribute is calculated through group analytic hierarchy process (GAHP), and the values of all quantitative and qualitative attributes are aggregated by the ER approach. At last, intelligence decision system (IDS) software is applied for the simulation of assessment to HEPT company, the result of which makes great contribution to the decision making by management department of passenger transportation.",,POD:978-0-7695-3643-9,10.1109/ISECS.2009.58,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5209703,Evidential Reasoning;Group Analytic Hierarchy Process;Highway Express Passenger Transportation;IDS;MADA,Aggregates;Electronic commerce;Erbium;Intelligent transportation systems;Intrusion detection;Quality management;Road transportation;Security;Technology management;Uncertainty,case-based reasoning;group decision support systems;traffic engineering computing;transportation,evidential reasoning approach;group analytic hierarchy process;highway express passenger transportation company evaluation system;intelligence decision system software,,0,,10,,no,22-24 May 2009,,IEEE,IEEE Conference Publications
A meta-analytic review of current CALL research on second language learning,H. w. V. Tang; M. s. Yin; P. j. Lou,"Department of Applied English, Ming Chuan University, 33399, Taiwan",2009 IEEE International Symposium on IT in Medicine & Education,20090915,2009,1,,677,684,"To access the research methods and data analysis procedures employed by research studies investigating the applications of computer-assisted language learning (CALL) to enhance second/foreign language learning, the study made extensive search for scholarly articles published from 2000 to 2008. The contributions of quantitative research on computer-assisted second/foreign language learning were reviewed under the categories of research designs, data analysis techniques, skill areas/language taught and the trends of CALL types investigated. Results of the study showed that frequently used research methods were pre- post-test control group experimental and within subject experimental research designs. Findings of the meta-analytic review also indicated that new data analysis techniques were introduced into the field of computer-assisted second/foreign language learning research. The overall interest in implementing technology in second language learning was reflected by the increasing number of research studies on computer mediated communication (CMC) and Web-based tools. The study concluded that there was obviously still a pressing need among the research community for valid research designs and statistical analyses. It is recommended that to meet professional standards, more inferential statistics be applied to provide generalized and significant research findings.",,POD:978-1-4244-3928-7,10.1109/ITIME.2009.5236335,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5236335,computer mediated communication;meta-analytic review;structural equation model,Application software;Computer applications;Computer mediated communication;Data analysis;Educational institutions;Natural languages;Pressing;Statistical analysis;Virtual reality;Writing,Internet;computer aided instruction;computer mediated communication;data analysis;linguistics,Web-based tools;computer mediated communication;computer-assisted language learning;current CALL research;data analysis techniques;meta-analytic review;second-foreign language learning,,0,,17,,no,14-16 Aug. 2009,,IEEE,IEEE Conference Publications
A model proposal for usability scoring of websites,C. B. Cakir; B. Oztaysi,"Istanbul Technical University, Turkey",2009 International Conference on Computers & Industrial Engineering,20090825,2009,,,1418,1422,"Usability has become a very critical factor for the success of a website. The study focuses on constructing a scoring model of websites that is based on observed usability test data. In the proposed model, the first usability test is designed including 4 important tasks. These tasks include finding information in the website about MMS regulations, calling fees, searching for IMEI numbers, and listing shop addresses. The usability test is followed by multi-attribute decision making model for the purpose of obtaining a usability score of the websites.",,POD:978-1-4244-4135-8,10.1109/ICCIE.2009.5223495,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5223495,Analytic Hierarchy Process;Scoring;Usability;Utility Function,Acceleration;Banking;Decision making;Fuzzy set theory;GSM;Internet;Proposals;Testing;Usability;Web sites,Web sites;decision making;software reusability;task analysis,Web sites;multiattribute decision making;usability scoring;usability test data,,0,,21,,no,6-9 July 2009,,IEEE,IEEE Conference Publications
A Multiple Criteria Decision Making Model for CNO Attack Scheme Evaluation,D. Wen; Y. Ji; X. Li; H. Wang,"Sch. of Econ. & Manage., Beihang Univ., Beijing, China",2009 International Conference on Computational Intelligence and Software Engineering,20091228,2009,,,1,7,"Computer network operation (CNO) decision making activity includes three sub-activities: mission definition, scheme design and scheme evaluation. Scheme design produces many attack schemes of the same target. During scheme evaluation, attack schemes are evaluated and the comparatively optimal one is selected as course of action (COA) based on certain criterions. In this study, a multiple criteria decision making model is proposed to evaluate CNO attack schemes. Attack effects, costs and risks of attack tasks and attack paths are considered and evaluated to select an optimal scheme. As a multiple criteria decision making method that combines both qualitative and quantitative analysis, the analytic hierarchy process (AHP) is introduced and applied in the model. An experiment is conducted to illustrate the availability and effectiveness of the decision making model.",,CD-ROM:978-1-4244-4507-3,10.1109/CISE.2009.5363559,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5363559,,Algorithm design and analysis;Computer networks;Computer science;Cost function;Decision making;Electronic mail;Engineering management,decision making;security of data,analytic hierarchy process;attack costs;attack paths;attack risks;attack tasks;computer network operation attack scheme evaluation;course of action;multiple criteria decision making model;should attack effects,,0,,11,,no,11-13 Dec. 2009,,IEEE,IEEE Conference Publications
A Multiple-Criteria Approach to Ranking Computer Operating Systems,R. R. Levary,Saint Louis University,IT Professional,20090724,2009,11,4,17,23,"To select an operating system, an organization must consider several essential characteristics during its initial evaluation process. The analytic hierarchy process (AHP) offers an appropriate solution; the author illustrates it with a realistic case study in which an organization evaluates and ranks Windows XP, Linux, and Mac OS X 10.4.",1520-9202;15209202,,10.1109/MITP.2009.67,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5173034,IT professional;analytic hierarchy process (AHP);data center design;multiple criteria decision making;operating systems,Application software;Computer applications;Computer peripherals;Computer security;Costs;Decision making;Human computer interaction;Operating systems;Software performance;Stability,Linux;computer evaluation;decision making;operating systems (computers),Linux;Mac OS X 10.4;Windows XP;analytic hierarchy process;computer operating systems;multiplecriteria approach,,1,,8,,no,July-Aug. 2009,,IEEE,IEEE Journals & Magazines
A new method of performance evaluation for search engine,C. Jing; G. Yanfen; N. Zhengang,"School of Information & Electronic Engineering, Hebei University of Engineering, Handan, China","2009 ISECS International Colloquium on Computing, Communication, Control, and Management",20090929,2009,2,,180,183,"In this paper, a user-oriented hierarchical analysis model is constructed so as to discuss briefly about the evaluation of search engine. According to the property that the ratio of corresponding elements is a constant in any two rows of consistency matrix, the indirect judgment information of the judgment matrix is comprehensively used to construct a constrained programming model in order to determine the ranking weight of judgment matrix. And the Genetic Algorithm is used to solve the model. Finally, an example is given to demonstrate this method. The results showed that weight dispersion got by this method is higher, and it is also much easier to identify the optimal plan.",2154-9613;21549613,POD:978-1-4244-4247-8,10.1109/CCCM.2009.5267949,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5267949,analytic hierarchy process;genetic algorithm;judgment matrix;performance evaluation;search engine,Algorithm design and analysis;Communication system control;Decoding;Engineering management;Genetic algorithms;Genetic mutations;Information analysis;Performance analysis;Quality management;Search engines,constraint handling;genetic algorithms;search engines;software performance evaluation,consistency matrix;constrained programming model;genetic algorithm;indirect judgment information;judgment matrix;performance evaluation;search engine;user-oriented hierarchical analysis model,,0,,12,,no,8-9 Aug. 2009,,IEEE,IEEE Conference Publications
A Novel Blind Recognition Algorithm for Modulated M-QAM Signals,X. Zhinan; B. Wenle,"Dept. of Inf. Eng., North China Univ. of Technol., Beijing",2009 WRI International Conference on Communications and Mobile Computing,20090304,2009,1,,461,465,"In this paper, we develop an algorithm using Hilbert transform for the blind recognition of QAM signals. Without requiring any prior knowledge of signal parameters, the proposed method employs incoming signal and its Hilbert transform to calculate the instantaneous amplitude of the analytic signal, and then uses subtractive clustering algorithm to find the clustering centers of the instantaneous amplitude. The modulation order (M) of QAM signal is determined according to the number of clustering centers obtained. Computer simulation shows that the proposed method has strong capability for recognition of higher order modulation signals in the presence of additive white Gaussian noise (AWGN).",,POD:978-0-7695-3501-2,10.1109/CMC.2009.360,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4797040,Baud Rate Estimation;Blind Modulation Recognition;M-QAM;Software Defined Radio;Subtractive Clustering,AWGN;Algorithm design and analysis;Classification algorithms;Clustering algorithms;Monitoring;Quadrature amplitude modulation;Roaming;Signal analysis;Signal processing;Timing,AWGN;pattern clustering;quadrature amplitude modulation;signal detection,AWGN;Hilbert transform;additive white Gaussian noise;blind recognition algorithm;instantaneous amplitude calculation;modulated M-QAM signals;modulation order;signal detection;subtractive clustering algorithm,,2,,9,,no,6-8 Jan. 2009,,IEEE,IEEE Conference Publications
A Novel Visualization Technique for Electric Power Grid Analytics,P. C. Wong; K. Schneider; P. Mackey; H. Foote; G. Chin Jr.; R. Guttromson; J. Thomas,"Pacific Northwest National Laboratory, Richland",IEEE Transactions on Visualization and Computer Graphics,20090316,2009,15,3,410,423,"The application of information visualization holds tremendous promise for the electric power industry, but its potential has so far not been sufficiently exploited by the visualization community. Prior work on visualizing electric power systems has been limited to depicting raw or processed information on top of a geographic layout. Little effort has been devoted to visualizing the physics of the power grids, which ultimately determines the condition and stability of the electricity infrastructure. Based on this assessment, we developed a novel visualization system prototype, GreenGrid, to explore the planning and monitoring of the North American Electricity Infrastructure. The paper discusses the rationale underlying the GreenGrid design, describes its implementation and performance details, and assesses its strengths and weaknesses against the current geographic-based power grid visualization. We also present a case study using GreenGrid to analyze the information collected moments before the last major electric blackout in the Western United States and Canada, and a usability study to evaluate the practical significance of our design in simulated real-life situations. Our result indicates that many of the disturbance characteristics can be readily identified with the proper form of visualization.",1077-2626;10772626,,10.1109/TVCG.2008.197,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4695829,Applications;Information visualization;Visualization systems and software;Visualization techniques and methodologies,,data visualisation;electricity supply industry;geographic information systems;power system analysis computing;power system economics,Canada;GreenGrid;North American electricity infrastructure;Western United States;electric blackout;electric power grid analytics;electric power industry;geographic layout;geographic-based power grid visualization;visualization technique,"Computer Graphics;Computer Simulation;Ecosystem;Electricity;Imaging, Three-Dimensional;Models, Theoretical;Power Plants;User-Computer Interface",20,,32,,no,May-June 2009,,IEEE,IEEE Journals & Magazines
A Post Evaluation Technique for Engineering Project Investment Based on ANP-ENTROPY-TOPSIS,L. s. Zhou; C. Li; X. h. Yu,"Sch. of Bus. & Adm., North China Electr. Power Univ., Beijing, China",2009 International Conference on Management and Service Science,20091030,2009,,,1,4,"Mutual influence always exists among the indices in engineering project investment evaluation system. Since ANP (Analytic Network Process) technique can scientifically reflect the mutual influence among the indices, it is introduced into this paper instead of AHP to overcome the defect that traditional AHP technique can not reflect the relationship among the indices. And in the weight determining process, entropy method is adopted in combination with ANP to avoid the respective unilateralism of subjective weight method and objective weight method. On this basis, TOPSIS method is applied in the comprehensive evaluation for the engineering project investment schemes to quantitatively rank these schemes. Finally, by means of Super Decision software, an example is provided to show the effectiveness and scientificity of this evaluation technique.",,POD:978-1-4244-4638-4,10.1109/ICMSS.2009.5305005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5305005,,Entropy;Feedback;Industrial relations;Investments;Power engineering and energy;Testing,decision making;investment;project management;statistical analysis,AHP technique;Super Decision software;TOPSIS method;analytic network process method;engineering project investment;entropy method;weight determining process,,0,,4,,no,20-22 Sept. 2009,,IEEE,IEEE Conference Publications
A self-maintenance clustering algorithm based on decision model for space information networks,Ning Ye; Zhiliang Zhu; Jun Liu; Weiyan Ren,"Information Science and Engineering College, Notheastern University, Shenyang, Liaoning, China",2009 IEEE International Conference on Communications Technology and Applications,20091208,2009,,,816,820,"Space information networks, which have become a popular research focus, are a new type of self-organizing networks. With fully considering characteristics distinguishing space information networks from common self-organizing networks such as terrestrial ad hoc networks and wireless sensor networks, the proposed clustering algorithm introduces a decision model based on analytic hierarchy process (AHP) to select cluster heads, and then forms non-overlapping k-hop clusters. The dynamical self-maintenance mechanisms take node mobility and cluster equalization into account. Besides of cluster merger/partition disposal, reaffiliation management and adaptive adjustment of information update period, mobile agents are used to migrate and duplicate functions of cluster heads in a recruiting way. Simulation results show that our clustering algorithm improves network scalability and is suitable for use in space information networks.",,POD:978-1-4244-4816-6,10.1109/ICCOMTA.2009.5349087,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5349087,AHP;ad hoc networks;clustering algorithm;mobile agent;space information networks,Ad hoc networks;Algorithm design and analysis;Clustering algorithms;Corporate acquisitions;Information analysis;Mobile agents;Partitioning algorithms;Self-organizing networks;Waste management;Wireless sensor networks,ad hoc networks;radio networks;satellite communication;signal processing;space communication links;wireless sensor networks,analytic hierarchy process;cluster head;decision model;self maintenance clustering algorithm;self maintenance mechanisms;self organizing network;space information networks,,0,,9,,no,16-18 Oct. 2009,,IEEE,IEEE Conference Publications
A simulation setup to optimize particle flow velocimetry,H. Gao; F. Kremer; H. F. Choi; J. U. Voigt; P. Claus; J. D'hooge,"Lab on Cardiovascular Imaging and Dynamics, Department of Cardiovascular Diseases, Catholic University of Leuven, Belgium",2009 IEEE International Ultrasonics Symposium,20100401,2009,,,1379,1382,"Particle Flow Velocimetry (PFV) has been introduced as a new ultrasound methodology to measure two-dimensional intraventricular flow patterns. It can potentially provide important new information on cardiac hemodynamics and function but how to optimize the whole process (such as frame rate, line density, contrast concentration) is still not clear. The aim of this study was therefore to build a simulation environment allowing to optimize this methodology by combining computational fluid dynamics (CFD) and ultrasound simulations. A 2D model of the left ventricular (LV) geometry was generated and meshed in order to be used as input to commercially available CFD software. An analytic description of a typical ventricular inflow velocity profile (showing an early and atrial filling phase) was used as a boundary condition at the inlet of the LV model and the dynamic flow field was simulated. Point scatterers were subsequently put at random positions within the model and their positions were updated over time based on the simulated flow field. From this dynamic scatterer field, ultrasound data could subsequently be obtained using a convolution-based model previously introduced by our lab. In order to test the simulation setup, RF signals from both a PW Doppler acquisition in the inlet portion of the model and a 2D color Doppler image sequence were simulated. For the PW Doppler spectrogram, the normalized RMSE of the estimated velocities relative to the CFD reference was 3.09%. Moreover, good qualitative agreement was found between the CFD and the color Doppler measurement. In conclusion, a simulation setup was constructed and shown to work correctly. It will be useful for optimizing PFV and for developing flow tracking methods in echocardiography further.",1051-0117;10510117,Electronic:978-1-4244-4390-1; POD:978-1-4244-4389-5,10.1109/ULTSYM.2009.5441655,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5441655,,Computational fluid dynamics;Computational modeling;Geometry;Hemodynamics;Optimization methods;Particle measurements;Particle scattering;Solid modeling;Ultrasonic imaging;Ultrasonic variables measurement,channel flow;computational fluid dynamics;echocardiography;flow visualisation;haemodynamics,Doppler image sequence;PW Doppler acquisition;RF signals;cardiac hemodynamics;computational fluid dynamics;convolution-based model;dynamic scatterer field;echocardiography;intraventricular flow patterns;left ventricular geometry;particle flow velocimetry;point scatterers;ultrasound methodology;ultrasound simulations,,1,,12,,no,20-23 Sept. 2009,,IEEE,IEEE Conference Publications
A Study on E-Government System Security Based on Fuzzy Analytic Hierarchy Process,Z. Wu; H. Li; C. Li,"Sch. of Manage. Studies, Shanghai Univ. of Eng. Sci., Shanghai, China","2009 5th International Conference on Wireless Communications, Networking and Mobile Computing",20091030,2009,,,1,6,"On the basis of analyzing various possible external threats and internal vulnerability in our country, this paper constructed a security index system of e-government system, introduced the algorithm of quantitative evaluation by using Fuzzy Analytic Hierarchy Process, and then proposed solutions to these threats.",2161-9646;21619646,POD:978-1-4244-3691-0,10.1109/WICOM.2009.5305680,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5305680,,Algorithm design and analysis;Application software;Data security;Electronic government;Engineering management;Fuzzy systems;Humans;Information security;National security;Network servers,fuzzy set theory;government data processing;security of data,e-government system security;fuzzy analytic hierarchy process;security index system,,0,,8,,no,24-26 Sept. 2009,,IEEE,IEEE Conference Publications
A survey-based study of the mapping of system properties to ISO/IEC 9126 maintainability characteristics,J. P. Correia; Y. Kanellopoulos; J. Visser,"Software Improvement Group Amsterdam, The Netherlands",2009 IEEE International Conference on Software Maintenance,20091030,2009,,,61,70,"The ISO/IEC 9126 international standard for software product quality is a widely accepted reference for terminology regarding the multi-faceted concept of software product quality. Based on this standard, the Software Improvement Group has developed a pragmatic approach for measuring technical quality of software products. This quality model introduces another level below the hierarchy defined by ISO/IEC 9126, which consists of system properties such as volume, duplication, unit complexity and others. A mapping between system properties and ISO/IEC 9126 characteristics is defined in a binary fashion: a property either influences a characteristic or not. This mapping embodies consensus among three experts based, in an informal way, on their experience in software quality assessment. We have conducted a survey-based experiment to study the mapping between system properties and quality characteristics. We used the Analytic Hierarchy Process as a formally structured method to elicit the relative importance of system properties and quality characteristics from a group of 22 software quality experts. We analyzed the results of the experiment with two objectives: (i) to validate the original binary mapping and (ii) to refine the mapping using the elicited relative weights.",1063-6773;10636773,POD:978-1-4244-4897-5,10.1109/ICSM.2009.5306346,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306346,,IEC standards;ISO standards;Measurement standards;Software maintenance;Software measurement;Software quality;Software standards;Stability analysis;Standards development;Terminology,IEC standards;ISO standards;software maintenance;software quality;software reliability;software standards,ISO/IEC 9126 international standard;ISO/IEC 9126 maintainability characteristics;analytic hierarchy process;binary mapping;software improvement group;software product quality;software quality assessment;survey-based study;system properties mapping,,2,,12,,no,20-26 Sept. 2009,,IEEE,IEEE Conference Publications
Adaptive Intra-Symbol SMSE Waveform Design Amidst Coexistent Primary Users,E. C. Like; M. A. Temple; S. C. Gustafson,"Dept. of Electr. & Comput. Eng., Air Force Inst. of Technol., Wright-Patterson AFB, OH, USA",2009 IEEE International Conference on Communications,20090811,2009,,,1,6,"An analytic approach is presented for optimizing spectrally modulated, spectrally encoded (SMSE) waveforms using independent selection of intra-symbol (within a symbol) subcarrier power and modulation order. The SMSE framework is well-suited for cognition-based, software defined radio (SDR) applications. By exploiting statistical knowledge about the spectral and temporal behavior of interfering signals, the inherent SMSE framework flexibility is leveraged to substantially increase system throughput while limiting coexistent interference. Results for a coexistent scenario are provided in which the analytic optimization of the SMSE waveform is demonstrated in the presence of multiple direct sequence spread spectrum (DSSS) signals. The results reveal significant performance benefits that demonstrate the potential of the SMSE framework to dynamically adapt to changing environmental conditions-key functionality required for future SDR implementations.",1550-3607;15503607,CD-ROM:978-1-4244-3435-0,10.1109/ICC.2009.5198702,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5198702,,Application software;Bit error rate;Communications Society;Interference;OFDM modulation;Signal analysis;Software radio;Spread spectrum communication;Throughput;USA Councils,cognitive radio;modulation coding;software radio;spread spectrum communication,cognition-based radio;direct sequence spread spectrum signals;interfering signals;intra-symbol subcarrier;modulation order;software defined radio;spectral behavior;spectrally modulated spectrally encoded waveform design;statistical knowledge;temporal behavior,,4,,19,,no,14-18 June 2009,,IEEE,IEEE Conference Publications
Amplifying performance of lever-type flexure hinge amplifying mechanism,J. Shen; Y. Zhao,"Department of Mechanical Engineering, Jiaxing University No.56, Yuexiu Road, Jiaxing, Zhejiang, China, 314001",2009 9th International Conference on Electronic Measurement & Instruments,20091002,2009,,,1-294,1-297,"Two kinds of layout model of lever-type flexure hinge amplifying mechanism are presented. Then the forces of flexure hinge in the two layouts are discussed in detail and the displacement losses are calculated. Finally, the displacement losses and the amplifying factors of flexure hinge amplifying mechanism are analyzed by using the finite element software ANSYS. The analytic results demonstrate that the theoretical results are agreement with the finite element analytic results well and that the amplifying performance of layout A is better than that of layout B.",,POD:978-1-4244-3863-1,10.1109/ICEMI.2009.5274870,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5274870,Amplifying performance;Flexure hinge amplifying mechanism;Layout,Fasteners;Finite element methods;Force measurement;Frequency response;Instruments;Manufacturing;Mechanical engineering;Mechanical variables measurement;Performance analysis;Piezoelectric actuators,bending;fasteners;finite element analysis,ANSYS;displacement losses;finite element analysis;layout model;lever-type flexure hinge amplifying mechanism,,1,,7,,no,16-19 Aug. 2009,,IEEE,IEEE Conference Publications
An analytic model of optimistic Software Transactional Memory,A. Heindl; G. Pokam; A. R. Adl-Tabatabai,"Department of Computer Science, University of Erlangen-Nuremberg, Germany",2009 IEEE International Symposium on Performance Analysis of Systems and Software,20090512,2009,,,153,162,"An analytic model is proposed to assess the performance of optimistic software transactional memory (STM) systems with in-place memory updates for write operations. Based on an absorbing discrete-time Markov chain, closed-form analytic expressions are developed, which are quickly solved iteratively to determine key parameters of the STM system. The model covers complex implementation details such as read/write locking, data consistency checks and conflict management. It provides fundamental insight into the system behavior, when we vary input parameters like number and size of concurrent transactions or the number of the data objects. Numerical results are validated by comparison with a discrete-event simulation.",,POD:978-1-4244-4184-6,10.1109/ISPASS.2009.4919647,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4919647,,Computer science;Concurrent computing;Hardware;Microprocessors;Performance analysis;Power system management;Programming profession;Software performance;Space exploration;Transaction databases,Markov processes;concurrency control;iterative methods;program diagnostics;storage management;transaction processing,closed-form analytic expression;concurrent transaction;discrete-time Markov chain;in-place memory update;iterative method;optimistic software transactional memory analytic model;system behavior;write operation,,3,,25,,no,26-28 April 2009,,IEEE,IEEE Conference Publications
An Analytic Network Process approach to the planning and managing of the energy politics,T. Gurbuz; Y. E. Albayrak; Y. C. Erensal; M. Ozyol,Galatasaray University Industrial Engineering Dept. &#199;&#191;ra&#191;an cad. No.36 &#191;stanbul / Turkey,2009 International Conference on Computers & Industrial Engineering,20090825,2009,,,1690,1693,"The obtainment of electricity in a cheap, reliable and stable way on time with good quality is of high priority for the management of countries. In order to do so, in our country where the need to electrical energy is rapidly increasing, the planning of production and development of electrical energy and its execution is very important from the economical point of view. However, the analysis of the final consumption of the country, the determination of all the factors affecting the final demand, is a multicriteria decision making (MCDM) problem which requires taking several different factors in account. In this study, the determination of the scenarios of the socio-economic and technique development and the problem of evaluating the values calculated for these scenarios will be solved with analytic network process (ANP)'s software superdecisions which will provide more accurate results by taking the relations between the evaluation factors into account. The results will be evaluated and their conformity to this process will be researched.",,POD:978-1-4244-4135-8,10.1109/ICCIE.2009.5223855,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5223855,Analytic Network Process;Energy Management;Energy Requirement Planning,Decision making;Energy consumption;Energy management;Fuel economy;Humans;Industrial engineering;Power generation economics;Process planning;Production planning;Quality management,decision making;decision support systems;electricity supply industry;energy consumption;energy management systems;supply and demand;town and country planning,analytic network process;energy management;energy requirement planning;multicriteria decision making;socio-economic scenario,,2,,12,,no,6-9 July 2009,,IEEE,IEEE Conference Publications
An Application of the AHP in Supplier Selection of Maintenance and Repair Parts,Y. Wang,"Sch. of Software, Tsinghua Univ., Beijing, China",2009 First International Conference on Information Science and Engineering,20100426,2009,,,4176,4179,"The supplier selection of maintenance and repair parts is very important for the maintenance of complicated equipments, as it is a multi-item, multi-person and multi-criteria decision problem. Among the recorded literature, the analytic hierarchy process (AHP) has been recognized as an appropriate approach to solve the above problem, which helps several decision-makers with different conflicting objectives to arrive at a consensus decision. In this paper, an innovative model which based on the AHP method is formulated and it can be applied to provide a framework for the organization to select a supplier that satisfies the customer specifications. We also could change the assessment indicator system a little to apply the framework supplied by our selection model in any other industry.",2160-1283;21601283,Electronic:978-1-4244-5728-1; POD:978-1-4244-4909-5,10.1109/ICISE.2009.237,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5455238,,Application software;Cost function;Decision making;Information science;Machinery;Manufacturing;Marketing management;Process planning;Production planning;Software maintenance,customer satisfaction;decision making;maintenance engineering,AHP;analytic hierarchy process;assessment indicator system;customer specifications;equipment maintenance;innovative model;maintenance parts;multicriteria decision problem;multiitem decision problem;multiperson decision problem;repair parts;supplier selection,,1,,21,,no,26-28 Dec. 2009,,IEEE,IEEE Conference Publications
An Approach for Eliciting Software Requirements and its Prioritization Using Analytic Hierarchy Process,M. Sadiq; S. Ghafir; M. Shahid,"Comput. Eng. Sect., Univ. Polytech., New Delhi, India",2009 International Conference on Advances in Recent Technologies in Communication and Computing,20091117,2009,,,790,795,"Most software engineering methods presume that requirements are explicitly and completely stated; however, experience shows that requirements are rarely complete and usually contain implicit requirements. The failure or success of a software system depends on the quality of the requirements. The quality of the requirements is influenced by the techniques employed during requirements elicitation. Requirements elicitation is most critical part of the software development because errors at this beginning stage propagate through the development process and the hardest to repair later. In this paper we have proposed an algorithmic approach to elicit the software requirements and its prioritization of the requirements using analytic hierarchy process (AHP).",,POD:978-1-4244-5104-3,10.1109/ARTCom.2009.58,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5328151,AHP;Elicitation;prioritization;software requirements,Communications technology;Computer science;Information management;Ontologies;Programming;Security;Software algorithms;Software engineering;Software systems;Technology management,decision making;software quality;systems analysis,analytic hierarchy process;requirement quality;software development;software engineering methods;software requirement elicitation,,6,,14,,no,27-28 Oct. 2009,,IEEE,IEEE Conference Publications
An Approach for Selecting Software-as-a-Service (SaaS) Product,M. Godse; S. Mulik,"Shailesh J Mehta Sch. of Manage., Indian Inst. of Technol. Bombay, Mumbai, India",2009 IEEE International Conference on Cloud Computing,20091009,2009,,,155,158,"Software-as-a-Service (SaaS) helps organizations avoid capital expenditure and pay for the functionality as an operational expenditure. Though enterprises are unlikely to use SaaS model for all their information systems needs, certain business functionalities such as Sales Force Automation (SFA), are more seen to be implemented using SaaS model. Such demand has prompted quite a few vendors to offer SFA functionality as SaaS. Enterprises need to adopt an objective approach to ensure they select the most appropriate SaaS product for their needs. This paper presents an approach that makes use of Analytic Hierarchy Process (AHP) technique for prioritizing the product features and also for expert-led scoring of the products.",2159-6182;21596182,POD:978-1-4244-5199-9,10.1109/CLOUD.2009.74,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5284286,AHP;Product Selection;SaaS;Software-as-a-Service,Application software;Automation;Cloud computing;Computer architecture;Conference management;Costs;Data security;Marketing and sales;Subscriptions;Usability,Web services;decision making,AHP;SaaS model;analytic hierarchy process;business functionalities;capital expenditure;expert-led scoring;operational expenditure;prioritizing;sales force automation;software-as-a-service product,,35,,11,,no,21-25 Sept. 2009,,IEEE,IEEE Conference Publications
An Approach of Real-Time Team Behavior Control in Games,Y. She; P. Grogono,"Dept. of Comput. Sci. & Software Eng., Concordia Univ., Montreal, QC, Canada",2009 21st IEEE International Conference on Tools with Artificial Intelligence,20091228,2009,,,546,550,"The design of NPC (non-player character) is an analytic process. It is relying on assumptions of human game players' behavior. In practice, however, different PCs (player characters) often exhibit variable behavior, making them difficult to predicate and complicating the design process. In this paper, we describe an approach for team AI planning and learning. This approach is based on procedural knowledge and a layered multi-agent architecture. We implement real-time transfer learning and adaptive mechanism for the team of NPCs. The team can react to the human player with the tactical awareness of seasoned team behavior. Results indicate that the approach of using the hybrid of transfer learning and adaptive mechanism can improve NPCs' overall performance in real-time.",1082-3409;10823409,POD:978-1-4244-5619-2,10.1109/ICTAI.2009.99,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5366965,AI;Game;Multi-agent,Artificial intelligence;Bismuth;Computer science;Databases;Debugging;Design engineering;Humans;Production;Software engineering;State-space methods,artificial intelligence;computer games;multi-agent systems;software architecture,adaptive mechanism;human game player behavior;multiagent architecture;nonplayer character;player characters;real-time team behavior control;real-time transfer learning;tactical awareness;team AI planning,,0,,9,,no,2-4 Nov. 2009,,IEEE,IEEE Conference Publications
An Information System Security Risk Assessment Model Based on Fuzzy Analytic Hierarchy Process,D. L. Liu; S. S. Yang,"Zhengzhou Inf. Sci. & Technol. Inst., Zhengzhou",2009 International Conference on E-Business and Information System Security,20090630,2009,,,1,4,"Information system is a large-scale complex system. It includes many uncertain factors, as software, hardware, people and so on. As a result, information systems security risk is related to many ambiguous factors, what are difficult to measure, with ambiguity. This paper introduces the information system security risk generating mechanism, and based on the risk assessment of factors, builds information system security risk assessment model based on fuzzy analytic hierarchy process, which could be used to evaluate the security situation of information system.",2161-5942;21615942,POD:978-1-4244-4589-9,10.1109/EBISS.2009.5137926,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5137926,,Fuzzy systems;Information analysis;Information science;Information security;Information systems;Internet;Large-scale systems;Risk analysis;Risk management;Uncertainty,decision making;fuzzy set theory;information systems;risk management;security of data,fuzzy analytic hierarchy process;information system security risk assessment;large-scale complex system;security situation,,3,,9,,no,23-24 May 2009,,IEEE,IEEE Conference Publications
An Integrated Supplier Selection Approach Based on Fuzzy Analytic Network Process,B. Pang,"Coll. of Manage., Harbin Univ. of Commerce, Harbin, China",2009 International Conference on Computational Intelligence and Software Engineering,20091228,2009,,,1,4,This paper develops a supplier evaluation approach based on the analytic network process (ANP) and fuzzy synthetic evaluation under a fuzzy environment. The importance weights of various criteria are considered as linguistic variables. These linguistic ratings can be expressed in triangular fuzzy numbers by using the fuzzy extent analysis. Fuzzy synthetic evaluation is used to select a supplier alternative and the Fuzzy ANP (FANP) method is applied to calculate the important of the criteria weights. Then an integrated FANP and fuzzy synthetic evaluation methodology is proposed for evaluating and selecting the most suitable suppliers. A hypothetical example is presented and the results indicated that the combination of ANP and fuzzy synthetic evaluation provided useful tool to select the optimal supplier.,,CD-ROM:978-1-4244-4507-3,10.1109/CISE.2009.5362809,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5362809,,Arithmetic;Business;Decision making;Educational institutions;Electronic mail;Environmental management;Feedback;Fuzzy set theory;Humans;Uncertainty,fuzzy set theory;supply chains,fuzzy analytic network process;fuzzy extent analysis;fuzzy synthetic evaluation;integrated supplier selection approach;linguistic variables;triangular fuzzy numbers,,0,,11,,no,11-13 Dec. 2009,,IEEE,IEEE Conference Publications
Analysis and evaluation of human factors in aviation maintenance based on fuzzy and AHP method,L. Wang; R. Sun; Z. Yang,"Research Institute of Civil Aviation Safety, Civil Aviation University of China, Tianjin, 300300, China",2009 IEEE International Conference on Industrial Engineering and Engineering Management,20100108,2009,,,876,880,"This paper aimed to develop a quantitative and objective method to analyze and evaluate human factors in aviation maintenance process. Firstly a detailed classification and causing analysis on human errors in aviation maintenance was carried out basing on SHEL and Reason Model; then an integrated evaluation solution was proposed out by using fuzzy and AHP method together. Then the solution was applied into practice for an evaluation case. Finally it concluded that Noise & vibration, Professional ethics & responsibility, Safety information sharing and Completeness of software & document took the four largest weights to the four first-level indices respectively. This comprehensive evaluation method was expected to be using in practice and provide effective support to prevent human errors and improve safety in maintenance organizations.",2157-3611;21573611,POD:978-1-4244-4869-2,10.1109/IEEM.2009.5372887,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5372887,AHP;Human factors;aviation maintenance;evaluation;fuzzy,Air accidents;Air safety;Aircraft propulsion;Computer aided analysis;Error analysis;Hardware;Human factors;Software maintenance;Software safety;Software tools,aircraft maintenance;decision making;human factors,AHP method;analytic hierarchy processing;aviation maintenance;fuzzy method;human error causing analysis;human factor evaluation;noise-vibration factor;professional ethics-responsibility factor;safety information sharing factor;software-document completeness factor,,0,,15,,no,8-11 Dec. 2009,,IEEE,IEEE Conference Publications
Analysis of influencing factors on performance evaluation of agricultural products network marketing based on AHM,Hua Jiang; Fangshan Wang,"School of Economics and Management, Hebei University of Engineering, Handan, China",2009 2nd International Conference on Power Electronics and Intelligent Transportation System (PEITS),20100205,2009,3,,140,143,"Agricultural products network marketing means fully introducing e-commerce systems into the sale process of agricultural products, using information technologies to publish and collect the demand and price information, and relying on agricultural production bases and logistics distribution systems to enhance brand images, improve customer services, develop online marketing channels and ultimately expand marketing activities. So, it is very important to evaluate the performance of agricultural products network marketing. According to the principles of purposefulness, scientific, practicality and comprehensiveness, the paper firstly established an evaluation index system of agricultural products network marketing performance, and then applied analytic hierarchical model (AHM) to analyze these factors. This can provide the basis for researchers to study how to improve the performance of agricultural products network marketing.",,Electronic:978-1-4244-4543-1; POD:978-1-4244-4544-8,10.1109/PEITS.2009.5406815,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5406815,agricultural products;analytic hierarchical model;influencing factors;network marketing,Agricultural products;Customer service;Decision making;Information technology;Intelligent transportation systems;Logistics;Marketing and sales;Performance analysis;Production systems;Testing,agricultural products;customer services;decision making;electronic commerce;logistics;marketing;statistical analysis,AHM;agricultural production;agricultural products network marketing;analytic hierarchical model;customer services;e-commerce systems;evaluation index system;logistics distribution systems;online marketing channels,,1,,8,,no,19-20 Dec. 2009,,IEEE,IEEE Conference Publications
Analytic Architecture Assessment in SOA Solution Design and its Engineering Application,N. Zhou; L. J. Zhang,"IBM T.J. Watson Res. Center, Hawthorne, NY, USA",2009 IEEE International Conference on Web Services,20090731,2009,,,807,814,"In this paper, we present an architecture-centric assessment approach for model evaluation over reference architecture to quantitatively estimate architecture maturity and quality. Such assessment is essential to support design-level refinement for an enterprise solution. To achieve this analytic goal, we select a nine-layer SOA solution stack (S3) as reference architecture, and introduce the necessary mathematical definitions and formulation. The baseline for such assessment is a model template composed of S3 solution patterns. A template is the starting point of creating a design model. The selection of such template will largely determine the architecture properties of the final SOA solution.The maturity analysis is carried out at different granularity levels (architecture building block, architecture layer, and architecture model) to justify the 'completeness' of a design. The quality assessment is accomplished through a set of quality-indicators to justify the 'goodness' of an architecture based on the relationships of architecture building block instances. Finally, using UML 2.0 to capture the model of S3, we provide a real assessment prototype developed over IBM RSA platform.",,POD:978-0-7695-3709-2,10.1109/ICWS.2009.117,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5175900,architecture building block;architecture maturity;architecture quality;graph theory;model completeness;model template;solution pattern,Data analysis;Design engineering;Graph theory;Prototypes;Quality assessment;Semiconductor optical amplifiers;Service oriented architecture;USA Councils;Unified modeling language;Web services,Unified Modeling Language;software architecture;software quality,S3 solution pattern;SOA solution design;SOA solution stack;UML;analytic architecture assessment;architecture building block;architecture layer;architecture maturity;architecture model;architecture quality;design-level refinement;engineering application;enterprise solution;model evaluation;quality assessment;quality indicator,,0,,9,,no,6-10 July 2009,,IEEE,IEEE Conference Publications
Analytic calculation of stress of multi-throttle-slices for twin tubes shock absorber,Changcheng Zhou; Chengjun Li,"School of Traffic and Vehicle Engineering, Shandong University of Technology, Shandong Zibo 255049, China",2009 IEEE International Conference on Intelligent Computing and Intelligent Systems,20091228,2009,1,,84,88,"In this paper, the deformation, internal forces and stress of single throttle slice of shock absorber was analyzed, the formula of the stresses of its were established. Studied the pressure and the stresses on each slice, the analytic stresses formulas of multi-slices were established. Followed a practical example for stresses computation of multi-slices with this new analytic method, and it was simulated and verified by ANSYS software. The results show that the stress computation method for multi-slices is accurate enough.",,POD:978-1-4244-4754-1,10.1109/ICICISYS.2009.5357930,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5357930,analytic calculation;multi-throttle-slices;stress;twin tubes shock absorber,Analytical models;Automobiles;Automotive engineering;Computational modeling;Educational institutions;Finite element methods;Internal stresses;Security;Shock absorbers;Vehicles,elasticity;finite element analysis;mechanical engineering computing;pipes;shock absorbers,ANSYS software;elastic mechanics;finite element analysis software;internal forces;multithrottle-slices;twin tubes shock absorber,,0,,6,,no,20-22 Nov. 2009,,IEEE,IEEE Conference Publications
"Analytic Hierarchy Process (AHP), Weighted Scoring Method (WSM), and Hybrid Knowledge Based System (HKBS) for Software Selection: A Comparative Study",A. Jadhav; R. Sonar,"Indian Inst. of Technol. Bombay, Mumbai, India",2009 Second International Conference on Emerging Trends in Engineering & Technology,20100122,2009,,,991,997,"Multi criteria decision making (MCDM) methods help decision makers to make preference decision over the available alternatives. Evaluation and selection of the software packages is multi criteria decision making problem. Analytical hierarchy process (AHP) and weighted scoring method (WSM) have widely been used for evaluation and selection of the software packages. Hybrid knowledge based system (HKBS) approach for evaluation and selection of the software packages has been proposed recently. Therefore, there is need to compare HKBS, AHP and WSM. This paper studies and compares these approaches by applying for evaluation and selection of the software components. The comparison shows that HKBS approach for evaluation and selection of the software packages is comparatively better than AHP and WSM with regard to (i) computational efficiency (ii) flexibility in problem solving (iii) knowledge reuse and (iv) consistency and presentation of the evaluation results.",2157-0477;21570477,Electronic:978-0-7695-3884-6; POD:978-1-4244-5250-7,10.1109/ICETET.2009.33,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5395484,,Computational efficiency;Decision making;Information technology;Knowledge based systems;Knowledge engineering;Packaging;Problem-solving;Software packages;Software systems;Software tools,decision making;knowledge based systems;object-oriented programming;problem solving;software engineering;software packages;software selection,analytic hierarchy process;computational efficiency;hybrid knowledge based system;knowledge reuse;multicriteria decision making methods;problem solving;software components;software packages;software selection;weighted scoring method,,2,,15,,no,16-18 Dec. 2009,,IEEE,IEEE Conference Publications
Analytic Scoring of Landscape CAD Course Based on BP Neural Network,Y. c. Zhang; L. f. Qiao; L. l. Zhang; T. Chen,"Sch. of Landscape Archit., Henan Inst. of Sci. & Technol., Xinxiang, China",2009 International Conference on Education Technology and Computer,20090721,2009,,,13,16,"Course scoring is the major criteria for measuring the teaching effect and a basis for improving the education quality. Analytic scoring of landscape CAD (computer-aided design) course is influenced by various factors. Relationships among these factors are complex, some are nonlinear, even some are random and fuzzy. It is difficult to explain their internal relationships with traditional method. This research combines back-propagation neural network and DPS software to establish a three-layer BP neural network model, which took 60 examination papers of landscape CAD course as samples and made predictions on the score in accordance with five factors, including landscape design standard, landscape design innovation, computer cartography standard, drawing effect and workload. The results show that BP neural network model has strong nonlinear approximation ability, could truly reflects the nonlinear relationships between global score of landscape CAD course and main controlling factors of analytic scoring, with small error between predicted values and the measured values, relative error lower than 5%. In the future, when analytic scoring of the landscape CAD course obtains from the teachers, the global scoring can be calculated by BPNN model automatically. This method showed wide application prospect to the courses need analytic scoring.",2155-1812;21551812,POD:978-0-7695-3609-5,10.1109/ICETC.2009.57,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5169443,BP Neural Network;analytic scoring;landscape CAD course,Automatic control;Computer errors;Computer networks;Computer science education;Design automation;Error correction;Neural networks;Predictive models;Software standards;Technological innovation,CAD;backpropagation;educational courses;neural nets,DPS software;analytic scoring;back-propagation neural network;computer cartography standard;computer-aided design;course scoring;drawing effect;drawing workload;landscape CAD course;landscape design innovation;landscape design standard;nonlinear approximation ability,,0,,10,,no,17-20 April 2009,,IEEE,IEEE Conference Publications
Analytics on historical data using a clustered insert-only in-memory column database,J. Schaffner; J. Kruger; S. Muller; P. Hofmann; A. Zeier,"Hasso Plattner-Institute for IT Systems Engineering at the University of Potsdam, Prof-Dr.-Helmert-Str. 2-3, 14482 Potsdam, Germany",2009 16th International Conference on Industrial Engineering and Engineering Management,20091204,2009,,,704,708,"In the field of OLAP and data warehousing, column stores and compressed main-memory data storage technology have successfully been implemented in products that enable a significant speed improvement of analytical queries with special performance requirements. We could soon see the majority of analytical workloads move to such main-memory based systems. Having one specialized OLAP DBMS explicitly aimed at performing ad-hoc queries on an ever-growing database requires the capability of an in-memory database to retain historical states so that applications can calculate consistent values based on previous states of the database, a requirement often found in financial and production planning analytical applications. This paper describes Rock, an in-memory analytics cluster based on a column store database, and proposes an architecture for historical query support as well as the prototypical implementation in Rock.",,POD:978-1-4244-3671-2,10.1109/ICIEEM.2009.5344497,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5344497,Databases;Software-as-a-Service,Calendars;Data analysis;Data engineering;Marketing and sales;Natural languages;Performance analysis;Systems engineering and theory;Transaction databases;Vacuum arcs;Warehousing,data compression;data mining;data warehouses;pattern clustering;query processing,DBMS;OLAP;ad-hoc query;analytical query;clustered insert-only in-memory column database;data compression;data warehousing;financial planning;historical data;main memory data storage technology;production planning,,0,1,15,,no,21-23 Oct. 2009,,IEEE,IEEE Conference Publications
Analytics-Driven Dashboards Enable Leading Indicators for Requirements and Designs of Large-Scale Systems,R. W. Selby,Northrop Grumman Space Technology,IEEE Software,20081222,2009,26,1,41,49,"Mining software repositories using analytics-driven dashboards provides a unifying mechanism for understanding, evaluating, and predicting the development, management, and economics of large-scale systems and processes. Dashboards enable measurement and interactive graphical displays of complex information and support flexible analytic capabilities for user customizability and extensibility. Dashboards commonly include system requirements and design metrics because they provide leading indicators for project size, growth, and volatility. This article focuses on dashboards that have been used on actual large-scale software projects as well as example empirical relationships revealed by the dashboards. The empirical results focus on leading indicators for requirements and designs of large-scale software systems based on insights from two sets of software projects containing 14 systems and 23 systems.",0740-7459;07407459,,10.1109/MS.2009.4,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721182,defects;designs;empirical analysis;leading indicators;metrics;requirements,Displays;Economic forecasting;Information analysis;Large scale integration;Large-scale systems;Project management;Risk management;Software development management;Software performance;Technology management,data mining;software engineering,analytics-driven dashboards;interactive graphical displays;large-scale systems;software repositories mining,,3,,16,,no,Jan.-Feb. 2009,,IEEE,IEEE Journals & Magazines
Analyzing Alternatives in Collaborative Technology of Software Contractor by Analytic Network Process,C. R. Wu; C. W. Chang; C. C. Liao,"Dept. & Grad. Inst. of Bus. Adm., Yuanpei Univ., Hsinchu, Taiwan","2009 Fourth International Conference on Innovative Computing, Information and Control (ICICIC)",20100217,2009,,,515,518,"Collaborative software technology management point of view to assist the industry in the use of collaborative enterprise to create competitive advantage, into the network era of economic and information systems to integrate control of the development platform to build synergies and reduce the occurrence of system customization. When the majority of the common problems faced by enterprises, will be faced with the choice of partners, to strengthen the demand for collaborative management industry, and decided to technical evaluation criteria. This study collect the advice of experts and scholars, see the relevant literature that the standard of their choice and analytic network process (ANP) to choose computer integrated manufacturing (CIM) software standards and weights. The structure of these advantages in this study under a more objective measure of the evaluation criteria forms the different types of coordination and technical guidance. Eventually, the study is a useful reference for enterprises to cooperate in the choice of technology development partners.",,Electronic:978-1-4244-5544-7; POD:978-1-4244-5543-0,10.1109/ICICIC.2009.104,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5412473,,Collaborative software;Computer industry;Computer integrated manufacturing;Control systems;Electrical equipment industry;Industrial control;Industrial economics;Management information systems;Software standards;Technology management,computer integrated manufacturing;groupware,analytic network process;collaborative enterprise;collaborative software technology management;evaluation criteria;information systems;software contractor;system customization,,0,,32,,no,7-9 Dec. 2009,,IEEE,IEEE Conference Publications
Application of AHP to tobacco enterprise performance appraisal,Shuang Zhang; Fengyan Hu; Shaobo Wang; Bo Liu; Qinghe Hu,"Software College, Box 349, Northeastern University, Shenyang 100004, China",2009 Chinese Control and Decision Conference,20090807,2009,,,2504,2507,"In order to achieve better economic benefit and social impression, tobacco enterprise must establish effective and operable performance appraisal system. It is very important to select indexes and their weight for performance appraisal. In the paper, weight for AHP-based (analytic hierarchy process) performance appraisal is studied by case. The result indicates that sales volume, inventory and purchasing plan are important indexes.",1948-9439;19489439,POD:978-1-4244-2722-2,10.1109/CCDC.2009.5192294,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5192294,AHP (Analytic Hierarchy Process);Index;Tobacco enterprise performance appraisal;Weight,Application software;Appraisal;Business;Educational institutions;Globalization;Indexing;Marketing and sales;Monopoly;Performance analysis;Technological innovation,inventory management;purchasing;sales management;tobacco industry,AHP based performance appraisal;analytic hierarchy process based performance appraisal;inventory;purchasing plan;sales volume;tobacco enterprise,,0,,7,,no,17-19 June 2009,,IEEE,IEEE Conference Publications
Application of AHP-TOPSIS to the Evaluation and Classification of Provincial Landscape Construction Level of China,Z. Yi-chuan; Q. Li-fang; C. Wei; P. Xing-zhi,"Sch. of Landscape Archit., Henan Inst. of Sci. & Technol., Xinxiang, China",2009 International Conference on Future Computer and Communication,20090804,2009,,,399,402,"Urban green system plays a vital role in the urban sustainable development. This study, using analytic hierarchy process (AHP) and technique for order preference by similarity to ideal solution (TOPSIS) models, taking provinces in China as unit, has made analysis and thus carried out a classification of the landscape construction level of provinces in China from a macroscopic view. The results show that the provincial landscape construction level is positively correlated with the economic development level. But due to the big difference of landscape construction level of each province, enough importance should be attached. The AHP-TOPSIS model has good discrimination in the evaluation on Chinapsilas provincial landscape construction level and it is also applicable to the evaluation and classification of urban landscape construction level among different cities.",,POD:978-0-7695-3591-3,10.1109/ICFCC.2009.55,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5189813,AHP-TOPSIS;classification;evaluation;provincial landscape construction level,Application software;Biological system modeling;Cities and towns;Computer applications;Computer architecture;Electronic mail;Environmental economics;Project management;Robustness;Sustainable development,decision making;decision theory;ecology;economic indicators;land use planning;pattern classification,AHP-TOPSIS model;Chinese economic development level;Chinese provincial landscape construction level classification;analytic hierarchy process;macroscopic view;technique-for-order-preference-by-similarity-to-ideal-solution;urban ecological infrastructure;urban green system;urban sustainable development,,0,,9,,no,3-5 April 2009,,IEEE,IEEE Conference Publications
Application of incomplete linguistic preference relations in predicting the success of ERP implementation,S. C. Hsu; T. C. Wang; T. H. Chang; J. Chang,"Department of Information Engineering, I-Shou University, Kaohsiung, Taiwan. 1, Section 1, Hsueh-Cheng Road, Ta-Hsu Hsiang, Kaohsiung, Taiwan 840",2009 IEEE International Conference on Fuzzy Systems,20091002,2009,,,1983,1988,"This study applies an analytic hierarchical prediction model based on Multi-Criteria Decision Making with Incomplete Linguistic Preference Relations(InLinPreRa) to help the organizations become aware of the essential factors affecting the Enterprise Resource Planning(ERP), as well as identify the actions necessary before implementing ERP. The subjectivity and vagueness in the prediction procedures are dealt with using linguistic terms quantified in an interval scale [-t, t] . Then predicted success/failure values are obtained to enable organizations to decide whether to initiate ERP, inhibit adoption or take remedial actions to increase the possibility of successful ERP. The empirical results not only demonstrate the senior manager support degree, organizational and coordination are the three most important influential factors in the ERP initiative process, but also reveal the applicability and feasibility of reciprocal Incomplete Linguistic Preference Relation(InLinPreRa) for solving complicated hierarchical multiple attribute prediction problems.",1098-7584;10987584,POD:978-1-4244-3596-8,10.1109/FUZZY.2009.5277255,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5277255,Analytical Hierarchy process;ERP;InLinPreRa;Incomplete Linguistic Preference Relations;Multi-Criteria Decision Making,Application software;Companies;Costs;Databases;Decision making;Enterprise resource planning;Packaging;Predictive models;Software packages;Supply chains,decision making;enterprise resource planning,ERP implementation;analytic hierarchical prediction model;enterprise resource planning;hierarchical multiple attribute prediction problem;incomplete linguistic preference relations;multicriteria decision making,,0,,9,,no,20-24 Aug. 2009,,IEEE,IEEE Conference Publications
Applying the AHP Approach to Evaluate the Competition of Telecommunications Companies,J. Jin; L. He,"Sch. of Econ. & Manage., Wuhan Univ., Wuhan, China",2009 International Conference on Computational Intelligence and Software Engineering,20091228,2009,,,1,4,"The rigorous competition has become a crucial challenge for Chinese telecommunications companies. This paper employs the analytic hierarchy process (AHP) approach to evaluate the competition of telecom companies. Specifically, the model proposed in the paper can be used to establish related factors and consequently their weights that are applied to calculate the competition index of telecommunications companies. In the case study, the paper employs the data from a questionnaire survey and the results indicate that the market shares have the highest weight and management is very important to build up the competition of telecommunications companies. Meanwhile, the results also find that using the AHP model can reduce time in evaluating competition of telecommunications companies.",,CD-ROM:978-1-4244-4507-3,10.1109/CISE.2009.5364849,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5364849,,Chaos;Chromium;Costs;Decision making;Fuzzy systems;Helium;History;Organizing;Technological innovation;Telecommunications,competitive intelligence;decision making;telecommunication services,AHP model;Chinese telecommunications company;analytic hierarchy process;competition index;market shares,,0,,6,,no,11-13 Dec. 2009,,IEEE,IEEE Conference Publications
Automatic fault detection and diagnosis in complex software systems by information-theoretic monitoring,M. Jiang; M. A. Munawar; T. Reidemeister; P. A. S. Ward,"Shoshin Distributed Systems Group, E&CE Department, University of Waterloo, Ontario N2L 3G1, Canada",2009 IEEE/IFIP International Conference on Dependable Systems & Networks,20090929,2009,,,285,294,"Management metrics of complex software systems exhibit stable correlations which can enable fault detection and diagnosis. Current approaches use specific analytic forms, typically linear, for modeling correlations. In this paper we use normalized mutual information as a similarity measure to identify clusters of correlated metrics, without knowing the specific form. We show how we can apply the Wilcoxon rank-sum test to identify anomalous behaviour. We present two diagnosis algorithms to locate faulty components: RatioScore, based on the Jaccard coefficient, and SigScore, which incorporates knowledge of component dependencies. We evaluate our mechanisms in the context of a complex enterprise application. Through fault injection experiments, we show that we can detect 17 out of 22 faults without any false positives. We diagnose the faulty component in the top five anomaly scores 7 times out of 17 using SigScore, which is 40% better than when system structure is ignored.",1530-0889;15300889,POD:978-1-4244-4422-9,10.1109/DSN.2009.5270324,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5270324,fault detection and diagnosis;information theory;self-managing systems;statistical techniques,Automatic testing;Clustering algorithms;Computerized monitoring;Entropy;Fault detection;Fault diagnosis;Fault location;Information theory;Predictive models;Software systems,fault diagnosis;fault tolerant computing;information theory;software maintenance;statistical analysis,Jaccard coefficient;RatioScore component;SigScore component;Wilcoxon rank-sum test;anomalous behaviour identification;automatic fault detection system;complex enterprise application;complex software system;fault diagnosis;information theoretic monitoring;management metrics;normalized mutual information,,13,,19,,no,June 29 2009-July 2 2009,,IEEE,IEEE Conference Publications
Batch on-line analytics for every user,T. Blevins; W. Wojsznis; B. Wojewodka,"Emerson Process Management, USA",2009 American Control Conference,20090710,2009,,,28,29,"Process Analytical Technologies (PAT), in particular Principal Component Analysis (PCA) for fault detection and Projection to Latent Structures (PLS) for end of batch quality prediction, are seen as pivotal techniques for improving process operation. A number of software packages are available today for off-line analysis. The difficulty for control or production engineers is that these tools are not designed for on-line operation. Many PAT issues may be addressed by tightly integrating analytic tools with the production and control system. This special session examines basic design requirements associated with batch analytics applied for on-line operation. The presentation delivers an in-depth look at the data processing requirements, calculations and limitations for the application of on-line analytics to a batch process. It will show how to achieve proper data alignment for different batches, a key requirement for building good statistical models, and how to use the model for on-line analysis. A typical batch operation in the chemical industry and a running simulation will be used to illustrate the advantages of on-line process analytics over traditional monitoring and control techniques. Details will be presented on the approach taken to integrate analytic tools and results into a commercial control system with examples of control and production operation screens.",0743-1619;07431619,POD:978-1-4244-4523-3,10.1109/ACC.2009.5159786,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5159786,,Analytical models;Batch production systems;Buildings;Chemical industry;Control systems;Data processing;Design engineering;Fault detection;Principal component analysis;Software packages,,,,0,,,,no,10-12 June 2009,,IEEE,IEEE Conference Publications
CAE software of twin-tubes shock absorber outer-characteristic,C. Zhou; J. Meng; Y. Liu,"School of Traffic and Vehicle Engineering, Shandong University of Technology, Zibo 255049, China",2009 11th IEEE International Conference on Computer-Aided Design and Computer Graphics,20090918,2009,,,442,447,"The pathway throttle and local throttle loss of oil flow in telescopic shock absorber were analyzed, and the pathway loss coefficient and equivalent length of piston holes were studied. According to the thickness and the pre-deformation of throttle slice, and the throttle holes area, the velocity points of valve opening were researched and the analytic formulas of shock absorber velocity when valve opening were given. With the velocity points of valve opening, the model of shock absorber outer characteristic was established by piecewise linear math function. Based on this, the CAE software for shock absorber outer characteristic was developed. A practical example of simulation of shock absorber outer characteristic was given with this CAE software, and the performance test was conducted to verify the CAE software. The results show that the CAE software is reliable, and the values simulated is close to that tested.",,POD:978-1-4244-3699-6,10.1109/CADCG.2009.5246863,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5246863,,Automobiles;Computational modeling;Computer aided engineering;Piecewise linear techniques;Pistons;Shock absorbers;Software performance;Software testing;Space technology;Valves,automotive components;mechanical engineering computing;shock absorbers;vehicle dynamics,local throttle loss;pathway loss coefficient;pathway throttle;piecewise linear math function;piston holes equivalent length;telescopic shock absorber;throttle holes area;throttle slice;twin-tubes shock absorber;valve opening,,0,,11,,no,19-21 Aug. 2009,,IEEE,IEEE Conference Publications
Calibrating Resource Allocation for Parallel Processing of Analytic Tasks,J. Yan; W. S. Li,"SAP Technol. Lab., Shanghai, China",2009 IEEE International Conference on e-Business Engineering,20091201,2009,,,327,332,"Cloud Computing, the long-held dream of computing as a utility, has the potential to transform a large part of the IT industry, making software even more attractive as a service and shaping the way IT hardware is designed and purchased. In this environment, any application needs a model of to achieve elasticity and the illusion of infinite capacity requires each of these resources to be virtualized to hide the implementation of how they are multiplexed and shared. Given the nature of parallel processing dynamic, how to assign numbers of servers, CPUs, cores to the tasks have great impacts to the resource utilization of a PaaS (Platform as a Service) provider. In this paper, we face the challenge in automated calibration of resource allocation for parallel processing of analytic tasks. The proposed framework does not assume availability of data statistics and application semantics but probeable tradeoff between parallelism benefits and overheads. To implement it, a Sampling-then-Calibrating algorithm is presented to sample the runtime statistic information and calibrate the resource allocation accordingly. The experiments validate effectiveness of our approach.",,POD:978-0-7695-3842-6,10.1109/ICEBE.2009.51,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5342096,,Application software;Application virtualization;Cloud computing;Computer industry;Elasticity;Hardware;Parallel processing;Resource management;Resource virtualization;Statistics,parallel processing;resource allocation,CPU;IT industry;PaaS provider;analytic task;automated resource allocation calibration;cloud computing;data statistics;parallel processing;platform-as-a-service;server assignment,,1,1,17,,no,21-23 Oct. 2009,,IEEE,IEEE Conference Publications
Carbon management in assembly manufacturing logistics,K. Sourirajan; P. Centonze; M. E. Helander; K. Katircioglu; M. Ben-Hamida; C. Boucher,"IBM Research Division, Thomas J. Watson Research Center, P.O. Box 218, Yorktown Heights, New York 10598, USA",IBM Journal of Research and Development,20100406,2009,53,3,8:01,8:16,"In this paper, we present the IBM Carbon Analyzer Tool, a software solution that models and quantifies carbon emissions and explores ways to reduce emissions through advanced analytics. The tool is designed to manage carbon emissions associated with the support logistics for an assembly manufacturing operation. The tool has four analytical modules. A shipment analysis module calculates carbon emissions from transportation activities and analyzes opportunities for reducing emissions by changing fuel types of vehicles and using larger vehicles that permit consolidated shipments. A sourcing analysis module compares sourcing alternatives, including changes to supplier locations, routing of shipments, frequency of orders, and transportation modes. A scenario analysis module explores various consolidation policies to minimize transportation, inventory, and carbon costs, subject to inventory availability requirements. A sensitivity analysis module quantifies the effects of changes to uncontrollable and uncertain inputs, such as manufacturing demand for components, carbon prices, and supplier reliability. The tool makes use of a Java‰ã¢-based graphical user interface and an IBM DB2å¨ (Database 2‰ã¢) platform to manage input and output data. A pilot implementation of the solution, using actual customer data, showed that emissions and transportation costs can be reduced simultaneously by optimizing vehicle use, fuel types, and shipment consolidation. Achieving a 20%‰ÛÒ30% reduction in emission was possible with minimal cost increase.",0018-8646;00188646,,10.1147/JRD.2009.5429021,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5429021,,,,,,0,,,,no,9-May,,IBM,IBM Journals & Magazines
Case study: Visual analytics in software product assessments,A. Telea; L. Voinea,"Institute for Math. and Computer Science, University of Groningen, the Netherlands",2009 5th IEEE International Workshop on Visualizing Software for Understanding and Analysis,20091117,2009,,,65,72,"We present how a combination of static source code analysis, repository analysis, and visualization techniques has been used to effectively get and communicate insight in the development and project management problems of a large industrial code base. This study is an example of how visual analytics can be effectively applied to answer maintenance questions and support decision making in the software industry. We comment on the relevant findings during the study both in terms of used technique and applied methodology and outline the favorable factors that were essential in making this type of assessment successful within tight time and budget constraints.",,POD:978-1-4244-5027-5,10.1109/VISSOF.2009.5336417,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5336417,,Cement industry;Computer architecture;Computer industry;Concrete;Data analysis;Data visualization;Documentation;Embedded software;Project management;Visual analytics,data visualisation;decision making;program diagnostics;program visualisation;project management;software maintenance,decision making;industrial code base;project management problems;repository analysis;software industry;software maintenance;software product assessment;static source code analysis;visual analytics;visualization techniques,,4,,18,,no,25-26 Sept. 2009,,IEEE,IEEE Conference Publications
Chaos phenomenon in UC3842 current-programmed flyback converters,F. H. Hsieh; K. M. Lin; J. H. Su,"Department of Electrical Engineering, Lung-Hwa University of Science and Technology, Taoyuan, Taiwan",2009 4th IEEE Conference on Industrial Electronics and Applications,20090630,2009,,,166,171,"This study studies the chaos phenomenon in a UC3842 current-programmed flyback converter. The primary side magnetizing inductance current of a transformer during the conversion operation is considered in continuous-conduction mode (CCM). The nonlinear behaviors of system operation from period-1 to doubling-bifurcation and to chaos is studied by varying the input voltage of the converter. First, mathematical models and discrete-time analytic solutions of the flyback converter are derived. Second, MATLAB/SIMULINK software to simulate the nonlinear behaviors of the converter. Furthermore, the actual circuit is implemented, and experiments are performed. The consistency of simulation and experimental results confirm that the proposed mathematical models and discrete-time analytic solutions accurately model the nonlinear dynamical behaviors of this converter.",2156-2318;21562318,POD:978-1-4244-2799-4,10.1109/ICIEA.2009.5138190,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5138190,Flyback converter;UC3842;bifurcation;chaos,Bifurcation;Buck converters;Chaos;Circuit simulation;Inductance;Light emitting diodes;MATLAB;Magnetic analysis;Mathematical model;Switching converters,chaos;discrete time systems;nonlinear systems;power convertors,MATLAB/SIMULINK software;UC3842 current-programmed flyback converters;chaos phenomenon;continuous-conduction mode;discrete-time analytic solutions;nonlinear system behavior;transformer primary,,5,,8,,no,25-27 May 2009,,IEEE,IEEE Conference Publications
Coastal online analysis and synthesis tool 2.0 (COAST),R. B. Brown; A. R. Navard; B. T. Nguyen,"Science Systems and Applications, Inc., Bldg. 1105, John C. Stennis Space Center, MS 39529 USA",OCEANS 2009,20100301,2009,,,1,7,"The Coastal Online Assessment and Synthesis Tool (COAST) geobrowser has been developed at NASA Stennis Space Center (SSC) for integration of previously disparate coastal datasets from NASA and other sources into a common desktop client tool. COAST will provide insightful new data visualization and analysis capabilities for the coastal researcher. COAST is built upon the NASA open source 3D geobrowser, World Wind, developed at the NASA Ames Research Center. COAST also integrated some of the value-added modifications and enhancements from the NASA Marshall Space Flight Center version of World Wind, SERVIR-Viz. COAST is being developed to maximize use of open source data access, viewing, and data manipulation software tools, creating a low-cost, widely installable base for potential users. Feedback from preliminary reviewers has led to more robust understanding of the data integration and visual analytic challenges and of the potential solutions that COAST can offer to the broader user community. Improved mode of functionality for these users will lead to a more refined methodology for implementation of COAST as an effective tool for a range of potential users varying from researcher to investigator to potential decision maker. Development of the Temporal Visualization Tool (TVT) plugin for COAST was begun in the 2007 Integrated Approach to Monitoring Hypoxia in the Northern Gulf of Mexico project. The origin of this time-based animated data overlay tool is the Naval Research Laboratory Monterey Weather plugin, which is still distributed with the present World Wind 1.4 package. Modifications to the TVT tool have been targeted to provide users the capability to connect to and map/integrate disparate datasets, located locally and online, into project sessions. The TVT allows direct data listing of accessible raster datasets, subsequent multi-select, temporally animated image overlays in the COAST browser, and transparency control over the animated layer within COAST via - a slider mechanism. The development of the Recursive Online Remote Data - Data Mapper (RECORD-DM) utility was driven by the need for an ability to map and add online remote image-product datasets to the TVT plugin's list of available images as needed. The RECORD-DM tool allows a user to map the current state, structure, and location of online raster data available for viewing in TVT. It also allows geographic position information to be attached and creates an XML file map of the data for immediate use in the TVT as either static or temporally animated overlays in the current COAST session. The Import Data Tool provides the ability to quickly add image and vector datasets in a COAST session without having to be a geospatial or image processing expert. The envisioned COAST end user community can vary from seasoned research scientists wanting to integrate decision model output into their sessions all the way to coastal community managers wanting to review local, state, and federal data products in their areas of interest.",0197-7385;01977385,Electronic:978-0-933957-38-1; POD:978-1-4244-4960-6,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5422062,,Animation;Data analysis;Data visualization;Feedback;NASA;Robustness;Sea measurements;Software tools;Visual analytics;Wind,XML;data visualisation;geophysics computing;information networks;information retrieval;oceanography;online front-ends;public domain software,COAST 2.0;COAST geobrowser;COAST temporal visualization tool plugin;Coastal Online Analysis and Synthesis Tool 2.0;NASA open source 3D geobrowser;RECORD-DM utility;SERVIR-Viz;TVT tool modifications;World Wind geobrowser;XML file map;accessible raster datasets;coastal datasets;common desktop client tool;direct data listing;geographic position information;import data tool;open source data access;open source data manipulation software tools;open source data viewing;recursive online remote data-data mapper;remote image product datasets;time based animated data overlay tool,,0,,2,,no,26-29 Oct. 2009,,IEEE,IEEE Conference Publications
Combining iterative analytical reasoning and software development using the visualization language Processing,C. Muller-Birn; L. Birn,"Carnegie Mellon University, USA",2009 IEEE Symposium on Visual Analytics Science and Technology,20091113,2009,,,,,"Processing is a very powerful visualization language which combines software concepts with principles of visual form and interaction. Artists, designers and architects use it but it is also a very effective programming language in the area of visual analytics. In the following contribution Processing is utilized in order to visually analyze data provided by IEEE VAST 2009 Mini Challenge Badge and Network Traffic. The applied process is iterative and each stage of the analytical reasoning process is accompanied by customized software development. The visual model, the process and the technical solution will be briefly introduced.",,POD:978-1-4244-5283-5,10.1109/VAST.2009.5334463,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5334463,D.2.3 [Coding Tools and Techniques]: Object-oriented programming;H.1.2 [User/Machine Systems]: Visual Analytics,Computer languages;Data visualization;Object oriented modeling;Programming;Software tools;Telecommunication traffic;Traffic control;Visual analytics,data mining;data visualisation;program visualisation;visual languages,Processing;customized software development;iterative analytical reasoning;programming language;visualization language,,0,,4,,no,12-13 Oct. 2009,,IEEE,IEEE Conference Publications
Comparative Analysis of System Evaluation Methods,X. Chen; H. Yang; Z. Sheng,"Sch. of Econ., Univ. of Jinan, Jinan, China",2009 First International Conference on Information Science and Engineering,20100426,2009,,,4223,4226,"This paper analyzes the classifications, foundations of the theory, principles, advantages, disadvantages, and the main application scope of common system evaluation methods including fuzzy comprehensive evaluation, grey forecasting method, technology-economy analytical method, principal component analysis, discrimination analysis, analytic hierarchy process and intelligent evaluation. Then these system evaluation methods are analyzed comparatively from several different viewpoints. Finally some problems needing attention in choosing system evaluation methods are proposed and the conclusions are given.",2160-1283;21601283,Electronic:978-1-4244-5728-1; POD:978-1-4244-4909-5,10.1109/ICISE.2009.407,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5455497,,Application software;Computer simulation;Data envelopment analysis;Economic forecasting;Fuzzy set theory;Fuzzy systems;Information analysis;Investments;Principal component analysis;Technology forecasting,decision making;decision theory;fuzzy set theory;grey systems;principal component analysis,analytic hierarchy process;discrimination analysis;fuzzy comprehensive evaluation;grey forecasting method;intelligent evaluation;principal component analysis;system evaluation method;technology economy analytical method,,0,,7,,no,26-28 Dec. 2009,,IEEE,IEEE Conference Publications
"Congestion location detection: Methodology, algorithm, and performance",Shao Liu; Mung Chiang; M. Jourdain; Jin Li,"Department of Electrical Engineering, Princeton University, USA",2009 17th International Workshop on Quality of Service,20090818,2009,,,1,9,"We address the following question in this study: Can a network application detect not only the occurrence, but also the location of congestion? Answering this question will not only help the diagnostic of network failure and monitor server's QoS, but also help developers to engineer transport protocols with more desirable congestion avoidance behavior. The paper answers this question through new analytic results on the two underlying technical difficulties: 1) synchronization effects of loss and delay in TCP, and 2) distributed hypothesis testing using only local loss and delay data. We present a practical congestion location detection (CLD) algorithm that effectively allows an end host to distributively detect whether congestion happens in the local access link or in more remote links. We validate the effectiveness of CLD algorithm with extensive experiments.",1548-615X;1548615X,Electronic:978-1-4244-3876-1; POD:978-1-4244-3875-4,10.1109/IWQoS.2009.5201404,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5201404,,Application software;Bandwidth;Communication system traffic control;Cross layer design;Delay effects;Internet;Streaming media;Testing;Transport protocols;Web server,computer networks;quality of service;synchronisation;telecommunication congestion control;transport protocols,CLD algorithm;QoS;congestion avoidance behavior;congestion location detection;distributed hypothesis testing;local access link;network application;network failure;remote links;synchronization effect;transport protocol,,1,,21,,no,13-15 July 2009,,IEEE,IEEE Conference Publications
Decision of Air Conditioning Cold and Heat Sources Based on Analytic Hierarchy Process,S. Hu; D. Pan; X. Li; R. Liu; D. Meng,"Sch. of Environ. & Municipal Eng., Qingdao Technol. Univ., Qingdao, China",2009 International Conference on Energy and Environment Technology,20091228,2009,1,,71,74,"The optimization model of analytic hierarchy process (AHP) is introduced firstly, and then using it, air conditioning cold and heat sources are decided in a project of Qingdao City. Considering economy, technical condition, environmental impact and social benefit, each of four schemes of air conditioning cold and heat sources is analyzed quantitatively by Excel software. Finally sea water resource heat pump system is selected. The results show that the AHP Excel algorithm is convenience in applying, but the application of this method should be based on specific cases.",,POD:978-0-7695-3819-8,10.1109/ICEET.2009.24,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5366343,Analytic Hierarchy Process;Excel software;air conditioning cold and heat source,Air conditioning;Application software;Cooling;Heat engines;Heat pumps;Heat recovery;Knowledge engineering;Power engineering and energy;Springs;Water heating,air conditioning;decision making;environmental factors;heat pumps;optimisation;socio-economic effects;water resources,AHP;Excel software;air conditioning;analytic hierarchy process;cold sources;decision making;economical benefits;environmental impact;heat pump system;heat sources;optimization;sea water resources;social benefits,,0,,7,,no,16-18 Oct. 2009,,IEEE,IEEE Conference Publications
Defending Mobile Phones from Proximity Malware,G. Zyba; G. M. Voelker; M. Liljenstam; A. Mehes; P. Johansson,"Dept. of Comput. Sci. & Eng., Univ. of California, La Jolla, CA",IEEE INFOCOM 2009,20090602,2009,,,1503,1511,"As mobile phones increasingly become the target of propagating malware, their use of direct pair-wise communication mechanisms, such as Bluetooth and WiFi, pose considerable challenges to malware detection and mitigation. Unlike malware that propagates using the network, where the provider can employ centralized defenses, proximity malware can propagate in an entirely distributed fashion. In this paper we consider the dynamics of mobile phone malware that propagates by proximity contact, and we evaluate potential defenses against it. Defending against proximity malware is particularly challenging since it is difficult to piece together global dynamics from just pair-wise device interactions. Whereas traditional network defenses depend upon observing aggregated network activity to detect correlated or anomalous behavior, proximity malware detection must begin at the device. As a result, we explore three strategies for detecting and mitigating proximity malware that span the spectrum from simple local detection to a globally coordinated defense. Using insight from a combination of real-world traces, analytic epidemic models, and synthetic mobility models, we simulate proximity malware propagation and defense at the scale of a university campus. We find that local proximity-based dissemination of signatures can limit malware propagation. Globally coordinated strategies with broadcast dissemination are substantially more effective, but rely upon more demanding infrastructure within the provider.",0743-166X;0743166X,POD:978-1-4244-3512-8,10.1109/INFCOM.2009.5062067,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5062067,,Analytical models;Bluetooth;Broadcasting;Communications Society;Computer science;Information technology;Mobile communication;Mobile handsets;Propagation losses;USA Councils,invasive software;mobile radio,analytic epidemic models;direct pair-wise communication mechanisms;malware detection;mobile phones;proximity malware;synthetic mobility models,,19,,22,,no,19-25 April 2009,,IEEE,IEEE Conference Publications
Deformation Analytic Computation of Throttle Slice of Shock Absorber,C. Zhou; C. Liu; L. Zhao,"Sch. of Transp. & Vehicle Eng., Shandong Univ. of Technol., Zibo, China",2009 International Conference on Computational Intelligence and Natural Computing,20090904,2009,2,,359,361,"Based on the governing differential equation for deformation of throttle slice, while satisfying required boundary conditions, an analytical formula for computing the deformation of throttle slice was presented through equivalency transformation, which is a concise, accurate and practical method for throttle slice design. The deformation at any radius was computed, compared with ANSYS FEA software by the simulation analysis. The result shows that the new method is an accurate analytical method for computing the deformation of throttle slice at any radius, suitable to use in the design of valves parameters.",,POD:978-0-7695-3645-3,10.1109/CINC.2009.264,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5230954,analytic computation;deformation;mathematical model;shock absorber;throttle slice,Automotive engineering;Boundary conditions;Computational intelligence;Design engineering;Differential equations;Elasticity;Finite element methods;Intelligent vehicles;Shock absorbers;Valves,deformation;differential equations;finite element analysis;shock absorbers;stress analysis,ANSYS FEA software;deformation analytic computation;differential equation;equivalency transformation;shock absorber throttle slice;throttle slice design;valve parameter design,,0,,6,,no,6-7 June 2009,,IEEE,IEEE Conference Publications
Demo VI Emergent analytics for enterprise management,M. Lang,"Revelytix Inc., Hunt Valley, Maryland, USA",2009 International Symposium on Collaborative Technologies and Systems,20090605,2009,,,xli,xlii,"This talk will present the results of an Army sponsored project. The objective of the project was to develop a semantic knowledgebase designed to offer increased visibility into the Army portfolio. The first step in this project was to develop a seed ontology of the Army's enterprise architecture. The process used to develop the ontology was driven by a set of analytic questions that needed to be answered by queries to the knowledgebase. It was a collaborative, bottom-up effort from start to finish. The members of the team, ontologists and subject matter experts, worked together using a semantic wiki to gain consensus on the structure and meaning of the ontology. Once a suitable seed ontology was created, the project was increased in scale and the team began to collect large amounts of data, primarily about programs, software services, and capabilities within the Army's portfolio, as well as common business processes, missions and operational environments. This data came from many different systems within the Army and other DoD agencies, it was integrated using the enterprise architecture ontology. As the knowledgebase was populated with data, the ontology continued to evolve within the collaborative environment to respond to new data sets and data needs. The knowledgebase has been used to analyze the contents of the Army portfolio enabling the Army to make more informed budgetary decisions and also gain a vastly better understanding of the software services that they have and how they can be combined and used to offer new, vital capabilities to soldiers at the tactical edge.",,POD:978-1-4244-4584-4,10.1109/CTS.2009.5067441,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5067441,,Biographies;Collaborative tools;Collaborative work;Companies;Engineering profession;Information systems;International collaboration;Ontologies;Portfolios;Technology management,,,,0,,,,no,18-22 May 2009,,IEEE,IEEE Conference Publications
Demystifying Visual Analytics,C. Chabot,Tableau Software,IEEE Computer Graphics and Applications,20090304,2009,29,2,84,87,"Visual analytics is the science of analytical reasoning facilitated by interactive visual interfaces. Technologies based on visual analysis are moving from research into widespread use, driven by the increased power of analytical databases and computer graphics hardware.",0272-1716;02721716,,10.1109/MCG.2009.23,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4797520,analytical databases;business intelligence;data visualization;data warehouses;information visualization;visual analysis;visual analytics,Computer graphics;Conference proceedings;Data analysis;Data visualization;Hardware;Mutual funds;Packaging;Proposals;Visual analytics;Visual databases,data visualisation,analytical reasoning;data visualization;visual analytics,,6,,1,,no,March-April 2009,,IEEE,IEEE Journals & Magazines
Dependable integrated surveillance systems for the physical security of metro railways,G. Bocchetti; F. Flammini; A. Pappalardo,"Concetta Pragliola, Ansaldo STS Italy, Naples, Italy",2009 Third ACM/IEEE International Conference on Distributed Smart Cameras (ICDSC),20091020,2009,,,1,7,"Rail-based mass transit systems are vulnerable to many criminal acts, ranging from vandalism to terrorism. In this paper, we present the architecture, the main functionalities and the dependability related issues of a security system specifically tailored to metro railways. Heterogeneous intrusion detection, access control, intelligent video-surveillance and sound detection devices are integrated in a cohesive Security Management System (SMS). In case of emergencies, the procedural actions required to the operators involved are orchestrated by the SMS. Redundancy both in sensor dislocation and hardware apparels (e.g. by local or geographical clustering) improve detection reliability, through alarm correlation, and overall system resiliency against both random and malicious threats. Video-analytics is essential, since a small number of operators would be unable to visually control a large number of cameras. Therefore, the visualization of video streams is activated automatically when an alarm is generated by smart-cameras or other sensors, according to an event-driven approach. The system is able to protect stations (accesses, technical rooms, platforms, etc.), tunnels (portals, ventilation shafts, etc.), trains and depots. Presently, the system is being installed in the Metrocampania underground regional railway. To the best of our knowledge, this is the first subway security system featuring artificial intelligence algorithms both for video and audio surveillance. The security system is highly heterogeneous in terms not only of detection technologies but also of embedded computing power and communication facilities. In fact, sensors can differ in their inner hardware-software architecture and thus in the capacity of providing information security and dependability. The focus of this paper is on the development of novel solutions to achieve a measurable level of dependability for the security system in order to fulfill the requirements of the specific application.",,CD-ROM:978-1-4244-4620-9,10.1109/ICDSC.2009.5289385,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5289385,Smart surveillance;critical infrastructure protection;detection reliability;physical security;system dependability,Access control;Communication system security;Information security;Intelligent control;Intelligent sensors;Intrusion detection;Power system security;Rail transportation;Surveillance;Terrorism,hardware-software codesign;railway engineering;security of data;video streaming;video surveillance,Metrocampania underground regional railway;access control;alarm correlation;artificial intelligence;audio surveillance;communication facilities;criminal acts;detection reliability;embedded computing power;event driven approach;hardware apparel;hardware-software architecture;heterogeneous intrusion detection;information security;integrated surveillance systems;intelligent video surveillance;malicious threat;metro railways;physical security;rail-based mass transit systems;security management system;sensor dislocation;smart cameras;sound detection device;subway security system;system resiliency;terrorism;vandalism;video analytics;video streams,,10,,16,,no,Aug. 30 2009-Sept. 2 2009,,IEEE,IEEE Conference Publications
Dependency Analysis Framework for Software Service Delivery,R. Ananthanarayanan; V. Chenthamarakshan; H. Chu; P. M. Deshpande; R. Krishnapuram; S. K. Mohammed,"Res. Labs., IBM India, Bangalore, India",2009 IEEE International Conference on Services Computing,20091013,2009,,,89,96,"Various phases in the delivery of software services such as solution design, application deployment, and maintenance require analysis of the dependencies of software products that form the solution. As software systems become more complex and involve a large number of software products from multiple vendors, availability of correct and up-to-date system requirement information becomes critical to ensure proper functioning of managed and maintained software solutions. System requirement information, is mostly made available in unstructured formats from sources such as websites or product documents and are not amenable to programmatic analysis. In this paper, we motivate the benefits of capturing this information in a structured format for software service delivery, and present a dependency analysis system that collects and integrates software dependency/interoperability information from multiple unstructured sources using text mining techniques. Information hence collected, is used to support analytics useful in software service delivery. We report the results of our experiments on mining millions of web pages to collect dependency information for more than 700 software products.",,POD:978-1-4244-5183-8,10.1109/SCC.2009.71,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5284017,Decision support systems;Information systems;Knowledge based systems,Application software;Data mining;Databases;Hardware;Information analysis;Operating systems;Software maintenance;Software systems;Software tools;Text mining,Internet;data mining;text analysis,Webpage;dependency analysis framework;software interoperability;software service delivery;system requirement information;text mining technique,,2,,9,,no,21-25 Sept. 2009,,IEEE,IEEE Conference Publications
Design for configurability: rethinking interdomain routing policies from the ground up,Y. Wang; I. Avramopoulos; J. Rexford,"Dept. of Comput. Sci., Princeton Univ., Princeton, NJ",IEEE Journal on Selected Areas in Communications,20090403,2009,27,3,336,348,"Giving ISPs more fine-grain control over interdomain routing policies would help them better manage their networks and offer value-added services to their customers. Unfortunately, the current BGP route-selection process imposes inherent restrictions on the policies an ISP can configure, making many useful policies infeasible. In this paper, we present Morpheus, a routing control platform that is designed for configurability. Morpheus enables a single ISP to safely realize a much broader range of routing policies without requiring changes to the underlying routers or the BGP protocol itself. Morpheus allows network operators to: (1) make flexible trade-offs between policy objectives through a weighted-sum based decision process, (2) realize customer-specific policies by supporting multiple route-selection processes in parallel, and allowing customers to influence the decision processes, and (3) configure the decision processes through a simple and intuitive configuration interface based on the Analytic Hierarchy Process, a decision-theoretic technique for balancing conflicting objectives. We also present the design, implementation, and evaluation of Morpheus as an extension to the XORP software router.",0733-8716;07338716,,10.1109/JSAC.2009.090409,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4808477,"BGP, interdomain routing, policy, configuration, analytic hierarchy process (AHP)",Computer architecture;Computer science;IP networks;Internet;Laboratories;Performance analysis;Protection;Routing protocols;Scalability;Service oriented architecture,computer networks;routing protocols,BGP;ISP;Morpheus;XORP software router;analytic hierarchy process;fine-grain control;interdomain routing policy;routing policies,,9,1,42,,no,9-Apr,,IEEE,IEEE Journals & Magazines
Design Quality Analytics of Traceability Enablement in Service-Oriented Solution Design Environment,L. J. Zhang; Z. H. Mao; N. Zhou,"IBM T.J. Watson Res. Center, Hawthorne, NY, USA",2009 IEEE International Conference on Web Services,20090731,2009,,,944,951,"This paper provides an artifact-pattern-matching framework and mathematical model to analyze the dynamic behaviors of the SOA solution design in model driven fashion and provide recommendations for optimal solution pattern enablement for solution artifacts. The artifact-pattern-matching system can be dynamically tuned based on the practitionerspsila final selections of there commendations. Specifically, we propose a set of solution patterns to guide SOA solution architects through the process of consuming and configuring SOA artifacts for composing SOA solutions. The resulting multi-dimensional cascading flagging method is also presented in this paper. As an example, impact analysis patterns are used as solution patterns to support traceability enablement. We present some future directions of leveraging reinforcement learning algorithms to enrich the design quality analytics of SOA solution.",,POD:978-0-7695-3709-2,10.1109/ICWS.2009.145,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5175917,Artifact-pattern-matching;SOA;design quality analytics;traceability enablement,Algorithm design and analysis;Industrial relations;Mathematical model;Pattern analysis;Pattern matching;Semiconductor optical amplifiers;Service oriented architecture;USA Councils;Web services;XML,learning (artificial intelligence);pattern matching;software architecture;software quality,artifact-pattern-matching system;multidimensional cascading flagging method;reinforcement learning algorithm;service-oriented architecture;service-oriented solution design environment,,4,,8,,no,6-10 July 2009,,IEEE,IEEE Conference Publications
E-Learning Platform Evaluation Using Fuzzy AHP,Q. Liu; R. Peng; A. Chen; J. Xie,"Mech. Eng. Sch., Univ. of South China, Hengyang, China",2009 International Conference on Computational Intelligence and Software Engineering,20091228,2009,,,1,4,"E-learning platform evaluation is one of the most important issues in e-learning platform management. It is a multi-criteria decision problem including various factors. Traditional methods have many shortcomings. Fuzzy AHP model is established to solve this problem. A general evaluation method is provided and steps are presented. As a case study, the model is implemented and a detailed example is given to illustrate the effectiveness of this approach.",,CD-ROM:978-1-4244-4507-3,10.1109/CISE.2009.5366686,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5366686,,Buildings;Education;Educational programs;Electronic learning;Engineering management;Information technology;Internet;Matrix converters;Mechanical engineering;Psychology,computer aided instruction;fuzzy set theory,analytic hierarchy process;e-learning platform management;fuzzy AHP model,,0,,6,,no,11-13 Dec. 2009,,IEEE,IEEE Conference Publications
Elastic scaling of data parallel operators in stream processing,S. Schneider; H. Andrade; B. Gedik; A. Biem; K. L. Wu,"Virginia Tech, Department of Computer Science, Blacksburg, VA, USA",2009 IEEE International Symposium on Parallel & Distributed Processing,20090710,2009,,,1,12,"We describe an approach to elastically scale the performance of a data analytics operator that is part of a streaming application. Our techniques focus on dynamically adjusting the amount of computation an operator can carry out in response to changes in incoming workload and the availability of processing cycles. We show that our elastic approach is beneficial in light of the dynamic aspects of streaming workloads and stream processing environments. Addressing another recent trend, we show the importance of our approach as a means to providing computational elasticity in multicore processor-based environments such that operators can automatically find their best operating point. Finally, we present experiments driven by synthetic workloads, showing the space where the optimizing efforts are most beneficial and a radioastronomy imaging application, where we observe substantial improvements in its performance-critical section.",1530-2075;15302075,POD:978-1-4244-3751-1,10.1109/IPDPS.2009.5161036,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5161036,,Application software;Availability;Computer science;Data analysis;Elasticity;Intelligent sensors;Multicore processing;Performance analysis;Runtime;Streaming media,media streaming;multiprocessing systems;parallel processing,computational elasticity;data analytics operator;data parallel operator;elastic scaling;multicore processor;radioastronomy imaging;stream processing environment;workload streaming,,24,7,24,,no,23-29 May 2009,,IEEE,IEEE Conference Publications
Embedded systems design - Scientific challenges and work directions,J. Sifakis,"CNRS/Verimag, France","2009 Design, Automation & Test in Europe Conference & Exhibition",20090623,2009,,,2,2,"Summary form only given. The development of a satisfactory Embedded Systems Design Science provides a timely challenge and opportunity for reinvigorating Computer Science. Embedded systems are components integrating software and hardware jointly and specifically designed to provide given functionalities, which are often critical. They are used in many applications areas including transport, consumer electronics and electrical appliances, energy distribution, manufacturing systems etc. Embedded systems design requires techniques taking into account extra-functional requirements regarding optimal use of resources such as time, memory and energy while ensuring autonomy, reactivity and robustness. Jointly taking into account these requirements raises a grand scientific and technical challenge extending Computer Science with paradigms and methods from Control Theory and Electrical Engineering. Computer Science is based on discrete computation models not encompassing physical time and resources which are by their nature very different from analytic models used by other engineering disciplines. We summarise some current trends in embedded systems design and point out some of their characteristics, such as the chasm between analytical and computational models and the gap between safety critical and best-effort engineering practices. We call for a coherent scientific foundation for embedded systems design, and we discuss a few key demands on such a foundation: the need for encompassing several manifestations of heterogeneity, and the need for design paradigms ensuring constructivity and adaptivity. We discuss main aspects of this challenge and associated research directions for different areas such as modelling, programming, compilers, operating systems and networks.",1530-1591;15301591,POD:978-1-4244-3781-8,10.1109/DATE.2009.5090623,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5090623,,Application software;Computational modeling;Computer science;Consumer electronics;Electrical products;Embedded software;Embedded system;Hardware;Manufacturing systems;Robustness,embedded systems;operating systems (computers);program compilers;software tools,adaptivity;best-effort engineering;chasm;compilers;constructivity;design paradigms;embedded systems design;modelling;networks;operating systems;programming;safety critical engineering;scientific challenges,,1,,,,no,20-24 April 2009,,IEEE,IEEE Conference Publications
Evaluating the risk of cyber attacks on SCADA systems via Petri net analysis with application to hazardous liquid loading operations,M. H. Henry; R. M. Layer; K. Z. Snow; D. R. Zaret,"The Johns Hopkins University Applied Physics Laboratory (JHU/APL), 11100 Johns Hopkins Road, Laurel, MD 20723-6099, USA",2009 IEEE Conference on Technologies for Homeland Security,20090721,2009,,,607,614,"This paper develops an analytic technique for quantifying the risk of computer network operations (CNO) against supervisory control and data acquisition (SCADA) systems. We measure risk in terms of the extent to which an attacker can manipulate process control elements, the consequences due to disruption of the controlled physical process, and the vulnerability of the SCADA system to malicious intrusion. The technique constitutes a novel application of Petri net state coverability analysis coupled with process simulation. As such, this framework permits a formal assessment of candidate policies to manage risk by diminishing aspects of the network vulnerability to intrusion, where the objective is to prevent malicious induction of catastrophic process failure modes. We extend earlier work on Petri nets for attack analysis by developing a detailed methodology including: a new algorithm for the automatic generation of Petri nets from the description of a SCADA network and its vulnerabilities; metrics for quantifying risk as a function of a Petri net's state; techniques for evaluating these metrics based on a Petri net's minimal coverability set; and a method for coupling the Petri net representation of the SCADA network to the controlled processes for failure mode and effects assessment. The paper concludes by presenting an example application of the analysis technique to evaluate the security of a hazardous liquid loading process.",,POD:978-1-4244-4178-5,10.1109/THS.2009.5168093,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5168093,Petri nets;attack modeling;scalability;security metrics;stateful attack analysis,Analytical models;Application software;Computer networks;Control systems;Failure analysis;Petri nets;Process control;Risk analysis;Risk management;SCADA systems,Petri nets;SCADA systems;hazardous materials;risk analysis;security of data,Petri net analysis;SCADA systems;attack analysis;candidate policies formal assessment;catastrophic process failure;computer network operations;cyber attacks;data acquisition systems;hazardous liquid loading operations;malicious induction;malicious intrusion;network vulnerability;process control;risk manage;state coverability analysis;supervisory control systems,,5,,24,,no,11-12 May 2009,,IEEE,IEEE Conference Publications
Evaluation Indicator System and Weights Research of Communication Effects in ERP Implementation Project,Y. Zhang; Y. Xu,"Inf. Sch., Renmin Univ. of China (RUC), Beijing, China",2009 International Forum on Computer Science-Technology and Applications,20100119,2009,3,,423,427,"Based on communication theory and characteristics of communication in ERP implementation project, this paper focuses on communication influencing factors analysis and proposes a new two level evaluation indicator system of communication effects for ERP implementation project. Besides, this paper carries on a weights research of the indicator system by means of the analytic hierarchy process (AHP) method, questionnaire survey and expert scoring method. The indicator system and weights research can serve as a basis to evaluate communication effects in the context of ERP implementation project and help management team to prioritize their attentions according to the weights of elements. Furthermore, this paper is an exploratory research about communication in ERP implementation project and provides rudimental idea for later research.",,Electronic:978-1-4244-5423-5; POD:978-0-7695-3930-0; POD:978-1-4244-5422-8,10.1109/IFCSTA.2009.343,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5384911,AHP;ERP implementation;communication;evaluation indicator system;weights analysis,Application software;Attenuation;Communication system control;Computer applications;Context;Enterprise resource planning;Failure analysis;Information analysis;Information filtering;Project management,business communication;decision making;enterprise resource planning,ERP implementation project;analytic hierarchy process method;communication effects;communication influencing factors analysis;evaluation indicator system;expert scoring method;management team;questionnaire survey;weights research,,1,,13,,no,25-27 Dec. 2009,,IEEE,IEEE Conference Publications
Evaluation Model for Computer Network Information Security Based on Analytic Hierarchy Process,X. Zhen-yuan; C. He; W. Xiang-zhong; S. Jian-ling; F. Yu-tao,"North China Inst. of Sci. & Technol., Beijing, China",2009 Third International Symposium on Intelligent Information Technology Application,20091231,2009,3,,186,189,"Evaluation for computer network information security is helpful for taking corresponding preventive measures. In order to obtain a comprehensive assessment of network security, analytic hierarchy process (AHP) model is proposed to assess the computer network information security. As the criteria and the relevant factors are decomposed hierarchically corresponding to evaluation and judgment of the problem, all kinds of factors of influencing network security are researched and the evaluation indexes for computer network information security are constructed, analytic hierarchy process (AHP) evaluation model for computer network information security is constructed on the basis of the evaluation indexes. The experimental results indicate that the evaluation of computer network information security by analytic hierarchy process is effective.",,POD:978-0-7695-3859-4,10.1109/IITA.2009.95,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5369486,,Application software;Computer networks;Computer security;Data security;Hardware;Information analysis;Information security;Information technology;Intelligent networks;Software safety,computer network security,analytic hierarchy process;computer network information security;network security;preventive measures,,0,,6,,no,21-22 Nov. 2009,,IEEE,IEEE Conference Publications
Evaluation of Image Spam Classification System Based on AHP,Z. Xu; H. g. Wang; Z. z. Shao,"Dept., Inf. Sci. & Eng., Shandong Normal Univ., Jinan, China",2009 International Conference on Computational Intelligence and Software Engineering,20091228,2009,,,1,4,"The ever increasing volumes of image-based spam e-mails are bringing more annoyance to Internet users, and how to filter it has become a pressing problem. This paper discussed the evaluation problem of image spam classification system using analytic hierarchy process (AHP). First propose the evaluating index system and build hierarchy structure model; Then build the comparative matrixes according to the rate of contribution the variant factors applied to the target; Finally calculate the index weight of each layer. At the end of this paper, the experiment results verify the correctness of our methods.",,CD-ROM:978-1-4244-4507-3,10.1109/CISE.2009.5366905,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5366905,,Computational efficiency;Electronic mail;Humans;Image analysis;Information filtering;Information filters;Information science;Internet;Pressing;Unsolicited electronic mail,Internet;classification;decision making;image processing;indexing;information filtering;unsolicited e-mail,Internet;analytic hierarchy process;email filtering;evaluating index system;image spam classification system;image-based spam e-mail,,0,,8,,no,11-13 Dec. 2009,,IEEE,IEEE Conference Publications
Evaluation of Management Information System Based on Fuzzy AHP and Multiple Matter Elements,J. Zhigang,"Dept. of the Libr., Hebei Univ. of Eng., Handan, China","2009 Pacific-Asia Conference on Circuits, Communications and Systems",20090904,2009,,,657,660,"It is a difficult task to do evaluation of management information systems, because the information systems have a lot of characteristics. According to the function requirements of management information systems (MIS), and in combination with the characteristics of MIS, the evaluation index system for MIS was established. We combine of fuzzy analytic hierarchy process and multiple matter elements to evaluate the management information system. In the end a case is used to test the validate atonality and reliability of the method. This electronic document is a ldquoliverdquo template.",,POD:978-0-7695-3614-9,10.1109/PACCS.2009.130,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5231968,fuzzy analytic hierarchy process;information systems;multiple matter elements,Circuits;Costs;Fuzzy systems;Information analysis;Information systems;Inspection;Investments;Management information systems;Performance analysis;Software maintenance,decision making;fuzzy set theory;management information systems,electronic document;evaluation index system;fuzzy AHP;fuzzy analytic hierarchy process;management information system;multiple matter elements,,0,,9,,no,16-17 May 2009,,IEEE,IEEE Conference Publications
Evaluation of the Current Situation and Planning of the Greenspace System in Huaibei City Using GIS Based Network Analysis,Z. Zhang; J. P. Wang,"Sch. of Urban Design, Wuhan Univ., Wuhan, China",2009 International Conference on Management and Service Science,20091030,2009,,,1,4,"In rapid urbanization, how to construct the rational landscape network have great significances in keeping the urban ecosystem healthy and improving the quantity and quality of the current greenspace system. The basic research materials are urban current land use, greenspace current situation, green space system planning and satellite images of Huaibei in 2006. Supported by GIS , the research chooses the main urban area in Huaibei with an area greater than 10 hm<sup>2</sup> of afforestation space as the node, according to the analytic approach of the network, the research establishes the idealized ecological networks, through index alpha , index beta , index gamma, compare their network structure integrality, choose the best network. The paper analyzes with the landscape analysis software Fragstats, uses the method of landscape pattern analytical. It selects the indexes of many kinds of analysis landscape for use, analyzes the systematic current situation of greenspace, greenspace system planning and the idealized ecological network scheme of Huaibei in terms of landscape ecology. It is an effective route to evaluate and improve the greenspace system planning with the ecological network analysis combines with the pattern analysis of the landscape, and the ecological network scheme planning to be made on the greenspace system planning can further improve main quantity and quality with systematic current greenspace of urban area.",,POD:978-1-4244-4638-4,10.1109/ICMSS.2009.5301262,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5301262,,Appraisal;Cities and towns;Ecosystems;Environmental factors;Geographic Information Systems;Land use planning;Pattern analysis;Satellites;Urban areas;Urban planning,ecology;geographic information systems;land use planning,Fragstats;GIS based network analysis;Huaibei city;ecological network;greenspace system;land use;landscape analysis software;landscape network;landscape pattern;network structure integrality;urban ecosystem;urbanization,,1,,9,,no,20-22 Sept. 2009,,IEEE,IEEE Conference Publications
Evaluation on Sustainable Development of Rapid Transit System Based on the Improved Entropy Method,X. Qingji; W. Xumin; Z. Xiaoling,"Dept. of Comput. & Sci., Nanyang Coll. of Technol., Nanyang, China",2009 Second International Conference on Intelligent Computation Technology and Automation,20091016,2009,4,,196,200,"Bus rapid transit system in the application of China's urban transportation is in the ascendant. With the technical advantages of rail transportation, it's economical, beneficial and having less impact on the environment. And the significance of sustainable development determines its high research value. This article establishes a comprehensive evaluation index system of sustainable development, using AHP to determine the target indicator system and improving entropy proportion means, experts scoring, qualitative and quantitative analysis to evaluate this new mode of urban transportation and its sustainable development. This article innovatively introduced the application of entropy right method in the choosing of urban transport mode on its sustainable development. To verify the reliability and simplicity of the method through empirical analysis, we have concluded that bus rapid transit system is relative superior.",,POD:978-0-7695-3804-4,10.1109/ICICTA.2009.914,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5288250,bus rapid transit system;entropy proportion means;evaluation index system;rail transit;sustainable development;transport planning,Application software;Appraisal;Automation;Educational institutions;Entropy;Human factors;Intelligent transportation systems;Rail transportation;Road transportation;Sustainable development,entropy;rapid transit systems;sustainable development,China urban transportation;analytic hierarchy process;bus rapid transit system;comprehensive evaluation index system;entropy proportion means;entropy right method;improved entropy method;rail transportation;sustainable development;target indicator system,,0,,4,,no,10-11 Oct. 2009,,IEEE,IEEE Conference Publications
Experiences in Global Software Development - A Framework-Based Analysis of Distributed Product Development Projects,M. T. Lane; P. J. Agerfalk,"Lero-The Irish Software Eng. Res. Centre, Univ. of Limerick, Limerick, Ireland",2009 Fourth IEEE International Conference on Global Software Engineering,20090807,2009,,,244,248,"Many authors have reported on various challenges and benefits encountered by teams engaged in global software development (GSD). Previous research has proposed a framework to structure these challenges and benefits within dimensions of distance and process. In this paper, the framework was used as an analytic device to investigate various projects performed by distributed teams in order to explore further the mechanisms used in industry both to overcome obstacles posed by distance and process challenges and also to exploit potential benefits enabled by global software development.",2329-6305;23296305,POD:978-0-7695-3710-8,10.1109/ICGSE.2009.32,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5196938,Distributed team configurations;Global Software Development,Communication system control;Costs;Face detection;Job production systems;Meeting planning;Packaging;Product development;Programming;Software engineering;Software packages,software engineering,distributed product development projects;global software development,,1,,8,,no,13-16 July 2009,,IEEE,IEEE Conference Publications
Exploring Temporal Egocentric Networks in Mobile Call Graphs,Q. Ye; B. Wu; D. Hu; B. Wang,"Beijing Key Lab. of Intell. Telecommun. Software & Multimedia, Beijing Univ. of Posts & Telecommun., Beijing, China",2009 Sixth International Conference on Fuzzy Systems and Knowledge Discovery,20091228,2009,2,,413,417,"The structure of customer communication network provides us the insights into the function of customers' relationships. In this paper, we use egocentric social network to explore how people manage their personal and group communications over time. Our primary goal is that our findings can provide business insights and help devise strategies for telecom service providers. We are interested in tracking changes in large-scale mobile networks and examining the evolution processes of customer egocentric networks. By defining several statistical metrics, we can investigate the egocentric networks' evolution trends and their communication patterns. We explore several temporal real-world mobile call graphs and find an interesting phenomenon in these temporal networks which is the neighboring vertices' egocentric networks have assortiative evolution trends. By taking a visual analytics approach, we track the changes in the customer egocentric networks and explore some highly correlated customers' egocentric networks visually. We detect several interesting communication patterns by visualizing the egocentric networks which may give us more hints on customers' communication trends in their egocentric networks.",,POD:978-0-7695-3735-1,10.1109/FSKD.2009.617,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5359482,,Communication networks;Evolution (biology);Information analysis;Large-scale systems;Mobile communication;Pattern analysis;Social network services;Telecommunication services;Visual analytics;Visualization,business data processing;graph theory;mobile computing,customer communication network;customer egocentric networks;egocentric social network;large-scale mobile networks;mobile call graphs;statistical metrics;telecom service providers;temporal egocentric networks,,1,,10,,no,14-16 Aug. 2009,,IEEE,IEEE Conference Publications
Fault-Tolerant Algorithm for Distributed Primary Detection in Cognitive Radio Networks,H. Qin; Y. Du; J. Su,"Comput. Sch., Yangtze Univ., Jingzhou","2009 International Conference on Networks Security, Wireless Communications and Trusted Computing",20090505,2009,1,,353,356,"This paper attempts to identify the reliability of detection of licensed primary transmission based on cooperative sensing in cognitive radio networks. With a parallel fusion network model, the correlation issue of the received signals between the nodes in the worst case is derived. Leveraging the property of false sensing data due to malfunctioning or malicious software, the optimizing strategy, namely fault-tolerant algorithm for distributed detection (FTDD) is proposed, and quantitative analysis of false alarm reliability and detection probability under the scheme is presented. In particular, the tradeoff between licensed transmissions and user cooperation among nodes is discussed. Simulation experiments are also used to evaluate the fusion performance under practical settings. The model and analytic results provide useful tools for reliability analysis for other wireless decentralization-based applications (e.g., those involving robust spectrum sensing).",,POD:978-0-7695-3610-1,10.1109/NSWCTC.2009.149,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4908281,cooperative communications;data fusion;distributed event detection;parallel fusion network;spectrum sensing,Chromium;Cognitive radio;Computer network reliability;Computer networks;Distributed computing;Fault detection;Fault tolerance;Robustness;Software algorithms;Telecommunication network reliability,cognitive radio;fault tolerance;probability;telecommunication network reliability,cognitive radio networks;cooperative sensing;detection probability;distributed primary detection;false alarm reliability;false sensing data;fault-tolerant algorithm;licensed primary transmission;optimizing strategy;parallel fusion network model;quantitative analysis;spectrum sensing,,0,,11,,no,25-26 April 2009,,IEEE,IEEE Conference Publications
FinVis: Applied visual analytics for personal financial planning,S. Rudolph; A. Savikhin; D. S. Ebert,"Purdue University Regional Visualization and Analytics Center (PURVAC), USA",2009 IEEE Symposium on Visual Analytics Science and Technology,20091113,2009,,,195,202,"FinVis is a visual analytics tool that allows the non-expert casual user to interpret the return, risk and correlation aspects of financial data and make personal finance decisions. This interactive exploratory tool helps the casual decision-maker quickly choose between various financial portfolio options and view possible outcomes. FinVis allows for exploration of inter-temporal data to analyze outcomes of short-term or long-term investment decisions. FinVis helps the user overcome cognitive limitations and understand the impact of correlation between financial instruments in order to reap the benefits of portfolio diversification. Because this software is accessible by non-expert users, decision-makers from the general population can benefit greatly from using FinVis in practical applications. We quantify the value of FinVis using experimental economics methods and find that subjects using the FinVis software make better financial portfolio decisions as compared to subjects using a tabular version with the same information. We also find that FinVis engages the user, which results in greater exploration of the dataset and increased learning as compared to a tabular display. Further, participants using FinVis reported increased confidence in financial decision-making and noted that they were likely to use this tool in practical application.",,POD:978-1-4244-5283-5,10.1109/VAST.2009.5333920,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5333920,Casual Information Visualization;economic decision-making;personal finance;visual analytics;visualization of risk,Application software;Data analysis;Decision making;Displays;Finance;Financial management;Instruments;Investments;Portfolios;Visual analytics,data visualisation;decision making;financial management;investment,FinVis software;applied visual analytics tool;financial data;financial decision-making;inter-temporal data;personal financial planning;portfolio diversification,,6,,39,,no,12-13 Oct. 2009,,IEEE,IEEE Conference Publications
First Steps to Netviz Nirvana: Evaluating Social Network Analysis with NodeXL,E. M. Bonsignore; C. Dunne; D. Rotman; M. Smith; T. Capone; D. L. Hansen; B. Shneiderman,"Univ. of Maryland, College Park, MD, USA",2009 International Conference on Computational Science and Engineering,20091009,2009,4,,332,339,"Social Network Analysis (SNA) has evolved as a popular, standard method for modeling meaningful, often hidden structural relationships in communities. Existing SNA tools often involve extensive pre-processing or intensive programming skills that can challenge practitioners and students alike. NodeXL, an open-source template for Microsoft Excel, integrates a library of common network metrics and graph layout algorithms within the familiar spreadsheet format, offering a potentially low-barrier-to-entry framework for teaching and learning SNA. We present the preliminary findings of 2 user studies of 21 graduate students who engaged in SNA using NodeXL. The majority of students, while information professionals, had little technical background or experience with SNA techniques. Six of the participants had more technical backgrounds and were chosen specifically for their experience with graph drawing and information visualization. Our primary objectives were (1) to evaluate NodeXL as an SNA tool for a broad base of users and (2) to explore methods for teaching SNA. Our complementary dual case-study format demonstrates the usability of NodeXL for a diverse set of users, and significantly, the power of a tightly integrated metrics/visualization tool to spark insight and facilitate sense-making for students of SNA.",,POD:978-1-4244-5334-4,10.1109/CSE.2009.120,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5284040,MILC;NodeXL;SNA;Social Network Analysis;graph drawing;information visualization;multi-dimensional in-depth long-term case studies;netviz nirvana;teaching;visual analytics,Computer science;Data visualization;Educational institutions;Information services;Internet;Libraries;Open source software;Programming profession;Social network services;Web sites,computer science education;data visualisation;social networking (online);spreadsheet programs,Microsoft Excel open-source template;Netviz Nirvana;NodeXL;graph layout algorithm;social network analysis;structural relationship;visualization tool,,8,,28,,no,29-31 Aug. 2009,,IEEE,IEEE Conference Publications
Foreword,,,2009 IEEE/ACM International Conference on Automated Software Engineering,20100318,2009,,,xi,xi,Presents the welcome message from the conference proceedings.,1938-4300;19384300,Electronic:978-0-7695-3891-4; POD:978-1-4244-5259-0,10.1109/ASE.2009.4,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5431692,,Application software;Cities and towns;Collaborative work;Computer Society;Computer industry;Conferences;Feedback;Programming;Software engineering;Visual analytics,,,,0,,,,no,16-20 Nov. 2009,,IEEE,IEEE Conference Publications
Full-Information Lookups for Peer-to-Peer Overlays,P. Fonseca; R. Rodrigues; A. Gupta; B. Liskov,"Max Planck Institute for Software Systems (MPI-SWS), Kaiserslautern and Saarbr&#x0FC;cken",IEEE Transactions on Parallel and Distributed Systems,20090728,2009,20,9,1339,1351,"Most peer-to-peer lookup schemes keep a small amount of routing state per node, typically logarithmic in the number of overlay nodes. This design assumes that routing information at each member node must be kept small so that the bookkeeping required to respond to system membership changes is also small, given that aggressive membership dynamics are expected. As a consequence, lookups have high latency as each lookup requires contacting several nodes in sequence. In this paper, we question these assumptions by presenting a peer-to-peer routing algorithm with small lookup paths. Our algorithm, called ldquoOneHop,rdquo maintains full information about the system membership at each node, routing in a single hop whenever that information is up to date and in a small number of hops otherwise. We show how to disseminate information about membership changes quickly enough so that nodes maintain accurate complete membership information. We also present analytic bandwidth requirements for our scheme that demonstrate that it could be deployed in systems with hundreds of thousands of nodes and high churn. We validate our analytic model using a simulated environment and a real implementation. Our results confirm that OneHop is able to achieve high efficiency, usually reaching the correct node directly 99 percent of the time.",1045-9219;10459219,,10.1109/TPDS.2008.222,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4641918,Distributed applications;Distributed data structures;Distributed systems;Network topology;Routing protocols;distributed data structures.;network topology;peer to peer;routing protocol,,data structures;peer-to-peer computing;routing protocols;telecommunication network topology,aggressive membership dynamics;analytic bandwidth requirement;distributed data structure;full-information lookup path;network topology;onehop protocol;peer-to-peer overlay;peer-to-peer routing algorithm;routing state per node;system membership change,,39,,28,,no,Sept. 2009,,IEEE,IEEE Journals & Magazines
G-MicroRNA: A New Tool for MicroRNA Genomics,X. H. Zhao; J. F. Yu; Y. k. Tang; J. H. Wang,"Dept. of Phys., Dezhou Univ., Dezhou, China",2009 First International Conference on Information Science and Engineering,20100426,2009,,,3661,3664,"With the post-genome era coming, it is interesting to establish a new bioinformatics database, further integrate related information of the sequences and develop new methods to reveal the mechanism of life based on databases of sequence. We have built a professional bioinformatics database of non-coding miRNA named G-microRNA by means of PHP, MYSQL and so on. The database integrates more than 2,000 miRNA sequences including several model organisms-human, arabidopsis thaliana and virus. The pre-calculated Z curves and characteristics of each sequence are displayed in the database. Simultaneously, we have improved analytic softwares, selected famous predicting tools and hundreds of outstanding references, and shared our database in the internet. The construction of the database will provide much useful information and operation platform for bioinformatics further research.",2160-1283;21601283,Electronic:978-1-4244-5728-1; POD:978-1-4244-4909-5,10.1109/ICISE.2009.614,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5455020,,Bioinformatics;Biophysics;Educational institutions;Genomics;Humans;Information science;Physics;Sequences;Spatial databases;Viruses (medical),bioinformatics;database management systems;genomics,G-microRNA;Internet;analytic softwares;bioinformatics database;microRNA genomics;noncoding miRNA sequences,,0,,14,,no,26-28 Dec. 2009,,IEEE,IEEE Conference Publications
Game Theoretical Formulation of Network Selection in Competing Wireless Networks: An Analytic Hierarchy Process Model,H. Pervaiz; J. Bigham,"Sch. of Electron. Eng. & Comput. Sci., Queen Mary Univ. of London, London, UK","2009 Third International Conference on Next Generation Mobile Applications, Services and Technologies",20091117,2009,,,292,297,Network Selection mechanisms play an important role in ensuring quality of service for users in a multi-network environment. These mechanisms handle the selection of an optimal wireless network to satisfy a user request. This paper proposes a radio resource management framework for integrated network selection mechanism control in multi- network environment as an interaction game between the service providers and customers in non-cooperative manner to maximize their rewards. The proposed scheme comprises two steps. The first applies the analytic hierarchy process (AHP) to determine the relative weights of the evaluative criteria according to customer preferences and network condition. The second calculates the payoffs based on the relative weights calculated in the previous step and a utility function evaluation by each wireless network of each customer. Analytical and simulation results demonstrate the effectiveness of proposed model to achieve optimum network utility for the wireless networks along with optimizing the customer's satisfaction. The proposed model is preliminary and its contribution is to create an admission policy that can adapt to different coverage areas of a wireless network and depends on the priority of customers and their requirements.,2161-2889;21612889,POD:978-0-7695-3786-3,10.1109/NGMAST.2009.28,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5337435,Analytic Hierarchy Process;Competitive networks;Game Theory;Network Selection;WiMAX,Application software;Bandwidth;Cellular networks;Game theory;Mobile computing;Multiagent systems;Next generation networking;Quality of service;Resource management;Wireless networks,game theory;optimisation;radio networks;telecommunication network management,analytic hierarchy process model;game theoretical formulation;multinetwork environment;noncooperative manner;radio resource management;service provider;utility function evaluation;wireless network,,15,,11,,no,15-18 Sept. 2009,,IEEE,IEEE Conference Publications
General Julia Sets of Non-analytic Families z^n+c,X. Liu; Z. Li; N. Jiang; J. Zhang,"Sch. of Comput. Sci. & Eng., Dalian Nat. Univ., Dalian, China",2009 International Workshop on Chaos-Fractals Theories and Applications,20091228,2009,,,387,390,"In this paper we present general Julia sets of non-analytic families zÌâåÀ<sup>n</sup>+ c (n <sup>3</sup> 2), we also propose some properties of these general Julia sets. Moreover, by iterated function systems theory, we give out two estimations of Hausdorff dimension of these general Julia sets when |c| is sufficient large or sufficient small.",,POD:978-0-7695-3853-2,10.1109/IWCFTA.2009.87,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5362042,Hausdorff dimension;Iterated Function Systems;Julia Set;Non-analytic Family,Application software;Chaos;Computer science;Eigenvalues and eigenfunctions;Jacobian matrices;Polynomials;Symmetric matrices,set theory;vectors,Hausdorff dimension;general Julia sets;iterated function systems theory;nonanalytic family,,0,,9,,no,6-8 Nov. 2009,,IEEE,IEEE Conference Publications
Generating visualization-based analysis scenarios from maintenance task descriptions,S. Hassaine; K. Dhambri; H. Sahraoui; P. Poulin,"DIRO, University of Montreal, Canada",2009 5th IEEE International Workshop on Visualizing Software for Understanding and Analysis,20091117,2009,,,41,44,"Software visualization is an efficient and flexible tool to inspect and analyze software data at various levels of detail. However, software analysts typically do not have a sufficient background in visualization and cognitive science to select efficient representations and parameters without the help of visualization experts. To overcome this problem, we propose an approach to generate software analysis tasks that use visualization. To this end, we use taxonomies of low-level analytic tasks, high-level interactive tasks, and perceptual rules to design an assistant that proposes analysis scenarios.",,POD:978-1-4244-5027-5,10.1109/VISSOF.2009.5336423,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5336423,,Cognitive science;Data analysis;Data visualization;Humans;Independent component analysis;Pattern recognition;Software maintenance;Software quality;Software tools;Taxonomy,data visualisation;program visualisation;software quality,cognitive science;maintenance task descriptions;software analysis tasks;software quality;software visualization;taxonomies;visualization-based analysis scenarios,,2,,9,,no,25-26 Sept. 2009,,IEEE,IEEE Conference Publications
Generation and calibration of compositional performance analysis models for multi-processor systems,W. Haid; M. Keller; K. Huang; I. Bacivarov; L. Thiele,"Computer Engineering and Networks Laboratory, ETH Zurich, 8092, Switzerland","2009 International Symposium on Systems, Architectures, Modeling, and Simulation",20091016,2009,,,92,99,"The performance analysis of heterogeneous multi-processor systems is becoming increasingly difficult due to the steadily growing complexity of software and hardware components. To cope with these increasing requirements, analytic methods have been proposed. The automatic generation of analytic system models that faithfully represent real system implementations has received relatively little attention, however. In this paper, an approach is presented in which an analytic system model is automatically generated from the same specification that is also used for system synthesis. Analytic methods for performance analysis of a system can thus be seamlessly integrated into the multi-processor design flow which lays a sound foundation for designing systems with a predictable performance.",,POD:978-1-4244-4502-8,10.1109/ICSAMOS.2009.5289246,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5289246,,Array signal processing;Calibration;Computer networks;Hardware;Laboratories;Multiprocessing systems;Performance analysis;Real time systems;Software performance;Space exploration,logic design;multiprocessing systems;performance evaluation,analytic system model;compositional performance analysis model;heterogeneous multiprocessor systems;multiprocessor design flow,,7,,33,,no,20-23 July 2009,,IEEE,IEEE Conference Publications
Grid-Based Region Management System Based on SOA Architecture Reseach,X. Guo; W. Huang,"Sch. of Bus. Adm., North China Electr. Power Univ., Beijing, China",2009 International Symposium on Information Engineering and Electronic Commerce,20090728,2009,,,537,541,"Region management information system is a three-dimensional structure which contains management dimension, system architecture dimension and data dimension. In this article, the author introduced the grid management model into region management information system as management dimension, and designed the region management system using SOA as system architecture dimension, and used grid-based region's spatial data and other business data as data dimension. This paper took the workflow engine service platform as the system's core, combined with the natural semantic analytic location serve and mobile client system based on SOA architecture, formed a complete grid-based region management system designed based on SOA architecture.",,POD:978-0-7695-3686-6,10.1109/IEEC.2009.119,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5175176,SOA;grid management;region management;workflow,Computer architecture;Energy management;Engineering management;Engines;Management information systems;Power system management;Resource management;Semiconductor optical amplifiers;Service oriented architecture;Technology management,grid computing;management information systems;mobile computing;software architecture;workflow management software,SOA architecture;data dimension;grid management model;management dimension;mobile client system;natural semantic analytic location serve;region management information system;system architecture dimension;three-dimensional structure;workflow engine service platform,,0,,7,,no,16-17 May 2009,,IEEE,IEEE Conference Publications
Harnessing the Information Ecosystem with Wiki-based Visualization Dashboards,M. McKeon,IBM Research,IEEE Transactions on Visualization and Computer Graphics,20091023,2009,15,6,1081,1088,"We describe the design and deployment of Dashiki, a public Website where users may collaboratively build visualization dashboards through a combination of a wiki-like syntax and interactive editors. Our goals are to extend existing research on social data analysis into presentation and organization of data from multiple sources, explore new metaphors for these activities, and participate more fully in the Web's information ecology by providing tighter integration with real-time data. To support these goals, our design includes novel and low-barrier mechanisms for editing and layout of dashboard pages and visualizations, connection to data sources, and coordinating interaction between visualizations. In addition to describing these technologies, we provide a preliminary report on the public launch of a prototype based on this design, including a description of the activities of our users derived from observation and interviews.",1077-2626;10772626,,10.1109/TVCG.2009.148,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290715,collaboration;dashboards;social data analysis;social software;visual analytics.;visualization;web;wikis,Application software;Collaborative software;Data analysis;Data visualization;Ecosystems;Environmental factors;Eyes;Feeds;Pipelines;Web page design,Internet;data analysis;data visualisation;social networking (online),Dashiki;Web information ecology;Web information ecosystem;Wiki-based visualization dashboards;coordinating interaction;dashboard pages;public Web site;social data analysis;wiki-like syntax,,6,1,26,,no,Nov.-Dec. 2009,,IEEE,IEEE Journals & Magazines
How interactive visualization can assist investigative analysis: Views and perspectives from domain experts,J. Stasko; S. Cohen; L. Hunter; J. Parry,"Georgia Tech, (Panel Organizer), USA",2009 IEEE Symposium on Visual Analytics Science and Technology,20091113,2009,,,279,281,"Interactive visualization could become an essential tool in the work of investigative analysts. Visualization could help analysts to explore large collections of data and documents, supporting the analysts investigative sense-making processes. This panel gathers recognized leaders from three important domains, investigative reporting, biosciences (genomics), and intelligence analysis, that all include a fundamental investigative analysis component. The panelists will provide a glimpse into their worlds, describing and illustrating the data they examine, the goals and methods of their analysts, and the culture of their respective professions. In particular, the panelists will explore how visualization could potentially benefit investigators from their domain and they will provide guidance for visualization researchers seeking to collaborate with their colleagues.",,POD:978-1-4244-5283-5,10.1109/VAST.2009.5332411,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5332411,genomics;intelligence analysis;interactive visualization;investigative analysis;investigative reporting;visual analytics,Bioinformatics;Collaboration;Data visualization;Genomics;Information analysis;Information systems;Military computing;Software systems;Text analysis;Visual analytics,data visualisation;interactive systems,genomics;intelligence analysis;interactive visualization;investigative analysis;investigative reporting;visual analytics,,0,,4,,no,12-13 Oct. 2009,,IEEE,IEEE Conference Publications
ICE--visual analytics for transportation incident datasets,M. L. Pack; K. Wongsuphasawat; M. VanDaniker; D. Filippova,"University of Maryland, College Park, Center for Advanced Transportation Technology Laboratory, College Park, 20742, USA",2009 IEEE International Conference on Information Reuse & Integration,20090821,2009,,,200,205,"Transportation systems are being monitored at an unprecedented scope resulting in tremendously detailed traffic and incident databases. While the transportation community emphasizes developing standards for storing this incident data, little effort has been made to design appropriate visual analytics tools to explore the data, extract meaningful knowledge, and represent results. Analyzing these large multivariate geospatial datasets is a non-trivial task. A novel, web-based, visual analytics tool called ICE (incident cluster explorer) is proposed as an application that affords sophisticated yet user-friendly analysis of transportation incident datasets. Interactive maps, histograms, two-dimensional plots and parallel coordinates plots are four visualizations that are integrated together to allow users to simultaneously interact with and see relationships between multiple visualizations. Accompanied by a rich set of filters, users can create custom conditions to filter data and focus on a smaller dataset. Due to the multivariate nature of the data, a rank-by-feature framework has been expanded to quantify the strength of relationships between the different fields.",,POD:978-1-4244-4114-3,10.1109/IRI.2009.5211551,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5211551,Data Mining;Information Visualization;Knowledge Discovery;Visual Analytics,Data mining;Data visualization;Filters;Histograms;Ice;Monitoring;Standards development;Transportation;Visual analytics;Visual databases,cartography;data mining;database management systems;geographic information systems;interactive systems;software tools;traffic engineering computing,data mining;geographic information systems;incident cluster explorer;incident databases;information visualization;interactive maps;knowledge discovery;knowledge extraction;multivariate geospatial datasets;rank-by-feature framework;traffic databases;traffic management centers;transportation incident datasets;transportation systems;visual analytics tools,,4,,19,,no,10-12 Aug. 2009,,IEEE,IEEE Conference Publications
IEEE International symposium on performance analysis of systems and software,,,2009 IEEE International Symposium on Performance Analysis of Systems and Software,20090512,2009,,,i,i,The following topics are dealt with: differentiating the roles of IR measurement and simulation for power and temperature-aware design; user- and process-driven dynamic voltage and frequency scaling; accuracy of performance counter measurements; GARNET: a detailed on-chip network model inside a full-system simulator; Cetra: a trace and analysis framework for the evaluation of Cell BE; Zesto: a cycle-level simulator for highly detailed microarchitecture exploration; Lonestar: a suite of parallel irregular program; exploring speculative parallelism in SPEC2006; machine learning based online performance prediction for runtime parallelization and task scheduling; WARP: enabling fast CPU scheduler development and evaluation; CMPSchedSim: evaluating OS/CMP interaction on shared cache management; understanding the cost of thread migration for multi-threaded Java applications running on a multicore platform; the data-centricity of Web 2.0 workloads and its impact on server performance; characterizing and optimizing the memory footprint of De Novo Short Read DNA sequence assembly; analytic model of optimistic software transactional memory; analyzing CUDA workloads using a detailed GPU simulator; evaluating GPUs for network packet signature matching; online compression of cache-filtered address traces; analysis of the TRIPS prototype block predictor; experiment flows and microbenchmarks for reverse engineering of branch predictor structures; analyzing the impact of on-chip network traffic on program phases for CMPs; SuiteSpecks and SuiteSpots: a methodology for the automatic conversion of benchmarking programs into intrinsically checkpointed assembly code; accurately approximating superscalar processor performance from traces; and QUICK: a flexible full-system functional model.,,POD:978-1-4244-4184-6,10.1109/ISPASS.2009.4919624,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4919624,,,Internet;Java;benchmark testing;coprocessors;electric potential;learning (artificial intelligence);multi-threading;multiprocessing systems;performance evaluation;reverse engineering;scheduling;simulation;storage management,CMPSchedSim;Cell BE;Cetra;De Novo Short Read DNA sequence assembly;GARNET;GPU simulator;Lonestar;SuiteSpecks;SuiteSpots;TRIPS prototype block predictor;WARP;Web 2.0;Zesto;assembly code;benchmarking programs;branch predictor structures;cache-filtered address traces;cycle-level simulator;data centricity;fast CPU scheduler development;flexible full-system functional model;frequency scaling;full-system simulator;machine learning;memory footprint;microarchitecture exploration;multi-threaded Java application;multicore platform;network packet signature matching;on-chip network model;on-chip network traffic;online compression;online performance prediction;optimistic software transactional memory;performance counter measurements;power-aware design;process-driven dynamic voltage;reverse engineering;runtime parallelization;server performance;shared cache management;superscalar processor performance;task scheduling;temperature-aware design;trace and analysis framework,,0,,,,no,26-28 April 2009,,IEEE,IEEE Conference Publications
Improving metadata management for small files in HDFS,G. Mackey; S. Sehrish; J. Wang,"School of Electrical Engineering and Computer Science, University of Central Florida, Orlando",2009 IEEE International Conference on Cluster Computing and Workshops,20091016,2009,,,1,4,"Scientific applications are adapting HDFS/MapReduce to perform large scale data analytics. One of the major challenges is that an overabundance of small files is common in these applications, and HDFS manages all its files through a single server, the Namenode. It is anticipated that small files can significantly impact the performance of Namenode. In this work we propose a mechanism to store small files in HDFS efficiently and improve the space utilization for metadata. Our scheme is based on the assumption that each client is assigned a quota in the file system, for both the space and number of files. In our approach, we utilize the compression method `harballing', provided by Hadoop, to better utilize the HDFS. We provide for new job functionality to allow for in-job archival of directories and files so that running MapReduce programs may complete without being killed by the jobtracker due to quota policies. This approach leads to better functionality of metadata operations and more efficient usage of the HDFS. Our analysis results show that we can reduce the metadata footprint in main memory by a factor of 42.",1552-5244;15525244,POD:978-1-4244-5011-4,10.1109/CLUSTR.2009.5289133,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5289133,,Application software;Availability;Computer architecture;Computer science;Data analysis;Engineering management;File servers;File systems;Large-scale systems;Performance analysis,cache storage;data analysis;meta data,HDFS-MapReduce program;Namenode;file storage;hadoop distributed file system;large datasets analysis;metadata management;single server,,37,,17,,no,Aug. 31 2009-Sept. 4 2009,,IEEE,IEEE Conference Publications
Information Seeking Can Be Social,E. H. Chi,Palo Alto Research Center,Computer,20090321,2009,42,3,42,46,"For reasons ranging from obligation to curiosity, users have a strong inclination to seek information from others during the search process. Search systems using statistical analytics over traces left behind by others can help support the search experience.",0018-9162;00189162,,10.1109/MC.2009.87,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4803887,Web 2.0;human-computer interaction;information-seeking support systems;social computing,Databases;Discussion forums;Electronic mail;Feedback;Monitoring;Packaging;Programming profession;Search engines;Social network services;Software packages,information retrieval;information retrieval systems;social networking (online),information seeking;search process;search systems;statistical analytics,,25,,7,,no,9-Mar,,IEEE,IEEE Journals & Magazines
Interactive poster: A proposal for sharing user requirements for visual analytic tools,J. Scholtz,"Pacific Northwest National Laboratory, USA",2009 IEEE Symposium on Visual Analytics Science and Technology,20091113,2009,,,215,216,"Although many in the community have advocated user-centered evaluations for visual analytic environments, a significant barrier exists. The users targeted by the visual analytics community (law enforcement personnel, professional information analysts, financial analysts, health care analysts, etc.) are often inaccessible to researchers. These analysts are extremely busy and their work environments and data are often classified or at least confidential. Furthermore, their tasks often last weeks or even months. It is simply not feasible to do such long-term observations to understand their jobs. How then can we hope to gather enough information about the diverse user populations to understand their needs? Some researchers, including the author, have been successful in getting access to specific end-users. A reasonable approach, therefore, would be to find a way to share user information. This work outlines a proposal for developing a handbook of user profiles for use by researchers, developers, and evaluators.",,POD:978-1-4244-5283-5,10.1109/VAST.2009.5333474,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5333474,user requirements;user-centered evaluation;visual analytics,Best practices;Information analysis;Law enforcement;Medical services;Personnel;Proposals;Social network services;User interfaces;Visual analytics;Visualization,data mining;data visualisation;software tools;user interfaces,user requirement sharing;user-centered evaluation;visual analytic tools,,0,,8,,no,12-13 Oct. 2009,,IEEE,IEEE Conference Publications
Investment risks evaluation on high-tech projects based on analytic hierarchy process and BP neural network,J. Hua,"School of Economics and Management, Hebei University of Engineering, Handan, China","2009 ISECS International Colloquium on Computing, Communication, Control, and Management",20090929,2009,1,,305,309,"In view of the existing problems of investment risks evaluation on high-tech industry projects such as a lack of systematic, with too much subjectivity and from the point to improve evaluation efficiency and effectiveness, the paper combined analytic hierarchy process (AHP) with BP neural network to establish a new and suitable risk evaluation model of high-tech projects. Firstly, we applied AHP to construct a comprehensive risk evaluation index system and screened the evaluation indexes according to their weights. On this basis, using MATLAB software with BP neural network model, we carried out example simulations and results were analyzed. The results showed that the combination model of analytic hierarchy process with BP neural network model (AHP-BPNN) is effective.",2154-9613;21549613,POD:978-1-4244-4247-8,10.1109/CCCM.2009.5268119,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5268119,BP neural network;analytic hierarchy process;high-tech projects;investment risks evaluation,Communication system control;Computer network management;Computer networks;Engineering management;Investments;Mathematical model;Neural networks;Project management;Risk analysis;Risk management,backpropagation;neural nets;risk management,BP neural network;analytic hierarchy process;evaluation indexes;investment risks evaluation,,0,,8,,no,8-9 Aug. 2009,,IEEE,IEEE Conference Publications
LiterMiner: A Literature Visual Analytic System,B. Wu; L. Suo,"Beijing Key Lab. of Intell. Telecommun. Software & Multimedia, Beijing Univ. of Posts & Telecommun., Beijing, China",2009 First International Conference on Information Science and Engineering,20100426,2009,,,891,894,"The data set of scientific literature contains abundant knowledge. Currently, most bibliography search engines provide only keyword search or similarity search functions for retrieving information directly stored in the database but neglect mining hidden information. In this article, we design a visual analytic tool called LiterMiner to extract entities such as article, author, affiliation, and keyword from these literature records, establish the relationships among them and display them to help analyst understand them effectively. Especially, LiterMiner uses community detection algorithm to explore academic teams. Our tool has three principle features. First, a sophisticated data clean process is designed for integration of many different formatted literature records. Second, LiterMiner can find academic teams and analyze the resume of an academic team. Third, an interactive and cooperate visualization solution is used to display the analysis result, which help user understand the result better.",2160-1283;21601283,Electronic:978-1-4244-5728-1; POD:978-1-4244-4909-5,10.1109/ICISE.2009.717,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5454639,,Bibliographies;Data mining;Detection algorithms;Displays;Information retrieval;Keyword search;Process design;Search engines;Visual analytics;Visual databases,bibliographic systems;data mining;data visualisation;search engines,LiterMiner;abundant knowledge;cooperate visualization solution;keyword search;literature records;literature visual analytic system;mining hidden information;retrieving information;scientific literature;search engines;search functions,,0,,10,,no,26-28 Dec. 2009,,IEEE,IEEE Conference Publications
LSAView: A tool for visual exploration of latent semantic modeling,P. J. Crossno; D. M. Dunlavy; T. M. Shead,"Sandia National Laboratories, USA",2009 IEEE Symposium on Visual Analytics Science and Technology,20091113,2009,,,83,90,"Latent Semantic Analysis (LSA) is a commonly-used method for automated processing, modeling, and analysis of unstructured text data. One of the biggest challenges in using LSA is determining the appropriate model parameters to use for different data domains and types of analyses. Although automated methods have been developed to make rank and scaling parameter choices, these approaches often make choices with respect to noise in the data, without an understanding of how those choices impact analysis and problem solving. Further, no tools currently exist to explore the relationships between an LSA model and analysis methods. Our work focuses on how parameter choices impact analysis and problem solving. In this paper, we present LSAView, a system for interactively exploring parameter choices for LSA models. We illustrate the use of LSAView's small multiple views, linked matrix-graph views, and data views to analyze parameter selection and application in the context of graph layout and clustering.",,POD:978-1-4244-5283-5,10.1109/VAST.2009.5333428,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5333428,Applications I.2.7 [Computing Methodologies]: Natural Language Processing;I.3.8 [Computing Methodologies]: Computer Graphics;Text analysis,Application software;Cities and towns;Data analysis;Laboratories;Layout;Matrix decomposition;Natural languages;Problem-solving;USA Councils;Visual analytics,data visualisation,LSA model;LSAView;automated processing;impact analysis;latent semantic analysis;latent semantic modeling;linked matrix-graph views;problem solving;rank parameter;scaling parameter;unstructured text data;visual exploration,,4,,27,,no,12-13 Oct. 2009,,IEEE,IEEE Conference Publications
Merging visual analysis with automated reasoning: Using Prajna to solve the traffic challenge,E. Swing,"Vision Systems & Technology, Inc., USA",2009 IEEE Symposium on Visual Analytics Science and Technology,20091113,2009,,,,,"The Internet traffic challenge required the development of a custom application to analyze internet traffic patterns coupled with building access records. To solve this challenge, the author applied the Prajna Project, an open-source Java toolkit designed to provide various capabilities for visualization, knowledge representation, semantic reasoning, and data fusion. By applying some of the capabilities of Prajna to this challenge, the author could quickly develop a custom application for visual analysis. The author determined that he could solve some of the analytical components of this challenge using automated reasoning techniques. Prajna includes interfaces to incorporate automated reasoners into visual applications. By blending the automated reasoning processes with visual analysis, the author could design a flexible, useful application to solve this challenge.",,POD:978-1-4244-5283-5,10.1109/VAST.2009.5332481,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5332481,Information Visualization;Knowledge Representation;Semantic Reasoning;Software Toolkit,Application software;Information analysis;Information filtering;Information filters;Internet;Knowledge representation;Pattern analysis;Workstations,Internet;Java;data visualisation;inference mechanisms;knowledge representation;sensor fusion,Internet traffic;Prajna Project;automated reasoning;data fusion;knowledge representation;open-source Java toolkit;semantic reasoning;visual analysis,,0,,4,,no,12-13 Oct. 2009,,IEEE,IEEE Conference Publications
Metric-based Evolution Analysis and Evaluation of Software Core Assets Library,D. JianJie; H. Hou; K. Hao; X. Guo,"Sch. of Inf. Sci. & Technol., Northwest Univ., Xi'an",2009 First International Workshop on Education Technology and Computer Science,20090526,2009,3,,799,803,"The core assets library is the most important component of any software product line; it evolved responding to organization businesses, origination object, technology renovation and time. From the metric of the core assets library, this paper gives several measurements and their definitions which correlate with the evolution of a core assets library. It is combing with data processing, putting forward an evaluation model with the evolution of a core assets library,using analytic hierarchy process to estimate the core assets librarypsilas evolution level, illuminating with an example at last.",,POD:978-0-7695-3557-9,10.1109/ETCS.2009.714,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4959431,core assests library;evolve;software measurement;software product line,Asset management;Computer science education;Educational products;Educational technology;Engineering management;Performance analysis;Software development management;Software libraries;Software measurement;Software quality,product development;software libraries;software maintenance;software metrics;software reusability,analytic hierarchy process;metric-based evolution analysis;organization business;origination object;software core assets library;software measurement;software product line;technology renovation,,0,,16,,no,7-8 March 2009,,IEEE,IEEE Conference Publications
Modeling and Performance Analysis of Software Rejuvenation Policies for Multiple Degradation Systems,X. Du; Y. Qi; D. Hou; Y. Chen; X. Zhong,"Dept. of Comput. Sci. & Technol., Xi 'an Jiaotong Univ., Xi'an, China",2009 33rd Annual IEEE International Computer Software and Applications Conference,20090922,2009,1,,240,245,"Software rejuvenation is a preventive and proactive technology to counteract the phenomenon of software aging and system failures and to improve the system reliability. In this paper we present and analyze three software rejuvenation policies for an operational software system with multiple degradations, called preemptive rejuvenation, delayed rejuvenation and mixed rejuvenation. These policies consider both history data and current running state, and the rejuvenation action is triggered on the basis of predetermined performance threshold and rejuvenation interval respectively. Continuous-time Markov chains are used to describe the analytic models. To evaluate these polices expediently, we utilize deterministic and stochastic Petri nets to solve the models. Numerical results show that the deployment of software rejuvenation in the system leads to significant improvement in availability and throughput. These three rejuvenation policies are better than the standard rejuvenation policy, and the mixed policy is the best one.",0730-3157;07303157,POD:978-0-7695-3726-9,10.1109/COMPSAC.2009.39,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5254254,,Aging;Degradation;Delay;History;Performance analysis;Petri nets;Reliability;Software performance;Software systems;Stochastic processes,Markov processes;Petri nets;software fault tolerance;software maintenance,analytic model;continuous-time Markov chain;multiple degradation system;operational software system;preventive technology;proactive technology;software aging;software rejuvenation policy;stochastic Petri nets;system failure;system reliability,,1,,18,,no,20-24 July 2009,,IEEE,IEEE Conference Publications
Modeling of Efficiency and Uniformity of Different Pumping Structures of Slab Lasers,J. Hou; Y. Wang,"Teaching & Res. Sect. of Opt. Eng., Ordnance Eng. Coll., Shijiazhuang, China",2009 Symposium on Photonics and Optoelectronics,20090901,2009,,,1,3,"Modeling for end-pumping and edged-pumping high power solid-state slab lasers. Both analytic and ray tracing methods were used to analyze the distributions of absorbed pumping power using laser-diode-array pump sources. The pumping light field profiles of the different pumping structures mentioned above are acquired, according to which the absorption power density and uniformity of the pumping light are analyzed. In order to compare the pumping uniformity and efficiency of the two pumping structures, cylinder lens are adopted for beam focus and shaping. Each cylinder lens gets different size, but coupling efficiency is about 91% as the same. The irradiance profiles of these two pumping structure are simulated via ray trace method of non-sequential component of ZEMAX software, each is confirmed finally, i.e. the absorbed pumping power density distribution is calculated. The edge-pumped and end-pumped slab lasers allow for a longer absorption path than in face-pumped slabs. However, the tradeoff made to reach higher pump efficiency is lower absorbed pump power distribution uniformity due to the long absorption path. By comparing simulation results of the different pumping structures, it was found that uniformity of absorbed pumping distribution on width-thickness plane in doped region of crystal with end-pumping structure is the best, and the absorbed peak power density is 0.0168 W/mm<sup>3</sup> which is the highest among three pumping structures. It is more uniformity distribution for pumping light in the Z direction of crystal axis.",2156-8464;21568464,POD:978-1-4244-4412-0,10.1109/SOPO.2009.5230307,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5230307,,Absorption;Laser excitation;Laser modes;Lenses;Power lasers;Pump lasers;Slabs;Solid lasers;Solid modeling;Solid state circuits,laser beams;lenses;optical focusing;optical pumping;ray tracing;solid lasers,ZEMAX software;absorption power density;analytical method;cylinder lens;edged-pumping high-power solid-state slab laser;efficiency 91 percent;end-pumped slab laser;laser-diode-array pump sources;light irradiance profile;optical crystal;ray tracing method,,0,,6,,no,14-16 Aug. 2009,,IEEE,IEEE Conference Publications
Moment-closure approximations for mass-action models,C. S. Gillespie,Newcastle University,IET Systems Biology,20090123,2009,3,1,52,58,"Although stochastic population models have proved to be a powerful tool in the study of process generating mechanisms across a wide range of disciplines, all too often the associated mathematical development involves nonlinear mathematics, which immediately raises difficult and challenging analytic problems that need to be solved if useful progress is to be made. One approximation that is often employed to estimate the moments of a stochastic process is moment closure. This approximation essentially truncates the moment equations of the stochastic process. A general expression for the marginal- and joint-moment equations for a large class of stochastic population models is presented. The generalisation of the moment equations allows this approximation to be applied easily to a wide range of models. Software is available from http://pysbml.googlecode.com/ to implement the techniques presented here.",1751-8849;17518849,,10.1049/iet-syb:20070031,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4762250,,,cellular biophysics;difference equations;method of moments;molecular biophysics;physiological models;stochastic processes,biological process;differential equations;joint-moment equations;marginal-moment equations;mass-action models;moment-closure approximations;stochastic population models,,7,,,,no,9-Jan,,IET,IET Journals & Magazines
Multiple step social structure analysis with Cytoscape,H. Zhou; A. A. Shaverdian; H. V. Jagadish; G. Michailidis,"Dept. of Statistics, University of Michigan, USA",2009 IEEE Symposium on Visual Analytics Science and Technology,20091113,2009,,,,,Cytoscape is a popular open source tool for biologists to visualize interaction networks. We find that it offers most of the desired functionality for visual analytics on graph data to guide us in the identification of the underlying social structure. We demonstrate its utility in the identification of the social structure in the VAST 2009 Flitter Mini Challenge.,,POD:978-1-4244-5283-5,10.1109/VAST.2009.5333961,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5333961,Cytoscape;Social Networks,Biological interactions;Data visualization;Social network services;Statistical analysis;Statistics;Visual analytics,biology computing;data visualisation;public domain software,Cytoscape;VAST 2009 Flitter Mini Challenge;graph data;interaction network analysis tool;multiple step social structure analysis;open source tool,,0,,3,,no,12-13 Oct. 2009,,IEEE,IEEE Conference Publications
nCompass Service Oriented Architecture for Tacit Collaboration Services,D. Schroh; N. Bozowsky; M. Savigny; W. Wright,,2009 13th International Conference Information Visualisation,20090804,2009,,,433,442,"nCompass is a flexible, Service Oriented Architecture (SOA) designed to support the research and deployment of advanced tacit collaboration technology services for analysts. nCompass allows a significantly larger number of individual analytic capabilities, applications and services to be integrated together quickly and effectively. Service integration results are described from several computational tacit collaboration experiments conducted with open source intelligence analysts working with open source data. Key to nCompass is the technical framework and unique analytic event logging schema that supports context sharing across diverse applications and services. It is by combining the analyst with shared context across multiple advanced computational capabilities in a system of systems that a breakthrough in collaborative open source analysis can be achieved. This paper introduces the nCompass framework and integration platform, describes key nCompass core services, and provides results on functional synergies achieved through technology service integration with nCompass.",1550-6037;15506037,POD:978-0-7695-3733-7,10.1109/IV.2009.62,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5190860,SOA integration;open source analysis;open source intelligence;tacit collaboration;visual analytics,Collaborative work;Computational intelligence;Computational linguistics;Context-aware services;Feedback;Image processing;Information analysis;International collaboration;Service oriented architecture;Visualization,groupware;public domain software;software architecture,advanced tacit collaboration;analytic event logging schema;computational tacit collaboration experiments;context sharing;flexible service oriented architecture;individual analytic capabilities;integration platform;nCompass core services;nCompass framework;nCompass service oriented architecture;open source intelligence analyst;service integration;tacit collaboration services;technical framework,,0,,31,,no,15-17 July 2009,,IEEE,IEEE Conference Publications
Needs Assessment for the Design of Information Synthesis Visual Analytics Tools,A. C. Robinson,"GeoVISTA Center, Pennsylvania State Univ., University Park, PA, USA",2009 13th International Conference Information Visualisation,20090804,2009,,,353,360,"Information synthesis is a key portion of the analysis process with visual analytics tools. This stage of work requires users to collect, organize, and add meaning to individual analytical results. This paper reports the results of a needs assessment study with technical and bio/chemical security analysts intended to characterize the ways in which users currently synthesize information,and to elicit ideas for future tools to support information synthesis. Our work used structured interviews to obtain knowledge from analysts. Responses indicate that synthesis is currently supported through the use of office productivity software, and current tools do not provide adequate support for the task of information synthesis.",1550-6037;15506037,POD:978-0-7695-3733-7,10.1109/IV.2009.85,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5190786,needs assessment;synthesis,Visual analytics,computer software;data handling;data visualisation;software tools;structured programming,biochemical security analyst;individual analytical result;information synthesis design;needs assessment study;office productivity software;structured interview;technical security analyst;visual analytics tool,,1,,20,,no,15-17 July 2009,,IEEE,IEEE Conference Publications
Network Worm Propagating Model Based on Network Topology Unit,W. Zhang; S. Guo; K. Zheng,"State Key Lab. of Networking & Switching Technol., Beijing Univ. of Posts & Telecommun., Beijing, China",2009 International Conference on Multimedia Information Networking and Security,20091231,2009,1,,383,387,"The existing analytic models on network worm propagation rarely consider the effect of topology structure, and it usually considers the probability and spread parameter as fixed values. They are not coinciding with real situation. Here in the paper, a microstructure based propagation model is proposed to inflect the variability with time. In this model, many probability factors are considered, for example the connecting probability from one node to another node, one node's immune probability, which make the model more general and suitable for simulation.",2162-8998;21628998,POD:978-0-7695-3843-3,10.1109/MINES.2009.119,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5371035,Model;Network;Propagation;Worm,Information security;Network topology,invasive software;probability,analytic models;immune probability;microstructure based propagation model;network topology unit;network worm propagating model;network worm propagation;topology structure,,0,,7,,no,18-20 Nov. 2009,,IEEE,IEEE Conference Publications
New Techniques for Visualising Web Navigational Data,V. Pascual-Cid; R. Baeza-Yates; J. C. DÌ_rsteler; S. Minguez; C. Middleton,"Fundacio Barcelona Media, Barcelona, Spain",2009 13th International Conference Information Visualisation,20090804,2009,,,621,626,"Understanding Web navigational data is a crucial task for Web analysts as it may influence the Web site improvement process. However, major Web analytics tools do not provide enough insight, taking into account the vast amount of data available in Web serverspsila log files. Moreover, some analysts argue that those tools have a lack of rich visualisations that enable the exploration of such data. In this paper we introduce new techniques applied to a highly interactive and exploratory tool, to allow drilling down through Web usage data. The system uses a set of coordinated visual abstractions from Web site structure and userspsila navigation to provide different perspectives and hence, to assist the conversion of data into knowledge.",1550-6037;15506037,POD:978-0-7695-3733-7,10.1109/IV.2009.53,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5190803,,Chromium;Coordinate measuring machines;Data visualization;Drilling;Feedback;Information analysis;Navigation;Performance analysis;Software measurement;Web server,Internet;data mining;data visualisation;information retrieval;interactive systems,Web analytics tool;Web navigational data visualisation;Web server;Web site improvement process;Web usage mining;coordinated visual abstraction;interactive system;log file,,1,,15,,no,15-17 July 2009,,IEEE,IEEE Conference Publications
Notice of Retraction<BR>An Evaluation System of Finance Method Bases on AHP,Z. m. Xu; Z. Mei,"Inf. Center of Zhejiang Provincial Finance, Hangzhou, China",2009 Asia-Pacific Conference on Information Processing,20090807,2009,2,,474,477,"Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR>First, this paper constructs a financial evaluation system, and then directly against the various relative importance of evaluation system at all levels and indicates, and the difficulty to determine scientifically to set up determine matrix of all levels, and use AHP to determine the financial evaluation index weight and conduct a strict consistency test of the level of single-sort, and a total sort. The result of the evaluation is objective and practical, and has the characteristics of stability and accuracy, and has the value of popularization and application.",,POD:978-0-7695-3699-6,10.1109/APCIP.2009.252,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5197239,Analytic Hierarchy Process;Determine the matrix;Evaluation system of local finance,Application software;Decision making;Educational institutions;Finance;Financial management;Information analysis;Information processing;Power generation economics;Stability;System testing,finance,financial evaluation index weight;financial evaluation system;strict consistency test,,0,,4,,no,18-19 July 2009,,IEEE,IEEE Conference Publications
Notice of Retraction<BR>Research and Implementation of AHP-Based Method Base - Model Base Application of Hierarchical Model,L. Gan; X. Wang; R. Li,"Sch. of Inf. Eng., East China Jiao Tong Univ., Nanchang, China",2009 Pacific-Asia Conference on Knowledge Engineering and Software Engineering,20100115,2009,,,107,110,"Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR>This paper introduces that the representation and storage of hierarchical model via a database table, achieves the dynamic generation and management of model. It main achieves model base based on AHP. That addresses a number of unstructured or semi-structured problems such as assessment and prediction in the modeling. Focusing on the creation of their corresponding AHP-based method base is to solve practical problems. Experiments show that this method base can be well integrated with model base. So it will be applied in common AHP cases.",,Electronic:978-0-7695-3916-4; POD:978-1-4244-5324-5,10.1109/KESE.2009.36,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5383609,AHP;method base;model base;structure,Application software;Conference management;Data engineering;Databases;Electronic mail;Engineering management;Gallium nitride;Knowledge engineering;Knowledge management;Software engineering,database management systems,AHP-based method base;analytic hierarchy process;database table;hierarchical model;semi-structured problem;unstructured problem,,1,,5,,no,19-20 Dec. 2009,,IEEE,IEEE Conference Publications
Novel overlay/underlay cognitive radio waveforms using SD-SMSE framework to enhance spectrum efficiency- part i: theoretical framework and analysis in AWGN channel,V. Chakravarthy; X. Li; Z. Wu; M. A. Temple; F. Garber; R. Kannan; A. Vasilakos,"Air Force Research Laboratory, Wright-Patterson Air Force Base, Dayton, OH",IEEE Transactions on Communications,20091211,2009,57,12,3794,3804,"Recent studies suggest that spectrum congestion is primarily due to inefficient spectrum usage rather than spectrum availability. Dynamic spectrum access (DSA) and cognitive radio (CR) are two techniques being considered to improve spectrum efficiency and utilization. The advent of CR has created a paradigm shift in wireless communications and instigated a change in FCC policy towards spectrum regulations. Within the hierarchical DSA model, spectrum overlay and underlay techniques are employed to enable primary and secondary users to coexist while improving overall spectrum efficiency. As employed here, spectrum overlay exploits unused (white) spectral regions while spectrum underlay exploits underused (gray) spectral regions. In general, underlay approaches use more spectrum than overlay approaches and operate below the noise floor of primary users. Spectrally modulated, spectrally encoded (SMSE) signals, to include orthogonal frequency domain multiplexing (OFDM) and multi-carrier code division multiple access (MC-CDMA), are candidate CR waveforms. The SMSE structure supports and is well suited for CR-based software defined radio (SDR) applications. This paper provides a general soft decision SMSE (SDSMSE) framework that extends the original SMSE framework to achieve synergistic CR benefits of overlay and underlay techniques. This extended framework provides considerable flexibility to design overlay, underlay and hybrid overlay/underlay waveforms that are scenario dependent. Overlay/underlay framework flexibility is demonstrated herein for a family of SMSE signals, including OFDM and MC-CDMA. Analytic derivation of CR error probability for overlay and underlay applications is presented. Simulated performance analysis of overlay, underlay and hybrid overlay/underlay waveforms is also presented and benefits discussed, to include improved spectrum efficiency and channel capacity maximization. Performance analysis of overlay/underlay CR waveform in fading channels wil- l be discussed in Part II of the paper.",0090-6778;00906778,,10.1109/TCOMM.2009.12.080400,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5351675,Cognitive radio;dynamic spectrum access;overlay waveform;spectrum efficiency;underlay waveform,AWGN channels;Additive white noise;Chromium;Cognitive radio;FCC;Gaussian noise;Multicarrier code division multiple access;OFDM;Performance analysis;Wireless communication,AWGN channels;OFDM modulation;channel capacity;code division multiple access;cognitive radio;fading channels;software radio,AWGN channel;CR-based software defined radio;SD-SMSE framework;channel capacity maximization;cognitive radio;dynamic spectrum access;fading channels;hierarchical DSA model;multi-carrier code division multiple access;orthogonal frequency domain multiplexing;overlay-underlay cognitive radio waveforms;spectrum availability;spectrum congestion;spectrum efficiency;spectrum overlay,,73,,33,,no,9-Dec,,IEEE,IEEE Journals & Magazines
On new screw propeller driven by a flexible shaft consisted of spheral gears and gimbals,Haijun Xu; Yuncun Pan; Xiaojun Xu; Han Zhou,"School of Mechatronics Engineering and Automation, National University of Defense Technology, Changsha, Hunan, 410073, China",2009 ASME/IFToMM International Conference on Reconfigurable Mechanisms and Robots,20090724,2009,,,189,194,"Taking the new screw propeller driven by a flexible transmission shaft into consideration, some analytic work has been brought out on the relationship between its input and output motions with its math model. With the help of the CFD software FLUENT, the paper also presented some research work on the hydro-resistance force and pressure distribution of the propeller, which is set into three stances with a aberrancy of 0deg, 45deg, 90deg . Under each stance, three kinds of flow velocities were given, they were 0.5 m/s, 1 m/s, 2 m/s respectively. The hydrokinetic simulation showed the screw propeller driven by a controllable flexible had better hydrokinetics than the one driven by a traditional rigid shaft. A prototype was made and some experiments had been carried out to validate the motion and function of the new screw propeller.",,POD:978-88-89007-37-2,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5173829,Flexible shaft;Hydrodynamics;Screw propeller;Spheral gear;Underwater robot,Automation;Fasteners;Gears;Mechatronics;Moon;Motion analysis;Propellers;Robots;Shafts;Shape,computational fluid dynamics;fasteners;gears;hydrodynamics;mathematical analysis;propellers;shafts,CFD software FLUENT;computational fluid dynamics;flexible transmission shaft;gimbals;hydrokinetic simulation;hydroresistance force;math modelling;pressure distribution;screw propeller;spheral gears,,0,,16,,no,22-24 June 2009,,IEEE,IEEE Conference Publications
One Method to Evaluate the Safety of Train Control Center,P. Zhou; H. Xu; J. Hou,"State Key Lab. of Rail Traffic Control Safety, Beijing Jiaotong Univ., Beijing","2009 International Asia Conference on Informatics in Control, Automation and Robotics",20090206,2009,,,20,23,"The train control center which plays a central role of message communication is a pivotal sub-system of the china train control system. On the basis of developing and debugging the engineering prototype of the train control center, an index system for evaluating the safety of the train control center was set up. Based on the analytic hierarchy process, a safety evaluation method for the train control center was proposed. Using the technology of verification, validation and accreditation, comprehensive analysis to the safety evaluation index of the train control center was carried on. At last, the safety evaluation index of the whole train control center was obtained, which is useful to the nationalized train control center.",1948-3414;19483414,POD:978-0-7695-3519-7,10.1109/CAR.2009.75,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4777186,analytic hierarchy process;china train control system;safety evaluation;train control center,Automatic control;Centralized control;Communication system traffic control;Control systems;Debugging;Rails;Railway safety;Robotics and automation;Software safety;Traffic control,railway engineering;railway safety,China train control center safety evaluation method;analytic hierarchy process;index system;message communication;pivotal sub-system;train engineering prototype,,1,,15,,no,1-2 Feb. 2009,,IEEE,IEEE Conference Publications
Playing roles in design patterns: An empirical descriptive and analytic study,F. Khomh; Y. G. Gueheneuc; G. Antoniol,"Ptidej Team-DGIGL, &#201;cole Polytechnique de Montr&#233;al, Qu&#233;bec, Canada",2009 IEEE International Conference on Software Maintenance,20091030,2009,,,83,92,"This work presents a descriptive and analytic study of classes playing zero, one, or two roles in six different design patterns (and combinations thereof). First, we answer three research questions showing that (1) classes playing one or two roles do exist in programs and are not negligible and that there are significant differences among the (2) internal (class metrics) and (3) external (change-proneness) characteristics of classes playing zero, one, or two roles. Second, we revisit a previous work on design patterns and changeability and show that its results were, in a great part, due to classes playing two roles. Third, we exemplify the use of the study results to provide a ranking of the occurrences of the design patterns identified in a program. The ranking allows developers to balance precision and recall.",1063-6773;10636773,POD:978-1-4244-4897-5,10.1109/ICSM.2009.5306327,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306327,,Computer architecture;Pattern analysis;Software design,software engineering,design patterns;software design;software engineering,,4,,24,,no,20-26 Sept. 2009,,IEEE,IEEE Conference Publications
Preconditioned Electric Field Integral Equation Using Calderon Identities and Dual Loop/Star Basis Functions,M. B. Stephanson; J. F. Lee,"Dept. of Electr. & Comput. Eng., Ohio State Univ., Columbus, OH",IEEE Transactions on Antennas and Propagation,20090407,2009,57,4,1274,1279,"An analytic preconditioner for the electric field integral equation, based on the Calderon identities, is considered. It is shown, based on physical reasoning, that RWG elements are not suitable for discretizing the electric field integral operator appearing in the preconditioner. Instead, the geometrically dual basis functions proposed by Buffa and Christiansen are used. However, it is found that this preconditioner is vulnerable to roundoff errors at low frequencies. A loop/star decomposition of the Buffa-Christiansen basis functions is presented, along with numerical results demonstrating its effectiveness.",0018-926X;0018926X,,10.1109/TAP.2009.2016173,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4812227,Duality;electric field integral equation (EFIE);preconditioning,Application software;Convergence;Eigenvalues and eigenfunctions;Frequency;H infinity control;Integral equations;Moment methods;Roundoff errors;Scattering,electric field integral equations;electromagnetic wave scattering,Buffa-Christiansen basis functions;Calderon identities;RWG elements;analytic preconditioner;dual loop-star basis functions;geometrically dual basis functions;loop-star decomposition;preconditioned electric field integral equation,,49,,16,,no,9-Apr,,IEEE,IEEE Journals & Magazines
Prioritisation and Selection of Software Security Activities,D. Byers; N. Shahmehri,"Dept. of Comput. & Inf. Sci., Linkopings Univ., Linkoping","2009 International Conference on Availability, Reliability and Security",20090605,2009,,,201,207,"Software security is accomplished by introducing security-related activities into the software development process or by altering existing activities so that security is taken into account. Since the importance of software security has only relatively recently received the recognition it deserves, security is not ingrained into the development processes in common use today. A variety of approaches to software security have been proposed, but they rarely support developers in determining which security activities are appropriate for them and which they should choose to implement. An exception to this rule is the sustainable software security process (S<sup>3</sup>P). This paper describes the final step of the S<sup>3</sup>P, which helps developers estimate the cost of security-related activities and select the combination of security activities that best suits their needs. This is accomplished by applying the analytic hierarchy process and an automated search heuristic, scatter search, to the models created as part of the S<sup>3</sup>P.",,POD:978-1-4244-3572-2,10.1109/ARES.2009.52,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5066474,Software security;analytic hierarchy process;software engineering;software process improvement,Availability;Buffer overflow;Computer security;Costs;Failure analysis;Information science;Information security;Programming;Scattering;Software engineering,safety-critical software;security of data;software process improvement,analytic hierarchy process;software development process;software process improvement;software security selection;sustainable software security process,,1,,24,,no,16-19 March 2009,,IEEE,IEEE Conference Publications
ProcessLine: Visualizing time-series data in process industry,X. Luo; H. Wang; F. Tian; W. Liu; D. Teng; G. Dai,"Graduate University, Chinese Academy of Sciences, Beijing, China",2009 IEEE Symposium on Visual Analytics Science and Technology,20091113,2009,,,231,232,"In modern process industry, it is often difficult to analyze a manufacture process due to its numerous time-series data. Analysts wish to not only interpret the evolution of data over time in a working procedure, but also examine the changes in the whole production process through time. To meet such analytic requirements, we have developed ProcessLine, an interactive visualization tool for a large amount of time-series data in process industry. The data are displayed in a fisheye timeline. ProcessLine provides good overviews for the whole production process and details for the focused working procedure. A preliminary user study using beer industry production data has shown that the tool is effective.",,POD:978-1-4244-5283-5,10.1109/VAST.2009.5333421,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5333421,Business visualization;Time-series data;Visual analytics;Visual design,Beverage industry;Data analysis;Data visualization;Logistics;Manufacturing industries;Manufacturing processes;Production;Testing;Time series analysis;Visual analytics,brewing industry;data visualisation;production engineering computing;time series,ProcessLine;beer industry production data;interactive visualization tool;process industry;production process;time-series data visualization,,0,,6,,no,12-13 Oct. 2009,,IEEE,IEEE Conference Publications
Prototype Implementation of a Goal-Based Software Health Management Service,M. Barry; G. Horvath,"Kestrel Technol., LLC, Los Altos, CA, USA",2009 Third IEEE International Conference on Space Mission Challenges for Information Technology,20090828,2009,,,117,124,"The FAILSAFE project is developing concepts and prototype implementations for software health management in mission-critical real-time embedded systems. The project unites features of the industry standard ARINC 653 Avionics Application Software Standard Interface and JPL's Mission Data System (MDS) technology. The ARINC 653 standard establishes requirements for the services provided by partitioned real-time operating systems. The MDS technology provides a state analysis method, canonical architecture, and software framework that facilitates the design and implementation of software-intensive complex systems. We use the MDS technology to provide the health management function for an ARINC 653 application implementation. In particular, we focus on showing how this combination enables reasoning about and recovering from application software problems. Our prototype application software mimics the space shuttle orbiter's abort control sequencer software task, which provides safety-related functions to manage vehicle performance during launch aborts. We turned this task into a goal-based function that, when working in concert with the software health manager, aims to work around software and hardware problems in order to maximize abort performance results. In order to make it a compelling demonstration for current aerospace initiatives, we additionally imposed on our prototype a number of requirements derived from NASA's Constellation Program. Lastly, the ARINC 653 standard imposes a number of requirements on the system integrator for developing the requisite error handler process. Under ARINC 653, the health monitoring (HM) service is invoked by an application calling the application error service or by the operating system or hardware detecting a fault. It is these HM and error process details that we implement with the MDS technology, showing how a state-analytic approach is appropriate for identifying fault determination details, and showing how the framework supp- orts acting upon state estimation and control features in order to achieve safety-related goals. We describe herein the requirements, design, and implementation of our software health manager and the software under control. We provide details of the analysis and design for the phase II prototype, and describe future directions for the remainder of phase II and the new topics we plan to address in phase III.",,POD:978-0-7695-3637-8,10.1109/SMC-IT.2009.21,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5226840,ARINC 653;IVHM;MDS;STTR;Software Health Management;State Analysis;State-based Architecture,Application software;Embedded software;Hardware;Operating systems;Project management;Prototypes;Real time systems;Software performance;Software prototyping;Software standards,aerospace computing;embedded systems;error handling;formal specification;integrated software;safety-critical software;state estimation;system recovery,ARINC 653 standard;Avionics Application Software Standard Interface;FAILSAFE project;JPL;MDS technology;Mission Data System technology;NASA Constellation Program;abort control sequencer software task;aerospace initiative;application error service;application software problem recovery;canonical architecture;fault determination detail identification;goal-based function;goal-based software health management service;health monitoring service;launch abort;mission-critical real-time embedded system;partitioned real-time operating system;phase II prototype;requisite error handler process;safety-related function;software framework;software-intensive complex system;space shuttle orbiter;state analysis method;state estimation;system integrator requirement;vehicle performance management,,1,,5,,no,19-23 July 2009,,IEEE,IEEE Conference Publications
Research and Design of Computer-Aided English Textbook Evaluation System,X. Wang; Y. Yang; X. Wen,"Sch. of Int. Languages & Cultures, Beijing Union Univ., Beijing",2009 First International Workshop on Education Technology and Computer Science,20090526,2009,3,,913,917,"To break through the traditional one-sided qualitative evaluation of English textbook, the authors put forward a new model of evaluation system which includes three parts: teacherpsilas evaluation, studentpsilas evaluation, and the objective evaluation based on the corpus linguistics. By the analysis of evaluation theory of English textbook, 70 evaluation criteria are set out of 127 items using Delphi method. The proposed fuzzy analytic hierarchy process (AHP) is applied to eliminate subjective elements in weighting the evaluation criteria based on expertspsila knowledge and subjective judgments of textbook users. . The statistic data for objective evaluation criteria are obtained by using computer software to analyze the material in English textbook corpus. The quantitative analysis based on these data makes the evaluation results more objective and veracity. A detailed case study, illustrating the application of the proposed evaluation system to College English textbook evaluation is given.",,POD:978-0-7695-3557-9,10.1109/ETCS.2009.741,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4959456,Corpus;Criteria Weight;Delphi;Evaluation Criteria;Fuzzy AHP;Textbook Evaluation,Aggregates;Application software;Books;Computer science;Computer science education;Educational institutions;Educational technology;Fuzzy set theory;Natural languages;Statistical analysis,computer aided instruction;decision making;decision theory;educational administrative data processing;fuzzy set theory;linguistics;teaching,AHP;Delphi method;computer software;computer-aided English textbook evaluation system;corpus linguistics;fuzzy analytic hierarchy process;objective evaluation criteria;statistical analysis;student evaluation criteria;teacher evaluation criteria;teaching,,1,,18,,no,7-8 March 2009,,IEEE,IEEE Conference Publications
Research and Implementation of Wushu Video Retrieval Based on Semantic,Y. Hu; G. Zhai,"Sch. of Electron. & Inf. Eng., Lanzhou Jiaotong Univ., Lanzhou",2009 International Workshop on Intelligent Systems and Applications,20090612,2009,,,1,4,"This paper put forward a semantic-based video analytic approach and it's two algorithms in the Wushu video retrieval. Firstly, it can be taken a edge detection to the special-temporal slices by using the edge detection operator based on bilateral filtering, and then extracting and training the classes used the key frame semantic based on SVM. The system which is completed by the two algorithms implement the segmentation and retrieval of the Wushu video. The experiment show that it is better to retrieval according to the characters of the Wushu video by using this system.",,POD:978-1-4244-3893-8,10.1109/IWISA.2009.5073035,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5073035,,Art;Filtering;Gabor filters;Gunshot detection systems;Image edge detection;Lenses;Object detection;Partitioning algorithms;Software engineering;Support vector machines,content-based retrieval;edge detection;filtering theory;image segmentation;support vector machines;video retrieval,SVM;Wushu content-based video retrieval;bilateral filtering;edge detection operator;semantic-based video analytic approach;video segmentation algorithm,,0,,9,,no,23-24 May 2009,,IEEE,IEEE Conference Publications
Research of Secure Multicast Key Management Protocol Based on Fault-Tolerant Mechanism,G. Liu; J. Xu; M. Cao; F. Zhou; B. Zhang,"Coll. of Inf. Sci. & Eng., Northeastern Univ., Shenyang","2009 International Conference on Networks Security, Wireless Communications and Trusted Computing",20090505,2009,2,,560,563,"As multicasting is increasingly used as an efficient communication mechanism for group-oriented applications in the Internet, the research of the multicast key management is becoming a hot issue. Firstly, we analyze the <i>n</i>-party GDH.2 multicast key management protocol and point out that it has the following flaws: lack of certification, vulnerability to man-in-the-middle attacks, and a single-point failure. In order to settle the issues mentioned above, a fault-tolerant and secure multicast key management protocol (FTS, for short) with using the fault-tolerant algorithm and the password authentication mechanism is proposed in this paper. In our protocol, legal members are able to agree on a key despite failures of other members. The protocol can also prevent man-in-the-middle attacks. Finally, we evaluate the security of FTS, and compare our protocol with the FTKM through performance analysis. The analytic results show that the protocol not only avoids the single-point failure but also improves the comprehensive performance.",,POD:978-0-7695-3610-1,10.1109/NSWCTC.2009.273,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4908530,FTS;Multicast;fault-tolerant mechanism;key management,Authentication;Certification;Failure analysis;Fault tolerance;Internet;Law;Legal factors;Multicast algorithms;Multicast protocols;Performance analysis,authorisation;cryptographic protocols;fault tolerant computing;message authentication;multicast communication;public key cryptography,Internet;fault-tolerant mechanism;group-oriented applications;man-in-the-middle attacks;n-party GDH.2 multicast key management protocol;password authentication mechanism;public key cryptography;security evaluation;single-point failure,,0,,11,,no,25-26 April 2009,,IEEE,IEEE Conference Publications
Research on logistics distribution center location problem based on genetic algorithm and AHP,Li Ji; Dong Huailin,"Software School, Xiamen University, China",2009 4th International Conference on Computer Science & Education,20090901,2009,,,213,217,"Logistics distribution center is crucial to the whole logistics system. The location of the distribution is the key of the logistics system analysis. In this paper, numerous elements are taken into consideration to establish logistics distribution centers; to economic factors, best solutions are found by improved genetic algorithm; environmental and service factors are also involved by using AHP. The combination of both methods solves this problem both quantitatively and qualitatively, which makes final solution better in accordance with practical demands.",,POD:978-1-4244-3520-3,10.1109/ICCSE.2009.5228492,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5228492,AHP;genetic algorithm;istribution center;logistics,Costs;Environmental economics;Genetic algorithms;Heuristic algorithms;Linear programming;Logistics;Mathematical model;Production facilities;Software algorithms;Transportation,distribution strategy;genetic algorithms;logistics,analytic hierarchy process;genetic algorithm;location problem;logistics distribution center;logistics system analysis,,2,,4,,no,25-28 July 2009,,IEEE,IEEE Conference Publications
Research on Method of Key Spare Parts Estimation and its Application,J. Liu; H. Tu; F. Xai; D. Yu,"Sch. of Mech. & Electron. Eng., Nanchang Univ., Nanchang, China",2009 WRI World Congress on Software Engineering,20091110,2009,4,,492,496,"Key spare parts management plays an important role in equipment management. To ensure the quality of equipment maintenance, the quality of spare parts must be guaranteed. So the evaluation problem of similar spare parts supplier must be solved effectively, by which the management and optimization of key spare parts can be realized. Firstly, the mathematic model of key spare parts evaluation is introduced in detail. Then based on the principle of AHP, the algorithm of key spare parts evaluation is established, which consists with four step: (1)establish AHP model; (2)construct comparative judgment matrix; (3)calculate the weight ranking and matrix consistency judgment under single rule; (4)calculate synthetic weight relative to target layer of element at each layer. It is a method of qualitative analysis and quantitative analysis on complex decision-making problems. Finally the specific example was given, and the result showed its effectiveness.",,POD:978-0-7695-3570-8,10.1109/WCSE.2009.283,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5319606,evaluation;key spare parts;the analytic hierarchy process,Application software;Data analysis;Decision making;Engineering management;Equipment failure;Resource management;Security;Software engineering;Technical activities;Technology management,decision making;maintenance engineering;optimisation,AHP;comparative judgment matrix;complex decision making problems;equipment maintenance;equipment management;key spare parts estimation;matrix consistency;optimization;synthetic weight;weight ranking,,0,,5,,no,19-21 May 2009,,IEEE,IEEE Conference Publications
Research on task allocation for Product Cooperative Development,Yongming Wang; Xiaoliu Yu,"School of Mechanical Engineering, Anhui University of Technology, Ma'anshan, China",2009 2nd IEEE International Conference on Computer Science and Information Technology,20090911,2009,,,240,243,"Based on analysis of the requirements of task allocation for product cooperative development (PCD), an evaluation index system was established for task allocation. This paper introduced the mathematical method of fuzzy analytic hierarchy process (FAHP) firstly, and established a hierarchy evaluation model of task allocation for PCD, which uses time, quality, cost as the first class evaluation indexes and uses work efficiency, technical merit, cooperation ability, work experience, personnel interest, work attitude, personnel wages and cooperation expenses as the secondary class evaluation indexes. Based on FAHP method, this paper realized task allocation for complicated product cooperative development. A software prototype of task allocation tool for PCD is implemented, which has features of friendly interface, easy operation and rapid calculation, etc.",,POD:978-1-4244-4519-6,10.1109/ICCSIT.2009.5234428,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5234428,evaluation index system;fuzzy analytic hierarchy process;product cooperative development;task allocation,Collaborative work;Computer aided manufacturing;Costs;Decision making;Mechanical engineering;Online Communities/Technical Collaboration;Personnel;Product development;Production;Remuneration,fuzzy set theory;groupware;human computer interaction;performance index;product development;production engineering computing,FAHP method;cooperation ability;cooperation expenses;evaluation index system;friendly interface;fuzzy analytic hierarchy process;hierarchy evaluation model;personnel interest;personnel wage;product cooperative development;task allocation;technical merit;work attitude;work efficiency;work experience,,0,,9,,no,8-11 Aug. 2009,,IEEE,IEEE Conference Publications
Research on the Performance of xVM Virtual Machine Based on HPCC,T. Zhao; Y. Ding; V. March; S. Dong; S. See,"Guangdong Key Lab. of Comput. Network, South China Univ. of Technol., Guangzhou, China",2009 Fourth ChinaGrid Annual Conference,20091117,2009,,,216,221,"The virtual machine (VM) technology has received an increasing interest a spotlight both in the industry and the research communities. Although the potential advantages of virtualization in HPC workloads have been documented, the potential impact to application performance in HPC environments is not clearly understood. This paper presents a study on performance evaluation of virtual HPC systems using High Performance Computing Challenge (HPCC) benchmark suite and xVM as the workload representative and VM technology, respectively. Based on the extended AHP (Analytic Hierarchy Process) method, we propose an efficient performance evaluation model based on extended AHP and analyze the results and quantify the performance overhead of xVM in terms of compute, memory, and network overhead. Our analysis shows that the computational and network performance in HVM is slightly better and the memory performance is significantly better compared to paravirtualization.",1949-131X;1949131X,POD:978-0-7695-3818-1,10.1109/ChinaGrid.2009.36,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5328965,HPCC;Virtual Machine;xVM,Application virtualization;Computer networks;High performance computing;Laboratories;Operating systems;Performance analysis;Security;Virtual machine monitors;Virtual machining;Virtual manufacturing,software performance evaluation;virtual machines,AHP;HPCC;analytic hierarchy process;high performance computing challenge;xVM virtual machine,,3,,15,,no,21-22 Aug. 2009,,IEEE,IEEE Conference Publications
Resilience in computer systems and networks,K. S. Trivedi; D. S. Kim; R. Ghosh,"Dept. of Electrical & Computer Engineering Duke University, Durham, NC, USA",2009 IEEE/ACM International Conference on Computer-Aided Design - Digest of Technical Papers,20091228,2009,,,74,77,"The term resilience is used differently by different communities. In general engineering systems, fast recovery from a degraded system state is often termed as resilience. Computer networking community defines it as the combination of trustworthiness (dependability, security, performability) and tolerance (survivability, disruption tolerance, and traffic tolerance). Dependable computing community defined resilience as the persistence of service delivery that can justifiably be trusted, when facing changes. In this paper, resilience definitions of systems and networks will be presented. Metrics for resilience will be compared with dependability metrics such as availability, performance, performability. Simple examples will be used to show quantification of resilience via probabilistic analytic models.",1092-3152;10923152,CD-ROM:978-1-60558-800-1,10.1145/1687399.1687415,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5361311,Availability;Performability;Performance;Resilience;Survivability;continuous time Markov chain,Accidents;Application software;Availability;Computer crime;Computer networks;Maintenance;Permission;Resilience;Safety;Telecommunication traffic,computer network performance evaluation;computer network reliability;probability,availability;computer networks;computer systems;degraded system state;dependability metrics;fast recovery;general engineering systems;performability;performance;probabilistic analytic models;service delivery,,10,,27,,no,2-5 Nov. 2009,,IEEE,IEEE Conference Publications
RF energy harvesting design using high Q resonators,T. Ungan; X. Le Polozec; W. Walker; L. Reindl,"Lab. for Electrical Instrumentation, Dep. of Microsystem Engineering (IMTEK), University of Freiburg, Germany","2009 IEEE MTT-S International Microwave Workshop on Wireless Sensing, Local Positioning, and RFID",20091103,2009,,,1,4,"This paper presents a new method for rectifying low ambient radiation sources to supply autonomous measurement systems such as microsystems. An impedance transformation with high quality factor (Q) in front of a Schottky-diode using a quartz resonator at 24 MHz is presented. In addition to an analytic computation and nonlinear simulation of the rectifier circuit with advanced design system (ADS) software, the circuit was built and the efficiency of the RF energy harvesting system was measured. The design has been optimized to achieve maximum sensitivity. The measurement results show a sensitivity of -30 dBm (1 muW) for DC output voltage of 1 V and an efficiency of more than 22 %.",,POD:978-1-4244-5060-2,10.1109/IMWS2.2009.5307869,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5307869,RF energy harvesting;RF-to-DC conversion;passive RFID;ultra-low-power rectifiers;wireless energy transmission,Analytical models;Circuit analysis computing;Circuit simulation;Computational modeling;Impedance;Q factor;Radio frequency;Rectifiers;Software design;Software systems,Q-factor;Schottky diodes;circuit simulation;crystal resonators;energy harvesting,RF energy harvesting design;Schottky diode;advanced design system software;ambient radiation sources;autonomous measurement systems;frequency 24 MHz;high Q resonators;nonlinear rectifier circuit simulation;quality factor;quartz resonator,,13,,16,,no,24-25 Sept. 2009,,IEEE,IEEE Conference Publications
"RPM-A: Who will win the battle for the Gigabit Wireless in your home: WirelessHD, 802.11n, wireless USB, or UWB?",C. P. Yue; A. Jerng,"UC Santa Barbara, USA",2009 IEEE Radio Frequency Integrated Circuits Symposium,20090626,2009,,,cxlvii,cxlvii,"The extraordinary growth in the HD multimedia market, during the last few years, has created an eminent need for truly seamless interconnectivity of the various home entertainment appliances, broadband content streaming machines and personal computing devices. According to DisplaySearch, the worldwide sales of HDTV topped US$100B for the first time in 2007 with 200M units. Meanwhile, set-top box revenue reached 41M units in 2007 according to Instat. IDC projected a large leap in worldwide laptop shipments, from 108 million in 2007 to 148.2 million in 2008. Strategy Analytics estimated that 30M units of Blu-ray players will be sold in 2008, out of which 20M units are Play Stations 3 by Sony. Apple reported that since the launch of iStore, between July and September, more than 100M iPhone software applications (US$40M sales) have been downloaded - an unprecedented adoption rate by any measure. The common theme that all these trends share is a massive increase in the amount of digital content and the desire to share them seamlessly.",1529-2517;15292517,POD:978-1-4244-3377-3,10.1109/RFIC.2009.5135458,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5135458,,HDTV;High definition video;Home appliances;Home computing;Marketing and sales;Military computing;Multimedia computing;Multimedia systems;Streaming media;Universal Serial Bus,,,,3,,,,no,7-9 June 2009,,IEEE,IEEE Conference Publications
Simulation Based on Matlab Software for Vehicle Transmission Shaft,Z. H. Sun; Y. Y. Guo; Z. H. Sun,"Teaching Div. of Modern Educ. Technol., Ludong Univ., Yantai, China",2009 International Conference on Information Technology and Computer Science,20090804,2009,2,,365,368,"A method in this paper, which the theory of planar analytic geometry is used to model and simulate in vehicle transmission shaft, is firstly presented. This method is clear, uncomplicated and lucid, which offers a more convenient way to analyze vehicle transmission shaft kinematics characteristic so that it is suitable for project designs. The Matlab software is used to make the simulation in vehicle transmission shaft, through which the kinematics characteristic will be gained and the conclusion will provide theoretic reference for the disposal of the vehicle transmission shaft and mechanics in the future.",,POD:978-0-7695-3688-0,10.1109/ITCS.2009.212,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5190255,model and simulate;planar analytic geometry;vehicle transmission shaft,Analytical models;Axles;Bridges;Geometry;Kinematics;Mathematical model;Shafts;Solid modeling;Springs;Vehicles,geometry;mathematics computing;mechanical engineering computing;shafts;vehicle dynamics,Matlab software;planar analytic geometry;vehicle transmission shaft kinematics,,0,,3,,no,25-26 July 2009,,IEEE,IEEE Conference Publications
Simulation model to investigate flexible workload management for healthcare and servicescape environment,M. Thorwarth; A. Arisha; P. Harper,"School of Management, Dublin Institute of Technology, Dublin, D2, Ireland",Proceedings of the 2009 Winter Simulation Conference (WSC),20100311,2009,,,1946,1956,"High demand and poor staffing conditions cause avoidable pressure and stress among healthcare personnel which results in burnout symptoms and unplanned absenteeism which are hidden cost drivers. The work environment within an emergency department is commonly arranged in a flexible workload which is highly dynamic and complex for the outside observer. Using detailed simulation modeling within structured modeling methods, a comprehensive model to characterize the nurses' time utilization in such flexible dynamic workload environment was investigated. The results have been used to derive a generalized analytic expression that describes certain settings that lead to an instable queuing system with serious consequences for the healthcare facility. Thus decision makers are hence equipped with a tool which allows identifying and preventing such conditions that affect service quality level.",0891-7736;08917736,Electronic:978-1-4244-5771-7; POD:978-1-4244-5770-0,10.1109/WSC.2009.5429210,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5429210,,Application software;Environmental management;Hospitals;Medical services;Multitasking;Packaging;Personnel;Psychology;System performance;Technology management,behavioural sciences;health care;personnel;queueing theory,burnout symptoms;flexible workload management;healthcare environment;healthcare facility;healthcare personnel;queuing system;servicescape environment;simulation model;unplanned absenteeism,,7,,25,,no,13-16 Dec. 2009,,IEEE,IEEE Conference Publications
Simulation of Cutting Force Based on Software Deform,W. Youjun; Z. Dinghua; W. Fujia; Y. Kai; H. Zhongming,"Northwestern Polytech. Univ., Xi'an, China",2009 Second International Conference on Intelligent Computation Technology and Automation,20091016,2009,2,,224,227,"Using analytic method to analyze the change rule of cutting force fall across hardness, because it relate to non-linear problem that is perplexing. Base on software deform, infection factors were simulated in the process of cutting.The factors cutting include deepness and reamer angle and enter measure. The simulation get the result that include the influence of the factors and the change rule of cutting force. To The analysis and the simulation of the cuts process through the finite element software that is helpful in optimizes the geometry parameter of the cutting tool and the processing craft parameter choice, exert the efficiency of machine tool farthest. Has saved the cutting tool, the material and laboratory uses experimental expense and so on, reduced the experimental cycle. It has found the new more effective modernized tool for the cutting rule.",,POD:978-0-7695-3804-4,10.1109/ICICTA.2009.291,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5288127,DEFORM;cutting force;finite elemen;simulation,Analytical models;Cutting tools;Deformable models;Finite element methods;Geometry;Goniometers;Machine tools;Software measurement;Software tools;Solid modeling,cutting;cutting tools;finite element analysis;machine tools;mechanical engineering computing,cutting force;cutting tool;finite element software;infection factor;machine tool;software deform,,0,,5,,no,10-11 Oct. 2009,,IEEE,IEEE Conference Publications
Simulation research of the Mixed-Model production line based on Flexsim,H. Chen; D. Yang; H. Sun,"College Of Mechanical Engineering, Inner Mongolia University Of Technology, Huhhot, Inner Mongolia, china",2009 16th International Conference on Industrial Engineering and Engineering Management,20091204,2009,,,1876,1881,"To meet product diversification and individuation, the paper reconstructed the original single object production line using Flexsim simulation software. Reconstructed model was optimized using the ECRS (eliminate, combine, rearrange, simple) analytic method. The study indicate that the mixed-model production line making up three production line can raise the comprehensive utilization ratio of equipment.",,POD:978-1-4244-3671-2,10.1109/ICIEEM.2009.5344303,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5344303,Mixed-Model Production;Optimization;Restructuring;Simulation,Analytical models;Data analysis;Databases;Failure analysis;Graphical user interfaces;Mechanical engineering;Optimization methods;Production;Sun;Time measurement,digital simulation;production engineering computing,"Flexsim simulation software;eliminate, combine, rearrange, simple analytic method;mixed-model production line;product diversification;product individuation;single object production line",,0,,6,,no,21-23 Oct. 2009,,IEEE,IEEE Conference Publications
Software Maintainability Metrics Based on the Index System and Fuzzy Method,J. Chen; X. Liu,"Software Testing Center, Shandong Comput. Sci. Center, Jinan, China",2009 First International Conference on Information Science and Engineering,20100426,2009,,,5117,5120,"An index system is proposed to solve the problem of software maintainability metrics. On the basis of quantitative procedural information, this paper uses Analytic Hierarchy Process (AHP) to determine the weight of the evaluation indices, uses the fuzzy evaluation method to deal with the quality of the indices, gets the quantitative result of software maintainability evaluation and solves the multi-index evaluation problems effectively. A representative example is applied to prove the index system and the evaluation methods feasibility.",2160-1283;21601283,Electronic:978-1-4244-5728-1; POD:978-1-4244-4909-5,10.1109/ICISE.2009.1073,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5454554,,Fuzzy systems;IEC standards;ISO standards;Information analysis;Information science;Maintenance engineering;Software maintenance;Software measurement;Software quality;Software testing,fuzzy systems;hierarchical systems;software maintenance;software metrics,analytic hierarchy process;fuzzy evaluation method;index system;multi index evaluation problems;software maintainability evaluation;software maintainability metrics,,2,,5,,no,26-28 Dec. 2009,,IEEE,IEEE Conference Publications
Solving the traffic and flitter challenges with tulip,P. Simonetto; P. Y. Koenig; F. Zaidi; D. Archambault; F. Gilbert; T. T. Phan-Quang; M. Mathiaut; A. Lambert; J. Dubois; R. Sicre; M. Brulin; R. Vieux; G. Melancon,"INRIA Bordeaux Sud-Ouest and LaBRI, Universit&#233; de Bordeaux I, France",2009 IEEE Symposium on Visual Analytics Science and Technology,20091113,2009,,,247,248,"We present our visualization systems and findings for the badge and network traffic as well as the social network and geospatial challenges of the 2009 VAST contest. The summary starts by presenting an overview of our time series encoding of badge information and network traffic. Our findings suggest that employee 30 may be of interest. In the second part of the paper, we describe our system for finding subgraphs in the social network subject to degree constraints. Subsequently, we present our most likely candidate network which is similar to scenario B.",,POD:978-1-4244-5283-5,10.1109/VAST.2009.5334456,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5334456,F.2.2 [Theory of Computation]: Nonnumerical Algorithms and Problems‰ÛÓPattern matching;H.5.0 [Information Systems]: Information Interfaces and Presentation‰ÛÓGeneral,Computer interfaces;Computer networks;Encoding;Information systems;Social network services;Software algorithms;Telecommunication traffic;Visualization,business communication;computational geometry;data visualisation;encoding;graph theory;graphical user interfaces;query processing;social networking (online);time series,2009 VAST contest;Tulip graph drawing software;badge traffic;candidate network traffic;embassy leak;employee suspicious transmission;flitter challenge;geospatial challenge;graph visualization system;social network;time series encoding;traffic challenge;visual query generator interface,,0,,1,,no,12-13 Oct. 2009,,IEEE,IEEE Conference Publications
SpRay: A visual analytics approach for gene expression data,J. Dietzsch; J. Heinrich; K. Nieselt; D. Bartz,"ZBIT, University of T&#252;bingen, Germany",2009 IEEE Symposium on Visual Analytics Science and Technology,20091113,2009,,,179,186,"We present a new application, SpRay, designed for the visual exploration of gene expression data. It is based on an extension and adaption of parallel coordinates to support the visual exploration of large and high-dimensional datasets. In particular, we investigate the visual analysis of gene expression data as generated by micro-array experiments; We combine refined visual exploration with statistical methods to a visual analytics approach that proved to be particularly successful in this application domain. We will demonstrate the usefulness on several multidimensional gene expression datasets from different bioinformatics applications.",,POD:978-1-4244-5283-5,10.1109/VAST.2009.5333911,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5333911,Visual analytics;bioinformatics;gene expression experiments;large-scale microarray;microarray data,Application software;Bioinformatics;Computer graphics;Data analysis;Data visualization;Gene expression;Spraying;Statistical analysis;Terminology;Visual analytics,bioinformatics;genetics;statistical analysis,SpRay;bioinformatics applications;high-dimensional datasets;multidimensional gene expression datasets;parallel coordinates;statistical methods;visual analytics approach;visual exploration,,5,,30,,no,12-13 Oct. 2009,,IEEE,IEEE Conference Publications
SQuAVisiT: A Flexible Tool for Visual Software Analytics,M. v. d. Brand; S. Roubtsov; A. Serebrenik,"Dept. of Math. & Comput. Sci., Tech. Univ. Eindhoven, Eindhoven",2009 13th European Conference on Software Maintenance and Reengineering,20090410,2009,,,331,332,"We present the Software Quality Assessment and Visualization Toolset (SQuAVisiT), a flexible tool for visual software analytics. Visual software analytics supports analytical reasoning about software systems facilitated by interactive visual interfaces. In particular, SQuAVisiT assists software developers, maintainers and assessors in performing quality assurance and maintenance tasks. Flexibility of SQuAVisiT allows for integration of multiple programming languages and variety of analysis and visualization tools.SQuAVisiT has been successfully applied in a number of case studies, ranging from hundreds to thousands KLOC, from homogeneous to heterogeneous systems.",1534-5351;15345351,POD:978-1-4244-3755-9,10.1109/CSMR.2009.19,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4812790,Visual analytics;analysis;software quality;toolset,Computer architecture;Computer languages;Computer science;Information analysis;Software maintenance;Software quality;Software systems;Software tools;Visual analytics;Visualization,program diagnostics;program visualisation;software maintenance;software quality;user interfaces,SQuAVisiT;analytical reasoning;interactive visual interfaces;multiple programming languages;quality assurance;software development;software maintenance;software quality assessment and visualization toolset;software systems;visual software analytics,,2,,10,,yes,24-27 March 2009,,IEEE,IEEE Conference Publications
Structure and Kinematics Decoupling Analysis of a Novel 3D Translations Spatial Parallel Robot Mechanism,H. Chen; H. Song; Z. Zou,"Sch. of Mech. Eng., Shandong Univ. of Technol., Zibo, China",2009 International Conference on Artificial Intelligence and Computational Intelligence,20100112,2009,2,,303,307,"In this paper, a novel spatial parallel robot mechanism that can carry out three-dimensional translations was proposed. Based on topology structure design theory of robot mechanism, the structure and motion output were analyzed. The DOF and the coupling coefficient were calculated. According to the characteristics of the mechanism, the forward and inverse position models of mechanism were presented by using of the coordinates transformation theory and projection theory of analytic geometry. Depend on the conclusions of the forward solution, the decoupling of the input and output was analyzed. The kinematics decoupling characteristic of the mechanism was simulated and verified by ADAMS software. The research provided theory foundation for future study and industrial application.",,Electronic:978-1-4244-3836-5; POD:978-1-4244-3835-8,10.1109/AICI.2009.129,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5375757,decoupling simulated;forward and inverse solution;parallel robot mechanism,Artificial intelligence;Computational intelligence;Industrial control;Inverse problems;Kinematics;Manipulator dynamics;Mechanical engineering;Motion analysis;Parallel robots;Topology,control system synthesis;geometry;robot kinematics,3D translations spatial parallel robot mechanism;ADAMS software;analytic geometry;coordinates transformation theory;degree of freedom;forward position model;inverse position model;kinematics decoupling analysis;motion output analysis;projection theory;structure analysis;topology structure design theory,,0,,8,,no,7-8 Nov. 2009,,IEEE,IEEE Conference Publications
Tactical cross-domain solutions: Current status and the need for change,K. Plyler; B. C. Tague; R. Thomas; S. Tsang,"General Dynamics C4 Systems, USA",MILCOM 2009 - 2009 IEEE Military Communications Conference,20100115,2009,,,1,7,"The rapid migration of system high information sharing to the tactical edge has made it imperative that the DoD reexamine tactical Cross Domain Solutions/Enterprise Services (CDS/ES). Prior to Operation Iraqi Freedom (OIF), information sharing requirements at the tactical edge were relatively few in number and nominal in terms of data throughput, data types, and users. Cross Domain Solutions (CDS) deployed back then were specialized, hardened, and resistant to hacking in the event of enemy overrun. Since OIF, both the volume of battle field system high tactical networks as well as the operational requirements to support each of these networks (i.e., increased data throughput, data types and variety of users) have significantly increased [On Point]. When combined with additional constraints inherent to the battle space such as low latency, Size, Weight and Power (SWaP), the current approach to addressing information sharing requirements in a tactical network breaks down. Taking the traditional, point-to-point approach by making a CDS smaller and more robust in the tactical environment may be adequate in the near term. However, this approach will not support the requirements levied upon next generation warfighting systems. In the future, interdependent tactical networks will be required to exhibit a dynamic (self organizing) nature, supporting adaptability and quick response to data ingress and egress. Nodes on these future networks will also need to operate with severely limited bandwidth and other operational and/or environment constraints. Therefore it is necessary to examine current and future information sharing requirements at the tactical edge from both the CDS/ES developer as well as user perspective. This position paper will discuss both perspectives in order to allow a better understanding of the current CD problem space, as well as gain insight into building the next generation CDS/ES.",2155-7578;21557578,POD:978-1-4244-5238-5,10.1109/MILCOM.2009.5379749,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5379749,,Artificial satellites;Costs;Government;Hardware;Intelligent networks;Military satellites;Packet switching;Performance analysis;Performance evaluation;Software standards,computer network security;military communication,Operation Iraqi freedom;battlefield system;data egress;data ingress;data throughput;data types;information sharing;interdependent tactical networks;multilevel security;next generation warfighting systems;point-to-point approach;tactical cross-domain solutions-enterprise sevices;user perspective,,0,,12,,no,18-21 Oct. 2009,,IEEE,IEEE Conference Publications
Tentative Application of Computer Simulation Technique to the Risk Evaluation of Large Concrete Dams,S. Hui; C. Zaitie; W. Zufu,"Shazhou Inst. of Technol., Zhangjiagang, China",2009 International Forum on Computer Science-Technology and Applications,20100119,2009,3,,172,175,"Failure probability calculation is the scientific basis for the quantitative analysis of large dam risk and thus, probing the method of calculating large dams' failure probability is of considerable scientific significance and is urgently required by engineering application. While risk evaluation is required for individual large dams, no country can afford the failure calamity test on actual large dams in view of the vast investment in the large dam construction, and it is only possible to reveal the mechanism of the failure by means of computer simulation of process of failure occurrence, on the basis of both indoor model experiment and large-dam operation monitoring data. At present, in the field of large dam engineering, JC method and Monte Carlo method are generally recommended for the quantitative calculation of the failure probability. Due to the complexity of large dams' structure and their boundary conditions, the randomicity of the load and resistance force and the fact that there exists no analytic expression between the output and input quantities but the usual implicit function, the calculating of large dam's failure probability by means of JC method or Monte Carlo method cannot do without computer numerical simulation to some extent. A tentative approach is made in making use of computer simulation technique to generate the random number of random variables of both the load and the resistance force, to establish the state function of the failure modes and to conduct independent transformation of relevant random variables. Based on the computer simulation results, the failure probability of a large dam is calculated out by means of JC method and Monte Carlo method respectively.",,Electronic:978-1-4244-5423-5; POD:978-0-7695-3930-0; POD:978-1-4244-5422-8,10.1109/IFCSTA.2009.281,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5384784,computer simulation technique;failure probability;large dam;risk evaluation,Application software;Computer simulation;Computerized monitoring;Concrete;Failure analysis;Investments;Probability;Random variables;Risk analysis;Testing,Monte Carlo methods;dams;digital simulation;risk management;structural engineering computing,JC method;Monte Carlo method;computer numerical simulation;computer simulation technique;failure calamity test;failure probability;large concrete dams;risk evaluation,,0,,8,,no,25-27 Dec. 2009,,IEEE,IEEE Conference Publications
The Analysis of the Hindering Factors of Implementation of Green Credit Policy Based on Fuzzy Comprehensive Evaluation,X. Lu; Z. Lin; H. Xue,"Sch. of Software, Nanchang Univ., Nanchang, China",2009 International Conference on Management and Service Science,20091030,2009,,,1,4,"Green credit policy as a new standard for bank loan has been known by more and more people. It is not only a new method to avoid bank's risks, but also a new measure to promote energy-saving, emission reduction and sustainable development. More qualitative analysis of green credit has been done. Yet, through the method of analytic hierarchy process (AHP) to ensure the weights of each index and setting up membership function by use of statistical methods, this paper evaluates the extent of obstacle of green credit and the size of the impact of various factors by fuzzy comprehensive evaluation. This paper has been designed to provide advice for decision-making.",,POD:978-1-4244-4638-4,10.1109/ICMSS.2009.5304083,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5304083,,Energy consumption;Environmental economics;Environmentally friendly manufacturing techniques;Forward contracts;Government;Industrial pollution;Mathematics;Power generation economics;Protection;Sustainable development,banking;credit transactions;decision making;environmental economics;fuzzy set theory;risk analysis;statistical analysis;sustainable development,analytic hierarchy process;bank loan;bank risk;decision making;emission reduction;energy saving;fuzzy comprehensive evaluation;green credit policy;hindering factor analysis;membership function;statistical method;sustainable development,,0,,11,,no,20-22 Sept. 2009,,IEEE,IEEE Conference Publications
The Architecture and Implementation of the New Generation Business System in a Commercial Bank,Z. Sheng; Z. Gang; C. Qing; W. Yan; L. Pingping,"Coll. of Inf. Eng., Taiyuan Univ. of Technol., Taiyuan, China",2009 International Conference on Business Intelligence and Financial Engineering,20090821,2009,,,501,504,"At present, there are many problems in domestic commercial banks. This paper describes the fundamental solution to these problems through the presentation of a domestic commercial bank, which integrates various business channels by centralizing business platforms and altering the business model based on accounting treatment into the operation model of customer-oriented. It builds a new generation of business system that is divided into access layer, support layer, application service layer and business system layer which respectively implements the functions of docking client, unifying data format, exchanging data, database services and etc. This paper describes the new architecture, technical support and the physical configuration of the new system. The new system includes eight application subsystems: Core and auxiliary systems; EAI-synthetical front-end system; plug-business system; agency business system; managed application system; analytic applications system; Payment, UnionPay and other external front-end system; Fore-end service channel and their support system. Meanwhile, take the business process of core business inquiring and reports printing for example to show the effectiveness of the new system.",,POD:978-0-7695-3705-4,10.1109/BIFE.2009.119,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5208834,B-CBS;B-EAI;C/S;CRM,Access protocols;Application software;Asset management;Business;Databases;Educational institutions;Forward contracts;Java;Middleware;Printing,banking;business process re-engineering,Core and auxiliary systems;EAI-synthetical front-end system;UnionPay;access layer;accounting treatment;agency business system;analytic applications system;application service layer;business model;business process;business system layer;domestic commercial banks;financial IT system platform;fore-end service channel;managed application system;new generation business system;plug-business system;support layer,,2,,6,,no,24-26 July 2009,,IEEE,IEEE Conference Publications
The Construction of Autonomous-resource Systems in åÒZero-approachåÓ Teaching Mode Based on Web,X. Tian,"Dept. of English, Wuhan Univ. of Sci. & Technol., Wuhan, China",2009 Second Pacific-Asia Conference on Web Mining and Web-based Application,20090904,2009,,,192,196,"With the teaching reform, ""zero-approach""teaching mode has been much more popular than before. The core spirit of the reform is to develop autonomy of students in universities. During the course of development, autonomous-material system is necessary to be built based on Web. The author has investigated the mode and put forward some useful measures to comply with present teaching reform in universities. In the investigation,the author paid much attention to the usages of methods of natural field observation and analytic statistical ones to get some effective conclusions.",,POD:978-0-7695-3646-0,10.1109/WMWA.2009.42,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5232499,autonomou-resource system;autonomous learning;zero-approach teaching,Building materials;Education;Educational technology;Information analysis;Materials testing;Software development management;Software testing;System testing,Internet;computer aided instruction;educational institutions;teaching,World Wide Web;autonomous-material system;autonomous-resource system;teaching reform;university;zero-approach teaching mode,,0,,6,,no,6-7 June 2009,,IEEE,IEEE Conference Publications
The Evaluation of College Teacher's Ability of Using Information Technology Based on Analytic Hierarchy Process Method,L. Danping,"Dept. of Teaching Affairs, Hohai Univ., Changzhou, China",2009 International Forum on Computer Science-Technology and Applications,20100119,2009,3,,74,77,"This thesis discusses the evaluation function of college teacher's ability of using information technology including oriented function, inspiring function and ensuring function, and establishes the evaluation index system including information ideals, information circumstances, basic knowledge, using ability and effects. This thesis studies its index sequence and weight with Analytic Hierarchy Process method and provides a quantitative analyzing method for colleges to practice the evaluation.",,Electronic:978-1-4244-5423-5; POD:978-0-7695-3930-0; POD:978-1-4244-5422-8,10.1109/IFCSTA.2009.256,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5384735,AHP;Analytic Hierarchy Process;college teacher;evaluation;index system;information technology,Application software;Computer applications;Computer science education;Continuing education;Educational institutions;Educational technology;Information analysis;Information technology;Multimedia systems;Safety,computer aided instruction;decision making;ergonomics;information technology;teacher training;teaching,analytic hierarchy process method;basic knowledge;college teachers ability evaluation;ensuring function;evaluation index system;information circumstances;information ideals;information technology;inspiring function;oriented function;quantitative analyzing method,,0,,8,,no,25-27 Dec. 2009,,IEEE,IEEE Conference Publications
The Evaluation of Software Trustworthiness with FAHP and FTOPSIS Methods,L. Shi; S. Yang,"Inst. of Comput. Network Syst., Hefei Univ. of Technol., Hefei, China",2009 International Conference on Computational Intelligence and Software Engineering,20091228,2009,,,1,5,"Trustworthy software has attracted increasing concern both in academia and industry. How to effectively evaluate the trustworthiness is becoming a closed question. This paper models the software trustworthiness evaluation (STE) problem as a multi-criteria decision-making (MCDM) problem, and proposes both an evaluation framework and a practical approach to evaluate the software trustworthiness based on the fuzzy analytic hierarchy process (FAHP) and fuzzy technique for order preference by similarity to ideal solution (FTOPSIS) methods. FAHP method is utilized to obtain the weights of evaluation criteria. The FTOPSIS method is used to determine the final ranking of the software alternatives. The uncertainty and vagueness included in evaluation procedure are represented as fuzzy triangular numbers. Finally, the proposed method is applied to the case study of evaluating the trustworthiness of project management (PM) software alternatives for a car manufacturer in China.",,CD-ROM:978-1-4244-4507-3,10.1109/CISE.2009.5365827,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5365827,,Application software;Computer industry;Computer network management;Computer networks;Decision making;Machine tools;Manufacturing;Project management;Software quality;Uncertainty,decision making;fuzzy set theory;number theory;operations research;security of data;software reliability,China;car manufacturer;fuzzy analytic hierarchy process;fuzzy technique for order preference by similarity to ideal solution method;fuzzy triangular numbers;multicriteria decision-making problem;project management software alternatives;software trustworthiness evaluation,,2,,23,,no,11-13 Dec. 2009,,IEEE,IEEE Conference Publications
The evaluation system of programming curriculums,L. Gao; F. Wang; X. f. Jiang,"College of Computer, Northwestern Polytechnical University, Xi'an, 710072, China",2009 IEEE International Symposium on IT in Medicine & Education,20090915,2009,1,,685,689,"In view of the paperless teaching and the test situation of programming curriculums, this paper employed the analytic hierarchy process (AHP) to construct achievement evaluation indicator system and the examination paper evaluation indicator system for programming curriculums group. This evaluation indicator system made full use of the wealth of information of the process of paperless teaching and examination. Finally, an evaluation software system was formed based on this evaluation indicator system. This evaluation software system can be used as a basis of the teaching evaluation for the teaching management department as well as a method of analyzing student's study situation and teacher's teaching situation.",,POD:978-1-4244-3928-7,10.1109/ITIME.2009.5236336,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5236336,,Computer science education;Education courses;Educational institutions;Educational programs;Educational technology;Paper technology;Programming profession;Quality management;Software systems;System testing,C++ language;decision making;educational computing;educational courses;teaching,achievement evaluation indicator system;analytic hierarchy process;evaluation software system;paperless teaching;programming curriculums evaluation system;teaching evaluation,,0,,6,,no,14-16 Aug. 2009,,IEEE,IEEE Conference Publications
The impact of priority generations in a multi-priority queueing system ‰ÛÓ A simulation approach,A. Krishnamoorthy; V. C. Narayanan; S. R. Chakravarthy,"Department of Mathematics, Cochin University of Science and Technology, Kochi 682022, India",Proceedings of the 2009 Winter Simulation Conference (WSC),20100311,2009,,,1622,1633,"In this paper, we consider a preemptive (multiple) priority queueing model in which arrivals occur according to a Markovian arrival process (MAP). An arriving customer belongs to priority type i, 1 ÌâåÀ i ÌâåÀ m + 1, with probability p<sub>i</sub>. The highest priority, labeled as 0, is generated by other priority customers while waiting in the system and not otherwise. Also, a customer of priority i can turn into a priority j, j ÌâåÀ i, 1 ÌâåÀ i, j ÌâåÀ m + 1, customer, after a random amount of time that is assumed to be exponentially distributed with parameter depending on the priority type. The waiting spaces for all but priority type m + 1 are assumed to be finite. The (m + 1)-st priority customers have unlimited waiting space. At any given time, the system can have at most one highest priority customer. Thus, all priority customers except the (m +1) - st are subject to loss. Customers are served on a first-come-first-served basis within their priority by a single server and the service times are assumed to follow a phase type distribution that may depend on the customer priority type. This queueing model, which is a level-dependent quasi-birth-and-death process, is amenable for investigation algorithmically through the well-known matrix-analytic methodology. However, here we propose to study through simulation using ARENA, a powerful simulation software as some key measures such as the waiting time distributions are highly complex to characterize analytically. The simulated results for a few scenarios are presented.",0891-7736;08917736,Electronic:978-1-4244-5771-7; POD:978-1-4244-5770-0,10.1109/WSC.2009.5429269,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5429269,,Analytical models;Educational institutions;Government;Manufacturing industries;Mathematics;Medical services;Queueing analysis;Software measurement;Time measurement;Virtual manufacturing,Markov processes;customer services;digital simulation;probability;queueing theory,ARENA;Markovian arrival process;customer service;first-come-first-served basis;matrix-analytic methodology;preemptive multipriority queueing model;priority generation;probability;quasi-birth-and-death process;simulation approach;simulation software,,1,,11,,no,13-16 Dec. 2009,,IEEE,IEEE Conference Publications
The management of crowdsourcing in business processes,S. Curran; K. Feeney; R. Schaler; D. Lewis,"Centre for Next Generation Localization: Trinity College Dublin, Ireland",2009 IFIP/IEEE International Symposium on Integrated Network Management-Workshops,20090807,2009,,,77,78,"The Centre for Next Generation localization [CNGL] is developing a number of systems in order to investigate the issues that arise in integrating centralized workflows with community-based value creation in the form of crowd- sourced localization. Consistent with current workflow and system integration practice these developed studies adopt a service oriented architecture, using Web service and Web service orchestration standards [bpel], integrated with data interchange standards from the localization industry, e.g. XLIFF. Idiom Worldserver, a leading commercial localization workflow management system was used to implement the segmentation, TM reuse, and reassembly portions of the workflow. A BPEL Glassflsh Web service orchestration engine was used to support several configurations of academic, open source and commercial text analytics and machine translation. Translation crowdsourcing was implemented through a plugin in the Drupal CMS. This integration provides us with an experimental platform for further investigating the integration of industrial workflows in this knowledge-based industry with the potential for crowdsourcing certain activities while being able to manage the end-to-end quality of the overall workflow.",,POD:978-1-4244-3923-2,10.1109/INMW.2009.5195939,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5195939,,Conferences,Web services;business data processing;social networking (online);software architecture;workflow management software,BPEL Glassflsh Web service orchestration engine;Idiom Worldserver;Next Generation localization Centre;Web service orchestration standards;business processes;centralized workflows;community-based value creation;crowd-sourced localization;crowdsourcing management;data interchange standards;knowledge-based industry;localization industry;localization workflow management system;machine translation;service oriented architecture;text analytics;translation crowdsourcing,,4,,5,,no,1-5 June 2009,,IEEE,IEEE Conference Publications
The optimization of the fluid machinery design parameter,Yindong Zhang; Haigui Kang; Hongpeng Zhang; Yulong Ji,"State Key Laboratory of Coastal and Offshore Engineering, Dalian University of Technology, Liaoning Province, China",2009 International Conference on Mechatronics and Automation,20090918,2009,,,2329,2333,"The method of CFD integrated with CAD is presented to optimize fluid machinery design parameter in the paper. Firstly, the parameter to be optimized is determined. Secondly, the CAD technology is applied to build three-dimensional fluid machinery parameterization models by the parameter in its range, which is used to calculate three-dimensional flow field in the fluid machinery model by CFD software. Then, with the evaluation indexes established, the calculation results of the inner flow field in the each parametric model are evaluated by synthesis of Analytic Hierarchy Process and Fuzzy Comprehensive Evaluation. Finally, by comparing the evaluation value of every parametric model, the optimal design parameter of the fluid machinery model is achieved.",2152-7431;21527431,POD:978-1-4244-2692-8,10.1109/ICMA.2009.5246567,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5246567,CAD;CFD;fluid machinery;optimization,Computational fluid dynamics;Design automation;Design optimization;Grid computing;Machinery;Mechatronics;Paper technology;Parametric statistics;Petroleum;Solid modeling,CAD;decision making;fluid dynamics;fuzzy set theory;mechanical engineering computing;plastic flow,3D fluid machinery parameterization model;CAD;analytic hierarchy process;evaluation index;fluid machinery design parameter;fuzzy comprehensive evaluation;inner flow field;optimal design parameter;parametric model,,0,,6,,no,9-12 Aug. 2009,,IEEE,IEEE Conference Publications
The service architecture of real-time video analytic system,T. Yu; B. Zhou; Q. Li; R. Liu; W. Wang; C. Chang,"HP Labs China, Beijing, China",2009 IEEE International Conference on Service-Oriented Computing and Applications (SOCA),20100208,2009,,,1,8,"Videos are an important source of information and require extremely computation expensive analysis in order to understand the high level semantics. The computational difficulties in extracting embedded information and bridging the semantic gap present the major challenges in interoperability, scalability and real-time response of video analytic systems. In this paper, we propose an intelligent video analytic system VIP (Video Intelligence Platform) that provides a distributed scalable infrastructure for supporting near real-time video stream analysis. VIP is based on service-orient architecture (SOA), in which the video analysis computation modules are wrapped as services and composed in a directed acrylic graph (DAG) structure to represent the application requirements. VIP leverages UIMA (Unstructured Information Management Architecture) framework as the data flow control engine and multiple commodity databases as the storage and computation resources. The actual executions of video analysis have been pushed down into database engine to minimize the data movement cost. We initially choose video surveillance in retail store as a representative application domain. As a case study, a prototype system has been developed to achieve fundamental functions of real-time video surveillance, including video capture/store, human detection/tracking and customer shopping trajectory analysis.",2163-2871;21632871,Electronic:978-1-4244-5299-6; POD:978-1-4244-5300-9,10.1109/SOCA.2009.5410267,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5410267,UIMA;dataflow;distributed;integration;interoperability;multi-databases;scalability;service-oriented architecture;video surveillance,Computational intelligence;Computer architecture;Databases;Embedded computing;Engines;Information analysis;Information resources;Intelligent systems;Real time systems;Video surveillance,directed graphs;information management;open systems;software architecture;video databases;video retrieval;video surveillance,SOA;VIP;customer shopping trajectory analysis;data flow control engine;directed acrylic graph structure;embedded information extraction;human detection;human tracking;intelligent video analytic system;interoperability;multiple commodity database;real time video analytic system;scalability;semantic gap;service architecture;service-orient architecture;unstructured information management architecture;video intelligence platform;video stream analysis;video surveillance,,3,,16,,no,14-15 Jan. 2009,,IEEE,IEEE Conference Publications
The Structure of Sale Teams' Cohesion: Confirmatory Factor Analysis,Z. Chen; H. Yu; Y. Hao,"Sch. of Manage., Hebei Univ. of Technol., Tianjin, China",2009 International Conference on Computational Intelligence and Software Engineering,20091228,2009,,,1,4,"The purpose of this study is to explore the dimension structure of sale teams' cohesion. Based on the relevant literature review and semi-structured interview study, an initial questionnaire of sale teams' cohesion was designed. Through pre-investigation, we obtained a 26-subject formal questionnaire, we surveyed 150 salesmen from 100 sales teams of Tianjin or Hebei province by random sampling, and we obtained 16 factors that affect the cohesion of sales team. We used group AHP (Analytic Hierarchy Process) to analyze the Matrix. And the result shows that the different factors play different roles to affect the cohesion.",,CD-ROM:978-1-4244-4507-3,10.1109/CISE.2009.5363599,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5363599,,Appraisal;Companies;Educational institutions;Environmental management;Impedance;International collaboration;Marketing and sales;Multidimensional systems;Sampling methods;Technology management,decision making;sales management,analytic hierarchy process;confirmatory factor analysis;sale team cohesion structure,,0,,8,,no,11-13 Dec. 2009,,IEEE,IEEE Conference Publications
Throughput-Based MAC Layer Handoff in WLAN,S. Seo; J. Song; H. Wu; Y. Zhang,"Dept. of Comput. Sci., Yonsei Univ., Seoul",IEEE INFOCOM Workshops 2009,20090612,2009,,,1,2,"We propose a MAC layer handoff mechanism for IEEE 802.11 WLAN to give benefit to bandwidth-greedy applications at STAs. In order to find an optimal AP, we investigate a new measurement method called TFC (Transient Frame Capture) to estimate the achievable throughput from APs. Since TFC is employed under promiscuous mode, it avoids service degradation through the current associated AP. In addition, the mechanism is a client-only solution which does not require any modification on APs. We develop an analytic model for the throughput estimation and demonstrate it through experimental studies.",,POD:978-1-4244-3968-3,10.1109/INFCOMW.2009.5072197,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5072197,,Application software;Asia;Bandwidth;Computer science;Degradation;Frequency modulation;Telecommunication traffic;Throughput;Wireless LAN;Wireless networks,access protocols;wireless LAN,IEEE 802.11;WLAN;bandwidth-greedy applications;throughput-based MAC layer handoff;transient frame capture;wireless local area networks,,1,,4,,no,19-25 April 2009,,IEEE,IEEE Conference Publications
To Score or Not to Score? Tripling Insights for Participatory Design,M. Smuc; E. Mayr; T. Lammarsch; W. Aigner; S. Miksch; J. GÌ_rtner,Danube University Krems,IEEE Computer Graphics and Applications,20090505,2009,29,3,29,38,"For evaluating visual-analytics tools, many studies confine to scoring user insights into data. For participatory design of those tools, we propose a three-level methodology to make more out of users' insights. The relational insight organizer (RIO) helps to understand how insights emerge and build on one each other. In recent years, computers have also been used to develop visual methods and tools that further support the data analysis process. With the advent of the emerging field of visual analytics (VA), the underlying concept of visual tools is taken a step further. In essence, VA combines human analytical capabilities with computer processing capacities. In the human-computer interaction process, the user generates new knowledge and gains insights.",0272-1716;02721716,,10.1109/MCG.2009.53,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4909116,formative evaluation;insight;participatory design;usability engineering,Benchmark testing;Data analysis;Design methodology;Humans;Mathematical analysis;Process design;Software tools;Visual analytics;Visualization;Web pages,data analysis;data visualisation;human computer interaction,computer processing capacity;data analysis process;human analytical capability;human-computer interaction process;participatory design;relational insight organizer;visual-analytic tool,,8,,8,,no,May-June 2009,,IEEE,IEEE Journals & Magazines
Tradeoff and Sensitivity Analysis of a Hybrid Model for Ranking Commercial Off-the-Shelf Products,H. Ibrahim; B. H. Far; A. Eberlein,"Univ. of Calgary, Calgary, AB",2009 16th Annual IEEE International Conference and Workshop on the Engineering of Computer Based Systems,20090424,2009,,,119,127,"Despite its popularity, The COTS-based development still faces some challenges, in particular the evaluation and selection process in which uncertainty plays a major role. A hybrid model, composed of the analytic hierarchy process (AHP) and Bayesian belief network (BBN), is proposed to evaluate and rank various COTS candidates while explicitly considering uncertainty. Several input parameters such as weights assigned to evaluation criteria, relative scores for various COTS candidates, and prior belief about the satisfaction of various attributes associated with the evaluation criteria need to be estimated. The estimation process of these input parameters is subject to uncertainty that limits the applicability of the modelpsilas results. In this paper, we apply sensitivity analysis to check the validity and robustness of the model. Further, we apply tradeoff analysis to explore the impact of relaxing one criterion in order to achieve an increase in another criterion that is considered as more desirable in a particular project context. A digital library system is used as a case study to illustrate how the proposed tradeoff and sensitivity analysis was performed.",,POD:978-0-7695-3602-6,10.1109/ECBS.2009.47,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4839238,Analytic Hierarchy Process;Bayesian belief networks;Commercial Off-the-Shelf (COTS);Sensitivity analysis;Tradeoff analysis;Uncertainty,Bayesian methods;Computer science;Conferences;Drives;Performance analysis;Robustness;Sensitivity analysis;Software libraries;Software quality;Uncertainty,belief networks;decision making;sensitivity analysis;software engineering;software packages,Bayesian belief network;analytic hierarchy process;commercial off-the-shelf products;evaluation criteria;sensitivity analysis;tradeoff analysis,,2,,21,,no,14-16 April 2009,,IEEE,IEEE Conference Publications
Users Take a Close Look at Visual Analytics,G. Lawton,,Computer,20090210,2009,42,2,19,22,The need to more easily and effectively analyze the mountains of data that organizations are gathering and also to see their findings in ways that are simple to understand and work with has created a growing interest in visual analytics.,0018-9162;00189162,,10.1109/MC.2009.61,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4781964,IaaS;PaaS;SaaS;blades;cloud computing;data centers;grids;virtualization,Application software;Data analysis;Data engineering;Data visualization;Decision making;Displays;Information analysis;Terrorism;User interfaces;Visual analytics,data analysis,data analysis;data gathering;visual analytics,,2,,,,no,Feb. 2009,,IEEE,IEEE Journals & Magazines
Using the Multi-Attribute Global Inference of Quality (MAGIQ) Technique for Software Testing,J. D. McCaffrey,"Volt Inf. Sci., Inc., Bellevue, WA",2009 Sixth International Conference on Information Technology: New Generations,20090610,2009,,,738,742,"The Multi-Attribute Global Inference of Quality (MAGIQ) technique is a simple way to assign a single measure of overall quality to each of a set of similar software systems. Software testing activities can produce a wide range of useful information such as bug counts, performance metrics, and mean time to failure data. However, techniques to aggregate quality and testing metrics into a single quality meta-value are not widely known or used. The MAGIQ technique uses rank order centroids to convert system comparison attributes into normalized numeric weights, and then computes an overall measure of quality as a weighted (by comparison attributes) sum of system ratings. MAGIQ was originally developed to validate the results of analytic hierarchy process (AHP) analyses. Although MAGIQ has not been subjected to extensive research, the technique has proven highly useful in practice.",,POD:978-1-4244-3770-2,10.1109/ITNG.2009.81,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5070708,Decision support systems;management decision-making;software metrics;software quality;software testing,Aggregates;Decision making;Decision support systems;Information technology;Quality management;Software measurement;Software metrics;Software quality;Software systems;Software testing,program testing;software metrics;software quality,Multi-Attribute Global Inference of Quality;analytic hierarchy process analysis;software system;software testing;testing metrics,,0,,9,,no,27-29 April 2009,,IEEE,IEEE Conference Publications
Utility analysis for Internet-oriented server consolidation in VM-based data centers,Y. Song; Y. Zhang; Y. Sun; W. Shi,"Key Laboratory of Computer System and Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China",2009 IEEE International Conference on Cluster Computing and Workshops,20091016,2009,,,1,10,"Server consolidation based on virtualization technology will simplify system administration, reduce the cost of power and physical infrastructure, and improve utilization in today's Internet-service-oriented enterprise data centers. How much power and how many servers for the underlying physical infrastructure are saved via server consolidation in VM-based data centers is of great interest to administrators and designers of those data centers. Various workload consolidations differ in saving power and physical servers for the infrastructure. The impacts caused by virtualization to those concurrent services are fluctuating considerably which may have a great effect on server consolidation. This paper proposes a utility analytic model for Internet-oriented server consolidation in VM-based data centers, modelling the interaction between server arrival requests with several QoS requirements, and capability flowing amongst concurrent services, based on the queuing theory. According to features of those services' workloads, this model can provide the upper bound of consolidated physical servers needed to guarantee QoS with the same loss probability of requests as in dedicated servers. At the same time, it can also evaluate the server consolidation in terms of power and utility of physical servers. Finally, we verify the model via a case study comprised of one e-book database service and one e-commerce Web service, simulated respectively by TPC-W and SPECweb2005 benchmarks. Our experiments show that the model is simple but accurate enough. The VM-based server consolidation saves up to 50% physical infrastructure, up to 53% power, and improves 1.7 times in CPU resource utilization, without any degradation of concurrent services' performance, running on Rainbow - our virtual computing platform.",1552-5244;15525244,POD:978-1-4244-5011-4,10.1109/CLUSTR.2009.5289190,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5289190,model;server consolidation;utility analysis,Concurrent computing;Costs;Databases;Degradation;Queueing analysis;Resource management;Upper bound;Web and internet services;Web server;Web services,Web services;computer centres;software architecture;virtual machines,CPU resource utilization;Internet-oriented server consolidation;Internet-service-oriented enterprise data centers;QoS requirements;SPECweb2005 benchmarks;TPC-W benchmarks;VM-based data centers;e-book database service;e-commerce Web service;queuing theory;server arrival requests;server consolidation;utility analysis;utility analytic model;virtual computing platform;virtualization technology,,9,,43,,no,Aug. 31 2009-Sept. 4 2009,,IEEE,IEEE Conference Publications
Value Measurement Model for Technology-Based Knowledge Products,C. Li; H. Xu,"Sci. & Technol. Inf. Res. Inst., Shandong Univ. of Technol., Zibo, China",2009 International Conference on Computational Intelligence and Software Engineering,20091228,2009,,,1,4,"This paper summarized the major current value measurement methods for technology-based knowledge products (TKP), and then made an analysis to these methods in the value measuring application, and we further exploited the main difficulties and established the value measurement model for the TKP value measuring, and on this basis, we built the value measuring model for the TKP. First, we establish the value measurement index system, and then got the weight distribution by the analytic hierarchy process(AHP). As for the comprehensive weights calculation, the model designs to suggest the experts to give monetary value directly in all sub-indicators according their experience, which is also the data acquisition method. Finally, we used a case analysis to prove the scientificity and feasibility of the model.",,CD-ROM:978-1-4244-4507-3,10.1109/CISE.2009.5363627,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5363627,,Computer industry;Costs;Current measurement;Data acquisition;Information analysis;Knowledge management;Paper technology;Pricing;Software;Weight measurement,knowledge management,analytic hierarchy process;data acquisition;monetary value;technology-based knowledge product;value measurement index model,,0,,9,,no,11-13 Dec. 2009,,IEEE,IEEE Conference Publications
VAST contest dataset use in education,M. A. Whiting; C. North; A. Endert; J. Scholtz; J. Haack; C. Varley; J. Thomas,"Pacific Northwest National Laboratory, USA",2009 IEEE Symposium on Visual Analytics Science and Technology,20091113,2009,,,115,122,"The IEEE Visual Analytics Science and Technology (VAST) Symposium has held a contest each year since its inception in 2006. These events are designed to provide visual analytics researchers and developers with analytic challenges similar to those encountered by professional information analysts. The VAST contest has had an extended life outside of the symposium, however, as materials are being used in universities and other educational settings, either to help teachers of visual analytics-related classes or for student projects. We describe how we develop VAST contest datasets that results in products that can be used in different settings and review some specific examples of the adoption of the VAST contest materials in the classroom. The examples are drawn from graduate and undergraduate courses at Virginia Tech and from the Visual Analytics ldquoSummer Camprdquo run by the National Visualization and Analytics Center in 2008. We finish with a brief discussion on evaluation metrics for education.",,POD:978-1-4244-5283-5,10.1109/VAST.2009.5333245,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5333245,education;evaluation;synthetic data,Application software;Computer science;Computer science education;Data visualization;Educational institutions;Educational technology;Government;Information analysis;Laboratories;Visual analytics,data visualisation;educational technology;information analysis,IEEE visual analytics science and technology;VAST;education;evaluation metrics;information analysts,,2,,24,,no,12-13 Oct. 2009,,IEEE,IEEE Conference Publications
VIDI surveillance - embassy monitoring and oversight system,C. Jones; M. Ogawa; J. Shearer; A. Tikhonova; K. L. Ma,"VIDI Group, University of California, Davis, USA",2009 IEEE Symposium on Visual Analytics Science and Technology,20091113,2009,,,,,"We hypothesized that potential spies would try to use other employees' terminals in order to not draw attention to themselves. We define one type of suspicious activity as IP use on a terminal when the owner is inside the classified area. We created a timeline visualization of IP usage, overlaid with classified area entrances and exits. The vertical axis divides the timelines into 31 rows, one for each day of the month. The horizontal axis represents the time of day from early morning to late evening. A single employee's entire month is viewed all at once using this visualization. The employee being viewed can be changed using the arrow keys. Every IP event is represented by a vertical bar positioned at the exact time of its appearance. We color the IP events by port number, which is either intranet, HTTP, tomcat, or email, and size the bar based on the outgoing data size. Whenever an employee enters the classified area, a semi-transparent yellow region is drawn until that user exits the classified area. In rare cases when the user double enters, the region is twice as opaque, and in the other rare case where a user leaves the exits without entering, a red region is drawn until the next time the employee enters. The legend key and office diagram showing the current selected employee, highlighted in red, can be seen in the top left-hand corner.",,POD:978-1-4244-5283-5,10.1109/VAST.2009.5333950,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5333950,,Cameras;Computer security;Packaging;Robustness;Software packages;Surveillance;Visualization,IP networks;access control;security;video surveillance,IP event monitoring;IP usage timeline visualization;VIDI surveillance;classified area intrusion;embassy monitoring;oversight system;suspicious IP activity,,0,,3,,no,12-13 Oct. 2009,,IEEE,IEEE Conference Publications
Visual Knowledge Discovery in Dynamic Enterprise Text Repositories,V. Sabol; W. Kienreich; M. Muhr; W. Klieber; M. Granitzer,"Know-Center, Austria",2009 13th International Conference Information Visualisation,20090804,2009,,,361,368,"Knowledge discovery involves data driven processes where data is transformed and processed by various algorithms to identify new knowledge. KnowMiner is a service oriented framework providing a rich set of knowledge discovery functionalities with focus on text data sets. Complementing results of automatic machine analysis with the immense processing power of human visual apparatus has the potential of significantly improving the process of acquiring new knowledge. VisTools is a lightweight visual analytics framework based on multiple coordinated views (MCV) paradigm designed for deployment atop the KnowMinerpsilas service architecture. In this paper we briefly present both frameworks and, driven by real-world customer requirements, describe how visual techniques can be synergistically combined with machine processing for effective analysis of dynamically changing, metadata-rich text documents sets.",1550-6037;15506037,POD:978-0-7695-3733-7,10.1109/IV.2009.35,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5190787,,Data visualization;Documentation;Feedback;Humans;Information analysis;Pattern analysis;Refining;Topology;Visual analytics;Writing,document handling;learning (artificial intelligence);meta data;software architecture,KnowMiner service architecture;VisTools;automatic machine analysis;data driven processes;dynamic enterprise text repositories;human visual apparatus;lightweight visual analytics framework;machine processing;metadata-rich text documents sets;multiple coordinated views paradigm;service oriented framework;text data sets;visual knowledge discovery,,3,,19,,no,15-17 July 2009,,IEEE,IEEE Conference Publications
Visualizing cyber security: Usable workspaces,G. A. Fink; C. L. North; A. Endert; S. Rose,Pacific Northwest National Laboratory,2009 6th International Workshop on Visualization for Cyber Security,20100108,2009,,,45,56,"The goal of cyber security visualization is to help analysts increase the safety and soundness of our digital infrastructures by providing effective tools and workspaces. Visualization researchers must make visual tools more usable and compelling than the text-based tools that currently dominate cyber analysts' tool chests. A cyber analytics work environment should enable multiple, simultaneous investigations and information foraging, as well as provide a solution space for organizing data. We describe our study of cyber-security professionals and visualizations in a large, high-resolution display work environment and the analytic tasks this environment can support. We articulate a set of design principles for usable cyber analytic workspaces that our studies have brought to light. Finally, we present prototypes designed to meet our guidelines and a usability evaluation of the environment.",,POD:978-1-4244-5413-6,10.1109/VIZSEC.2009.5375542,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5375542,"Cyber analytics;H.5.2 [User Interfaces]: Interaction styles, Prototyping, Screen design, User-Centered Design;cognitive task analysis;cyber security;high-resolution displays;large;usability;visualization",Application software;Computer displays;Computer networks;Computer security;Data visualization;Information analysis;Prototypes;Switches;Usability;User centered design,data visualisation;security of data,cyber analytics work environment;cyber security visualization;digital infrastructures;information foraging;usability evaluation;usable workspaces,,10,,25,,no,11-11 Oct. 2009,,IEEE,IEEE Conference Publications
Web opinions analysis with scalable distance-based clustering,C. C. Yang; T. D. Ng,"College of Information Science and Technology, Drexel University, Philadelphia, USA",2009 IEEE International Conference on Intelligence and Security Informatics,20090626,2009,,,65,70,"Due to the advance of Web 2.0 technologies, a large volume of Web opinions are available in computer-mediated communication sites such as forums and blogs. Many of these Web opinions involve terrorism and crime related issues. For instances, some terrorist groups may use Web forums to propagandize their ideology, some may post threaten messages, and some criminals may recruit members or identify victims through Web social networks. Analyzing and clustering Web opinions are extremely challenging. Unlike regular documents, Web opinions usually appear as short and sparse text messages. Using typical document clustering techniques on Web opinions produce unsatisfying result. In this work, we propose the scalable distance-based clustering technique for Web opinions clustering. We have conducted experiments and benchmarked with the density-based algorithm. It shows that it obtains higher micro and macro accuracy. This Web opinions clustering technique is useful in identifying the themes of discussions in Web social networks and studying their development as well as the interactions of active participants.",,POD:978-1-4244-4172-3,10.1109/ISI.2009.5137273,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5137273,Web forum analysis;content analysis;document clustering;social media analytics;social networks,Blogs;Clustering algorithms;Computer mediated communication;Educational institutions;Information analysis;Information science;Laboratories;Social network services;Software libraries;Visualization,computer mediated communication;social networking (online);terrorism;text analysis,Web 2.0 technology;Web blog;Web forum;Web opinions clustering;Web social network;computer-mediated communication site;crime-related issues;density-based clustering algorithm;scalable distance-based document clustering technique;terrorism issues;text message,,4,,15,,no,8-11 June 2009,,IEEE,IEEE Conference Publications
Web semantics in action: Web 3.0 in e-Science,A. Carusi; T. Clark; M. S. Marshall,"OeRC, University of Oxford",2009 5th IEEE International Conference on E-Science Workshops,20100208,2009,,,192,193,"Semantic Web technologies have moved beyond the point of being promising futuristic technologies and demonstration projects, to being technologies in action in realistic contexts and conditions. Semantic Web applications are being developed for many aspects of scientific research, from experimental data management, discovery and retrieval, to analytic workflows, hypothesis development and testing, to research publishing and dissemination. This workshop intends to explore the questions that arise as Semantic Web applications are increasingly grounded within the actual lifecycle of scientific research, from observation and hypothesis formulation to publication, dissemination and criticism. We aim to bring together researchers across the disciplines, to discuss the use, development and embedding of these technologies in varied research domains and contexts. We will discuss the actuality of Semantic Web technologies in use and the emergent practices through which they are being developed and deployed. We aim to encourage vigorous discussion around aims, methods, applications and pragmatics. This workshop will look at the theoretical, methodological and pragmatic issues of grounding the development, deployment and evolution of ontologies and applications in Semantic e-Science in practical scientific problems and activity. How do we ground deductive Semantic Web information management and retrieval in the practical conditions of evolving sciences based on experiment, observation and induction? How do we bridge gaps and conflicts in approach between computer scientists developing research tools, and research practitioners using those tools? Is it possible to develop Semantic Web practices in e-Science that deal explicitly with hypothesis formulation, testing, challenge and refinement? Semantic Web applications have the potential to substantially accelerate research. Are all domains of research equally promising for the development of targeted semantic web applications? Are- ‰ÛÏsemantic‰Ûù domains defined by particular areas of research, by a particular form of scientific question, by discipline or sub-discipline or by particular aspects or stages of the scientific process? Are there types of enquiry which are intractable to the solutions offered by the Semantic Web, and if so, why? What are the specific challenges of Semantic Web applications in different disciplines, and how might Semantic Web applications shape and be shaped by them? The incorporation of semantic technologies with existing social web practices ‰ÛÓ ‰ÛÏWeb 3.0‰Ûù ‰ÛÓ promises to change the scientific research, publication and discussion model we now have to a much more fluid, ‰ÛÏhigher-velocity‰Ûù model. It also poses many questions for technologists and researchers alike. Can abstract and formal understandings of the underlying ontologies be supplemented or even replaced by more informal social conceptions of ontologies? What are the advantages and disadvantages of top-down or bottom-up development processes? What are best practices regarding user engagement and usability; what are the different roles of stakeholders in the process of development and deployment? What if any changes in scientific practice may be required to exploit the promise of semantic e-Science?",,Electronic:978-1-4244-5947-6; POD:978-1-4244-5946-9,10.1109/ESCIW.2009.5407976,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5407976,,Application software;Bridges;Grounding;Information management;Information retrieval;Ontologies;Publishing;Research and development management;Semantic Web;Testing,,,,2,,,,no,9-11 Dec. 2009,,IEEE,IEEE Conference Publications
3-D Quantification of the Aortic Arch Morphology in 3-D CTA Data for Endovascular Aortic Repair,S. Worz; H. v. Tengg-Kobligk; V. Henninger; F. Rengier; H. Schumacher; D. Bockler; H. U. Kauczor; K. Rohr,"Department of Bioinformatics and Functional Genomics, Biomedical Computer Vision Group, University of Heidelberg, BIOQUANT, IPMB, and DKFZ Heidelberg, Germany",IEEE Transactions on Biomedical Engineering,20100916,2010,57,10,2359,2368,"We introduce a new model-based approach for the segmentation and quantification of the aortic arch morphology in 3-D computed tomography angiography (CTA) data for thoracic endovascular aortic repair (TEVAR). The approach is based on a model-fitting scheme using a 3-D analytic intensity model for thick vessels in conjunction with a two-step refinement procedure, and allows us to accurately quantify the morphology of the aortic arch. Based on the fitting results, we additionally compute the (local) 3-D vessel curvature and torsion as well as the relevant lengths not only along the 3-D centerline, but particularly also along the inner and outer contour. These measurements are important for preoperative planning in TEVAR applications. We have validated our approach based on 3-D synthetic as well as 3-D MR phantom images. Moreover, we have successfully applied our approach using 3-D CTA datasets of the human thorax and have compared the results with ground truth obtained by a radiologist. We have also performed a quantitative comparison with a commercial vascular analysis software.",0018-9294;00189294,,10.1109/TBME.2010.2053539,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5491100,3-D analytic intensity model;3-D aortic arch segmentation;3-D computed tomography angiography (CTA) data;endovascular graft (EVG),,blood vessels;computerised tomography;diagnostic radiography;image segmentation;medical image processing;phantoms,3D CTA data;3D MR phantom images;3D analytic intensity model;3D centerline;3D quantification;3D vessel curvature;aortic arch morphology;computed tomography angiography;image segmentation;model-fitting scheme;preoperative planning;thick vessels;thoracic endovascular aortic repair;torsion;two-step refinement procedure;vascular analysis software,"Angiography;Aorta, Thoracic;Blood Vessel Prosthesis Implantation;Humans;Image Processing, Computer-Assisted;Imaging, Three-Dimensional;Phantoms, Imaging;Surgery, Computer-Assisted;Thorax;Tomography, X-Ray Computed",17,,37,,no,Oct. 2010,,IEEE,IEEE Journals & Magazines
A closer look at note taking in the co-located collaborative visual analytics process,N. Mahyar; A. Sarvghad; M. Tory,University of Victoria,2010 IEEE Symposium on Visual Analytics Science and Technology,20101210,2010,,,171,178,"This paper highlights the important role that record-keeping (i.e. taking notes and saving charts) plays in collaborative data analysis within the business domain. The discussion of record-keeping is based on observations from a user study in which co-located teams worked on collaborative visual analytics tasks using large interactive wall and tabletop displays. Part of our findings is a collaborative data analysis framework that encompasses note taking as one of the main activities. We observed that record-keeping was a critical activity within the analysis process. Based on our observations, we characterize notes according to their content, scope, and usage, and describe how they fit into a process of collaborative data analysis. We then discuss suggestions for the design of collaborative visual analytics tools.",,Electronic:978-1-4244-9487-3; POD:978-1-4244-9488-0,10.1109/VAST.2010.5652879,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5652879,collaboration;history;note taking;provenance;recording;tabletop;wall display,Business;Collaboration;Data visualization;Marketing and sales;Software;Visual analytics,business data processing;data analysis;data visualisation;interactive devices;recording,analysis process;co-located collaborative visual analytics process;collaborative data analysis;collaborative visual analytic tool;large interactive wall;note taking;record keeping;tabletop display,,4,,33,,no,25-26 Oct. 2010,,IEEE,IEEE Conference Publications
A computational selection strategy of dynamic knowledge alliance members on collaborative product design under the distributed knowledge resource environments,Zhang Zhenyu,"School of Automation and Electrical Engineering, Zhejiang University of Science and Technology, Hangzhou, China",2010 International Conference On Computer Design and Applications,20100809,2010,4,,V4-221,V4-225,"As products collaborative design has an essential effect on the creativity of complex product, products collaborative design will need many design resources in the distributed environments. How to quantify and use the approximate and even uncertain information to evaluate design resources on collaborative design under the distributed knowledge resource environments is a crucial issue. This paper focuses on the collaborative design system reconfiguration of design resources. A collaborative design unit model is proposed. The evaluation system of dynamic knowledge alliance members in collaborative product design is then built up. A computational selection strategy method called Multi-objective Weighted Fuzzy Analytic Hierarchy Process is then proposed to realize the computational selection of dynamic knowledge alliance members on collaborative product design under the distributed knowledge resource environments. This paper also proposes a model of collaborative design based on distributed knowledge resources. The framework of distributed knowledge resource is then put forward to support collaborative design. A computer-aided collaborative design software platform based on distributed knowledge resources is developed. The collaborative design of new devices for construction site personnel wireless location system is given as an example, which demonstrates that the methodology is obviously helpful to reuse and reorganize distributed knowledge for collaborative design and product innovation.",,Electronic:978-1-4244-7164-5; POD:978-1-4244-7162-1,10.1109/ICCDA.2010.5541210,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5541210,collaborative design;computational selection strategy;distributed knowledge resource;dynamic knowledge alliance members,Collaborative software;Collaborative work;Design automation;Distributed computing;International collaboration;Manufacturing;Personnel;Product design;Software design;Technological innovation,CAD;decision making;distributed processing;fuzzy set theory;groupware;product design;production engineering computing,collaborative design system reconfiguration;collaborative design unit model;collaborative product design;complex product;computational selection;computer-aided collaborative design software;construction site personnel wireless location system;design resources;distributed knowledge resource environment;dynamic knowledge alliance members;multiobjective weighted fuzzy analytic hierarchy process;product innovation,,0,,14,,no,25-27 June 2010,,IEEE,IEEE Conference Publications
A function point analysis model based-AHP,Chen Qiansheng,"Nanchang Military College, Department of Science & Culture, 330103, Jiangxi, China",The 2nd International Conference on Information Science and Engineering,20110117,2010,,,4249,4252,"Function point analysis is a method of software size measurement. It has become the mainstream software measurement methods in software engineering. Traditional function point analysis method makes adjustment to the results by generic system characteristics. But it exist limitations. In order to improve it, this article presents an approach based on AHP software function point analysis model.",2160-1283;21601283,DVD:978-1-4244-7617-6; Electronic:978-1-4244-7618-3; POD:978-1-4244-7616-9,10.1109/ICISE.2010.5691239,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5691239,Analytic Hierarchy Process;Function Points Analysis;Functional Size Measurement,Analytical models;Companies;ISO standards;Manuals;Size measurement;Software;Software measurement,,,,0,,8,,no,4-6 Dec. 2010,,IEEE,IEEE Conference Publications
A Fuzzy Synthetic Evaluation Method for Software Quality,X. Liu; J. Pang,"Dept. of Finance & Econ., Guangxi Univ. of Technol., Liuzhou, China",2010 2nd International Conference on E-business and Information System Security,20100527,2010,,,1,4,"The software quality evaluation problem is becoming increasingly important for software providers and their users. Existing most methods suitable for software quality evaluation are provided based on multiple-attribute decision-making (MADM) model. In this paper, an integration method of a fuzzy matter element (FME) and analytic hierarchy process (AHP) is used to consider quantitative and qualitative factors in evaluating the software quality. The fuzzy synthetic evaluation method is proposed to build the multiple software quality characteristics evaluation model based on the theory of fuzzy mathematics and matter element theory. Then, the AHP method is applied to calculate the weight of each evaluation index. Finally, a case study show that the new method is applicable for both theoretical and practical purposes.",2161-5942;21615942,Electronic:978-1-4244-5895-0; POD:978-1-4244-5893-6,10.1109/EBISS.2010.5473544,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5473544,,Computer networks;Educational institutions;Finance;Mathematical model;Mathematics;Mechanical engineering;Probes;Software engineering;Software quality;Uncertainty,decision making;fuzzy logic;software quality,analytic hierarchy process;fuzzy mathematics;fuzzy matter element;fuzzy synthetic evaluation method;multiple-attribute decision-making model;software quality,,0,,13,,no,22-23 May 2010,,IEEE,IEEE Conference Publications
A High-Performance Multi-user Service System for Financial Analytics Based on Web Service and GPU Computation,G. H. King; Z. Y. Cai; Y. Y. Lu; J. J. Wu; H. P. Shih; C. R. Chang,"Dept. Inf. Manage., Van-Nung Univ., Taoyuan, Taiwan",International Symposium on Parallel and Distributed Processing with Applications,20101111,2010,,,327,333,"In finance, securities, such as stocks, funds, warrants and bonds, are actively traded in financial markets. Abundance of market data and accurate pricing of a security can help the practitioners arbitrage or hedge their position. It can also help researhers and traders design better trading strategies. In this work, we develop a pricing and data/information service system for financial analytics with the following goals: (1) supporting fast pricing and data/information services for massive multiple users, (2) saving cost in hardware equipments and reducing energy consumption, and (3) easy maintenance and service expansion of the system. To achieve the first two goals, we use one traditional server paired with one Tesla C1060 and a set of PCs each paired with an Nvidia x275, and enhance GPU performance using a set of simple yet very effective optimization techniques. For the third goal, we develop our service system based on Web Service and the Service-Oriented-Architecture design principles. Our initial experiment results show that our GPU-based service system can deliver 4.8 tera flops computing speed, which achieves over 6000% performance increase compared to a cluster of eight Intel i7 quad-core servers. Cost-wise, the GPU-based service system costs 40% of the i7 server cluster, and consumes 50% of energy that is required by the i7 cluster. We also gives an overview of our system architecture and describe the workflow for processing pricing and information service requests.",2158-9178;21589178,Electronic:978-0-7695-4190-7; POD:978-1-4244-8095-1,10.1109/ISPA.2010.20,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5634351,GPGPU computation;Web services;parallel financial analytics;service-oriented architecture,Graphics processing unit;Instruction sets;Optimization;Pricing;Security;Servers;Web services,Web services;financial data processing;information services;software architecture;stock markets,GPU based service system;GPU computation;GPU performance enhancement;Nvidia x275;Tesla C1060;Web service;fast pricing support;financial analytics;financial market;high performance multiuser service system;information service system;market data;optimization techniques;service oriented architecture design principle,,2,,15,,no,6-9 Sept. 2010,,IEEE,IEEE Conference Publications
A qualitative and quantitative assessment method for software process model,X. Biyang; J. Min,"Software School, Hunan University, Changsha China",2010 IEEE International Conference on Software Engineering and Service Sciences,20100819,2010,,,286,291,"Aimed at the problems of high-cost, non-completeness and ambiguity existed in the traditional assessment methods for Software Process Model (SPM), this paper proposes a qualitative and quantitative assessment method. On the basis of assessment theory and domain experience of SPM, the unclear goals in the project start-up phase are qualitatively described in the form of problem set and expert problem domain is constructed as the assessment criteria on the implementation capability of SPM in the proposed method. With the integration of qualitative analysis and quantitative analysis by using a multi-index synthetic assessment algorithm of AHP (Analytic Hierarchy Process), the weight vector of goals and the reciprocal comparative matrix of problem domain are calculated and then the assessment result in dimensionless index is obtained. In order to reduce the cost of assessment, questionnaires instead of project tracking and audit are adopted to extract project goals in the method. Meanwhile, SPM is comprehensively assessed from four aspects including personnel, method, product and process, and the assessment result can intuitively reflect the capability of different SPM in a numerical form. Finally, an example of how to assess and choose four classical SPMs in the initial stage of a practical project is given out to show the validity of this assessment method.",2327-0586;23270586,Electronic:978-1-4244-6055-7; POD:978-1-4244-6054-0,10.1109/ICSESS.2010.5552434,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5552434,Analytic Hierarchy Process (AHP);Assessment of software process model;Four meta group for problem domain;GI assessment framework;Integration of quality and quantity;Nine-levels method;Reciprocal comparative matrix,Analytical models;Chromium;Indexes;Manganese;Object oriented modeling;Software;Standardization,software development management;software process improvement;software quality,analytic hierarchy process;audit;multiindex synthetic assessment algorithm;project goals;project tracking;reciprocal comparative matrix;software process model,,0,,14,,no,16-18 July 2010,,IEEE,IEEE Conference Publications
A Reliable Service Quality Evaluation Model in the Web Services Community,L. p. Chen; G. j. Zhang; W. t. Ha,"Dept. of Comput. Sci., Weinan Teachers Univ., Weinan, China",2010 International Conference on Computational Intelligence and Security,20110120,2010,,,457,460,"As the development of Web service, reliable service quality evaluation has become the key in the service optimization. This paper addresses this problem by proposing a reliable service quality evaluation model in the Web services community. The model initially imports Web service community aiming at evaluation unreliable when involved criterions are beyond service domain. The assistant domain Ontology besides the domain Ontology is defined, and criterion trees and criterion forest are built up to evaluate service reliably. Then in the model a new TFAHP which has fuzzy evaluation adjustment attribute is proposed, and approach of fuzzy comprehensive evaluation is improved. At the same time based on combination of them, a new evaluation algorithm to the model is created. Finally, an experiment is designed and performed to prove the feasibility and effectiveness of the model.",,Electronic:978-0-7695-4297-3; POD:978-1-4244-9114-8,10.1109/CIS.2010.106,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5696321,Web service community;extended Web service evaluation model;fuzzy comprehensive evaluation;new triangular fuzzy analytic hierarchy process,,Web services;decision making;fuzzy set theory;software quality;software reliability,TFAHP;Web services community;assistant domain ontology;fuzzy analytic hierarchy process;fuzzy comprehensive evaluation;fuzzy evaluation adjustment attribute;reliable service quality evaluation model,,1,,10,,no,11-14 Dec. 2010,,IEEE,IEEE Conference Publications
A research on the selection and evaluation of supplier for laptop,Z. Xiaolan; J. Jun,"Faculty of Management and Economy, Kunming University of Science and Technology, China 650093",2010 International Conference on Logistics Systems and Intelligent Management (ICLSIM),20100506,2010,2,,674,676,"According to the characteristics of laptop, a system of supplier selection's indicators could be drawn up on the basis of references and experts queries. EXPERT CHOICE is adopted in calculating data which is from AHP. Using EXPORT CHOICE tends to be easy and correct. The paper also discusses the application of AHP at the evaluation and selection of suppliers for laptop.",,Electronic:978-1-4244-7330-4; POD:978-1-4244-7331-1,10.1109/ICLSIM.2010.5461333,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5461333,AHP;Laptop;Suppliers,Continuous improvement;Costs;Customer satisfaction;Delay;Marketing and sales;Performance loss;Portable computers;Product development;Production;Technology management,computer selection;decision making;decision support systems;laptop computers,EXPERT CHOICE software;analytic hierarchy process;decision-support tool;expert queries;laptop supplier;supplier selection indicators,,0,,4,,no,9-10 Jan. 2010,,IEEE,IEEE Conference Publications
A Study of Factors Influencing Customers' Online Shopping Behavior Based on ANP,P. Sun; C. Wang,"Dept. of Manage., Tianjin Univ. of Technol., Tianjin, China",2010 International Conference on Management and Service Science,20100916,2010,,,1,3,This research explores which factors affect the online shopping behavior of Internet customer. This paper adopts the Analytic Network Process (ANP) as the main analytical tool. ANP was an effective method for considering the feed back and the relations between factors. The research results point out that the security of network transactions was the most important factors for customer's online shopping.,,Electronic:978-1-4244-5326-9; POD:978-1-4244-5325-2,10.1109/ICMSS.2010.5578473,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5578473,,Decision making;Indexes;Internet;Security;Software;Stochastic processes,Internet;retail data processing;statistical analysis,ANP;analytic network process;customer online shopping behavior;network transaction security,,0,,11,,no,24-26 Aug. 2010,,IEEE,IEEE Conference Publications
A Study on Flexibility of ERP System Based on Grey Evaluation Model,S. Wang; X. Liu,"Coll. of Econ. & Manage., Shandong Univ. of Sci. & Technol., Qingdao, China",2010 2nd International Workshop on Database Technology and Applications,20101206,2010,,,1,4,"The competitive environment and changing business processes propose new requirements to ERP system's flexibility and its evaluation becomes the first and critical step to solve these problems. This paper begins with the analysis of flexibility's definition.Basing on the core idea of three-tier software architecture, the author refines the index system from the aspects of presentation flexibility, business flexibility and design flexibility. Then, treating the grey theory as a guide, the evaluation system is established by integrating AHP and Grey Evaluation Model. Finally, the corresponding example is given to illustrate the model.",2167-1923;21671923,Electronic:978-1-4244-6977-2; POD:978-1-4244-6975-8,10.1109/DBTA.2010.5659109,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5659109,,Biological system modeling;Business;Indexes;Object oriented modeling;Software;Software architecture;Stability analysis,business data processing;decision making;enterprise resource planning;grey systems;software architecture,AHP;ERP system;analytic hierarchy process;business flexibility;business process;design flexibility;grey evaluation model;grey theory;index system;presentation flexibility;three tier software architecture,,0,,7,,no,27-28 Nov. 2010,,IEEE,IEEE Conference Publications
A visualization analysis tool for DNS amplification attack,H. Yu; X. Dai; T. Baxliey; X. Yuan; T. Bassett,"Department of Computer Science, North Carolina A&T State University, Greensboro, USA",2010 3rd International Conference on Biomedical Engineering and Informatics,20101118,2010,7,,2834,2838,"This paper presents a visualization analysis tool for detecting, analyzing and responding to the Distributed Denial of Service attack termed the Domain Name Service (DNS) amplification attack. The tool integrates agent technology, visual analytics and interactive visualization techniques to allow users to interact with the system in real-time, to monitor the network traffic, to analyze traffic information, to detect abnormal behaviors, and to respond to the DNS amplification attack. Three algorithms that are the filter algorithm, the mapping and visualizing algorithm, and the response algorithm have been developed to detect and respond to the DNS amplification attack automatically or manually. A set of experiments have been conducted in an isolated laboratory and produced expected results.",1948-2914;19482914,Electronic:978-1-4244-6498-2; POD:978-1-4244-6495-1,10.1109/BMEI.2010.5639324,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5639324,Network security;visualization,Computational modeling;Computer crime;Computers;Data visualization;IP networks;Monitoring;Servers,Internet;data visualisation;security of data;software agents;telecommunication traffic,agent technology;distributed denial of service attack;domain name service amplification attack;filter algorithm;interactive visualization technique;mapping algorithm;network traffic monitoring;response algorithm;visual analytics;visualization analysis tool;visualizing algorithm,,3,,7,,no,16-18 Oct. 2010,,IEEE,IEEE Conference Publications
AC Loss Analysis of HTS Power Cable With RABiTS Coated Conductor,S. Kim; K. Sim; J. Cho; H. M. Jang; M. Park,"Korea Electrotechnol. Res. Inst., Changwon, South Korea",IEEE Transactions on Applied Superconductivity,20100524,2010,20,3,2130,2133,"Numerical analysis of AC loss for a HTS power cable is investigated using commercial FEM software package. AC loss of the HTS power cable, which is made by 2 G conductor, is hard to experimentally measure due to very small signal compared to that made by 1G conductor. The FEM model describes current distribution and AC loss inside the HTS conductor for the AC transport current through nonlinear E-J correlation. For the verification of the AC loss analysis model, the results were compared with the well known analytic solution of a single strip HTS conductor and experiments. Unlike IBAD substrate, magnetization of the RABiTS has influence on the precise estimation of the AC loss and it is also considered in the FEM model. Moreover, several conductors should be stacked to meet the large transport current because of the small critical current at present and the effect of stacking configuration is also investigated. In this paper, AC loss analysis results are presented for various HTS power cable configurations such as stacking directions. The results are compared with the experimental results of a model HTS power cable and the best configuration to minimize AC loss is suggested.",1051-8223;10518223,,10.1109/TASC.2010.2041439,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5439739,AC loss;coated conductor;magnetization,,finite element analysis;power cables,2G conductor;AC loss analysis;E-J correlation;FEM software package;HTS power cable;RABiTS coated conductor;current distribution,,10,,9,,no,10-Jun,,IEEE,IEEE Journals & Magazines
Agent-based modeling of mems fluidic self-assembly,M. Mastrangeli; C. Van Hoof; R. Baskaran; J. P. Celis; K. F. BÌ_hringer,"IMEC, Leuven (BE)",2010 IEEE 23rd International Conference on Micro Electro Mechanical Systems (MEMS),20100408,2010,,,476,479,"The dynamics of MEMS 3D fluidic self-assembly (FSA) was modeled using interactive software agents, i.e. by agent-based modeling (ABM). ABM enables realistic simulations of 3D FSA dynamics taking into account spatial parameters - hard to include in analytic models. Our ABM model was tested by reproducing the experimental data of Zheng and Jacobs's 3D FSA process, and it was used to investigate the influence of design parameters and assembly strategies on FSA yield. The ABM model is a significant advance in the modeling of FSA and may represent the natural framework to explore open issues in this promising field.",1084-6999;10846999,Electronic:978-1-4244-5764-9; POD:978-1-4244-5761-8; USB:978-1-4244-5763-2,10.1109/MEMSYS.2010.5442463,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5442463,,Assembly;Fluid dynamics;Jacobian matrices;Micromechanical devices;Microscopy;Packaging;Physics;Self-assembly;Software agents;Stochastic processes,electronic engineering computing;microfluidics;multi-agent systems;self-assembly,3D FSA dynamics;ABM model;MEMS 3D fluidic self-assembly;MEMS fluidic self-assembly;agent based modeling;agent-based modeling;interactive software agents;realistic simulation;spatial parameters,,2,,11,,no,24-28 Jan. 2010,,IEEE,IEEE Conference Publications
An Analytic Framework for Detailed Resource Profiling in Large and Parallel Programs and Its Application for Memory Use,U. Finkler,"IBM T.J. Watson Research Center, Yorktown Heights",IEEE Transactions on Computers,20100126,2010,59,3,358,370,"Profiling is an essential and widely used technique to understand the resource use of applications. For example, the memory use of large applications is becoming an important cost factor. Very large systems are typically sized to accommodate designated tasks, and thus, the price, as well as cache and TLB efficiency, depends significantly on the memory footprint of the target applications. Importantly, the increasing use of multicore systems magnifies the problem since memory use grows with the number of parallel tasks. Additionally, the presence of multiple tasks or threads makes the problem of correlating resource use to the program structure harder. Thus, tools that correlate resource use with program structure with quantitative error margins are essential for optimizing the resource use of complex software applications. While efficient tools for the profiling of execution time are available, the choices for detailed profiling of memory use or other hardware resources are very limited. We were unable to find tools that provided sufficiently accurate insight into, e.g., memory use without adding unacceptable overhead in memory use and execution time for the performance analysis of very large applications. In this paper, we present a highly efficient probabilistic method for profiling that provides detailed resource usage information R<sub>?</sub>(t) indexed by the full location descriptor ? (e.g., process id, thread id, and call chain) and time t. Importantly, we provide an analytical framework, which provides error estimates and allows to analyze and quantitatively optimize a wide variety of profiling scenarios. We employed the probabilistic approach to implement a memory profiling tool that adds minimal overhead and does not require recompilation or relinking. The tool provides the memory use M<sub>?</sub> (t) for all location descriptors ? over the execution time for single and multithreaded programs. Experimental results confirm that execution time and memory o- - verhead are less than 10 percent of the unprofiled, optimized execution. Importantly, the technique is sufficiently general to be applicable to profiling of other hardware resources as cache or TLB misses over time for all location descriptors with similarly low overhead and across multiple processes, threads, and processors.",0018-9340;00189340,,10.1109/TC.2009.149,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5276794,Resource usage;call chain;memory usage;numerical.;probabilistic;profiling,Application software;Computer errors;Costs;Hardware;Multicore processing;Performance analysis;Software tools;Yarn,cache storage;multi-threading;program diagnostics;resource allocation,TLB efficiency;analytic framework;cache efficiency;complex software applications;cost factor;detailed resource profiling;full location descriptor;memory footprint;memory use;multithreaded programs;parallel programs;performance analysis;probabilistic method;quantitative error margins;resource usage information,,1,,27,,no,10-Mar,,IEEE,IEEE Journals & Magazines
An Analytic Model for Fault Diagnosis in Power Systems Considering Malfunctions of Protective Relays and Circuit Breakers,W. Guo; F. Wen; G. Ledwich; Z. Liao; X. He; J. Liang,"School of Electrical Engineering, South China University of Technology, Guangzhou, China",IEEE Transactions on Power Delivery,20100621,2010,25,3,1393,1401,"When a fault occurs on a section or a component in a given power system, if one or more protective relays (PRs) and/or circuit breakers (CBs) associated do not work properly, or in other words, a malfunction or malfunctions happen with these PRs and/or CBs, the outage area could be extended. As a result, the complexity of the fault diagnosis could be greatly increased. The existing analytic models for power system fault diagnosis do not systematically address the possible malfunctions of PRs and/or CBs, and hence may lead to incorrect diagnosis results if such malfunctions do occur. Given this background, based on the existing analytic models, an effort is made to develop a new analytic model to well take into account of the possible malfunctions of PRs and/or CBs, and further to improve the accuracy of fault diagnosis results. The developed model does not only estimate the faulted section(s), but also identify the malfunctioned PRs and/or CBs as well as the missing and/or false alarms. A software system is developed for practical applications, and realistic fault scenarios from an actual power system are served for demonstrating the correctness of the presented model and the efficiency of the developed software system.",0885-8977;08858977,,10.1109/TPWRD.2010.2048344,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5471043,Alarm message;analytic model;fault diagnosis;malfunctions of protective relays and circuit breakers;power system,,circuit breakers;fault diagnosis;power system faults;relay protection,analytic model;circuit breaker mafunctions;power system fault diagnosis;protective relay malfunctions;software system,,27,,25,,no,10-Jul,,IEEE,IEEE Journals & Magazines
An Analytic Model for Impedance Calculation of an RFID Metal Tag,S. K. Kuo; L. G. Liao,"Iron and Steel Research and Development Department, China Steel Corporation, Kaohsiung, Taiwan",IEEE Antennas and Wireless Propagation Letters,20100712,2010,9,,603,607,"Since low-profile metal tags are infamous for their narrow bandwidth, obtaining fast and accurate impedance computation is therefore an important issue in the antenna design stage. This letter presents a method for computing antenna impedance of a patch-type radio frequency identification (RFID) tag. In order to perform impedance calculation in a very short time, an analytic model is proposed, and the result is highly agreeable with that obtained by the commercial software. Two important parameters are also identified for the purpose of independent impedance tuning of the real and imaginary parts. A prototype was designed and manufactured based on the proposed method, showing a dimension of 118 ÌÑ 43 ÌÑ 1.5 mm<sup>3</sup> and a reading range of 6 m.",1536-1225;15361225,,10.1109/LAWP.2010.2053511,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5491048,Antenna;RFID metal tag;coupled microstrip lines;radio frequency identification (RFID),,microstrip antennas;radiofrequency identification,RFID metal tag;antenna design stage;commercial software;impedance calculation;impedance tuning;patch-type radio frequency identification tag,,7,,10,,no,2010,,IEEE,IEEE Journals & Magazines
An Analytic Model of Atomic Service for Services Descriptions,D. Chen; S. Han; M. Munro; A. Soomro; W. Song,"Dept. of Comput. Sci. & Technol., Zhejiang Univ., Hangzhou, China",2010 International Conference on Service Sciences,20100624,2010,,,197,202,"The key question with Service Science is to be how to formulate a theoretic foundation for service description, service discovery and service composition in the context of basic service system development and the building of service architecture. However, the current research and development in the field of Service Science assumes that the mapping between the service requesters' requirements and the formal and executable services provided by the service providers (or developers) is straightforward, without a clear view of unbalance or even semantic conflict between a requirement very likely in NL and a service as a programming module. In this paper, we attempt to address this issue by proposing an analytic model of atomic service for service composition based on our initial analysis of service structures and manipulations.",2165-3828;21653828,Electronic:978-0-7695-4017-7; POD:978-1-4244-6603-0,10.1109/ICSS.2010.62,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5494281,atomic service;service composition;service scheduling;web services,Buildings;Computer architecture;Computer science;Context modeling;Context-aware services;Humans;Natural languages;Processor scheduling;Research and development;Web services,Internet;Web services;software architecture,atomic service analytic model;service architecture;service composition;service description;service discovery;services descriptions,,1,,20,,no,13-14 May 2010,,IEEE,IEEE Conference Publications
An Analytic Network Process Model for Engineering Device Procurement Evaluation,R. Chen,"Sch. of Manage. Sci. & Eng., Anhui Univ. of Technol., Maanshan, China",2010 International Conference on Management and Service Science,20100916,2010,,,1,4,"The ANP method is used in this paper to evaluate engineering device procurement. The evaluation criteria are given, their control hierarchy is built and weights are calculated. The evaluation can be carried out in four perspectives: the requirements, the finance, the supplier and the procurement procedure. The weights can be used in a semi-automatic evaluation process in which a group of dropdown lists is to be selected and the final evaluation score is the addition of the weighted score. The weight calculation is carried out in Excel and the evaluation can be carried out manually or in software.",,Electronic:978-1-4244-5326-9; POD:978-1-4244-5325-2,10.1109/ICMSS.2010.5575835,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5575835,,Analytical models;Book reviews;Contracts;Finance;ISO;Procurement;Software,engineering;procurement,Excel;analytic network process model;control hierarchy;dropdown lists;engineering device procurement evaluation;semiautomatic evaluation process;weight calculation,,0,,7,,no,24-26 Aug. 2010,,IEEE,IEEE Conference Publications
An Application of Six Sigma and Simulation in Software Testing Risk Assessment,V. Bubevski,"Legal & Gen. Account, TATA Consultancy Services Ltd., London, UK","2010 Third International Conference on Software Testing, Verification and Validation",20100603,2010,,,295,302,"The conventional approach to Risk Assessment in Software Testing is based on analytic models and statistical analysis. The analytic models are static, so they don't account for the inherent variability and uncertainty of the testing process, which is an apparent deficiency. This paper presents an application of Six Sigma and Simulation in Software Testing. DMAIC and simulation are applied to a testing process to assess and mitigate the risk to deliver the product on time, achieving the quality goals. DMAIC is used to improve the process and achieve required (higher) capability. Simulation is used to predict the quality (reliability) and considers the uncertainty and variability, which, in comparison with the analytic models, more accurately models the testing process. Presented experiments are applied on a real project using published data. The results are satisfactorily verified. This enhanced approach is compliant with CMMI<sup>å¨</sup> and provides for substantial Software Testing performance-driven improvements.",2159-4848;21594848,Electronic:978-1-4244-6436-4; POD:978-1-4244-6435-7,10.1109/ICST.2010.23,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477075,DMAIC;Risk Assessment;Simulation;Six Sigma;Software Testing,Analytical models;Application software;Performance evaluation;Predictive models;Risk analysis;Risk management;Six sigma;Software testing;Statistical analysis;Uncertainty,Capability Maturity Model;digital simulation;program testing;risk management;six sigma (quality);software quality;statistical analysis,CMMI;analytic models;six sigma;software quality;software testing performance-driven improvements;software testing risk assessment simulation;statistical analysis;testing process uncertainty,,0,2,18,,no,6-10 April 2010,,IEEE,IEEE Conference Publications
An Architecture for a Task-Oriented Surveillance System: A Service- and Event-Based Approach,J. MoÌÙgraber; F. Reinert; H. Vagts,"Fraunhofer Inst. of Optronics, Syst. Technol. & Image Exploitation IOSB, Karlsruhe, Germany",2010 Fifth International Conference on Systems,20100513,2010,,,146,151,"Due to the increasing threat posed by crime, industrial espionage and even terrorism, video surveillance systems have become more important and powerful during the last years. While most commercially available surveillance systems have to be managed by human operators, who constantly monitor all video streams, several experimental systems from different research groups already include robust video processing approaches for (semi-) automated surveillance. Still, most research activities focus on a sensor-oriented approach to video analytics of large and distributed camera networks, aiming to extract, analyze and store all extractable information from the video streams. In real-life applications, however, only a limited set of specific threats needs to be covered. Accordingly, only a small subset of potentially extractable information has to be monitored. Besides the huge amount of raw video data, modern surveillance systems are also extended with other sensors that deliver even more data. As a consequence, a new paradigm is introduced, called task-oriented information and data processing for surveillance systems. In the proposed system NEST (Network Enabled Surveillance and Tracking) following the task-oriented approach, every resource allocation, data acquisition, and analysis process is assigned to a specific surveillance task. In order to meet the requirements of task-oriented surveillance, the proposed architecture combines a Service-Oriented Architecture with an Event-Driven Architecture (Event-driven SOA).",,Electronic:978-1-4244-6232-2; POD:978-1-4244-6231-5,10.1109/ICONS.2010.32,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5464144,communication buses;event-driven SOA;multi-camera tracking;surveillance;task-oriented,Computerized monitoring;Data mining;Humans;Information analysis;Power system management;Robustness;Service oriented architecture;Streaming media;Terrorism;Video surveillance,data acquisition;resource allocation;software architecture;task analysis;video signal processing;video streaming;video surveillance,NEST;analysis process;data acquisition;data processing;distributed camera networks;event-based approach;event-driven architecture;extractable information;human operators;industrial espionage;network enabled surveillance and tracking;resource allocation;robust video processing approaches;semiautomated surveillance;sensor-oriented approach;service-based approach;service-oriented architecture;task-oriented information;task-oriented surveillance system;terrorism;video analytics;video streams monitoring;video surveillance systems,,4,,10,,no,11-16 April 2010,,IEEE,IEEE Conference Publications
An Effect Evaluation Model for Vulnerability Testing of Web Application,D. Jing-Nong; L. Yan-Sheng,"Coll. of Comput. Sci. & Technol., HuaZhong Univ. of Sci. & Technol., WuHan, China","2010 Second International Conference on Networks Security, Wireless Communications and Trusted Computing",20100607,2010,1,,382,385,"In this paper, we propose a security evaluation model for the web application and define a security evaluation function based on the Analytic Hierarchy Process (AHP) to describe the model. We use the evaluation method proposed by this paper to evaluate the vulnerability test effect of a BBS application named IPB. The experiment result reveals that the evaluation value calculated by the security evaluation function is positively correlated with the number of vulnerabilities found in the security test. It proves that the security evaluation method proposed by this paper is practical and reliable.",,Electronic:978-1-4244-6598-9; POD:978-1-4244-6597-2,10.1109/NSWCTC.2010.94,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5480968,Analytic Hierarchy Process;security evaluation;vulnerability testing;web application,Application software;Buffer overflow;Computer networks;Computer science;Computer security;Educational institutions;Electronic mail;File systems;Testing;Wireless communication,Web services;decision making;security of data;testing,BBS application;IPB;Web application;analytic hierarchy process;invision power board;security evaluation function;vulnerability testing,,0,,7,,no,24-25 April 2010,,IEEE,IEEE Conference Publications
An improved analytic hierarchy process model on Software Quality Evaluation,Miao Fan; Yi Luo; Guoshi Wu; Xiangling Fu,"School of Software Engineering, Beijing University of Posts and Telecommunications, China, 100876",The 2nd International Conference on Information Science and Engineering,20110117,2010,,,1838,1842,"With new global demands for better quality of software products, effective and efficient SQE (Software Quality Evaluation) becomes necessary and indispensible. Existing methods of SQE are as it is, however, subjective, non-quantitative and uncompleted to some extent. In this paper, we introduce a novel method, an improved AHPM (Analytic Hierarchy Process Model) incorporated traditional AHPM and some quality characteristics from ISO/IEC 9126, which attempts to propose a better method of software quality evaluation that is relatively objective, quantitative and completed. On this basis, current software can be well assessed as well as future products can be well predicted.",2160-1283;21601283,DVD:978-1-4244-7617-6; Electronic:978-1-4244-7618-3; POD:978-1-4244-7616-9,10.1109/ICISE.2010.5690372,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5690372,ISO/IEC 9126;improved analytic hierarchy model;software quality evaluation;software quality prediction;weight,Analytical models;IEC standards;ISO standards;Software engineering;Software quality;Testing,,,,1,,11,,no,4-6 Dec. 2010,,IEEE,IEEE Conference Publications
"An Insuanrance Model for Guranteeing Service Assurance, Integrity and QoS in Cloud Computing",M. Luo; L. J. Zhang; F. Lei,,2010 IEEE International Conference on Web Services,20100823,2010,,,584,591,"SOA and cloud computing have brought new opportunities for the long expected agility, reuse and the adaptive capability of IT to the ever changing business requirements and environments. But due to the immature nature of the rapidly evolving technologies, especially in the areas of security, service or information integrity, privacy, quality of service and their possible detrimental consequences, many enterprises have been hesitating to make the shift. This paper adopts the concept of insurance and establishes a framework and the supporting reference model for cloud computing. We utilize the value-at-risk (VAR) approach to establish several appropriate mechanisms, and use a set of measurable metrics. Those quantitative or qualitative metrics can be applied as the basis for the business value and risk assessment, and eventually for insurance premium and compensation calculation for the failures of the services offered in Cloud environment. This model can also establish a potential new innovative market branch for the insurance industry.",,Electronic:978-0-7695-4128-0; POD:978-1-4244-8146-0,10.1109/ICWS.2010.113,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5552738,Analytics;Cloud Computing;Integrity;Privacy;Quality of Service (QoS);Security;Service Assessment and Valuation;Service Insurance Model;Service Risks,Biological system modeling;Cloud computing;Clouds;Insurance;Risk management;Security,Web services;insurance;quality of service;software architecture,cloud computing;quality of service;service assurance guranteeing;service insurance model;value-at-risk approach,,6,,14,,no,5-10 July 2010,,IEEE,IEEE Conference Publications
Analysis of Smartphone User Behavior,H. Verkasalo,,2010 Ninth International Conference on Mobile Business and 2010 Ninth Global Mobility Roundtable (ICMB-GMR),20100624,2010,,,258,263,"Holistic, objective and precise data on mobile user behavior and experience are needed in today's product development and marketing activities. This article presents a framework for mobile audience measurements, for collecting data at the point of convergence - devices. The paper compares the presented framework to alternative methods of mobile user research, and identifies the unique advantages of on-device measurements along with the key weaknesses. In addition to elaborating on data collection, the paper addresses the related analytics, presenting adoption modeling and stickiness analysis that complement the data collection processes and deliver practical insights. The insights can be provided to device vendors, application developers and carriers, who can use the insights in product portfolio management, product development, and marketing.",,Electronic:978-1-4244-7424-0; POD:978-1-4244-7423-3,10.1109/ICMB-GMR.2010.74,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5494859,mobile audience measurements;mobile usage;smartphones,Aerospace industry;Application software;Convergence;Data analysis;Data mining;Internet;Market research;Product development;TV;Technological innovation,market research;mobile computing;product development;social aspects of automation,mobile consumption;product development activities;smartphone user behavior;stickiness analysis;strategic market research,,9,1,19,,no,13-15 June 2010,,IEEE,IEEE Conference Publications
Analysis of substation availability,C. K. C. Arruda; H. P. Amorim; L. A. M. C. Domingues; R. C. Fonte; P. A. Lisboa; S. L. Zaghetto; L. F. A. Nascimento; L. F. Queiroz,"Cepel, Rio de Janeiro, Brazil",2010 IEEE/PES Transmission and Distribution Conference and Exposition: Latin America (T&D-LA),20110505,2010,,,422,426,"The paper discusses the modeling of substations for reliability, dealing with each equipment and its influence in the topology. The study consider the power availability for each load, using either an analytic and a simulation model. Some results and a software implementation are presented.",,Electronic:978-1-4577-0488-8; POD:978-1-4799-1682-5,10.1109/TDC-LA.2010.5762916,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5762916,availability;reliability;substation,Biological system modeling;Circuit breakers;Load modeling;Maintenance engineering;Power system reliability;Reliability;Substations,power system reliability;substations,power availability;substation availability;substations reliability,,0,,8,,no,8-10 Nov. 2010,,IEEE,IEEE Conference Publications
Application of AHP to Evaluation on Failure Causes Analysis for Lithography Machine,P. S. Ko; C. C. Wu; H. H. Chen; C. W. Yang,"Dept. of Public Finance & Taxation, Nat. Kaohsiung Univeisity of Appl. Sci., Kaohsiung, Taiwan",2010 Second International Conference on Computer and Network Technology,20100601,2010,,,571,573,"This study conducted hierarchical analysis on the evaluation item of the stability index of the lithography machine, and established a set of evaluation mechanism for failure prediction, in order to provide references and indicators of troubleshooting for lithography machine. The results showed, when the lithography machine is out of order, the possible failure causes are mainly be found based on the past experiences. This study also found that, under the good configuration of maintenance system, adequate information is closely associated a good system. As for lithography process in semiconductor industry, the complexity of broken Wafer is first considered. Thus, the overall lithography process of semiconductor relies on engineers' experience. More specifically, a quick error interpretation and repair are required in field maintenance. As in a competitive market of semiconductor processing with high-tech and high-cost, a timely maintenance in the lithography machine is urgent and requested.",,Electronic:978-1-4244-6962-8; POD:978-1-4244-6961-1,10.1109/ICCNT.2010.111,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5474433,Fuzzy Analytic Hierarchy Process;failure analysis;lithography machine,Application software;Computer networks;Condition monitoring;Diagnostic expert systems;Etching;Failure analysis;Fault diagnosis;Lithography;Production;Public finance,failure analysis;lithography;maintenance engineering;production equipment;semiconductor industry;statistical analysis,AHP;failure causes analysis;hierarchical analysis;lithography machine;lithography process;maintenance system;repair;semiconductor industry;wafer complexity,,0,,8,,no,23-25 April 2010,,IEEE,IEEE Conference Publications
Application of Non-superconducting Fault Current Limiter to improve transient stability,M. T. Hagh; S. B. Naderi; M. Jafari,"Mechatronic Center of Excellence, University of Tabriz, Tabriz, Iran",2010 IEEE International Conference on Power and Energy,20110120,2010,,,646,650,"In this paper, enhancement of transient stability of Single Machine Infinite Bus (SMIB) system with a double circuit transmission line using a Non-superconducting Fault Current Limiter (NSFCL) is proposed. Stability analysis for such system is discussed in detail. It is shown that, the stability depends on the resistance of NSFCL in fault condition. To effective improvement of stability, the optimum value of NSFCL resistance is calculated. Simulation results by PSCAD/EMTDC software are presented to confirm the analytic analysis accuracy.",,Electronic:978-1-4244-8946-6; POD:978-1-4244-8947-3,10.1109/PECON.2010.5697660,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5697660,fault current limiter;non-superconducting coil;optimum resistor;transient stability,,fault current limiters;power system transient stability,NSFCL resistance;PSCAD-EMTDC software;fault condition;nonsuperconducting fault current limiter;single machine infinite bus system;transient stability improvement,,2,,12,,no,Nov. 29 2010-Dec. 1 2010,,IEEE,IEEE Conference Publications
Applying excel in the weight calculation of software maintainability evaluation based on AHP,J. p. Zhang; X. Zhu; G. f. Bu; X. Liang; N. Yu,"Department of Equipment Command and Management Ordnance Engineering College, OEC, Shijiazhuang, China","2010 International Conference on Computer, Mechatronics, Control and Electronic Engineering",20101025,2010,1,,10,13,"In order to assure the quality of software maintainability evaluation, setting up a reasonable index system and distribute the index weights with reason is the most important thing. In this situation, this paper set up an index system of software maintainability evaluation firstly, based on two principles such as giving prominence to main indexes and that indexes are usually required be measurable. Then, it did calculation upon the index weights with the application of analytic hierarchy process (AHP). Finally, the implementation of AHP in Excel validates fully that AHP is an available and efficient solution to calculate the index weights during software maintainability evaluation. In conclusion, using Excel to calculate the weights of software maintainability evaluation based on AHP is original.",2159-6026;21596026,Electronic:978-1-4244-7958-0; POD:978-1-4244-7957-3,10.1109/CMCE.2010.5609649,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5609649,analytic hierarchy process;index system;software maintainability evaluation,Bismuth;Software,software maintenance;software quality,AHP;Excel;analytic hierarchy process;index system;quality of software maintainability;weight calculation,,0,,7,,no,24-26 Aug. 2010,,IEEE,IEEE Conference Publications
Applying fuzzy AHP to select an automatic container number identification system in port terminals,S. L. Chao; Y. L. Lin,"Department of Shipping and Transportation Management, National Taiwan Ocean University, Keelung, Taiwan",2010 8th International Conference on Supply Chain Management and Information,20110106,2010,,,1,7,"Container traffic passing through terminals has increased dramatically. The massive flows of trucks and containers typically cause traffic jams when processing efficiency at a gate is insufficient. Therefore, automatic gate systems that recognize container numbers and truck license plates are needed in container terminals. However, several systems, such as radio frequency identification (RFID) and optical character recognition (OCR), can be used for gate automation. How to choose a suitable system is an important planning problem for terminal operators. This study identified the determinants influencing terminal operators while choosing an automated gate system. An objective structure was hierarchically constructed by exploratory factor analysis (EFA) to aid decision-making when choosing an automated gate system. An empirical study based on a dedicated terminal at the Keelung Port was conducted to assess the suitability of the RFID and OCR systems. The fuzzy analytical hierarchy process was applied to calculate the relative weights of criteria and the alternatives. Analytical results demonstrate that the RFID system is the most suitable system for the Keelung Port terminal.",,Electronic:978-962-367-696-0; POD:978-1-4244-7981-8,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5681704,OCR;RFID;container terminal;fuzzy AHP;liner shipping,Automation;Containers;Loading;Logic gates;Optical character recognition software;Radiofrequency identification;Security,decision making;fuzzy set theory;optical character recognition;planning;radiofrequency identification;sea ports;transportation,automatic container number identification system;automatic gate systems;container traffic;exploratory factor analysis;fuzzy analytic hierarchy processing;optical character recognition;port terminals;radiofrequency identification;terminal operator planning,,1,,17,,no,6-9 Oct. 2010,,IEEE,IEEE Conference Publications
Axial flux machines modelling with the combination of 2D FEM and analytic tools,A. Egea; G. Almandoz; J. Poza; A. Gonzalez,"University of Mondragon/Faculty of Engineering, (Spain)",The XIX International Conference on Electrical Machines - ICEM 2010,20101025,2010,,,1,6,"This paper deals with the development of performance analysis tools for axial flux permanent magnet machines. Modeling with 3D Finite Element Method (FEM3D) software could take too much time, and both the definition and the problem solving may be very arduous. In this work an analysis method for axial flux machines is proposed. This method consists in the combination of FEM2D simulations in the average radius plane with analytical models. The obtained results prove that the proposed method could be a very interesting option in terms of time and accuracy.",,Electronic:978-1-4244-4175-4; POD:978-1-4244-4174-7,10.1109/ICELMACH.2010.5608115,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5608115,Axial flux machines;FEM;analytic models,Analytical models;Conductors;Finite element methods;Harmonic analysis;Magnetic flux;Shape;Three dimensional displays,finite element analysis;permanent magnet machines,2D FEM;3D finite element method;FEM2D simulations;FEM3D software;axial flux machines;permanent magnet machines;radius plane,,2,,8,,no,6-8 Sept. 2010,,IEEE,IEEE Conference Publications
"BioExtract Server‰ÛÓAn Integrated Workflow-Enabling System to Access and Analyze Heterogeneous, Distributed Biomolecular Data",C. Lushbough; M. K. Bergman; C. J. Lawrence; D. Jennewein; V. Brendel,"University of South Dakota, Vermillion",IEEE/ACM Transactions on Computational Biology and Bioinformatics,20100205,2010,7,1,12,24,"Many in silico investigations in bioinformatics require access to multiple, distributed data sources and analytic tools. The requisite data sources may include large public data repositories, community databases, and project databases for use in domain-specific research. Different data sources frequently utilize distinct query languages and return results in unique formats, and therefore researchers must either rely upon a small number of primary data sources or become familiar with multiple query languages and formats. Similarly, the associated analytic tools often require specific input formats and produce unique outputs which make it difficult to utilize the output from one tool as input to another. The BioExtract Server (http://bioextract.org) is a Web-based data integration application designed to consolidate, analyze, and serve data from heterogeneous biomolecular databases in the form of a mash-up. The basic operations of the BioExtract Server allow researchers, via their Web browsers, to specify data sources, flexibly query data sources, apply analytic tools, download result sets, and store query results for later reuse. As a researcher works with the system, their ??steps?? are saved in the background. At any time, these steps can be preserved long-term as a workflow simply by providing a workflow name and description.",1545-5963;15455963,,10.1109/TCBB.2008.98,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4626945,Bioinformatics (genome or protein) databases;Database integration;Distributed architectures;Heterogeneous Databases.;data integration;distributed architectures;heterogeneous databases;mash-up;scientific workflow automation.,,bioinformatics;distributed databases;molecular biophysics;online front-ends;query languages;query processing,BioExtract Server;Web browsers;Web-based data integration application;bioinformatics;community databases;data consolidattion;data sources;heterogeneous distributed biomolecular data analysis;integrated workflow-enabling system;mash-up;project databases;public data repositories;query languages,"Biopolymers;Computational Biology;Data Mining;Database Management Systems;Databases, Factual;Information Dissemination;Internet;Software;Workflow",4,,52,,no,Jan.-March 2010,,IEEE,IEEE Journals & Magazines
Business Intelligence analytics without conventional data warehousing,W. Haque; B. Demerchant,"Univ. of Northern British Columbia, Prince George, BC, Canada",2010 International Conference on Information Society,20110915,2010,,,278,284,"The implementation of a Business Intelligence (BI) solution in an environment where traditional barriers prohibit incorporating analytics in operational and strategic decision making remains challenging. The solution must overcome a very tight budget, severe restrictions on data access due to security concerns, and a tradition of using conventional legacy tools for primary reporting. We propose a method that minimizes both the ETL (Extract Transform Load) and data warehousing components of the solution and allows for agile development and incremental adoption. This is achieved by taking advantage of the organization's current legacy reporting structure as the basis and then building a BI reporting layer on top for a high level view of the information. Our methodology has demonstrated benefits in the form of reduced complexity, reduced risk, reduced cost in both time and money, and a solution that provides a greater return on investment.",,Electronic:978-0-9564263-3-8; POD:978-1-4577-1823-6,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6018712,,Bismuth;Buildings;Data mining;Data warehouses;Databases;Organizations,competitive intelligence;data warehouses;decision making;software maintenance,BI reporting layer;agile development;business intelligence analytics;conventional data warehousing;extract transform load;incremental adoption;legacy reporting structure;strategic decision making,,0,,12,,no,28-30 June 2010,,IEEE,IEEE Conference Publications
C-NEDAT: A cognitive network engineering design analytic toolset for MANETs,L. Kant; A. McAuley; K. Manousakis; D. Shallcross; K. Sinkar; M. Tauil; O. Younis; K. Young; C. Graff; M. Patel; D. Yee; S. Mizan,"Applied Research, Telcordia Technologies Inc., Piscataway, NJ 08854, USA",2010 - MILCOM 2010 MILITARY COMMUNICATIONS CONFERENCE,20110106,2010,,,2333,2338,"Future force networks of the types envisioned for the network centric warfare (NCW) paradigm will be highly diverse, with the diversity spanning a wide range of (a) requirements (e.g., need for capacity, connectivity, survivability), (b) resources (e.g., radios with widely different capabilities and `smart' (e.g., Software Defined Radios (SDRs)), and (c) environments (e.g., urban, rural). The need to facilitate robust and adaptable communications in such networks has in turn triggered research in the area of cognitive networks that have the ability to `learn' and generate real-time control actions to adapt to the wide diversity of requirements, resources and environments. However, the combination of diversity and ‰ÛÏsmart‰Ûù networking exacerbates the problem of generating reliable and robust network designs. We present in this paper, our work on the use of cognitive mechanisms to assist with the design and analysis of robust NCW-like networks. Based on formal network-science based approaches, our Cognitive Network Engineering Design Analytic Toolset (C-NEDAT) provides for a systematic way to design, analyze and maintain robustness of future force MANETs. We provide in this paper an overview of the key functional modules and design capabilities of C-NEDAT and present example results.",2155-7578;21557578,Electronic:978-1-4244-8180-4; POD:978-1-4244-8178-1,10.1109/MILCOM.2010.5680350,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5680350,,Algorithm design and analysis;Delay;Mobile computing;Protocols;Robustness;Routing;Topology,cognitive radio;military communication;mobile ad hoc networks;real-time systems;software radio,C-NEDAT;MANET;NCW-like networks;analytic toolset;cognitive network engineering design;diversity spanning;force networks;formal network science;network centric warfare;real time control actions;software defined radios,,9,,5,,no,Oct. 31 2010-Nov. 3 2010,,IEEE,IEEE Conference Publications
Chinese Mobile Banking Service Evaluation Based on AHP Method,Q. Zhuo; Y. Li,"Dept. of Manage. Sci. & Eng., East China Univ. of Sci. & Technol., Shanghai, China",2010 International Conference on E-Product E-Service and E-Entertainment,20101210,2010,,,1,5,"In the mobile era with a growing number of mobile phone users, the financial needs of the users should not be underestimated. Mobile financial services market's enormous potential has driven the major banks to expand mobile financial market with Intensive efforts. The 3G era also propelled this trend. Mobile phone banks are beginning to be accepted by more and more customers. This paper establishes an evaluation model for the mobile banking service .The model involves five main factors: system quality, information quality, interface design quality, brand quality and fees and benefits. We applied the analytic hierarchy process (AHP) to determine the weights of different factors. And then, a ranking of eight major banks' mobile phone banking services is given by using the evaluation model. Finally, we provide some suggestions for banks' further optimization with insights into the ranking results.",,Electronic:978-1-4244-7161-4; POD:978-1-4244-7159-1,10.1109/ICEEE.2010.5661025,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5661025,,Banking;Business;Customer service;Mobile communication;Mobile handsets;Safety;Software,banking;decision making;mobile computing,AHP method;Chinese mobile banking service evaluation;analytic hierarchy process;mobile financial services,,1,,13,,no,7-9 Nov. 2010,,IEEE,IEEE Conference Publications
Clear and Precise Specification of Ecological Data Management Processes and Dataset Provenance,L. J. Osterweil; L. A. Clarke; A. M. Ellison; E. Boose; R. Podorozhny; A. Wise,"Dept. of Comput. Sci., Univ. of Massachusetts, Amherst, MA, USA",IEEE Transactions on Automation Science and Engineering,20091228,2010,7,1,189,195,"With the availability of powerful computational and communication systems, scientists now readily access large, complicated derived datasets and build on those results to produce, through further processing, yet other derived datasets of interest. The scientific processes used to create such datasets must be clearly documented so that scientists can evaluate their soundness, reproduce the results, and build upon them in responsible and appropriate ways. Here, we present the concept of an <i>analytic web</i>, which defines the scientific processes employed and details the exact application of those processes in creating derived datasets. The work described here is similar to work often referred to as ??scientific workflow,?? but emphasizes the need for a semantically rich, rigorously defined process definition language. We illustrate the information that comprises an analytic web for a scientific process that measures and analyzes the flux of water through a forested watershed. This is a complex and demanding scientific process that illustrates the benefits of using a semantically rich, executable language for defining processes and for supporting automatic creation of process provenance metadata.",1545-5955;15455955,,10.1109/TASE.2009.2021774,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5191091,"Dataset provenance;process definition;scientific <emphasis emphasistype=""bold"" role=""nolinebreak"">workflow</emphasis>",,data handling;meta data;scientific information systems;workflow management software,analytic web;clear specification;communication systems availability;complicated derived datasets;dataset provenance;derived datasets;derived datasets interest;ecological data management processes;powerful computational availability;precise specification;process definition language;provenance metadata process;scientific processes;scientific workflow;supporting automatic creation,,7,,31,,no,Jan. 2010,,IEEE,IEEE Journals & Magazines
Computer-Supported Assessment of Software Verification Proofs,C. A. Usener; S. Gruttmann; T. A. Majchrzak; H. Kuchen,"European Research Center for Information Systems (ERCIS), University of Muenster, Germany",2010 International Conference on Educational and Information Technology,20101025,2010,1,,V1-115,V1-121,"Most conventional e-assessment systems are not suited for examining analytic, creative and constructive skills and the few existing ones have too limited functionality to appropriately support Computer Science assessments. On this account the e-assessment system EASy has been developed which e.g. provides relevant exercise modules for Computer Science tasks. Recently the system has been extended by a module for the computer-supported examination of software verification proofs based on the Hoare Logic. In this work we discuss this module and evaluate its applicability, usability and acceptance in terms of a lecture on Formal Specification.",,Electronic:978-1-4244-8035-7; POD:978-1-4244-8033-3,10.1109/ICEIT.2010.5607766,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5607766,E-Assessment;Evaluation;Hoare Logic;Software Verification,Computers;Estimation,computer science education;educational administrative data processing;formal logic;formal verification,EASy;Hoare Logic;computer science assessment;computer-supported assessment;e-assessment system;formal specification;software verification,,0,,17,,no,17-19 Sept. 2010,,IEEE,IEEE Conference Publications
Convergency and Error Estimate of Nonlinear Fredholm Fuzzy Integral Equations of the Second Kind by Homotopy Analysis Method,M. Lotfi,"Islamic Azad UniversityScience & Res. Branch, Tehran, Iran",2010 Second International Conference on Computer Engineering and Applications,20100412,2010,2,,183,190,"A powerful, easy-to-use analytic tool for nonlinear problems in general, namely the homotopy analysis method, is further improved and systematically described through a typical nonlinear problem. In this paper, a Nonlinear Fredholm fuzzy integral equations is solved by using the homotopy analysis method(HAM). The approximate solution of this equation is calculated in the form of a series which its components are computed easily. The convergence and error estimate of the proposed method are proved.",,Electronic:978-1-4244-6080-9; POD:978-1-4244-6079-3,10.1109/ICCEA.2010.190,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5445598,Frednolm integral equation;Fuzzy integral equation;Homotopy analysis method,Application software;Computer applications;Computer errors;Fuzzy control;Fuzzy sets;Fuzzy systems;Integral equations;Nonlinear equations;Power engineering and energy;Power engineering computing,approximation theory;convergence of numerical methods;fuzzy set theory;integral equations;nonlinear equations,convergency analysis;error analysis;homotopy analysis method;nonlinear Fredholm fuzzy integral equations;nonlinear problems,,0,,11,,no,19-21 March 2010,,IEEE,IEEE Conference Publications
Cross Enterprise Improvements Delivered via a Cloud Platform: A Game Changer for the Consumer Product and Retail Industry,T. Chieu; S. Kapoor; A. Mohindra; A. Shaikh,"T.J. Watson Res. Center, IBM, Yorktown Heights, NY, USA",2010 IEEE International Conference on Services Computing,20100826,2010,,,530,537,"Gaining visibility into their retail supply chain has become a top priority for the Consumer Product (CP) industry. However, taking a ‰ÛÏdo-it-yourself‰Ûù approach to the problem is proving to be both expensive and complex. Cloud Computing, with its on-demand provisioning capability on shared resources, has emerged as a new paradigm to address the challenges of the CP industry. In this paper, we describe a framework for deployment of business analytic solutions on a Cloud platform. We illustrate the benefits of the approach in context of the Demand Driven Business Analytic solution that provides demand signals to CP manufacturers.",,Electronic:978-0-7695-4126-6; POD:978-1-4244-8147-7,10.1109/SCC.2010.35,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5557274,Analytics;Cloud;Composite Appliance;Demand Signal Repository;Supply Chain,Business;Cloud computing;Clouds;Computer architecture;Home appliances;Software;Virtual machining,Internet;consumer products;retail data processing;supply chain management,cloud computing;consumer product industry;cross enterprise improvement;demand driven business analytic solution;on-demand provisioning capability;retail supply chain,,3,,13,,no,5-10 July 2010,,IEEE,IEEE Conference Publications
Database architecture (R)evolution: New hardware vs. new software,S. Harizopoulos; T. Argyros; P. A. Boncz; D. Dietterich; S. Madden; F. M. Waas,"HP Labs, USA",2010 IEEE 26th International Conference on Data Engineering (ICDE 2010),20100415,2010,,,1210,1210,"The last few years have been exciting for data management system designers. The explosion in user and enterprise data coupled with the availability of newer, cheaper, and more capable hardware have lead system designers and researchers to rethink and, in some cases, reinvent the traditional DBMS architecture. In the space of data warehousing and analytics alone, more than a dozen new database product offerings have recently appeared, and dozens of research system papers are routinely published each year. In this panel, we will ask our panelists, a mix of industry and academic experts, which of those trends will have lasting effects on database system design, and which directions hold the biggest potential for future research. We are particularly interested in the differences in views and approaches between academic and industrial research.",1063-6382;10636382,Electronic:978-1-4244-5446-4; POD:978-1-4244-5445-7; USB:978-1-4244-5444-0,10.1109/ICDE.2010.5447682,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5447682,,Computer architecture;Data analysis;Data processing;Database systems;Educational institutions;Explosions;Field programmable gate arrays;Hardware;Software algorithms;Warehousing,database management systems,DBMS;data management system;data warehousing;database architecture;database system design,,0,,12,,no,1-6 March 2010,,IEEE,IEEE Conference Publications
DataStorm An Ontology-Driven Framework for Cloud-Based Data Analytic Systems,T. W. Wlodarczyk; C. Rong; B. Jia; L. Cocanu; C. I. Nyulas; M. A. Musen,"Dept. of Electr. Eng. & Comput. Sci., Univ. of Stavanger, Stavanger, Norway",2010 6th World Congress on Services,20100916,2010,,,123,127,"Cloud-based systems have proven to be a powerful technology for building data-intensive applications. However, the process of designing and deploying such applications is still primarily a manual one. There is a need for mechanisms and tools to help automate the required development steps. Using the Semantic Web ontology language OWL and the Hadoop platform we have developed a number of models and associated software tools that provide an end-to-end solution for designing and deploying cloud-based systems. This solution supports the construction of detailed models of data dependencies and their validation. It also enables generation and deployment of cloud-based data flows from those models. We illustrate its use for detecting alarm scenarios using data from vast underwater sensor-network.",2378-3818;23783818,Electronic:978-0-7695-4129-7; POD:978-1-4244-8199-6,10.1109/SERVICES.2010.92,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5575575,Cloud computing;Hadoop;MapReduce;data analysis;ontology;task-method decomposition,Algorithm design and analysis;Computational modeling;Connectors;Data analysis;OWL;Ontologies;Process control,data analysis;ontologies (artificial intelligence);semantic Web;software tools,DataStorm;Hadoop platform;OWL;cloud-based data analytic systems;cloud-based data flows;data analysis;data dependencies;ontology-driven framework;semantic Web ontology language;software tools;underwater sensor-network,,2,,24,,no,5-10 July 2010,,IEEE,IEEE Conference Publications
Deductive method with Sample Problems on Computational Object Knowledge Base and construct to intelligent educational softwares,N. V. Do; H. D. Nguyen,"University of Information and Technology, Viet Nam",2010 International Conference on Artificial Intelligence and Education (ICAIE),20101118,2010,,,805,810,"In computer science and information technology, ontology has been researched and developed in application for knowledge representation. Ontology COKB-ONT (Computational Object Knowledge Base) is an ontology was researched and applied in designing knowledge base systems, such as domain of knowledge about analytic geometry, linear algebra. However, when dealing with a practical problem, we often do not immediately find a new solution, but we search related problems which have been solved before and then proposing an appropriate solution for the problem. In this paper, the extension model of ontology COKB-ONT has been presented. In this model, Sample Problems, which are related problems, will be used like the experience of human about practical problem, simulate the way of human thinking about finding solution of problem. This extension model is applied to construct a program for solving plane geometry problems in middle school.",,Electronic:978-1-4244-6936-9; POD:978-1-4244-6935-2,10.1109/ICAIE.2010.5641419,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5641419,artificial intelligence;educational software;knowledge base system;knowledge representation;ontology,Algebra;Computational modeling;Indexes,computer aided instruction;knowledge based systems;ontologies (artificial intelligence),computational object knowledge base;deductive method;intelligent educational software;knowledge representation;ontology COKB-ONT;plane geometry problem,,0,,15,,no,29-30 Oct. 2010,,IEEE,IEEE Conference Publications
Design considerations for the gate circuit in distributed amplifiers,G. Bartolucci; F. Giannini; L. Scucchia,"Department of Electronics Engineering, University of Roma 'Tor Vergata', via del Politecnico 1, 00133 Roma, Italy","IET Circuits, Devices & Systems",20100524,2010,4,3,181,187,"An analytic model for the design of the elementary cell of the gate circuit in distributed amplifiers is discussed. The used approach is based on the image parameter representation of two-port networks. A closed form expression for the input impedance of the amplifier is provided. Design considerations for the gate circuit are presented based on this analytical result. The simulation of the gate network is performed by means of a commercial software package, including the physical models of the microstrip transmission lines and discontinuities present in the actual structure. Moreover, an hybrid circuit with the same configuration of the gate network has been manufactured. The agreement between experimental numerical and analytical data is remarkable.",1751-858X;1751858X,,10.1049/iet-cds.2008.0319,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5471255,,,distributed amplifiers;network synthesis;two-port networks,distributed amplifiers;gate circuit;image parameter representation;input impedance;two-port networks,,2,,,,no,10-May,,IET,IET Journals & Magazines
Design optimization of meta-material transmission lines for linear and non-linear microwave signal processing,S. Simion; R. Marcelli; G. Bartolucci; E. Proietti; G. De Angelis; A. Lucibello,"National Institute for Research and Development in Microtechnologies, Bucharest, Romania","35th International Conference on Infrared, Millimeter, and Terahertz Waves",20101028,2010,,,1,2,"The possibility to use CRLH (Composite Right/Left-Handed) cells to realize both distributed wide-band filters for linear signal processing and non-linear devices like frequency doublers is investigated analytically and numerically. Full-wave electromagnetic simulations are performed for the filtering structure by means of a commercial software package and confirm the validity of the analytic results. Numerical results for CRLH NLTL (Non-Linear Transmission Line) obtained by using the Microwave Office are discussed, providing design considerations about the synthesis of such a component.",2162-2027;21622027,Electronic:978-1-4244-6657-3; POD:978-1-4244-6655-9,10.1109/ICIMW.2010.5612958,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5612958,,Band pass filters;Capacitance;Capacitors;Microwave filters;Power transmission lines;Resonant frequency;Resonator filters,filtering theory;metamaterials;optimisation;signal processing;transmission lines,Microwave Office;commercial software package;composite right/left-handed cells;design optimization;distributed wide-band filters;filtering structure;frequency doublers;linear signal processing;meta-material transmission lines;nonlinear devices;nonlinear microwave signal processing;nonlinear transmission line,,0,,8,,no,5-10 Sept. 2010,,IEEE,IEEE Conference Publications
Designing visual analytics systems for disease spread and evolution: VAST 2010 mini challenge 2 and 3 award: Good overall design and analysis,J. Wood; A. Slingsby; J. Dykes,School of Informatics City University London,2010 IEEE Symposium on Visual Analytics Science and Technology,20101210,2010,,,285,286,"Using two of the VAST 2010 mini challenges as a case study, we report on the design decisions and software development process used to create visual analytics software for understanding disease spread and mutation. The software we developed and the analysis conducted attempted to help us understand (a) how a fictitious disease may have spread between selected cities around the globe; and (b) how genetic sequences taken from infected patients may be used to chart the evolution of the disease and changes in its severity, drug resistance and other characteristics.",,Electronic:978-1-4244-9487-3; POD:978-1-4244-9488-0,10.1109/VAST.2010.5652689,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5652689,,,data analysis;data visualisation;diseases;genetic engineering;health care;software engineering,VAST 2010 mini challenges;disease evolution;disease spread;genetic sequences;infected patients;software development;visual analytics systems,,0,,7,,no,25-26 Oct. 2010,,IEEE,IEEE Conference Publications
Dynamic application and model updating service,S. Guarino; C. Wu; J. Campolongo; E. Rundensteiner,"Charles River Analytics, Cambridge, MA",2010 International Symposium on Collaborative Technologies and Systems,20100603,2010,,,423,429,"Army Battle Command (BC) applications routinely collaborate to perform BC operations, particularly information collection, management, and analysis. Successful collaboration is highly dependent on timely and efficient access to common authoritative sources of information. These sources must satisfy the dynamic needs of active units, incorporating features that arise only after the initial deployment of the software infrastructure. Therefore, the data models for these sources must also be dynamic and allow for continuous modification while robustly propagating changes to affected applications. The applications then need to remain online (i.e., no recompilation or redeployment) to support ongoing operations while seamlessly adapting to any changes. To support these requirements, we are developing a Dynamic Application and Model Updating System (DYNAMUS). DYNAMUS provides a dynamic data model that includes the metadata required to track and visualize changes to the data model, a Software Development Kit (SDK) that supports the development of dynamic applications, and editing tools that support model architects while preventing application disruptions. These capabilities support timely and non-disruptive distribution of updates while maintaining operational effectiveness, usefulness, interoperability, and multilevel security.",,Electronic:978-1-4244-6622-1; POD:978-1-4244-6619-1,10.1109/CTS.2010.5478481,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5478481,Adaptive Displays;Dynamic Applications;Dynamic Data Model;Metadata;Schema Restructuring,Application software;Collaboration;Data models;Data visualization;Information analysis;Information management;Information resources;Performance analysis;Programming;Robustness,data models;meta data;military computing;software architecture,DYNAMUS;army battle command application;data model;dynamic application and model updating system;meta data;software development kit;software infrastructure,,0,,15,,no,17-21 May 2010,,IEEE,IEEE Conference Publications
Dynamic Evaluation Model for the Prototype Information System Development Process,M. Luo; S. Zhao; J. Jiang,"Sch. of Manage., Northwestern Polytech. Univ., Xi'an, China",2010 International Conference on Management and Service Science,20100916,2010,,,1,4,"The purpose of this paper is to promote the developing efficiency of information system (IS) and to make greatest possible effort to eliminate the information asymmetry between user and developer through constructing the dynamic evaluation model for prototype IS development process. Methods of fuzzy analytic hierarchy process (FAHP) and a new fuzzy matrices combination were employed. By constructing the hierarchy of IS development process, determining the indicators weights using FAHP and calculating the score and level the indicator belongs to, both developer and user can clearly figure out the main problem In accordance with the different IS developing stage, and reduce the lose to the minimum extent at development process.",,Electronic:978-1-4244-5326-9; POD:978-1-4244-5325-2,10.1109/ICMSS.2010.5576575,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5576575,,Computers;Humans;Industries;Information systems;Pragmatics;Prototypes;Software,fuzzy set theory;information systems;matrix algebra,dynamic evaluation model;fuzzy analytic hierarchy process;fuzzy matrices;information asymmetry;prototype information system development process,,0,,14,,no,24-26 Aug. 2010,,IEEE,IEEE Conference Publications
Dynamic Programming Based Hybrid Strategy for Offline Cursive Script Recognition,G. Sulong; T. Saba; A. Rehman,"Dept. of Comput. Graphics & Multimedia, Univ. of Technol. Malaysia (UTM), Skudai, Malaysia",2010 Second International Conference on Computer Engineering and Applications,20100412,2010,2,,580,584,"In domain of analytic cursive word recognition, there are two main approaches: explicit segmentation based and implicit segmentation based. However, both approaches have their own shortcomings. To overcome individual weaknesses, this paper presents a hybrid strategy for recognition of strings of characters (words or numerals). In a two stage dynamic programming based, lexicon driven approach, first an explicit segmentation is applied to segment either cursive handwritten words or numeric strings. However, at this stage, segmentation points are not finalized. In the second verification stage, statistical features are extracted from each segmented area to recognize characters using a trained neural network. To enhance segmentation and recognition accuracy, lexicon is consulted using existing dynamic programming matching techniques. Accordingly, segmentation points are altered to decide true character boundaries by using lexicon feedback. A rigorous experimental protocol shows high performance of the proposed method for cursive handwritten words and numeral strings.",,Electronic:978-1-4244-6080-9; POD:978-1-4244-6079-3,10.1109/ICCEA.2010.287,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5445714,cursive character recognition;dynamic programming;explicit segmentation;hybrid strategy;implicit segmentation,Application software;Character generation;Character recognition;Computer applications;Computer graphics;Dynamic programming;Feature extraction;Image recognition;Image segmentation;Turing machines,dynamic programming;feature extraction;handwritten character recognition;image segmentation;neural nets;statistics,cursive handwritten words;cursive numeric strings;cursive word recognition;dynamic programming;explicit segmentation;implicit segmentation;lexicon feedback;offline cursive script recognition;statistical features extraction;trained neural network,,1,,22,,no,19-21 March 2010,,IEEE,IEEE Conference Publications
E-business system integrations of enterprise post merger and acquisition based on continuous assurance - A case study,Chun-Hsiu Yeh; Wei-Cheng Shen,"Department Information Management, Chung Chou Institute of Technology, Information Management Center, Forhouse Corp.Ltd., Taiwan",2010 3rd IEEE International Conference on Ubi-Media Computing,20100809,2010,,,260,265,"Recently, because of the competitive environment, Taiwan's high-technology industries have undergone enormous change in a series of enterprise mergers and acquisitions. The enterprise mergers and acquisitions led us to study the e-business integration impact issues on continuous assurance. Our study found that the information technology integration strategies and assurance process integration methods had an impact on their continuous assurance performances and effectiveness. We used case study research methodology that including data collection, case attendant interview, data analysis and summary finding steps for creating new model applications of information technology integrations. The information technology integration assurance (ITIS) model steps include information technology infrastructure building, assurance process integrations, database immigration and analytic and alarm methods. Our findings close the literature theory gaps between IT integrations and continuous assurance; it will be applicable for information technology integration and continuous assurance about post-enterprise mergers and acquisitions.",,Electronic:978-1-4244-6709-9; POD:978-1-4244-6708-2,10.1109/UMEDIA.2010.5544452,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5544452,,Application software;Corporate acquisitions;Data analysis;Databases;Enterprise resource planning;Information analysis;Information management;Information systems;Information technology;Proposals,business data processing;corporate acquisitions;information technology,Taiwan;assurance process integration;continuous assurance;data analysis;database immigration;e-business system integration;enterprise acquisition;enterprise post merger;high technology industries;information technology infrastructure building;information technology integration assurance,,0,,24,,no,5-6 July 2010,,IEEE,IEEE Conference Publications
Efficient procedure to evaluate electromagnetic transients on three-phase transmission lines,E. C. M. Costa; S. Kurokawa; J. Pissolato; A. J. Prado,"Universidade Estadual de Campinas, Brazil","IET Generation, Transmission & Distribution",20100819,2010,4,9,1069,1081,"This paper presents a hybrid way mixing time and frequency domain for transmission lines modelling. The proposed methodology handles steady fundamental signal mixed with fast and slow transients, including impulsive and oscillatory behaviour. A transmission line model is developed based on lumped elements representation and state-space techniques. The proposed methodology represents an easy and practical procedure to model a three-phase transmission line directly in time domain, without the explicit use of inverse transforms. The proposed methodology takes into account the frequency-dependent parameters of the line, considering the soil and skin effects. In order to include this effect in the state matrices, a fitting method is applied. Furthermore the accuracy of proposed the developed model is verified, in frequency domain, by a simple methodology based on line distributed parameters and transfer function related to the input/output signals of the lumped parameters representation. In addition, this article proposes the use of a fast and robust analytic integration procedure to solve the state equations, enabling transient and steady-state simulations. The results are compared with those obtained by the commercial software Microtran (EMTP), taking into account a three-phase transmission line, typical in the Brazilian transmission system.",1751-8687;17518687,,10.1049/iet-gtd.2009.0660,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5551072,,,EMTP;matrix algebra;power engineering computing;power overhead lines;transfer functions,Brazilian transmission system;EMTP;Microtran software;electromagnetic transients evaluation;fitting method;frequency domain;hybrid way mixing time;impulsive behaviour;line distributed parameters;lumped elements representation;oscillatory behaviour;overhead transmission line models;state matrices;state-space techniques;steady fundamental signal;three-phase transmission lines;transfer function,,5,,,,no,10-Sep,,IET,IET Journals & Magazines
Embedded systems design ‰ÛÓ Scientific challenges and work directions,J. Sifakis,Verimag,Formal Methods in Computer Aided Design,20110519,2010,,,11,11,"Summary form only given. The development of a satisfactory Embedded Systems Design Science provides a timely challenge and opportunity for reinvigorating Computer Science. Embedded systems are components integrating software and hardware jointly and specifically designed to provide given functionalities, which are often critical. They are used in many applications areas including transport, consumer electronics and electrical appliances, energy distribution, manufacturing systems, etc. Embedded systems design requires techniques taking into account extra-functional requirements regarding optimal use of resources such as time, memory and energy while ensuring autonomy, reactivity and robustness. Jointly taking into account these requirements raises a grand scientific and technical challenge: extending Computer Science with paradigms and methods from Control Theory and Electrical Engineering. Computer Science is based on discrete computation models not encompassing physical time and resources which are by their nature very different from analytic models used by other engineering disciplines. We summarize some current trends in embedded systems design and point out some of their characteristics, such as the chasm between analytical and computational models, and the gap between safety critical and best-effort engineering practices. We call for a coherent scientific foundation for embedded systems design, and we discuss a few key demands on such a foundation: the need for encompassing several manifestations of heterogeneity, and the need for design paradigms ensuring constructivity and adaptivity. We discuss main aspects of this challenge and associated research directions for different areas such as modelling, programming, compilers, operating systems and networks.",,Electronic:978-0-9835678-0-6; POD:978-1-4577-0734-6,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770926,,Adaptation model;Analytical models;Computational modeling;Computer science;Electrical products;Embedded systems;Robustness,computer science;embedded systems;systems analysis,compilers;computer science;consumer electronics;control theory;discrete computation models;electrical appliances;electrical engineering;embedded systems design science;energy distribution;manufacturing systems;operating systems;safety critical;scientific challenges;transport;work directions,,0,,,,no,20-23 Oct. 2010,,IEEE,IEEE Conference Publications
Evaluation of Teacher's Performance in Independent Colleges Based on AHP and Multi-Level Matter Element Extension Measurement Models,G. Hu; W. Li; Y. Li,"Dept. of Art Design, Henan Inst. of Eng., Zhengzhou, China",2010 International Conference on Computational Intelligence and Software Engineering,20101230,2010,,,1,4,"Evaluation of Teacher's Performance is the important instrument of carrying out teaching management, and is important assurance of inspiring teachers to further efforts, improving teaching quality and carrying out scientific teaching management and decision. In this article we studied target system of Independent Colleges' teacher performance evaluation setting up, how to count target weight based on AHP, multi-level matter element extension measurement evaluation model and arithmetic realization. Considering the deficiency of evaluation methods of teacher's performance, the evaluation index system was established by analyzing the major factors affecting evaluation of teacher's Performance y. Applied the multilevel matter element theory into the multilevel matter element evaluation model for teacher's performance and expatiated the analyzing method and process. Aiming at the qualification requirements that are proposed to teachers by universities, considering the three tasks of teaching work, Scientific Research and Working Attitude, following up the principles that are integrated with the transition from the qualitative to quantitative, a multi-level matter element extension measurement performance evaluation model for teachers is raised; it can make the performance evaluation for teachers more scientific, objective and reasonable.",,Electronic:978-1-4244-5392-4; POD:978-1-4244-5391-7,10.1109/CISE.2010.5676753,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676753,,Analytical models;Appraisal;Educational institutions;Indexes;Performance evaluation;Weight measurement,decision making;educational institutions;teaching,AHP;analytic hierarchy process;evaluation index system;independent college;multilevel matter element extension;teacher performance evaluation;teaching management;teaching quality,,0,,7,,no,10-12 Dec. 2010,,IEEE,IEEE Conference Publications
Evaluation on the factors affecting the quality of social audit based on FAHP,Y. Li; J. He,"School of Business Administration, North China Electric Power University, Baoding, China",2010 IEEE International Conference on Software Engineering and Service Sciences,20100819,2010,,,659,662,"In recent years, the exposure of the mansions of Silver, Enron and other major financial scandals led an unprecedented credit crisis and moral crisis to the audit, and they also raises people's researching interest in the quality of audit and its impact factors. At present, domestic and foreign scholars have done lots of work on the factors affecting the quality of audit, but the literature is mostly qualitative not quantitative. This paper uses fuzzy analytic hierarchy process to determine the weight of main factors affecting the quality of auditing.",2327-0586;23270586,Electronic:978-1-4244-6055-7; POD:978-1-4244-6054-0,10.1109/ICSESS.2010.5552257,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5552257,audit environment;audit object;audit quality;audit subject;fuzzy AHP,Artificial neural networks;Companies;Educational institutions;Law;Power systems,decision making;finance;fuzzy set theory,FAHP;credit crisis;fuzzy analytic hierarchy process;moral crisis;social audit,,0,,7,,no,16-18 July 2010,,IEEE,IEEE Conference Publications
Flood Hazard Evaluation and GIS in Guangzhou,A. Pan; M. Yang; B. Chen,"Sch. of Geogr. Sci., Guangzhou Univ., Guangzhou, China",2010 International Conference on Multimedia Technology,20101111,2010,,,1,4,"The author used both the analytic hierarchy process (AHP) and the factor superposition method to evaluate flood risk and vulnerability of environments subject to continuous hazards in areas and districts of Guangzhou. Natural environmental flood risk factors were evaluated as was the subsequent level of vulnerability to human activities in each area in Guangzhou. To express its spatial relationships, a Flood Hazard Evaluation map was created using GIS software. Analysis of the results produced the following conclusions: flood hazard intensity in Guangzhou is higher in the south and lower in the north; considered from west to east, western and eastern areas have a higher flood hazard intensity while the city's center is low. These findings are related to central Guangzhou being dominated by mountains and no major rivers, while the western and eastern areas consist of low-lying areas with many rivers and streams. Results from this study should serve as baseline information both for flood prevention and for its mitigation in Guangzhou and similar cities.",,Electronic:978-1-4244-7874-3; POD:978-1-4244-7871-2,10.1109/ICMULT.2010.5631139,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5631139,,Cities and towns;Economic indicators;Environmental factors;Floods;Rivers;Tides,decision making;floods;geographic information systems;geophysics computing,GIS software;Guangzhou;analytic hierarchy process;factor superposition method;flood hazard evaluation map;natural environmental flood risk factors,,1,,11,,no,29-31 Oct. 2010,,IEEE,IEEE Conference Publications
Formal Verification of Industrial Software with Dynamic Memory Management,S. Labbe; A. Sangnier,"R&D, EDF, Chatou, France",2010 IEEE 16th Pacific Rim International Symposium on Dependable Computing,20110128,2010,,,77,84,"Tool-based analytic techniques such as formal verification may be used to justify the quality, correctness and dependability of software involved in digital control systems. This paper reports on the development and application of a tool-based methodology, the purpose of which is the formal verification of freedom from intrinsic software faults related to dynamic memory management. The paper introduces the operational and research context in the power generation industry, in which this work takes place. The theoretical framework and the tool at the cornerstone of the methodology are then presented. The paper also presents the practical aspects of the research: software under analysis, experimental results and lessons learned. The results are seen promising, as the methodology scales accurately in identified conditions of analysis, and has a number of perspectives which are currently under study in ongoing work.",,Electronic:978-0-7695-4289-8; POD:978-1-4244-8975-6,10.1109/PRDC.2010.19,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5703230,Digital control systems;data structures;formal verification;memory allocation;software dependability,,data structures;digital control;electricity supply industry;formal verification;power engineering computing;software fault tolerance;software quality;storage allocation;storage management,digital control;dynamic memory management;formal verification;industrial software;intrinsic software fault;operational research;power generation industry;tool based analytic technique,,0,,23,,no,13-15 Dec. 2010,,IEEE,IEEE Conference Publications
"From Student to Teacher: Transforming Industry Sponsored Student Projects into Relevant, Engaging, and Practical Curricular Materials",J. Bolinger; K. Yackovich; R. Ramnath; J. Ramanathan; N. Soundarajan,"Dept. of Comput. Sci. & Eng., Ohio State Univ., Columbus, OH, USA",2010 IEEE Transforming Engineering Education: Creating Interdisciplinary Skills for Complex Global Environments,20100715,2010,,,1,21,"Over the past several years we have collaborated with a variety of industrial partners to carry out applied research and capstone design projects in cooperation with our students. Although the projects have varied widely, more often than not, success or failure lies within the students' ability to see beyond the technical challenges into the subtleties of the business and the meaning of value. Looking back at our traditional software engineering curriculum it is not so surprising that gaps in technical skills are not typically the source of problems. With a strong traditional focus on the construction of software, we have been producing graduates who can build relatively complex stand-alone systems. Unfortunately, in today's world, being able to build software is only a small, albeit necessary, skill for software engineers and it is miles away from being sufficient. The challenges inherent in providing a portfolio of innovative, integrated, and strategic IT services are well beyond any of the techniques or conceptual frameworks historically taught in many software engineering curriculums, including our own. To address these shortcomings we have recently begun experimenting with a new curriculum that presents software engineering in its larger context as a strategic business function. We are also beginning to stress the importance of using a set of analytic frameworks to guide the evolution and development of software systems starting with the business and its context, through the architecture and design stages, and finally into implementation and support. To create materials for this curriculum we have gone back to the original voice of the problem and are attempting to assemble learning materials from the projects that industry has championed for us in the past. Our goal is not merely to showcase the software that was built, but rather to expose the reasons behind their conception and the frameworks used to make critical decisions throughout the process.",,Electronic:978-1-4244-6042-7; POD:978-1-4244-6040-3,10.1109/TEE.2010.5508872,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5508872,,Acoustic materials;Acoustical engineering;Collaboration;Computer industry;Computer science;Design engineering;Portfolios;Software engineering;Software systems;Stress,computer science education;decision making;educational courses;engineering education;software engineering,design projects;industry sponsored student projects;software construction;software engineering curriculum;software engineers;software systems development;stand-alone systems;strategic IT services;strategic business function;student ability;technical skills,,4,,18,,no,6-9 April 2010,,IEEE,IEEE Conference Publications
Fuzzy Comprehensive Evaluation of Corrosion-Resistant Properties of PTFE Compounded Fibers,L. Yang; X. Fan,"Zhongyuan Univ. of Technol., Zhengzhou, China",2010 International Conference on Computational Intelligence and Software Engineering,20101230,2010,,,1,4,"PTFE compounded fiber is the raw material of the industrial fabrics which have been applied for separating particles from industrial exhaust under harsh industrial conditions such as the oxidant and high temperature environments. It is a decision process with multiple factor analysis to select an optimal PTFE compounded fiber with good corrosion-resistant property among several samples. The study applied Analytic Hierarchy Process and a new fuzzy comprehensive evaluation method to evaluate the corrosion-resistant properties of four tested PTFE compounded fibers, which are PTFE fiber with chemical coat and PTFE fiber without chemical coat and PTFE/glass compounded fibers with membrane laminating and PTFE/glass compounded fibers with chemical coat. The results show that the compounded technology of PTFE/glass fibers and chemical coating can all improve the corrosion-resistant properties of the industrial fabrics against the oxidant and high temperature. It also can be concluded that the new fuzzy comprehensive evaluation method have advantages in the selection of the optimal one among many PTFE compounded fiber samples.",,Electronic:978-1-4244-5392-4; POD:978-1-4244-5391-7,10.1109/CISE.2010.5677055,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5677055,,Chemicals;Fabrics;Glass;Optical fiber testing;Optical fiber theory;Resistance;Temperature,coatings;corrosion resistance;fabrics;fuzzy set theory;glass fibres,PTFE compounded fiber;PTFE-glass compounded fiber;analytic hierarchy process;chemical coating;corrosion-resistant property;fuzzy comprehensive evaluation;industrial exhaust;industrial fabrics;multiple factor analysis,,0,,14,,no,10-12 Dec. 2010,,IEEE,IEEE Conference Publications
Grey Analytic Hierarchy Process Applied to Effectiveness Evaluation for Crime Prevention System,H. Bu; X. Guo; S. Chen,"Nat. Eng. Res. Center for Multimedia Software, Wuhan Univ., Wuhan, China",2010 International Conference on Biomedical Engineering and Computer Science,20100506,2010,,,1,4,"With the rapid development of industrialization and urbanization, crime prevention issues become serious in China each day. Many cities have established a number of crime prevention systems in order to maintain social stability. However, crime prevention system is a complex system, which is controlled by a number of interrelated factors and is difficult to estimate. To overcome the obstacles of conventional effectiveness evaluation for crime prevention system, this paper proposed an effectiveness evaluation method for crime prevention system which combined the advantages of the analytic hierarchy process (AHP) and a grey clustering method to guarantee the accuracy and objectivity of weight coefficients. After constructing an index system of crime prevention system effectiveness evaluation based on correlated factors, we calculated the weight of every index with AHP and gave an evaluation result by means of a grey clustering method. A case study was given to validate the design of the underlying grey analytic hierarchy process model. Results show the feasibility and reliability of the model, which will be helpful to realize the quantitative analysis in crime prevention system effectiveness evaluation and provide a decision support tool for decision makers.",2165-9192;21659192,Electronic:978-1-4244-5316-0; POD:978-1-4244-5315-3,10.1109/ICBECS.2010.5462408,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5462408,,Cities and towns;Clustering methods;Computer crime;Computer industry;Control systems;Electrical equipment industry;Probability;Protection;Security;Stability,computer crime;grey systems,correlated factor;crime prevention system;decision support tool;grey analytic hierarchy process;grey clustering method,,0,,12,,no,23-25 April 2010,,IEEE,IEEE Conference Publications
"Hierarchical Aggregation for Information Visualization: Overview, Techniques, and Design Guidelines",N. Elmqvist; J. D. Fekete,"Purdue University, West Lafayette",IEEE Transactions on Visualization and Computer Graphics,20100311,2010,16,3,439,454,"We present a model for building, visualizing, and interacting with multiscale representations of information visualization techniques using hierarchical aggregation. The motivation for this work is to make visual representations more visually scalable and less cluttered. The model allows for augmenting existing techniques with multiscale functionality, as well as for designing new visualization and interaction techniques that conform to this new class of visual representations. We give some examples of how to use the model for standard information visualization techniques such as scatterplots, parallel coordinates, and node-link diagrams, and discuss existing techniques that are based on hierarchical aggregation. This yields a set of design guidelines for aggregated visualizations. We also present a basic vocabulary of interaction techniques suitable for navigating these multiscale visualizations.",1077-2626;10772626,,10.1109/TVCG.2009.84,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5184827,Aggregation;clustering;clutter reduction;massive data sets;visual analytics.;visual exploration,,data visualisation,aggregated visualizations;design guidelines;hierarchical aggregation for;information visualization;multiscale functionality;multiscale visualizations;node-link diagrams;visual representations,"Algorithms;Computer Graphics;Image Interpretation, Computer-Assisted;Information Storage and Retrieval;Models, Theoretical;Software;Software Design;User-Computer Interface",72,1,82,,no,May-June 2010,,IEEE,IEEE Journals & Magazines
HLA-based parallel test grid simulation,W. Lei; F. Yang-wang; X. Xin; H. Lei-Gang,"Engineering School of the Air Force Engineering University Xi'an, 710038, China",2010 IEEE AUTOTESTCON,20101028,2010,,,1,5,"Reconfigured Parallel Test Grid (RPTG) technology advanced a novel theory to improve test and ATE using efficiency. In order to analyze the function and efficiency of RPTG, it is important to simulate the RPTG environment authentically. Based on parallel and reconfigured characteristic of RPTG, HLA-based layered analytic simulation architecture was put forward. This solution can exploit the parallelism of the analytic simulation applications and support them well; also test tasks could be distributed effectively and dynamically.",1088-7725;10887725,Electronic:978-1-4244-7961-0; POD:978-1-4244-7960-3,10.1109/AUTEST.2010.5613631,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613631,HLA;High-performance Simulation;Parallel Processing;RPTG,Analytical models;Computational modeling;Computer architecture;Computers;Schedules;Software;Test equipment,automatic testing;digital simulation;grid computing;parallel processing,ATE;HLA based parallel test grid simulation;analytic simulation applications;reconfigured parallel test grid technology,,0,,7,,no,13-16 Sept. 2010,,IEEE,IEEE Conference Publications
How Information Visualization Novices Construct Visualizations,L. Grammel; M. Tory; M. A. Storey,University of Victoria,IEEE Transactions on Visualization and Computer Graphics,20101028,2010,16,6,943,952,"It remains challenging for information visualization novices to rapidly construct visualizations during exploratory data analysis. We conducted an exploratory laboratory study in which information visualization novices explored fictitious sales data by communicating visualization specifications to a human mediator, who rapidly constructed the visualizations using commercial visualization software. We found that three activities were central to the iterative visualization construction process: data attribute selection, visual template selection, and visual mapping specification. The major barriers faced by the participants were translating questions into data attributes, designing visual mappings, and interpreting the visualizations. Partial specification was common, and the participants used simple heuristics and preferred visualizations they were already familiar with, such as bar, line and pie charts. We derived abstract models from our observations that describe barriers in the data exploration process and uncovered how information visualization novices think about visualization specifications. Our findings support the need for tools that suggest potential visualizations and support iterative refinement, that provide explanations and help with learning, and that are tightly integrated into tool support for the overall visual analytics process.",1077-2626;10772626,,10.1109/TVCG.2010.164,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613431,Empirical study;novices;visual analytics;visual mapping;visualization;visualization construction,Decision support systems;IEEE Computer Society;Visualization,data analysis;data visualisation,data attribute selection;exploratory data analysis;human mediator;information visualization;iterative refinement;iterative visualization construction;sales data;visual analytics process;visual mapping specification;visual template selection;visualization interpretation;visualization specifications,,27,,40,,no,Nov.-Dec. 2010,,IEEE,IEEE Journals & Magazines
I/Q imbalance effects in quadrature ë£ëÓ modulators ‰ÛÓ Analysis and signal processing,J. Marttila; M. AllÌ©n; M. Valkama,"Department of Communications Engineering, Tampere University of Technology P.O. Box 553, FI-33101, Tampere, Finland",2010 IEEE International Microwave Workshop Series on RF Front-ends for Software Defined and Cognitive Radio Solutions (IMWS),20100412,2010,,,1,4,"This article focuses on the in-phase/quadrature (I/Q) imbalance problem in quadrature ë£ëÓ modulators (Që£ëÓM). In the literature, such quadrature modulators have been seen as a promising possibility for software defined radio (SDR) receivers, but matching the quadrature circuits is an inevitable problem for this kind of systems. In this article, an analytic closed-form model will be derived for a mismatched first order Që£ëÓM by means of the modulator transfer functions and the underlying implementation components. In addition, notching the signal transfer function (STF) on the desired signal mirror frequency inside the modulator is proposed for cancelling the mirror frequency interference originating from the blocking signal on that band. This is shown to be effective in case of the modulator feedback branch mismatches and can be implemented with simple additional feedforward coefficient(s) compared to ordinary modulator.",,Electronic:978-1-4244-5753-3; POD:978-1-4244-5751-9,10.1109/IMWS.2010.5440963,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5440963,Analog-digital conversion;I/Q imbalance;bandpass filters;complex filters;digital radio;mirror frequency interference;radio receivers;sigma-delta modulation,Circuits;Feedback;Frequency modulation;Interference cancellation;Mirrors;Receivers;Signal analysis;Signal processing;Software radio;Transfer functions,interference (signal);radio receivers;sigma-delta modulation;software radio,analytic closed-form model;in-phase-quadrature imbalance problem;mirror frequency interference;modulator feedback branch;modulator transfer functions;quadrature ë£ëÓ modulators;signal transfer function;software defined radio receivers,,2,,18,,no,22-23 Feb. 2010,,IEEE,IEEE Conference Publications
Imaging and reconstruction of a 3D complex target using downward-looking step-frequency radar,J. Dai; Y. Q. Jin,"Key Laboratory of Wave Scattering and Remote Sensing Information (MoE), Fudan University, Shanghai 200433, China","Proceedings of the 9th International Symposium on Antennas, Propagation and EM Theory",20110120,2010,,,892,896,"A technique of imaging and reconstruction for a three-dimensional (3D) complex-shaped Perfect Electric Conductor (PEC) target is developed using step-frequency radar observation synthesizing a two-dimensional aperture. The radar works in downward-looking spotlight mode moving within a 2D circular arc aperture to eliminate geometric disortions and shadowing effect, and the backscattered electrical fields in both the amplitude and phase are obtained. The three-dimensional fast Fourier transform (3D-FFT) algorithm is adopted for uniformly resampling data, which are aquired by interpolating the collected backscattering fields to quickly form a focus image. The bidirectional analytic ray tracing (BART) method [1] is applied to fast calculate the backscattering fields from the 3D complex target, e.g. a tank. Automatic reconstruction of the target is well demonstrated. As a validation, the scattering fields are also computed and compared using widely accepted software FEKO based on Physical Optics.",,Electronic:978-1-4244-6908-6; POD:978-1-4244-6906-2,10.1109/ISAPE.2010.5696614,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5696614,,,image reconstruction;physical optics;radar imaging;radar tracking;target tracking,2D circular arc aperture;3D complex target imaging;3D complex target reconstruction;3D complex-shaped perfect electric conductor;FEKO software;backscattered electrical field;bidirectional analytic ray tracing method;downward-looking spotlight mode;downward-looking step-frequency radar;geometric disortion elimination;physical optics;shadowing effect elimination;step-frequency radar observation;three-dimensional fast Fourier transform algorithm,,2,,12,,no,Nov. 29 2010-Dec. 2 2010,,IEEE,IEEE Conference Publications
Indicator-system based adaptive information assurance evaluation for aeronautical information system,Z. Wu; D. Zhao; Y. Peng,"Tianjin Key Laboratory for Advanced Signal Processing, Civil Aviation University of China, China, 300300",2010 IEEE International Conference on Intelligent Computing and Intelligent Systems,20101206,2010,1,,14,17,"With the consideration of the system features, an indicator system of information assurance for aeronautical information system is established. The indicator system is based on SSE (Systems Security Engineering) theory, and DSC (Dynamic System Control) method. The indicator system is established from four aspects, stratagem, technique, management and engineering. Fuzzy-AHP (Analytic Hierarchy Process) is used to evaluate the static indicators. A quite good result is got.",,Electronic:978-1-4244-6585-9; POD:978-1-4244-6582-8,10.1109/ICICISYS.2010.5658434,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5658434,DSC method;Fuzzy-AHP;SSE;aeronautical information system;indicator system,Bismuth;Software,aerospace computing;decision making;fuzzy set theory;information systems;security of data,adaptive information assurance evaluation;aeronautical information system;analytic hierarchy process;dynamic system control;fuzzy AHP;indicator system;systems security engineering,,0,,7,,no,29-31 Oct. 2010,,IEEE,IEEE Conference Publications
Influence factor analysis of product development process,K. Zhang; B. Guo,"Department of System Engineering, College of information system and management, National University of Defense Technology, Changsha, Hunan, China, 410073",2010 IEEE International Conference on Industrial Engineering and Engineering Management,20101223,2010,,,2361,2365,"In the product development process considering design iteration, it's more difficult to reflect interrelationship between variables with simulation analysis. Especially there is no certain function expressing the effect of parameter interrelationship on the product development time. The existence of iteration complicates the structure of analytic model, and the metamodel can simplify the problem and reduce the costs. The paper analyzed the impact of the parameters on average completion time of product development in design structure maxim by building the metamodel, and identified the main impact factors.",2157-3611;21573611,Electronic:978-1-4244-8503-1; POD:978-1-4244-8501-7; USB:978-1-4244-8502-4,10.1109/IEEM.2010.5674356,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5674356,design iteration;influence factor;metamodel;product development process,Assembly;Optimized production technology;Software;Training,iterative methods;product development,design iteration;influence factor analysis;parameter interrelationship;product development process,,0,,12,,no,7-10 Dec. 2010,,IEEE,IEEE Conference Publications
Information Security Risk Assessment Methodology Research: Group Decision Making and Analytic Hierarchy Process,Z. Xinlan; H. Zhifang; W. Guangfu; Z. Xin,"Sch. of Econ. & Manage., China Univ. of Geosci., Wuhan, China",2010 Second World Congress on Software Engineering,20110222,2010,2,,157,160,"Information security risk can be measured by probability of the potential risk incident and its impact. Various quantitative methodologies are given to compute information security risks, but among the existed research, seldom of them considered the difficulties of obtaining data of risk probability and risk impact. Considering the efficiency and operability of collecting data, as well as the effectiveness of output for risk management support, this paper presents a risk assessment methodology for information systems security with the application of group decision making and analytic hierarchy process methods. Procedure of this methodology is provided, and a test case is given to illustrate the effectiveness of this methodology.",,Electronic:978-0-7695-4303-1; POD:978-1-4244-9287-9,10.1109/WCSE.2010.55,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5718368,analytic hierarchy process;group decision making;information security;risk assessment,Decision making;Equations;Information security;Mathematical model;Risk management,decision making;risk management;security of data,analytic hierarchy process method;group decision making;information security risk assessment methodology;risk impact;risk management;risk probability,,2,,11,,no,19-20 Dec. 2010,,IEEE,IEEE Conference Publications
Information support of corporate governance and strategic management using analytical software,D. Isaev,"Business Analytics Department, Higher School of Economics (HSE), Moscow, Russian Federation",2010 IEEE International Conference on Intelligent Computing and Intelligent Systems,20101206,2010,3,,44,48,"In the paper an information aspect of corporate governance and strategic management is considered. Relying on analysis of merits and limitations of current developments the concept of performance management information support system and the appropriate modeling approach are proposed. Finally, the possibilities of use of analytical information systems for corporate governance and strategic management are discussed.",,Electronic:978-1-4244-6585-9; POD:978-1-4244-6582-8,10.1109/ICICISYS.2010.5658550,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5658550,analytical applications;business intelligence;corporate governance;performance management systems;strategic management,Analytical models;Biological system modeling;Economics;Industries;Organizations;Personnel;Predictive models,competitive intelligence;information systems;strategic planning,analytical information system;analytical software;business intelligence;corporate governance;information support;performance management;strategic management,,1,,15,,no,29-31 Oct. 2010,,IEEE,IEEE Conference Publications
Innovative video analytics for maritime surveillance,A. Samama,"Automatic Sea Vision, S&#x00E8;vres, France",2010 International WaterSide Security Conference,20110314,2010,,,1,8,"In this paper, we are presenting a new software dedicated to maritime surveillance and security named AUTOMATIC SEA VISION<sup>å¨</sup> (ASV) (Waquet, 2007) ASV is the first entirely automatic optical software processing (video analytics) providing a smart solution to maritime safety and security issues, whether the intended use is for applications on board ships, or for protection of critical coastal areas and port facilities. The software processes in real time the output from multiple sensors (mainly IR camera, but also GPS, Inertial Navigation Unit...) in order to automatically detect any boat or object for a 24/7 surveillance. We rely on maritime environment and IR sensor data specificities to develop tailored algorithms to perform automatic object detection. These algorithms have been packaged into a complete solution which performs from data acquisition through a user friendly graphical user interface (GUI). Extensive tests have been carried out to validate our solution. We present a quantitative evaluation done on ground truth data. System performance has been calculated using Detection/False Alarm rates over multiple sequences acquired from numerous cameras. Moreover, qualitative evaluations have demonstrated the robustness of the system in numerous different weather conditions. The last section of this paper tackles algorithms enhancements as well as future evolutions of the system.",2166-1782;21661782,Electronic:978-1-4244-8893-3; POD:978-1-4244-8894-0,10.1109/WSSC.2010.5730280,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5730280,Optronics;automatic surveillance;critical infrastructure protection;harbor protection;infrared sensor;maritime safety;maritime security;maritime surveillance;video analytics,Cameras;Marine vehicles;Meteorology;Object detection;Sensors;Surveillance,Global Positioning System;data acquisition;graphical user interfaces;infrared detectors;marine safety;military equipment;ships;video surveillance,AUTOMATIC SEA VISION;GPS;IR camera;Waquet;automatic object detection;automatic optical software processing;board ships;coastal areas;data acquisition;graphical user interface;inertial navigation unit;maritime safety;maritime security;maritime surveillance;multiple sensors;video analytics,,2,,9,,no,3-5 Nov. 2010,,IEEE,IEEE Conference Publications
Intelligent Sensor Information System For Public Transport å_ To Safely GoåÉ,P. Miller; W. Liu; C. Fowler; H. Zhou; J. Shen; J. Ma; J. Zhang; W. Yan; K. McLaughlin; S. Sezer,"Centre for Secure Inf. Technol., Queen's Univ. Belfast, Belfast, UK",2010 7th IEEE International Conference on Advanced Video and Signal Based Surveillance,20101007,2010,,,533,538,"The Intelligent Sensor Information System (ISIS) is described. ISIS is an active CCTV approach to reducing crime and anti-social behavior on public transport systems such as buses. Key to the system is the idea of event composition, in which directly detected atomic events are combined to infer higher-level events with semantic meaning. Video analytics are described that profile the gender of passengers and track them as they move about a 3-D space. The overall system architecture is described which integrates the on-board event recognition with the control room software over a wireless network to generate a real-time alert. Data from preliminary data-gathering trial is presented.",,Electronic:978-1-4244-8311-2; POD:978-1-4244-8310-5,10.1109/AVSS.2010.36,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5597103,,Cameras;Driver circuits;Face;Security;Software;Streaming media;Surveillance,closed circuit television;image recognition;intelligent sensors;radio access networks;traffic information systems;transportation,active CCTV approach;atomic events;buses;control room software;event recognition;intelligent sensor information system;public transport systems;video analytics;wireless network,,1,,10,,no,Aug. 29 2010-Sept. 1 2010,,IEEE,IEEE Conference Publications
Kalman filtering with multiple nonlinear-linear mixing state constraints,X. Fu; Y. Jia; J. Du; F. Yu,"Seventh Research Division and the Department of Systems and Control, Beihang University (BUAA), Beijing 100191, China",49th IEEE Conference on Decision and Control (CDC),20110222,2010,,,340,345,"This paper is devoted to the problem of state estimation for a class of dynamic system with multiple nonlinear-linear (NL) mixing state constraints. A computational algorithm was developed by C. Yang and E. Blasch to incorporate a positive definite quadratic form equality constraint into the Kalman filtering for the ground vehicle tracking. However, when the target tracked in multidimensional space, a single positive definite quadratic form constraint is not enough due to the involvement of other surface such as hyperboloid, paraboloid, or even plane. A analytic method of incorporating multiple nonlinear-linear mixing state constraints into the Kalman filter is presented here, and a sufficient condition is obtained that guarantee the existence of solution. The constraints may be time varying. At each time step the unconstrained Kalman filter solution is projected onto the state constraints surface. This significantly improves the prediction accuracy of the filter. The use of this algorithm is demonstrated on a nonlinear-linear mixing spacecraft positioning problem.",0191-2216;01912216,Electronic:978-1-4244-7746-3; POD:978-1-4244-7745-6,10.1109/CDC.2010.5718030,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5718030,State estimation;nonlinear-linear mixing constraints;spacecraft positioning,Covariance matrix;Kalman filters;Noise;Optimization;State estimation;Symmetric matrices,Kalman filters;nonlinear dynamical systems;space vehicles;state estimation;tracking,Kalman filtering;dynamic system;ground vehicle tracking;multiple nonlinear-linear mixing state constraints;nonlinear-linear mixing spacecraft positioning problem;quadratic form equality constraint;state estimation,,1,,13,,no,15-17 Dec. 2010,,IEEE,IEEE Conference Publications
Kinematic Analysis of a Novel 4-UPS-UPU PCMM,X. Chen; Y. Deng; G. Hu; J. Yin,"Dept. of Mech. & Electron. Eng., Shandong Univ. of Sci. & Technol., Qingdao, China",2010 International Conference on Measuring Technology and Mechatronics Automation,20100506,2010,2,,550,553,"A novel 4-UPS-UPU PCMM (parallel coordinate measuring machine) with three translations and two rotations is proposed, and its kinematic analysis is studied systematically. First, the structure characteristics of the 4-UPS-UPU PCMM is analyzed. Second, a new solution for position analysis of 4-UPS-UPU PCMM, which without the construction of mathematical model, is put forward. Third, The curves of the length of actuating limbs respecting with motion of pose was obtained by Matlab software, the curves of velocity and acceleration were generated from the curve of the actuating limbs. The analytic results are verified by its simulation mechanism.",2157-1473;21571473,Electronic:978-1-4244-5739-7; POD:978-1-4244-5001-5,10.1109/ICMTMA.2010.240,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5460237,PCMM;mathematical model;positon analysis;simulation mechanism,Acceleration;Analytical models;Automation;Coordinate measuring machines;Kinematics;Mathematical model;Mechanical variables measurement;Mechatronics;Rotation measurement;Structural engineering,control engineering computing;coordinate measuring machines;robot kinematics,4-UPS-UPU PCMM;Matlab software;actuating limbs;kinematic analysis;mathematical model;parallel coordinate measuring machine;structure characteristics,,0,,16,,no,13-14 March 2010,,IEEE,IEEE Conference Publications
Log-Based Reliability Analysis of Software as a Service (SaaS),S. Banerjee; H. Srikanth; B. Cukic,"Lane Dept. of Comput. Sci. & Electr. Eng., West Virginia Univ., Morgantown, WV, USA",2010 IEEE 21st International Symposium on Software Reliability Engineering,20101111,2010,,,239,248,"Software as a Service (SaaS) has gained momentum in the past few years and businesses have been increasingly moving to SaaS model for their IT solutions. SaaS is a newer and transformed model where software is delivered to customers as a service over the web. With the SaaS model, there is a need for service providers to ensure that the services are available and reliable for end users at all times, which introduces significant pressure on the service provider to ensure right test processes and methodologies to minimize any impact to the provisions in Service Level Agreements (SLA). There is lack of research on the unique approaches to reliability analysis of SaaS suites. In this paper, we expand traditional approaches to reliability analysis of traditional web servers and propose methods tailored towards assessing the workload and reliability of SaaS applications. In addition we show the importance of data filtration when assessing SaaS reliability from log files. Finally, we discuss the suitability of reliability measures with respect to their relevance in the context of SLAs.",1071-9458;10719458,Electronic:978-0-7695-4255-3; POD:978-1-4244-9056-1,10.1109/ISSRE.2010.46,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635046,Software Analytics;Software Reliability;Software as a Service,Navigation;Organizations;Software reliability;Usability;Web server,Internet;software architecture;software reliability,Web;data filtration;log based reliability analysis;service level agreements;software as a service,,7,,27,,no,1-4 Nov. 2010,,IEEE,IEEE Conference Publications
Logistics system performance evaluation of the fresh and live agricultural products with an application of analytic network process,Y. Wang; L. Yang; X. Huang; Y. Wang; T. Lu,"School of Economics and Management, Beijing Jiaotong University, Beijing 100044",2010 International Conference on Logistics Systems and Intelligent Management (ICLSIM),20100506,2010,1,,130,134,"In the first place, this thesis, based on an analysis of the features of the logistics system of the fresh and live agriculture products, the affecting factors of its performance and influencing mechanism, considering the specificity of the whole fresh and live agriculture products logistics system, constructs an indicating system on the evaluation of logistics system performance of the fresh and live agriculture products. Secondly, based on an analysis of the applicability of the analytic network process, this paper, analyses the dependent network process relationship between different indicators in the evaluation system of performance of the fresh and live agriculture products logistics system establishes a network construction of the fresh and live agriculture products and confirms partial weight and overall weight. Practically, through a survey of the relative departments and professors in Chang Tai in Fujian province, this paper, with an empirical analysis of the software Super Decision, raised some managerial suggestions which can the improve the performance.",,Electronic:978-1-4244-7330-4; POD:978-1-4244-7331-1,10.1109/ICLSIM.2010.5461452,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5461452,Analytic Network Process;Fresh and Live Agriculture Products;Logistics System Performance,Agricultural products;Agriculture;Logistics;Marketing and sales;Performance analysis;Procurement;Product safety;System performance;Testing;Transportation,agricultural products;agriculture;decision making;logistics,Super Decision software;analytic network process;fresh agricultural product;live agricultural product;logistics system performance evaluation;network construction;network process relationship;overall weight;partial weight,,0,,7,,no,9-10 Jan. 2010,,IEEE,IEEE Conference Publications
Measurement-based underwater acoustic physical layer simulation,B. Borowski; D. Duchamp,"Stevens Institute of Technology, Department of Computer Science, Castle Point on Hudson, Hoboken, NJ 07030 USA",OCEANS 2010 MTS/IEEE SEATTLE,20101210,2010,,,1,8,"Simulation is important in the study of underwater networking because of the difficulty and expense of performing experiments. Underwater acoustic propagation is influenced by a wide variety of environmental factors, rendering analytic models complex, inaccurate, or both. Therefore, simulations based on models are of uncertain utility. In contrast, this simulator uses measured impulse response, CTD, noise, and transmission loss data in an effort to more realistically simulate the channel. The application layer generates data packets whose modulated waveforms are ‰ÛÏmixed‰Ûù with the channel's properties and sent to a receiver implemented fully in software, where the simulated bit error rate (BER) is measured. For a shallow time-invariant test channel, this process results in a simulated BER that is, on average, within 3.34% of the true BER.",0197-7385;01977385,Electronic:978-1-4244-4333-8; POD:978-1-4244-4332-1,10.1109/OCEANS.2010.5664378,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5664378,,Computational modeling;Correlation;Frequency shift keying;MATLAB;Noise;Phase shift keying;Receivers,environmental factors;error statistics;rendering (computer graphics);underwater acoustic communication;underwater acoustic propagation;wireless channels,BER;bit error rate;environmental factors;modulated waveforms;rendering analytic models;shallow time-invariant test channel;underwater acoustic propagation;underwater networking,,3,,17,,no,20-23 Sept. 2010,,IEEE,IEEE Conference Publications
Mechanical stress analysis of port crane based on the condition of temperature field,Min Feng; Hanbin Xiao; Zhengyan Zhang; Shiqing Lu,"School of Logistics Engineering, Wuhan University of Technology, China",2010 The 2nd International Conference on Industrial Mechatronics and Automation,20100803,2010,1,,172,176,"This paper systematically study the calculative theory of of large-scale port crane's the temperature field and thermal stress, Use the finite element software ANSYS to mode and analyse the large-scale port crane under the temperature field and temperature stress. Carry out a detailed study to results of the analysis and determine the impact of temperature stress on port crane structure. The analytic results show the temperature impact has a non-negligible effect on the structural strength of the port crane, which is likely to result in the main factors of the local structure's cracking.",,Electronic:978-1-4244-7656-5; POD:978-1-4244-7653-4,10.1109/ICINDMA.2010.5538057,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5538057,port cranes;sunshine radiati;temperature field;temperature stress;thermal analysis,Computer industry;Cranes;Heat transfer;Large-scale systems;Solar heating;Temperature distribution;Thermal conductivity;Thermal loading;Thermal stresses;Wind speed,,,,0,,4,,no,30-31 May 2010,,IEEE,IEEE Conference Publications
Migrating from Per-Job Analysis to Per-Resource Analysis for Tighter Bounds of End-to-End Response Times,M. K. Yoon; C. G. Lee; J. Han,"University of Illinois at Urbana-Champaign, Urbana",IEEE Transactions on Computers,20100524,2010,59,7,933,942,"As the software complexity drastically increases for multiresource real-time systems, industries have great needs for analytically validating real-time behaviors of their complex software systems. Possible candidates for such analytic validations are the end-to-end response time analysis techniques that can analytically find the worst-case response times of real-time transactions over multiple resources. The existing techniques, however, exhibit severe overestimation when real-time transactions visit the same resource multiple times, which we call a multiple visit problem. To address the problem, this paper proposes a novel analysis that completely changes its analysis viewpoint from classical per-job basis-aggregation of per-job response times-to per-resource basis-aggregation of per-resource total delays. Our experiments show that the proposed analysis can find significantly tighter bounds of end-to-end response times compared with the existing per-job-based analysis.",0018-9340;00189340,,10.1109/TC.2009.174,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5342409,Per-resource analysis;controller area network;end-to-end response time analysis;real-time and embedded systems.,Aerospace electronics;Aircraft;Computer industry;Computer science;Control system analysis;Control systems;Delay;Embedded system;Real time systems;Software systems,controller area networks;real-time systems;resource allocation;scheduling;software metrics,classical per-job basis-aggregation;complex software systems;end-to-end response time analysis;multiple visit problem;multiresource real-time systems;per-job analysis;per-job response times;per-resource analysis;per-resource basis-aggregation;per-resource total delays;real-time behaviors;real-time transactions;software complexity;worst-case response times,,3,,19,,no,10-Jul,,IEEE,IEEE Journals & Magazines
Min-Max Based AHP Method for Route Selection in Cognitive Wireless Network,N. Uchida; K. Takahata; X. Zhang; K. Takahata; Y. Shibata,"Fac. of Software & Inf. Sci., Iwate Prefectural Univ., Iwate, Japan",2010 13th International Conference on Network-Based Information Systems,20101115,2010,,,22,27,"Cognitive wireless network is one of efficient wireless transmission methods to solve today's wireless network problems like a lack of wireless radio resource or congestion of wireless bands. However, Cognitive wireless network still has some problems to realize like efficient algorisms, control methods, and technical problems to attain efficient transmission. In this paper, the Min-Max based AHP method for route selection in cognitive wireless network considering is especially discussed. First, at the observation stage, the physical data such as user policy, electric field strength, bit error rate, jitter, latency, packet error rate, and throughput are observed. Then, at the decision stage, AHP (Analytic hierarchy process) is applied for decision making process with those parameters. Finally, the action stage, one of the suitable links and routes is chosen and changed links and networks by the results of the proposal methods. In the simulation, ns2 are used for the computational results to the effectiveness of the suggested transmission methods in cognitive wireless networks.",2157-0418;21570418,Electronic:978-0-7695-4167-9; POD:978-1-4244-8053-1,10.1109/NBiS.2010.90,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635691,AHP;Cognitive Wireless Network;QoS,Ad hoc networks;Bit error rate;Routing;Routing protocols;Throughput;Wireless networks,cognitive radio;decision making;electric fields;error statistics;telecommunication network routing,bit error rate;cognitive wireless networks;decision making process;electric field strength;min-max based analytic hierarchy process method;wireless bands congestion;wireless radio resource;wireless transmission methods,,3,,14,,no,14-16 Sept. 2010,,IEEE,IEEE Conference Publications
Miniaturized Implantable Wireless Sensor System for Realtime Measurement of Well-Being of Fishes,C. Brockmann; V. GroÌÙer; J. Hefer; S. Guttowski; H. Reichl,"Res. Center for Microperipherik Technol., Tech. Univ. Berlin, Berlin, Germany",2010 Fourth International Conference on Sensor Technologies and Applications,20100826,2010,,,453,457,A miniaturized and implantable wireless underwater sensor system for realtime monitoring of vital parameters for sweet water fishes is presented. The wireless communication between the nodes and the gateway is realized by a RF radio link with a datarate of 250 kbps. The sensor nodes are directly implanted under the membrane of the fish and sample eight different sensors every second. With a lifetime of nearly one month with a coin cell battery the system delivers fine granulate data about the well-fare of the fish. Complex analytic-software can use the data for dedicated diagnostics.,,Electronic:978-1-4244-7537-7; POD:978-1-4244-7538-4,10.1109/SENSORCOMM.2010.72,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5558112,underwater networks;vital parameter monitoring;wireless sensor system,Electrical resistance measurement;Electrodes;Sensor systems;Temperature measurement;Temperature sensors;Wireless communication;Wireless sensor networks,aquaculture;radio links;underwater acoustic communication;wireless sensor networks,RF radio link;implantable wireless sensor system;miniaturized wireless underwater sensor system;realtime measurement;sweet water fish,,0,,10,,no,18-25 July 2010,,IEEE,IEEE Conference Publications
Mobile Audience Measurements in User Experience Research,H. Verkasalo,,2010 IEEE Wireless Communication and Networking Conference,20100708,2010,,,1,6,"Holistic, objective and precise data on mobile user behavior and experience are needed in today's product development and marketing activities. This article presents a framework for mobile audience measurements, for collecting data at the point of convergence-devices. The paper compares the presented framework to alternative methods of mobile user research, and identifies the unique advantages of on-device measurements along with the key weaknesses. In addition to elaborating on data collection, the paper addresses the related analytics, presenting adoption modeling and stickiness analysis that complement the data collection processes and deliver practical insights. The insights can be provided to device vendors, application developers and carriers, who can use the insights in product portfolio management, product development, and marketing.",1525-3511;15253511,Electronic:978-1-4244-6398-5; POD:978-1-4244-6396-1,10.1109/WCNC.2010.5506573,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5506573,,Aerospace industry;Application software;Communications Society;Data analysis;Data mining;Electronic mail;Internet;Product development;TV;Technological innovation,marketing;mobile radio;product development,adoption modeling;data collection process;marketing;mobile audience measurements;mobile user behavior;on-device measurements;product development;product portfolio management;stickiness analysis;user experience research,,1,,19,,no,18-21 April 2010,,IEEE,IEEE Conference Publications
Modeling and analysis of software rejuvenation in a server virtualized system,F. Machida; D. S. Kim; K. S. Trivedi,"Service Platforms Research Laboratories, NEC Corporation, Kawasaki, Japan",2010 IEEE Second International Workshop on Software Aging and Rejuvenation,20110303,2010,,,1,6,"As server virtualization is used as an essential software infrastructure of various software services such as cloud computing, availability management of server virtualized system is becoming more significant. Although time-based software rejuvenation is useful to postpone/prevent failures due to software aging in a server virtualized system, the rejuvenation schedules for virtual machine (VM) and virtual machine monitor (VMM) need to be determined in a proper way for the VM availability, since VMM rejuvenation affects VMs running on the VMM. This paper presents analytic models using stochastic reward nets for three time-based rejuvenation techniques of VMM; (i) Cold-VM rejuvenation in which all VMs are shut down before the VMM rejuvenation, (ii) Warm-VM rejuvenation in which all VMs are suspended before the VMM rejuvenation and (iii) Migrate-VM rejuvenation in which all VMs are moved to the other host server during the VMM rejuvenation. We compare the three techniques in terms of steady-state availability and the number of transactions lost in a year. We find the optimal combination of rejuvenation trigger intervals for each rejuvenation technique by a gradient search method. The numerical analysis shows the interesting result that Warm-VM rejuvenation does not always outperform Cold-VM rejuvenation in terms of steady-state availability depending on rejuvenation trigger intervals. Migrate-VM rejuvenation is better than the other two as long as live VM migration rate is large enough and the other host server has a capacity to accept the migrated VM.",,Electronic:978-1-61284-346-9; POD:978-1-61284-344-5,10.1109/WOSAR.2010.5722098,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5722098,Analytic model;optimization;server virtualization;stochastic reward net;time-based software rejuvenation,,gradient methods;software performance evaluation;virtual machines,gradient search method;server virtualization;server virtualized system;software aging;software rejuvenation;software services;steady-state availability;stochastic reward nets;virtual machine;virtual machine monitor,,17,,16,,no,2-2 Nov. 2010,,IEEE,IEEE Conference Publications
Modeling SW to HW task migration for MPSOC performance analysis,I. Bennour; D. Sebai; A. Jemai,"E&#191;E laboratory, Faculty of sciences at Monastir, Tunisia",5th International Conference on Design & Technology of Integrated Systems in Nanoscale Era,20100617,2010,,,1,6,"Codesign choices of a system differ in terms of different hardware/software partitions, different types of architectural components, different communication architectures, etc. This paper presents an analytic method to estimate the gain on a system throughput when a software task is selected to be moved to hardware during the codesign process. The method is based on formal transformations of a Synchronous Data Flow Graph that models the application as well as its mapping to architecture. The proposed method is applied to the MJPEG decoder using the predictable MPSOC design tool SDF<sup>3</sup>.",,Electronic:978-1-4244-6340-4; POD:978-1-4244-6338-1,10.1109/DTIS.2010.5487581,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5487581,MPSOC;SW to HW task migration;Synchronous DataFlow;performance estimation;predictable design flow,Analytical models;Computer architecture;Decoding;Equations;Flow graphs;Hardware;Laboratories;Mathematical model;Performance analysis;Throughput,data flow graphs;hardware-software codesign;multiprocessing systems;system-on-chip,MPSOC performance analysis;SW to HW task migration;architectural components;codesign choices;codesign process;communication architectures;hardware/software partitions;software task;synchronous data flow graph,,0,,10,,no,23-25 March 2010,,IEEE,IEEE Conference Publications
Multi-core Video Analytics Engine (MVE) for security and surveillance applications,C. Rekeczky; A. Zarandy; P. Foldesy,"Eutecus Inc, Berkeley, CA, USA",2010 12th International Workshop on Cellular Nanoscale Networks and their Applications (CNNA 2010),20100329,2010,,,1,1,"The Multi-core Video Analytics Engine (MVE??) is an easily configurable, compact, high-performance processing architecture that can be used to implement complete video analytics solutions in a single FPGA embedded in intelligent surveillance cameras. With the steadily growing demand for increased processing power at lower costs for video analytics systems, especially intelligent cameras with embedded Digital Signal Processors (DSPs), traditional -software-only approaches are breaking down under the heavy burden of sheer computational complexity. MVE addresses this problem right at the heart, combining an inherently parallel multi-core processing architecture with embedded complex video analytics algorithms in a configurable SoC (System-on-Chip) solution. This initial commercial implementation of MVE is on the Xilinx Spartan-3A DSP 3400A FPGA chip. MVE is based on Eutecus' Cellular Multi-core Video Analytics (C-MVA??) processor - containing specialized image processing IP cores - which has been developed based on substantial research into cellular architectures that mimics the processing found in human vision. The performance of the MVE?? will be demonstrated in both indoor and outdoor security and surveillance applications.",2165-0144;21650144,Electronic:978-1-4244-6680-1; POD:978-1-4244-6679-5,10.1109/CNNA.2010.5430331,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5430331,,Computational intelligence;Costs;Digital cameras;Engines;Field programmable gate arrays;Intelligent systems;Security;Signal analysis;Smart cameras;Surveillance,field programmable gate arrays;image sensors;multiprocessing systems;parallel architectures;system-on-chip;video signal processing;video surveillance,Eutecus cellular multicore video analytics processor;FPGA;MVE;Xilinx Spartan-3A DSP 3400A FPGA chip;computational complexity;configurable SoC;embedded complex video analytics algorithms;embedded digital signal processors;human vision;image processing;indoor security;intelligent surveillance cameras;multicore video analytics engine;outdoor security;parallel multicore processing architecture;system-on-chip,,0,,,,no,3-5 Feb. 2010,,IEEE,IEEE Conference Publications
Multidimensional data dissection using attribute relationship graphs,C. Weaver,School of Computer Science and Center for Spatial Analysis The University of Oklahoma,2010 IEEE Symposium on Visual Analytics Science and Technology,20101210,2010,,,75,82,"Visual exploration and analysis is a process of discovering and dissecting the abundant and complex attribute relationships that pervade multidimensional data. Recent research has identified and characterized patterns of multiple coordinated views, such as cross-filtered views, in which rapid sequences of simple interactions can be used to express queries on subsets of attribute values. In visualizations designed around these patterns, for the most part, distinct views serve to visually isolate each attribute from the others. Although the brush-and-click simplicity of visual isolation facilitates discovery of many-to-many relationships between attributes, dissecting these relationships into more fine-grained one-to-many relationships is interactively tedious and, worse, visually fragmented over prolonged sequences of queries. This paper describes: (1) a method for interactively dissecting multidimensional data by iteratively slicing and manipulating a multigraph representation of data values and value co-occurrences; and (2) design strategies for extending the construction of coordinated multiple view interfaces for dissection as well as discovery of attribute relationships in multidimensional data sets. Using examples from different domains, we describe how attribute relationship graphs can be combined with cross-filtered views, modularized for reuse across designs, and integrated into broader visual analysis tools. The exploratory and analytic utility of these examples suggests that an attribute relationship graph would be a useful addition to a wide variety of visual analysis tools.",,Electronic:978-1-4244-9487-3; POD:978-1-4244-9488-0,10.1109/VAST.2010.5652520,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5652520,D.2.2 [Software Engineering]: Design Tools and Techniques-[User Interfaces];H.2.3 [Information Systems]: Database Management-[Languages];H.5.2 [Information Systems]: Information Interfaces and Presentation-[User Interfaces],Awards activities;Data visualization;Encoding;Layout;Motion pictures;Pipelines;Visualization,data visualisation;graph theory;program slicing;query processing,attribute relationship graphs;design strategies;manipulating;multidimensional data dissection;multigraph representation;queries;slicing;visual analysis tools;visual exploration,,4,1,37,,no,25-26 Oct. 2010,,IEEE,IEEE Conference Publications
Notice of Retraction<BR>A study on chemical process control and optimization system development based on web services,Zhihui Wu; Lei Li; Ming Pan; Y. Qian,"State Key Laboratory of Pulp & Paper Engineering, South China University of Technology Guangzhou, China",2010 IEEE International Conference on Advanced Management Science(ICAMS 2010),20100823,2010,1,,504,508,"Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR>In process system engineering field, plant-wide optimization becomes an important research issue in relation to model based control strategy and software aided solution integration. A platform with the function of simulation, data analysis, and fault diagnosis for operation system optimal control is proposed. The function of five levels in the proposed platform is discussed. Web services technology is employed to integration the different process analytic software. With the key web services application, the dynamic integration and optimization control for chemical product development are realized. Finally, a case study is carried out on TE process to verify the proposed platform performance.",,Electronic:978-1-4244-6932-1; POD:978-1-4244-6931-4,10.1109/ICAMS.2010.5553104,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5553104,dynamic optimization;process control;system development;web service,Artificial neural networks;Computer languages;Engines;Ice;Monitoring;Optimization;Process control,Web services;chemical industry;data analysis;fault diagnosis;optimisation;process control;production engineering computing,TE process;Web services;chemical process control;data analysis;fault diagnosis;model based control strategy;optimization system development;process system engineering field;software aided solution integration,,0,,9,,no,9-11 July 2010,,IEEE,IEEE Conference Publications
Notice of Retraction<BR>Currency Structure Optimization in China's External Debt Management,X. Bai; L. You,"Sch. of Econ. & Manage., Beihang Univ., Beijing, China",2010 Asia-Pacific Power and Energy Engineering Conference,20100415,2010,,,1,4,"Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR>Unceasing deepening along with the financial liberalization and integration of the world, the scale and speed of international capital flowing are also enlarging, which becomes a reduced factor to result in the regional economical and financial crises. Therefore, introducing the foreign capital with certain scale and structure safely and effectively to debtor countries appears very important. On the assumption that the debtor country only borrows 5 kinds of money at the beginning of the loan period, i.e., U.S. dollar, Japanese yen, HK. dollar, pound and euro and with the loan period of five years, we set up the multi-objective linear programming model of the optimal currency structure. Then Matlab mathematical software is used to solve the model. Finally, the analytic conclusions are given.",2157-4839;21574839,Electronic:978-1-4244-4813-5; POD:978-1-4244-4812-8,10.1109/APPEEC.2010.5448814,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5448814,,Acceleration;Costs;Crisis management;Economic indicators;Financial management;Linear programming;Mathematical model;Risk management;Safety;Security,foreign exchange trading;linear programming;mathematics computing,China external debt management;Matlab mathematical software;currency structure optimization;financial crises;financial liberalization;foreign capital;multiobjective linear programming model;optimal currency structure;regional economical crises,,0,,7,,no,28-31 March 2010,,IEEE,IEEE Conference Publications
Notice of Retraction<BR>Designing and formulating organization performance evaluation model in AHP method based on EFQM criteria (case study),S. Iranzadeh; F. Chakherlouy,"Islamic Azad university,Tabriz Branch, Iran",2010 IEEE International Conference on Advanced Management Science(ICAMS 2010),20100823,2010,1,,606,609,"Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR>Designing and formulating a comprehensive organization performance evaluation model based on European Foundation for Quality Management (EFQM) in AHP (Analytic Hierarchy Process) method is the main aim of the present research study. Evaluation is considered as one of the most important activities in each organization in a way that reformation of processes and procedures of doing activity without evaluation of results will be impossible. At the present research activity, AHP (Analytic Hierarchy Process) has been used as one of MADM (Multi-Attribute Decision Making) methods for the evaluation of performance of organizations through the application of EFQM (European Foundation for Quality Management) excellence model criteria. Also, Municipality of City of Tabriz has been selected as subjects for testing the presented model. In the same direction, seven districts of this municipality were selected as sample model. Necessary and required information were accumulated through questionnaire, interview and also taking advantage of data and library resources, details of which were analyzed and studied through the application of advanced Excel and Expert Choice 11.5 software package system. Eventually, various districts of this organization were evaluated and prioritized in terms of performance.",,Electronic:978-1-4244-6932-1; POD:978-1-4244-6931-4,10.1109/ICAMS.2010.5553098,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5553098,Analytic Hierarchy Process (AHP);EFQM (European Foundation for Quality Management);Evaluation of Performance,Analytical models;Europe;Films;Integrated circuits;Lead,decision making;organisational aspects;quality management,European Foundation for quality management;Excel;Expert Choice 11.5 software package system;Tabriz;analytic hierarchy process method;excellence model criteria;multiattribute decision making method;organization performance evaluation model,,0,,8,,no,9-11 July 2010,,IEEE,IEEE Conference Publications
Notice of Retraction<BR>Study of the distribution of initial stress for mechanical properties of guyed transmission tower,Fenglin Gan; Xiaolei Li,"School of Civil Engineering, Northeast Dianli University, Jilin, China",2010 International Conference on Computer Application and System Modeling (ICCASM 2010),20101104,2010,6,,V6-144,V6-147,"Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR>With the purpose of studying the mechanical properties of guyed transmission tower which is effected by initial stress of guy. The paper takes a guyed transmission tower as an example, building corresponding nonlinear finite element model by using general finite analytic software ANSYS, making comparison on mechanical properties of transmission tower structure which under different states of initial stress of guy, summing up the law which between the distribution of initial stress and the mechanical properties of the structure, providing the reasonable value for the design and constriction of tower structure, and ensuring self-reliance and stability of transmission tower structure.",2161-9069;21619069,Electronic:978-1-4244-7237-6; POD:978-1-4244-7235-2,10.1109/ICCASM.2010.5619290,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5619290,guyed transmission tower;initial stress of guy;mechanical properties;nonlinear finite element,Educational institutions;Industries;Mechanical factors;Poles and towers;Steel,elastic constants;finite element analysis;flexible structures;internal stresses;poles and towers;power transmission;structural engineering,ANSYS finite analytic software;guyed transmission tower;initial stress distribution;mechanical property;nonlinear finite element model,,0,,9,,no,22-24 Oct. 2010,,IEEE,IEEE Conference Publications
Notice of Retraction<BR>University network security risk assessment based On fuzzy analytic hierarchy process,Zhu Jundong; Liu Li,"Information Center, North China Coal Medical University, TangShan, China",2010 International Conference on Computer Application and System Modeling (ICCASM 2010),20101104,2010,9,,V9-213,V9-217,"Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR>In this paper, university network security evaluation system based on fuzzy AHP evaluation model, with examples given using the evaluation model for network security assessment procedures and methods. Evaluation results show that the proposed approach is feasible and guidance.",2161-9069;21619069,Electronic:978-1-4244-7237-6; POD:978-1-4244-7235-2,10.1109/ICCASM.2010.5623049,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5623049,FAHP;evaluation index system;fuzzy comprehensive evaluation;network security,Computer applications;Educational institutions;Indexes;Modeling;Safety;Security;Software measurement,computer network security;decision making;educational administrative data processing;risk management,fuzzy analytic hierarchy process;university network security evaluation system;university network security risk assessment,,0,,17,,no,22-24 Oct. 2010,,IEEE,IEEE Conference Publications
On stability of KIII model based on nonlinear dynamics,S. Zhu; J. Zhang; N. Wang; R. Wang,"School of Software, Hunan University, Changsha 410082, China",Proceedings of the 29th Chinese Control Conference,20100920,2010,,,2442,2446,"As a bionic system simulating biologic olfactory structure and characteristics, KIII model is different from traditional artificial neural networks on pattern recognition. But there is no quantificational index to judge the stability of KIII model. Based on nonlinear dynamics index, the problem is researched in this paper and a quantificational index, Lyapunov exponent, is used to judge the stability of KIII model. Three stages in pattern recognition process of KIII model are analyzed quantificationally using wolf method. The calculational results show that KIII model can change from chaotic stage to stable stage quickly and presents obvious synchronization stage, which is consistent with the analytic result drawn from phase graph. It is also shown that Lyapunov exponent is an effective method to judge the stability of KIII model.",1934-1768;19341768,Electronic:978-7-8946-3104-6; POD:978-1-4244-6263-6,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5572225,KIII Model;Lyapunov Exponent;Pattern Recognition;Stability;Synchronization,Analytical models;Biological system modeling;Chaos;Feature extraction;Olfactory;Stability criteria,Lyapunov methods;biocybernetics;neural nets;nonlinear dynamical systems;pattern recognition;stability,KIII model;Lyapunov exponent;artificial neural networks;biologic olfactory characteristic;biologic olfactory structure;bionic system;chaotic stage;nonlinear dynamics index;pattern recognition;phase graph;quantificational index;stability;synchronization stage;wolf method,,0,,14,,no,29-31 July 2010,,IEEE,IEEE Conference Publications
On the detection probability of parallel code phase search algorithms in GPS receivers,B. C. Geiger; M. Soudan; C. Vogel,"Signal Processing and Speech Communication Laboratory, Graz University of Technology, Austria","21st Annual IEEE International Symposium on Personal, Indoor and Mobile Radio Communications",20101217,2010,,,865,870,"The first stage of the signal processing chain in a Global Positioning System (GPS) receiver is the acquisition, which provides for a desired satellite coarse code phase and Doppler frequency estimates to subsequent stages. Thus, acquisition is a two-dimensional search, implemented as demodulation and non-coherent correlation. For a certain Doppler estimate, software-defined GPS receivers typically compute the correlation for all time lags in parallel. One way to detect the presence of a signal is by comparing the ratio between the largest and the second largest correlation peak against a threshold. For this type of receivers, the detection and false alarm probabilities are derived. Interestingly, the false alarm probability is independent of the noise power spectral density, which allows a fixed threshold setting. The analytic results are verified by a series of simulations.",2166-9570;21669570,Electronic:978-1-4244-8016-6; POD:978-1-4244-8017-3,10.1109/PIMRC.2010.5672040,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5672040,,Correlation;Doppler effect;Global Positioning System;Measurement;Noise;Receivers;Satellites,Doppler effect;Global Positioning System;demodulation;probability;signal detection;software radio,Doppler frequency;Global Positioning System;correlation peak;demodulation;detection probability;false alarm probability;noise power spectral density;noncoherent correlation;parallel code phase search algorithm;satellite coarse code phase;signal processing chain;software-defined GPS receiver,,6,,20,,no,26-30 Sept. 2010,,IEEE,IEEE Conference Publications
On the Processor Sharing Properties of File Transfers in a WLAN Testbed,G. Hoekstra; R. v. d. Mei,"Centre for Math. & Comput. Sci. (CWI), Amsterdam, Netherlands",2010 2nd International Conference on Evolving Internet,20101101,2010,,,36,41,"802.11-based WLAN deployments have become a commodity to provide today's wireless Internet access. In this paper, we conduct a practical study on the performance of FTP file transfers over real WLAN equipment. To this end, we propose a new analytic model that translates the highly complex dynamics of the FTP/TCP/IP/MAC-stack, and their interactions, into a single parameter, which will be called the effective load. The effective load is used to describe the flow-level behavior of FTP-based file transfers over WLANs without admission control as a Processor-Sharing (PS)-model. Next, despite the fact that PS models are heavily used in modeling flow-level performance in WLAN networks, an extensive validation of such models with real equipment has not been conducted. Motivated by this, we validate in the present paper our analytic model by comparing the model-based response times against the outcomes obtained from a testbed environment. The results show (a) that the obtained mean download times are fairly insensitive to the file-size distribution, as suggested by the PS-model, and (b) that the model leads to accurate predictions over a broad range of parameters combinations, including different file-size distributions and light- and heavy-load scenarios.",2156-7190;21567190,Electronic:978-0-7695-4185-3; POD:978-1-4244-8150-7,10.1109/INTERNET.2010.16,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5615513,Performance Modeling;Processor-Sharing queues;Wireless LAN,Analytical models;IP networks;Load modeling;Mathematical model;Protocols;Servers;Wireless LAN,Internet;computer communications software;radio access networks;transport protocols;wireless LAN,802.11-based WLAN deployment;FTP file transfer;FTP stack;MAC stack;PS-model;TCP/IP stack;WLAN testbed;admission control;complex dynamics;effective load;file size distribution;flow level behavior;model based response time;parameter combination;processor sharing property;wireless Internet access,,1,,14,,no,20-25 Sept. 2010,,IEEE,IEEE Conference Publications
Optimal Preventive Maintenance Scheduling in Semiconductor Manufacturing Systems: Software Tool and Simulation Case Studies,J. A. Ramirez-Hernandez; J. Crabtree; X. Yao; E. Fernandez; M. C. Fu; M. Janakiram; S. I. Marcus; M. O'Connor; N. Patel,"Department of Electrical and Computer Engineering, University of Cincinnati, Cincinnati, OH, USA",IEEE Transactions on Semiconductor Manufacturing,20100803,2010,23,3,477,489,"This paper presents the architecture and implementation of a preventive maintenance optimization software tool (PMOST), based on algorithms for the optimal scheduling of preventive maintenance (PM) tasks in semiconductor manufacturing operations. We also present results from four complex simulation case studies, based on real industrial data and employing full fab models, to illustrate the use, data needs and outcomes produced by PMOST. These results demonstrate significant improvements in tool production and consolidation of PM tasks. We give a description of the different software modules that compose PMOST, to provide guidelines as well as a template for other implementations of the PM optimization algorithms utilized by PMOST.",0894-6507;08946507,,10.1109/TSM.2010.2051731,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5475204,Optimal preventive maintenance (PM) scheduling;simulation case studies;software tool,,optimisation;preventive maintenance;production engineering computing;scheduling;semiconductor device manufacture;software tools,PM optimization algorithms;PMOST;optimal preventive maintenance scheduling;optimal scheduling;semiconductor manufacturing operations;semiconductor manufacturing systems;software modules;software tool,,8,,22,,no,Aug. 2010,,IEEE,IEEE Journals & Magazines
Performance Testing: Far from Steady State,R. Mansharamani; A. Khanapurkar; B. Mathew; R. Subramanyan,"Performance Eng. Res. Centre, Tata Consultancy Services, Mumbai, India",2010 IEEE 34th Annual Computer Software and Applications Conference Workshops,20101101,2010,,,341,346,"The dot com era ushered in a number of industry standard load testing tools. While there is no doubt that these tools have helped improve the quality of IT systems, performance testing in the IT industry is far from steady state. There are still severe gaps between performance test results and production systems performance in IT projects. This paper proposes a number of areas where performance testing needs to improve radically, several of which can be incorporated in to load testing tools. Examples are also provided of simple analytics during single user performance testing to demonstrate the effectiveness of this extra but necessary step in the testing process.",,Electronic:978-0-7695-4105-1; POD:978-1-4244-8089-0,10.1109/COMPSACW.2010.66,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5614034,load testing tools;performance emulation;performance testing;think time variability,Databases;Extrapolation;Industries;Testing;Throughput;Time factors;Tuning,DP industry;Internet;electronic commerce;performance evaluation;testing,IT system quality;industry standard load testing tools;production systems;single user performance testing,,1,,7,,no,19-23 July 2010,,IEEE,IEEE Conference Publications
Periscopic visualizes symptomatology of pandemic: VAST 2010 Mini Challenge 2 award: Effective visualization of symptoms,K. Rees,Periscopic,2010 IEEE Symposium on Visual Analytics Science and Technology,20101210,2010,,,283,284,"By using affordable, off-the-shelf software and novel visualization methods, characterizing the spread of a pandemic was achieved for the VAST Challenge. Dashboards of the presenting symptomatology, anomalies, and death records all contributed to creating ways for health officials better detect and analyze the spread of the disease.",,Electronic:978-1-4244-9487-3; POD:978-1-4244-9488-0,10.1109/VAST.2010.5652678,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5652678,data mining;data sharing;medical information systems;visual analytics,,data analysis;data mining;data visualisation;diseases;health care;medical administrative data processing,VAST Challenge;anomalies;dashboards;death records;disease;health officials;off-the-shelf software;pandemic spread;periscopic visualizes;symptomatology,,0,,3,,no,25-26 Oct. 2010,,IEEE,IEEE Conference Publications
Predictive Analytics Using a Blackboard-Based Reasoning Agent,J. Yue; A. Raja; W. Ribarsky,"Dept. of Software & Inf. Syst., Univ. of North Carolina at Charlotte, Charlotte, NC, USA",2010 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology,20101101,2010,2,,97,100,"Significant increase in collected data for investigative tasks and the increased complexity of the reasoning process itself have made investigative analytical tasks more challenging. These tasks are time critical and typically involve identifying and tracking multiple hypotheses; gathering evidence to validate the correct hypotheses and eliminating the incorrect ones. In this paper we specifically address predictive tasks that are concerned with predicting future trends. We describe RESIN, an AI blackboard-based agent that leverages interactive visualizations and mixed-initiative problem solving to enable analysts to explore and pre-process large amounts of data in order to perform predictive analytics. Our empirical evaluation discusses the advantages and challenges of predictive analytics in a complex domain like intelligence analysis.",,Electronic:978-0-7695-4191-4; POD:978-1-4244-8482-9,10.1109/WI-IAT.2010.155,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5616323,MDP;mixed-initiative system;predicitve analytics;visualization,,artificial intelligence;blackboard architecture;data handling;inference mechanisms;multi-agent systems,AI blackboard-based agent;RESIN;blackboard-based reasoning agent;interactive visualization;mixed-initiative problem solving;predictive analytics,,1,,10,,no,Aug. 31 2010-Sept. 3 2010,,IEEE,IEEE Conference Publications
Priority Queueing with Finite Buffer Size and Randomized Push-Out Mechanism,V. Zaborovsky; O. Zayats; V. Mulukha,"Petersburg State Polytech. Univ., St. Petersburg, Russia",2010 Ninth International Conference on Networks,20100601,2010,,,316,320,The non-preemptive priority queuing with a finite buffer is considered. We introduce a randomized push-out buffer management mechanism which allows to control very efficiently the loss probability of priority packets. The packet loss probabilities for priority and non-priority traffic are calculated using the generating function approach. For the particular case of the standard non-randomized push-out scheme we obtain explicit analytic expressions. The theoretical results are illustrated by a numerical example.,,Electronic:978-1-4244-6084-7; POD:978-1-4244-6083-0,10.1109/ICN.2010.56,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5473996,finite budder;priority queueing;randomized push-out;traffic;transport protocols,Application software;Computer networks;Equations;Exponential distribution;Mathematical model;Probability;Telematics;Throughput;Traffic control;Transport protocols,probability;queueing theory;telecommunication traffic;transport protocols,finite buffer size;generating function approach;nonpreemptive priority queuing;nonpriority traffic;priority packet loss probability;priority traffic;randomized push-out buffer management mechanism;transport protocol,,5,,9,,no,11-16 April 2010,,IEEE,IEEE Conference Publications
ProDV ‰ÛÓ A case study in delivering visual analytics,D. Overby; J. Keyser; J. Wall,"Department of Computer Science and Engineering, Texas A&M University",2010 IEEE Symposium on Visual Analytics Science and Technology,20101210,2010,,,247,248,"We present a custom visual analytics system developed in conjunction with the test and evaluation community of the US Army. We designed and implemented a visual programming environment for configuring a variety of interactive visual analysis capabilities. Our abstraction of the visualization process is based on insights gained from interviews conducted with expert users. We show that this model allowed analysts to implement multiple visual analysis capabilities for network performance, anomalous sensor activity, and engagement results. Long-term interaction with expert users led to development of several custom visual analysis techniques. We have conducted training sessions with expert users, and are working to evaluate the success of our work based on performance metrics captured in a semi-automated fashion during these training sessions. We have also integrated collaborative analysis features such as annotations and shared content.",,Electronic:978-1-4244-9487-3; POD:978-1-4244-9488-0,10.1109/VAST.2010.5650219,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5650219,Visualization system and toolkit design,Analytical models;Communities;Data models;Data visualization;Training;Visual analytics,data analysis;data visualisation;interactive systems;software architecture;user interfaces;visual programming,ProDV;collaborative analysis;performance metrics;visual analytics system;visual programming,,0,,4,,no,25-26 Oct. 2010,,IEEE,IEEE Conference Publications
Proposal of Transmission Control Methods with Multihopped Environments in Cognitive Wireless Networks,N. Uchida; K. Takahata; Y. Shibata,"Fac. of Software & Inf. Sci., Iwate Prefectural Univ., Takizawa, Japan",2010 IEEE 24th International Conference on Advanced Information Networking and Applications Workshops,20100607,2010,,,127,132,"Remarkable wireless networks technology developments have made us to expect the realization of new applications like the advanced traffic system, the disaster prevention system, and the adhoc network system. However the resources of wireless bandwidths are not enough to use for such new applications because it is not efficient usage. Therefore, it is necessary to develop with new efficient wireless transmission methods like cognitive wireless network. In this paper, the transmission control methods in cognitive wireless network considering with multihopped environments are discussed. First, at the observation stage, the physical data such as user policy, electric field strength, bit error rate, jitter, latency, packet error rate, and throughput are observed. Then, at the decision stage, extended AHP (Analytic hierarchy process) is applied for decision making process with those parameters, and the suitable routing is decided. Finally, the action stage, one of the suitable link and route are chosen and changed links and networks. In the simulation, ns2 are used for the computational results to the effectiveness of the suggested transmission methods in cognitive wireless networks.",,Electronic:978-1-4244-6702-0; POD:978-1-4244-6701-3,10.1109/WAINA.2010.102,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5480838,AHP;AODV;Cognitive Wireless Network;QoS,Bandwidth;Bit error rate;Communication system traffic control;Decision making;Delay;Error analysis;Jitter;Proposals;Throughput;Wireless networks,cognitive radio;radio networks;telecommunication control;telecommunication network routing,analytic hierarchy process;bit error rate;cognitive wireless networks;decision making process;electric field strength;jitter;multihopped environments;ns2 simulation;packet error rate;transmission control methods;user policy,,0,,14,,no,20-23 April 2010,,IEEE,IEEE Conference Publications
Proposal of Transmission Control Methods with User Oriented Environments in Cognitive Wireless Networks,N. Uchida; Y. Shibata; K. Takahata,"Fac. of Software & Inf. Sci., Iwate Prefectural Univ., Takizawa, Japan","2010 International Conference on Complex, Intelligent and Software Intensive Systems",20100415,2010,,,188,192,"Remarkable wireless networks technology developments have made us to expect the realization of new applications like the advanced traffic system, the disaster prevention system, and the adhoc network system. However the resources of wireless bandwidths are not enough to use for such new applications because it is not efficient usage. Therefore, it is necessary to develop with new efficient wireless transmission methods like cognitive wireless network. In this paper, the transmission control methods in cognitive wireless network considering with cross layers including user policies are discussed. First, at the observation stage, the physical data such as user policy, electric field strength, bit error rate, jitter, latency, packet error rate, and throughput are observed. Then, at the decision stage, AHP (Analytic hierarchy process) is applied for decision making process with those parameters. Finally, the action stage, one of the suitable link is chosen and changed links and networks. In the simulation, ns2 are used for the computational results to the effectiveness of the suggested transmission methods in cognitive wireless networks.",,Electronic:978-1-4244-5918-6; POD:978-1-4244-5917-9,10.1109/CISIS.2010.186,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5447425,AHP;AODV;Cognitive Wireless Network;QoS,Bandwidth;Bit error rate;Communication system traffic control;Cross layer design;Delay;Error analysis;Jitter;Proposals;Throughput;Wireless networks,cognitive radio;telecommunication control,AHP;analytic hierarchy process;cognitive wireless networks;ns2;transmission control;user oriented environments;user policies;wireless transmission methods,,0,,23,,no,15-18 Feb. 2010,,IEEE,IEEE Conference Publications
Quality Attributes Assessment for Feature-Based Product Configuration in Software Product Line,G. Zhang; H. Ye; Y. Lin,"Sch. of Electr. Eng. & Comput. Sci., Univ. of Newcastle, Callaghan, NSW, Australia",2010 Asia Pacific Software Engineering Conference,20110120,2010,,,137,146,"Product configuration based on a feature model in software product lines is the process of selecting the desired features based on customers' requirements. In most cases, application engineers focus on the functionalities of the target product during product configuration process whereas the quality attributes are handled until the final product is produced. However, it is costly to fix the problem if the quality attributes have not been considered in the product configuration stage. The key issue of assessing a quality attribute of a product configuration is to measure the impact on a quality attribute made by the set of functional variable features selected in a configuration. Current existing approaches have several limitations, such as no quantitative measurements provided or requiring existing valid products and heavy human effort for the assessment. To overcome theses limitations, we propose an Analytic Hierarchical Process (AHP) based approach to estimate the relative importance of each functional variable feature on a quality attribute. Based on the relative importance value of each functional variable feature on a quality attribute, the level of quality attributes of a product configuration in software product lines can be assessed. An illustrative example based on the Computer Aided Dispatch (CAD) software product line is presented to demonstrate how the proposed approach works.",1530-1362;15301362,Electronic:978-0-7695-4266-9; POD:978-1-4244-8831-5,10.1109/APSEC.2010.25,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5693189,Analytic Hierarchical Process (AHP);product configuration;quality attributes assessment,Design automation;Encryption;Estimation;Product development;Software;Solid modeling,configuration management;decision making;product development;software quality;software reusability,analytic hierarchical process;application engineers;computer aided dispatch software product line;customer requirements;feature based product configuration;quality attribute assessment,,1,,13,,no,Nov. 30 2010-Dec. 3 2010,,IEEE,IEEE Conference Publications
Reducing the digital divide of the electronic government of the 921 reconstruction areas in Taiwan,I. M. Huang; K. Fang,"Department of Information Management, National Yunlin University of Science & Technology, Yunlin, Taiwan, R.O.C.",PICMET 2010 TECHNOLOGY MANAGEMENT FOR GLOBAL ECONOMIC GROWTH,20101014,2010,,,1,8,"With great efforts over the past two decades, Taiwan has become one of leading countries in E-government practice. People have benefited from the efficiency of E-government services and Taiwan government will develop the next stage E-government, which is integrated, innovative, real time, interactive, and personalized, to establish a virtually trusted society that connects each citizen via the Internet. At 1:47 a.m. on September 21, 1999, a massive earthquake measuring 7.3 on the Richter scale struck central Taiwan. This earthquake is a terrible disaster that causes tragic loss of life, severe property damage, and sharp decline in living standards and the regional prosperity. The earthquakes of the last decade also brought to light the importance of Earthquake Disaster Management (EDM) operations. As a result, there is an urgent call for applying digital services, broadening geographical service scope, enriching service options, and lowering costs for the reconstruction areas. This study employed the Analytic Hierarchy Process (AHP) to analyze the Taiwan government's supply and demand of the information services in the reconstruction areas to heed the call for reducing uneven opportunities on availability of information and telecommunication technology.",2159-5100;21595100,Electronic:978-1-890843-21-2; POD:978-1-4244-8203-0,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5603448,,Earthquakes;Electronic government;Internet;Software,Internet;disasters;earthquakes;information services,Internet;Taiwan;analytic hierarchy process;e-government practice;earthquake disaster management;electronic government;information services;reconstruction areas,,0,,24,,no,18-22 July 2010,,IEEE,IEEE Conference Publications
Refinement-Friendly Bigraphs and Spygraphs,M. Goldsmith; S. Creese,"WMG Digital Lab., Univ. of Warwick, Coventry, UK",2010 8th IEEE International Conference on Software Engineering and Formal Methods,20101115,2010,,,203,207,"Over the past decade the successful approach to specification and mechanical analysis of correctness and security properties using CSP and its refinement checker FDR has been extended to contexts including mobile ad-hoc networks and pervasive systems. But the more scope for network reconfiguration the system exhibits, the more intricate and less obviously accurate the models must become in order to accommodate such dynamic behaviour in a language with a basically static process and communication graph. Milner's Bigraph framework, on the other hand, and in particular Blackwell's Spygraph specialisation, are ideally suited for describing intuitively such dynamic reconfigurations of a system and support notions of locality and adjacency which fit them well for reasoning, for instance, about the interface between physical and electronic security; but they lack powerful analytic tool support. Our long-term goal is to combine the best of both approaches. Unfortunately the canonical labelled transition system induced by the category-theoretic semantics of a bigraphical reactive system present a number of challenges to the refinement-based approach. Prominent amongst these is the feature that the label on a transition is the 'borrowed context' required to make the redex of some reaction rule appear in the augmented source bigraph; this means that any reaction which can already take place entirely within a given bigraph gives rise to a transition labelled only with the trivial identity context, equivalent to a tau transition in CCS or CSP, with the result that neither the reaction rule nor the agents involved can be distinguished. This makes it quite impossible for an observer of the transition system to determine whether such a reaction was desirable with respect to any specification. We are investigating ways to remedy this situation. Here we present a systematic transformation of a bigraphical reactive system, both its rules and the underlying bigraphs, with the effec- - t that every transition becomes labelled with the specific rule that gave rise to it and the set of agents involved. We also consider how that now possibly over-precise labelling can be restricted through selective hiding and judicious forgetful renaming.",1551-0255;15510255,Electronic:978-0-7695-4153-2; POD:978-1-4244-8289-4,10.1109/SEFM.2010.25,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5637430,bigraphical reactive systems;bigraphs;refinement semantics;transfomation,Algebra;Calculus;Context;Presses;Security;Semantics;Wire,concurrency control;cryptography;formal specification;graph theory;refinement calculus,CSP;augmented source bigraph;bigraphical reactive system;canonical labelled transition system;category-theoretic semantics;communication graph;mobile ad-hoc network;pervasive system;refinement checker FDR;refinement-friendly bigraph;specification analysis;spygraph,,0,,8,,no,13-18 Sept. 2010,,IEEE,IEEE Conference Publications
Research and Realization of WEB Security Auto-Testing Tool Based on AHP,R. Wang; Y. Xu; Y. Xiang,"Sch. of Comput. Eng., Qingdao Technol. Univ., Qingdao, China",2010 International Conference on Computational Intelligence and Software Engineering,20101230,2010,,,1,4,"In the process of software production, testing is the premise to guarantee the quality of software. With the extensive application of network software, Web security test has become a key point that can not neglect. Based on the Analytic Hierarchy Process (AHP) algorithm, a new kind of Web security testing programme was introduced in this paper. According to which it realized the Web Security auto-Testing Tool including Black-box testing and White-box testing. Through the experiments on the B/S mode software platform, its validity was verified.",,Electronic:978-1-4244-5392-4; POD:978-1-4244-5391-7,10.1109/CISE.2010.5676792,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676792,,Algorithm design and analysis;Databases;Maintenance engineering;Security;Software;Sorting;Testing,Internet;decision making;program testing;security of data;software quality,AHP;B/S mode software platform;Web security autotesting tool;analytic hierarchy process algorithm;black-box testing;software production;software quality;white-box testing,,0,,10,,no,10-12 Dec. 2010,,IEEE,IEEE Conference Publications
Research of ship oil spill risk monitoring & evaluation system in Shanghai Yangshan Port water area,Sun Yongming; Han Houde,"Merchant Marine College, Shanghai Maritime University, China",The 6th International Conference on Networked Computing and Advanced Information Management,20100916,2010,,,412,416,"The paper studies on ship oil spill risk monitoring and evaluation of Shanghai Yangshan Port. Through the analysis on general situation of water and transportation, it discusses reasons of ship oil spill accidents happened in Yangshan Port and summarizes the regular pattern. Single-vessel oil spill evaluation module of Yangshan Port was set up using gray analytic hierarchy process and corresponding software tool was developed using Visual Basic computer programming language, providing scientific and swift risk assessment user interface, which made the ship oil spill monitoring and evaluation of Shanghai Yangshan Port quantitative.",,Electronic:978-89-88678-26-8; POD:978-1-4244-7671-8,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5572067,Yangshan Port;gray analytic hierarchy process;monitoring and evaluation;ship oil spill,,Visual BASIC;decision making;environmental science computing;grey systems;marine accidents;marine pollution;oil pollution;risk management;ships;software tools;user interfaces,Shanghai Yangshan Port water area;Visual Basic computer programming language;evaluation system;gray analytic hierarchy process;risk assessment user interface;risk monitoring;ship oil spill accident;ship oil spill monitoring;single-vessel oil spill evaluation module;software tool;transportation,,0,,7,,no,16-18 Aug. 2010,,IEEE,IEEE Conference Publications
Research of the organizational changes model in maritime companies,D. M. Mitrovi€à; N. M. Ralevic,"Faculty for Business Management, Marsala Tita br. 7, 30 300 Bar, Montenegro",IEEE 8th International Symposium on Intelligent Systems and Informatics,20101129,2010,,,519,522,"After theoretical considerations of organizational changes, the research findings were presented in relation to the following: the selection of the optimal model for implementation of organizational changes in the maritime company that is the subject of analysis and establishing of the character of the correlation between the organizational changes, the productivity level and the ship service time (components related to cargo handling operations). Through the implementation of AHP (Analytic Hierarchy Process) method, it has been identified that the optimal model for implementation of organizational changes in the maritime company being the research subject is restructuring.",1949-047X;1949047X,Electronic:978-1-4244-7396-0; POD:978-1-4244-7394-6,10.1109/SISY.2010.5647061,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5647061,AHP method;Maritime company;Organizational changes,Companies;Correlation;Optimization;Productivity;Software;Standards organizations,decision making;marine engineering;organisational aspects;ships;transportation,AHP;maritime companies;organizational changes model;ship service time,,0,,12,,no,10-11 Sept. 2010,,IEEE,IEEE Conference Publications
Research on a Software Trustworthy Measure Model,B. Yu; Q. Wang; Y. Yang,"Sch. of Manage. Sci. & Eng., Shandong Inst. of Bus. & Technol., Yantai, China","2010 Second International Conference on Networks Security, Wireless Communications and Trusted Computing",20100607,2010,2,,518,521,"Through analyzing the factors affecting software trustworthy, established the index system of trustworthy software estimation. Apply Analytic Hierarchy Process (AHP) to determine the relative importance of factors and indicator items affecting software trustworthy, and then determine the score of estimation index system through fuzzy estimation model, use the combination of both to measure software trustworthy. Finally, this paper presents an example to validate the validity and objectivity of this model.",,Electronic:978-1-4244-6598-9; POD:978-1-4244-6597-2,10.1109/NSWCTC.2010.276,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5480976,Fuzzy Estimation;Hierarchical Analysis;Software Trustworthy,Availability;Decision making;Fault tolerant systems;Fuzzy systems;Information security;Real time systems;Software measurement;Software safety;Software systems;Technology management,decision making;fuzzy set theory;security of data;statistical analysis,analytic hierarchy process;fuzzy estimation model;software trustworthy estimation index system;software trustworthy measure model,,3,,6,,no,24-25 April 2010,,IEEE,IEEE Conference Publications
Research on e-government security risk assessment based on improved D-S evidence theory and entropy weight AHP,Xinlan Zhang; Xin zhang,"School of Economics and Management, China University of Geosciences, Wuhan, China","2010 International Conference on Computer, Mechatronics, Control and Electronic Engineering",20101025,2010,1,,93,96,"With the application of e-government becomes more and more extensive, the security issues become increasingly important. The paper firstly presents a relatively complete e-government security risk evaluation index system basing on comprehensive analysis of risk factors which affects e-government security. Then we conduct orthogonal transformation to the indexes for eliminating duplication between them. After that, we combine entropy weight coefficient method and AHP to determine the weight of risk factor index, and the improved D_S evidence theory is introduced to deal with the uncertainty in risk assessment. Finally, an example is cited to verify the validity of this method, compared with the result of fuzzy comprehensive evaluation, we can see the effectiveness of this method in e-government security risk assessment.",2159-6026;21596026,Electronic:978-1-4244-7958-0; POD:978-1-4244-7957-3,10.1109/CMCE.2010.5610536,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5610536,D-S evidence theory;E-government;analytical hierarchy process;entropy weight coefficient;index system;orthogonal transformation;risk assessment,Security;Software,case-based reasoning;decision making;entropy;fuzzy set theory;government data processing;indexing;risk management;security of data,D-S evidence theory;analytic hierarchy process;e-government security risk assessment;e-government security risk evaluation index system;entropy weight coefficient;fuzzy comprehensive evaluation;orthogonal transformation;risk factor index;security issue,,1,,8,,no,24-26 Aug. 2010,,IEEE,IEEE Conference Publications
Research on Online Static Risk Assessment for Urban Power System,R. Liu; J. Zhang; W. Qiu; L. Su; Z. Guo; G. Wang,"Electr. Eng. Dept., North China Electr. Power Univ., Beijing, China",2010 Asia-Pacific Power and Energy Engineering Conference,20100415,2010,,,1,4,"With the rapid development of urbanization, the importance of city power grids safety has been gradually recognized. Given a full consideration for the characteristics of city power grids, we design a complete set of risk evaluation index system based on probability theory by employing risk theory and analytic hierarchy process (AHP) in power system online static security risk assessment. Further, we have developed city grid online risk assessment software, and have provided a clear description of some key technologies of implementation and calculation process after installation. Finally, the test result shows the functionality and applicability of our software.",2157-4839;21574839,Electronic:978-1-4244-4813-5; POD:978-1-4244-4812-8,10.1109/APPEEC.2010.5448732,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5448732,,Cities and towns;Electrical safety;Power grids;Power system analysis computing;Power system faults;Power system security;Power systems;Risk analysis;Risk management;Software testing,decision making;power engineering computing;power grids;power system security;probability;risk analysis,AHP;analytic hierarchy process;online static risk assessment;power grids;power system security;probability theory;urban power system,,1,,10,,no,28-31 March 2010,,IEEE,IEEE Conference Publications
Research on service quality of campus vehicles based on SPSS,Haitao Su; Haiqing Guo; Xiaowei Ma,"School of Economics and Management, Nanchang University, China",2010 International Conference on Networking and Digital Society,20100607,2010,1,,394,398,"The quality of service is an important aspect of current social concern, and to meet the transport needs of teachers and students on campus is an important prerequisite for quality services. In the paper the evaluation index system of campus vehicle service quality was established based on AHP, and the Delphi method was used to determine the relative importance of individual index. The quality of campus vehicle service in Nanchang University was taken as an example, through multi-disciplinary and multi-level sampling survey of 120 people, 11 evaluation indexes were analyzed with SPSS statistical software. The analysis shows that the method proposed in the paper can accurately measure the current service quality of campus vehicles. The corresponding software and hardware improvement methods can be taken to improve the service quality for the emerging problems, which provides a fundamental guarantee for the scientific and standardized service management.",,Electronic:978-1-4244-5161-6; POD:978-1-4244-5162-3,10.1109/ICNDS.2010.5479215,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5479215,SPSS;campus vehicles;service quality,Conference management;Electronic mail;Hardware;Libraries;Quality management;Quality of service;Security;Software quality;Vehicle driving;Vehicle safety,educational institutions;forecasting theory;mathematics computing;sampling methods;transportation,AHP;Delphi method;SPSS statistical software;analytic hierarchy process;campus vehicle service quality;evaluation index system;multidisciplinary sampling;multilevel sampling;standardized service management,,0,,8,,no,30-31 May 2010,,IEEE,IEEE Conference Publications
Research on ship welding process planning with case-based reasoning,J. Chen; Z. Zhang; X. Jia; J. Geng,"Institute of CAPP & Manufacturing Engineering Software, Northwestern Polytechnical University, Xi'an, 710072, P. R. China",2010 2nd IEEE International Conference on Information Management and Engineering,20100603,2010,,,605,608,"In order to solve the problem that the judgment for the necessity of the welding procedure qualification (WPQ) depends excessively on the experience and the accumulated process knowledge of enterprise can't be unitized effectively in the traditional welding ship process planning, according to analyzing influencing factors of the ship welding procedure, an advanced planning approach based on case-based reasoning (CBR) was proposed. With this approach combining analytic hierarchy process (AHP) and similarity system theory, the weight and similarity of each factor were calculated, and then the process-similarity was presented to assist the process designer to judge. Adopting the result of judgment, the process planning would be achieved efficiently. Finally, a case study was demonstrated to show the rationality and adaptation of this approach.",,Electronic:978-1-4244-5264-4; POD:978-1-4244-5263-7,10.1109/ICIME.2010.5478182,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5478182,AHP;CBR;WPQ;ship welding;similarity,Artificial intelligence;Automotive engineering;Knowledge engineering;Manufacturing processes;Marine vehicles;Materials testing;Process design;Process planning;Qualifications;Welding,case-based reasoning;decision making;process planning;production engineering computing;shipbuilding industry;welding,analytic hierarchy process;case-based reasoning;ship welding process planning;similarity system theory;welding procedure qualification,,0,,9,,no,16-18 April 2010,,IEEE,IEEE Conference Publications
Research on the Evaluation of Performance of Integration Emergency Supply Chain with an Application of the Analytic Network Process,X. Shi; L. Yang,"Sch. of Econ. & Manage., Beijing Jiaotong Univ., Beijing, China",2010 International Conference on Internet Technology and Applications,20100909,2010,,,1,5,"In the first place, this thesis, based on an analysis of the features of the Integration Emergency Supply Chain and the affecting factors of its performance, considering the specificity of Integration Emergency Supply Chain, constructs an indicating system on the evaluation of overall performance of Integration Emergency Supply Chain. Secondly, based on an analysis of the applicability of the analytic network process, this thesis, analyses the dependent network process relationship between different indicators in the evaluation system of overall performance of the Integration Emergency Supply Chain, establishes a network construction of Performance Evaluation and confirms partial weight and overall weight. Meanwhile, through a survey of experts, this paper, with an empirical analysis of the software Super Decision, raised some managerial suggestions which can improve the performance.",,Electronic:978-1-4244-5143-2; POD:978-1-4244-5142-5,10.1109/ITAPP.2010.5566459,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5566459,,Construction industry;Economics;Educational institutions;Performance evaluation;Software;Supply chains,performance evaluation;risk management;supply chain management,Super Decision software;analytic network process;dependent network process relationship;integration emergency supply chain;performance evaluation,,0,,4,,no,20-22 Aug. 2010,,IEEE,IEEE Conference Publications
Research on the simulation problem of support vehicle requirement among the Multi-Aircraft Flight Support Process,Tian Feng; Xing Qing-hua; Wang San-tao; Xiong Zhe,"Missile Institute, Air Force Engineering University, sanyuan, China",2010 International Conference on Computer Application and System Modeling (ICCASM 2010),20101104,2010,15,,V15-450,V15-454,"Among the Multi-Aircraft Flight Support Process, as it refers to much influence factors for determining the requirement of the support vehicles, and the process of vehicle scheduling is complicated and changeful, so it is difficult to establish the analytic model for the above-mentioned NP hard problem. But the simulation model about the process of support vehicle scheduling could be found by making use of the discrete event dynamic system's modeling and simulation technology, through setting the different kinds of aircraft' flight batch interval in the simulation software Arena, Simulates the flight support process. Until the output of simulation which satisfies the requirement of flight ready model is obtained by continually adjusting the number of different kinds of support vehicle, whose scheduled utilization rate is higher than the other support vehicle in the software's process analyzer tool. The simulation result shows that the proposed method provides a new way to deal with the requirement problem of Multi-Aircraft flight support vehicles.",2161-9069;21619069,Electronic:978-1-4244-7237-6; POD:978-1-4244-7235-2,10.1109/ICCASM.2010.5622559,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5622559,multi-aircraft;requirement;scheduling;simulation;support vehicle,Atmospheric modeling;Nitrogen;Petroleum;Vehicles,aerospace computing;aircraft;computational complexity;discrete event simulation;scheduling,Arena simulation software;NP hard problem;discrete event dynamic system modeling;multiaircraft flight support process;simulation problem;software process analyzer tool;support vehicle requirement;support vehicle scheduling,,0,,6,,no,22-24 Oct. 2010,,IEEE,IEEE Conference Publications
Research on Wavelet Threshold De-noising Method of Remainder Detection for Stand-Alone Electronic Equipments in Satellite,Y. Wu; S. Wang; S. Wang,"Sch. of Electr. Eng. & Autom., HIT, Harbin, China","2010 First International Conference on Pervasive Computing, Signal Processing and Applications",20101115,2010,,,1013,1017,"Stand-alone electronic equipments are important parts of a satellite system, inside which remainders will bring great harm to reliability of the satellite system. Particle shock noise detection (PIND) is an effective method to detect the remainders in stand-alone electronic equipments. Signal to noise ratio (SNR) of detection signal is a bottleneck to improve processing accuracy and robustness of testing software. In this paper, key factors of wavelet threshold de-noising method were investigated and a wavelet threshold algorithm was proposed to improve the SNR of remainder detection signal of the stand-alone electronic equipments. The analytic results proved that the wavelet threshold method efficiently improved the de-noising effect.",,Electronic:978-0-7695-4180-8; POD:978-1-4244-8043-2,10.1109/PCSPA.2010.250,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635857,remaider detection;signal processing;stand-alone electronic equipments in satellite;wavelet threshold de-noising method,Electronic equipment;Noise reduction;Satellites;Signal to noise ratio;Wavelet analysis;Wavelet coefficients,artificial satellites;signal denoising;signal detection;wavelet transforms,PIND;particle shock noise detection;processing accuracy;remainder detection signal;satellite system;signal to noise ratio;stand-alone electronic equipments;testing software;wavelet threshold algorithm;wavelet threshold de-noising method,,0,,9,,no,17-19 Sept. 2010,,IEEE,IEEE Conference Publications
Revisiting the Determination of Busway Impedance Using First Principles,C. R. Rodrigues; G. S. O'Nan; D. Wittmer; J. Richter,"Power North America Analytics and Innovation, Schneider Electric North America/Square D Company, Smyrna, TN, USA",IEEE Transactions on Industry Applications,20100518,2010,46,3,1103,1108,"An approach using a combination of well-known circuit principles is used to convert the impedance matrix-determined for three-phase single lamination per phase busway-to industry-specified ‰ÛÏaverage‰Ûù impedance values as specified by the National Electrical Manufacturers Association. This paper details how this approach can be used to predict average busway impedance with reasonable accuracy, and therefore help in reducing the number of physical tests and related costs in the design process. Four cases are provided for validation purposes. The results show good correlation with published data.",0093-9994;00939994,,10.1109/TIA.2010.2045337,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5438810,Busway;electromagnetic software;impedance matrix;inductance;resistance,,electric impedance;electric vehicles,National Electrical Manufacturers Association;busway impedance;impedance matrix;three-phase single lamination,,0,,12,,no,May-june 2010,,IEEE,IEEE Journals & Magazines
Risk evaluation of EPC project based on ANP fuzzy comprehensive evaluation,H. y. Yang; W. b. Lv; H. l. Xu,"School of Civil Engineering, Northeast Forestry University, Harbin, China",2010 IEEE 17Th International Conference on Industrial Engineering and Engineering Management,20101129,2010,,,1091,1096,"In order to assess and analyze the importance of the elements which have a significant impact on the risk, analytic network process is applied to establish the network structure model of risk evaluation index system for EPC project. And hypermatrix is utilized to compute and determine the weight of all the indexes. An EPC project risk evaluation model is constructed based on ANP (Analytic Network Process) -Fuzzy Comprehensive Evaluation to evaluate the integrated risk level. Validity of such EPC project risk evaluation model would be proved by a demonstration case analysis with help of Super Decision software. Results are shown at the last part of this paper.",,Electronic:978-1-4244-6484-5; POD:978-1-4244-6483-8,10.1109/ICIEEM.2010.5646427,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5646427,Analytic Network Process;EPC general contract;risk evaluation,Fluctuations;Organizations,decision support systems;fuzzy set theory;risk management,ANP fuzzy comprehensive evaluation;EPC project risk evaluation model;analytic network process;hypermatrix;network structure model;risk evaluation index system;super decision software,,0,,8,,no,29-31 Oct. 2010,,IEEE,IEEE Conference Publications
Scale and rotation invariant recognition of cursive Pashto script using SIFT features,R. Ahmad; S. H. Amin; M. A. U. Khan,"Department of Computer Science, FAST-NUCES Peshawar Campus, Pakistan",2010 6th International Conference on Emerging Technologies (ICET),20101115,2010,,,299,303,"Cursive scripts such as Urdu, Pashto and Arabic contain large number of unique shapes called ligatures. Recognition of thousands of ligatures is challenging due to variations of various kinds including scaling, orientation, font style, spatial location/registration of ligatures and limited number of samples available for training. Accurate segmentation is a key challenge for analytic approaches, whereas holistic approaches suffer due to limitations of various feature representation schemes. In this paper, the use of SIFT descriptor has been proposed to evaluate its effectiveness for representing Pashto ligatures while overcoming above mentioned challenges in a holistic framework. The proposed approach is script independent and can be easily adapted to other cursive languages. A comparison of recognition results against classical methods such as PCA is provided to test the effectiveness of feature representation. Our research shows that SIFT descriptor perform better than classical feature representation methods such as PCA. The proposed recognition is holistic using ligature (word) based classification. We have tested 1000 unique ligatures (images) with 4 different sizes, along with their rotated images; and average recognition rate that is obtained is 74%.",,Electronic:978-1-4244-8058-6; POD:978-1-4244-8057-9,10.1109/ICET.2010.5638470,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5638470,,Character recognition;Feature extraction;Image segmentation;Optical character recognition software;Principal component analysis;Shape;Training,optical character recognition,PCA;SIFT descriptor;cursive Pashto script;holistic recognition;image recognition;ligatures;rotation invariant recognition,,4,,14,,no,18-19 Oct. 2010,,IEEE,IEEE Conference Publications
Selective Routing Protocol for Cognitive Wireless Networks Based on User's Policy,N. Uchida; G. Sato; K. Takahata; Y. Shibata,"Fac. of Software & Inf. Sci., Iwate Prefectural Univ., Iwate, Japan",2010 IEEE 30th International Conference on Distributed Computing Systems Workshops,20101111,2010,,,112,117,"Cognitive Wireless Network (CWN) is expected as one of the most efficient transmission methods to solve today's wireless problems like the lack of wireless bands or the efficient usage of limited wireless resources. However, CWN have not established effective transmission methods considered with QoS or upper layers protocols yet. In this paper, selective transmission control method in cognitive wireless network considering with cross layers including user policies is proposed. First, at the observation stage, the physical data such as user policy, electric field strength, bit error rate, jitter, latency, packet error rate, and throughput are observed. Then, at the decision stage, AHP (Analytic hierarchy process) is applied for decision making process with those parameters. Finally, the action stage, one of the suitable link is chosen and changed links and networks. Also, the route selecting method is proposed based on AODV protocols. When route changes come to need, considerable wireless routes are listed by AODV protocol including network conditions. Then, the following three methods are used for route selection; 1)Route selection so that Maximizing End-to-End throughput among all of the routes for CBR. 2) Route selection so that Minimizing End-to-End delay time among all of the routes for VoIP. 3) Route selection so that Optimize the policy based AHP for Web service. In our simulation, ns2 is used for the computational results to the effectiveness of the suggested transmission methods in cognitive wireless networks. The results showed the effectiveness of the proposed methods.",1545-0678;15450678,Electronic:978-1-4244-7472-1; POD:978-1-4244-7471-4,10.1109/ICDCSW.2010.44,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5628746,AHP;AODV;Cognitive Wireless Network;QoS,Ad hoc networks;Delay;Routing protocols;Throughput;Wireless networks,cognitive radio;decision making;error statistics;jitter;routing protocols;wireless sensor networks,AODV protocols;analytic hierarchy process;bit error rate;cognitive wireless networks;decision making process;electric field strength;jitter;latency;packet error rate;route selection;selective routing protocol;throughput;transmission control method;user policy,,2,,14,,no,21-25 June 2010,,IEEE,IEEE Conference Publications
Sensitivity of circular coils to conductivity changes: Analytical and numerical solutions,K. Ì_. Ì_zkan; N. G. GenÌ_er,"Elektrik ve Elektronik M&#252;hendisligi B&#246;l&#252;m&#252;, ODT&#220;",2010 15th National Biomedical Engineering Meeting,20100607,2010,,,1,4,"The method of contactless conductivity imaging is based on the magnetic-induction magnetic-measurement principle. In this study, the sensitivity of the circular coil used for measuring magnetic fields, to conductivity changes is investigated. To do so, the electrical circuit model of the receiver coil and that of the conductive object is developed. The conductive object is modelled as a thin disk (cylinder) coaxially placed inside the circular coil. Another cylindrical object is assumed to be coaxially placed within the conductive object. It is assumed that, the conductive object is placed within a time varying but spatially constant magnetic field. An analytic formulation is developed for the determination of the sensitivity of the circular coil to the variations of the radius and conductivity of the second object (inhomogeneity). The validity of the model is tested with numerical simulations employing the ANSYS magnetic simulation software. The nonlinearity error between the analytic and numerical solutions is calculated as 1.38% of the full scale for ë_=0.1, and 11.2% of the full scale for ë_=0.2.",,Electronic:978-1-4244-6382-4; POD:978-1-4244-6380-0,10.1109/BIYOMUT.2010.5479791,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5479791,,Circuits;Coaxial components;Coils;Conductivity measurement;Contacts;Magnetic analysis;Magnetic field measurement;Magnetic fields;Nonuniform electric fields;Software testing,bioelectric phenomena;biomagnetism;biomedical imaging;coils;conducting bodies;electromagnetic induction,ANSYS magnetic simulation software;circular coil sensitivity;conductive object;contactless conductivity imaging;cylinder;electrical circuit model;magnetic fields;magnetic-induction magnetic-measurement principle;nonlinearity error;receiver coil;thin disk,,0,,15,,no,21-24 April 2010,,IEEE,IEEE Conference Publications
Simulation of parallel-plate pulsed plasma Teflonå¨ thruster based on the electromechanical model,Lei Yang; Xiangyang Liu; Wenjun Guo; Zhiwen Wu; Ningfei Wang,"School of Aerospace Engineering, Beijing Institute of Technology, China",2010 2nd International Conference on Advanced Computer Control,20100617,2010,5,,148,151,"Electric propulsion devices have been used widely in space flight mission for its superior performance. As a kind of electromagnetic thruster, the parallel-plate pulsed plasma thruster (PPT) as well as its operation process is theoretically analyzed and discussed. Based on the electromechanical model of PPT-an idealized, quasi-steady, analytic model including the ablated mass process of Teflon, an interactive intelligent computer-aided software which is used to predict the influences of variations of several electric parameters (capacitor capacitance, initial voltage, circuit resistance and electron temperature) and configuration parameters (electrodes distance, electrode width) on PPT performance is designed and implemented in MATLAB environment. And an application example of Lincoln Experimental Satellite VI (LES-6) PPT shows that this software has characteristics of easy operation, friendly human-computer interaction interface, high display speed and precision for PPT preliminary design. Finally, certain deficiencies in the presented model are identified and addressed with considerations for future improvement.",,Electronic:978-1-4244-5848-6; POD:978-1-4244-5845-5,10.1109/ICACC.2010.5487276,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5487276,electric propulsion;electromechanical model;human-computer interaction;pulsed plasma thruster,EMP radiation effects;Electrodes;Electromagnetic analysis;Mathematical model;Performance analysis;Plasma devices;Plasma simulation;Predictive models;Propulsion;Space missions,aerospace propulsion;artificial satellites;digital simulation;electric propulsion;electromagnetic devices;interactive systems,Lincoln experimental satellite VI;ablated mass process;electric propulsion devices;electromagnetic thruster;electromechanical model;interactive intelligent computer aided software;parallel plate pulsed plasma Teflonå¨ thruster;space flight mission,,0,,7,,no,27-29 March 2010,,IEEE,IEEE Conference Publications
Skills evaluation of junior-level software talents,W. Sun; Y. h. Ni; X. y. Wu; Y. h. You; K. Liu,"Department of Information Technology, Beijing Normal University ( ZhuHai ), Guangdong, China",2010 Seventh International Conference on Fuzzy Systems and Knowledge Discovery,20100909,2010,3,,1044,1048,"Many software industrial association established in all provincial capital cities in China. Providing service for talents and company is their main responsibilities. The evaluation of software talents skills is multi-factor, multi-objective and multi-criteria decision-making problems. The current selection and evaluation method which the company used commonly is single, non-objective, non-universal and low input-output ratio. Base on a lot of investigation and research in typical software industry, the method of AHP (analytic hierarchy process) and FCEM (fuzzy comprehensive evaluation method) are used to evaluate the skills level of junior-level software talents. It makes evaluation results more objective and reasonable.",,Electronic:978-1-4244-5934-6; POD:978-1-4244-5931-5,10.1109/FSKD.2010.5569577,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5569577,AHP;FCEM;evaluation of junior-level software talents skills,Companies;Decision making;Industries;Knowledge engineering;Software;Software engineering;Testing,DP industry;DP management;decision making;fuzzy set theory;personnel,Zhuhai software industrial association;analytic hierarchy process;fuzzy comprehensive evaluation method;junior-level software talents;skills evaluation;software industry;talent evaluation method;talent selection method,,0,,19,,no,10-12 Aug. 2010,,IEEE,IEEE Conference Publications
Soft Decision Design of Spectrally Partitioned CI-SMSE Waveforms for Coexistent Applications,E. Like; M. Temple; Z. Wu,,2010 IEEE International Conference on Communications,20100701,2010,,,1,6,"Applicability of Spectrally Modulated, Spectrally Encoded (SMSE) waveform design has been expanded for future Cognitive Radio (CR)-based Software Defined Radio (SDR) applications. As previously demonstrated, the SMSE waveform design process can exploit statistical knowledge of PU spectral and temporal behavior to maximize SMSE system throughput (bits/second) while adhering to SMSE and Primary User (PU) spectral constraints. The capacity of SMSE systems is extended here using spectral partitioning with carrier-interferometry (CI) coding to increase SMSE waveform agility in the presence of a spectrally diverse transmission channel. By adaptively varying the modulation order and optimally allocating power within each spectral partition, inherent SMSE flexibility is more fully exploited and substantially increases system throughput while meeting Power Spectral Density (PSD) constraints. A coexistent scenario is provided in which the analytic optimization of the SMSE waveform is demonstrated while meeting spectral mask requirements. Results show that spectrally partitioned CI-SMSE waveforms have a significantly greater ability to adapt to varying spectral requirements.",1550-3607;15503607,Electronic:978-1-4244-6404-3; POD:978-1-4244-6402-9,10.1109/ICC.2010.5502446,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5502446,,Application software;Bit error rate;Chromium;Cognitive radio;Interference constraints;Multicarrier code division multiple access;OFDM modulation;Process design;Software radio;Throughput,cognitive radio;software radio,CI-SMSE waveform;carrier interferometry;cognitive radio;power spectral density;soft decision design;software defined radio;spectrally diverse transmission channel;spectrally modulated spectrally encoded waveform design;temporal behavior,,1,,30,,no,23-27 May 2010,,IEEE,IEEE Conference Publications
Soil water division by SPSS statistics analysis software in Hebei Province,H. Xia; H. L. Han; L. H. Yang; L. L. Sheng; H. S. Wang,"College of Urban and Rural Construction, Agriculture University of Hebei, Baoding, 071001, China",2010 International Conference on Machine Learning and Cybernetics,20100920,2010,4,,1980,1986,"Water resource in Hebei Province is extremely deficient; however, there is plenty of precipitation so that the soil water is abundance. If the soil water there can be taken full advantage of, part of the water resource can be saved. In order to scientifically and rationally evaluate and develop the technology of exploiting the soil water in Hebei Province, to utilize the soil water adjusting measures to local conditions in different regions, but to adopt unified irrigation regime and irrigation requirement in the same region, and to be unified planning and management, the soil water should be divided into different kinds of soil water regions so that to utilize soil water sufficiently. In this paper, making reference to the measure of the water resource regionalization, basing on the information of the soil water in Hebei Province, the topography and physiognomy of the earth's surface, drought index, soil texture and vegetation were selected to make up of the indices system, the weight of each index was calculated by utilizing the analytic hierarchy process, and in the end SPSS (Statistical Product and Service Solutions) statistics analysis software was used to divide the soil water resource. Finally, the soil water resource in Hebei Province was divided into 8 subdivisions. It is necessary to utilize the soil water reasonably and efficiently.",2160-133X;2160133X,Electronic:978-1-4244-6527-9; POD:978-1-4244-6526-2,10.1109/ICMLC.2010.5580517,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5580517,Analytic hierarchy process;Division of the soil water;Hebei Province submission;SPSS statistics analysis software,Indexes;Irrigation;Software;Soil;Vegetation;Water resources,irrigation;mathematics computing;statistical analysis;vegetation;water resources,Earth surface;Hebei province;Statistical Product and Service Solutions statistics analysis software;analytic hierarchy process;drought index;indices system;physiognomy;precipitation;soil texture;soil water division;topography;unified irrigation regime;vegetation;water resource regionalization,,0,,5,,no,11-14 July 2010,,IEEE,IEEE Conference Publications
Study of Influencing Factor Index System of tourism development for Henan based on AHP method,Ran Wei,"School of Economics & Management, Zhongyuan University of Technology, Zhengzhou 450007, China",2010 International Conference on Future Information Technology and Management Engineering,20101203,2010,1,,578,581,"An analytical way to reach the best decision is more preferable in many business platforms. In this study, Analytic Hierarchy Process has been applied for evaluating the impact factors of foreign exchange income by tourism market in Henan province. Moreover, the score methodology by specialists and MATLAB software are also used for getting the impact factors' weight scores. The results show that the top four among the evaluated impact factors are Signtseeing, Long-distance, Arrivals and Accommodation, which are the key factors influencing foreign exchange income by tourism market of Henan province.",,Electronic:978-1-4244-9090-5; POD:978-1-4244-9087-5,10.1109/FITME.2010.5655798,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5655798,Analytic Hierarchy Process;Henan province;Influencing Factor Index System;foreign exchange income;tourism,Indexes;Industries;Materials;Radio access networks;Software;Transportation;Tutorials,decision making;foreign exchange trading;travel industry,Henan based on AHP method;Henan province;MATLAB software;analytic hierarchy process;factor index system;foreign exchange income;tourism development;tourism market,,0,,11,,no,9-10 Oct. 2010,,IEEE,IEEE Conference Publications
Study of modeling and simulation of Flexsim-based inventory management system,X. y. Jiang; P. Chen; R. Zheng,"Department of Management Science, Xiamen University of Technology, China",2010 IEEE 17Th International Conference on Industrial Engineering and Engineering Management,20101129,2010,,,1591,1594,"This paper examines the modeling and simulation process of an assembly line of a certain computer manufacturer. It employs the powerful re-develop technology of the Flexsim software to perform advanced simulation modeling, thereby eliminating the difficulties in the modeling of inventory management system and in its simulation optimization. This paper also presents the codes for advanced simulation. It has practical significance for facilitating the complex modeling of inventory management system, and provides managers with substantial analytic data and close-to-reality visual scenarios to make sound decisions.",,Electronic:978-1-4244-6484-5; POD:978-1-4244-6483-8,10.1109/ICIEEM.2010.5646120,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5646120,Flexsim;assembly line;inventory management system;simulation,Layout;Machinery;Systematics,DP industry;decision making;inventory management;production engineering computing,Flexsim based inventory management system;advanced simulation modeling;complex modeling;computer manufacturer;decision making;powerful redevelop technology;simulation optimization;substantial investigate analytic data,,0,,10,,no,29-31 Oct. 2010,,IEEE,IEEE Conference Publications
Study on the Improvement of Analytic Hierarchy Process under College Course Evaluation System,C. Zhou; L. Huang,"Dept. of Sci. & Technol., Jiangxi Univ. of Sci. & Technol., Ganzhou, China",2010 International Conference on Intelligent Computation Technology and Automation,20100726,2010,3,,598,601,"Under the guidance of analytic hierarchy process and genetic algorithms, establish three evaluation index systems for courses, by means of analytic hierarchy process, which improves accelerating genetic algorithm by real-coding, to weigh the evaluation index systems, and you can well evaluate the rationality of courses set up by colleges.",,Electronic:978-1-4244-7280-2; POD:978-1-4244-7279-6,10.1109/ICICTA.2010.78,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5522889,Accelerating Genetic Algorithm;Analytic Hierarchy process;Course Selection;Evaluation System,Acceleration;Acoustic testing;Algorithm design and analysis;Automatic testing;Educational institutions;Employment;Genetic algorithms;Life testing;Psychology;Shape control,educational courses;genetic algorithms,analytic hierarchy process improvement;college course evaluation system;evaluation index systems;genetic algorithms;real coding,,0,,7,,no,11-12 May 2010,,IEEE,IEEE Conference Publications
Supplier selection and evaluation models for a design change scheme in collaborative manufacturing environment,Y. J. Tseng; Y. J. Su; F. Y. Huang,"Department of Industrial Engineering and Management, Yuan Ze University, Taoyuan, Taiwan",The 40th International Conference on Computers & Indutrial Engineering,20101213,2010,,,1,6,"A design change is commonly applied to change a portion of an existing product for the purpose of adding some functional values or reducing certain costs. In a global supply chain, a design change not only changes a portion of a product, but also changes the selection of suppliers. Therefore, it is important to evaluate a design change and its effect on the selection of suppliers in a collaborative manufacturing environment. In this research, a fuzzy analytic hierarchy process evaluation model is presented for evaluating the different design change alternative cases. First, a component relational matrix is modeled. If an initial component change is made, the other components that are affected by the initial component are identified and represented using a component relational matrix. A component relational matrix is used to obtain the relational values of the relational factors for the different suppliers. Second, a fuzzy analytic hierarchy process evaluation method is presented to establish the hierarchy of the indices and analyze the final total relational values of the relational factors for the different suppliers. The total relational values are compared to select the most suitable suppliers. Implementation and test results of a mobile example product are presented.",,Electronic:978-1-4244-7297-0; POD:978-1-4244-7295-6,10.1109/ICCIE.2010.5668206,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5668206,collaborative Manufacturing;design change;fuzzy analytic hierarchy process;supplier selection,Analytical models;Assembly;Collaboration;Computational modeling;Computer aided software engineering;Materials,cost reduction;decision making;fuzzy set theory;groupware;matrix algebra;product design;production engineering computing;supply chain management,collaborative manufacturing environment;component change;component relational matrix;cost reduction;design change alternative cases;design change scheme;evaluation models;functional values;fuzzy analytic hierarchy process evaluation model;global supply chain;relational factors;relational values;supplier selection,,0,,8,,no,25-28 July 2010,,IEEE,IEEE Conference Publications
Test Case Reusability Metrics Model,Zhang Juan; L. Cai; W. Tong; Yuan Song; Li Ying,"School of Computer Engineering and Science, Shanghai University, China",2010 2nd International Conference on Computer Technology and Development,20101129,2010,,,294,298,"As organizations implement systematic reuse test cases in software testing to improve productivity and quality, they must be able to measure reusability of test case and identify the most effective reuse strategies. This is done with Test Case Reusability Metrics Models(TCRMM). In this article we survey metrics of test case reusability, and provide reusability factors division that will help establish reusability assessment model. It combines four metrics factors into reusability metrics based on analytic hierarchy process. The prominent characteristic of this model is that the process of reusability metric is automated when the judgment matrix has been constructed.",,Electronic:978-1-4244-8845-2; POD:978-1-4244-8844-5,10.1109/ICCTD.2010.5645869,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5645869,AHP;TCRMM;metric;reusability;test case,,decision making;program testing;software metrics;software reusability,analytic hierarchy process;judgment matrix;reusability assessment model;reusability factors division;test case reusability metrics model,,2,,15,,no,2-4 Nov. 2010,,IEEE,IEEE Conference Publications
Testing of a spatial impulse response algorithm for double curved transducers,D. BÌ_k; J. A. Jensen; M. Willatzen,"Center for Fast Ultrasound Imaging, Department of Electrical Engineering, Technical University of Denmark",2010 IEEE International Ultrasonics Symposium,20110630,2010,,,1226,1229,"The spatial impulse response (SIR) method for solving the Rayleigh integral is a well known method for fast time response simulation of acoustic waves. Several analytical expressions have been found for simple transducer geometries such as rectangles and discs. However, no analytical solution is known for double curved transducers (DCT), i.e. transducers with both concave and convex radius. To calculate the SIR from such transducers Field II uses a far-field approximation by dividing the surface into smaller flat elements and then performs a summation of the response from all the elements using Huygen's principle. This calculation method involves several summations, and it relies on exact phase calculation to avoid numerical noise in the response. A stable analytical expression for the SIR would thus be beneficial to the Field II software as an alternative solver. A semi-analytic algorithm (SAA) has been developed, and it is the objective of this work to validate an analytical approximation of the algorithm as an alternative solver for Field II. Two approximations of a SAA that efficiently finds the SIR for DCT have been implemented into a MATLAB and a C-code environment. The root mean square (RMS) error of calculating the SIR using Field II and the C-implemented approximation are calculated relative to a high resolution solution obtained with MATLAB on a DCT, a linear concave, and a flat transducer. The computation time for solving a point 400 times is also found. Calculations are performed at sampling frequencies ranging from 100 MHz to 15 GHz in steps of 100 MHz. The transducer width is 250 ë_m and the height is 10 mm. The C-implementation exhibits errors ranging from 4.9-10<sup>-1</sup> % to 0.91 % and Field II 0.0117 % to 0.94 %. A slight trade off between accuracy and computation time is found. Field II outperforms the SAA in computation time if high accuracy is not needed. However, if a higher accuracy is required, the SAA is the best model choice.",1051-0117;10510117,Electronic:978-1-4577-0381-2; POD:978-1-4577-0382-9,10.1109/ULTSYM.2010.5935558,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5935558,,Approximation methods;Discrete cosine transforms;Geometry;Mathematical model;Pistons;System-on-a-chip;Transducers,C language;Rayleigh waves;electronic engineering computing;physics computing;transient response;ultrasonic transducers,C-code environment;DCT SIR;Field II software alternative;Huygen principle;MATLAB environment;Rayleigh integral solution;SIR algorithm testing;acoustic wave fast time response simulation;concave-convex radius transducers;double curved transducers;exact phase calculation;far field approximation;flat transducer comparison;frequency 100 MHz to 15 GHz;linear concave transducer comparison;response summation;semianalytic algorithm;size 10 mm;size 250 mum;spatial impulse response;transducer height;transducer width,,0,,17,,no,11-14 Oct. 2010,,IEEE,IEEE Conference Publications
The computation and application of random consistency index,Ye Lin; Deng Xiaohong,"Teaching affairs offices, Qingdao technological university 266520, China",The 2nd International Conference on Information Science and Engineering,20110117,2010,,,693,695,"Analytic hierarchy process (AHP) is a method frequently used in mathematical modeling. Its consistency must be tested, after the positive reciprocal matrix of paired comparison is constructed. The formal test results of random consistency indices were of lower order. In practical application, for various factors are often involved, the computation of the test results of random indices of higher order is often complicated. Using Matlab software, we resolved the problem that positive reciprocal Matrix of any order be generated at random, thus we solved the problem that how to calculate random consistency index of high-order the better. The method is also applied to ‰ÛÏthe mathematical modeling of coal-mining safety alarm system‰Ûù, with ideal results.",2160-1283;21601283,DVD:978-1-4244-7617-6; Electronic:978-1-4244-7618-3; POD:978-1-4244-7616-9,10.1109/ICISE.2010.5691977,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5691977,Matlab Software;Positive and reciprocal Matrix;Random index,Accidents;Boilers;Indexes;Mathematical model;Slurries;Software,,,,0,,5,,no,4-6 Dec. 2010,,IEEE,IEEE Conference Publications
The evaluation of the integrated ecological environment status of Shizuishan City based on Analytic Hierarchy Process,G. Yingqi; Q. Qingwen,"Institute of Geographic Sciences and Natural Resource Research, CAS, Graduate University of Chinese Academy of Sciences, Beijing, China",The 2nd International Conference on Information Science and Engineering,20110117,2010,,,4147,4150,"Distinguish the importance degree of the elements affecting the ecological environmental status of ShiZuishan City by expert scoring, establish the evaluation index system and calculate the weight of each element contributing to affect the quality of the ecological environmental status based on the Analytic Hierarchy Process. The analysis result shows that the most important four elements affecting the ecological environmental status of ShiZuishan City are as follows: vegetation cover condition, geomorphological Characteristics, soil erodibility and pollution status. Sketch out the integrated geographical union using the remote sensing image, endow the attribute to every integrated geographical union according to the basic materials, and then, make the attribute values being dimensionless, assort the attributes into several classifications, finally, use the modeling tool in Arctoolbox module of the ArcGIS9.2 software to build weighted comprehensive evaluation model, draw the final evaluation map of the integrated ecological environment status of Shizuishan city by stacking the four elements weighted. The ecological environment status of Shizuishan City is graded into four ranks, excellent, good, general and worse. Based on the comprehensive evaluation result, putting forward some suggestions to Shizuishan City on promoting sustainable development of economy and society, for example, suggest Shizuishan City to accelerate the closing of the high water consumption, high energy consumption and high pollution enterprises, lay the development emphases on PingLuo country, construct tourism facilities around the lakes and wetlands within limited, prohibit exploiting the nature reserve on the Ho-lan Mountain and cancel the coal mines out in the nature reserve.",2160-1283;21601283,DVD:978-1-4244-7617-6; Electronic:978-1-4244-7618-3; POD:978-1-4244-7616-9,10.1109/ICISE.2010.5690663,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5690663,Analytic Hierarchy Procest;ecological environment evaluation;weighted stack,Cities and towns;Fuel processing industries;Indexes;Pollution;Software;Soil;Vegetation mapping,,,,1,,6,,no,4-6 Dec. 2010,,IEEE,IEEE Conference Publications
The implementation of general evaluation model in the e-government,Song Shao-zhong; Oyang-tao,"Department of Information Engineering, Jilin Business and Technology College, Changchun 130062, China",2010 3rd International Conference on Advanced Computer Theory and Engineering(ICACTE),20100920,2010,6,,V6-337,V6-340,"At first, in this paper AHP and Fuzzy Synthetic Evaluation Method are abstracted with Mathematics tools and packaged then evaluation index is filled by the system administrator with semi-automatic mode applying open database technology the filled data are related to the packaged algorithm. Finally the paper implements the evaluation based on B/S.",2154-7491;21547491,Electronic:978-1-4244-6542-2; POD:978-1-4244-6539-2,10.1109/ICACTE.2010.5579167,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5579167,AHP;B/S Mode;Fuzzy Synthetic Evaluation Method;Open Database,Databases,database management systems;decision making;fuzzy set theory;government data processing,AHP;analytic hierarchy processing;e-government;fuzzy synthetic evaluation method;general evaluation model;open database technology,,0,,7,,no,20-22 Aug. 2010,,IEEE,IEEE Conference Publications
The improvement of the performance of DS-UWB system based on a switched chaotic sequence,Liping Yuan; Guangyi Wang; Qinqin Wu,School of Electronics Information/Hangzhou Dianzi University/ China,"2010 International Conference on Communications, Circuits and Systems (ICCCAS)",20100923,2010,,,760,763,"This paper proposes a chaotic PN (pseudo-noise) sequence, which is generated by a novel switched chaotic system. Its fifteen performances are analyzed via current international standard NIST, which is working in the Linux operating system. The analytic results indicate that the switched chaotic sequence can pass NIST test and satisfy the command of DS-UWB system. Also the model of chaos-based DS-UWB communication system is established in the purpose of analyzing the performance of its reception. Using the switched chaotic sequence as spread spectrum address code and simulating its performance with the MATLAB software, at the same time assuming ideal synchronization at the receivers. Simulation results show that the new chaotic sequence is a good PN sequence for bit error rate (BER) performance of the DS-UWB system and more excellent than the conventional sequences such as the m sequence.",,Electronic:978-1-4244-8225-2; POD:978-1-4244-8224-5,10.1109/ICCCAS.2010.5581879,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5581879,,Chaos;Computer languages;Signal to noise ratio;Switches,chaotic communication;error statistics;pseudonoise codes;spread spectrum communication;ultra wideband communication,BER;DS-UWB system;MATLAB;bit error rate;ideal synchronization;pseudonoise sequence;spread spectrum address code;switched chaotic sequence,,0,,14,,no,28-30 July 2010,,IEEE,IEEE Conference Publications
The Measurement Model for Enterprise Informationization Capability Maturity,Y. Yang; H. Ma; L. Dai; E. Zhang,"Inf. Coll., Capital Univ. of Econ. & Bus., Beijing, China",2010 Third International Joint Conference on Computational Science and Optimization,20100729,2010,2,,197,202,"As a dynamic development process, informationization involves a development process from immaturity to maturity and from imperfection to perfection. Constant improvement of the informationization process is a must to make the final achievement. This paper, which focuses on the extension and expansion of the content and measurement system of the Enterprise Informationization Capability Maturity Model (EICMM), discusses the measurement analysis from the dimensions of the development level, development quality, development capacity and suitability of enterprise informationization based on the hierarchical framework for the enterprise informationization maturity. This paper analyzes in detail the concrete concepts and contents of the EICMM, provides corresponding measurement paths, and helps enterprises identify the problems behind informationization construction to achieve the process and level upgrade of informationized enterprises.",,Electronic:978-1-4244-6813-3; POD:978-1-4244-6812-6,10.1109/CSO.2010.206,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5533051,analytic hierarchy process;fuzzy comprehensive assessment;measurement of enterprise informationization capability maturity,Capability maturity model;Computer aided instruction;Conference management;Costs;Inventory management;Logistics;Marketing and sales;Supply chain management;Supply chains;Technology management,Capability Maturity Model;organisational aspects;software houses,content system;dynamic development process;enterprise informationization capability maturity model;measurement system,,0,,5,,no,28-31 May 2010,,IEEE,IEEE Conference Publications
The Mouse HRV Analysis Based on Radius of FOD,H. Du; D. Liu; Q. Li,"Sch. of Math. Sci., Univ. of Electron. Sci. & Technol. of China, Chengdu, China",2010 International Conference on Computational Intelligence and Software Engineering,20101230,2010,,,1,4,"Environment change induces a great influence on mouse heartbeat. The Poincare plot and the first-order differential (FOD) plot are usually introduced to analyze heart rate variability (HRV), while both of them are qualitative methods to the research object. A quantitative approach is suggested to analyze abnormal HRV based on the FOD radius in this paper. Based on FOD sequence between the RR intervals, we get the FOD plot. FOD radius is estimated which divide FOD plot into two different regions: normal and abnormal region. So we get the information containing classification of RR intervals. Our research results show that, Concentrated Ambient Particles (CAPs) have significant adverse health effects on both healthy and unhealthy mice, and it's much more sensitive to unhealthy mice.",,Electronic:978-1-4244-5392-4; POD:978-1-4244-5391-7,10.1109/CISE.2010.5677088,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5677088,,Correlation;Heart beat;Heart rate variability;Mice;Rats;Time domain analysis;Time frequency analysis,biomedical measurement;cardiology;medical signal processing;signal classification,FOD radius;Poincare plot;RR intervals;abnormal HRV;concentrated ambient particles;first order differential plot;health effects;heart rate variability;mouse HRV analysis;mouse heartbeat,,0,,4,,no,10-12 Dec. 2010,,IEEE,IEEE Conference Publications
The multi-dimension component quality evaluation,Ying Jiang; Ying-Na Li; Yun Du; Xiao-Dong Fu,"Faculty of Information Engineering and Automation, Kunming University of Science and Technology, China",2010 International Conference On Computer Design and Applications,20100809,2010,2,,V2-250,V2-254,"Recently, software quality evaluation based on ISO/IEC 9126 and ISO/IEC 14598 has been used widely. However, these standards for software quality don't provide practical guidelines to apply the quality model and the evaluation process to component based software. This paper presents a multi-dimension quality model and a quantitative quality evaluation model for components and component based software. Particularly, our model provides the weights of quality characteristics and sub-characteristics using analytic hierarchical process technique. We also present the evaluation process using different methods for component developers, reusers and the third party. As a result, we believe that the proposed model will be helpful to evaluate component quality.",,Electronic:978-1-4244-7164-5; POD:978-1-4244-7162-1,10.1109/ICCDA.2010.5541423,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5541423,component quality model;evaluation model;evaluation process;multi-dimension,Application software;Educational technology;IEC standards;ISO standards;Object oriented modeling;Programming;Software engineering;Software quality;Software standards;Software systems,object-oriented programming;software performance evaluation;software quality;software reusability;software standards,analytic hierarchical process;component based software;multidimension component quality evaluation model;quantitative quality evaluation model;software quality evaluation;software reuse;software standard,,1,,17,,no,25-27 June 2010,,IEEE,IEEE Conference Publications
"The Practitioner's Cycles, Part 2: Solving Envisioned World Problems",R. R. Hoffman; S. V. Deal; S. Potter; E. M. Roth,IHMC,IEEE Intelligent Systems,20100601,2010,25,3,6,11,"Previous Human-Centered Computing department articles have reflected on the mismatch that can occur between the promise of intelligent technology and the results of technological interventions. Part 1 on the Practitioner's Cycles illustrated ways in which actual world problems-the forces and constraints of procurement-are at odds with the goals of human centering. This article culminated in a practitioner's tale, in which individuals acted on their own initiative and at their own risk, short-circuiting the rules and constraints that limit success at procurement. This paper presents a model based on the tale and focuses on how the model applies to envisioned world problems-the creation of intelligent technologies for new work systems.",1541-1672;15411672,,10.1109/MIS.2010.89,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5475081,Software engineering;Spiral method;Waterfall method;practitioner's cycle approach;requirements engineering,Collaboration;Feeds;Humans;Procurement;Prototypes;Rivers;Software prototyping;System testing;Systems engineering and theory;Weather forecasting,artificial intelligence;procurement;production engineering computing,Practitioner Cycles;human-centered computing;intelligent technologies;procurement,,6,,19,,no,May-June 2010,,IEEE,IEEE Journals & Magazines
The selection of project management software by FAHP and FMCDM in automobile R&D process,Lei Fu; Li Shi; Ying Yang; Bengong Yu,"Key Laboratory of Process Optimization & Intelligent Decision-making of Ministry of Education, School of Management, Hefei University of Technology, China",2010 International Conference on Networking and Digital Society,20100607,2010,1,,66,69,"The selection of project management (PM) software is a complex issue and has significant impacts to the efficiency of the automobile research & development (R&D) process. Given two alternatives based on critical path method (CPM) and critical chain project management (CCPM) respectively, decision-makers usually cause confusion. This paper analyzed the characteristic of automobile R&D project and developed an evaluation model based on the fuzzy analytic hierarchy process (FAHP) and fuzzy multiple criteria decision making (FMCDM) methods, which can help the managers in automobile industry to select more appropriate software in the course of product R&D. In particular, the FAHP method is used to obtain the weights of evaluation criteria, and the FMCDM method is used to determine the final rank of the software. The uncertainty and vagueness in evaluation procedure were presented as the fuzzy triangular numbers. The proposed method is applied to the case study of evaluating the performance of PM software based on CPM and CCPM respectively for a car manufacturer. The result of evaluation not only assisted managers in choosing suitable software but also outline the trend in development of CCPM software.",,Electronic:978-1-4244-5161-6; POD:978-1-4244-5162-3,10.1109/ICNDS.2010.5479297,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5479297,CCPM;CPM;FAHP;FMCDM;PM software;automobile R&D,Application software;Automobiles;Computer industry;Decision making;Job shop scheduling;Project management;Research and development;Research and development management;Software development management;Software performance,automobile industry;critical path analysis;fuzzy set theory;operations research;project management;research and development management;software selection,automobile R&D process;automobile industry;car manufacturer;critical chain project management;critical path method;fuzzy multiple criteria decision making;fuzzy triangular number;project management software selection,,0,,19,,no,30-31 May 2010,,IEEE,IEEE Conference Publications
The Study of Test Evaluation Method Based on Bank Assessment System,J. Song; F. Huang,"Sch. of Comput. Sci. & Technol., SooChow Univ., Suzhou, China",2010 International Conference on Computational Intelligence and Software Engineering,20101230,2010,,,1,4,"In this paper, the personal financial assessment system of Industrial and Commercial Bank of China branch in Suzhou was used for a example, during the designing and testing in parallel, according to test results, we propose a method to calculate the satisfaction of the software system, to judge the performance of the system and to analysis the extent of the various stages of testing changes, then evaluate the quality of the system changes.",,Electronic:978-1-4244-5392-4; POD:978-1-4244-5391-7,10.1109/CISE.2010.5676846,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676846,,Appraisal;Indexes;Inspection;Mathematical model;Programming;Software;Testing,bank data processing;decision making;program testing;software performance evaluation,Industrial and Commercial Bank of China;Suzhou;analytic hierarchy process;bank assessment system;financial assessment system;test evaluation method,,1,,3,,no,10-12 Dec. 2010,,IEEE,IEEE Conference Publications
Three dimensional finite element analysis of radial-flow impeller temperature field,Liang-wei Zhong; Kang-min Chen,"College of Power Engineering, University of Shanghai for Science and Technology, 200093, China",2010 International Conference on Mechanic Automation and Control Engineering,20100803,2010,,,420,423,"The technique of three dimensional solid element model and assembly was used to determine the temperature field of radial-flow impeller. The FEM software Cosmos was applied to analyze model system, and the precise analytic results were obtained. The results show that the analytic model can reflect the stead-state and transient temperature field characteristics of impeller directly, and thus can be worthy reference to analyze and calculate the temperature field of impeller in engineering design.",,DVD:978-1-4244-7738-8; Electronic:978-1-4244-7739-5; POD:978-1-4244-7737-1,10.1109/MACE.2010.5535344,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5535344,assembly;finite element;runoff blade wheel rotor;temperature field;three dimension,Blades;Distributed computing;Educational institutions;Finite element methods;Impellers;Temperature distribution;Thermal engineering;Transient analysis;Turbines;Wheels,finite element analysis;impellers,3D finite element analysis;Cosmos;FEM software;radial-flow impeller temperature field;solid element model,,0,,7,,no,26-28 June 2010,,IEEE,IEEE Conference Publications
Ticker text extraction from Bangla news videos,A. Tiwari; H. Ghosh,"Innovation labs, Delhi, Tata Consultancy Services",2010 Annual IEEE India Conference (INDICON),20110214,2010,,,1,4,"In this paper, a framework for recognition of Bangla ticker text<sup>1</sup> from the Bangla news videos is presented. Tesseract OCR [1] has been used for Bangla script recognition. Tesseract OCR gives good results for text recognition in documents. But in case of images and videos, some processing is required beforehand. Approach here is to provide processed images to the Tesseract OCR to get better results than directly providing the raw video frames to the Tesseract OCR. The ticker text recognized can further be used for indexing of news videos on the basis of recognized keywords. Indexing of news videos is important for news monitoring agencies. Till now this is done manually. Automation of the monitoring process and indexing the news videos can save a lot of time as well as the efficiency of the news monitoring system.",2325-940X;2325940X,Electronic:978-1-4244-9074-5; POD:978-1-4244-9072-1,10.1109/INDCON.2010.5712595,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5712595,Optical character recognition;Tesseract;Ticker text;Video analytics,Character recognition;Image segmentation;Monitoring;Optical character recognition software;Pixel;Text recognition;Videos,feature extraction;monitoring;optical character recognition;text analysis;video signal processing,Bangla news video;Tesseract OCR;monitoring process system;text recognition;ticker text extraction,,0,,10,,no,17-19 Dec. 2010,,IEEE,IEEE Conference Publications
Time efficient method for automated antenna design for wireless energy harvesting,H. J. Visser; R. J. M. Vullers,"Imec / Holst Centre, PO Box 8550, 5605 KN Eindhoven, The Netherlands",2010 Loughborough Antennas & Propagation Conference,20101213,2010,,,433,436,"The rectifier circuit in a rectenna (rectifying antenna) is analyzed employing a fast, efficient time-marching algorithm. The thus found complex input impedance dictates the antenna design. To maximize RF-to-DC conversion efficiency we do not want to employ an impedance matching and filtering network. Instead, we require the input impedance of the antenna to be equal to the complex conjugate value of the input impedance of the rectifier circuit. One antenna type feasible of supplying the required complex input impedance is the wire folded dipole array antenna. For this antenna type, an efficient, analytic model has been developed. The fast calculation times when implemented in software allow for an automated antenna design employing a Genetic Algorithm optimization. Thus, antenna designs can be generated within minutes employing standard office computing equipment. The design of a complete rectenna can be accomplished within hours. The direct complex conjugate matching ensures that the design is power-efficient and physically compact.",,Electronic:978-1-4244-7307-6; POD:978-1-4244-7304-5,10.1109/LAPC.2010.5666213,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5666213,,Dipole antennas;Impedance;Mathematical model;Schottky diodes;Wire,dipole antenna arrays;genetic algorithms;rectennas;rectifiers;wire antennas,RF to DC conversion efficiency;automated antenna design;genetic algorithm optimization;rectenna;rectifier circuit;time efficient method;wire folded dipole array antenna;wireless energy harvesting,,4,,7,,no,8-9 Nov. 2010,,IEEE,IEEE Conference Publications
Tourism Planning Assessment Based on Analytic Hierarchy Process,X. Hong-yong; H. Song,"Dept. of Tourism, Chongqing Educ. Coll., Chongqing, China",2010 Third International Conference on Knowledge Discovery and Data Mining,20100318,2010,,,351,354,"The assessment of tourism planning is significant to tourism development. As expert evaluation is short of scientificity, analytic hierarchy process is applied to assessment for tourism planning in the paper. Assessment indexes of tourism planning are analyzed, assessment indexes of tourism planning are composed of infrastructure and investment cost, social and natural environment, and passenger origin condition and expected income. Then, assessment model of tourism planning is established. Weiyuan reservoir basin in Jinggu county, Yunnan province is used as our application object. The experimental results indicate the effectiveness of analytic hierarchy process.",,Electronic:978-1-4244-5398-6; POD:978-1-4244-5397-9,10.1109/WKDD.2010.78,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5432592,analytic hierarchy process;assessment model;hierarchic structure;tourism planning,Application software;Computer science education;Costs;Data mining;Educational institutions;Educational technology;Investments;Process planning;Reservoirs;Technology planning,costing;decision making;investment;planning;travel industry,Weiyuan reservoir basin;analytic hierarchy process;assessment index;expected income;infrastructure cost;investment cost;natural environment;passenger origin condition;social environment;tourism development;tourism planning assessment,,0,,5,,no,9-10 Jan. 2010,,IEEE,IEEE Conference Publications
Towards a Foundation for Quantitative Service Analysis An Approach from Business Process Models,N. Kulkarni; D. Parachuri; S. Trivedi,"Software Eng. & Technol. Labs., Infosys Technol. Ltd., Bangalore, India",2010 IEEE International Conference on Services Computing,20100826,2010,,,618,620,"Recent advancements in process centric systems, the concept of Services have been adopted widely over business processes. Services which are discrete reusable functional blocks have become the choice to achieve highest order of reuse. However, key to having such services is to identify them with right objectives. Current methods and techniques for service analysis are heuristic and rely heavily on the experience and intuition of the designer. We present the prerequisite formalism for service analysis from business process models and basic quantitative metrics.",,Electronic:978-0-7695-4126-6; POD:978-1-4244-8147-7,10.1109/SCC.2010.59,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5557286,Business Process;Data analytics;Service Analysis,Atmospheric measurements;Business;Complexity theory;Couplings;Indexes;Particle measurements,business process re-engineering;software architecture;software metrics,business process models;discrete reusable functional blocks;process centric systems;quantitative metrics;quantitative service analysis;service analysis,,0,,7,,no,5-10 July 2010,,IEEE,IEEE Conference Publications
Trust Model of Software Behaviors Based on Check Point Risk Evaluation,J. Tian; J. Feng,"Coll. of Math. & Comput. Sci., HeBei Univ., Baoding, China",2010 Third International Symposium on Information Science and Engineering,20110711,2010,,,54,57,"In order to evaluate the credibility of software behaviors more reasonably and accurately, a trust model of software behaviors based on check point risk evaluation is presented. Firstly, adopt fuzzy analytic hierarchy process (FAHP) to figure out the weight of every risk factor of check point, meanwhile, assess the modules value according to Markov Chain Usage Model, thus calculating out the risk value of check point and accumulating every suspected risk check point, and then adopt rewarding or punishment mechanism to evaluate a software behaviors trustworthy, which can judge whether software behaviors are credible or not. The simulated experiment shows that this model can distinguish the potential risk effectively in software behaviors, evaluate the risk value trustworthy and provide objective and reliable information to judge whether software behaviors are credible or not.",2160-1283;21601283,Electronic:978-0-7695-4360-4; POD:978-1-61284-428-2,10.1109/ISISE.2010.79,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5945050,Check Point;FAHP;Markov Chain Usage Model;Risk Evaluation;Software Behaviors,Equations;Markov processes;Mathematical model;Monitoring;Security;Software systems,Markov processes;decision making;software engineering,FAHP;Markov Chain;check point risk evaluation;credibility;fuzzy analytic hierarchy process;software behavior;trust model,,0,,24,,no,24-26 Dec. 2010,,IEEE,IEEE Conference Publications
"Twitter analytics: Architecture, tools and analysis",R. D. W. Perera; S. Anand; K. P. Subbalakshmi; R. Chandramouli,"CERDEC, Ft Monmouth, NJ 07703-5113",2010 - MILCOM 2010 MILITARY COMMUNICATIONS CONFERENCE,20110106,2010,,,2186,2191,"We study the temporal behavior of messages arriving in a social network. We specifically study the tweets and re-tweets sent to president Barack Obama on Twitter. We characterize the inter-arrival times between the tweets, the number of re-tweets and the spatial coordinates (latitude, longitude) of the users who sent the tweets. The modeling of the arrival process of tweets in Twitter can be applied to predict co-ordinated user behavior in social networks. While there is sufficient literature on social networks that present large volumes of collected data, the modeling and characterization of the data have been rarely discussed. The available data are usually very expensive and not comprehensive. Here, we develop a software architecture that uses a Twitter application program interface (API) to collect the tweets sent to specific users. We then extract the user ids and the exact time-stamps of the tweets. We use the collected data to characterize the inter-arrival times between tweets and the number of re-tweets. Our studies indicate that the arrival process of new tweets to a user can be modeled as a Poisson Process while the number of re-tweets follow a geometric distribution. Our data collection architecture is operating system (OS) independent. The results obtained in this research can be applied to study correlations between patterns of user behavior and their locations.",2155-7578;21557578,Electronic:978-1-4244-8180-4; POD:978-1-4244-8178-1,10.1109/MILCOM.2010.5680493,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5680493,,Accuracy;Computer architecture;Data models;Queueing analysis;Stochastic processes;Twitter,application program interfaces;geometry;social networking (online);stochastic processes,Poisson process;Twitter analytics;application program interface;coordinated user behavior;geometric distribution;social network,,3,,15,,no,Oct. 31 2010-Nov. 3 2010,,IEEE,IEEE Conference Publications
Understanding text corpora with multiple facets,L. Shi; F. Wei; S. Liu; L. Tan; X. Lian; M. X. Zhou,"IBM Research - China, 19 Zhongguancun Software Park, Beijing 100193, China",2010 IEEE Symposium on Visual Analytics Science and Technology,20101210,2010,,,99,106,"Text visualization becomes an increasingly more important research topic as the need to understand massive-scale textual information is proven to be imperative for many people and businesses. However, it is still very challenging to design effective visual metaphors to represent large corpora of text due to the unstructured and high-dimensional nature of text. In this paper, we propose a data model that can be used to represent most of the text corpora. Such a data model contains four basic types of facets: time, category, content (unstructured), and structured facet. To understand the corpus with such a data model, we develop a hybrid visualization by combining the trend graph with tag-clouds. We encode the four types of data facets with four separate visual dimensions. To help people discover evolutionary and correlation patterns, we also develop several visual interaction methods that allow people to interactively analyze text by one or more facets. Finally, we present two case studies to demonstrate the effectiveness of our solution in support of multi-faceted visual analysis of text corpora.",,Electronic:978-1-4244-9487-3; POD:978-1-4244-9488-0,10.1109/VAST.2010.5652931,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5652931,multi-facet data visualization;text visualization,Correlation;Data mining;Data models;Data visualization;Layout;Navigation;Visualization,data models;data visualisation;text analysis;user interfaces,data facets;data model;text analysis;text corpora;text visualization;visual analysis;visual interaction method,,19,2,29,,no,25-26 Oct. 2010,,IEEE,IEEE Conference Publications
Using Cloud Technologies to Optimize Data-Intensive Service Applications,D. Habich; W. Lehner; S. Richly; U. Assmann,"Database Technol. Group, Dresden Univ. of Technol., Dresden, Germany",2010 IEEE 3rd International Conference on Cloud Computing,20100826,2010,,,19,26,"The role of data analytics increases in several application domains to cope with the large amount of captured data. Generally, data analytics are data-intensive processes, whose efficient execution is a challenging task. Each process consists of a collection of related structured activities, where huge data sets have to be exchanged between several loosely coupled services. The implementation of such processes in a service-oriented environment offers some advantages, but the efficient realization of data flows is difficult. Therefore, we use this paper to propose a novel SOA-aware approach with a special focus on the data flow. The tight interaction of new cloud technologies with SOA technologies enables us to optimize the execution of data-intensive service applications by reducing the data exchange tasks to a minimum. Fundamentally, our core concept to optimize the data flows is found in data clouds. Moreover, we can exploit our approach to derive efficient process execution strategies regarding different optimization objectives for the data flows.",2159-6182;21596182,Electronic:978-0-7695-4130-3; POD:978-1-4244-8207-8,10.1109/CLOUD.2010.56,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5558140,data cloud;data-intensive;service applications,Business;Clouds;Computational modeling;Semantics;Service oriented architecture;Simple object access protocol,Internet;data analysis;data flow analysis;optimisation;software architecture,cloud technologies;data analytics;data flow;data intensive service applications optimization;service oriented environment,,4,,15,,no,5-10 July 2010,,IEEE,IEEE Conference Publications
Utilization of DLT Theory in 2D Image Analysis System,K. Bai; Chengyin; Liuqin; Gaoming,"Health Inst., Wuhan Inst. of Phys. Educ., Wuhan, China",2010 Second World Congress on Software Engineering,20110222,2010,1,,279,282,"This paper analyzed several mistakes in the traditional 2D analytic theories and then introduced in detail a new method basing on DLT theory from two aspects: the foundation stone of new 2D analytic theories, program design, and also utilized a direct inspection method to analyze the errors between the new method and the old one. The results showed that new method could obtain more accurate than the traditional one.",,Electronic:978-0-7695-4303-1; POD:978-1-4244-9287-9,10.1109/WCSE.2010.108,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5718313,2D analytic system;DLT;camera frame,Calibration;Cameras;Coordinate measuring machines;Inspection;Joints;Measurement uncertainty;Three dimensional displays,image motion analysis,2D image analysis system;DLT theory;direct inspection method;program design,,0,,6,,no,19-20 Dec. 2010,,IEEE,IEEE Conference Publications
VACCINATED ‰ÛÓ Visual analytics for characterizing a pandemic spread VAST 2010 Mini Challenge 2 award: Support for future detection,A. Malik; S. Afzal; E. Hodgess; D. S. Ebert; R. Maciejewski,Purdue Visual Analytics Center - Purdue University,2010 IEEE Symposium on Visual Analytics Science and Technology,20101210,2010,,,281,282,"Given a set of hospital admittance and death records, the challenge was to characterize the spread of a pandemic in terms of the attack and mortality rates, spatiotemporal patterns of onset and the recovery time. We began the analysis by preprocessing the hospital admittance records using the University of Pittsburgh's CoCo classifier. CoCo is a text classification software that takes hospital admittance fields and classifies them into chief complaint categories (Botulinic, Constitutional, Gastrointestinal, Hemorrhagic, Neurological, Rash, Respiratory, and Other). The choice of the CoCo classifier was based on its online availability as well as its well documented classification performance metrics, see. Once the data was classified, we utilized and extended work done by the Purdue University Visual Analytics Center on healthcare analysis. Our system consists of a combination of linked views, showing time series views of syndromes and death rates through line graph views (Figure 1 - Top), stacked graph views showing deaths (Figure 1 - Bottom), geographical map views showing the impact by country (not illustrated in this paper), and summary windows providing statistical breakdowns of the data (not illustrated in this paper). All views are linked through an interactive time slider that allows users to explore the data over time. Extensions to our previous work include the stacked graph view, summary windows, new control chart methods, and an interactive `tape measure' tool.",,Electronic:978-1-4244-9487-3; POD:978-1-4244-9488-0,10.1109/VAST.2010.5652661,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5652661,,,classification;data analysis;data visualisation;health care;medical administrative data processing;text analysis,CoCo classifier;Purdue University visual analytics center;University of Pittsburgh;VACCINATED;VAST 2010 Mini Challenge 2 Award;chief complaint categories;death records;documented classification performance metrics;geographical map views;healthcare analysis;hospital admittance;mortality rates;pandemic spread;spatiotemporal patterns;stacked graph views;text classification software;visual analytics,,1,,3,,no,25-26 Oct. 2010,,IEEE,IEEE Conference Publications
Vision-Based Autonomous Vehicle Guidance for Indoor Security Patrolling by a SIFT-Based Vehicle-Localization Technique,K. C. Chen; W. H. Tsai,"Research Center for Information Technology Innovation, Academia Sinica, Taipei , Taiwan",IEEE Transactions on Vehicular Technology,20100913,2010,59,7,3261,3271,"A novel method for guidance of vision-based autonomous vehicles for indoor security patrolling using scale-invariant feature transformation (SIFT) and vehicle localization techniques is proposed. Along-path objects to be monitored are used as landmarks for vehicle localization. The localization work is accomplished by three steps: SIFT-based object image feature matching, 2-D affine transformation using the Hough transform, and analytic 3-D space transformation. Object monitoring can be simultaneously achieved during the vehicle-localization process, and most planar-surfaced objects can be utilized in the process, greatly enhancing the applicability of the proposed method. Vehicle trajectory deviations from the planned path due to mechanic error accumulation are also estimated by setting up a calibration line on the monitored object image and applying the 3-D space transformation. Moreover, a path-correction technique is proposed to conduct a path adjustment and guide the vehicle to navigate to the next path node. Analysis of the accuracy of the vehicle-localization and path-correction results is finally included. The experimental results show that the proposed method, utilizing only a single view of each object, can guide the vehicle to navigate accurately and monitor objects successfully.",0018-9545;00189545,,10.1109/TVT.2010.2052079,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5482146,3-D space transformation;Autonomous vehicle;computer vision;guidance;landmarks;planar-surfaced objects;scale-invariant feature transformation (SIFT);security patrolling;vehicle localization,Application software;Computer science;Computer security;Computer vision;Mobile robots;Monitoring;Navigation;Permission;Remotely operated vehicles;Space vehicles,Hough transforms;image matching;mobile robots;object detection;path planning;robot vision;security;vehicles,2D affine transformation;Hough transform;analytic 3D space transformation;indoor security patrolling;mechanic error accumulation;object image feature matching;object monitoring;path-correction technique;scale-invariant feature transformation;vehicle trajectory deviations;vehicle-localization technique;vision-based autonomous vehicle guidance,,13,,26,,no,Sept. 2010,,IEEE,IEEE Journals & Magazines
"Visual analysis of large graphs using (X,Y)-clustering and hybrid visualizations",V. Batagelj; W. Didimo; G. Liotta; P. Palladino; M. Patrignani,"University of Ljubljana, Slovenia",2010 IEEE Pacific Visualization Symposium (PacificVis),20100311,2010,,,209,216,"Many different approaches have been proposed for the challenging problem of visually analyzing large networks. Clustering is one of the most promising. In this paper we propose a new goal for clustering that is especially tailored to hybrid-visualization tools. Namely, that of producing both intra-cluster graphs and inter-cluster graph that are suitable for highly-readable visualizations within different representation conventions. We formalize this concept in the (X,Y)-clustering framework, where Y is the class that defines the desired topological properties of intra-cluster graphs and X is the class that defines the desired topological properties of the inter-cluster graph. By exploiting this approach hybrid-visualization tools can effectively combine different node-link and matrix-based representations, allowing the users to interactively explore the graph by expansion/contraction of clusters without loosing their mental map. As a proof of concept, we describe the system VHYXY (Visual Hybrid (X,Y)-clustering) that integrates our techniques and we present the results of case studies to the visual analysis of co-authorship networks.",2165-8765;21658765,Electronic:978-1-4244-6686-3; POD:978-1-4244-6685-6,10.1109/PACIFICVIS.2010.5429591,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5429591,Graph Clustering;Hybrid Visualization;Large Graphs;Visual Analytics,Algorithm design and analysis;Application software;Computer interfaces;Data visualization;Humans;Information systems;Inspection;Mathematics;Pattern recognition;Visual analytics,data visualisation;graph theory;mathematics computing;network theory (graphs);pattern clustering,"(X,Y)-clustering framework;co-authorship networks;hybrid-visualization tools;inter-cluster graph;intra-cluster graph;matrix-based representation;network visual analysis;node-link based representation;topological property;visual hybrid (X,Y)-clustering",,3,,41,,no,2-5 March 2010,,IEEE,IEEE Conference Publications
Visual analytics of a pandemic spread: VAST 2010 Mini Challenge 2 award: Thorough description of analytic process,A. Astefanoaie; R. Bozianu; M. Broghammer; R. Jungnickel; C. Rohrdantz; J. Schniertshauer; D. Spretke; P. Bak,University of Konstanz,2010 IEEE Symposium on Visual Analytics Science and Technology,20101210,2010,,,277,278,"In this paper, automated medical data analysis and visualisation are discussed. The task of the VAST 2010 Mini Challenge 2 was to characterize the spread of an epidemic outbreak. The analysis should take into consideration symptoms, mortality rates and temporal patterns of the disease. Finally, the outbreak should be compared across different locations searching for anomalies. For the preprocessing and the automated analysis of the data we used the Konstanz Information Miner (KINME), which is a modular data exploration platform that enables the user to visually create dataflows, the R software for statistical computing and some selfwritten Java programs for data preprocessing. For conducting the visual investigations we applied Many Eyes and Protovis, which compose scalable and customized views for datasets of interest.",,Electronic:978-1-4244-9487-3; POD:978-1-4244-9488-0,10.1109/VAST.2010.5653071,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5653071,,,Java;data analysis;data mining;data visualisation;diseases;medical administrative data processing,Java program;Konstanz information miner;Many Eyes3;Protovis4;R2 software;VAST 2010 mini challenge 2;automated data analysis;data preprocessing;disease pattern;modular data exploration platform;mortality rates;pandemic spread;statistical computing;visual analytics process,,0,,4,,no,25-26 Oct. 2010,,IEEE,IEEE Conference Publications
Visualising Computational Intelligence Through Converting Data into Formal Concepts,S. Andrews; C. Orphanides; S. Polovina,"Conceptual Struct. Res. Group, Sheffield Hallam Univ., Sheffield, UK","2010 International Conference on P2P, Parallel, Grid, Cloud and Internet Computing",20110113,2010,,,302,307,"Formal Concept Analysis (FCA) is an emerging data technology that complements collective intelligence such as that identified in the Semantic Web by visualising the hidden meaning in disparate and distributed data. The paper demonstrates the discovery of these novel semantics through a set of FCA open source software tools FcaBedrock and In-Close that were developed by the authors. These tools add computational intelligence by converting data into a Boolean form called a Formal Context, prepare this data for analysis by creating focused and noise-free sub-Contexts and then analyse the prepared data using a visualisation called a Concept Lattice. The Formal Concepts thus visualised highlight how data itself contains meaning, and how FCA tools thereby extract data's inherent semantics. The paper describes how this will be further developed in a project called CUBIST, to provide in-data-warehouse visual analytics for RDF-based triple stores.",,Electronic:978-0-7695-4237-9; POD:978-1-4244-8538-3,10.1109/3PGCIC.2010.50,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5662774,Concept Lattice;FCA;Formal Concept;Formal Concept Analysis;Formal Context;Galois connection;RDF;Semantic Web;attributes;data warehousing;disparate data;distributed data;in-warehouse analytics;objects;visualisation,,data analysis;data visualisation;semantic Web;software tools,Boolean form;CUBIST;FCA open source software tools;FcaBedrock;In-Close;RDF-based triple stores;collective intelligence;computational intelligence;concept lattice;data technology;focused sub-contexts;formal concept analysis;in-data-warehouse visual analytics;noise-free sub-contexts;semantic Web,,1,,20,,no,4-6 Nov. 2010,,IEEE,IEEE Conference Publications
VisWorks text and network visual analytics: VAST 2010 Mini Challenge 1 award: ‰ÛÏEffective interactive visualization of document contents‰Ûù,L. Shi; W. Qian; F. Wei; L. Tan,IBM Research - China,2010 IEEE Symposium on Visual Analytics Science and Technology,20101210,2010,,,269,270,"VisWorks is a software package for text and network visual analytics. This paper introduces its visualization, analytic process and lesson learned in solving Mini-Challenge 1 of VAST 2010 contest.",,Electronic:978-1-4244-9487-3; POD:978-1-4244-9488-0,10.1109/VAST.2010.5651204,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5651204,,,data analysis;data visualisation;text analysis,VisWorks;analytic process;document contents;interactive visualization;network visual analytics;software package;text visual analytics,,0,,2,,no,25-26 Oct. 2010,,IEEE,IEEE Conference Publications
VMCAnalytic: Developing a Collaborative Video Analysis Tool for Education Faculty and Practicing Educators,G. Agnew; C. M. Mills; C. A. Maher,"Rutgers Univ., Piscataway, NJ, USA",2010 43rd Hawaii International Conference on System Sciences,20100311,2010,,,1,10,"This paper describes the genesis, design and prototype development of the VMCAnalytic, a repository-based video annotation and analysis tool for education. The VMCAnalytic is a flexible, extensible analytic tool that is unique in its integration into an open source repository architecture to transform a resource discovery environment into an interactive collaborative where practicing teachers and faculty researchers can analyze and annotate videos to support a range of needs from longitudinal research to improving individual teaching performance. This paper presents an overview of the design and functionality of the VMCAnalytic, which is a key component of the NSF-funded Video Mosaic Collaborative (VMC), together with a description of the underlying repository service architecture. The paper also describes the synergistic collaboration between digital library technologists and education researchers to build a research environment that can integrate with the VMCAnalytic tool to create a digital collaboration space. The prototype tool is available in as of January 2010 at the VMC website: www.video-mosaic.org.",1530-1605;15301605,Electronic:978-1-4244-5510-2; POD:978-1-4244-5509-6,10.1109/HICSS.2010.438,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5428756,,Collaborative tools;Education;Educational institutions;International collaboration;Mathematics;Performance analysis;Prototypes;Software libraries;Space technology;Videoconference,educational computing;groupware;public domain software;software architecture;software prototyping;teaching;video communication,NSF funded video mosaic collaborative;VMCAnalytic;collaborative video analysis;digital library;education faculty;education researchers;faculty researchers;interactive collaborative environment;open source repository architecture;prototype development;repository service architecture;resource discovery environment;teachers,,1,,18,,no,5-8 Jan. 2010,,IEEE,IEEE Conference Publications
White's Three Disciplines and Relative Valuation Order: Countering the Social Ignorance of Automated Data Collection and Analysis,S. McDermott,"Inst. of Commun. Studies, Univ. of Leeds, Leeds, UK",2010 International Conference on Advances in Social Networks Analysis and Mining,20100907,2010,,,72,79,"This paper asks which of White's (2009) three disciplines and relative valuation orders does the Singapore blogosphere adhere to. Analysing not just the hyperlink connections but the textual discourse; and in doing so attempts to highlight certain limitations of using automated data mining and analysis software. Using the Singapore blogosphere, described by Lin, Sundaram, Chi, Tatemura, and Tseng, (2006) and Hurst (2006), as an isolated and distinct network with no theme or focus, I have targeted blogs using social network analysis uncovering the key players, with higher levels of `betweenness centrality' (de Nooy & Mrvar et al., 2005) and the themes and discipline of the Singapore blogosphere. This case study will help highlight the analytic framework, benefits and limitations of using social network analysis and an ethnographical approach to networks. This paper also highlights the use of various software technology; blogs, IssueCrawler, HTTrack, NetDraw, and Leximancer while using an ethnographic approach to counter the social ignorance of automated electronic software.",,Electronic:978-0-7695-4138-9; POD:978-1-4244-7787-6,10.1109/ASONAM.2010.16,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562788,data mining;disciplines;semantic network analysis;social ignorance;social network analysis,Blogs;Context;Cost accounting;Data mining;Semantics;Social network services;Software,data mining;semantic Web;social networking (online),HTTrack;IssueCrawler;Leximancer;NetDraw;Singapore blogosphere;automated analysis software;automated data mining;automated electronic software;ethnographical approach;hyperlink connection;relative valuation order;social ignorance;social network analysis;software technology;textual discourse;whites three disciplines,,0,,39,,no,9-11 Aug. 2010,,IEEE,IEEE Conference Publications
A component and query approach for scientific data exploration applications,A. SantanchÌ¬; P. Baumann; D. Costa; N. Oliveira,"Institute of Computing (IC), University of Campinas - UNICAMP, Campinas, Brazil",2011 IEEE International Conference on Service-Oriented Computing and Applications (SOCA),20120309,2011,,,1,8,"The immense increase of scientific databases, combined with the spread of open standards for service interfaces, has resulted in unprecedented opportunities of data exploration and exploitation for scientists. From a client point of view, scientists often perform data combination and analysis, based on interactive incremental refinement and visualization. While this abstract description applies to virtually any science, the concrete presentation of data and queries offered varies greatly across the individual domains. The result is a repetitive effort in crafting streamlined client interfaces, which is particular tedious in face of interactive graphics. We address this problem by establishing a modular Web client toolkit offering common analysis input and output widgets. The backend generates database queries from the input and displays query results through the output widgets. We present this toolkit, its first implementation version, and an earth science scenario using an Array Database System as retrieval backend.",2163-2871;21632871,Electronic:978-1-4673-0319-4; POD:978-1-4673-0318-7,10.1109/SOCA.2011.6166204,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6166204,digital content component;geo services;microformats;ogc-wcps;scientific analytics,Database languages;Servers;Software;Standards;Synchronization;Unified modeling language;Visualization,Internet;data analysis;data visualisation;interactive systems;query languages;query processing;scientific information systems,array database system;data analysis;data combination;data exploitation;data retrieval;data visualization;database query;earth science scenario;interactive graphics;interactive incremental refinement;modular Web client toolkit;scientific data exploration application;scientific databases;service interfaces;streamlined client interface,,0,,32,,no,12-14 Dec. 2011,,IEEE,IEEE Conference Publications
A decision model for agile software release,X. Wang; L. Zou,"Department of Medical Information Engineering, Zunyi City, Guizhou Province, China","The Proceedings of 2011 9th International Conference on Reliability, Maintainability and Safety",20110811,2011,,,766,770,"Analyzing the agile software development process, combining the evolution laws of agile software project, applying AHP method, an integration assessment model for agile software release is presented. The index system for agile software release plans is built, and its target layer, criterion layer and scheme layer are specified. Comparison matrix and judgment matrix are constructed with three-demarcation analytic hierarchy process (TDAHP). Combining with the views of agile experts about index matrix and weight, the qualitative and quantitative analysis about software release scheme can be carried out, and an optimal scheme can be selected. Finally, a numerical example is given, which shown this model is scientific and feasible.",,Electronic:978-1-61284-666-8; POD:978-1-61284-667-5,10.1109/ICRMS.2011.5979368,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5979368,AHP;Agile development process;evaluation index;software release,Eigenvalues and eigenfunctions;Indexes;Matrix converters;Numerical models;Planning;Programming;Software,decision making;matrix algebra;software engineering,TDAHP;agile software project;agile software release;comparison matrix;decision model;integration assessment model;judgment matrix;three-demarcation analytic hierarchy process,,1,,14,,no,12-15 June 2011,,IEEE,IEEE Conference Publications
A Framework for Discovering Internal Financial Fraud Using Analytics,P. K. Panigrahi,"Inf. Syst. Dept., Indian Inst. of Manage. Indore, Indore, India",2011 International Conference on Communication Systems and Network Technologies,20110728,2011,,,323,327,"In today's knowledge based society, financial fraud has become a common phenomenon. Moreover, the growth in knowledge discovery in databases and fraud audit has made the detection of internal financial fraud a major area of research. On the other hand, auditors find it difficult to apply a majority of techniques in the fraud auditing process and to integrate their domain knowledge in this process. In this Paper a framework called ""Knowledge-driven Internal Fraud Detection (KDIFD)"" is proposed for detecting internal financial frauds. The framework suggests a process-based approach that considers both forensic auditor's tacit knowledge base and computer-based data analysis and mining techniques. The proposed framework can help auditor in discovering internal financial fraud more efficiently.",,Electronic:978-0-7695-4437-3; POD:978-1-4577-0543-4,10.1109/CSNT.2011.74,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5966462,Data Mining;Forensic Data Analysis;Internal financial fraud;Knowledge-driven internal fraud detection,Data analysis;Data mining;Data structures;Databases;Forensics;Knowledge based systems;Software,data analysis;data mining;financial data processing;fraud,KDIFD;computer based data analysis;computer based data mining techniques;forensic auditors;fraud audit;fraud auditing process;internal financial fraud discovery;knowledge discovery;knowledge driven internal fraud detection,,1,,17,,no,3-5 June 2011,,IEEE,IEEE Conference Publications
A Framework for Efficient Data Analytics through Automatic Configuration and Customization of Scientific Workflows,M. Hauder; Y. Gil; Y. Liu,"Inst. for Software & Syst. Eng., Univ. of Augsburg, Augsburg, Germany",2011 IEEE Seventh International Conference on eScience,20120109,2011,,,379,386,"Data analytics involves choosing between many different algorithms and experimenting with possible combinations of those algorithms. Existing approaches however do not support scientists with the laborious tasks of exploring the design space of computational experiments. We have developed a framework to assist scientists with data analysis tasks in particular machine learning and data mining. It takes advantage of the unique capabilities of the Wings workflow system to reason about semantic constraints. We show how the framework can rule out invalid workflows and help scientists to explore the design space. We demonstrate our system in the domain of text analytics, and outline the benefits of our approach.",,Electronic:978-0-7695-4597-4; Electronic:978-1-4577-2163-2; POD:978-1-4577-2163-2,10.1109/eScience.2011.59,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6123302,Data Analytics;Scientific Workflows,Algorithm design and analysis;Clustering algorithms;Correlation;Machine learning algorithms;Prediction algorithms;Software;Software algorithms,data analysis;data mining;learning (artificial intelligence);natural sciences computing;text analysis;workflow management software,Wings workflow system;computational experiment;data analysis;data analytics;data mining;machine learning;scientific workflow configuration;scientific workflow customization;text analytics,,2,1,25,,no,5-8 Dec. 2011,,IEEE,IEEE Conference Publications
A framework for evidence-based health care incentives simulation,J. P. Bigus; C. H. Chen-Ritzo; R. Sorrentino,"IBM T.J. Watson Research Center, 1101 Kitchawan Road, Yorktown Heights, NY 10598, USA",Proceedings of the 2011 Winter Simulation Conference (WSC),20120209,2011,,,1103,1116,"We present a general simulation framework designed for modeling incentives in a health care delivery system. This first version of the framework focuses on representing provider incentives. Key framework components are described in detail, and we provide an overview of how data-driven analytic methods can be integrated with this framework to enable evidence-based simulation. The software implementation of a simple simulation model based on this framework is also presented.",0891-7736;08917736,Electronic:978-1-4577-2109-0; POD:978-1-4577-2108-3; USB:978-1-4577-2107-6,10.1109/WSC.2011.6147833,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6147833,,Analytical models;Biomarkers;Contracts;Data models;Diseases;Software,health care;incentive schemes;software engineering,data-driven analytic method;evidence-based health care incentive simulation;evidence-based simulation;health care delivery system;modeling incentive;simulation model;software implementation,,1,,21,,no,11-14 Dec. 2011,,IEEE,IEEE Conference Publications
A fuzzy approach to bilingual education assessment and analysis,Y. Ding; M. Tang,"School of Resources and Environmental Engineering, Jiangxi University of Science and Technology, Ganzhou 341000, People's Republic of China",2011 IEEE 3rd International Conference on Communication Software and Networks,20110908,2011,,,147,151,"This paper is concerned with the problem of applying fuzzy theory and analytic hierarchy process in bilingual education assessment and analysis. By the triangular fuzzy number theory, a bilingual education assessment model is developed. Based on this model, we can get the bilingual education assessment rating, and by analysis the estimation results, we can develop a scheme to guide the decision-makers to improve the bilingual education rating. Furthermore, the numerical example is given to show the effectiveness of the proposed theorems.",,Electronic:978-1-61284-486-2; POD:978-1-61284-485-5,10.1109/ICCSN.2011.6013797,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6013797,assessment;bilingual education;fuzzy theory;indices,Educational institutions;Indexes;Lead,decision making;education;fuzzy set theory;natural language processing,analytic hierarchy process;bilingual education analysis;bilingual education assessment model;bilingual education rating;decision maker;triangular fuzzy number theory,,0,,7,,no,27-29 May 2011,,IEEE,IEEE Conference Publications
A guarantee Model based on ANP for Implementing ‰ÛÏA plan for educating and training outstanding engineers‰Ûù,Niu Xiao; Hou Kaihu; Yang Lin; Zhong Jinyuan; Chen Lihua,"Faculty of Mechanical and Electronic Engineering, Kunming University of Science and Technology, 650093, China",Proceedings of 2011 International Conference on Computer Science and Network Technology,20120412,2011,1,,377,381,"Educating and training outstanding engineers involves the government, the institution of higher learning and enterprises. The chief problems of implementing ‰ÛÏA Plan for Educating and Training Outstanding Engineers (PETOE)‰Ûù are how to build an guarantee evaluation index system and determine the guarantee index weight and interaction among indexes. In this paper, the method of Analytic Network Process (ANP) were adopted, eleven indexes, such as the policy, capital and teachers involved in implementing guarantee were analyzed, the guarantee Model and supermatrix of implementing PETOE Based on ANP were built, the SD software was used to find out the convergence state of the associated supermatrix, the stable limit supermatrix, the index weights were calculated and their total sequences of affecting indexes was sorted. The results show that the guarantee model of implementing PETOE based on ANP is reasonable.",,Electronic:978-1-4577-1587-7; POD:978-1-4577-1586-0,10.1109/ICCSNT.2011.6181980,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6181980,ANP;PETOE;guarantee model,Bismuth;Computational modeling,engineering education;industrial training,PETOE;SD software;analytic network process;associated supermatrix;guarantee evaluation index system;guarantee model;higher learning;outstanding engineer education;outstanding engineer training;stable limit supermatrix,,0,,8,,no,24-26 Dec. 2011,,IEEE,IEEE Conference Publications
A Logistics model in natural disaster,Chen Ying-xin; Bai Shi-zhen,"School of Management, Harbin University of Commerce, China",MSIE 2011,20110204,2011,,,1265,1268,"An emergency logistics model based on Location-routing Problem (LRP) is proposed in this paper, and a two-phase heuristic algorithm is used to solve the model. It uses the minimization envelopment analysis method to solve the LRP in the emergency supplies center, and then calculates K shorter paths by the application of the nearest neighbor method. Considering the indexes of time, security, economic cost and environment cost of emergency transportation routing, the paper establishes a shortest-path with the method of Analytic Network Process (ANP). Based on the analysis and calculation of an example, the results indicate that the model and method are effective and feasible.",,Electronic:978-1-4244-8385-3; POD:978-1-4244-8383-9,10.1109/MSIE.2011.5707652,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5707652,analytic network process analysis;emergency logistics;location-routing problem,Analytical models;Biological system modeling;Clustering algorithms;Indexes;Logistics;Software;Transportation,decision making;disasters;emergency services;logistics,ANP;LRP;analytic network process;economic cost;emergency logistics model;emergency supplies center;emergency transportation routing;environment cost;location routing problem;logistics model;natural disaster;nearest neighbor method,,1,,6,,no,8-11 Jan. 2011,,IEEE,IEEE Conference Publications
A method for system auditing based on baseline assessment,J. Zhang; G. Xu; Y. Yang; S. Guo,"Key Laboratory of network and information attack & defense technology of MOE, Beijing University of Posts and Telecommunications, Beijing, P. R. China",2011 IEEE 3rd International Conference on Communication Software and Networks,20110908,2011,,,568,572,"Common Criteria (CC) provides only the standard for evaluating information security product or system. CC based evaluation on system auditing is considered crucial for the overall evaluation and in trouble without an effective method; however, the information system is a large-scale complex system. It includes many uncertain factors, as software, hardware, people and so on. As a result, information systems security risk is related to many ambiguous factors, what are difficult to measure, with ambiguity. In this paper, a method for system auditing based on baseline assessment was presented, In our method, analytic hierarchy process is introduced, which could be used to evaluate the security situation of information system.",,Electronic:978-1-61284-486-2; POD:978-1-61284-485-5,10.1109/ICCSN.2011.6014334,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014334,analytical hierarchy process(AHP);baseline assessment;configration auditing,Hardware;IP networks;Protocols;Security;Syntactics;XML,decision making;information systems;security of data,analytic hierarchy process;baseline assessment;common criteria;information security product;information system;large-scale complex system;system auditing,,0,,11,,no,27-29 May 2011,,IEEE,IEEE Conference Publications
A non-orthogonal SVD-based decomposition for phase invariant error-related potential estimation,R. Phlypo; N. Jrad; S. Rousseau; M. Congedo,"Vision and Brain Signal Processing Research Group at GIPSA-lab, Universities of Grenoble/CNRS UMR 5216, Grenoble, France",2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society,20111201,2011,,,6963,6966,"The estimation of the Error Related Potential from a set of trials is a challenging problem. Indeed, the Error Related Potential is of low amplitude compared to the ongoing electroencephalographic activity. In addition, simple summing over the different trials is prone to errors, since the waveform does not appear at an exact latency with respect to the trigger. In this work, we propose a method to cope with the discrepancy of these latencies of the Error Related Potential waveform and offer a framework in which the estimation of the Error Related Potential waveform reduces to a simple Singular Value Decomposition of an analytic waveform representation of the observed signal. The followed approach is promising, since we are able to explain a higher portion of the variance of the observed signal with fewer components in the expansion.",1094-687X;1094687X,Electronic:978-1-4577-1589-1; POD:978-1-4244-4121-1; USB:978-1-4244-4122-8,10.1109/IEMBS.2011.6091760,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6091760,Error Related Potential (ErrP);Hilbert transform;Singular Value Decomposition (SVD);analytic signal;phase plane,Electric potential;Electroencephalography;Estimation;Principal component analysis;Signal representations;Transforms;Vectors,electroencephalography;medical signal processing;singular value decomposition;waveform analysis,analytic waveform representation;electroencephalographic activity;nonorthogonal SVD-based decomposition;phase invariant error-related potential estimation;singular value decomposition,"Algorithms;Data Interpretation, Statistical;Electroencephalography;Evoked Potentials;Fourier Analysis;Humans;Models, Statistical;Principal Component Analysis;Reproducibility of Results;Signal Processing, Computer-Assisted;Signal-To-Noise Ratio;Software;Time Factors",0,,6,,no,Aug. 30 2011-Sept. 3 2011,,IEEE,IEEE Conference Publications
A QoS-Aware Service Evaluation Method for Co-selecting a Shared Service,D. Wanchun; L. Chao; Z. Xuyun; J. Chen,"State Key Lab. for Novel Software Technol., Nanjing Univ., Nanjing, China",2011 IEEE International Conference on Web Services,20110905,2011,,,145,152,"In service selection, an end user often has his or her personal preferences imposing on a candidate service's non-functional properties. For a service selection process promoted by a group of users, candidate services are often evaluated by a group of end users who may have different preferences or priorities. In this situation, it is often a challenging effort to make a tradeoff among various preferences or priorities of the users. In view of this challenge, a multi-criteria decision-making method, named AHP (Analytic Hierarchy Process), is introduced to transform both qualitative personal preferences and users' priorities into numeric weights. Furthermore, a QoS-aware service evaluation method is presented for a shared service's co-selection taking advantage of AHP theory. At last, a case study is presented to demonstrate the feasibility of the method.",,Electronic:978-0-7695-4463-2; POD:978-1-4577-0842-8,10.1109/ICWS.2011.11,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6009383,AHP;Quality of Service (QoS);Web service,Availability;Collaboration;Companies;Decision making;Quality of service;Web services,decision making;quality of service,AHP theory;QoS-aware service evaluation method;analytic hierarchy process;candidate service;end user;multicriteria decision making method;nonfunctional property;numeric weight;personal preference;service selection process;shared service selection,,7,,23,,no,4-9 July 2011,,IEEE,IEEE Conference Publications
A rank-reducing and division-free algorithm for inverse of square matrices,Wang Xingbo,"Department of Mechtronic Engineering, Foshan University, Guangdong Province, China",2011 IEEE International Workshop on Open-source Software for Scientific Computation,20120423,2011,,,17,21,"The paper puts forward a new direct algorithm for computing the inverse of a square matrix. The algorithm adopts a skill to compute the inverse of a regular matrix via computing the inverse of another lower-ranked matrix and contains neither iterations nor divisions in its computations-it is division-free. Compared with other direct algorithms, the new algorithm is easier to implement with either a recursive procedure or a recurrent procedure and has a preferable time complexity for denser matrices. Mathematical deductions of the algorithm are presented in detail and analytic formulas are exhibited for time complexity and spatial complexity. Also, the recursive procedure and the recurrent procedure are demonstrated for the implementation, and applications are introduced with comparative studies to apply the algorithm to tridiagonal matrices and bordered tridiagonal matrices.",,Electronic:978-1-61284-495-4; POD:978-1-61284-492-3,10.1109/OSSC.2011.6184687,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6184687,Direct method;Division-free;Matrix inverse;Rank-reducing,Algorithm design and analysis;Complexity theory;Computational efficiency;Matrices;Signal processing algorithms;Software algorithms,computational complexity;matrix algebra,denser matrices;division free algorithm;rank reducing algorithm;recurrent procedure;recursive procedure;spatial complexity;square matrices;time complexity;tridiagonal matrices,,0,,10,,no,12-14 Oct. 2011,,IEEE,IEEE Conference Publications
A review of the research on quantitative reliability Prediction and Assessment for electronic components,Yang Zhao; Xiaoyan Yin; R. Kang; K. S. Trivedi,"School of Reliability and Systems Engineering, Beihang University, China",2011 Prognostics and System Health Managment Confernece,20110704,2011,,,1,7,"A review is carried out on how quantitative approaches have been applied so far to the Reliability Prediction and Assessment (RPA) for computer and communication systems. A series of the reliability evaluation technology based on analytic models and computer simulations are developed for use in product design and test, shape, system operation and maintenance, during a research initiative towards understanding the quantitative characteristics of hardware and software systems. The implementation of these techniques guarantees that the sophisticated system satisfying the high standard on reliability, availability, performability, maintainability and safety. Such approaches conduct quantitative assessment for reliability at the system level. Such reliability quantitative assessments are effectively used in the system decision-making for fault detection, failures elimination, optimization, maintenance and safety.",2166-563X;2166563X,Electronic:978-1-4244-7950-4; POD:978-1-4244-7951-1; USB:978-1-4244-7949-8,10.1109/PHM.2011.5939553,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5939553,Assessment;Prediction;Reliability,Aerospace engineering;Companies;Continuous wavelet transforms;Random access memory;Software;Software reliability,decision making;failure analysis;life testing;reliability;standards,electronic components;failures elimination;fault detection;maintenance;optimization;reliability evaluation technology;reliability prediction and assessment;safety;system decision-making,,0,,55,,no,24-25 May 2011,,IEEE,IEEE Conference Publications
A software toolkit for visualizing enterprise routing design,X. Sun; Jinliang Wei; S. G. Rao; G. G. Xie,"School of Electrical and Computer Engineering, Purdue University, USA",2011 4th Symposium on Configuration Analytics and Automation (SAFECONFIG),20111226,2011,,,1,8,"Routing design is widely considered as one of the most challenging parts of enterprise network design. The challenges come from the typical large scale of such networks, the diverse objectives to meet through design, and a wide variety of protocols and mechanisms to choose from. As a result network operators often find it difficult to understand and trouble-shoot the routing design of their networks. Furthermore, today's common practice of focusing on one router or one protocol at a time makes it a onerous task to reason about the network-wide routing behavior.We believe that to mitigate the problem, there is a need for software tools to produce effective visualization of enterprise routing designs. In this paper we report on our experience building such a toolkit. We begin by abstracting various routing mechanisms into a small number of design primitives. The abstraction allows for a more concise representation of routing design without losing important design information, which is critical for making the tool scalable. Guided by the abstraction, we then develop a set of algorithms and heuristics, which take router configuration files as input and output a graphical representation of the routing design. The layout and components of the graph are highly customized to optimize its readability and power to infer the network-wide design pattern. We also present a case study using the toolkit to analyze the routing design of two large campus networks, and report on our findings. Our experience confirms the effectiveness of our toolkit in revealing key design characteristics of the networks, and in illustrating the network-wide routing behavior.",,Electronic:978-1-4673-0402-3; POD:978-1-4673-0401-6,10.1109/SafeConfig.2011.6111664,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6111664,,Joining processes;Logic gates;Peer to peer computing;Routing;Routing protocols;Visualization,data visualisation;design engineering;educational administrative data processing;software tools,campus networks;enterprise network design;enterprise routing design visualization;network-wide design pattern;network-wide routing behavior;router configuration files;routing design graphical representation;software toolkit,,0,,7,,no,Oct. 31 2011-Nov. 1 2011,,IEEE,IEEE Conference Publications
A study of hierarchical network security situation evaluation system for electric power enterprise based on Grey Clustering Analysis,Kunlun Gao; Ruzhi Xu; Yufei Wang; Yikang Li,"Information & Communication Department, China Electric Power Research Institute, Beijing, China",2011 International Conference on Computer Science and Service System (CSSS),20110804,2011,,,1990,1995,"Focused on the network security situation evaluation, a novel hierarchical evaluation system, which is based on Grey Clustering Analysis, is proposed. In this system, network attacks are classified into ""Strong"", ""Medium"", and ""Weak"" three harmful levels by Grey Clustering Analysis to construct a hierarchical index system. Then, the Analytic Hierarchy Process is used to calculate the impact factor of each network attack, and form the evaluation system to calculate the network security situation value. With Grey Clustering Analysis, harmful level ownership of each network attacks is determined, and each network attacks' influence can be truly reflected. Moreover, network security situation value's calculation speed can be improved by Analytic Hierarchy Process. Finally, large amounts of electric power on-site experiments indicated that the evaluation system is well performed by showing network security situation in both coarse-grained and fine-grained analysis.",,DVD:978-1-4244-9761-4; Electronic:978-1-4244-9763-8; POD:978-1-4244-9762-1,10.1109/CSSS.2011.5974463,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5974463,Analytic Hierarchy Process;Grained Analysis;Grey Clustering Analysis;Network Security Situation Assessment;Network Security Situation Value,Computer hacking;Guidelines;Indexes;Monitoring;Power systems;Software,decision making;grey systems;power system security;smart power grids,analytic hierarchy process;coarse-grained analysis;electric power enterprise;electric power on-site experiment;fine-grained analysis;grey clustering analysis;hierarchical index system;hierarchical network security situation evaluation system;network attack;network security situation value calculation speed,,0,,15,,no,27-29 June 2011,,IEEE,IEEE Conference Publications
A study on evaluation system of bridge technical condition based on fuzzy synthetic judgment,Huang Shengqian; Yang Yongqing,"School of Civil Engineering, Southwest Jiaotong University, Chengdu, China",2011 International Conference on Electric Technology and Civil Engineering (ICETCE),20110527,2011,,,4755,4758,"Bused on the fundamental principle of analytic hierarchy process, taking the simply supported reinforced concrete beam bridge as an example, the methods to select assess indexes and to establish evaluation model of bridge technical condition were expounded. The ways to determine the weights of indexes with credit degree method of group decision making in analytic hierarchy process, and to evaluate bridge technical condition with fuzzy synthetic judgment, were illustrated. In addition, the main functions and basic formations of the evaluation system for bridge technical condition, and the basic ideas to develop the related software were introduced. This system can provide decision basis for the maintenance of bridges and has extensive practicality. It can not only satisfy the technical condition evaluation for general bridges, but also add new evaluation models for other special bridges if necessary.",,Electronic:978-1-4577-0290-7; POD:978-1-4577-0289-1,10.1109/ICETCE.2011.5776052,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5776052,bridge technical condition;evaluation model;evaluation system;fuzzy syntheic judgment;software development,Bridges;Civil engineering;Concrete;Decision making;Educational institutions;Indexes;Presses,beams (structures);bridges (structures);decision making;fuzzy set theory;group theory;maintenance engineering;reinforced concrete,analytic hierarchy process;bridge maintenance;bridge technical condition;credit degree method;evaluation model;fuzzy synthetic judgment;group decision making;reinforced concrete beam bridge,,0,,6,,no,22-24 April 2011,,IEEE,IEEE Conference Publications
A Systematic Approach to Model Checking Human‰ÛÒAutomation Interaction Using Task Analytic Models,M. L. Bolton; R. I. Siminiceanu; E. J. Bass,"National Aeronautics and Space Administration Ames Research Center, San Jose State University Research Foundation, Moffet Field, CA, USA","IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans",20110818,2011,41,5,961,976,"Formal methods are typically used in the analysis of complex system components that can be described as ‰ÛÏautomated‰Ûù (digital circuits, devices, protocols, and software). Human-automation interaction has been linked to system failure, where problems stem from human operators interacting with an automated system via its controls and information displays. As part of the process of designing and analyzing human-automation interaction, human factors engineers use task analytic models to capture the descriptive and normative human operator behavior. In order to support the integration of task analyses into the formal verification of larger system models, we have developed the enhanced operator function model (EOFM) as an Extensible Markup Language-based, platform- and analysis-independent language for describing task analytic models. We present the formal syntax and semantics of the EOFM and an automated process for translating an instantiated EOFM into the model checking language Symbolic Analysis Laboratory. We present an evaluation of the scalability of the translation algorithm. We then present an automobile cruise control example to illustrate how an instantiated EOFM can be integrated into a larger system model that includes environmental features and the human operator's mission. The system model is verified using model checking in order to analyze a potentially hazardous situation related to the human-automation interaction.",1083-4427;10834427,,10.1109/TSMCA.2011.2109709,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5735232,Formal methods;human‰ÛÒautomation interaction;model checking;task analysis,Analytical models;Computational modeling;Humans;Object oriented modeling;Syntactics;Visualization;XML,XML;automobiles;computational linguistics;formal verification;human factors;man-machine systems;task analysis,analysis independent language;automobile cruise control;complex system component;enhanced operator function model;extensible markup language;formal method;formal semantics;formal syntax;formal verification;human factors;human operator behavior;human-automation interaction;model checking language;platform independent language;symbolic analysis laboratory;task analytic model,,31,,58,,no,Sept. 2011,,IEEE,IEEE Journals & Magazines
A universal analytic potential function applied to diatomic molecules,Y. Changfeng; W. Zhiwei,"Department of physics, School of Science Xi'an Polytechnic University Xi'an, Shaanxi, P.R. China",2011 IEEE 3rd International Conference on Communication Software and Networks,20110908,2011,,,105,110,"In this paper, a new method on constructing analytical potential energy functions is presented, and from this a analytical potential energy function applied to both neutral diatomic molecules and charged diatomic molecular ions is obtained. This potential energy function includes three dimensionless undetermined parameters which can be determined uniquely by solving linear equations with the experimental spectroscopic parameters of molecules. The solutions of the dimensionless undetermined parameters are real numbers rather than complex numbers, this ensures that the analytical potential energy function has extensive universality. Finally, the potential energy function is examined with four kinds of diatomic molecules or ions homonuclear neutral diatomic molecule H<sub>2</sub>(X<sup>1</sup> ë£<sub>g</sub><sup>+</sup>), homonuclear charged diatomic molecular ion, He<sub>2</sub><sup>+</sup>(X<sup>2</sup> ë£<sub>u</sub><sup>+</sup>) heternuclear neutral diatomic Molecule AlBr(A<sup>1</sup>ëÊ) and heternuclear charged diatomic Molecular ion BC<sup>-</sup>(X<sup>3</sup>ëÊ), as a consequence, good results are obtained.",,Electronic:978-1-61284-486-2; POD:978-1-61284-485-5,10.1109/ICCSN.2011.6014229,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014229,diatomic molecules and ions;force constants;phase factor;potential energy function;spectroscopic parameters,Helium;Irrigation,aluminium compounds;boron compounds;helium ions;hydrogen neutral molecules;molecular force constants;negative ions;positive ions;potential energy functions,AlBr;BC<sup>-</sup>;H<sub>2</sub>;He<sub>2</sub><sup>+</sup>;analytical potential energy functions;dimensionless undetermined parameters;extensive universality;heteronuclear charged diatomic molecular ion;heteronuclear neutral diatomic molecule;homonuclear charged diatomic molecular ion;homonuclear neutral diatomic molecule;linear equations;spectroscopic parameters;universal analytic potential function,,0,,16,,no,27-29 May 2011,,IEEE,IEEE Conference Publications
Agile way of BI implementation,B. Rehani,"Business Intelligence -Technology excellence Group, Tata Consultancy services",2011 Annual IEEE India Conference,20120126,2011,,,1,6,"The rapidly changing IT economy has influenced the Business Intelligence (BI) systems to look at innovative ways to be equally fast and flexible. There is a need to be more intuitive and quick in implementation so as to adapt to the changing environment. One of the ways by which organizations can achieve these goals is by using Agile based BI development models. There are many components in a successful BI solution which include data integration, analytics, data quality, metadata management, enterprise data warehouse, dashboards and so on. Each of these components are critical for an organization, and stakeholders are ready to invest in these. The only issue is how quickly we can provide these solutions and how flexible these solutions are with the changing demands. Traditionally, we have been using the waterfall SDLC model for BI implementations which encourages getting requirements clarity in the initial phases of the projects and having distinct deliverables for each phase. With time the approach has been customized and enhanced to `iterative waterfall approach' where a chunk of requirements is implemented in one SDLC cycle. Though this approach has been successful in the past, the BI practitioners recognize that business requirements are not static and we must be able to effectively mould the deliverables based on changing requirements. Hence, we cannot continue with the Waterfall (or Iterative waterfall) project management approach that is neither fast nor flexible. Applying the concepts of agile development to BI is the intuitive way forward. The aim of this paper is to provide a background on agile project management & development techniques, and suggest some guidelines and best practices which can help in successful Agile BI implementations.",2325-940X;2325940X,Electronic:978-1-4577-1109-1; POD:978-1-4577-1110-7,10.1109/INDCON.2011.6139618,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6139618,Agile;Buisness Intelligence;Development methodlogy;Scrum,Bismuth;Business;Data models;Data warehouses;Programming;Software;Testing,competitive intelligence;data analysis;data warehouses;meta data;software engineering,Agile based BI development model;BI system;IT economy;agile project development;agile project management;business intelligence;dashboard;data analytics;data integration;data quality;enterprise data warehouse;information technology;iterative waterfall project management;metadata management;systems development life cycle;waterfall SDLC model,,1,,4,,no,16-18 Dec. 2011,,IEEE,IEEE Conference Publications
AHP study on energy indicators system for sustainable development of Henan province,W. Ran,"School of Economics & Management, Zhongyuan University of Technology, Zhengzhou 450007, China",2011 IEEE 3rd International Conference on Communication Software and Networks,20110908,2011,,,175,179,"Analytic Hierarchy Process is an approach to decision making that involves structuring multiple choice criteria into a hierarchy, assessing the relative importance of these criteria, comparing alternatives for each criterion, and determining an overall ranking of the alternatives. This paper applies AHP for evaluating the energy indicators system for sustainable development. A case applied for Henan province is studied. All weight coefficients for energy indicators system are gotten from an investigation. Moreover, the score methodology by MATLAB software is also used for getting the energy indicators' weight scores. The results show that there are four indicators, which are the key indicators influencing sustainable development of energy for Henan province.",,Electronic:978-1-61284-486-2; POD:978-1-61284-485-5,10.1109/ICCSN.2011.6014245,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014245,Analytic Hierarchy Proces;Energy Indicators System;Henan province;sustainable development,Analytical models;Atomic measurements;Electricity;MATLAB;Phase locked loops;Tutorials,decision making;energy consumption;mathematics computing;sustainable development;town and country planning,AHP method;Henan province;Matlab software;analytic hierarchy process;decision making;energy indicators system;sustainable development,,0,,16,,no,27-29 May 2011,,IEEE,IEEE Conference Publications
AHP-based assessment of emergency response agencies,J. Baohua; Z. Lihui; L. Jinxu; H. Zhenyuan; Z. Liang,"School of Computer and Communication Engineering, Zhengzhou University of Light Industry, Zhengzhou, China, 450002",2011 IEEE 3rd International Conference on Communication Software and Networks,20110908,2011,,,82,85,"Advanced emergency management agency or not, directly related to the handling of public emergency is in place in time. At present, the management level of domestic emergency response agencies between provinces exists irregularity, in order to solve this problem, this paper proposed an emergency response agencies assessment method based on Analytic Hierarchy Process, used AHP to quantity evaluating indicator, assessed emergency response agencies between Henan Hunan and Shanxi provinces, finally we can get a more accurate evaluation result. Which will improve management level of emergency response agencies at the same time evaluate emergency response agencies scientifically and objectively.",,Electronic:978-1-61284-486-2; POD:978-1-61284-485-5,10.1109/ICCSN.2011.6014680,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014680,AHP;Assessment of emergency response agencies;Emergency,Bismuth;Educational institutions;Emergency services;Gold,decision making;emergency services,AHP-based assessment;Henan Hunan province;Shanxi province;analytic hierarchy process;domestic emergency response agency;emergency management agency;public emergency handling,,0,,9,,no,27-29 May 2011,,IEEE,IEEE Conference Publications
An analysis of mechanism kinematics accuracy based on linear elastic,Q. Hou; W. Cui; T. Yu; F. Wan; B. Song,"School of Aeronautics, Northwestern Polytechnical University, 710072, Xi&#x2032;an, China","The Proceedings of 2011 9th International Conference on Reliability, Maintainability and Safety",20110811,2011,,,404,408,"The simulation-based prediction of the mechanism kinematics accuracy is of great importance in engineering application. For widely used and low-cost open-loop mechanism having no feedback signals, it is important to take a balance between economic consideration and high accuracy requirement, thus containing high research value. Based on current widely used methods, this paper proposes a more precise method to revise the kinematics accuracy of movement mechanism. Considering the manufacture error and joint clearance in mechanism, we reckon in elastic deformation displacement caused by gravity load and inertia force, that the analytic expression of displacement correction is presented, and that it is independent of the Business Software. Theoretical analysis and case studies show that this amendment for the precision request is necessary.",,Electronic:978-1-61284-666-8; POD:978-1-61284-667-5,10.1109/ICRMS.2011.5979302,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5979302,Kinematics accuracy;Open-loop mechanism;Topology;ndom process,Accuracy;Equations;Gravity;Kinematics;Mathematical model;Reliability;Trajectory,elastic deformation;mechanical products,business software;displacement correction;gravity load;inertia force;joint clearance;linear elastic deformation;low-cost open-loop mechanism;manufacture error;movement mechanism kinematics accuracy;simulation-based prediction,,0,,9,,no,12-15 June 2011,,IEEE,IEEE Conference Publications
An Analytic Network Process Model of Trust in B2C E-Commerce,J. He,"Sch. of Inf., Xi'an Univ. of Finance & Econ., Xi'an, China",2011 2nd International Symposium on Intelligence Information Processing and Trusted Computing,20111215,2011,,,173,176,"Online consumer trust in Business to Consumer (B2C) e-commerce trust has been viewed as a key differentiator that determines the success or failure of many companies conducting their business over the Internet. In order to explore the influence factor of consumer trust and their priority, and to look for the way to promote consumer trust, an Analytic Network Process (ANP) model is constructed in this paper. Based on three dimension of consumer trust, an index system is suggested. By analyzing the interaction of indicators, the ANP model can rationally calculate the weight of each indicator with Super Decisions software. From data analysis, the research comes to a conclusion that brand of the company, law and technique, scale of the company and reputation of the website have a great impact on consumer trust. And at the present stage, consumers take more attention to competence and integrity dimension than benevolence dimension. Besides, the model also shows that 360buy wins the most consumer trust compared with the other two selected B2C companies. The consistence with the reality proves that the ANP model can effectively find the priority of influence factors.",,Electronic:978-0-7695-4498-4; POD:978-1-4577-1130-5,10.1109/IPTC.2011.51,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6103565,Analytic Network Process;B2C;model;trust,Analytical models;Companies;Indexes;Logistics;Security;Software,Internet;Web sites;consumer behaviour;customer relationship management;electronic commerce,ANP model;B2C e-commerce;Internet;Super Decisions software;Web site;analytic network process model;business to consumer e-commerce trust;online consumer trust,,0,,8,,no,22-23 Oct. 2011,,IEEE,IEEE Conference Publications
An Evidence Based Analytical Approach for Process Automation: Case Study in Finance and Administration Process Delivery Services,S. Zeng; J. A. Laredo; S. Mehta; H. Chauhan; A. Verma; D. Williams; B. Browne,"T.J. Watson Res. Center, IBM, Yorktown Heights, NY, USA",2011 IEEE 8th International Conference on e-Business Engineering,20111215,2011,,,286,292,"Process Automation is a critical activity to reduce the need for human work in the production of goods and services to make the processes for uniform and efficient. It is always challenging to make informed decisions on areas for automation which addresses time-to-value to the business. In this paper, we present an evidence based analytics approach to identify top opportunities for process automation, and provide objective assessment of benefit to enable process leaders to take informed decisions. This approach is composed of three major steps. The first step is to identify the top hitters of human intensity in the delivery processes through analyzing evidence gathered from activity time-motion monitoring and onsite process deep dive. The second step is to prioritize the opportunities for automation by analyzing technology choices and estimated business impact. The final step is to assess the benefit through deep analysis on the additional evidence gathered on time-motion data on operational procedures through sampling and extrapolation, and degree of automation can be achieved through technology components. A case study in Finance and Administration Process Delivery Services is used to illustrate the core idea of our analytical approach.",,Electronic:978-0-7695-4518-9; POD:978-1-4577-1404-7,10.1109/ICEBE.2011.31,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6104630,Desktop Automation;Evidence based Analytical Approach;Finance and Administration;Process Automation;Time Motion,Automation;Humans;Industries;Investments;Optical character recognition software;Organizations,business data processing;finance;outsourcing,activity time-motion monitoring;administration process delivery services;evidence based analytical approach;extrapolation;finance;process automation,,0,,12,,no,19-21 Oct. 2011,,IEEE,IEEE Conference Publications
An experimental testbed to predict the performance of XACML Policy Decision Points,B. Butler; B. Jennings; D. Botvich,"FAME, Telecommunications Software & Systems Group (TSSG), Waterford Institute of Technology, Ireland",12th IFIP/IEEE International Symposium on Integrated Network Management (IM 2011) and Workshops,20110818,2011,,,353,360,"The performance and scalability of access control systems is a growing concern as organisations deploy ever more complex communications and content management systems. This paper describes how an (offline) experimental testbed may be used to address performance concerns. To begin, timing measurements are collected from a server component incorporating the Policy Decision Point (PDP) under test, using representative policies and corresponding requests. Our experiments with two XACML PDP implementations show that measured request service times are typically clustered by request type; thus an algorithm for request cluster identification is presented. Cluster characterisations are used as inputs to a PDP performance model for a given policy/request mix and an analytic (queueing) model is used to estimate the equilibrium server load for different mixes of request clusters. The analytic performance prediction model is validated and extended by discrete event simulation of a PDP subject to additional load. These predictive models enable network administrators to explore the capacity of the PDP for different overall loadings (requests per unit time) and profiles (relative frequencies) of requests.",1573-0077;15730077,Electronic:978-1-4244-9221-3; POD:978-1-4244-9219-0; USB:978-1-4244-9220-6,10.1109/INM.2011.5990711,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5990711,,Analytical models;Biological system modeling;Computational modeling;Control systems;Handheld computers;Instruments;Time frequency analysis,XML;authorisation;protocols,XACML policy decision points;access control systems;analytic performance prediction model;cluster characterisations;content management systems;discrete event simulation;policy decision point,,3,,17,,no,23-27 May 2011,,IEEE,IEEE Conference Publications
An integrated visualization on network events VAST 2011 Mini Challenge #2 Award: ‰ÛÏOutstanding integrated overview display‰Ûù,W. M. Lamagna,"Universidad de Buenos Aires, Master on Datamining and Knowledge Discovery, Spain",2011 IEEE Conference on Visual Analytics Science and Technology (VAST),20111215,2011,,,319,321,"To visualize security trends for the data set provided by the VAST 2011 Mini Challenge #2 a custom tool has been developed. Open source tools [1,2], web programming languages [4,7] and an open source database [3] has been used to work with the data and create a visualization for security log files containing network security trends. In this paper, the tools and methods used for the analysis are described. The methods include the log synchronization with different timezone and the development of heat maps and parallel coordinates charts. To develop the visualization, Processing and Canvas [4,7] was used.",,Electronic:978-1-4673-0014-8; POD:978-1-4673-0015-5,10.1109/VAST.2011.6102493,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6102493,heat map;logs;security trends;vast challenge;visual analysis,Arrays;Data visualization;Fires;Heating;Image color analysis;Security;Synchronization,Internet;data visualisation;programming languages;public domain software;security of data,VAST 2011 mini challenge #2 award;Web programming languages;canvas;data set security trend visualization;heat map development;integrated visualization;log synchronization;network security trends;open source database;open source tools;outstanding integrated overview display;parallel coordinates charts;processing,,1,,8,,no,23-28 Oct. 2011,,IEEE,IEEE Conference Publications
An IT Project selection method based on fuzzy analytic network process,Hao Bai; Zhiqiang Zhan,"State key laboratory of Networking and Switching, Beijing University of Posts and Telecommunications, China","2011 International Conference on System science, Engineering design and Manufacturing informatization",20111117,2011,2,,275,279,"IT Project selection is a multi-criteria decision-making (MCDM) problem. The key process of IT project selection is the assessment criteria framework establishment and criteria weight evaluation. Several criteria should be included into the judging system, and the complexity of multi-criteria judgment could even increase if relationships among selection criteria are taken into consideration. A method named fuzzy analytic network process (FANP) is proposed to tackle with the relationships of criteria and the uncertainty of them. A network structure of criteria is developed according to ANP method first, and then fuzzy method is used to evaluate the interdependent importance between criteria. After the weight determination of relative criteria, project selection result could be clear. At the end, an example is given to explain the usage and availability of this method.",,Electronic:978-1-4577-0246-4; POD:978-1-4577-0247-1,10.1109/ICSSEM.2011.6081297,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6081297,Fuzzy ANP;IT Project selection;MCDM,Silicon;Software,decision making;fuzzy set theory;information technology;project management,ANP method;IT project selection;MCDM problem;assessment criteria;criteria weight evaluation;fuzzy analytic network process;fuzzy method;multicriteria decision-making;multicriteria judgment;network structure,,0,,11,,no,22-23 Oct. 2011,,IEEE,IEEE Conference Publications
Analysis of data clustering support for service,YunWei Zhao; Chi-Hung Chi; Chen Ding,"School of Software, Tsinghua University, Beijing, China",2011 IEEE 2nd International Conference on Software Engineering and Service Science,20110811,2011,,,27,30,"With the current direction of service and cloud, unique characteristics of online software services impose new algorithmic requirements and cause differential applicability/ suitability of different clustering approaches in service analytics. In this paper, we investigate the efficiency and effectiveness of current important data clustering techniques, partitioning and hierarchical, for service analytics. It is our goal that results from this paper will serve as requirement guidelines for developing and deploying future intelligence services.",2327-0586;23270586,Electronic:978-1-4244-9698-3; POD:978-1-4244-9699-0,10.1109/ICSESS.2011.5982246,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5982246,Software services;data clustering;intelligence computation;quality of service;service analytics,Algorithm design and analysis;Clustering algorithms,cloud computing;pattern clustering,cloud computing;data clustering support;hierarchical clustering;partition clustering;service analytics;software-as-a-service,,1,,5,,no,15-17 July 2011,,IEEE,IEEE Conference Publications
Analysis software of general dynamic circuit based on MATLAB,B. Jin; W. Cai,"Electronics and Information School, Yangtze University, Jingzhou, China",2011 International Conference on Electrical and Control Engineering,20111024,2011,,,996,999,"The analysis software of General dynamic circuit is developed based on GUI programming in MATLAB. For the equation of circuit formed by node-listing method of the circuit theory the popularity and applicability of the circuit analysis is improved. The visualization of the input and output signal, power function transformation and initial value problem can be solved by GUI activex and programming. In the paper, we accomplish automatic computing, wave out and analytic solution of the general dynamic circuit. The computation examples verify the accuracy and validity of the program.",,DVD:978-1-4244-8164-4; Electronic:978-1-4244-8165-1; POD:978-1-4244-8162-0,10.1109/ICECENG.2011.6057226,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6057226,GUI;MATLAB;dynamic circuit;node-listing method,Circuit analysis;Educational institutions;Graphical user interfaces;MATLAB;Presses;Steady-state,circuit analysis computing;data visualisation;graphical user interfaces;mathematics computing,GUI activex;GUI programming;MATLAB;analysis software;general dynamic circuit;initial value problem;input-output signal visualization;power function transformation,,0,,12,,no,16-18 Sept. 2011,,IEEE,IEEE Conference Publications
Analysis to Industry Agglomeration Degree for Changzhou Hi-Tech Zone Based on an AHP Model,X. w. Zhu; G. b. Zhu; K. g. Gou; X. m. Zhu,"Int. Sch. of Software, Wuhan Univ., Wuhan, China",2011 International Conference on Management and Service Science,20110825,2011,,,1,6,"This paper proposes a multi-criteria evaluation model for assessing industry agglomeration degree referring to Changzhou Hi-Tech Zone. By using Analytic Hierarchy Process (AHP) method, the model combines qualitative analysis and quantitative measurement, which is derived from Changzhou Industry Information System for Changzhou Hi-Tech Zone, resulting in a reasonable evaluation scientifically and authoritatively.",,Electronic:978-1-4244-6581-1; POD:978-1-4244-6579-8,10.1109/ICMSS.2011.5999069,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5999069,,Analytical models;Economics;Indexes;Industries;Investments;Marketing and sales;Roads,decision making;manufacturing industries,Changzhou hi-tech zone;Changzhou industry information system;analytic hierarchy processing;industry agglomeration degree;multicriteria evaluation model;qualitative analysis;quantitative measurement,,0,,15,,no,12-14 Aug. 2011,,IEEE,IEEE Conference Publications
Analytic calculation of magnetic field and force in Halbach permanent magnet linear motor,Zhang Qi; Pan Mengchun; Chen Dixiang,"College of Mechatronics Engineering and Automation, National Univ. of Defense Technology, Changsha, 410073, Hunan, China",IEEE 2011 10th International Conference on Electronic Measurement & Instruments,20111010,2011,4,,77,80,"The calculating formula of magnetic field produced by single permanent magnet (PM) is deduced based on equivalent current model and magnetic field of Halbach PM array is also given. Secondly, A model of linear synchronous motor whose stator is air-cored coil and mover is Halbach PM array is designed. Then, air-gap magnetic field and force are calculated by both the above formula and commercial electromagnetic simulation software MaxWell. The error between the two methods is less than 2 percent. Compared with the software, the formula presented in the paper is accurate and timesaving which can be used as a fast calculating method for designing and validating linear synchronous motor with Halbach PM array.",,Electronic:978-1-4244-8161-3; POD:978-1-4244-8158-3,10.1109/ICEMI.2011.6037951,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6037951,Analytic calculation;Halbach;linear synchronous motor;magnetic field,Arrays;Coils;Force;Magnetic fields;Magnetic flux;Permanent magnet motors;Synchronous motors,air gaps;linear synchronous motors;permanent magnet motors;stators,Halbach PM array;Halbach permanent magnet linear motor;MaxWell commercial electromagnetic simulation software;air-cored coil;air-gap magnetic field;air-gap magnetic force;equivalent current model;linear synchronous motor;stator,,1,,12,,no,16-19 Aug. 2011,,IEEE,IEEE Conference Publications
Analytic hierarchy process integrated hybrid agent system for intelligent legal assistance,S. Cyril Naves,"Dept. of Inf. Technol., Anna Univ., Chennai, India",2011 2nd International Conference on Intelligent Agent & Multi-Agent Systems,20111020,2011,,,41,45,"Legal systems developed around the world tend to be very complex in the eyes of a common man with no idea as to how to approach a judiciary seeking justice or exhibiting his right to information about a particular law. Mostly a man with a need for judicial assistance approaches with an advocate to deal with his case, but this may not be useful when a person is interested in knowing and applying the exact law approach to petty issues in his life and also to avoid being a scapegoat at sometimes by few advocates who elongates the case system for his benefits. The proposed approach has an intelligent hybrid agent system composed of two subsystems, law gatherer and law decider to address the two key issues, finding out right laws and making a right choice. The law gatherer subsystem is based on the ontology web service architecture to define the ontology which is used for processing the semantic content of gathered law information. The law gatherer subsystem is composed of a knowledge system that combines Case-Based Reasoning (CBR) and Rule-Based Reasoning (RBR). Simple Object Access protocol (SOAP) is used for establishing the communication interface and gathering XML-based contents between the agent system and the database system created, through remote procedure calls with condition parameters. In the second subsystem Law Decider, the Analytic Hierarchy Process which is a structured technique for dealing with complex decisions is used to make an optimal decision for satisfying the case structure of the individual. The proposed Hybrid agent system will assist the people with no judicial knowledge to feed their case details into the system to get the appropriate law suitable for that particular instance.",,DVD:978-1-4577-0877-0; Electronic:978-1-4577-0878-7; POD:978-1-4577-0876-3,10.1109/IAMA.2011.6049001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6049001,Analytic Hierarchy Process;Artificial Intelligence;Intelligent Agent;Ontology;Web Service;case-based reasoning;rule-based systems,Cognition;Database systems;Equations;Law;Mathematical model;Ontologies,Web services;case-based reasoning;decision making;law administration;ontologies (artificial intelligence);software agents,analytic hierarchy process;case-based reasoning;integrated hybrid agent system;intelligent legal assistance;judicial assistance;law decider agent;law gatherer agent;legal system;ontology Web service architecture;rule-based reasoning;simple object access protocol,,0,,9,,no,7-9 Sept. 2011,,IEEE,IEEE Conference Publications
Analytic model of FlexRay synchronization mechanism,J. Sobotka; J. NovÌÁk; J. MalinskÌ_,"Czech Technical University, Faculty of Electrical Engineering, Technick&#x00E1; 2, 166 27 Prague 6, Czech Republic",Proceedings of the 6th IEEE International Conference on Intelligent Data Acquisition and Advanced Computing Systems,20111110,2011,2,,969,974,"FlexRay is an incoming standard for automotive distributed systems. For safety critical applications like x-by-wire systems it is necessary to develop techniques for their verification and validation. This paper presents a model of FlexRay Synchronization Mechanism, validation of this model and offers its usage for validation of parameters of synchronization mechanism in real FlexRay networks.",,Electronic:978-1-4577-1425-2; POD:978-1-4577-1426-9,10.1109/IDAACS.2011.6072918,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6072918,analytic model;flexray;model;synchronization mechanism;validation,Analytical models;Computational modeling;Generators;Mathematical model;Synchronization;Time frequency analysis,automotive engineering;control engineering computing;distributed processing;safety-critical software,FlexRay synchronization mechanism;automotive distributed systems;safety critical applications;x-by-wire systems,,1,,8,,no,15-17 Sept. 2011,,IEEE,IEEE Conference Publications
Analytic Solution for Variations of Magnetic Fields in Closed Circuits: Examination of Deviations From the ‰ÛÏStandard‰Ûù Ampere's Law Equation,A. E. Umenei; Y. Melikhov; D. C. Jiles,"Wolfson Centre for Magnetics, School of Engineering,, Cardiff University,, Cardiff , U.K.",IEEE Transactions on Magnetics,20110322,2011,47,4,734,737,"The calculation of magnetic fields in devices with a nonuniform distribution of magnetomotive force (MMF) and with nonlinear magnetic components has proven problematic within electromagnetic systems. This is caused by insufficiently precise determination of the dependence of magnetic induction B, on magnetic field H. This paper develops a method for analytically calculating the variation of magnetic fields H, in magnetic circuits by introducing a specific expression into to the Ampere's circuital law formula to take into account nonuniform MMF. The new formula uses a conformal mapping procedure which allows calculation of the magnetic field at different displacements from the field generating coil. The analytic approximation proposed is developed for the specific problem of a closed circuit magnetic core. This analytic model gives accurate results faster than can be achieved in FEM software.",0018-9464;00189464,,10.1109/TMAG.2011.2105884,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5686938,Ampere's law equation;FEM simulation;ferromagnetic cores;magnetic field distribution;magnetic modeling,,conformal mapping;electromagnetic induction;magnetic circuits;magnetic fields,FEM software;closed circuit magnetic core;conformal mapping procedure;electromagnetic systems;magnetic fields;magnetic induction;magnetomotive force;nonlinear magnetic components;standard ampere law equation,,5,,9,,no,11-Apr,,IEEE,IEEE Journals & Magazines
Analytical Hierarchy Process and PROMETHEE application in measuring object oriented software quality,A. Halim; A. Sudrajat; A. Sunandar; I. K. R. Arthana; S. Megawan; P. Mursanto,"Enterprise Computing Laboratory, Faculty of Computer Science, University of Indonesia, Indonesia",2011 International Conference on Advanced Computer Science and Information Systems,20120126,2011,,,165,170,This paper describes the combination of Analytic Hierarchy Process (AHP) and PROMETHEE to measure the quality of object oriented software design based on predetermined criteria. The measurement is performed by calculating the metric values that indicate the quality factors. The metric values will be evaluated by AHP method and that result will be used by PROMETHEE method for selecting the most optimal software design that fit the predetermined criteria.,,Electronic:978-979-1421-11-9; POD:978-1-4577-1688-1,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6140791,,Band pass filters;Q factor;Software design;Software measurement;Software quality,decision making;object-oriented methods;software metrics;software quality,AHP;PROMETHEE;analytical hierarchy process;metric value;object oriented software design;object oriented software quality;quality factor,,2,,12,,no,17-18 Dec. 2011,,IEEE,IEEE Conference Publications
Analytics for similarity matching of IT cases with collaboratively-defined activity flows,H. R. Motahari Nezhad; C. Bartolini; P. Joshi,"Hewlett Packard Labs, Palo Alto, California, USA",2011 IEEE 27th International Conference on Data Engineering Workshops,20110516,2011,,,273,278,"Handling IT support cases efficiently is very important for operational excellence of IT organizations. Many IT service centers receive thousands of cases per day, some of which are similar to previously reported cases. To improve efficiency it is important to build upon lessons learned from past cases in the resolution of new cases. Therefore, a desired functionality of case management tools is finding similar previous cases to an open one, in order to leverage information about previous cases to effectively find resolution. A new generation of tools for IT case management, e.g., IT Support Conversation Manager, enables collaborative and adaptive process definition for IT case resolution. Leveraging collaborative and social networking technology makes the case information model increasingly richer and more structured compared to flat textual format case reports in traditional IT case management systems. We have developed an automated method for matching IT support cases that takes into account multiple information attributes including the collaborative flow of activities during case handling. We evaluated the system and the early evaluation results show that this method achieves a higher accuracy and comparable efficiency to text-based similarity approaches.",,Electronic:978-1-4244-9196-4; POD:978-1-4244-9195-7; USB:978-1-4244-9194-0,10.1109/ICDEW.2011.5767639,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5767639,,Accuracy;Collaboration;Companies;Context;Indexing;Software,DP management;organisational aspects;social networking (online),IT case management tools;IT case resolution;IT incident management;IT infrastructure library;IT service centers;IT support cases;IT support conversation manager;collaboratively-defined activity flows;similarity matching;social networking technology,,2,,13,,no,11-16 April 2011,,IEEE,IEEE Conference Publications
Analytics-driven asset management,A. Hampapur; H. Cao; A. Davenport; W. S. Dong; D. Fenhagen; R. S. Feris; G. Goldszmidt; Z. B. Jiang; J. Kalagnanam; T. Kumar; H. Li; X. Liu; S. Mahatma; S. Pankanti; D. Pelleg; W. Sun; M. Taylor; C. H. Tian; S. Wasserkrug; L. Xie; M. Lodhi; C. Kiely; K. Butturff; L. Desjardins,"IBM Research Division, Thomas J. Watson Research Center, Yorktown Heights, NY, USA",IBM Journal of Research and Development,20110120,2011,55,1.2,13:01,13:19,"Asset-intensive businesses across industries rely on physical assets to deliver services to their customers, and effective asset management is critical to the businesses. Today, businesses may make use of enterprise asset-management (EAM) solutions for many asset-related processes, ranging from the core asset-management functions to maintenance, inventory, contracts, warranties, procurement, and customer-service management. While EAM solutions have transformed the operational aspects of asset management through data capture and process automation, the decision-making process with respect to assets still heavily relies on institutional knowledge and anecdotal insights. Analytics-driven asset management is an approach that makes use of advanced analytics and optimization technologies to transform the vast amounts of data from asset management, metering, and sensor systems into actionable insight, foresight, and prescriptions that can guide decisions involving strategic and tactical assets, as well as customer and business models.",0018-8646;00188646,,10.1147/JRD.2010.2092173,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5697272,,Asset management;Predictive maintenance;Water conservation;Water resources,,,,2,,,,no,Jan.-March 2011,,IBM,IBM Journals & Magazines
ANP-based fuzzy matter-element model for Gas Enterprise Risk Assessment,J. p. Jin,"Department of Management, Tianjin University, China",2011 IEEE 18th International Conference on Industrial Engineering and Engineering Management,20111010,2011,Part 2,,1215,1220,"The establishment of Gas Enterprise Risk Assessment Model is an important basis for Enterprises to implement risk management decisions. We make use of some research methods, such as systems analysis and empirical analysis for Chinese gas enterprises own characteristics and adopt a combination of ANP and fuzzy matter-element assessment method for the establishment of the urban gas enterprise risk assessment index system in the paper. Based on the risk assessment index system, combining with the gas corporate social welfare, market competition and other characteristics, we build the gas enterprise risk assessment model that took into account the complex network relationship among the various risk factors and their fuzziness, and it provides bases for the gas enterprise managers to do quantitative analysis of their own risk.",,Electronic:978-1-61284-449-7; POD:978-1-61284-446-6,10.1109/ICIEEM.2011.6035375,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6035375,ANP;Fuzzy matter-element;Gas enterprise;Risk assessment;Risk management,Analytical models;Decision making;Fluctuations;Indexes;Reactive power;Risk management;Software,fuzzy set theory;gas industry;risk analysis;risk management,ANP;analytic network process;fuzzy matter-element assessment method;gas enterprise risk assessment;risk management,,0,,7,,no,3-5 Sept. 2011,,IEEE,IEEE Conference Publications
Application of analytic hierarchy process in the selection of transit route for large-scale cargo transportation,J. t. Kang; H. h. Zhang,"Sch. of Civil Eng. &amp; Archit., Wuhan Univ. of Technol., Wuhan, China",2011 Eighth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD),20110915,2011,1,,598,602,"The selection of the optimized transit route is the prime task in the early stage of large-scale cargo transportation. The article first discusses in detail the principles and process of the selection of transit route for large-scale cargo transportation, and then, the analytic hierarchy process is applied, in combination of Matlab mathematical software, to establish a mathematical model for the selection of the optimized transit route of large-scale cargo transportation which can be used to solve the problem of routes optimization of an actual project. This evaluation model is proved to be stable and effective by the project evaluation, and provides a simple practical method for engineering and technical personnel in route optimization.",,Electronic:978-1-61284-181-6; POD:978-1-61284-180-9,10.1109/FSKD.2011.6019526,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6019526,analytic hierarchy process;large-scale cargo transportation;transport alignment,Bridges;Educational institutions;Eigenvalues and eigenfunctions;Indexes;Inspection;Road transportation,decision making;goods distribution;optimisation;transportation,Matlab mathematical software;analytic hierarchy process;large-scale cargo transportation;mathematical model;optimized transit route selection;project evaluation,,0,,9,,no,26-28 July 2011,,IEEE,IEEE Conference Publications
Application of technology combined GIS and transient electromagnetic technique in coal mine water burst prevention ‰ÛÓ A case of Sihe Coal mine,M. Guo; X. Chen; W. Liu; Y. Wang,"School of Surveying & Land Information Enginering, Henan Polytechnic University, Jiaozuo, Henan 454003, China",2011 19th International Conference on Geoinformatics,20110811,2011,,,1,5,"Transient Electromagnetic Method (TEM) has the characters with sensitivity to water body, high resolution and less influence from topography. Geographic Information System (GIS) has the characters with powerful geospatial analytics, modeling and visualization. Measured data surveyed with TEM are loaded into ArcGIS, then multi-feature overlaid with the panel water-rich distribution map. Three-dimensional distribution of rich water was obtained. And then the water yield, water rich zone in front of roadway can be analyzed. The application of this technique in west area of Sihe Colliery was proved that this method was one effective methods in detecting the rich water distribution and range around mining and tunnelling face. And more the analytical results was visual. Reliable basis of prediction and prevention water burst was obtained.",2161-024X;2161024X,Electronic:978-1-61284-848-8; POD:978-1-61284-849-5,10.1109/GeoInformatics.2011.5980837,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5980837,3D Visualization;ArcGIS;Forecasting and prevention of the coal mine water burst;transient electromagnetic method,Coal;Conductivity;Geology;Software;Three dimensional displays;Transient analysis,flow visualisation;geographic information systems;hydrological techniques;mining;topography (Earth);tunnelling;water resources,ArcGIS;China;Geographic Information System;Sihe Colliery;Sihe coal mine;coal mine water burst prevention;geospatial analytics;mining;panel water-rich distribution map;topography;transient electromagnetic method;tunnelling;water body;water rich zone,,1,,6,,no,24-26 June 2011,,IEEE,IEEE Conference Publications
Applying the balanced score card in the team strategic performance management,Wang Ting,"School of Economics & Management of Henan Polytechnic University, Henan Polytechnic University, Jiaozuo, China","2011 2nd International Conference on Artificial Intelligence, Management Science and Electronic Commerce (AIMSEC)",20110905,2011,,,990,993,"The traditional performance management which uses measure methods by financial indicators can only reflect the past situation not to judge the future development of the team and its profit ability. In the new circumstances, how to establish strategic performance management system and guide organization and personnel's behavior toward the strategic goal to avoid becoming obsolete in the future competition is placed an arduous task in front of the team managers. This article discusses the balanced score card' three aspects: dimension analysis, implementation, system support in the team strategic performance management application by model of grey analytic evaluation, with the purpose of enhancing the team to improve performance management which has an important realistic and profound significance.",,Electronic:978-1-4577-0536-6; POD:978-1-4577-0535-9,10.1109/AIMSEC.2011.6010534,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6010534,balanced score card;dimension analysis;grey analytic evaluation;team strategic performance management,Appraisal;Indexes;Marketing and sales;Organizations;Production;Software,grey systems;profitability;team working,balanced score card;dimension analysis;financial indicators;grey analytic evaluation;profitability;strategic performance management system;system support;team strategic performance management,,0,,7,,no,8-10 Aug. 2011,,IEEE,IEEE Conference Publications
Assessment of the Groundwater Renewability in Beijing Plain Area,J. Zheng; Y. Teng; J. Wang; L. Hu,"Key Lab. for Water & Sediment Sci. of Minist. of Educ., Beijing Normal Univ., Beijing, China",2011 5th International Conference on Bioinformatics and Biomedical Engineering,20110531,2011,,,1,4,"Groundwater is the main water source of Beijing plain area. It plays a vital role in the safety of water supply. Assessment of the groundwater renewability in study area is important to its water resources sustainable utilization. In this study, 5 thematic maps including precipitation, precipitation infiltration coefficient, aquifer structure, slope and, drainage density were selected to integrate using ArcGIS software. Standardization of index values is expressed by membership function. The weights were assigned by analytic hierarchy process (AHP). The result of groundwater renewability zone shows groundwater renewability of Beijing plain area decreases from alluvial fans top to fringe and it is very low in the centre of study area for the small precipitation infiltration coefficient.",2151-7614;21517614,Electronic:978-1-4244-5089-3; POD:978-1-4244-5088-6,10.1109/icbbe.2011.5780854,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5780854,,Fans;Geographic Information Systems;Geology;Indexes;Rivers;Software;Water resources,atmospheric precipitation;geographic information systems;groundwater;water resources;water supply,ArcGIS software;Beijing plain area;China;analytic hierarchy process;aquifer structure;drainage density;groundwater renewability assessment;membership function;precipitation infiltration coefficient;thematic map;water resource sustainable utilization;water supply safety,,0,,12,,no,10-12 May 2011,,IEEE,IEEE Conference Publications
Automated biofilm region recognition and morphology quantification from confocal laser scanning microscopy imaging,J. S. Zielinski; A. K. Zielinska; N. Bouaynaya; J. G. Vaughan; M. S. Smeltzer,"Department of Applied Science, University of Arkansas at Little Rock, 2801 S. University Avenu, Little Rock, Arkansas 72204",Proceedings of the 2011 Biomedical Sciences and Engineering Conference: Image Informatics and Analytics in Biomedicine,20110609,2011,,,1,4,"Staphylococcus aureus is an opportunistic human pathogen and a primary cause of nosocomial infections. Its biofilm forming capability is an adaptation strategy utilized by many species of bacteria to overcome stressful environmental conditions and provides both resistance to antimicrobial treatments and protection from the host immune system. This paper addresses a growing demand for an objective, fully automated method of biofilm structure description with standardized parameters that are independent of user input. In this study, we used watershed segmentation to analyze and compare confocal laser scanning microscopy (CLSM) images of two S. aureus strains with different biofilm-forming capabilities. Results are compared with manual calculations as well as the commonly used COMSTAT software.",,Electronic:978-1-61284-410-7; POD:978-1-61284-411-4,10.1109/BSEC.2011.5872327,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5872327,CLSM;Staphylococcus aureus;biofilm;mathematical morphology;watershed,Biomass;Biomedical imaging;Image segmentation;Immune system;Rocks;Software;Software algorithms,image recognition;medical image processing;microorganisms;optical microscopy,COMSTAT software;Staphylococcus aureus;adaptation strategy;antimicrobial treatment;automated biofilm region recognition;confocal laser scanning microscopy imaging;human pathogen;morphology quantification;nosocomial infection;watershed segmentation,,0,,8,,no,15-17 March 2011,,IEEE,IEEE Conference Publications
"Automated computer network defence technology demonstration project (ARMOUR TDP): Concept of operations, architecture, and integration framework",R. E. Sawilla; D. J. Wiemer,"Defence Research & Development Canada - Ottawa, Ottawa, Canada",2011 IEEE International Conference on Technologies for Homeland Security (HST),20111219,2011,,,167,172,"Modern militaries rely heavily on computer networks and they have become a part of the critical infrastructure that must be protected. Computer networks, both military and non-military, are constantly being attacked and data about new vulnerabilities and attacks must be analyzed and processed at a speed that enables timely mitigation. The ARMOUR Technology Demonstration Project (TDP) is a five-year activity that will demonstrate automated Computer Network Defence (CND) capabilities based on the Observe, Orient, Decide and Act (OODA) decision process. We present the ARMOUR TDP Concept of Operations and Architecture, and promote an open source integration framework for collaborative development.",,Electronic:978-1-4577-1376-7; POD:978-1-4577-1375-0,10.1109/THS.2011.6107865,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6107865,architectures;attack analytic tools;computer network management;cyber security;decision support systems;mission assurance,Computer architecture;Computer security;Connectors;Databases;Measurement;Software,computer network security;military computing,ARMOUR TDP;ARMOUR Technology Demonstration Project;CND;OODA decision process;Observe Orient Decide and Act;automated computer network defence technology demonstration project;collaborative development;computer networks;critical infrastructure;modern militaries;nonmilitary;open source integration framework,,1,,7,,no,15-17 Nov. 2011,,IEEE,IEEE Conference Publications
Automatic Dent-landmark detection in 3-D CBCT dental volumes,E. Cheng; J. Chen; J. Yang; H. Deng; Y. Wu; V. Megalooikonomou; B. Gable; H. Ling,"Center for Data Analytics & Biomedical Informatics, Computer & Information Science Department, Temple University, Philadelphia, PA, 19122 USA",2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society,20111201,2011,,,6204,6207,"Orthodontic craniometric landmarks provide critical information in oral and maxillofacial imaging diagnosis and treatment planning. The Dent-landmark, defined as the odontoid process of the epistropheus, is one of the key landmarks to construct the midsagittal reference plane. In this paper, we propose a learning-based approach to automatically detect the Dent-landmark in the 3D cone-beam computed tomography (CBCT) dental data. Specifically, a detector is learned using the random forest with sampled context features. Furthermore, we use spacial prior to build a constrained search space other than use the full three dimensional space. The proposed method has been evaluated on a dataset containing 73 CBCT dental volumes and yields promising results.",1094-687X;1094687X,Electronic:978-1-4577-1589-1; POD:978-1-4244-4121-1; USB:978-1-4244-4122-8,10.1109/IEMBS.2011.6091532,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6091532,,Context;Dentistry;Feature extraction;Testing;Three dimensional displays;Training;Vegetation,computerised tomography;dentistry;diagnostic radiography;feature extraction;medical image processing;object detection;orthotics,3-D CBCT dental volumes;3D cone-beam computed tomography;automatic dent-landmark detection;epistropheus;midsagittal reference plane;orthodontic craniometric landmarks;treatment planning,"Algorithms;Cephalometry;Cone-Beam Computed Tomography;Humans;Imaging, Three-Dimensional;Models, Statistical;Orthodontics;Radiographic Image Interpretation, Computer-Assisted;Time Factors;Tooth",1,,10,,no,Aug. 30 2011-Sept. 3 2011,,IEEE,IEEE Conference Publications
Automation for creating and configuring security manifests for hardware containers,E. Leontie; G. Bloom; R. Simha,"Department of Computer Science, George Washington University, USA",2011 4th Symposium on Configuration Analytics and Automation (SAFECONFIG),20111226,2011,,,1,2,Hardware containers provide fine-grained memory access control to isolate memory regions and sandbox memory references between components of an application. A hardware reference monitor enforces a security manifest of memory access permissions for the currently executing component. In this paper we discuss how automation tools can help software developers to create the security manifest that configures hardware containers. The goal of this work is to foster discussion about our proposals for automation tools: to date we know of no solutions for extracting the metadata (permissions) required for fine-grained memory access control.,,Electronic:978-1-4673-0402-3; POD:978-1-4673-0401-6,10.1109/SafeConfig.2011.6111677,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6111677,,Access control;Automation;Containers;Hardware;Monitoring;Software,authorisation;software development management,automation tool;fine-grained memory access control;hardware container;hardware reference monitor;memory access permission;memory region;sandbox memory reference;security manifest;software developer,,0,,9,,no,Oct. 31 2011-Nov. 1 2011,,IEEE,IEEE Conference Publications
Autonomic management of client concurrency in a distributed storage service,M. Tauber; G. Kirby; A. Dearle,"School of Computer Science, University of St Andrews, North Haugh, KY16 9SX, Scotland, UK",12th IFIP/IEEE International Symposium on Integrated Network Management (IM 2011) and Workshops,20110818,2011,,,1109,1115,"A distributed autonomic system adapts its constituent components to a changing environment. This paper reports on the application of autonomic management to a distributed storage service. We developed a simple analytic model which suggested potential benefit from tuning the degree of concurrency used in data retrieval operations, to suit dynamic conditions. We then validated this experimentally by developing an autonomic manager to control the degree of concurrency. We compared the resulting data retrieval performance with non-autonomic versions, using various combinations of network capacity, membership churn and workload patterns. Overall, autonomic management yielded improved retrieval performance. It also produced a distinct but not significant increase in network usage relative to one non-autonomic configuration, and a significant reduction relative to another.",1573-0077;15730077,Electronic:978-1-4244-9221-3; POD:978-1-4244-9219-0; USB:978-1-4244-9220-6,10.1109/INM.2011.5990521,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5990521,autonomic management;distributed storage,Bandwidth;Delay;Educational institutions;Monitoring;Planning;Servers,distributed processing;information retrieval;software fault tolerance;storage management,autonomic management;client concurrency;data retrieval performance;distributed autonomic system;distributed storage service,,0,,11,,no,23-27 May 2011,,IEEE,IEEE Conference Publications
Boosting text extraction from biomedical images using text region detection,S. Xu; M. Krauthammer,"Oak Ridge National Laboratory, One Bethel Valley Road, Oak Ridge, TN 37831, USA",Proceedings of the 2011 Biomedical Sciences and Engineering Conference: Image Informatics and Analytics in Biomedicine,20110609,2011,,,1,4,"In this paper, we show that domain-optimized text detection in biomedical images is important for boosting text extraction recall via off-the-shelf OCR engines. Methodologically, we contrast OCR performance when processing raw biomedical images, compared to preprocessing those images, and performing OCR on detected image text regions only. To quantify OCR extraction results, we rely on a gold standard image text corpus with manually identified image text strings. To demonstrate the positive effect on biomedical image retrieval, we apply image text detection and extraction to a large corpus of biomedical images in the Yale Image Finder system. We show that improved text extraction results in the retrieval of a larger number of relevant images for a set of domain-relevant keyword searches.",,Electronic:978-1-61284-410-7; POD:978-1-61284-411-4,10.1109/BSEC.2011.5872319,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5872319,,Biomedical imaging;Engines;Gold;Guidelines;Image retrieval;Optical character recognition software;Videos,feature extraction;medical image processing;optical character recognition;text analysis,Yale Image Finder system;biomedical image retrieval;domain-relevant keyword searches;image text string;off-the-shelf OCR engines;text extraction;text region detection,,1,2,16,,no,15-17 March 2011,,IEEE,IEEE Conference Publications
Boundary element analysis of rolling tire noise,Guolin Wang; Zhujun Mao; Haichao Zhou; Long Gao,"School of Automobile and Traffic Engineering, Jiangsu University, Zhenjiang, China","Proceedings 2011 International Conference on Transportation, Mechanical, and Electrical Engineering (TMEE)",20120514,2011,,,1970,1973,"The vibration and radiated noise characteristics of rolling tire was analyzed by virtual prediction method. The acceleration response spectrum of rolling tire was acquired through the method of analyzing the dynamic response under the road impact excitations by the modal superposition approach. The acceleration date of rolling tire surface were imported to the acoustics analytic software as the input load of the boundary element model, and the noise radiated from rolling tire by vibration were calculated. The disrupted and characters of the noise in different the tread pattern and road surface was discussed.",,Electronic:978-1-4577-1701-7; POD:978-1-4577-1700-0,10.1109/TMEE.2011.6199601,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6199601,acoustic simulation;boundary element method;radiation noise;rolling tire;tread pattern,Acceleration;Educational institutions;Finite element methods;Noise;Roads;Tires;Vibrations,acoustics;automotive components;boundary-elements methods;impact (mechanical);roads;tyres;vibrations,acceleration response spectrum;acoustics analytic software;boundary element analysis;dynamic response analysis;modal superposition approach;radiated noise characteristics;road impact excitations;road surface;rolling tire noise;rolling tire vibration;tread pattern;virtual prediction method,,0,,10,,no,16-18 Dec. 2011,,IEEE,IEEE Conference Publications
Business Intelligence in the Cloud,S. Ouf; M. Nasr,"Faculty of Computers & Information, Helwan University, Egypt",2011 IEEE 3rd International Conference on Communication Software and Networks,20110908,2011,,,650,655,"Business Intelligence (BI) deals with integrated approaches to management support. Currently, there are constraints to BI adoption and a new era of analytic data management for business intelligence these constraints are the integrated infrastructures that are subject to BI have become complex, costly, and inflexible, the effort required consolidating and cleansing enterprise data and Performance impact on existing infrastructure / inadequate IT infrastructure. So, in this paper Cloud computing will be used as a possible remedy for these issues. We will represent a new environment atmosphere for the business intelligence to make the ability to shorten BI implementation windows, reduced cost for BI programs compared with traditional on-premise BI software, Ability to add environments for testing, proof-of-concepts and upgrades, offer users the potential for faster deployments and increased flexibility. Also, Cloud computing enables organizations to analyze terabytes of data faster and more economically than ever before. Business intelligence (BI) in the cloud can be like a big puzzle. Users can jump in and put together small pieces of the puzzle but until the whole thing is complete the user will lack an overall view of the big picture. In this paper reading each section will fill in a piece of the puzzle.",,Electronic:978-1-61284-486-2; POD:978-1-61284-485-5,10.1109/ICCSN.2011.6014351,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014351,Cloud Computing;business Intelligence;platform as a services,Bismuth;Business;Hardware;Industries;Servers,cloud computing;competitive intelligence,IT infrastructure;analytic data management;business intelligence adoption;cloud computing;enterprise data;performance impact,,0,,21,,no,27-29 May 2011,,IEEE,IEEE Conference Publications
Cataloga: A Software for Semantic-Based Terminological Data Mining,A. Elia; M. Monteleone; A. Postiglione,"Dipt. di Sci. Politiche, Sociali e della Comun., Univ. degli Studi di Salerno, Fisciano, Italy","2011 First International Conference on Data Compression, Communications and Processing",20111027,2011,,,153,156,"This paper is focused on Catalog a, a software package based on Lexicon-Grammar theoretical and practical analytical framework and embedding a ling ware module built on compressed terminological electronic dictionaries. We will here show how Catalog a can be used to achieve efficient data mining and information retrieval by means of lexical ontology associated to terminology-based automatic textual analysis. Also, we will show how accurate data compression is necessary to build efficient textual analysis software. Therefore, we will here discuss the creation and functioning of a software for semantic-based terminological data mining, in which a crucial role is played by Italian simple and compound-word electronic dictionaries. Lexicon-Grammar is one of the most profitable and consistent methods for natural language formalization and automatic textual analysis it was set up by French linguist Maurice Gross during the '60s, and subsequently developed for and applied to Italian by Annibale Elia, Emilio D'Agostino and Maurizio Martin Elli. Basically, Lexicon-Grammar establishes morph syntactic and statistical sets of analytic rules to read and parse large textual corpora. The analytical procedure here described will prove itself appropriate for any type of digitalized text, and will represent a relevant support for the building and implementing of Semantic Web (SW) interactive platforms.",,Electronic:978-0-7695-4528-8; POD:978-1-4577-1458-0,10.1109/CCP.2011.42,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6061017,Automatic Textual Analysis;Cataloga software;Information Retrieval;Lexicon-Grammar;Semantic-Based Terminological Data Mining,Compounds;Data mining;Dictionaries;Geology;Pragmatics;Semantics;Software,cataloguing;data compression;data mining;dictionaries;grammars;information retrieval;interactive systems;ontologies (artificial intelligence);semantic Web;text analysis,Cataloga software package;Italian compound-word electronic dictionary;Italian simple-word electronic dictionary;automatic text analysis;compressed terminological electronic dictionary;data compression;information retrieval;lexical ontology;lexicon-grammar practical analytical framework;lexicon-grammar theoretical analytical framework;lingware module;morphosyntactic analytic rule sets;natural language formalization;semantic Web interactive platform;semantic-based terminological data mining;statistical analytic rule sets;terminology-based automatic text analysis;text analysis software;textual corpora,,0,,19,,no,21-24 June 2011,,IEEE,IEEE Conference Publications
Challenges for Creating Highly Dependable Service Based Systems,S. Banerjee; H. Srikanth; B. Cukic,"Lane Dept. of Comput. Sci. & Electr. Eng., West Virginia Univ., Morgantown, WV, USA",2011 14th IEEE International Symposium on Object/Component/Service-Oriented Real-Time Distributed Computing Workshops,20110421,2011,,,264,273,"The paradigm shift from traditional on-premise software to a service based model has gained significant momentum in the past decade. One such concept, Software as a Service (SaaS), delivers the functionality of traditional on-premise software as a service over the web. While a defect or a malfunction in a traditional on-premise application may affect a single user, the affected user base in a SaaS application may span the entire group of customers serviced by the provider. The physical disconnect between end users and the SaaS applications puts onus on service providers to deliver highly dependable systems that are available and reliable at all times. In this paper, we explore the general challenges faced in delivering and analyzing highly dependable service based systems. We quantify the challenges of dependability assessment utilizing a commercial case study. Furthermore, we explore one facet of dependability assessment related to log entries not necessarily related to dependability. We provide a novel approach to log filtering and show that the removal of benign log entries leads to more realistic system dependability analysis. We also show the need to merge multiple types of SaaS logs to support effective analysis.",,CD-ROM:978-0-7695-4377-2; Electronic:978-1-4577-0783-4; POD:978-1-4577-0303-4,10.1109/ISORCW.2011.33,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5753536,Dependable Systems;Software Analytics;Software Dependability;Software as a Service,Business;Data mining;Filtration;Measurement;Reliability;Servers;Software,cloud computing;statistical analysis,SaaS application;service based systems;software as a service;system dependability analysis,,0,,40,,no,28-31 March 2011,,IEEE,IEEE Conference Publications
City sentinel - VAST 2011 mini challenge 1 award: ‰ÛÏOutstanding integration of computational and visual methods‰Ûù,N. BÌÁnfi; L. DudÌÁs; Z. Fekete; J. GÌ_bÌ_ lÌ_s-SzabÌ_; A. LukÌÁcs; ÌÅ. Nagy; A. SzabÌ_; Z. SzabÌ_; G. SzÅ±cs,"Computer and Automation Research Institute (MTA SZTAKI), Hungarian Academy of Sciences, Hungary",2011 IEEE Conference on Visual Analytics Science and Technology (VAST),20111215,2011,,,305,306,"We present City Sentinel, an in-house built visual analytic software capable of handling a large collection of textual documents by combining diverse text mining and visualization tools. We applied this tool for the Vast Challenge 2011, Mini Challenge 1 over millions of tweet messages. We demonstrate how City Sentinel aided the analyst in retrieving the hidden information from the tweet messages to analyze and locate a hypothetical epidemic outbreak.",,Electronic:978-1-4673-0014-8; POD:978-1-4673-0015-5,10.1109/VAST.2011.6102485,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6102485,,Accidents;Blood;Cities and towns;Data mining;Image color analysis;Motion pictures;Tag clouds,data analysis;data mining;data visualisation;epidemics;information retrieval;medical computing,City sentinel;VAST 2011 mini challenge 1 award;computational methods;epidemic outbreak;hidden information retrieval;in-house built visual analytic software;text mining;textual documents;tweet messages;visual methods;visualization tools,,0,,3,,no,23-28 Oct. 2011,,IEEE,IEEE Conference Publications
CloudChecker: An imperative framework for cloud configuration management,S. Al-Haj; E. Al-Shaer,"University of North Carolina Charlotte, USA",2011 4th Symposium on Configuration Analytics and Automation (SAFECONFIG),20111226,2011,,,1,1,"Summary form only given. Cloud computing became one of the major research areas recently. The interest in cloud computing increases day by day because of the features provided by cloud providers. Pay-as-you-go is one of these features that attract customers to adopt this idea. Another feature is providing different levels of services to the customers; Software, Platform, and Infrastructure as a Service are the major services provided by clouds.",,Electronic:978-1-4673-0402-3; POD:978-1-4673-0401-6,10.1109/SafeConfig.2011.6111666,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6111666,,Boolean functions;Cloud computing;Computational modeling;Data structures;Fires;Virtual machine monitors,cloud computing,CloudChecker;cloud computing;cloud configuration management;customer service feature;infrastructure-as-a-service;pay-as-you-go feature;platform service;software service,,0,,,,no,Oct. 31 2011-Nov. 1 2011,,IEEE,IEEE Conference Publications
Colored Petri nets model based conformance test generation,J. Liu; X. Ye; J. Li,"Institute of Computing Technology, Chinese Academy of Sciences",2011 IEEE Symposium on Computers and Communications (ISCC),20110818,2011,,,967,970,"A novel Colored Petri Nets (CP-nets) model based test case generation approach is proposed to makes the best of advantages of the ioco testing theory and the CP-nets modeling, where the Conformance Testing orientated CP-nets (CT-CPN) is proposed for modeling certain software systems, and PN-ioco relation is defined as a new conformance relation, and finally test cases are generated through simulating the system CT-CPN models. CP-nets model simulation based test generation approach reflects the data-dependent control flow of the system behaviors, so all test cases are completely feasible for the actual test executions. Besides, better formal modeling and analytic capabilities in CP-nets modeling quite facilitate validating the accuracy of the system CT-CPN model. For effectively extending the applicability of the Petri nets based testing technologies, our novel CT-CPN model based test generation approach may well become a competent choice.",1530-1346;15301346,Electronic:978-1-4577-0681-3; POD:978-1-4577-0680-6; USB:978-1-4577-0678-3,10.1109/ISCC.2011.5983967,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5983967,colored Petri nets;ioco conformance;model simulation;test case generation,Analytical models;Data models;Fires;Petri nets;Software;Suspensions;Testing,Petri nets;program testing,PN-ioco conformance relation;colored Petri nets model;conformance test generation;conformance testing,,1,,7,,no,June 28 2011-July 1 2011,,IEEE,IEEE Conference Publications
Computational simulation of rotating noise of fan,T. Feng; J. Wang; B. Liu; N. Li; X. Wu,"School of Material and Mechanical Engineering, Beijing Technology and Business University, Beijing, China",2011 IEEE 3rd International Conference on Communication Software and Networks,20110908,2011,,,728,731,"A rotating fan could produce noise of both broadband and discrete frequency. The discrete part typically consists of shaft frequency, blade passage frequency (BPF) and their higher harmonics. The shaft-order noise, which is mainly induced by unbalanced rotating, is often hard to be eliminated. In this paper, the rotating noise caused by geometric asymmetry is discussed and predicted. Analytic point force model is firstly used to predict the noise at points in different directions. Then a hybrid method based on computational fluid dynamic (CFD) simulation and FW-H equation is used to predict the noise of a simplified fan model. Compared with the case of symmetric rotating, a case of asymmetric rotating is studied, and the results show that geometric asymmetry could cause the increase of fan noise considerably at certain frequencies.",,Electronic:978-1-61284-486-2; POD:978-1-61284-485-5,10.1109/ICCSN.2011.6014371,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014371,aeroacoustic formatting;computational simulation;fan;insert (key words);rotating noise;style;styling,Analytical models;Atmospheric modeling;Computational modeling;Cooling;Educational institutions;Lead;Noise,aerodynamics;blades;computational fluid dynamics;fans;harmonics;mechanical engineering computing;noise,analytic point force model;asymmetric rotating;blade passage frequency;broadband frequency;computational fluid dynamic simulation;computational simulation;discrete frequency;geometric asymmetry;harmonics;rotating fan noise;shaft frequency;shaft-order noise;simplified fan model;unbalanced rotating,,1,,14,,no,27-29 May 2011,,IEEE,IEEE Conference Publications
ConfigChecker: A tool for comprehensive security configuration analytics,E. Al-Shaer; M. N. Alsaleh,"Department of Software and Information Systems, University of North Carolina at Charlotte, USA",2011 4th Symposium on Configuration Analytics and Automation (SAFECONFIG),20111226,2011,,,1,2,"Recent studies show that configurations of network access control is one of the most complex and error prone network management tasks. For this reason, network misconfiguration becomes the main source for network unreachablility and vulnerability problems. In this paper, we present a novel approach that models the global end-to-end behavior of access control configurations of the entire network including routers, IPSec, firewalls, and NAT for unicast and multicast packets. Our model represents the network as a state machine where the packet header and location determines the state. The transitions in this model are determined by packet header information, packet location, and policy semantics for the devices being modeled. We encode the semantics of access control policies with Boolean functions using binary decision diagrams (BDDs). We then use computation tree logic (CTL) and symbolic model checking to investigate all future and past states of this packet in the network and verify network reachability and security requirements. Thus, our contributions in this work is the global encoding for network configurations that allows for general reachability and security property-based verification using CTL model checking. We have implemented our approach in a tool called ConfigChecker. While evaluating ConfigChecker, we modeled and verified network configurations with thousands of devices and millions of configuration rules, thus demonstrating the scalability of this approach. We also present a SCAP-based tool on top of ConfigChecker that integrates host and network configuration compliance checking in one model and allows for executing comprehensive analysis queries in order to verify security and risk requirements across the end-to-end network as a single system.",,Electronic:978-1-4673-0402-3; POD:978-1-4673-0401-6,10.1109/SafeConfig.2011.6111667,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6111667,,Access control;Boolean functions;Computational modeling;Data structures;IP networks;Software,authorisation;configuration management;finite state machines;formal verification,Boolean function;CTL model checking;ConfigChecker;IPSec;SCAP-based tool;access control configuration;binary decision diagram;computation tree logic;end-to-end network;firewall;multicast packet;network configuration;network reachability;network security;packet header information;packet location;policy semantics;risk requirement;router;security configuration;state machine;symbolic model checking;unicast packet,,4,,5,,no,Oct. 31 2011-Nov. 1 2011,,IEEE,IEEE Conference Publications
CONFLuEnCE: Implementation and application design,P. Neophytou; P. K. Chrysanthis; A. Labrinidis,"Advanced Data Management Technologies Laboratory, Department of Computer Science, University of Pittsburgh, USA","7th International Conference on Collaborative Computing: Networking, Applications and Worksharing (CollaborateCom)",20120202,2011,,,181,190,"Data streams have become pervasive and data production rates are increasing exponentially, driven by advances in technology, for example the proliferation of sensors, smart phones, and their applications. This fact effectuates an unprecedented opportunity to build real-time monitoring and analytics applications, which when used collaboratively and interactively, will provide insights to every aspect of our environment, both in the business and scientific domains. In our previous work, we have identified the need for workflow management systems which are capable of orchestrating the processing of multiple heterogeneous data streams, while enabling their users to interact collaboratively with the workflows in real time. In this paper, we describe CONFLuEnCE (CONtinuous workFLow ExeCution Engine), which is an implementation of our continuous workflow model. CONFLuEnCE is built on top of Kepler, an existing workflow management system, by fusing stream semantics and stream processing methods as another computational domain. Furthermore, we explicate our experiences in designing and implementing real-life business and scientific continuous workflow monitoring applications, which attest to the ease of use and applicability of our system.",,Electronic:978-1-936968-36-7; POD:978-1-4673-0683-6; USB:978-1-936968-32-9,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6144803,continuous workflows;data streams;monitoring applications;workflow,Availability;Engines;Indexes;Monitoring;Receivers;Semantics,business data processing;groupware;interactive systems;scientific information systems;sensor fusion;workflow management software,CONFLuEnCE;Kepler;business domain;collaborative interaction;continuous workflow execution engine;data production rate;multiple heterogeneous data streams;real-time monitoring;scientific domain;stream processing method;stream semantics;workflow management system,,0,,19,,no,15-18 Oct. 2011,,IEEE,IEEE Conference Publications
Continuous remote vital sign/environment monitoring for returning soldier adjustment assessment,S. Genc; D. J. Cleary; T. Yardibi; J. C. Wood; M. E. Stachura; E. V. Astapova,"Sensor Informatics and Technologies Laboratory, Software Sciences and Analytics, General Electric Global Research, Niskayuna, NY 12309 USA",2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society,20111201,2011,,,2216,2219,"A three-stage study to develop and test an unobtrusive room sensor unit and subject data management system to discover correlation between sensor-based time-series measurements of sleep quality and clinical assessments of combat veterans suffering from Post-traumatic Stress Disorder (PTSD) and mild Traumatic Brain Injury (TBI), is described. Experiments and results for testing sensitivity and robustness of the sensor unit and data management protocol are provided. The current sensitivity of remote vital sign monitoring system is below 20% and 10% for respiration and heart rates, respectively.",1094-687X;1094687X,Electronic:978-1-4577-1589-1; POD:978-1-4244-4121-1; USB:978-1-4244-4122-8,10.1109/IEMBS.2011.6090419,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6090419,,Data acquisition;Databases;Doppler radar;Heart rate;Sensitivity;Sleep,injuries;medical disorders;patient monitoring;sleep;time series,Post-traumatic Stress Disorder;Traumatic Brain Injury;clinical assessment;environment monitoring;remote vital sign monitoring;returning soldier adjustment assessment;robustness;sensitivity;sleep quality;subject data management system;time series;unobtrusive room sensor unit,"Brain Injuries;Diagnosis, Computer-Assisted;Environmental Monitoring;Equipment Design;Equipment Failure Analysis;Humans;Military Personnel;Monitoring, Ambulatory;Polysomnography;Reproducibility of Results;Sensitivity and Specificity;Social Adjustment;Stress Disorders, Post-Traumatic;Vital Signs",1,,14,,no,Aug. 30 2011-Sept. 3 2011,,IEEE,IEEE Conference Publications
Contractor selection using fuzzy comparison judgement,R. H. Alias; N. M. M. Noor; M. Y. M. Saman; M. L. Abdullah; A. Selamat,"Computer Science Department, Faculty of Science and Technology, Universiti Malaysia Terengganu",2011 Malaysian Conference in Software Engineering,20120126,2011,,,388,392,"A model is proposed for selecting the optimal result of contractor selection under multi criteria environment. Fuzzy comparing judgment is used to tackling the vagueness and uncertainty in choosing significant preferences by decision maker regarding to the subjective opinion. Finally, the model was tested in tender evaluation processes for awarding the most beneficial contractor to perform the construction project.",,Electronic:978-1-4577-1531-0; POD:978-1-4577-1530-3,10.1109/MySEC.2011.6140703,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6140703,Analytic hierarchy process;Contractor selection;fuzzy sets,Decision making;Expert systems;Fuzzy sets;Humans;Mathematical model;Pragmatics;Vectors,construction industry;decision making;fuzzy set theory,construction project;contractor selection;fuzzy comparison judgement;multicriteria environment;subjective opinion;tender evaluation processes,,0,,24,,no,13-14 Dec. 2011,,IEEE,IEEE Conference Publications
Data analytics for game development: NIER track,K. Hullett; N. Nagappan; E. Schuh; J. Hopson,"University of California at Santa Cruz, Santa Cruz, CA, USA",2011 33rd International Conference on Software Engineering (ICSE),20111010,2011,,,940,943,"The software engineering community has had seminal papers on data analysis for software productivity, quality, reliability, performance etc. Analyses have involved software systems ranging from desktop software to telecommunication switching systems. Little work has been done on the emerging digital game industry. In this paper we explore how data can drive game design and production decisions in game development. We define a mixture of qualitative and quantitative data sources, broken down into three broad categories: internal testing, external testing, and subjective evaluations. We present preliminary results of a case study of how data collected from users of a released game can inform subsequent development.",0270-5257;02705257,Electronic:978-1-4503-0445-0; POD:978-1-4503-0445-0,10.1145/1985793.1985952,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032557,game design;game development;game metrics,Communities;Games;Industries;Testing;USA Councils;Usability;Vehicles,computer games;data analysis;software quality;software reliability,data analysis;desktop software;digital game industry;external testing;game design;game development;game production decision;internal testing;software engineering;software performance;software productivity;software quality;software reliability;subjective evaluation;telecommunication switching system,,4,,16,,no,21-28 May 2011,,IEEE,IEEE Conference Publications
Data intensive architecture for scalable cyber analytics,B. Olsen; J. R. Johnson; T. Critchlow,"Pacific Northwest National Laboratory, Richland, WA",2011 IEEE International Conference on Technologies for Homeland Security (HST),20111219,2011,,,390,395,"Cyber analysts are tasked with the identification and mitigation of network exploits and threats. These compromises are difficult to identify due to the characteristics of cyber communication, the volume of traffic, and the duration of possible attack. In this paper, we describe a prototype implementation designed to provide cyber analysts an environment where they can interactively explore a month's worth of cyber security data. This prototype utilized On-Line Analytical Processing (OLAP) techniques to present a data cube to the analysts. The cube provides a summary of the data, allowing trends to be easily identified as well as the ability to easily pull up the original records comprising an event of interest. The cube was built using SQL Server Analysis Services (SSAS), with the interface to the cube provided by Tableau. This software infrastructure was supported by a novel hardware architecture comprising a Netezza TwinFin for the underlying data warehouse and a cube server with a FusionIO drive hosting the data cube. We evaluated this environment on a month's worth of artificial, but realistic, data using multiple queries provided by our cyber analysts. As our results indicate, OLAP technology has progressed to the point where it is in a unique position to provide novel insights to cyber analysts, as long as it is supported by an appropriate data intensive architecture.",,Electronic:978-1-4577-1376-7; POD:978-1-4577-1375-0,10.1109/THS.2011.6107901,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6107901,,Bismuth;Cities and towns;Computer architecture;Data warehouses;Home appliances;IP networks;Servers,SQL;computer network security;data mining;data warehouses;query processing;software architecture,FusionIO drive;Netezza TwinFin;OLAP technology;SQL server analysis service;Tableau;cyber analysts;cyber communication;cyber security data;data cube server;data intensive architecture;data warehouse;hardware architecture;network exploit mitigation;network threat identification;online analytical processing technique;scalable cyber analytics;software infrastructure,,0,,14,,no,15-17 Nov. 2011,,IEEE,IEEE Conference Publications
Data-driven diffusion modeling to examine deterrence,M. J. Lanham; G. P. Morgan; K. M. Carley,"The Institute for Software Research, Carnegie Mellon University, Pittsburgh, PA 15213, USA",2011 IEEE Network Science Workshop,20110905,2011,,,1,8,"The combination of social network extraction from texts, network analytics to identify key actors, and then simulation to assess alternative interventions in terms of their impact on the network is a powerful approach for supporting crisis de-escalation activities. In this paper, we describe how researchers used this approach as part of a scenario-driven modeling effort. We demonstrate the strength of going from data-to-model and the advantages of data-driven simulation. We conclude with a discussion of the limitations of this approach for the chosen policy domain and our anticipated future steps.",,Electronic:978-1-4577-1051-3; POD:978-1-4577-1049-0,10.1109/NSW.2011.6004651,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6004651,Belief Diffusion;Network Models;Rapid Prototyping;Text Mining,Analytical models;Cleaning;Data models;Organizations;Predictive models;Thesauri,data mining;social networking (online);text analysis,crisis de-escalation activities;data-driven diffusion modeling;data-driven simulation;network analytics;social network extraction;text mining,,1,,19,,no,22-24 June 2011,,IEEE,IEEE Conference Publications
Defining malware families based on analyst insights,J. Gennari; D. French,"CERT Program Software Engineering Institute, Carnegie Mellon University Pittsburgh, Pennsylvania 15213",2011 IEEE International Conference on Technologies for Homeland Security (HST),20111219,2011,,,396,401,"Determining whether arbitrary files are related to known malicious files is often useful in network and host-based defense. Doing so can give network defenders sufficient exemplars of a particular threat to develop comprehensive signatures and heuristics for identifying the threat, leading to decreased response time and improved prevention of a cyber attack. Identifying these malicious families is a complex process involving the categorization of potentially malicious code into sets that share similar features, while being distinguishable from unrelated threats or non-malicious code. Current methods for automatically or manually describing malware families are typically unable to distinguish between indicators derived from the structure of the malware and indicators derived from the behavior of the malware. Further, attempts to cluster potentially related files by mapping them into alternate domains, including histograms, fuzzy hashes, Bloom filters, and so on often produces clusters of files solely derived from structural information. These similarity measurements are often very effective on crudely similar files, yet they fail to identify files that have similar or identical behavior and semantics. We propose an analytic method, driven largely by human experience and based on objective criteria, for assigning arbitrary files membership in a malicious code family. We describe a process for iteratively refining the criteria used to select a malicious code family, until such criteria described are both necessary and sufficient to distinguish a particular malicious code family. We contrast this process with similar processes, such as antivirus signature generation and automatic and blind classification methods. We formalize this process to describe a roadmap for practitioners of malicious code analysis and to highlight opportunities for improvement and automation of both the process and the observation of relevant criteria. Finally, we provide experimental results of- applying this methodology to real-world malware.",,Electronic:978-1-4577-1376-7; POD:978-1-4577-1375-0,10.1109/THS.2011.6107902,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6107902,,Algorithm design and analysis;Humans;Implants;Malware;Reverse engineering;Runtime,computer viruses,Bloom filters;analyst insights;analytic method;antivirus signature generation;automatic classification methods;blind classification methods;cyber attack;fuzzy hash;histograms;host-based defense;malicious code family;malicious files;malware family definition;network defenders;threat identification,,1,,21,,no,15-17 Nov. 2011,,IEEE,IEEE Conference Publications
Design and implementation of a data analytics infrastructure in support of crisis informatics research: NIER track,K. M. Anderson; A. Schram,"University of Colorado, Boulder, CO, USA",2011 33rd International Conference on Software Engineering (ICSE),20111010,2011,,,844,847,"Crisis informatics is an emerging research area that studies how information and communication technology (ICT) is used in emergency response. An important branch of this area includes investigations of how members of the public make use of ICT to aid them during mass emergencies. Data collection and analytics during crisis events is a critical pre-requisite for performing such research, as the data generated during these events on social media networks are ephemeral and easily lost. We report on the current state of a crisis informatics data analytics infrastructure that we are developing in support of a broader, interdisciplinary research program. We also comment on the role that software engineering research plays in these increasingly common, highly-interdisciplinary research efforts.",0270-5257;02705257,Electronic:978-1-4503-0445-0; POD:978-1-4503-0445-0,10.1145/1985793.1985920,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032533,crisis informatics;scalability;software frameworks,Databases;Informatics;Java;Reliability;Software;Software engineering;Twitter,emergency services;information technology;software engineering,ICT;NIER track;crisis informatics;data analytics;data analytics infrastructure;data collection;emergency response;informatics research crisis;information and communication technology;social media networks;software engineering,,5,,3,,no,21-28 May 2011,,IEEE,IEEE Conference Publications
"Developing a common analytical framework for models, simulations, and data",N. R. Adam,"Science & Technology Directorate, U.S. Department of Homeland Security, Washington, DC, USA",2011 International Conference on Collaboration Technologies and Systems (CTS),20110711,2011,,,22,22,"DHS has recently established the Analytic Capability Development Working Group (ACDWG) whose aim is to reduce the lifecycle and capital investment costs of analytic efforts while continually enhancing the ability to inform decision-makers within mandated timeframes. Specifically, the working group is helping to develop a common analytic frameworks in order to address the challenges that users and developers of analytic tools face, including increasing awareness around existing analytical capabilities, spurring collaborations that reduce stove-piping and duplication of analytic effort, creating consistent, transparent, defensible, and shared knowledge, and streamlining development efforts at all levels. In this talk we discuss some of the challenges of developing a common analytic framework including models, simulations, and data.",,Electronic:978-1-61284-639-2; POD:978-1-61284-638-5,10.1109/CTS.2011.5928660,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5928660,Analytic Tools;Awareness;Collaboration;Decision Making;Frameworks;Shared Knowledge,Analytical models;Biological system modeling;Computational modeling;Data models;Information systems;Software;Terrorism,,,,0,,,,no,23-27 May 2011,,IEEE,IEEE Conference Publications
EdiFlow: Data-intensive interactive workflows for visual analytics,V. Benzaken; J. D. Fekete; P. L. HÌ©mery; W. Khemiri; I. Manolescu,"LRI, Universit&#x00E9; Paris Sud-11, Bat 490. Universit&#x00E9; 91405 Orsay, France",2011 IEEE 27th International Conference on Data Engineering,20110516,2011,,,780,791,"Visual analytics aims at combining interactive data visualization with data analysis tasks. Given the explosion in volume and complexity of scientific data, e.g., associated to biological or physical processes or social networks, visual analytics is called to play an important role in scientific data management. Most visual analytics platforms, however, are memory-based, and are therefore limited in the volume of data handled. More over, the integration of each new algorithm (e.g. for clustering) requires integrating it by hand into the platform. Finally, they lack the capability to define and deploy well-structured processes where users with different roles interact in a coordinated way sharing the same data and possibly the same visualizations. We have designed and implemented EdiFlow, a workflow platform for visual analytics applications. EdiFlow uses a simple structured process model, and is backed by a persistent database, storing both process information and process instance data. EdiFlow processes provide the usual process features (roles, structured control) and may integrate visual analytics tasks as activities. We present its architecture, deployment on a sample application, and main technical challenges involved.",1063-6382;10636382,Electronic:978-1-4244-8960-2; POD:978-1-4244-8959-6; USB:978-1-4244-8958-9,10.1109/ICDE.2011.5767914,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5767914,,Data models;Data visualization;Databases;Electronic publishing;Encyclopedias;Visual analytics,data analysis;workflow management software,data analysis;data intensive interactive workflow;data visualization;persistent database;scientific data management;visual analytic,,3,,34,,no,11-16 April 2011,,IEEE,IEEE Conference Publications
Efficient Fault Detection and Diagnosis in Complex Software Systems with Information-Theoretic Monitoring,M. Jiang; M. A. Munawar; T. Reidemeister; P. A. S. Ward,"University of Waterloo, Waterloo",IEEE Transactions on Dependable and Secure Computing,20110519,2011,8,4,510,522,"Management metrics of complex software systems exhibit stable correlations which can enable fault detection and diagnosis. Current approaches use specific analytic forms, typically linear, for modeling correlations. In practice, more complex nonlinear relationships exist between metrics. Moreover, most intermetric correlations form clusters rather than simple pairwise correlations. These clusters provide additional information and offer the possibility for optimization. In this paper, we address these issues by using Normalized Mutual Information (NMI) as a similarity measure to identify clusters of correlated metrics, without assuming any specific form for the metric relationships. We show how to apply the Wilcoxon Rank-Sum test on the entropy measures to detect errors in the system. We also present three diagnosis algorithms to locate faulty components: RatioScore, based on the Jaccard coefficient, SigScore, which incorporates knowledge of component dependencies, and BayesianScore, which uses Bayesian inference to assign a fault probability to each component. We evaluate our approach in the context of a complex enterprise application, and show that 1) stable, nonlinear correlations exist and can be captured with our approach; 2) we can detect a large fraction of faults with a low false positive rate (we detect up to 18 of the 22 faults we injected); and 3) we improve the diagnosis with our new diagnosis algorithms.",1545-5971;15455971,,10.1109/TDSC.2011.16,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5714701,Self-managing systems;autonomic systems.;fault detection;fault diagnosis;information theory;mutual information,Computational modeling;Correlation;Entropy;Measurement;Monitoring;Random variables;Uncertainty,entropy;fault location;fault tolerant computing;program testing;software metrics,Bayesian inference;BayesianScore;Jaccard coefficient;RatioScore;SigScore;Wilcoxon Rank-Sum test;complex enterprise application;complex software system;diagnosis algorithm;entropy measure;fault detection;fault diagnosis;fault probability;faulty component;information-theoretic monitoring;intermetric correlation;management metrics;normalized mutual information;omponent dependency;similarity measure,,7,1,20,,no,July-Aug. 2011,,IEEE,IEEE Journals & Magazines
Enabling Analysis and Measurement of Conventional Software Development Documents Using Project-Specific Formalism,T. Nakamura; H. Takeuchi; F. Iwama; K. Mizuno,"IBM Res. Tokyo, Yamato, Japan",2011 Joint Conference of the 21st International Workshop on Software Measurement and the 6th International Conference on Software Process and Product Measurement,20111229,2011,,,48,54,"We describe a new approach to modeling and analyzing software development documents that are typically written using conventional office applications. Our approach brings automation to content extraction, quality checking and measurement of massive document artifacts that tend to be handled by labor-intensive manual work in industry today. Rather than seeking an approach based on creation or rewriting of contents using more rigid, machine-friendly representations such as standardized formal models and restricted languages, we provide a method to deal with the diversity of document artifacts by making use of project-specific formalism that exists in target documents. We demonstrate that such project-specific formalism often tends to ""naturally"" exist at syntactic levels, and it is possible to define a ""document model"", a logical data representation gained by transformation rule from the physical, syntactic structure to the logical, semantic structure. With this transformation, various quality checking rules for completeness, consistency, traceability, etc., are realized by evaluating constraints for data items in the logical structure, and measurement of these quality aspects is automated. We developed a tool to allow a user to easily define document models and checking rules, and provide the insights on transformations when defining document models for various industry specification documents written in word processor files, spreadsheets and presentations. We also demonstrate the use of natural language processing can improve document modeling and quality checking by compensating for a weakness of formalism and applying analysis to specific parts of the target documents.",,Electronic:978-0-7695-4565-3; POD:978-1-4577-1930-1,10.1109/IWSM-MENSURA.2011.51,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6113043,document modeling;document quality metrics;project document analysis;quality;quality measurement;text analytics,Analytical models;Business;Data models;Estimation;Industries;Programming;Text analysis,formal verification;software quality;word processing,content extraction;document artifact;document modeling;industry specification documents;logical data representation;logical structure;natural language processing;physical structure;project specific formalism;quality checking rules;semantic structure;software development document;spreadsheets;syntactic structure;target document;transformation rule;word processor files,,1,,17,,no,3-4 Nov. 2011,,IEEE,IEEE Conference Publications
Ensuring tight computational security against higher-order DPA attacks,D. Khurana; A. Gaurav,"Department of Electrical Engineering, Indian Institute of Technology- Delhi, New Delhi, India","2011 Ninth Annual International Conference on Privacy, Security and Trust",20110804,2011,,,96,101,"While DES has been proven to be breakable within a day given sufficient computational power, AES is still in use because it is extremely resistant to cryptanalytic attacks. Power Analytic Attacks use power consumption traces of the hardware or software implementation of these algorithms to reduce search space exponentially in the size of the key, thereby making computational complexity several orders of magnitude lower. This paper analyzes the increase in the computational advantage of an adversary who uses DPA and higher order power analysis attacks as opposed to algorithmic cryptanalysis. We highlight why there can be no perfect masking against DPA, and then define a standard for the security of masking countermeasures to such attacks. The main contribution is a security metric for systems and a cut-off for the number of encryptions allowable for a given order of masking to make the system immune to higher order DPA attacks.",,Electronic:978-1-4577-0584-7; POD:978-1-4577-0582-3,10.1109/PST.2011.5971970,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5971970,complexity of power analysis attacks;computational security;higher-order Differential Power Analysis (HO-DPA);higher-order masking;security metric,Algorithm design and analysis;Complexity theory;Correlation;Cryptography;Hamming weight;Noise,computational complexity;cryptography;search problems,algorithmic cryptanalysis;computational complexity;computational security;cryptanalytic attack;differential power analysis;higher order power analysis attack;higher-order DPA attack;masking countermeasure;power analytic attack;power consumption trace;search space;security metric,,0,,18,,no,19-21 July 2011,,IEEE,IEEE Conference Publications
Establishing the expert decision-making strategy to improve the TFT-LCD quality,A. P. Chen; C. C. Chen; C. W. Chang; C. R. Wu,"Institute of Information Management, National Chiao Tung University, 1001 University Road, Hsin Chu 30010, Taiwan, R.O.C.",The 2nd International Conference on Next Generation Information Technology,20110728,2011,,,63,67,"At present, TFT-LCD industry demand and expand the development budget, the experts in collaboration design systems leadership and raise the alarm management system (AMS) quality. This study uses the analytic network process (ANP) and cause-effect grey relational analysis (CEGRA) model to establish the expert decision-making strategy to solve he problems in the TFT-LCD industry. Finally, the expert decision-making strategy approach to rank the collaboration design systems' quality of the TFT-LCD industry performance.",,Electronic:978-89-88678-39-8; POD:978-1-4577-0266-2,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5967473,TFT-LCD;alarm management system (AMS);analytic network process (ANP);cause-effect grey relational analysis (CEGRA);collaborative design;information technology (IT),Collaboration;Decision making;Expert systems;Industries;Manufacturing systems;Semiconductor device modeling;Software,budgeting;decision making;electronics industry;grey systems;groupware;liquid crystal displays;thin film transistors,AMS quality;CEGRA;TFT-LCD quality;alarm management system;analytic network process;cause-effect grey relational analysis;collaboration design systems;development budget;expert decision making strategy,,0,,31,,no,21-23 June 2011,,IEEE,IEEE Conference Publications
Evaluating conservation voltage reduction: An application of GridLAB-D: An open source software package,K. P. Schneider; J. C. Fuller; D. Chassin,"Pacific Northwest National Laboratory located at the Battelle Seattle Research Center in Seattle, Washington",2011 IEEE Power and Energy Society General Meeting,20111010,2011,,,1,6,"Conservation Voltage Reduction (CVR) is the reduction of energy consumption resulting from a reduction of the service voltage. While there have been numerous CVR deployments in North America, there has been little substantive analytic analysis of the effect; the majority of the published results are based on empirical field measurements. Due to the lack of analytic study, it is difficult to determine the impacts of CVR outside of sites that have conducted demonstration projects. This panel paper will examine a framework for the analysis of CVR using the open source software package GridLAB-D. An open source simulation environment is used to highlight the effectiveness of open source software programs and their ability to be used for evaluating multi-disciplinary smart grid technologies.",1932-5517;19325517,Electronic:978-1-4577-1002-5; POD:978-1-4577-1000-1; USB:978-1-4577-1001-8,10.1109/PES.2011.6039467,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6039467,Conservation voltage reduction;Newton-Raphson Method;distribution system analysis;forward-backward sweep method;load modeling;open source;power modeling;power simulation;smart grid,Analytical models;Buildings;Load modeling;Mathematical model;Smart grids;Solar heating;Waste heat,energy consumption;power engineering computing;public domain software;smart power grids,GridLAB-D;North America;conservation voltage reduction;empirical field measurements;energy consumption reduction;multidisciplinary smart grid technology;open source simulation environment;open source software package;service voltage reduction,,10,1,33,,no,24-29 July 2011,,IEEE,IEEE Conference Publications
"Evaluating human-automation interaction using task analytic behavior models, strategic knowledge-based erroneous human behavior generation, and model checking",M. L. Bolton; E. J. Bass,"San Jos&#x00E9; State University Research Foundation, NASA Ames Research Center, Moffett Field, CA 94035, USA","2011 IEEE International Conference on Systems, Man, and Cybernetics",20111121,2011,,,1788,1794,"Human-automation interaction, including erroneous human behavior, is a factor in the failure of complex, safety-critical systems. This paper presents a method for automatically generating task analytic models encompassing both erroneous and normative human behavior from normative task models by manipulating modeled strategic knowledge. Resulting models can be automatically translated into larger formal system models so that safety properties can be formally verified with a model checker. This allows analysts to prove that a human automation-interactive system (as represented by the formal model) will or will not satisfy safety properties with both normative and generated erroneous human behavior. This method is illustrated with a case study: the programming of a patient-controlled analgesia pump. In this example, a problem resulting from a generated erroneous human behavior is discovered and a potential solutions is explored. Future research directions are discussed.",1062-922X;1062922X,Electronic:978-1-4577-0653-0; POD:978-1-4577-0652-3,10.1109/ICSMC.2011.6083931,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6083931,Task analysis;formal methods;human error;human-automation interaction;model checking;system safety,Analytical models;Automation;Delay;Humans;Pressing;Principal component analysis;Programming,behavioural sciences computing;formal verification;human computer interaction;knowledge based systems;safety-critical software;task analysis,complex failure;erroneous human behavior;formal system model;human-automation interaction;modeled strategic knowledge manipulation;patient-controlled analgesia pump;safety property;safety-critical system;strategic knowledge-based erroneous human behavior generation;task analytic behavior model;task model checking,,3,,30,,no,9-12 Oct. 2011,,IEEE,IEEE Conference Publications
Evaluation of a PET prototype using LYSO:Ce monolithic detector blocks,R. Cuerdo; I. Sarasola; P. GarcÌ_a de Acilu; J. Navarrete; M. CaÌ±adas; J. C. Oller; J. M. Cela; P. Rato; L. Romero; C. Willmott,"CIEMAT (Centro de Investigaciones Energ&#x00E9;ticas, Medioambientales y Tecnol&#x00F3;gicas), Avda. Complutense 40, 28040 Madrid, Spain",2011 IEEE Nuclear Science Symposium Conference Record,20120220,2011,,,3342,3346,We have analyzed the performance of a PET demonstrator formed by two sectors of four monolithic detector blocks placed face-to-face. Both front-end and read-out electronics have been evaluated by means of coincidence measurements using a rotating <sup>22</sup>Na source placed at the center of the sectors in order to emulate the behavior of a complete full ring. A continuous training method based on neural network (NN) algorithms has been carried out to determine the entrance points over the surface of the detectors. Reconstructed images from 1 MBq <sup>22</sup>Na point source and <sup>22</sup>Na Derenzo phantom have been obtained using both filtered back projection (FBP) analytic methods and the OSEM 3D iterative algorithm available in the STIR software package [1]. Preliminary data on image reconstruction from a <sup>22</sup>Na point source with Ì÷ = 0.25 mm show spatial resolutions from 1.7 to 2.1 mm FWHM in the transverse plane. The results confirm the viability of this design for the development of a full-ring brain PET scanner compatible with magnetic resonance imaging for human studies.,1082-3654;10823654,Electronic:978-1-4673-0120-6; POD:978-1-4673-0118-3,10.1109/NSSMIC.2011.6152605,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6152605,,Application specific integrated circuits;Continuous wavelet transforms;Crystals;Magnetic resonance imaging;Magnetosphere;Positron emission tomography;Three dimensional displays,brain;cerium;image reconstruction;lutetium compounds;medical image processing;neural nets;phantoms;positron emission tomography;solid scintillation detectors;yttrium compounds,Derenzo phantom;LYSO:Ce monolithic detector blocks;Lu<sub>1.8</sub>Y<sub>0.2</sub>SiO<sub>5</sub>:Ce;OSEM 3D iterative algorithm;PET prototype;STIR software package;filtered back projection analytic methods;front-end electronics;full-ring brain PET scanner;image reconstruction;magnetic resonance imaging;monolithic scintillator crystals;neural network algorithms;read-out electronics;sodium point source,,1,,8,,no,23-29 Oct. 2011,,IEEE,IEEE Conference Publications
Exact localization of acoustic reflectors from quadratic constraints,A. Canclini; F. Antonacci; M. R. P. Thomas; J. Filos; A. Sarti; P. A. Naylor; S. Tubaro,"Politecnico di Milano, p.zza Leonardo da Vinci, 32, 20133, Italy",2011 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA),20111117,2011,,,17,20,In this paper we discuss a method for localizing acoustic reflectors in space based on acoustic measurements on source-to-microphone reflective paths. The method converts Time of Arrival (TOA) and Time Difference of Arrival (TDOA) into quadratic constraints on the line corresponding to the reflector. In order to be robust against measurement errors we derive an exact solution for the minimization of a cost function that combines an arbitrary number of quadratic constraints. Moreover we propose a new method for the analytic prediction of reflector localization accuracy. This method is sufficiently general to be applicable to a wide range of estimation problems.,1931-1168;19311168,Electronic:978-1-4577-0693-6; POD:978-1-4577-0692-9; USB:978-1-4577-0691-2,10.1109/ASPAA.2011.6082277,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6082277,Microphone arrays;acoustic reflector localization;environment reconstruction;space-time audio processing,Acoustics;Lips;Software;World Wide Web,acoustic measurement;acoustic radiators;acoustic signal processing;measurement errors;microphone arrays;minimisation;quadratic programming;time-of-arrival estimation,acoustic measurements;acoustic reflector localisation;cost function minimization;measurement errors;quadratic constraints;source-to-microphone reflective paths;time difference of arrival;time of arrival estimation,,4,,9,,no,16-19 Oct. 2011,,IEEE,IEEE Conference Publications
Exploring Approaches of Integration Software Architecture Modeling with Quality Analysis Models,L. Dobrica,"Fac. of Autom. & Comput., Univ. Politeh. of Bucharest, Bucharest, Romania",2011 Ninth Working IEEE/IFIP Conference on Software Architecture,20110721,2011,,,113,122,"One of the important benefits of model-to-model transformation is that it allows architects to design iteratively by analyzing and studying alternative or optimal solutions without redesign of the software architecture models or quality analytic models. The main contribution of this work is the presentation of five recently approaches based on the definition of a framework which applies separation of concerns in viewpoints and perspectives. The framework definition identifies viewpoints and their sets of concerns regarding the approaches achieving the goal of integration and interoperability of tools in a model-driven and quality-driven software architecture development. Each approach presentation is a multiple views description, where a view conforms to a viewpoint. The quality of each approach depends on the perspective under which the approach is analyzed. By applying various perspectives on the views composing each approach a software architect or a modeler can select the most appropriate one. Also based on this framework of presentation, the study identifies a current state of the research in the domain defined by these representative approaches, existent limitations and future research directions.",,Electronic:978-0-7695-4351-2; POD:978-1-61284-399-5,10.1109/WICSA.2011.23,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5959725,interoperability;model;quality;software architecture;transformation,Analytical models;Biological system modeling;Computer architecture;Object oriented modeling;Predictive models;Software architecture;Unified modeling language,integrated software;open systems;software architecture;software quality,interoperability;model-driven architecture;model-to-model transformation;quality analytic models;quality-driven architecture;software architecture;software development;software integration,,0,,33,,no,20-24 June 2011,,IEEE,IEEE Conference Publications
Extracting Named Entities at Web Scale for Competitive Intelligence,F. Pouilloux,"IXXO, France",2011 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology,20111010,2011,1,,501,501,"Summary form only given.Businesses of all sizes have now realized that the Web is an invaluable resource for competitive intelligence, and consequently business decision making. But many have trouble collecting targeted & useful information, and are often further overwhelmed by the time required for analysis & monitoring. On another hand, text mining techniques have become widely used for information analysis in the scientific community in general, and are now ubiquitous in most Web Intelligence fields. With the availability of services such as Google Prediction API, or mature open source software such as GATE, RapidMiner or NLTK, one can expect a much wider adoption of text mining and associated machine learning techniques by expert developers. But how can these techniques benefit to the daily life of a wider business audience? As competitive intelligence is often focused on products, people, customers and competitors, there is an added value for systems providing analytics on these entities, whose recognition is fundamental to text mining and semantic analysis, and consequently is still under active scientific investigation. In this talk we will tour some of the specific requirements and options for building an efficient Web based competitive intelligence system with named entity analytics. We will see how some savvy simplifications can help to overcome common issues such as Web scale and Web content noise, and finally deliver acceptable usability and value for non-specialists, business users.",,Electronic:978-0-7695-4513-4; POD:978-1-4577-1373-6,10.1109/WI-IAT.2011.284,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6040721,,Communities;Competitive intelligence;Industries;Logic gates;Software;Text mining,competitive intelligence;data mining;learning (artificial intelligence);public domain software;text analysis,GATE;Web Intelligence;Web based competitive intelligence system;Web content noise;Web scale;business decision making;business users;information analysis;machine learning technique;open source software;semantic analysis;text mining,,0,,,,no,22-27 Aug. 2011,,IEEE,IEEE Conference Publications
Filter models of CDM measurement channels and TLP device transients,T. J. Maloney; A. Daniel,"Intel Corporation, 2200 Mission College Blvd., SC9-09, Santa Clara, CA 95054 USA",EOS/ESD Symposium Proceedings,20111017,2011,,,1,9,Charged Device Model (CDM) waveforms are fast enough to be altered considerably by the oscilloscope used. Step response of CDM measurement channels allow time-domain finite impulse response (FIR) filters to be formulated in software. Similar analytic methods allow filter-like representation of devices as measured by transmission-line pulsing (TLP).,Pending,,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6045616,,Convolution;Filtering theory;Finite impulse response filter;Generators;IIR filters;Oscilloscopes;Transmission line measurements,FIR filters;oscilloscopes,CDM measurement channels;TLP device transients;charged device model measurement channels;charged device model waveforms;filter models;filter-like representation;oscilloscope;time-domain FIR filters;time-domain finite impulse response filter;transmission-line pulsing device transient,,0,,15,,no,11-16 Sept. 2011,,IEEE,IEEE Conference Publications
Finite Element Analysis on Vibration of Lifting Pipeline,Z. Zhijin; L. Hao; W. Zejun; W. Zhao,"Coll. of Electromech. Eng., Hunan Univ. of Sci. & Technol. Xiangtan, Xiangtan, China",2011 Third International Conference on Measuring Technology and Mechatronics Automation,20110228,2011,2,,225,229,"According to the theory of wave and current, the load of deep-sea mining lifting pipeline suffered in ocean environment is analyzed. Mechanical model of lifting pipeline is established by adopting Finite Element software of ANSYS, the stress of the first section of pipeline below sea level under combined effect of hydrostatic pressure, wave and current and plane stress of lifting pipeline at different water depth of 2000 m, 3000 m, 4000 m, 5000 m are studied in details, mode analysis of lifting pipeline of 5000 m length is also researched, and the first ten order natural frequency and modal shape are attained. Analytic results have an important significance on the design, check and choose of lifting pipeline, and it also provides some references for the further study on vibration mechanism and reduction.",2157-1473;21571473,Electronic:978-0-7695-4296-6; POD:978-1-4244-9010-3,10.1109/ICMTMA.2011.344,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5721162,Finite Element Analysis;Lifting Pipeline;Mode AnalysisI;Stress,Finite element methods;Force;Hydrodynamics;Pipelines;Sea level;Stress,finite element analysis;hydrostatics;lifting;mining;pipelines;vibrations,ANSYS;current theory;deep-sea mining lifting pipeline;finite element analysis;hydrostatic pressure;lifting pipeline vibration;mechanical model;ocean environment;wave theory,,0,,10,,no,6-7 Jan. 2011,,IEEE,IEEE Conference Publications
Fluid controlled models of computer networks under denial of service attacks,Ignatenko Oleksii Petrovich,"Institute of Software Systems NAS Ukraine, Kiev, Ukraine",2011 5th International Conference on Application of Information and Communication Technologies (AICT),20111222,2011,,,1,5,In this paper we consider fluid controlled models of networks. We proposed an new approach to analyzing network work using fluid models and conflict control theory domain. We obtain analytic solution for certain class of networks and proved time optimality. In the network game we found Nash equilibrium. We consider special case of network activity - denial of service attack and found dependency on download time depending of attack direction and traffic volume. The theoretical results were tested in an simulation environment OMNET++.,,Electronic:978-1-61284-832-7; POD:978-1-61284-831-0,10.1109/ICAICT.2011.6110970,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6110970,conflict controlled system;denial of service attacks;fluid model,Computer crime;Control systems;Educational institutions;Fluids;Games;Internet;Software systems,computer network security;game theory,Nash equilibrium;attack direction;computer networks;conflict control theory domain;denial of service attacks;download time;fluid controlled models;network activity;network game;simulation environment OMNET++;time optimality;traffic volume,,0,,10,,no,12-14 Oct. 2011,,IEEE,IEEE Conference Publications
GenAMap: Visualization strategies for structured association mapping,R. E. Curtis; P. Kinnaird; E. P. Xing,"Joint Carnegie Mellon-University of Pittsburgh, PhD Program for Computational Biology",2011 IEEE Symposium on Biological Data Visualization (BioVis).,20111205,2011,,,87,94,"Association mapping studies promise to link DNA mutations to gene expression data, possibly leading to innovative treatments for diseases. One challenge in large-scale association mapping studies is exploring the results of the computational analysis to find relevant and interesting associations. Although many association mapping studies find associations from a genome-wide collection of genomic data to hundreds or thousands of traits, current visualization software only allow these associations to be explored one trait at a time. The inability to explore the association of a genomic location to multiple traits hides the inherent interaction between traits in the analysis. Additionally, researchers must rely on collections of in-house scripts and multiple tools to perform an analysis, adding time and effort to find interesting associations. In this paper, we present a novel visual analytics system called GenAMap. GenAMap replaces the time-consuming analysis of large-scale association mapping studies with exploratory visualization tools that give geneticists an overview of the data and lead them to relevant information. We present the results of a preliminary evaluation that validated our basic approach.",,Electronic:978-1-4673-0004-9; POD:978-1-4673-0003-2,10.1109/BioVis.2011.6094052,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6094052,eQTL analysis;genome-wide association studies;structured association mapping;visual analytics,Algorithm design and analysis;Bioinformatics;Data visualization;Gene expression;Genomics;Heating,DNA;biology computing;data analysis;data mining;data visualisation,DNA mutations;GenAMap;computational analysis;gene expression data;in-house scripts;structured association mapping;visual analytics system;visualization software;visualization strategies,,0,,32,,no,23-24 Oct. 2011,,IEEE,IEEE Conference Publications
GPU-based beamformer: Fast realization of plane wave compounding and synthetic aperture imaging,B. Y. S. Yiu; I. K. H. Tsang; A. C. H. Yu,"Med. Eng. Program, Univ. of Hong Kong, Hong Kong, China","IEEE Transactions on Ultrasonics, Ferroelectrics, and Frequency Control",20110822,2011,58,8,1698,1705,"Although they show potential to improve ultrasound image quality, plane wave (PW) compounding and synthetic aperture (SA) imaging are computationally demanding and are known to be challenging to implement in real-time. In this work, we have developed a novel beamformer architecture with the real-time parallel processing capacity needed to enable fast realization of PW compounding and SA imaging. The beamformer hardware comprises an array of graphics processing units (GPUs) that are hosted within the same computer workstation. Their parallel computational resources are controlled by a pixel-based software processor that includes the operations of analytic signal conversion, delay-and-sum beamforming, and recursive compounding as required to generate images from the channel-domain data samples acquired using PW compounding and SA imaging principles. When using two GTX-480 GPUs for beamforming and one GTX-470 GPU for recursive compounding, the beamformer can compute compounded 512 ÌÑ 255 pixel PW and SA images at throughputs of over 4700 fps and 3000 fps, respectively, for imaging depths of 5 cm and 15 cm (32 receive channels, 40 MHz sampling rate). Its processing capacity can be further increased if additional GPUs or more advanced models of GPU are used.",0885-3010;08853010,,10.1109/TUFFC.2011.1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5995229,,Apertures;Arrays;Delay;Firing;Imaging;Instruction sets;Real time systems,array signal processing;biomedical transducers;biomedical ultrasonics;coprocessors;medical image processing;parallel processing;ultrasonic transducer arrays,GPU array;GPU based beamformer;GTX-470 GPU;GTX-480 GPU;analytic signal conversion;beamformer architecture;beamformer hardware;channel domain data samples;delay and sum beamforming;graphics processing units;pixel based software processor;plane wave compounding;real time parallel processing;recursive compounding;synthetic aperture imaging;ultrasound image quality improvement,,30,,11,,no,11-Aug,,IEEE,IEEE Journals & Magazines
High availability in Controller Area Networks,R. Pinto; J. Rufino; C. Almeida,"FC - UL Lisbon, Portugal",2011 IEEE EUROCON - International Conference on Computer as a Tool,20110623,2011,,,1,4,"The Controller Area Network (CAN) fieldbus is a popular technology for distributed embedded system networking. Its usage spans several domains, ranging from home automation to factory control. There are, however, restrictions to its application in a specific domain: highly dependable applications. A crucial step towards CAN-based dependable systems was taken by the CAN Enhanced Layer (CANELy) architecture. This architecture provided the analytic models of CAN operation. Based on these models, it defines both the hardware and software mechanisms for dependable and timely CAN-based network service provision. This paper provides the materialization of CANELy network availability mechanisms. The mapping of the error monitoring and fault-confinement mechanisms defined by CANELy into hardware has proven to be resource-effective, allowing their integration in an inexpensive Field Programmable Gate Array (FPGA) device. This opens room for cost-effective high dependability CAN-based solutions in several domains, e.g. the aerospace industry.",,Electronic:978-1-4244-7487-5; POD:978-1-4244-7486-8,10.1109/EUROCON.2011.5929399,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5929399,Controller Area Network (CAN);Dependability;Embedded and Real-time Systems;High Availability,Architecture;Availability;Computer architecture;Field programmable gate arrays;Media;Monitoring;Redundancy,controller area networks;embedded systems;factory automation;field buses;field programmable gate arrays;home automation;telecommunication network reliability,CAN Enhanced Layer architecture;CAN fleldbus;CANELy architecture;FPGA device;aerospace industry;controller area network fleldbus;distributed embedded system networking;error monitoring mapping;fault-confinement mechanism;field programmable gate array device,,0,,11,,no,27-29 April 2011,,IEEE,IEEE Conference Publications
Human centricity and perception-based perspective of architectures of Granular Computing,W. Pedrycz,"Department of Electrical & Computer Engineering, University of Alberta, Edmonton Canada","Cognitive Informatics & Cognitive Computing (ICCI*CC ), 2011 10th IEEE International Conference on",20110908,2011,,,3,3,"In spite of their striking diversity, numerous tasks and architectures of intelligent systems such as those permeating multivariable data analysis (e.g., time series, spatio-temporal, and spatial dependencies), decision-making processes along with their models, recommender systems and others exhibit two evident commonalities. They promote human centricity and vigorously engage perceptions (rather than plain numeric entities) in the realization of the systems and their usage. Information granules play a pivotal role in such settings. In the sequel, Granular Computing delivers a cohesive framework supporting a formation of information granules and facilitating their processing. We exploit two essential concepts of Granular Computing. The first one, formed with the aid of a principle of justifiable granularity, deals with the construction of information granules. The second one, based on an idea of an optimal allocation of information granularity, helps endow constructs of intelligent systems with a very much required conceptual and modeling flexibility. The talk covers in detail two representative studies. The first one is concerned with a granular interpretation of temporal data where the role of information granularity is profoundly visible when effectively supporting human centric description of relationships existing in data. In the second study being focused on the Analytic Hierarchy Process (AHP) used in decision-making, we show how an optimal allocation of granularity helps facilitate collaborative activities (e.g., consensus building) in group decision-making.",,Electronic:978-1-4577-1697-3; POD:978-1-4577-1695-9,10.1109/COGINF.2011.6016114,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6016114,,Helium,decision making;granular computing;groupware;human factors;knowledge based systems;software architecture,AHP;analytic hierarchy process;collaborative activities;decision making;granular computing;human centricity;information granularity;intelligent systems;perception-based perspective,,1,,,,no,18-20 Aug. 2011,,IEEE,IEEE Conference Publications
In-Memory Database Support for Source Code Search and Analytics,O. Panchenko,"Hasso Plattner Inst. for Software Syst. Eng., Potsdam, Germany",2011 18th Working Conference on Reverse Engineering,20111117,2011,,,421,424,"Software engineers are coerced to deal with a large amount of information about source code. Appropriate tools could assist to handle it, but existing tools are not capable of processing and presenting such a large amount of information sufficiently. With the advent of in-memory column-oriented databases the performance of some data-intensive applications could be significantly improved. This has resulted in a completely new user experience of those applications and enabled new use-cases. This PhD thesis investigates the applicability of in-memory column-oriented databases for supporting daily software engineering activities. The major research question addressed in this thesis is as follows: does in-memory column-oriented database technology provide the necessary performance advantages for working interactively with large amounts of fine-grained structural information about source code? To investigate this research question two scenarios have been selected that particularly suffer from low performance. The first selected scenario is source code search. Existing source code repositories contain a large amount of structural data. Interface definitions, abstract syntax trees, and call graphs are examples of such structural data. Existing tools have solved the performance problems either by reducing the amount of data because of using a coarse-grained representation, or by preparing answers to developers' questions in advance, or by reducing the scope of search. All currently existing alternatives result in the loss of developers' productivity. The second scenario is source code analytics. To complete reverse engineering tasks software engineers often are required to analyze a number of atomic facts that have been extracted from source code. Examples of such atomic facts are occurrences of certain syntactic patterns in code, software product metrics or violations of development guidelines. Each fact typically has several characteristics, such as the type of the fact,- - the location in code where found, and some attributes. Particularly, analysis of large software systems requires the ability to process a large amount of such facts efficiently. During industrial experiments conducted for this thesis it was evidenced that in-memory technology provides performance gains that improve developers' productivity and enable scenarios previously not possible. This thesis overlaps both software engineering and database technology. From the viewpoint of software engineering, it seeks to find a way to support developers in dealing with a large amount of structural data. From the viewpoint of database technology, source code search and analytics are domains for studying fundamental issues of storing and querying structural data.",1095-1350;10951350,Electronic:978-0-7695-4582-0; POD:978-1-4577-1948-6,10.1109/WCRE.2011.60,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6079871,Source code search;in-memory database technology;source code analytics,Business;Conferences;Database languages;Databases;Software engineering;Software systems,database management systems;software engineering;source coding,abstract syntax trees;atomic facts;call graphs;coarse-grained representation;data-intensive application;in-memory column-oriented database technology;in-memory database support;interface definition;reverse engineering task;software engineering activity;software product metrics;software system;source code analytics;source code repository;source code search;structural data;syntactic patterns,,0,,18,,no,17-20 Oct. 2011,,IEEE,IEEE Conference Publications
Innovation capacity appraisal of junior college engineering students based on AHP-TOPSIS,D. Chunhua,"Graduate School Wuhan University of Technology, Wuhan, China",2011 IEEE 3rd International Conference on Communication Software and Networks,20110908,2011,,,85,88,"Cultivating the practical capacity and technical operations of students should be attached the greatest importance by junior college, at the premise of ensuring theoretical knowledge during teaching process. Only equipped with innovation capacity, can engineering students combine theory and technology, engineering and science, school and social reality, so as to be able to meet the needs of society. In this study, Analytic Hierarchy Process (AHP) and Technique for Order Preference by Similarity to an Ideal Solution (TOPSIS)) are employed. By sampling 30 students from the College of Henan Mechanical and Electrical majoring in Mechanical, the innovation evaluation model is established according to evaluation indicators as integrated analysis capacity, project planning capacity, program development capacity, project implementation capacity, innovation and development capacity, and organizational coordination capacity. At first, the attribute weight vectors of indicators shall be obtained by using AHP, and then the corresponding indicators shall be calculated by TOPSIS, so that the integrated evaluation result is given. The result shows that this method has a very good discrimination in innovation capacity evaluation of junior college engineering students.",,Electronic:978-1-61284-486-2; POD:978-1-61284-485-5,10.1109/ICCSN.2011.6013782,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6013782,AHP;TOPSIS;engineering;evaluation;innovation capacity;junior college,Instruments,decision making;engineering education;further education;innovation management;mechanical engineering;teaching,AHP-TOPSIS;analytic hierarchy process;college of Henan;innovation capacity appraisal;integrated analysis capacity;junior college engineering students;mechanical major;program development capacity;project implementation capacity;project planning capacity;social reality;teaching process,,0,,9,,no,27-29 May 2011,,IEEE,IEEE Conference Publications
"Interaction, Mediation, and Ties: An Analytic Hierarchy for Socio-Technical Systems",D. D. Suthers,"Dept. of Inf. & Comput. Sci., Univ. of Hawaii, Honolulu, HI, USA",2011 44th Hawaii International Conference on System Sciences,20110222,2011,,,1,10,"To understand how technological designs encourage synergistic encounters between people and ideas within socio-technical systems, techniques are needed to bridge between levels of description from process traces such as log data, through individual trajectories of activity that interact with each other, to dynamic networks of associations that are both created by and further shape these interactions. Towards this end, we have developed an analytic hierarchy and associated representations. Process traces are abstracted to contingency and uptake graphs: directed graphs that record observed relationships (contingencies) between events that offer evidence for interaction and other influences between actors (uptake). Contingency graphs are further abstracted to associograms: two-mode directed graphs that record how associations between actors are mediated by digital artifacts. Patterns in associograms summarize sequential patterns of interaction. Transitive closure of associograms yields sociograms, to which existing network analytic techniques may be applied. We discuss how the hierarchy bridges between theoretical levels of analysis.",1530-1605;15301605,Electronic:978-0-7695-4282-9; POD:978-1-4244-9618-1,10.1109/HICSS.2011.248,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5718896,,Analytical models;Bridges;Context modeling;Media;Mediation;Sociotechnical systems;Software,decision making;directed graphs;graphs;social networking (online);social sciences computing,analytic hierarchy system;association network;associogram pattern;contingency graph;digital artifact;directed graph;log data;socio-technical system;technological design;transitive closure,,5,,25,,no,4-7 Jan. 2011,,IEEE,IEEE Conference Publications
Interactive data analysis with nSpace2<sup>å¨</sup>,C. M. Canfield; D. Sheffield,"Oculus Info Inc., USA",2011 IEEE Conference on Visual Analytics Science and Technology (VAST),20111215,2011,,,327,328,"nSpace2 is an innovative visual analytics tool that was the primary platform used to search, evaluate, and organize the data in the VAST 2011 Mini Challenge #3 dataset. nSpace2 is a web-based tool that is designed to facilitate the back-and-forth flow of the multiple steps of an analysis process, including search, data triage, organization, sense-making, and reporting. This paper describes how nSpace2 was used to assist every step of the analysis process for this VAST challenge.",,Electronic:978-1-4673-0014-8; POD:978-1-4673-0015-5,10.1109/VAST.2011.6102497,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6102497,analysis workflow;human information interaction;sense-making;visual analytics,Arrays;Data visualization;Electronic mail;Graphical user interfaces;Humans;Terrorism;Visual analytics,Internet;data analysis;data visualisation;public domain software,VAST 2011 Mini Challenge 3 dataset;Web-based tool;back-and-forth flow analysis process;innovative visual analytics tool;interactive data analysis;nSpace2,,0,,3,,no,23-28 Oct. 2011,,IEEE,IEEE Conference Publications
Intraoperative device targeting using real-time MRI,E. K. Brodsky; W. F. Block; A. L. Alexander; M. E. Emborg; C. D. Ross; K. A. Sillay,"Departments of Medical Physics, Radiology, and Neurological Surgery at the University of Wisconsin, Madison, WI USA",Proceedings of the 2011 Biomedical Sciences and Engineering Conference: Image Informatics and Analytics in Biomedicine,20110609,2011,,,1,4,"Real-time MRI has the potential to significantly improve intraoperative surgical processes. However, development of these technologies is complicated by the need for software tools that support task-oriented visualization of 2D and 3D datasets, enable flexible real-time image processing pathways, and interface with proprietary scanner hardware. Developing such tools requires substantial software engineering effort for these low-level tasks that distracts from a focus on developing and implementing algorithms that relate to the surgical technique in question. RTHawk and Vurtigo are extensible software platforms that simplify the development of tools for performing real-time MRI-guided procedures. We present here initial work on a system for performing intracerebral drug infusion under real-time MRI guidance using the Navigus pivot point-based MRI-compatible external trajectory guide. Initial testing performed using a GE scanner targeting a rigid object in a water-filled phantom allowed realtime aiming and infusion monitoring with 1 mm targeting accuracy. In vivo testing is forthcoming.",,Electronic:978-1-61284-410-7; POD:978-1-61284-411-4,10.1109/BSEC.2011.5872335,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5872335,,Catheters;Magnetic resonance imaging;Monitoring;Phantoms;Real time systems;Three dimensional displays;Trajectory,biomedical MRI;biomedical equipment;brain;drug delivery systems;medical computing;phantoms;surgery,GE scanner;Navigus pivot point-based MRI-compatible external trajectory guide;RTHawk software platform;Vurtigo software platform;in-vivo testing;intracerebral drug infusion;intraoperative device;real-time MRI-guided procedures;surgical processes;water-filled phantom,,1,,8,,no,15-17 March 2011,,IEEE,IEEE Conference Publications
Keynote Abstracts,Deyi Li; T. Li,"Inst. of Electron. Syst. Eng. of China, Beijing, China",2011 IEEE 8th International Conference on e-Business Engineering,20111215,2011,,,xxxii,xxxiv,The following topics are dealt with: e-business engineering; data management; service-oriented knowledge management; emergency communications; multi-agent supply chain collaboration operation model; cloud computing; business analytics; business optimization; Web service selection; user centric security mode; mobile commerce; pervasive commerce; service engineering; software engineering; warehousing approach; RFID; process automation; nonintrusive load monitoring system; e-marketplace integration; e-marketplace interoperability; cloud services; and business intelligence.,,Electronic:978-0-7695-4518-9; POD:978-1-4577-1404-7,10.1109/ICEBE.2011.80,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6104586,,,cloud computing;competitive intelligence;knowledge management;mobile commerce;multi-agent systems;open systems;radiofrequency identification;security of data;service-oriented architecture;supply chain management;warehouse automation,RFID;Web service selection;business analytics;business intelligence;business optimization;cloud computing;cloud services;data management;e-business engineering;e-marketplace integration;e-marketplace interoperability;emergency communications;mobile commerce;multiagent supply chain collaboration operation model;nonintrusive load monitoring system;pervasive commerce;process automation;service engineering;service-oriented knowledge management;software engineering;user centric security mode;warehousing approach,,0,,,,no,19-21 Oct. 2011,,IEEE,IEEE Conference Publications
Keynote Addresses,T. Kamimura; D. Port,,2011 Joint Conference of the 21st International Workshop on Software Measurement and the 6th International Conference on Software Process and Product Measurement,20111229,2011,,,xiv,xv,These tutorials discuss the following: Business analytics and optimization in software development-experience at IBM Rational; and Measurement impossible: How a measure for value saved NASA JPL's software assurance program.,,Electronic:978-0-7695-4565-3; POD:978-1-4577-1930-1,10.1109/IWSM-MENSURA.2011.46,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6113035,,,business data processing;software metrics;software process improvement,IBM Rational;business analytics;business optimization;measurement impossible;product measurement;software development;software measurement;software process;value saved NASA JPL software assurance program,,0,,,,no,3-4 Nov. 2011,,IEEE,IEEE Conference Publications
Keynote talk 2: How to build an industrial R&D center in Vietnam: A case study,H. L. Nguyen,"TMA Solutions, Vietnam",The 2011 International Conference on Advanced Technologies for Communications (ATC 2011),20110926,2011,,,2,2,"Summary form only given. To be competitive in today's market, the IT industry faces many challenges in the development and maintenance of enterprise information systems. Engineering these largescaled systems efficiently requires making decisions about a number of issues. In addition, customers expectations imply continuous software delivery in predictable quality. The operation such systems demands for transparency of the software in regard to lifecycle, change and incident management as well as cost efficiency. Addressing these challenges, we learned how to benefit from traditional industries. Contrary to the fact that the IT business calls itself gladly an industry, the industrialization of software engineering in most cases moves on a rather modest level. Industrialization means not only to build a solution or product on top of managed and well-defined processes, but also to have access to structured information about the current conditions of manufacturing at any time. Comparably with test series and assembly lines of the automobile industry, each individual component and each step from the beginning of manufacturing up to the final product should be equipped with measuring points for quality and progress. Even one step further the product itself, after it has left the factory, should be able to continuously provide analytic data for diagnostic reasons. Information is automatically collected and builds the basic essentials for process control, optimization and continuous improvement of the software engineering process. This presentation shows by means of a practical experience report how AdNovum managed to build its software engineering based on a well-balanced system of processes, continuous measurement and control ‰ÛÓ as well as a healthy portion of pragmatism. We implemented an efficient and predictable software delivery pipeline",2162-1020;21621020,Electronic:978-1-4577-1207-4; POD:978-1-4577-1206-7,10.1109/ATC.2011.6027420,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6027420,,,,,,0,,,,no,2-4 Aug. 2011,,IEEE,IEEE Conference Publications
"Knowledge Extraction and Reuse within ""Smart"" Service Centers",C. Wang; R. Akella; S. Ramachandran; D. Hinnant,"Technol. & Inf. Manage., Univ. of California Santa Cruz, Santa Cruz, CA, USA",2011 Annual SRII Global Conference,20110718,2011,,,163,176,"In this paper, we describe the initial version of a text analytics system under development and use at Cisco, where the objective is to ""optimize"" the productivity and effectiveness of the service center. More broadly, we discuss the practical needs in industry for developing powerful ""Smart"" Service Centers and the gaps in research to meet these needs. Ideally, service engineers in service centers should be utilized to handle issues which have not been solved previously and machines should be used to solve problems already solved, or at least help the service engineers obtain pertinent information from related and solved service cases when responding to a new request. Such a role for a machine would be a core element of the ""Smart Services"" offering. Hence, design of a highly efficient human-machine combination to derive insights from text and respond to a user request, is critical and fundamental, this enables service agents to capture relevant information quickly and accurately, and to develop the foundation for upper layer applications. Despite extensive earlier literature, the optimization for service process that involves very long, unstructured documents referencing a number of technology and product related terms with implicit inter-relationships has not been fully investigated. Our approach enables firms such as Cisco to achieve efficient service delivery by automating knowledge extraction to support ""Self Service"" by end users. The Cisco text analytics system termed Service Request Analyzer and Recommender (SRAR) addresses gaps in the Support Services function, by optimizing the use of human resources and software analytics in the service delivery process. The Analyzer is able to handle complex service requests (SRs) and to present categorized and pertinent information to service agents, based on which the Recommender, an upper layer application, is built to retrieve similar solved SRs, when presented with a new request. Our contributions in the context of- - text analysis and system design are three-fold. First, we identify the elements of the diagnostic process underlying the creation of SRs, and design a hierarchical classifier to decompose the complex SRs into those elements. Such decomposition provides specific information from the functional perspectives about ""What was the problem?"" ""Why did it occur?"" and ""How was it solved?"" which assists service agents in acquiring the knowledge they need more effectively and rapidly. Second, we build an SR Recommender on top of SR Analyzer to extend the system functionality for improved knowledge reuse, to measure SR similarity for more accurate recommendation of SRs. Third, we validate our SRAR in an initial pilot study in the service center for Cisco network diagnostics and support, and demonstrate the effectiveness and extensibility of our system. Our system appears applicable to the service centers across multiple domains, including networks, aerospace, semiconductors, automotive, health care, and financial services, and potentially adapted and expanded to all the other business functions of an enterprise. We conclude by indicating open research problems and new research directions, to expand the set of problems that need to be addressed in developing a Smart Support Services capability, and the solutions required to achieve them. These include the capture, retrieval, and reuse of more refined, structured and granulated knowledge, as well as the use of forum threads and semi-automated, dynamic categorization, together with considerations of the optimal use of humans and machine learning based software. Other aspects we discuss include recommendation systems based on temporal pattern clustering and incentives for experts to permit their expertise to be captured for machine (re-)use.",2166-0778;21660778,Electronic:978-0-7695-4371-0; POD:978-1-61284-415-2,10.1109/SRII.2011.28,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5958085,diagnostic business process;information retrieval;knowledge extraction;knowledge reuse;service centers;smart services;text mining,Business;Context;Data mining;Humans;Knowledge engineering;Semantics;Strontium,data mining;information retrieval;knowledge acquisition;learning (artificial intelligence);recommender systems;text analysis,Cisco;SRAR;complex service requests;human-machine combination;information retrieval;knowledge extraction;knowledge reuse;machine learning based software;optimization;recommendation systems;service delivery process;service request analyzer and recommender;smart service centers;support services function,,0,,21,,no,March 29 2011-April 2 2011,,IEEE,IEEE Conference Publications
"Landslide vulnerability evaluation: A case study from Sichuan, China",T. Liao; D. Hu; X. Li; Y. Chen,"College of Resource Environment & Tourism, Capital Normal University, Beijing, China 100048",2011 19th International Conference on Geoinformatics,20110811,2011,,,1,4,"Selecting Sichuan Province as the study area, setting rainfall-induced shallow landslide hazard for the research objectives, choosing slope, fault zone, rivers, lithology, soil type and vegetation as the main evaluation factors of the environmental vulnerability assessment. Based on the landslides disaster database since 5.12 Earthquake in Sichuan Province, the weights of the evaluation factors are determined. Producing 2008-2010 environmental vulnerability assessment maps in Sichuan Province Using Analytic Hierarchy Process with the GIS software, and the Spatial resolution of which is 250 meters. The risk of the environmental vulnerability assessment maps are divided into five levels: Slight(I), mild(II), moderate(III), high(IV) and the highest (V) risk areas. From this study, we conclude that vulnerability is with temporal and spatial distribution, and it can be quantified effectively using AHP model.",2161-024X;2161024X,Electronic:978-1-61284-848-8; POD:978-1-61284-849-5,10.1109/GeoInformatics.2011.5981061,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5981061,Analytic Hierarchy Process;environmental vulnerability assessment;rainfall-induced shallow landslide hazard,Databases;Hazards;Reliability;Rocks;Soil;Terrain factors,disasters;geographic information systems;geomorphology;hazards,AD 2008 to 2010;China;GIS software;Sichuan;analytic hierarchy process;environmental vulnerability assessment maps;evaluation factors;fault zone;high risk level;highest risk level;landslide vulnerability evaluation;lithology;mild risk level;moderate risk level;rainfall-induced shallow landslide hazard;rivers;slight risk level;soil type;spatial resolution;vegetation,,1,,11,,no,24-26 June 2011,,IEEE,IEEE Conference Publications
LAR-CC: Large atomic regions with conditional commits,E. Borin; Y. Wu; M. Breternitz; C. Wang,"Institute of Computing, University of Campinas",International Symposium on Code Generation and Optimization (CGO 2011),20110505,2011,,,54,63,"HW/SW Co-designed systems rely on dynamic binary translation and optimizations for efficient execution of binary code. Due to memory ordering properties and other architectural constraints, most binary optimizations are applied to regions of code that are atomically executed. To ensure that the underlying hardware has enough speculative resources to execute the whole atomic region, these systems typically form short atomic regions, with only 20 to 30 instructions. However, the shorter is the atomic region the smaller is the scope for optimizations. We present LAR-CC, a novel technique that enables HW/SW co-designed systems to optimize large atomic regions and dynamically fit them into the available speculative hardware resources by means of conditional commits. The LAR-CC technique consists of two major components: 1) conditional branch instructions to conditionally skip commit operations; 2) code transformations that replace commit operations by conditional commits and enable optimizations to be applied on the large atomic regions. Our experiments show that LAR-CC can effectively achieve dynamic atomic region sizes larger than 1000 instructions, providing sufficiently large scope to apply many advanced optimizations on HW/SW co-designed systems.",,Electronic:978-1-61284-357-5; POD:978-1-61284-356-8,10.1109/CGO.2011.5764674,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5764674,,Buffer storage;Gears;Hardware;Optimization;Program processors;Runtime,binary codes;hardware-software codesign;optimising compilers,HW/SW co-designed systems;LAR-CC technique;architectural constraints;binary code;code transformations;conditional branch instructions;conditional commits;dynamic binary translation;large atomic regions;memory ordering properties,,2,,23,,no,2-6 April 2011,,IEEE,IEEE Conference Publications
Large in-memory cyber-physical security-related analytics via scalable coherent shared memory architectures,J. R. Williams; S. Herrero; C. Leonardi; S. Chan; A. Sanchez; Z. Aung,"Engineering Systems Division, Massachusetts Institute of Technology, Cambridge USA",2011 IEEE Symposium on Computational Intelligence in Cyber Security (CICS),20110711,2011,,,1,9,"Cyber-physical security-related queries and analytics run on traditional relational databases can take many hours to return. Furthermore, programming analytics on distributed databases requires great skill, and there is a shortage of such talent worldwide. In this talk on computational intelligence within cyber security, we will review developments of processing large datasets in-memory using a coherent shared memory approach. The coherent shared memory approach allows programmers to view a cluster of servers as a system with a single large RAM. By hiding the actual system architecture under a software layer, we proffer a more intuitive programming model. Furthermore, the design of applications is ‰ÛÏtimeless‰Ûù since hardware upgrades require no changes to the software. The advantages of shared memory are countered by some disadvantages in that race conditions can occur; however, in many of these cases, we can provide models that protect us against such problems. Exemplars include sensemaking of Twitter feeds, the processing of Smart Meter datasets, and the large scale simulation of the caching of files at disparate points around the globe.",,Electronic:978-1-4244-9906-9; POD:978-1-4244-9905-2,10.1109/CICYBS.2011.5949414,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5949414,coherent shared memory;multicore;sensemaking,Algorithm design and analysis;Computer security;Electricity;Meter reading;Random access memory;Support vector machine classification;Twitter,distributed databases;relational databases;security of data;shared memory systems,cyber-physical security-related queries;distributed databases;relational databases;scalable coherent shared memory architectures;smart meter datasets;software layer,,0,,19,,no,11-15 April 2011,,IEEE,IEEE Conference Publications
Log File Analysis of E-commerce Systems in Rich Internet Web 2.0 Applications,C. J. Aivalis; A. C. Boucouvalas,"Dept. of Telecommun. Sci. & Technol., Univ. of Peloponnese, Tripoli, Greece",2011 15th Panhellenic Conference on Informatics,20111103,2011,,,222,226,"This paper describes the implications of the new trends in web development languages on log file analysis for e-commerce. The new trends in Software Development and the available Web Analytics technologies are explained. The focus is placed on the diversifications introduced to the traces left on the system that by Rich Internet Applications (RIAs). Finally a novel hybrid solution is proposed that is based on the junction of log files with operational data and page tagging, which allows even exacter measurements of customer behavior. It allows a customization of the Analysis Tool and survives the shift of the technologies.",,Electronic:978-0-7695-4389-5; POD:978-1-61284-962-1,10.1109/PCI.2011.31,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065092,E-Commerce;GWT;JavaScript;Log File Analysis;Rich Internet Applications;Web Analytics,Browsers;Databases;Google;Java;Web servers,Internet;consumer behaviour;electronic commerce;file organisation;software engineering,Web 2.0 applications;Web analytic technologies;Web development languages;customer behavior;e-commerce;exacter measurements;hybrid solution;log file analysis;operational data;page tagging;rich Internet application;software development,,2,,12,,no,Sept. 30 2011-Oct. 2 2011,,IEEE,IEEE Conference Publications
Magnetic field analysis of bearingless permanent magnet motor,N. Zhong-jin; F. Liang; C. Mao-jun; X. Shu-dong; N. Zhong-jin; F. Liang; C. Mao-jun,"College of Engineering, Zhejiang A&F University, Linan, China","2011 International Conference on Consumer Electronics, Communications and Networks (CECNet)",20110516,2011,,,450,453,"As bearingless motors have all advantages of magnetic bearings; they play an important role in solving difficult problems of special electric drives. The simplest and most versatile form of bearingless motor is the bearingless slice rotor motor. This kind of motor with centrifugal pump has a wide range of applications in life science, chemical processing industry, semiconductor manufacturing industry, foodstuff processing industry, and so on. This paper presents the analysis of the operational principle, basic equations is given for a bearingless permanent magnet slice electromotor first. Then a modeling of a bearingless permanent magnet slice electromotor is built using Maxwell 3D of ANSOFT finite element software. Finally, the magnetic field and the torque force are analyzed for the two different structures of P<sub>B</sub>=P<sub>M</sub>å±1. According to the analytic results, the structure of P<sub>B</sub>=P<sub>M</sub>+1 is more steady. In addition; its control is easier to get in the actual implementation and application.",,DVD:978-1-61284-457-2; Electronic:978-1-61284-459-6; POD:978-1-61284-458-9,10.1109/CECNET.2011.5768466,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5768466,ANSOFT;Bearingless Permanent Magnet motor;Finite Element;Slice,Finite element methods;Force;Magnetic fields;Permanent magnet motors;Rotors;Torque;Windings,finite element analysis;magnetic fields;permanent magnet motors;rotors;torque motors,bearingless permanent magnet motor;bearingless permanent magnet slice electromotor;bearingless slice rotor motor;magnetic field analysis;torque force,,1,,8,,no,16-18 April 2011,,IEEE,IEEE Conference Publications
Maleku: An evolutionary visual software analysis tool for providing insights into software evolution,A. GonzÌÁlez-Torres; R. TherÌ_n; F. J. GarcÌ_a-PeÌ±alvo; M. Wermelinger; Y. Yu,"Department of Computer Sciences, University of Salamanca, Spain",2011 27th IEEE International Conference on Software Maintenance (ICSM),20111117,2011,,,594,597,"Software maintenance is a complex process that requires the understanding and comprehension of software project details. It involves the understanding of the evolution of the software project, hundreds of software components and the relationships among software items in the form of inheritance, interface implementation, coupling and cohesion. Consequently, the aim of evolutionary visual software analytics is to support software project managers and developers during software maintenance. It takes into account the mining of evolutionary data, the subsequent analysis of the results produced by the mining process for producing evolution facts, the use of visualizations supported by interaction techniques and the active participation of users. Hence, this paper proposes an evolutionary visual software analytics tool for the exploration and comparison of project structural, interface implementation and class hierarchy data, and the correlation of structural data with metrics, as well as socio-technical relationships. Its main contribution is a tool that automatically retrieves evolutionary software facts and represent them using a scalable visualization design.",1063-6773;10636773,Electronic:978-1-4577-0664-6; POD:978-1-4577-0663-9,10.1109/ICSM.2011.6080838,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080838,,Data mining;Data visualization;Engines;Measurement;Software;Visual analytics,data mining;data visualisation;software maintenance;software tools;user interfaces,Maleku tool;evolutionary data mining;evolutionary visual software analysis tool;interaction technique;scalable visualization design;software evolution;software maintenance;software project detail;structural data;user participation,,2,,7,,yes,25-30 Sept. 2011,,IEEE,IEEE Conference Publications
Managing Procurement Spend Using Advanced Compliance Analytics,P. Chowdhary; M. Ettl; A. Dhurandhar; S. Ghosh; G. Maniachari; B. Graves; B. Schaefer; Y. Tang,"Bus. Analytics & Math Sci., IBM T J Watson Res. Center, Yorktown Heights, NY, USA",2011 IEEE 8th International Conference on e-Business Engineering,20111215,2011,,,139,144,"Often the processes for purchasing commodities and services within a business enterprise are centralized into a procurement organization. These purchases are often sourced from one or more suppliers, or vendors, based on contract terms and conditions (such as price, payment terms etc.), availability, and quality or legacy habit of purchasing service with known vendors. We have found that many organizations lack appropriate processes and disciplines to drive demand to preferred suppliers. Thus these enterprises are unable to leverage the value of the pre-negotiated contracts due to lack of process education, approval process steps or appropriate purchasing tools that could result in significant amounts of spending that would be considered not compliant (not being sourced through preferred suppliers). Depending upon the size of the organization, such transactions range from several million dollars to billions of dollars. Manually sifting or employing typical query tools to review large amounts of spend transaction data with multiple attributes to identify the level of non compliant spend and identify areas to take action is a daunting task. In this paper, we discuss a software solution for spend compliance analytics that includes measurements of cost savings due to increased compliance and identification of areas where spend tends to be non compliant. We have developed a web enabled advanced analytical solution called Compliance Analytics Tool (CAT) that embeds a two phase methodology for compliance management. In the first phase, we use advanced data mining techniques to segment a large amount of historical spend transactions to quickly identify promising areas of improvement, exploiting a multitude of purchasing attributes such as business unit, procurement category, suppliers, etc. The second phase employs portfolio optimization techniques to further focus on specific segments that provide maximum benefit based on desired compliance targets or available budget. We- also discuss the solution architecture that integrates business analytics along with business intelligence tools, dashboards, and data warehousing.",,Electronic:978-0-7695-4518-9; POD:978-1-4577-1404-7,10.1109/ICEBE.2011.57,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6104610,CPLEX;Compliance Analytics;Data Mining;Optimization;Procurement;SPSS Modeler;Spend Analysis,Companies;Engines;Investments;Optimization;Procurement,Internet;budgeting data processing;business data processing;competitive intelligence;contracts;data mining;data warehouses;procurement;purchasing,Web enabled advanced analytical solution;advanced compliance analytics;advanced data mining techniques;approval process steps;budget;business analytics;business enterprise;business intelligence tools;compliance analytic tool;compliance management;contract conditions;contract terms;cost savings;dashboards;data warehousing;historical spend transactions;legacy habit;portfolio optimization techniques;prenegotiated contracts;process education;procurement organization;procurement spending management;purchasing service;query tools;software solution;spend compliance analytics,,0,,8,,no,19-21 Oct. 2011,,IEEE,IEEE Conference Publications
Mapping an epidemic outbreak: Effective analysis and presentation,K. Boone; E. Swing,"Vision Systems & Technology, Inc., a SAS Company, Australia",2011 IEEE Conference on Visual Analytics Science and Technology (VAST),20111215,2011,,,307,308,"The microblog challenge presented an opportunity to use commercial software for visual analysis. An epidemic outbreak occurred in the city of Vastopolis, requiring visualizations of symptoms and their spread over time. Using these tools, analysts could successfully identify the outbreak's origin and pattern of dispersion. The maps used to analyze the data and present the results provided clear, easily understood representations, and presented a logical explanation of a complex progression of events.",,Electronic:978-1-4673-0014-8; POD:978-1-4673-0015-5,10.1109/VAST.2011.6102486,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6102486,ArcGIS;Information Visualization;Spatial Analysis,Bridges;Data visualization;Dispersion;Rivers;Software;Stomach;Synthetic aperture sonar,Web sites;cartography;data analysis;data visualisation;epidemics;medical computing,Vastopolis city;commercial software;data analysis;dispersion pattern;epidemic outbreak mapping;map;microblog challenge;outbreak origin;symptom visualization;visual analysis,,0,,1,,no,23-28 Oct. 2011,,IEEE,IEEE Conference Publications
Measuring firewall security,S. Al-Haj; E. Al-Shaer,"Department of Software and Information Systems, University of North Carolina Charlotte, USA",2011 4th Symposium on Configuration Analytics and Automation (SAFECONFIG),20111226,2011,,,1,4,"In the recent years, more attention is given to firewalls as they are considered the corner stone in Cyber defense perimeters. The ability to measure the quality of protection of a firewall policy is a key step to assess the defense level for any network. To accomplish this task, it is important to define objective metrics that are formally provable and practically useful. In this work, we propose a set of metrics that can objectively evaluate and compare the hardness and similarities of access policies of single firewalls based on rules tightness, the distribution of the allowed traffic, and security requirements. In order to analyze firewall polices based on the policy semantic, we used a canonical representation of firewall rules using Binary Decision Diagrams (BDDs) regardless of the rules format and representation. The contribution of this work comes in measuring and comparing firewall security deterministically in term of security compliance and weakness in order to optimize security policy and engineering.",,Electronic:978-1-4673-0402-3; POD:978-1-4673-0401-6,10.1109/SafeConfig.2011.6111669,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6111669,,Complexity theory;Data structures;Equations;Fires;Indexes;Measurement;Security,authorisation;binary decision diagrams;computer network security,BDD;binary decision diagram;cyber defense perimeter;firewall policy semantic;firewall security measurement;metrics set,,0,,8,,no,Oct. 31 2011-Nov. 1 2011,,IEEE,IEEE Conference Publications
Metrics for service granularity in Service Oriented Architecture,Haiqing Bu,"School of Software Shanghai Jiao Tong University, China",Proceedings of 2011 International Conference on Computer Science and Network Technology,20120412,2011,1,,491,494,"Service Oriented Architecture (SOA) is becoming an increasingly popular architectural style for many organizations due to the promised agility, flexibility benefits. One of the prominent principles of designing services is the matter of how abstract services should be i.e. granularity. Since service-oriented analysis and design methods lack on providing a quantitative metric for service granularity level evaluation, identification of optimally granular services is the key challenge in service-oriented solution development. This paper is divided into three types of services, and listed on the properties of service granularity. Various properties of the three service types the weight is not the same, in order to get the appropriate service granularity, Using the analytic hierarchy process analysis of the attributes in each service type.",,Electronic:978-1-4577-1587-7; POD:978-1-4577-1586-0,10.1109/ICCSNT.2011.6182003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6182003,AHP;SOA;service granularity,Business;Complexity theory;Context modeling;Helium;Measurement;Service oriented architecture,decision making;service-oriented architecture;software metrics,SOA;abstract services;analytic hierarchy process analysis;quantitative metric;service granularity level evaluation;service oriented architecture,,0,,7,,no,24-26 Dec. 2011,,IEEE,IEEE Conference Publications
Mining Twitter using cloud computing,A. Rashid Hussain; M. A. Hameed; N. P. Hegde,"Research & Development, Host Analytics Software Pvt. Ltd., India",2011 World Congress on Information and Communication Technologies,20120130,2011,,,187,190,"Today Social Networks have an undeniable impact on how users communicate search and share data online. This data has opened up new venues of research on sociology. Unfortunately mining and analyzing this large amount of data can be a difficult task and expensive in terms of computational resources. With the advent of cloud computing, data mining and analysis can be made more accessible due to its cost-effective computational resources. As a source of large data, we propose to use data from Twitter considering both its popularity and the open nature of its API. A large study of information propagation within Twitter reveals that a majority of users are passive information consumers and do not forward the content to even their own network. In this paper, we propose and demonstrate the usage of cloud computing platforms as a possible solution for mining and analyzing large amounts of data, while inspecting the Retweet Graph of a user to calculate their influence within their network.",,Electronic:978-1-4673-0126-8; POD:978-1-4673-0127-5,10.1109/WICT.2011.6141241,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6141241,cloud computing;data mining;social graph,Cloud computing;Data mining;Engines;Google;Twitter,application program interfaces;cloud computing;data analysis;data mining;social networking (online),API;Retweet Graph;Twitter mining;are passive information consumers;cloud computing;computational resources;data anlysis;data mining;social networks;sociology,,2,,15,,no,11-14 Dec. 2011,,IEEE,IEEE Conference Publications
Modeling and Analyzing Server System with Rejuvenation through SysML and Stochastic Reward Nets,E. C. Andrade; F. Machida; D. S. Kim; K. S. Trivedi,"Inf. Center, Fed. Univ. of Pernambuco (UFPE), Recife, Brazil","2011 Sixth International Conference on Availability, Reliability and Security",20111017,2011,,,161,168,"High-availability assurance of server systems is becoming an important issue, since many mission-critical applications are implemented on server systems. To achieve high-availability, software rejuvenation is a practical technique to reduce unexpected downtime caused by software aging in software applications running on server systems. Although analytic models of software rejuvenation are well-studied, such analysis is not used in server system administration due to the complexity of modeling. In this paper, we present an availability modeling method for server system with software rejuvenation based on SysML that is used to describe system configurations and maintenance operations semi-formally. The proposed approach allows system administrators, who do not have expertise in availability modeling, to design and study the effects of different rejuvenation policies deployed in server systems. To show the applicability of the proposed modeling and evaluation process, a case study of a web application server is presented. We show the correctness of our modeling method by comparing the conventional models for condition-based and time-based software rejuvenation.",,Electronic:978-0-7695-4485-4; POD:978-1-4577-0979-1,10.1109/ARES.2011.28,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6045928,Availability assessment;SysML;server system;software rejuvenation;stochastic reward nets,Availability;Clocks;Security;Through-silicon vias;Tin,Internet;Petri nets;file servers;software engineering,SysML;Web application server;condition-based software rejuvenation;server system;software aging;stochastic reward nets;time-based software rejuvenation,,2,,12,,no,22-26 Aug. 2011,,IEEE,IEEE Conference Publications
Multi-elements Decision-making method based on grey decision model and analytic hierarchy process,Jian-Ming Zhou; Xing-wen Hao; Ze-Wang Ju,"School of Accounting and Statistics, WeiFang College of Education, Qingzhou, Shandong, China, 262500","2011 International Conference on Remote Sensing, Environment and Transportation Engineering",20110728,2011,,,3288,3291,"Multi-elements Decision-making is one of the important methods which simulate reality problems in both social and economic fields. By using grey decision model and analytic hierarchy process, it probes a newly method of multi-elements decision-making. And by using the software of Matlab and Excel, it also develops a process of optimization sorting algorithm which could describe and calculate the actual problem accurately. The multi-element decision-making method proposed is feasibility and available with high practical value. Base on selecting and sorting multi-elements in decision-making, the method could be used in the design project selection, policy analysis, decision forecast.",,DVD:978-1-4244-9170-4; Electronic:978-1-4244-9171-1; POD:978-1-4244-9172-8,10.1109/RSETE.2011.5965015,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5965015,Analytical Hierarchy process;Grey Decision Model;Matlab;Technique for Order Preference by Similarity to Ideal Solution,Analytical models;Decision making;Economics;Educational institutions;Indexes;Mathematical model,decision making;grey systems;optimisation;sorting,Excel;Matlab software;analytic hierarchy process;grey decision model;multielements decision-making method;optimization sorting algorithm,,0,,7,,no,24-26 June 2011,,IEEE,IEEE Conference Publications
Multiple non-functional criteria service selection based on user preferences and decision strategies,R. Karim; C. Ding; C. H. Chi,"Department of Computer Science, Ryerson University, Toronto, Ontario, Canada",2011 IEEE International Conference on Service-Oriented Computing and Applications (SOCA),20120309,2011,,,1,8,"In order to choose from a list of functionally similar services, users often need to make their decisions based on multiple non-functional criteria they require on the target service. It is a natural fit to apply the Multi-Criteria Decision Making (MCDM) theory to this selection problem. However, the high demand of MCDM approaches on user expertise and user involvement could become an obstacle of using them for service selection. In this paper, we address this issue by taking a user-centric standpoint to design the non-functional criteria based service selection system. On one hand, we try to reduce the workload and the skill level requirement on users. On the other hand, we still give them the flexibility to define the necessary information, which include their preferences on multiple criteria, as well as the decision strategies they would follow to select the desired services from a list of alternatives. The former is crucial for optimal decision making. The latter is often ignored by most of the service selection systems and a common default selection strategy is to find a service which has the best overall score calculated by a certain formula. In reality, users may not necessarily follow this strategy and there are many other possible strategies they may follow. We should take this into consideration when designing selection systems. We use a case study to show that our system could produce a more accurate and customized result for individual users.",2163-2871;21632871,Electronic:978-1-4673-0319-4; POD:978-1-4673-0318-7,10.1109/SOCA.2011.6166200,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6166200,Analytic Network Process (ANP);Decision Strategy;Multi-Criteria Decision Making (MCDM);Non-functional Criteria;Service Selection,Companies;Decision making;Educational institutions;Filtering;Reliability;Scalability;Security,decision making;operations research,MCDM approach;decision strategy;designing selection system;multicriteria decision making theory;multiple nonfunctional criteria;multiple nonfunctional criteria based service selection system design;optimal decision making;service selection system;skill level requirement;target service;user-centric standpoint,,2,,19,,no,12-14 Dec. 2011,,IEEE,IEEE Conference Publications
Never Die Network Extended with Cognitive Wireless Network for Disaster Information System,N. Uchida; K. Takahata; Y. Shibata; N. Shiratori,"Fac. of Software & Inf. Sci., Iwate Prefectural Univ., Takizawa, Japan","2011 International Conference on Complex, Intelligent, and Software Intensive Systems",20110818,2011,,,24,31,"In the actual disaster case, there is a certain possibility that electric power line is damaged and power energy cannot be supplied to those communication network devices, and eventually those wireless LANs cannot be functioned. Although mobile wireless network is easy to reconstruct than wired network, there may be the case that network disconnection is not affordable after disaster. That is, Disaster Information System needs a robust Never Die Network (NDN) which will be unaffected by any changes in environment after severe disaster. Satellite Network System is one of possible solution for such a severe disaster, but it has some problems like low throughput, large latency, high cost, and so on. On the other hands, single wireless communication like IEEE802.11a/b/g also have some problems like a possible transmission distance, throughput limitation for maintaining QoS for urgent user's situations. Therefore, NDN needs to consider about additional functions for these problems of Satellite and Wireless Network System. In this paper, we introduce Satellite System for optimal transmission control method in Cognitive Wireless Network in order to consider with severe disaster. First, as our previous study, proper wireless link and route selection is held by Extend AHP and Extend AODV with Min-Max AHP value methods for optimal transmission control in Cognitive Wireless Network. Then, check-alive function, alternate data transmission function, possible alternative route suggestion, and network reconfiguration are introduced to our proposed Disaster Information Network by using Satellite System. In the simulation, ns2 are used for the computational results to the effectiveness of the suggested transmission methods in the hybrid system of cognitive wireless and satellite network system.",,Electronic:978-0-7695-4373-4; POD:978-1-61284-709-2,10.1109/CISIS.2011.14,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5989014,Cognitive Wireless Network;Disaster Information Network;Never Die Network;QoS,Data communication;Electric fields;Satellite broadcasting;Satellites;Throughput;Wireless networks,ad hoc networks;cognitive radio;disasters;quality of service;satellite communication;telecommunication control;telecommunication network routing;wireless LAN,IEEE802.11a/b/g;QoS;ad hoc on-demand distance vector;analytic hierarchy process;cognitive wireless network;communication network devices;data transmission;disaster information system;extend AHP;extend AODV;min-max AHP value;mobile wireless network;never die network;optimal transmission control;route selection;satellite network system;wireless LAN;wireless link,,12,,10,,no,June 30 2011-July 2 2011,,IEEE,IEEE Conference Publications
New analytic results for the incomplete Toronto function and incomplete Lipschitz-Hankel Integrals,P. C. Sofotasios; S. Freear,"School of Electronic and Electrical Engineering, University of Leeds, UK",2011 SBMO/IEEE MTT-S International Microwave and Optoelectronics Conference (IMOC 2011),20120315,2011,,,44,47,"This paper provides novel analytic expressions for the incomplete Toronto function, T<sub>B</sub>(m, n, r), and the incomplete Lipschitz-Hankel Integrals of the modified Bessel function of the first kind, Ie<sub>ë_, n</sub>(a, z). These expressions are expressed in closed-form and are valid for the case that m ‰ä´ n and n being an odd multiple of 1/2, i.e. n å± 0.5 ‰öö ‰ã¥ Capitalizing on these, tight upper and lower bounds are subsequently proposed for both T<sub>B</sub>(m, n, r) function and Ie<sub>ë_, n</sub>(a, z) integrals. Importantly, all new representations are expressed in closed-form whilst the proposed bounds are shown to be rather tight. To this effect, they can be effectively exploited in various analytical studies related to wireless communication theory. Indicative applications include, among others, the performance evaluation of digital communications over fading channels and the information-theoretic analysis of multiple-input multiple-output systems.",Pending,Electronic:978-1-4577-1664-5; POD:978-1-4577-1662-1,10.1109/IMOC.2011.6169356,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6169356,Closed-form representations;Incomplete Lipschitz-Hankel Integrals;Incomplete Toronto function;Marcum Q-function;fading;special functions;upper and lower bounds,Closed-form solutions;Educational institutions;Fading;Integral equations;MIMO;Software packages;Wireless communication,Bessel functions;MIMO communication;fading channels;information theory,Bessel function;Toronto function;digital communications;fading channels;incomplete Lipschitz-Hankel integrals;information theoretic analysis;multiple-input multiple-output systems,,3,,13,,no,Oct. 29 2011-Nov. 1 2011,,IEEE,IEEE Conference Publications
New approach for application architecture adequacy in hardware/software embedded system design,Y. Manai; J. HaggÌ¬ge; M. Benrejeb,"LA.R.A, Ecole Nationale d'Ing&#x00E9;nieurs de Tunis, BP 37, le Belv&#x00E9;d&#x00E8;re, 1002, Tunisie",2011 Faible Tension Faible Consommation (FTFC),20110711,2011,,,39,42,"This paper deals with a new approach to minimize the gap between application development and the architecture synthesis, in the hardware/software embedded system design flow. Indeed, two intermediate models are proposed, the analytic H<sub>model</sub>, and the MAC<sub>Builder</sub> environment. The first one allows embedded system modelling with a state space approach in order to determine the recursive equations, whereas, the second one builds a set of task graph that models the system functionalities. Further, this paper investigates how a similar approach can be applied to improve the controller architecture design through a case study.",,Electronic:978-1-61284-647-7; POD:978-1-61284-646-0,10.1109/FTFC.2011.5948913,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5948913,Application/Architecture Adequacy;Building model MAC<inf>Builder</inf>;Direct Acyclic Graph;MAC Operation;analytic model H,Hardware;MATLAB;Mathematical model,controllers;embedded systems;recursive estimation,MAC<;sub>;Builder<;/sub>; environment;controller architecture design;hardware-software embedded system design;recursive equations,,0,,9,,no,May 30 2011-June 1 2011,,IEEE,IEEE Conference Publications
Non-linear compensation algorithm of LOS locating in aerial remote sensor,J. Li; J. Xiu; P. Huang; Y. Li; L. Chen,"Changchun Inst. of Opt., Fine Mech. &amp; Phys., Chinese Acad. of Sci., Changchun, China",Proceedings of 2011 International Conference on Electronic & Mechanical Engineering and Information Technology,20110919,2011,5,,2454,2457,"A non-linear light-of-sight(LOS) locating compensation algorithm for certain aerial remote sensor is presented in this paper. The LOS locating system is composed of two main components, which are an inertial reference frame to provide coarse LOS locating and a 2-axis Fast Steering Mirror to remove the frame's residual error. The azimuth and elevation of LOS are both affected by the FSM two-degree-of-freedom rotation, which should be dealt with to raise the accuracy of LOS location. This paper extrapolates LOS locating algorithm of FSM by coordinate transformation, and gives corresponding deviation analysis for different LOS poiting angles. It is found that the angle deviation grows rapidly with range of FSM's travel enlarging, so the deviation must be eliminated. Through the data analysis for deviation, a non-linear compensation algorithm is proved effective to reduce the error of LOS significantly. Taking LOS changing in [-3 deg, 3 deg] as an example, two analytic expressions are given, and the LOS locating accuracy rises by 34 times compared with non-linear compensation unused, and the cross coupling of LOS loating is 0.08%.",,DVD:978-1-61284-086-4; Electronic:978-1-61284-088-8; POD:978-1-61284-087-1,10.1109/EMEIT.2011.6023539,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6023539,FSM;aerial remote sensor;coordinate transformation;light-of-sight;non-linear compensation,Accuracy;Algorithm design and analysis;Mirrors;Optical sensors;Remote sensing;Software,geophysical equipment;mirrors;optical sensors;remote sensing,LOS poiting angles;aerial remote sensor;coordinate transformation;data analysis;fast steering mirror;inertial reference frame;light-of-sight locating compensation algorithm;nonlinear compensation algorithm;two-degree-of-freedom rotation,,0,,6,,no,12-14 Aug. 2011,,IEEE,IEEE Conference Publications
Notice of Retraction<BR>Assessment of the logistics service supply Chain's coordination performance based on ANP,Q. Juan-juan,"Business school, Tianjin University of Finance & Economics, Tianjin, China",2011 International Conference on E-Business and E-Government (ICEE),20110616,2011,,,1,4,"Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR>The index system of logistics service supply chain's coordination performance is constructed from four aspects: service flow coordination, information flow coordination, capital flow coordination and work flow coordination, considering the importance of the coordination in the logistics service supply chain. The ANP is used to build the assessment model. The model is solved by adopting the Super Decision software, with the example, to get the final results of the coordination performance assessment.",,Electronic:978-1-4244-8694-6; POD:978-1-4244-8691-5,10.1109/ICEBEG.2011.5881285,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5881285,Logistics service supply chain;Super Decision;analytic network process (ANP);assessment;coordination,Analytical models;Contracts;Indexes;Software;Supply chains,,,,0,,16,,no,6-8 May 2011,,IEEE,IEEE Conference Publications
Notice of Retraction<BR>Study and application of optimization design scheme of roadway bolt support parameter,J. Zhang; X. Gong; L. Guo; Z. Kang,"College of Resources and Environment, Hebei Polytechnic University, Tangshan 063009 China",2011 International Conference on Electric Information and Control Engineering,20110527,2011,,,6137,6140,"Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR>According to the incomplete clear mechanism of bolt support action presently, the bolt support parameter of 12910 face haulage roadway is given optimized design in HANDAN TAOYI mine by the analytic system software: ANSYS, using theory analysis, numerical simulation and engineering actual measurement. The surrounding rock failure region of the optimized roadway is 12.54m<sup>2</sup>, decreased by 31.0% compared to present bolt support roadway, which has theoretical significance and practical value in popularizing bolt support technique and ensuring the support roadway safe and reliable.",,Electronic:978-1-4244-8039-5; POD:978-1-4244-8036-4,10.1109/ICEICE.2011.5777514,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5777514,ANSYS;bolt support;numerical simulation;optimization,Coal;Educational institutions;Fasteners;Finite element methods;Numerical simulation;Rocks;Stress,failure (mechanical);fasteners;mechanical engineering computing;numerical analysis;roads;rocks,12910 face haulage roadway;HANDAN TAOYI mine;analytic system software;bolt support parameter;bolt support technique;numerical simulation;optimization design scheme;roadway bolt support parameter;rock failure region,,0,,12,,no,15-17 April 2011,,IEEE,IEEE Conference Publications
Optimal Model-Based Policies for Component Migration of Mobile Cloud Services,R. Gabner; H. P. Schwefel; K. A. Hummel; G. Haring,"Telecommun. Res. Center Vienna, Vienna, Austria",2011 IEEE 10th International Symposium on Network Computing and Applications,20111010,2011,,,195,202,"Two recent trends are major motivators for service component migration: the upcoming use of cloud-based services and the increasing number of mobile users accessing Internet-based services via wireless networks. While cloud-based services target the vision of Software as a Service, where services are ubiquitously available, mobile use leads to varying connectivity properties. In spite of temporary weak connections and even disconnections, services should remain operational. This paper investigates service component migration between the mobile client and the infrastructure-based cloud as a means to avoid service failures and improve service performance. Hereby, migration decisions are controlled by policies. To investigate component migration performance, an analytical Markov model is introduced. The proposed model uses a two-phased approach to compute the probability to finish within a deadline for a given reconfiguration policy. The model itself can be used to determine the optimal policy and to quantify the gain that is obtained via reconfiguration. Numerical results from the analytic model show the benefit of reconfigurations and the impact of different reconfigurations applied to three service types, as immediate reconfigurations are in many cases not optimal, a threshold on time before reconfiguration can take place is introduced to control reconfiguration.",,Electronic:978-0-7695-4489-2; POD:978-1-4577-1052-0,10.1109/NCA.2011.33,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6038602,Markov model;component migration;mobile services,Analytical models;Computational modeling;Delay;Markov processes;Mobile communication;Mobile computing;Mobile handsets,Markov processes;Web services;cloud computing;mobile computing;software fault tolerance,Internet-based service;analytical Markov model;connectivity property;infrastructure-based cloud;mobile client;mobile cloud services;optimal model-based policy;probability;reconfiguration policy;service component migration;service failure avoidance;software as a service;two-phased approach;wireless network,,3,,19,,no,25-27 Aug. 2011,,IEEE,IEEE Conference Publications
"Orion: A system for modeling, transformation and visualization of multidimensional heterogeneous networks",J. Heer; A. Perer,"Stanford University, USA",2011 IEEE Conference on Visual Analytics Science and Technology (VAST),20111215,2011,,,51,60,"The study of complex activities such as scientific production and software development often require modeling connections among heterogeneous entities including people, institutions and artifacts. Despite numerous advances in algorithms and visualization techniques for understanding such social networks, the process of constructing network models and performing exploratory analysis remains difficult and time-consuming. In this paper we present Orion, a system for interactive modeling, transformation and visualization of network data. Orion's interface enables the rapid manipulation of large graphs-including the specification of complex linking relationships-using simple drag-and-drop operations with desired node types. Orion maps these user interactions to statements in a declarative workflow language that incorporates both relational operators (e.g., selection, aggregation and joins) and network analytics (e.g., centrality measures). We demonstrate how these features enable analysts to flexibly construct and compare networks in domains such as online health communities, academic collaboration and distributed software development.",,Electronic:978-1-4673-0014-8; POD:978-1-4673-0015-5,10.1109/VAST.2011.6102441,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6102441,data management;data transformation;end-user programming;graphs;social network analysis;visualization,Analytical models;Communities;Data models;Data visualization;Joining processes;Social network services,data visualisation;graph theory;graphical user interfaces;social networking (online),Orion interface;academic collaboration;declarative workflow language;distributed software development;drag-and-drop operations;graph manipulation;multidimensional heterogeneous network modelling;multidimensional heterogeneous network transformation;multidimensional heterogeneous network visualization;network analytics;online health communities;relational operators;social networks,,5,,34,,no,23-28 Oct. 2011,,IEEE,IEEE Conference Publications
Performance Analysis of Key Management Schemes in Wireless Sensor Network Using Analytic Hierarchy Process,N. Ruan; Y. Ren; Y. Hori; K. Sakurai,"Dept. of Inf., Kyushu Univ., Fukuoka, Japan","2011IEEE 10th International Conference on Trust, Security and Privacy in Computing and Communications",20120102,2011,,,1739,1744,"To achieve security in wireless sensor networks (WSNs), key management is one of the most challenging issues in design of WSN due to resource-constrained sensor nodes. Various key management schemes (KMs) have been proposed to enable encryption and authentication in WSN for different application scenarios. According to different requirements, it is important to select the trustworthy KMs in a WSN for setting up a fully appropriate WSN mechanism. An Analytic Hierarchy Process (AHP)-aided method helping with the complex decision has been presented in our previous work. Our purpose in this paper is to do performance analysis of KMs in WSN using our previous AHP-aided method. We analyze the characters of abundance KMs intuitively. The following five performance criteria are considered: scalability, key connectivity, resilience, storage overhead and communication overhead. As all permutations of five performance criteria include 120 types' situations, experimental analyses on 43 KMs for the optimum selection are presented.",2324-898X;2324898X,Electronic:978-0-7695-4600-1; POD:978-1-4577-2135-9,10.1109/TrustCom.2011.243,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6121040,Analytic Hierarchy Process;Experimental analysis;Key management schemes;Optimum selection;Wireless sensor network,Linear matrix inequalities;Performance analysis;Proposals;Scalability;Security;Vectors;Wireless sensor networks,decision making;telecommunication network reliability;telecommunication security;wireless sensor networks,AHP-aided method;WSN;analytic hierarchy process-aided method;authentication;communication overhead;encryption;key connectivity;key management scheme;key management schemes;resilience;resource-constrained sensor node;scalability;storage overhead;wireless sensor network,,0,,32,,no,16-18 Nov. 2011,,IEEE,IEEE Conference Publications
Performance appraisal of Student Affairs Management based on group analytic hierarchy process,Y. Xiao; S. Fu,"Foreign Language Department, Hunan University of Finance and Economics, Changsha, P.R. China",2011 IEEE 3rd International Conference on Communication Software and Networks,20110908,2011,,,196,200,"Student Affairs Management is a key component of College Management and one of important factors affecting the long-term development of colleges. Furthermore, the management quality of student affairs directly determines the core competitiveness of a college. This paper has built the index system of Student Affairs Management performance appraisal in College A. This index system applies the Group AHP, which is based on cluster analysis, to determine index weights and fuzzy comprehensive evaluation, which aims at collected data, to carry on a comprehensive evaluation about the Student Affairs Management level of College A. Evaluation results verify and demonstrate that the combination of group AHP cluster analysis and fuzzy comprehensive evaluation is more scientific and effective, and that it can be better applied to the performance appraisal of Student Affairs Management of this college and other colleges in the similar level.",,Electronic:978-1-61284-486-2; POD:978-1-61284-485-5,10.1109/ICCSN.2011.6013808,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6013808,cluster analysis;fuzzy comprehensive evaluation;group analytic hierarchy process (Group AHP);performance appraisal;student affairs management,Educational institutions;Indexes;Lead,appraisal;decision making;education;fuzzy set theory,AHP cluster analysis;cluster analysis;college management;fuzzy comprehensive evaluation;group analytic hierarchy process;index weights;performance appraisal;student affairs management,,0,,6,,no,27-29 May 2011,,IEEE,IEEE Conference Publications
Performance evaluation of the Integrated Avionics Simulation system based on grey theory,T. Haibao; G. Wentian; W. Yong; Z. Jiandong,"School of Electrical Information, Northwestern Polytechnical University, Xi'an China",2011 IEEE 3rd International Conference on Communication Software and Networks,20110908,2011,,,56,59,Performance Evaluation of the Integrated Avionics simulation is more and more important in the current military. The grey theory based on Analytic Hierarchy Process (AHP) is an effective method for the Performance Evaluation of the Integrated Avionics Simulation System. In this paper AHP and grey evaluation model are expounded firstly and then it tells the combination of the two. The method which the article expounds is smart. It gives a better result for the Performance Evaluation of the Integrated Avionics Simulation System.,,Electronic:978-1-61284-486-2; POD:978-1-61284-485-5,10.1109/ICCSN.2011.6013544,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6013544,Analytic Hierarchy Process (AHP);Integrated Avionics Simulation system;Performance Evaluation;grey evaluation model,Acceleration;Aerospace electronics;Analytical models;Atmospheric modeling;Indexes;Inertial navigation;Performance evaluation,decision making;digital simulation;grey systems;military avionics;military computing;performance evaluation,AHP;analytic hierarchy process;grey evaluation model;grey theory;integrated avionics simulation system;military field;performance evaluation,,0,,5,,no,27-29 May 2011,,IEEE,IEEE Conference Publications
Person attribute search for large-area video surveillance,J. Thornton; J. Baran-Gale; D. Butler; M. Chan; H. Zwahlen,"MIT Lincoln Laboratory, 244 Wood St., Lexington, Ma. 02420",2011 IEEE International Conference on Technologies for Homeland Security (HST),20111219,2011,,,55,61,"This paper describes novel video analytics technology which allows an operator to search through large volumes of surveillance video data to find persons that match a particular attribute profile. Since the proposed technique is geared for surveillance of large areas, this profile consists of attributes that are observable at a distance (including clothing information, hair color, gender, etc) rather than identifying information at the face level. The purpose of this tool is to allow security staff or investigators to quickly locate a person-of-interest in real time (e.g., based on witness descriptions) or to speed up the process of video-based forensic investigations. The proposed algorithm consists of two main components: a technique for detecting individual moving persons in large and potentially crowded scenes, and an algorithm for scoring how well each detection matches a given attribute profile based on a generative probabilistic model. The system described in this paper has been implemented as a proof-of-concept interactive software tool and has been applied to different test video datasets, including collections in an airport terminal and collections in an outdoor environment for law enforcement monitoring. This paper discusses performance statistics measured on these datasets, as well as key algorithmic challenges and useful extensions of this work based on end-user feedback.",,Electronic:978-1-4577-1376-7; POD:978-1-4577-1375-0,10.1109/THS.2011.6107847,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6107847,,Cameras;Clothing;Computational modeling;Image color analysis;Shape;Streaming media;Surveillance,criminal law;forensic science;image recognition;video surveillance,clothing information;gender;generative probabilistic model;hair color;individual moving person;interactive software tool;large area video surveillance;law enforcement monitoring;performance statistics;person attribute search;person-of-interest;surveillance video data;video analytics technology;video based forensic investigation,,7,,15,,no,15-17 Nov. 2011,,IEEE,IEEE Conference Publications
Plenary talk: Big data and real time analytics,G. V. N. A. Rao,"CTS, Chennai",2011 International Conference on Recent Trends in Information Technology (ICRTIT),20110804,2011,,,4,4,"Moore's law predicted the processor and other IT technologies providing us faster computing storage and networking power for cheaper cost, year after year. This helped us to automate most of the industries and processes, leading to creation of terabytes data every day in organizations. Social computing is also fueling the growth of the data. If one can able to process this data in real time, it will provide market trends and user behavior, which can be used in various businesses. To manage and efficiently process this growing terabytes or petabytes of data, per day, the current data management technologies does not meet the requirements. New concepts and models are emerging to meet these challenges.",,Electronic:978-1-4577-0590-8; POD:978-1-4577-0588-5,10.1109/ICRTIT.2011.5972495,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5972495,,Buildings;Companies;Open source software;Real time systems;Social network services,business data processing;marketing data processing;real-time systems;social sciences computing,IT technologies;Moore law;computing storage;market trends;real time analytics;social computing,,2,,,,no,3-5 June 2011,,IEEE,IEEE Conference Publications
Polynomial modeling of acoustic guided wave propagation in homogeneous cylinder of infinite length,L. Elmaimouni; J. E. Lefebvre; F. E. Ratolojanahary; A. Raherison; T. Gryba,"ERSITA, Facult&#x00E9; polydisciplinaire d'Ouarzazate, Univ., Ibn Zohr, BP 638, 45000 Ouarzazate, Morocco",2011 International Conference on Multimedia Computing and Systems,20110711,2011,,,1,6,"In this paper, we present a polynomial approach for determining the acoustic guided waves in homogeneous infinitely long cylinders using elastic materials of cylindrical anisotropy. The formulation is based on linear three-dimensional elasticity using an analytic form for the displacement field. The approach incorporates the stress-free boundary conditions directly into the equations of motion which are solved numerically by expanding each displacement component using Legendre polynomials and trigonometric functions. The problem is then reduced for anisotropic homogeneous structures to a tractable eigenvalue problem allowing the dispersion curves and associated profiles to be easily calculated. Numerical results of the guided waves for axisymmetric and flexural modes are presented and compared with those published earlier in order to check up the accuracy and range of applicability of the approach. The developed software proves to be very efficient to retrieve the guided waves of any nature and the modes of all orders. The computational advantages of the approach are described.",Pending,Electronic:978-1-61284-732-0; POD:978-1-61284-730-6,10.1109/ICMCS.2011.5945602,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5945602,Anisotropic material;Field profiles;Hollow and solid cylinders;Legendre polynomial;Normalized frequencies,Dispersion;Polynomials;Solids;Steel;Stress,Legendre polynomials;acoustic wave propagation;acoustic waveguides;eigenvalues and eigenfunctions;elasticity,3D elasticity;Legendre polynomial;acoustic guided wave propagation;cylindrical anisotropy;displacement field;eigenvalue problem;homogeneous cylinder;polynomial modeling;stress-free boundary condition;trigonometric function,,0,,21,,no,7-9 April 2011,,IEEE,IEEE Conference Publications
Predicting in-memory database performance for automating cluster management tasks,J. Schaffner; B. Eckart; D. Jacobs; C. Schwarz; H. Plattner; A. Zeier,"Hasso Plattner Institute, University of Potsdam, August-Bebel-Str. 88, 14482, Germany",2011 IEEE 27th International Conference on Data Engineering,20110516,2011,,,1264,1275,"In Software-as-a-Service, multiple tenants are typically consolidated into the same database instance to reduce costs. For analytics-as-a-service, in-memory column databases are especially suitable because they offer very short response times. This paper studies the automation of operational tasks in multi-tenant in-memory column database clusters. As a prerequisite, we develop a model for predicting whether the assignment of a particular tenant to a server in the cluster will lead to violations of response time goals. This model is then extended to capture drops in capacity incurred by migrating tenants between servers. We present an algorithm for moving tenants around the cluster to ensure that response time goals are met. In so doing, the number of servers in the cluster may be dynamically increased or decreased. The model is also extended to manage multiple copies of a tenant's data for scalability and availability. We validated the model with an implementation of a multi-tenant clustering framework for SAP's in-memory column database TREX.",1063-6382;10636382,Electronic:978-1-4244-8960-2; POD:978-1-4244-8959-6; USB:978-1-4244-8958-9,10.1109/ICDE.2011.5767936,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5767936,,Benchmark testing;Databases;Equations;Mathematical model;Rocks;Servers;Time factors,cloud computing;database management systems;pattern clustering;storage management,TREX;analytics-as-a-service;automating cluster management tasks;in-memory column database performance prediction;multitenant in-memory column database clusters;software-as-a-service,,11,,34,,no,11-16 April 2011,,IEEE,IEEE Conference Publications
Prediction of mean arterial blood pressure with linear stochastic models,S. Genc,"Sensor Informatics and Technologies Laboratory, Software Sciences and Analytics, General Electric Global Research, Niskayuna, NY 12309 USA",2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society,20111201,2011,,,712,715,"A model-based approach that integrates known portion of the cardiovascular system and unknown portion through a parameter estimation to predict evolution of the mean arterial pressure is considered. The unknown portion corresponds to the neural portion that acts like a controller that takes corrective actions to regulate the arterial blood pressure at a constant level. The input to the neural part is the arterial pressure and output is the sympathetic nerve activity. In this model, heart rate is considered a proxy for sympathetic nerve activity. The neural portion is modeled as a linear discrete-time system with random coefficients. The performance of the model is tested on a case study of acute hypotensive episodes (AHEs) on PhysioNet data. TPRs and FPRs improve as more data becomes available during estimation period.",1094-687X;1094687X,Electronic:978-1-4577-1589-1; POD:978-1-4244-4121-1; USB:978-1-4244-4122-8,10.1109/IEMBS.2011.6090161,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6090161,,Blood pressure;Control systems;Heart rate;Mathematical model;Prediction algorithms;Predictive models;Unsolicited electronic mail,,,"Animals;Blood Pressure;Blood Pressure Determination;Computer Simulation;Heart;Humans;Linear Models;Models, Cardiovascular;Models, Statistical;Sympathetic Nervous System",0,,17,,no,Aug. 30 2011-Sept. 3 2011,,IEEE,IEEE Conference Publications
Prioritizing Architectural Concerns,L. Pareto; A. Sandberg; P. Eriksson; S. Ehnebom,"Dept. of Comput. Sci. & Eng., Chalmers Univ. of Gothenburg, Gothenburg, Sweden",2011 Ninth Working IEEE/IFIP Conference on Software Architecture,20110721,2011,,,22,31,"Efficient architecture work involves balancing the degree of architectural documentation with attention to needs, costs, agility and other factors. This paper presents a method for prioritizing architectural concerns in the presence of heterogeneous stakeholder groups in large organizations that need to evolve existing architecture. The method involves enquiry, analysis, and deliberation using collaborative and analytical techniques. Method outcomes are action principles directed to managers and improvement advice directed to architects along with evidence for recommendations made. The method results from 3 years of action research at Ericsson AB with the purpose of adding missing views to architectural documentation and removing superfluous ones. It is illustrated on a case where 29 senior engineers and managers within Ericsson prioritized 37 architectural concerns areas to arrive at 8 action principles, 5 prioritized improvement areas, and 24 improvement suggestions. Feedback from the organization is that the method has been effective in prioritizing architectural concerns, that data collection and analysis is more extensive compared to traditional prioritization practices, but that extensive analysis seems inevitable in architecture improvement work.",,Electronic:978-0-7695-4351-2; POD:978-1-61284-399-5,10.1109/WICSA.2011.13,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5959695,analytic-deliberative decision making;architectural concern;concern prioritization;software architecture design,Conferences;Documentation;Interviews;Organizations;Planning;Reflection;Taxonomy,groupware;software architecture;system documentation,analytical techniques;architectural concern prioritization;architectural documentation;collaborative techniques;heterogeneous stakeholder groups,,0,,28,,no,20-24 June 2011,,IEEE,IEEE Conference Publications
Query Performance Tuning in Supply Chain Analytics,X. Zeng; D. Lin; Q. Xu,"Sch. of Econ. & Manage., Changsha Univ. of Sci. & Technol., Changsha, China",2011 Fourth International Joint Conference on Computational Sciences and Optimization,20110718,2011,,,327,331,"Data explosion with knowledge shortage is becoming increasingly prominent. By utilizing business intelligence technology, supply chain analytics turns data into business insights and optimizes supply chain management decisions. Firstly, this paper describes the levels of Business Intelligence analytics, and formulates the architecture of supply chain analytics topics, then explains the analytics details of each topic. Furthermore, as OLAP is the most important decision support analysis tools of which query performance directly impacts the quality of analytics system end user experience, this paper proposes a variety of tuning technologies to accelerate query performance, including optimizing design of dimension, table aggregations, partitions, column store and tuning server resources technologies etc. A use scenario shows performance can be dramatically improved by dropping the processing time from previous 6-8 seconds to less than 0.1 seconds when aggregating 20+ million business transaction records.",,Electronic:978-0-7695-4335-2; POD:978-1-4244-9712-6,10.1109/CSO.2011.212,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5957672,Business Intelligence;Online Analytical Processing;Query Performance Tuning;Supply Chain Analytics,Databases;Marketing and sales;Procurement;Servers;Supply chains;Tuning,business data processing;competitive intelligence;knowledge management;query processing;supply chain management,business intelligence technology;business transaction records;data explosion;knowledge shortage;query performance tuning;supply chain analytics;supply chain management,,0,,7,,no,15-19 April 2011,,IEEE,IEEE Conference Publications
Radiation characteristics of the electric dipole excited by surge impulse current,H. Chen,"School of Mechanical Electronic & Information Engineering, China University of Mining & Technology, Beijing, China",2011 3rd International Conference on Advanced Computer Control,20110912,2011,,,582,585,"Surge is a kind of transient over-voltage or over-current with short duration and high energy in power system, which mainly conducts through wires, also produces electromagnetic radiation in space. It was obtained that the amplitude spectrum of surge impulse current principally concentrates within 1 MHz based on the spectrum analysis. The electromagnetic radiation model of the electric dipole excited by surge impulse current was deduced by using retarded vector potential and electromagnetic field theory, and the analytic expressions in time domain of the radiated electromagnetic fields were given. Moreover, results were acquired by software MATLAB. The results show that the amplitude of the radiated electromagnetic field of an electric dipole is proportional to the surge peak current; the radiated E-field and H-field attenuate with distance; and the radiated electromagnetic field amplitude changes with time in pulse shape at a field point.",,Electronic:978-1-4244-8810-0; POD:978-1-4244-8809-4,10.1109/ICACC.2011.6016481,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6016481,electric dipole;radiation characteristic;radiation model;surge current,Argon,electromagnetic field theory;electromagnetic waves;power system protection;surge protection;vectors,E-field;H-field;MATLAB software;electric dipole radiation characteristics;electromagnetic field amplitude;electromagnetic field theory;electromagnetic radiation model;frequency 1 MHz;power system;retarded vector potential;surge impulse current;transient overcurrent;transient overvoltage,,2,,8,,no,18-20 Jan. 2011,,IEEE,IEEE Conference Publications
Ranking of technology transfer barriers in developing countries; case study of Iran's biotechnology industry,K. Yazdani; K. Y. Rashvanlouei; K. Ismail,"Faculty of Management and Human Resource Development, University Technology Malaysia, Johor Bahru, Malaysia",2011 IEEE International Conference on Industrial Engineering and Engineering Management,20111229,2011,,,1602,1606,"In this paper, first we shall define a list of technology transfer barriers in the biotechnology field through interviews and literature reviews. Next, we shall categorize them into four major categories of technology, and at last introduce all of the hypotheses on the existence of relationship between each part of technology and failure in the technology transfer process. Questionnaires consisting of two parts were created and handed out to all of the specialists and biotechnology idea-holders in the Iranian National Research center of Biotechnology, Lidco Co, Iran Pastor Institute, Biotechnology Department of University of Tehran, etc. Going on, we've evaluated the factors' validity through statistical tests and the recognized barriers' priorities in each category, which were generally, identified by Analytical Hierarchy Process (AHP) via the software Expert Choice. These priorities describe the high importance of organization-ware (34.1%), information-ware (27%), technique-ware (20.7%) and eventually human-ware (18.1%) respectively.",2157-3611;21573611,Electronic:978-1-4577-0739-1; POD:978-1-4577-0740-7; USB:978-1-4577-0738-4,10.1109/IEEM.2011.6118187,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6118187,Analytic Hierarchy Process (AHP);Developing Countries;Management of Technology;Technology Transfer,Biotechnology;Educational institutions;Humans;Industries;Organizations;Reliability;Technology transfer,biotechnology;decision making;technology transfer,Expert Choice software;Iran;analytical hierarchy process;biotechnology industry;human-ware;information-ware;organization-ware;technique-ware;technology transfer barrier ranking;technology transfer process,,0,,10,,no,6-9 Dec. 2011,,IEEE,IEEE Conference Publications
Recovery from Failures Due to Mandelbugs in IT Systems,K. S. Trivedi; R. Mansharamani; D. S. Kim; M. Grottke; M. Nambiar,"Dept. of Electr. & Comput. Eng., Duke Univ., Durham, NC, USA",2011 IEEE 17th Pacific Rim International Symposium on Dependable Computing,20120119,2011,,,224,233,"Several studies have been carried out on software bugs analysis and classification for life and mission critical systems, which include reproducible bugs called Bohrbugs, and hard to reproduce bugs called Mandelbugs. Although software reliability in IT systems has been studied for years, there are only a few formal analytic models for recovery from Mandelbugs. This paper discusses in detail several real cases of Mandelbugs and presents a simple flowchart which describes the recovery processes implemented in IT systems for a large variety of Mandelbugs. The flowchart is based on more than 10 IT systems that are running in production. The paper then presents a closed-form expression of the mean time to recovery from these bugs. Measures of interest including mean time to recovery and system unavailability are computed. A numerical and parametric sensitivity analysis of the model parameters are carried out. This analysis allows the designer to find out important parameter(s) for the recovery from failures due to Mandelbugs.",,Electronic:978-0-7695-4590-5; POD:978-1-4577-2005-5,10.1109/PRDC.2011.34,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6133084,Mandelbugs;availability;mean time to recovery;sensitivity analysis,Analytical models;Computer bugs;Flowcharts;Manuals;Servers;Software;Testing,program debugging;sensitivity analysis;software reliability;system recovery,Bohrbugs;IT systems;Mandelbugs;failure recovery;formal analytic models;mission critical system;numerical sensitivity analysis;parametric sensitivity analysis;software bug analysis;software bug classification;software reliability,,13,,20,,no,12-14 Dec. 2011,,IEEE,IEEE Conference Publications
Research of evaluation information processing of inventory optimization based on grey clustering and fuzzy evaluation method,Lina Yang; Junle Yu,"Coll. of Comput. Sci. &amp;Software, Tianjin Polytech. Univ., Tianjin, China",Proceedings of 2011 6th International Forum on Strategic Technology,20110915,2011,2,,860,863,"Market competition is no longer among individual enterprises, but among the Supply Chains of enterprises. How to set up and maintain a reasonable inventory level to balance the risk and loss brought by the shortage of inventory shortage and the increased inventory cost and capital cost caused by too much inventory has become an imperative problem to logistics enterprises. In this paper, the AHP method is used to determine the various classification indexes weights of the inventory, and based on the gray clustering analysis, inventory optimization evaluation model and algorithm is presented with fuzzy evaluation method, which provides a broad prospect for logistics enterprise inventory optimization and evaluation technology.",,DVD:978-1-4577-0397-3; Electronic:978-1-4577-0399-7; POD:978-1-4577-0398-0,10.1109/IFOST.2011.6021156,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6021156,Fuzzy Evaluation;Grey Clustering;inventory optimization evaluation;logistics enterprise,Analytical models;Educational institutions;Indexes;Information processing;Logistics;Optimization;Sun,decision making;fuzzy set theory;grey systems;inventory management;logistics;supply chain management,AHP method;analytic hierarchy processing;classification index;evaluation information processing;fuzzy evaluation method;grey clustering;inventory optimization;inventory optimization evaluation model;logistics enterprise inventory;supply chain enterprise,,0,,6,,no,22-24 Aug. 2011,,IEEE,IEEE Conference Publications
Research on auto-filling hole based on mesh information of car body panel,Z. Enyuan; L. Ruijun,"Coll. of Traffic &amp;Civil Eng., Beihua Univ., Jilin, China","2011 International Conference on Mechatronic Science, Electric Engineering and Computer (MEC)",20110922,2011,,,644,647,"In terms of the feature of the Self-Piercing Riveting process program, the evaluating index system for the process is created. The weighting aggregate is created with the analytic hierarchy process.The correctness of the weighting aggregate is tested by test results, Selecting the right test program basede on the value of weighting aggregate and synthetical importance.",,Electronic:978-1-61284-722-1; POD:978-1-61284-719-1,10.1109/MEC.2011.6025548,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6025548,Auto;Coons surface;Filling hole;Panel;Surface,Algorithm design and analysis;Approximation algorithms;Design automation;Filling;Software;Software algorithms;Surface treatment,automobile manufacture;decision making;riveting;structural panels,analytic hierarchy process;auto-filling hole;car body panel;evaluating index system;mesh information;self-piercing riveting process program;weighting aggregate,,0,,14,,no,19-22 Aug. 2011,,IEEE,IEEE Conference Publications
Research on Chinese Services Outsourcing Vendors' Capabilities Based on Fuzzy Comprehensive Evaluation,Y. Pei; W. Xue; Q. Tao; Y. Su,"Sch. of Manage., Beijing Union Univ., Beijing, China",2011 International Conference on Management and Service Science,20110825,2011,,,1,5,"This paper establishes index system of Chinese services outsourcing vendors' capabilities, uses analytic hierarchy process to determine the weight of each level index, establishes the fuzzy comprehensive evaluation model of Chinese services outsourcing vendors' capabilities, finally the case study shows that it is reasonable and credible to evaluate Chinese services outsourcing vendors' capabilities with analytic hierarchy process and fuzzy comprehensive evaluation.",,Electronic:978-1-4244-6581-1; POD:978-1-4244-6579-8,10.1109/ICMSS.2011.5998719,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5998719,,Humans;Indexes;Industries;Outsourcing;Project management;Research and development;Software,fuzzy set theory;outsourcing,Chinese services outsourcing vendor capability;analytic hierarchy process;fuzzy comprehensive evaluation;level index system,,0,,15,,no,12-14 Aug. 2011,,IEEE,IEEE Conference Publications
Research on Evaluating and Selecting of Suppliers Based on AHP in Supply Chain Design,Y. Zhu; H. Xiao,"Sch. of Logistics Eng., Wuhan Univ. of Technol., Wuhan, China",2011 International Conference on Internet Technology and Applications,20110829,2011,,,1,5,"Supplier quality determines the quality of products offered, and how to quickly and effectively select high-quality suppliers is one of the core issues facing enterprise. From the perspective of the supply chain, this article proposes the theoretical model of the supplier evaluation and selection, establishes a index system of comprehensive evaluation, and applies Analytic Hierarchical Process (AHP) to built the decision model of supplier evaluation and selection. In the final analysis, the feasibility and scientific validity of this system are verified through the simulation analysis based on Simulink module in Matlab software, and the practical example.",,Electronic:978-1-4244-7255-0; POD:978-1-4244-7253-6,10.1109/ITAP.2011.6006423,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6006423,,Automotive engineering;Indexes;Investments;Mathematical model;Presses;Supply chains,decision making;supply chains,AHP;Matlab software;analytic hierarchical process;supplier evaluation;supplier quality;supplier selection;supply chain design,,0,,13,,no,16-18 Aug. 2011,,IEEE,IEEE Conference Publications
Research on fuzzy evaluation of teaching practice based on AHP: ‰ÛÓ An example of evaluating curriculum design performance in computer major,S. Wang; M. Song,"School of Information Science and Engineering, Henan University of Technology Zheng Zhou, P.R. China",2011 IEEE 3rd International Conference on Communication Software and Networks,20110908,2011,,,728,731,"Evaluating performance of practice course is a difficult problem in the process of teaching. On the basis of analyzing the existing fuzzy evaluation models, this paper establishes a fuzzy comprehensive evaluation model based on analytic hierarchy process (AHP). Three-level index evaluation system is set up combining the characteristics of curriculum design in computer major. Relative and comprehensive weights of indexes have been determined using AHP method. And membership degree function is designed in order to transform index score into a fuzzy vector, thus the fuzzy evaluation model can be established. This model reduces the subjectivity which giving a mark accurately brings and has been already applied in teaching.",,Electronic:978-1-61284-486-2; POD:978-1-61284-485-5,10.1109/ICCSN.2011.6014193,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014193,analytic hierarchy process;fuzzy comprehensive evaluation model;practice course,Education;Indexes,computer science education;decision making;educational courses;fuzzy set theory;performance evaluation;teaching,analytic hierarchy process;computer major;curriculum design performance evaluation;fuzzy comprehensive evaluation model;fuzzy vector;practice course;teaching practice,,0,,6,,no,27-29 May 2011,,IEEE,IEEE Conference Publications
Research on Goat Health Management System,L. Li; J. Dong; X. Song; L. Nie; S. Zhang; M. Liu,"Coll. of Animal Husbandry & Veterinary Med., Shenyang Agric. Univ., Shenyang, China",2011 3rd International Workshop on Intelligent Systems and Applications,20110613,2011,,,1,4,"A goat health management system is presented in this paper, which can manage goat disease from each stage including goat file creation, routine monitoring, disease prevention and control. Integrate electronic medical records was set up, which based on medical records include goat basic information and routine monitoring results and disease prevention information. It can implement statistical analytic function of disease rate and guide goat immunization. This system includes four subsystems, goat basic information management subsystem, goat individual health monitoring and evaluation subsystem, goat electronic medical records subsystem and goat disease prevention and control subsystem. With the help of system analysis and software design techniques, it can manage goat disease on goat farm effectually.",,Electronic:978-1-4244-9857-4; POD:978-1-4244-9855-0,10.1109/ISA.2011.5873323,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5873323,,Animals;Databases;Diseases;Drugs;Immune system;Information management;Monitoring,diseases;farming;health and safety;management information systems;software engineering,disease prevention;goat disease control subsystem;goat disease prevention;goat farm;goat file creation;goat health management system;goat immunization;information management subsystem;integrate electronic medical records;routine monitoring;software design techniques;statistical analytic function,,0,,6,,no,28-29 May 2011,,IEEE,IEEE Conference Publications
Research on performance management of service firm based on BSC and F-ANP,T. Huang,"Department of Transport, Wuhan University of Technology, Wuhan, Hubei Province, China",2011 IEEE 3rd International Conference on Communication Software and Networks,20110908,2011,,,502,506,"The feature of the service firm are highly opening, substantial interactions among customers and companies. Hence, there will be flaws if only evaluate the performance from classical financial indicators. As an instrument for performance evaluation, Balanced Scorecard (BSC) generally evaluate the performance from four dimensions, which are financial condition, customer service, internal business processes and learning and growth. Fuzzy Analytic Network Process (F-ANP) is a quantitative method which can deal with complicated decision-making problem. By introducing BSC together with F-ANP to the performance evaluation of service firm, this performance evaluation model will have functions of determining the weight of each index reasonably and improving the effectiveness of BSC in the application of the performance evaluation.",,Electronic:978-1-61284-486-2; POD:978-1-61284-485-5,10.1109/ICCSN.2011.6014619,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014619,Balanced Scorecard (BSC);Fuzzy Analytic Network Process (F-ANP);performance evaluation;performance management;service firm,Finance;Training,customer services;decision making;economic indicators;learning (artificial intelligence);performance evaluation;service industries,BSC;F-ANP;balanced scorecard;business process;customer service;decision making problem;financial indicator;fuzzy analytic network process;performance management;service firm,,0,,7,,no,27-29 May 2011,,IEEE,IEEE Conference Publications
Research on safety monitoring and evaluation system of dangerous goods transportation,Xinyuan Xiao; Huaijun Li; Yueqi Liu; Qihui Lv,"School of Automotive Engineering, Guang Dong Communication Polytechnic College, Guangzhou, China","Proceedings 2011 International Conference on Transportation, Mechanical, and Electrical Engineering (TMEE)",20120514,2011,,,1900,1904,"The micro controller with the LPC2294 and ARM7TDMI-S kernel is designed to realize the realtime monitoring of the vehicles, meanwhile, the system software of safety monitoring and evaluation of dangerous goods transportation is developed based on Microsoft Visual web developer, ASP.NET and Microsoft SQL Server. The three levels index system about evaluation model of dangerous goods transportation was established based on experts' survey and research. The analytic hierarchy process (AHP) was use to structure jugging matrix which can ascertain the weights of each level evaluation index. Dynamic and static indexes were quantitated respectively in different ways, and then to realize the technology integration between them. Finally, the Fuzzy Comprehensive Evaluation method is used to evaluate the dangerous goods transportation corporations and then obtain the security level of those corporations. Facts have shown that this monitoring and Evaluation system is applicable and effective, and it can be used in the safety monitoring and evaluation of existing dangerous goods transportation.",,Electronic:978-1-4577-1701-7; POD:978-1-4577-1700-0,10.1109/TMEE.2011.6199585,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6199585,Dangerous Goods Transportation;Fuzzy Comprehensive Evaluation;Index Integration;Safety Monitoring;System Software,Indexes;Monitoring;Safety;Temperature measurement;Temperature sensors;Vehicles,SQL;computerised monitoring;control engineering computing;decision making;expert systems;fuzzy set theory;goods distribution;hazardous materials;microcontrollers;organisational aspects;program diagnostics;road vehicles;safety systems;software performance evaluation,AHP;ARM7TDMI-S kernel;ASP.NET;LPC2294;Microsoft SQL Server;Microsoft Visual web developer;analytic hierarchy process;dangerous goods transportation corporations;dynamic index;evaluation model;evaluation system;expert research;expert survey;fuzzy comprehensive evaluation method;jugging matrix;level evaluation index;micro controller;safety monitoring;security level;static index;system software;technology integration;three levels index system;vehicle realtime monitoring,,2,,5,,no,16-18 Dec. 2011,,IEEE,IEEE Conference Publications
Research on selection of third-party logistics enterprise based on the extenics,F. Lei; Y. Long,"School of Transportation, Wuhan University of Technology, Wuhan, China",2011 IEEE 3rd International Conference on Communication Software and Networks,20110908,2011,,,242,246,"Facing the increasing development of third-party logistics, a cargo owner should set a systematic approach to choose the right logistics enterprises. In this paper, an evaluate index system of third-party logistics enterprise was built up, which originates from balanced scorecard's four dimensions. After introducing the concept of extension theory, this paper constructs an evaluation model which combined extension theory with fuzzy analytic hierarchy process. At last, a case study was given to validate this method.",,Electronic:978-1-61284-486-2; POD:978-1-61284-485-5,10.1109/ICCSN.2011.6014432,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014432,AHP;BSC;Extenics;TPL,Indexes,decision making;fuzzy set theory;logistics;organisational aspects,cargo owner;evaluate index system;extenics;extension theory;fuzzy analytic hierarchy process;scorecard four dimension;third-party logistics enterprise,,0,,6,,no,27-29 May 2011,,IEEE,IEEE Conference Publications
Research on the Performance Indicators of civil aviation E-government basic on IT governance,Xu Zhang; Jian Wei; Yujie Zhu,"College of aviation transportation, Shanghai University of Engineering Science, China","2011 2nd International Conference on Artificial Intelligence, Management Science and Electronic Commerce (AIMSEC)",20110905,2011,,,597,600,"With the deeper construction of civil aviation informatization, the development and construction of the new system of civil aviation E-government in modern information society and civil aviation, will meet the development needs of civil aviation and economic society in post-industrial era. This paper studies based on the status of civil aviation government and the design principle of performance indicator, use analytic hierarchy process, and puts forward a performance indicator include 35 index, such as degree of government affairs information digitization etc. Then we calculate the weight and the normalized processing by using yaahp5.0 software, and give a comprehensive evaluation plan at last. The evaluation result of civil aviation E-government has positive and effective simulative effect to improve civil aviation E-government affairs system efficiency.",,Electronic:978-1-4577-0536-6; POD:978-1-4577-0535-9,10.1109/AIMSEC.2011.6010478,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6010478,Analytic Network Process(ANP);IT governance;civil aviation E-government;performance indicator;workflow,Decision making;Electronic government;Indexes;Software;Transportation,aerospace computing;decision making;government data processing,IT governance;analytic hierarchy process;civil aviation e-government;civil aviation informatization;economic society;government affair information digitization;modern information society;performance indicator;yaahp5.0 software,,0,,6,,no,8-10 Aug. 2011,,IEEE,IEEE Conference Publications
Research on the roller feed track during spinning of hollow part with triangle cross-section,Zhouyi Lai; Qinxiang Xia; Zheng Huang; Xiuquan Cheng,"School of Mechanical and Automotive Engineering, South China University of Technology, Guangzhou, China",2011 Second International Conference on Mechanic Automation and Control Engineering,20110818,2011,,,649,652,"The roller feed track during spinning of hollow part with straight edge triangle cross-section was researched. The variation characteristic of the contact point between the roller and workpiece was obtained: the contact point moves not only along the axial direction caused by the roller axial feed, but also along the radial, circumferential and axial directions caused by the non-circular cross-section profile of workpiece. Based on the variation characteristic of the contact point, a graphical analytic method for the calculation of roller radial feed movement was put forward. The software MSC.ADAMS was adopted to verify the calculation results, the tolerance range is [-0.09mm, +0.09mm], it indicates that the calculation method is reasonable and can be used to calculate the roller feed track during spinning of hollow parts with various non-circular cross-section.",,DVD:978-1-4244-9438-5; Electronic:978-1-4244-9439-2; POD:978-1-4244-9436-1,10.1109/MACE.2011.5987008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5987008,calculation method;roller feed track;spinning;triangle cross-section,Clocks;Feeds;Simulation;Software;Spinning;Strontium;Tracking,mechanical contact;rollers (machinery);rolling,MSC ADAMS software;contact point;graphical analytic method;hollow part;noncircular cross-section profile;roller axial feed;roller feed track;roller radial feed movement;spinning;straight edge triangle cross-section;workpiece,,0,,7,,no,15-17 July 2011,,IEEE,IEEE Conference Publications
Resource allocation contracts for Open Analytic Runtime models,M. Y. Nam; D. de Niz; L. Wrage; L. Sha,"Department of Computer Science, University of Illinois at Urbana-Champaign, 201 North Goodwin Ave, 61801-2302, USA",2011 Proceedings of the Ninth ACM International Conference on Embedded Software (EMSOFT),20111031,2011,,,13,22,"Open Analytic Runtime (OAR) Models embed analysis algorithms into runtime architectural models, thus integrating the model and its analytic interpretations. Such an integration is critical for Cyber-Physical Systems (CPS) when model parts are independently developed by different teams as it is the case in multi-tier industries, e.g. avionics and automotive. Analysis algorithms play a central role augmenting the designer's capacity to automatically verify properties of interest in systems at the scale and complexity required by these industries. Unfortunately, the verification results are valid only if the assumptions of the different analysis algorithms (analytic assumptions) are consistent with each other. This paper presents our work on the automatic verification of one important class of analytic assumptions in OAR models: resource allocation assumptions. These assumptions are modeled as Resource Allocation (RA) contracts. RA contract constructs include not only the typical assumes and guarantees but also runtime facts and implications. Finally, we automatically determine the correct sequence of execution of the analysis algorithms based on the contract input/output dependencies described in our models. Together these characteristics enable the automatic assumption verification that preserves the scalability of analytic models. We illustrate our approach using an example model with analysis algorithms for security, schedulability, and energy efficiency.",,Electronic:978-1-4503-0714-7; POD:978-1-4503-0714-7,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064507,AADL;Assumption management;Cyber-Physical Systems;Design by Contract;Resource Allocation,Algorithm design and analysis;Analytical models;Contracts;Mathematical model;Resource management;Runtime;Unified modeling language,formal verification;resource allocation;scheduling;security of data;software architecture,CPS;OAR models;RA contracts;analysis algorithms;analytic assumptions;analytic interpretations;automatic assumption verification;automatic verification;cyber-physical systems;designers capacity;energy efficiency;multitier industry;open analytic runtime models;resource allocation assumptions;resource allocation contracts;runtime architectural models;runtime facts;schedulability;security,,0,,26,,no,9-14 Oct. 2011,,IEEE,IEEE Conference Publications
S-preconditioner for Multi-fold Data Reduction with Guaranteed User-Controlled Accuracy,Y. Jin; S. Lakshminarasimhan; N. Shah; Z. Gong; C. S. Chang; J. Chen; S. Ethier; H. Kolla; S. H. Ku; S. Klasky; R. Latham; R. Ross; K. Schuchardt; N. F. Samatova,"North Carolina State Univ., Raleigh, NC, USA",2011 IEEE 11th International Conference on Data Mining,20120123,2011,,,290,299,"The growing gap between the massive amounts of data generated by petascale scientific simulation codes and the capability of system hardware and software to effectively analyze this data necessitates data reduction. Yet, the increasing data complexity challenges most, if not all, of the existing data compression methods. In fact, lossless compression techniques offer no more than 10% reduction on scientific data that we have experience with, which is widely regarded as effectively incompressible. To bridge this gap, in this paper, we advocate a transformative strategy that enables fast, accurate, and multi-fold reduction of double-precision floating-point scientific data. The intuition behind our method is inspired by an effective use of preconditioners for linear algebra solvers optimized for a particular class of computational ""dwarfs"" (e.g., dense or sparse matrices). Focusing on a commonly used multi-resolution wavelet compression technique as the underlying ""solver"" for data reduction we propose the S-preconditioner, which transforms scientific data into a form with high global regularity to ensure a significant decrease in the number of wavelet coefficients stored for a segment of data. Combined with the subsequent EQ-calibrator, our resultant method (called S-Preconditioned EQ-Calibrated Wavelets (SPEQC-Wavelets)), robustly achieved a 4- to 5-fold data reduction-while guaranteeing user-defined accuracy of reconstructed data to be within 1% point-by-point relative error, lower than 0.01 Normalized RMSE, and higher than 0.99 Pearson Correlation. In this paper, we show the results we obtained by testing our method on six petascale simulation codes including fusion, combustion, climate, astrophysics, and subsurface groundwater in addition to 13 publicly available scientific datasets. We also demonstrate that application-driven data mining tasks performed on decompressed variables or their derived quantities produce results of comparable quality with the ones for- the original data.",1550-4786;15504786,Electronic:978-0-7695-4408-3; POD:978-1-4577-2075-8,10.1109/ICDM.2011.138,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6137233,data mining over decompressed data;data reduction;extreme-scale data analytics;in situ data analytics;preconditioners for data mining,Accuracy;Correlation;Data models;Indexes;Sparse matrices;Vectors;Wavelet transforms,data analysis;data mining;data reduction;mean square error methods;wavelet transforms,Pearson correlation;S-preconditioner;application driven data mining;data analysis;data complexity;data reconstruction;double-precision floating-point scientific data;linear algebra solvers;lossless data compression techniques;multifold data reduction;multiresolution wavelet compression technique;petascale scientific simulation codes;system hardware capability;system software capability;transformative strategy;user-controlled accuracy;wavelet coefficients,,0,,26,,no,11-14 Dec. 2011,,IEEE,IEEE Conference Publications
SaaS Vendor Selection Basing on Analytic Hierarchy Process,C. Yiming; Z. Yiwei,"Econ. & Manage. Sch., Changsha Univ. of Sci. & Technol., Changsha, China",2011 Fourth International Joint Conference on Computational Sciences and Optimization,20110718,2011,,,511,515,"Analytic hierarchy process is used to select the best SaaS vendor for enterprises. By constructing the hierarchy model, analyzing the attributes and calculating the attribute values, the decision problem with multi-objectives in selecting the best SaaS vendor for enterprises is effectively solved. An example is utilized to show the calculating process. Calculative result shows that the SaaS vendor with the highest score by the method is the best choice and the analytic hierarchy process is an effective approach for enterprises to choose the best SaaS vendor.",,Electronic:978-0-7695-4335-2; POD:978-1-4244-9712-6,10.1109/CSO.2011.232,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5957714,Analytic hierarchy process;Evaluation attributes;SaaS vendor,Analytical models;Business;Economics;Maintenance engineering;Security;Software;Stability analysis,cloud computing;decision making;enterprise resource planning,SaaS vendor selection;analytic hierarchy process;decision problem,,2,,7,,no,15-19 April 2011,,IEEE,IEEE Conference Publications
Safety evaluation study on groundwater for drinking in Yinchuan Plain,X. Jing; W. k. Wang; Z. Yang; F. c. Zhang; Y. Cao,"College of Geology and Environmental Engineering, Xi'an University of Science and Technology, China",2011 International Symposium on Water Resource and Environmental Protection,20110616,2011,1,,358,361,"Due to exposed to serious pollution and influence of natural conditions, drinking water safety has become a hotspot issue drawing high attention from the state and government. Yinchuang Plain is in serious quality-induced water shortage. Through investigation and analysis, this paper established a drinking water safety evaluation system from the perspective of headwaters and human health, studying quality and quantity of groundwater, as well as geological protective performance; safety evaluation on groundwater exploration for drinking in Yinchuan Plain was conducted by adopting analytic hierarchy process and overlay and index methods with the help of GIS software platform. The evaluation result has practical significance for groundwater resource protection and groundwater safety supply.",,Electronic:978-1-61284-340-7; POD:978-1-61284-339-1,10.1109/ISWREP.2011.5893018,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5893018,Drinking Water Safety;Index System Evaluation;Yinchuan Plain,Geology;Humans;Indexes;Safety;Solids;Water pollution;Water resources,geographic information systems;geophysics computing;groundwater;health hazards;hydrological techniques;water conservation;water quality;water resources,China;GIS software platform;Yinchuang Plain;analytic hierarchy process;drinking water safety evaluation system;geological protective performance;groundwater exploration;groundwater resource protection;groundwater safety supply;headwaters;human health;index system evaluation;quality-induced water shortage,,0,,10,,no,20-22 May 2011,,IEEE,IEEE Conference Publications
Safety-driven software reliability allocation in medical device application,Y. Wei; J. Qin,"St Paul, MN, USA","2011 International Conference on Quality, Reliability, Risk, Maintenance, and Safety Engineering",20110804,2011,,,105,109,"Software reliability allocation deals with setting reliability objectives and allocating testing resources for software components or sub-systems. The whole allocation process is complicated as many factors, such as cost, complexity, maintainability, and etc, are included for consideration and there are many uncertainty resources. In this paper, a simplified approach is presented. The approach starts from safety features which are the top concerns in medical device applications; allocates reliability to components associated with safety features via utilizing analytic hierarchy process and fault tree analysis; and finalizes the allocation by combining the reliability contribution from all safety features. The approach makes some reasonable approximations to simplify the allocation process and resolve the problem fast and more efficiently. The allocation result is conservative and practicable in medical device applications.",,Electronic:978-1-4577-1232-6; POD:978-1-4577-1229-6,10.1109/ICQR2MSE.2011.5976578,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5976578,medical device application;software reliability allocation,Approximation methods;Fault trees;Resource management;Safety;Software;Software reliability,decision making;fault trees;medical computing;safety-critical software;software reliability,analytic hierarchy process;fault tree analysis;medical device application;safety driven software reliability allocation;software components,,0,,15,,no,17-19 June 2011,,IEEE,IEEE Conference Publications
SCAP based configuration analytics for comprehensive compliance checking,M. N. Alsaleh; E. Al-Shaer,"Department of Software and Information Systems, University of North Carolina at Charlotte, USA",2011 4th Symposium on Configuration Analytics and Automation (SAFECONFIG),20111226,2011,,,1,8,"Computing systems today have large number of security configuration settings that are designed to offer flexible and robust services. However, incorrect configuration increases the potential of vulnerability and attacks. Security Content Automation Protocol provides a unified mean to automate the process of checking the desktop system compliance using standard interfaces. However, misconfiguration can be identified only if global checking that includes network and desktop configuration is performed, as many of these configurations are highly interdependent. In this work we present a SCAP-based tool that integrates host and network configuration compliance checking in one model and allows for executing comprehensive analysis queries in order to verify security and risk requirements across the end-to-end network as a single system. Our proposed tool translates XCCDF reports generated from SCAP tools into logical objects that can be further composed to create global logical analysis using more advanced security analytic tools such as ConfigChecker and PROLOG-based tools. This project also shows the value of building on the effort of standard tools to improve the state of the art.",,Electronic:978-1-4673-0402-3; POD:978-1-4673-0401-6,10.1109/SafeConfig.2011.6111674,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6111674,,Analytical models;Benchmark testing;Engines;Indexes;Measurement;Security;Software,computer network security;configuration management;conformance testing;cryptographic protocols;formal verification;query processing;software tools,ConflgChecker tools;PROLOG-based tools;SCAP tools;XCCDF reports;comprehensive analysis query;desktop configuration;desktop system compliance;global checking;global logical analysis;host configuration compliance checking;network configuration compliance checking;security analytic tools;security configuration;security content automation protocol;vulnerability,,2,2,15,,no,Oct. 31 2011-Nov. 1 2011,,IEEE,IEEE Conference Publications
Scattering Simulation and Reconstruction of a 3-D Complex Target Using Downward-Looking Step-Frequency Radar,J. Dai; Y. Q. Jin,"Key Lab. of Wave Scattering &amp; Remote Sensing Inf. (MoE), Fudan Univ., Shanghai, China",IEEE Transactions on Geoscience and Remote Sensing,20110926,2011,49,10,4035,4047,"Numerical simulations of polarized scattering, 3-D imaging, and profile reconstruction of a complex-shaped electric-large target above a rough surface are developed. The bidirectional analytic ray tracing method is first applied to calculation of polarized scattering from volumetric target and underlying surface. By using the step-frequency radar working in downward-looking spotlight mode and moving within a 2-D circular arc aperture, a 3-D matrix of backscattering fields in both the amplitude and phase is obtained. The 3-D fast Fourier transform algorithm is adopted for uniformly resampling data, which are acquired by interpolating the collected uniformly sampling backscattering fields to quickly form a focused image. Automatic reconstruction of the target is then well demonstrated. As validation and comparison, the scattering fields are also computed and compared using widely accepted software FEKO based on physical optics. The technique of imaging and reconstruction for a 3-D complex-shaped perfect electric conductor electric-large target, such as a tank-like object, is presented.",0196-2892;01962892,,10.1109/TGRS.2011.2131659,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5764516,Bidirectional analytic ray tracing (BART);fast Fourier transform (FFT);scattering simulation;step-frequency (SF) radar;target and rough surface;target reconstruction;three-dimensional imaging,Image reconstruction;Imaging;Radar imaging;Rough surfaces;Scattering;Surface roughness,backscatter;electromagnetic wave polarisation;electromagnetic wave scattering;fast Fourier transforms;image reconstruction;image sampling;interpolation;matrix algebra;radar detection;radar imaging;ray tracing;rough surfaces,2D circular arc aperture;3D imaging;3D matrix;FEKO;backscattering fields;bidirectional analytic ray tracing method;downward looking spotlight mode;fast Fourier transform;image reconstruction;image sampling;interpolation;numerical simulations;polarized scattering;rough surface;step frequency radar;target detection;target reconstruction,,7,,28,,no,Oct. 2011,,IEEE,IEEE Journals & Magazines
Security considerations in data center configuration management,K. Kant; Meixing Le; S. Jajodia,"George Mason University, Fairfax, VA, 22030, USA",2011 4th Symposium on Configuration Analytics and Automation (SAFECONFIG),20111226,2011,,,1,9,"Data centers need to manage a large amount of configuration information for a variety of computational, storage and networking assets at multiple levels (e.g., individual devices to entire data center). The increasingly sophisticated configuration management required to support virtualization significantly enhances chances of misconfigurations and exploitation by hackers that could impact data center operations. In this paper, we expose a number of attack/misconfiguration scenarios for data center resources. We also propose a mechanism, called Sentry, for securing this data by exploiting the hierarchical setup and redundancy in the server and network configurations. We show that the scheme can secure configuration data with only a small overhead.",,Electronic:978-1-4673-0402-3; POD:978-1-4673-0401-6,10.1109/SafeConfig.2011.6111676,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6111676,Common Information Model;Configuration Management;Data Center;Security,Authentication;Computer hacking;Computer integrated manufacturing;Servers;Software;XML,computer centres;configuration management;security of data,Sentry;configuration information;data center configuration management;hierarchical setup;network configurations;secure configuration data;security considerations;virtualization,,0,,23,,no,Oct. 31 2011-Nov. 1 2011,,IEEE,IEEE Conference Publications
Security infrastructure for commercial and military ports,L. L. Faulkner; B. P. Kritzstein; J. J. Zimmerman,"Battelle Memorial Institute, Columbus, Ohio, USA",OCEANS'11 MTS/IEEE KONA,20111219,2011,,,1,6,"Battelle has been developing robust detection and scanning technologies that integrate with existing and future port security systems as part of overall integrated security networks that can be tailored to meet individual requirements for commercial ports or Navy facilities. Major challenges include the water side issues such as geography, bathymetry, channel dimensions, changing bottom topology, environmental conditions, operation under all conditions and sea states suitable for vessel and small craft operation, harbor arrangement, and port-specific unique threats. However, many port security systems develop and implement new sensor technologies rather than assess the overall security need-that is, they follow a technology push rather than a market pull approach. While stated security approaches may integrate sensor data, stored data, intelligence databases, and human observations in various forms and formats, they generally do not provide the port-specific vulnerability and threat information required to alert security personnel with an integrated, composite threat analysis. Most approaches depend upon real-time person-in-the-loop detection strategies that can falter when faced with real world constraints: limited personnel resources, human fatigue and error, distractions, and misinterpretation of anomalies. Statistical and probabilistic analyses for anomaly identification are necessary to provide viable potential threat solution sets. Starlight‰ã¢, a Battelle developed software tool, combines information from multiple sources and multiple formats into a single information management and analysis composite to identify anomalies and threats. The system rapidly integrates text, network, geospatial, and temporal data, and then processes the disparate information to provide one cohesive threat solution to alert security staff. This synergistic approach enables multiple data base and information formats to be addressed holistically instead of through stovepipes,- giving the ability to uncover a threat pattern within the otherwise overwhelming amount of information. Starlight‰ã¢ differs from traditional systems by incorporating a range of processing algorithms to analyze individual information streams and form a composite threat solution. Current applications include intelligence gathering, network intrusion, entity extraction, law enforcement, visual intelligence (VISINT) processing, and threat anticipation. The overall strategy is the identification of appropriate sensors for integration into multiple real time data sets that combine sensor data with stored data, intelligence information, and human observations in various forms and formats. Individual data bases and sensor streams are processed with algorithms appropriate for their content and format. Data fusion, processing, statistical and probabilistic criterion provide synergistic solution sets that are ranked to identify threats and to select appropriate responses ranging from nonlethal deterrents to disabling force. Implementation strategies assimilate data and information necessary to process and identify risks compatible with and supportive of security personnel functions. Critical implementation requirements for adding or expanding port security systems include minimal impact to vessel traffic, minimal real-time human monitoring; minimizing false positives and false negatives; calibration; and flexibility to be adapted, expanded, and integrated for port-specific needs. In-water installation issues include secure anchoring and safety, bio-fouling protection, corrosion protection, reliability, availability, ease of maintenance, and life cycle costs. Implementation is accommodated by a spiral building block approach.",0197-7385;01977385,Electronic:978-0-933957-39-8; POD:978-1-4577-1427-6,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6107174,Data Synthesis;Database Composites;Port Security;Visual Analytics,Data visualization;Databases;Humans;Information systems;Personnel;Security;Visualization,military computing;security of data;sensor fusion;statistical analysis,Battelle;Starlight software tool;commercial port;data fusion;detection technology;geospatial data;market pull approach;military port;navy facilities;network data;person-in-the-loop detection strategy;port security system;probabilistic analysis;scanning technology;security infrastructure;security network;statistical analysis;technology push approach;temporal data;text data;threat analysis,,0,,10,,no,19-22 Sept. 2011,,IEEE,IEEE Conference Publications
SEGrapher: Visualization-based SELinux policy analysis,S. Marouf; M. Shehab,"Department of Software & Information Systems, University of North Carolina at Charlotte, USA",2011 4th Symposium on Configuration Analytics and Automation (SAFECONFIG),20111226,2011,,,1,8,"Performing SELinux policy analyses can be difficult due to the complexity of the policy language and the sheer number of policy rules and attributes involved. For example, the default policy on most SELinux-enabled systems has over 1; 500; 000 flat rules, involving over 1; 780 types. Simple analyses between types can result in a large amount of data, which is poorly presented to administrators in existing analysis tools. We propose and implement a policy analysis tool ‰ÛÏSEGrapher‰Ûù that addresses the above challenges. SEGrapher visually presents analysis results as a simplified directed graph, where nodes are types, and edges are corresponding policy rules between types. Graphs are generated via a proposed clustering algorithm that clusters types based on their accesses. Clusters provide an abstraction layer that removes undesired data, and focuses on analysis attributes specified by the administrator.",,Electronic:978-1-4673-0402-3; POD:978-1-4673-0401-6,10.1109/SafeConfig.2011.6111675,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6111675,,Access control;Algorithm design and analysis;Clustering algorithms;Linux;Optimization;Servers,Linux;data visualisation;directed graphs;pattern clustering;security of data,SEGrapher;abstraction layer;clustering algorithm;directed graph;policy attributes;policy language;policy rules;security enhanced Linux;visualization-based SELinux policy analysis,,0,,23,,no,Oct. 31 2011-Nov. 1 2011,,IEEE,IEEE Conference Publications
Simple and Accurate Approximations for the Two Dimensional Gaussian Q-Function,P. C. Sofotasios; S. Freear,"Sch. of Electron. & Electr. Eng., Univ. of Leeds, Leeds, UK",2011 IEEE 73rd Vehicular Technology Conference (VTC Spring),20110718,2011,,,1,4,"The aim of this work is the derivation of two approximated expressions for the two dimensional Gaussian Q-function, Q(x, y, ìÅ). These expressions are highly accurate and are expressed in closed-form. Furthermore, their algebraic representation is relatively simple and therefore, convenient to handle both analytically and numerically. This feature is particularly useful for two reasons: firstly because it renders the derived expressions useful mathematical tools that can be utilized in numerous analytic performance evaluation studies in digital communications under fading; secondly because the two dimensional Gaussian Q-function is neither tabulated nor a built-in function in popular mathematical software packages such as Maple, Mathematica and Matlab.",1550-2252;15502252,Electronic:978-1-4244-8331-0; POD:978-1-4244-8332-7,10.1109/VETECS.2011.5956733,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5956733,,Accuracy;Approximation methods;Closed-form solution;Digital communication;Fading;Performance evaluation;Software packages,Gaussian processes;digital communication,Maple;Mathematica;Matlab;algebraic representation;analytic performance evaluation;digital communications;mathematical software packages;mathematical tools;two dimensional Gaussian Q-function,,7,,14,,no,15-18 May 2011,,IEEE,IEEE Conference Publications
Simulation of motion artifacts in offset flat-panel cone-beam CT,E. Hansis; L. Shao,"Philips Healthcare, Nuclear Medicine, 3860 N 1st St, San Jose, CA 95134, USA",2011 IEEE Nuclear Science Symposium Conference Record,20120220,2011,,,2471,2475,"Cone-beam computed tomography (CBCT) with a tangentially offset flat-panel detector is used for target imaging in image-guided radiotherapy and for localization and attenuation correction in SPECT/CT imaging. The offset-detector geometry offers a large field-of-view for a given detector size. Data are acquired on a circular trajectory over 360 degrees, with gantry rotation times ranging from 12 s to 60 s and longer on commercially available systems. These acquisition times, much longer than in conventional CT, make flat-panel CBCT more susceptible to motion artifacts from voluntary or involuntary patient motion. Here, we present phantom studies on motion artifacts in flat-panel CBCT, with the goal to facilitate better artifact assessment on clinical cases and to provide a basis for future investigations into artifact mitigation methods. Software phantoms for five typically observed motion types were simulated; phantom reconstructions are presented and compared to clinical cases. In addition, three analytic and iterative reconstruction methods were evaluated with respect to their performance on motion-affected data.",1082-3654;10823654,Electronic:978-1-4673-0120-6; POD:978-1-4673-0118-3,10.1109/NSSMIC.2011.6152670,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6152670,Cone-beam computed tomography;SPECT/CT;iterative reconstruction;motion artifacts,Computed tomography;Image reconstruction,biomechanics;computerised tomography;image reconstruction;iterative methods;medical image processing;motion compensation;phantoms;radiation therapy,SPECT/CT imaging;analytic reconstruction method;artifact mitigation method;attenuation correction;cone beam computed tomography;image guided radiotherapy;involuntary patient motion;iterative reconstruction method;motion artifact simulation;offset flat-panel cone beam CT;phantom reconstruction;software phantom,,0,,11,,no,23-29 Oct. 2011,,IEEE,IEEE Conference Publications
Simulation standard for business process management,J. Januszczak; G. Hook,"Meta Software Corporation, 15 New England Executive Park, Burlington, MA 01803, USA",Proceedings of the 2011 Winter Simulation Conference (WSC),20120209,2011,,,741,751,"Simulation is considered a key component for business process management suites. Within business process management, simulation can be readily used for both process design, and ongoing improvement. Despite the predictive capabilities of simulation, the lack of wide scale adoption within business process management compared with what might be expected suggests that more can be done to better integrate, and use, simulation with business process management suites. While mature standards exist for the definitions of the business processes, there is a lack of standards for defining business process simulation parameters. This paper describes the current challenges when using simulation for business process management, and shows how a standard for defining business process simulation scenarios would help organizations implementing business process management suites leverage the prescriptive power of simulation. The components of such a standard and how this standard might be extended into a complete process analytics framework will be presented.",0891-7736;08917736,Electronic:978-1-4577-2109-0; POD:978-1-4577-2108-3; USB:978-1-4577-2107-6,10.1109/WSC.2011.6147801,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6147801,,Analytical models;Automation;Business;Computational modeling;Data models;Mathematical model;Predictive models,business data processing,business process management;predictive capability;process design;simulation standard,,2,,11,,no,11-14 Dec. 2011,,IEEE,IEEE Conference Publications
Smarter bridges through advanced structural health monitoring,P. P. Giangarra; B. Metrovich; M. M. Schwitters; B. P. Semple,"IBM Software Group, Austin, TX, USA",IBM Journal of Research and Development,20110120,2011,55,1.2,9:01,9:10,"This paper describes an IBM and the University of Miami joint research project providing infrastructure and application components to advance structural health monitoring of civil infrastructures such as bridges. We discuss a newly developed software infrastructure that enables a shift from batch to continuous bridge monitoring, providing continuous real-time sensor data collection and forwarding to a monitoring location where the data is cleansed (e.g., corrected), normalized, and recorded. We also discuss how significant load events are detected and fracture mechanics analytics are applied to assess bridge structural health. Finally, we discuss visualization of the raw and processed data. The University of Miami civil engineers developed the continual analytics techniques based on fracture mechanics and acoustic-emission analytics that together with the software infrastructure make it possible to perform more accurate and real-time monitoring of bridge deterioration. For civil structural health monitoring, this introduces a shift from time- to event-based analytics by exploiting a rolling, potentially complex, significant event-based threshold that triggers the analytics. The system was deployed on a single laboratory test specimen to assess the validity of the messaging and analytical tools in a simulated bridge environment.",0018-8646;00188646,,10.1147/JRD.2010.2086250,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5697271,,Bridges;Data visualization;Maintenance engineering;Monitoring;Real time systems,,,,0,,,,no,Jan.-March 2011,,IBM,IBM Journals & Magazines
Study on ARM-based fire alarm networking unit,Wang Huaixiu; Wang Dong; Jiang Zhijian,"School of Electronic & Information Engineering, Beijing University of Civil Engineering And Architecture, 100044, China",IEEE 2011 10th International Conference on Electronic Measurement & Instruments,20111010,2011,4,,354,356,"An ARM-based fire alarm unit was presented in the paper. The unit hardware was composed of temperature, smog sensors, LPC2478 and GPRS transmission. The unit software was composed of ë_C/OS - II embedded operation system, data acquisition, the fire identification algorithms and transmission programs. The unit was able to monitor and analyze the room's temperature and smog, and then the analytic result was transmitted to the server by the GPRS wireless module. The comprehensive fuzzy similar algorithm was used to identify the fire and the reliability of alarm was improved effectively.",,Electronic:978-1-4244-8161-3; POD:978-1-4244-8158-3,10.1109/ICEMI.2011.6038014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6038014,GPRS transmission;RAM microprocessor;fire identification,Computational modeling;Detectors;Educational institutions;Fires;Probability,packet radio networks;smoke detectors;temperature sensors,ARM-based fire alarm networking unit;GPRS transmission;GPRS wireless module;LPC2478;smog sensors;temperature,,0,,6,,no,16-19 Aug. 2011,,IEEE,IEEE Conference Publications
Study on performance of non-isolated double time slots algorithm of conflict resolution,Xiao Wang; Dong Feng Zhao,"Software College of YunNan University, KunMing, China",Proceedings of 2011 Cross Strait Quad-Regional Radio Science and Wireless Technology Conference,20111010,2011,1,,784,786,"It is analyzed and studied non-isolated double time slots algorithm of conflict resolution. The average resolution times analytic expression, resolution efficiency analytic expression and the average resolution delay analytic expression of conflict packet are exlicitly obtained. Furthermore, some exiting problems of doubel time slots algorithm are discussed and some measures of improve performance of algorithm are given.",,Electronic:978-1-4244-9793-5; POD:978-1-4244-9792-8,10.1109/CSQRWC.2011.6037069,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6037069,average resolution delay;average resolution times;double time slots;no-isolated algorithm;resolution efficiency,Algorithm design and analysis;Educational institutions,access protocols;multi-access systems,average resolution delay analytic expression;average resolution times analytic expression;conflict packet;conflict resolution;multiple access control protocol;nonisolated double time slots algorithm;resolution efficiency analytic expression,,0,,8,,no,26-30 July 2011,,IEEE,IEEE Conference Publications
Study on pid parameters tuning method based on Matlab/Simulink,S. Li; Q. Jiang,"Chaohu University, Chaohu 238000, China",2011 IEEE 3rd International Conference on Communication Software and Networks,20110908,2011,,,408,411,"Enormous calculation of proportional-integral-derivative (PID) controller parameters tuning with analytic methods is an important problem demanding prompt solution. Parameters tuning based on Matlab/Simulink is simplicity, visual manipulation method which leaves out above program. According to the Ziegler-Nichols (Z-N) method, this paper introduces how to reduce and validate the PID controller parameter with the help of MATLAB tool taking a certain control model as an example. The simulation results show the effectiveness of this method and can be fit for application in the engineering.",,Electronic:978-1-61284-486-2; POD:978-1-61284-485-5,10.1109/ICCSN.2011.6014596,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014596,Matlab/Simulink;PID control;Parameters tuning;Z-N method,Educational institutions;MATLAB;Stability analysis;Time factors;Tuning,control engineering computing;control system synthesis;parameter estimation;software packages;three-term control,Matlab-Simulink;PID parameter tuning method;Z-N method;Ziegler-Nichols method;proportional-integral-derivative controller parameter tuning;visual manipulation method,,0,,12,,no,27-29 May 2011,,IEEE,IEEE Conference Publications
Study on the wind energy resources assessment in wind power generation,T. Dai; B. Song; S. W. Shu,"School of Electrical Engineering, Wuhan University, China","2011 2nd International Conference on Artificial Intelligence, Management Science and Electronic Commerce (AIMSEC)",20110905,2011,,,6804,6807,"The wind energy resources assessment refers to a way to confirm the possibility of establishing a wind electric field based on the observation data on wind speed and direction through wind test station, as well as the data analysis and process for related data got from the local meteorological office long-range observation. At present, there has no universal wind energy analytic software for wind electric field in the domestic market, either few correlated software exist in the global market. And the RisoWA<sup>S</sup>P (Wind Atlas Analysis and Application Program) developed by Denmark Laboratory is a common use for wind energy resources assessment. This article analyzed both the advantages and disadvantages of the RisoWA<sup>S</sup>P software and some powerful software in china, and together with the comprehensive reference to the advantages of other assessment software, it concluded with a relative completed assessment for wind energy resources. In addition, with a view to such characteristics as a large data process and graphic making work in the wind energy resources assessment, this article put forward a concept to develop a new wind energy resources assessment software based on both the VB and ORACLE, which results from the wind energy resources analysis according to such integrated advantages as friendly interface of VB development software, strong diagram service and data.",,Electronic:978-1-4577-0536-6; POD:978-1-4577-0535-9,10.1109/AIMSEC.2011.6009905,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6009905,parameter calculation;wind energy resources assessment;wind-driven power generation,Electric fields;Electricity;Software;Wind energy;Wind power generation;Wind speed,data analysis;electric fields;globalisation;power engineering computing;power markets;wind power;wind power plants,ORACLE;VB development software;data analysis;domestic market;global market;wind electric field;wind energy resources assessment;wind power generation;wind test station,,1,,6,,no,8-10 Aug. 2011,,IEEE,IEEE Conference Publications
Synthesis Algorithm of Homologous Detection Based on Analytic Hierarchy Process,B. Cui; C. Bian; T. Guo; Y. Hao; J. Wang,"Sch. of Comput. Sci. & Technol., Univ. of Posts & Telecommun., Beijing, China",2011 Seventh International Conference on Computational Intelligence and Security,20120112,2011,,,826,830,"Nowadays, the research of software homologous detection is especially important in the flourishing software market while the existing detection techniques based on text, token, abstract syntax tree always get the real similarity inaccurately. In this paper, a synthesis algorithm based on Analytic Hierarchy Process(AHP) is proposed, which analyzes text, token, syntax tree three comparison algorithms synthetically and reflects software homologous similarity more accurately by artificial simulation plagiarism and experts calculation in the homologous field. This algorithm compares superiorities of homologous detection algorithms based on text, token and abstract syntax tree in performances factors such as miss rate, error rate, location accuracy, It also combines three comparison algorithms linearly according to their contributions to performances factors of synthesis similarity. The result of this synthesis algorithm meets the demand of homologous detection, reflects the reasonable real similarity which is calculated by experts.",,Electronic:978-0-7695-4584-4; POD:978-1-4577-2008-6,10.1109/CIS.2011.187,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6128148,Analytic Hierarchy Process;software homology;synthesis algorithm,Accuracy;Algorithm design and analysis;Detection algorithms;Error analysis;Plagiarism;Software algorithms;Syntactics,DP industry;computational linguistics;decision making;object-oriented methods;software engineering,abstract syntax tree;analytic hierarchy process;artificial simulation plagiarism;software design;software development;software homologous detection;software homologous similarity;software market;synthesis algorithm;synthesis similarity,,0,,11,,no,3-4 Dec. 2011,,IEEE,IEEE Conference Publications
System construction of comprehensive occupational abilities of engineering students in junior college,D. Chunhua,"Graduate School, Wuhan University of Technology, Wuhan, China",2011 IEEE 3rd International Conference on Communication Software and Networks,20110908,2011,,,89,91,"Education of junior college is employment-oriented and good occupational ability training is very important for students' future work and career development after graduation. Using group analytic hierarchy process (Group-AHP), a model for evaluating the comprehensive occupational abilities of engineering students in junior college has been built in this study and this model is composed of objective level, criteria level and index level. Criteria level includes 3 factors of basic capacity, major skill and key competency. Index level includes 14 indices: work ethics, judgment and discrimination, teamwork, humanity, language performance, computer operation, occupational position knowledge, technical proficiency, practical operating ability, application ability of new materials, processes and techniques as well as new facilities, interpersonal communication, psychological endurance, organizational managing ability, development and innovation, etc. The results have shown that, among the comprehensive occupational abilities of students, interpersonal communication, practical operation ability, teamwork, etc. have the largest weights, which is consistent with the teaching objective of junior college towards engineering students. Based on this model, grades of comprehensive occupational abilities of students could be calculated scientifically to provide the basis for improving the teaching quality, model and methods of college.",,Electronic:978-1-61284-486-2; POD:978-1-61284-485-5,10.1109/ICCSN.2011.6013783,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6013783,AHP;Group Decision;engineering major;junior college;occupational abilities,Adaptation models;Educational institutions;Engineering profession;Teamwork,decision making;engineering education,comprehensive occupational ability system construction;computer operation;criteria level;engineering students;group analytic hierarchy process;group- AHP;index level;interpersonal communication;junior college education;language performance;occupational position knowledge;organizational managing ability;practical operating ability;psychological endurance;teaching quality;technical proficiency;work ethics,,0,,7,,no,27-29 May 2011,,IEEE,IEEE Conference Publications
Systemic assessment of risks for projects: A systems and Cybernetics approach,T. Vinnakota,"Tata Consultancy Services, Bus. Syst. &amp; Cybern. Centre (BS&amp;CC), Hyderabad, India",2011 IEEE International Conference on Quality and Reliability,20110929,2011,,,376,380,"The current and past success rate of software projects is poor. This is mainly due to the reductionist/ analytic methods used in project risk assessments. The risk assessment practices adhered today based on standards or otherwise are non-systemic. These assessments cannot provide or deal with the systemic view of Project risks. They handle the project context and complex issues in Projects inadequately. The issue with reductionist thinking is that it leads to reductionist approach to problem solving and history has shown us Project failures continue to happen. This paper explains the Systemic Assessment of Risks (SAR) methodology that is proposed for assessment of project risks by considering Project as a system. This methodology uses the Cybernetics Risk Influence Diagramming (CRID) technique for identification of probable interconnected, interrelated and emergent risks. SAR's application in a software development project in a telecommunications enterprise demonstrates the methodology with the project risks assessed systemically.",,Electronic:978-1-4577-0628-8; POD:978-1-4577-0626-4,10.1109/ICQR.2011.6031745,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6031745,CRID;Cybernetics;SAR;Systemic;Systemic Risk Thinking;emergent,Context;Cybernetics;Feedback loop;Organizations;Risk management;Safety;Software,cybernetics;project management;risk analysis;service industries;software engineering;telecommunication industry,SAR methodology;cybernetics approach;cybernetics risk influence diagramming technique;project context;project risk assessments;reductionist thinking;reductionist-analytic methods;software development project;systemic assessment-of-risk methodology;telecommunication enterprise,,1,,14,,no,14-17 Sept. 2011,,IEEE,IEEE Conference Publications
Tangshan construction modern industry system's foundation ability appraisal and countermeasure,Hongrui Zhu; Zhijun Zheng,"Department of Discipline Construction, HeBei United University, 46 XinHua Road, TangShan City, China","2011 2nd International Conference on Artificial Intelligence, Management Science and Electronic Commerce (AIMSEC)",20110905,2011,,,4796,4799,"Construction modern industry system's foundation ability has reflected a local development modern industry system's comprehensive power and the development potential. Sharpens this local construction modern industry system's foundation ability, develops the modern industry system's powerful guarantee. This article utilized the factor analytic method comparative external analysis Tangshan construction modern industry system's foundation ability, and put forward the promotion construction modern industry system ability countermeasure proposal.",,Electronic:978-1-4577-0536-6; POD:978-1-4577-0535-9,10.1109/AIMSEC.2011.6010186,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6010186,Factorial analysis;Foundation ability;Modern industry system,Economic indicators;Industries;Investments;Production;Software;Technological innovation,construction industry,Tangshan construction modern industry;ability appraisal;system foundation,,0,,3,,no,8-10 Aug. 2011,,IEEE,IEEE Conference Publications
Target Value Sequencing Based on Weighted Evidence Theory,L. x. Wang; L. q. Rong; X. m. Shi; M. Zhao,"Dept. of Manage., Shijiazhuang Mech. Eng. Coll., Shijiazhuang, China",2011 Third International Conference on Intelligent Human-Machine Systems and Cybernetics,20111010,2011,2,,31,35,"We proposed a new approach for target value sequencing problem using weighted evidence theory. Target value evaluation index system is constructed located at artillery battlefield, the uncertain target information are fused based on the evidence theory, the index weight is decided based on analytic network process. The case proves the reliability of the results and the validity of this method. This method provides the artillery commander with a reference to the theoretic basis of decision-making of the target value sequencing.",,Electronic:978-0-7695-4444-1; POD:978-1-4577-0676-9,10.1109/IHMSC.2011.78,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6038208,analytic network process (ANP);evidence theory;index system;target value sequencing,Bismuth;Companies;Decision making;Indexes;Probability;Reliability;Software,decision making;military systems;reliability;weapons,analytic network process;artillery battlefield;artillery commander;decision making;reliability;target value evaluation index system;target value sequencing problem;weighted evidence theory,,0,,5,,no,26-27 Aug. 2011,,IEEE,IEEE Conference Publications
Technology transition of network defense visual analytics: Lessons learned from case studies,B. F. O'Brien; A. D'Amico; M. E. Larkin,"Secure Decisions Division, Applied Visions, Inc. Northport, NY 11768 USA",2011 IEEE International Conference on Technologies for Homeland Security (HST),20111219,2011,,,481,486,"Despite more than a decade of significant government investment in network defense research and technology development, there have been relatively few successful transitions across the chasm between research and operational use. Prior work describes approaches to crossing the ‰ÛÏvalley of death‰Ûù from the perspective of the government sponsor or independent tester. The researcher and developer's perspective offered in this paper adds to our understanding of the challenges faced and solutions applied to deployment of advanced technologies into operational environments. The paper describes lessons learned from recent transitions of two information assurance technologies - the VIAssistå¨ netflow visualization tool and the MeerCATå¨ wireless vulnerability analysis tool - into operational use by the Department of Homeland Security (DHS) and the Department of Defense (DoD).",,Electronic:978-1-4577-1376-7; POD:978-1-4577-1375-0,10.1109/THS.2011.6107916,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6107916,computer security;homeland security;network defense;technology;technology readiness level;technology transition;transfer,Government;Production;Prototypes;Security;Software;Testing;Training,computer network security;data visualisation;military computing;national security,MeerCAT wireless vulnerability analysis tool;VIAssist netflow visualization tool;department of defense;department of homeland security;government investment;information assurance technologies;network defense research;network defense visual analytics technology transition;valley of death,,2,,9,,no,15-17 Nov. 2011,,IEEE,IEEE Conference Publications
The application of FANP in selecting an optimal construction project,Z. Xiao-guang; C. Jian-shuang,"School of Economics and Management, University of Science and Technology Beijing, Beijing, China",2011 6th IEEE Conference on Industrial Electronics and Applications,20110804,2011,,,1845,1850,"An index system for selecting an optimal construction project is established. Due to the uncertainty and the inaccuracy of information during the evaluation process, an evaluation selection model based on fuzzy analytic network process method is proposed. The weight of each index, including the weight of the criterion level indicators, the weight of independent sub-indices and the weight of dependent sub-indices are determined by fuzzy preference programming method with the help of Matlab software. Meanwhile, the unweighted supermatrix is built for those interactional indicators, and the limit supermatrix is calculated after randomizing the unweighted supermatrix. Accordingly, a comprehensive weight of each index and the final score of each alternative can be calculated. Finally, an example is given by proposed method, and the result is shown that it can deal well with this kind of problems.",2156-2318;21562318,Electronic:978-1-4244-8756-1; POD:978-1-4244-8754-7,10.1109/ICIEA.2011.5975892,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5975892,fuzzy analytic network process(FANP);fuzzy preference programming(FPP);project selection;triangular fuzzy numbers,Conferences;Equations;Indexes;Industrial electronics;Mathematical model;Profitability;Programming,construction;fuzzy set theory;matrix algebra,Matlab software;evaluation selection model;fuzzy analytic network process;fuzzy preference programming;index system;limit supermatrix;optimal construction project;unweighted supermatrix,,1,,14,,no,21-23 June 2011,,IEEE,IEEE Conference Publications
The application of fuzzy analytic network process in risk evaluation of dynamic alliance,X. g. Zhou; Y. t. Song,"School of Economics and Management, University of Science and Technology Beijing, China",2011 International Conference on Business Management and Electronic Information,20110623,2011,5,,157,162,"Due to its adaptation to environmental change and market competition, the organizational form of dynamic alliance has been developing rapidly in recent years, but it also brings significant risk to enterprises. Taking into account the interaction and feedback relationships between criteria/indices, an index system for evaluating the risk of dynamic alliance is presented. Considering the vagueness and inaccuracy of information during the evaluation process, a risk evaluation model for dynamic alliance based on fuzzy analytic network process method is proposed. The local weights of indices are determined by fuzzy preference programming method. Meanwhile, an unweighted supermatrix based on its network structure is built for those interactional indicators, and the limit supermatrix is calculated. Matlab software is used to solve the problems. Finally, a numerical example of the risk evaluation of a dynamic alliance is illustrated by proposed method.",,Electronic:978-1-61284-109-0; POD:978-1-61284-108-3,10.1109/ICBMEI.2011.5914449,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5914449,dynamic alliance;fuzzy analytic network process;risk evaluation,Companies;Indexes;Mathematical model;Pragmatics;Programming;Risk management;Software,competitive intelligence;decision making;fuzzy set theory;organisational aspects;risk analysis,Matlab software;dynamic alliance organizational form;fuzzy analytic network process;fuzzy preference programming method;index system;interactional indicators;market competition;risk evaluation model;unweighted supermatrix,,1,,15,,no,13-15 May 2011,,IEEE,IEEE Conference Publications
The design and flow simulation of the volute casing of a mixed flow pump,C. Ji; Y. Wang,"Sch. of Energy &amp; Power Eng., Dalian Univ. of Technol., Dalian, China",Proceedings of 2011 International Conference on Electronic & Mechanical Engineering and Information Technology,20110919,2011,2,,820,823,"Based on the three dimensional geometry of the prototype design of a mixed flow pump, a numerical flow simulation and analytic platform based on commercial CFD software NUMECA was established to simulate the three-dimensional flows in the pumps. The pump volute with scroll-type and different cross-sections were studied. The key sources of the flow losses were analyzed and detailed structural modifications were proposed in this paper. Final results indicate that the efficiency can be increased by 4%.",,DVD:978-1-61284-086-4; Electronic:978-1-61284-088-8; POD:978-1-61284-087-1,10.1109/EMEIT.2011.6023220,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6023220,Cross-sections;Mixed flow pump;Numerical analysis;Volute,Analytical models;Blades;Impellers;Numerical models;Prototypes;Shape;Tongue,computational fluid dynamics;confined flow;design engineering;flow simulation;geometry;mixing;numerical analysis;pumps,commercial CFD software NUMECA;mixed flow pump;numerical flow simulation;prototype design;pump volute casing;structural modification;three dimensional geometry,,0,,5,,no,12-14 Aug. 2011,,IEEE,IEEE Conference Publications
The information security risk assessment based on AHP and fuzzy comprehensive evaluation,S. Fu; H. Zhou,"Department of Information Management, Hunan University of Finance and Economics, Changsha, P.R. China",2011 IEEE 3rd International Conference on Communication Software and Networks,20110908,2011,,,124,128,"Information security risk assessment is the most fundamental basis of the risk management about information security. Moreover, the objectivity and accuracy of information security risk assessment play an important role to safeguard the information system security. Through the study on the information security risk of the network system in a university, we firstly build an index system of information security risk assessment and analyze the functions of its essential elements in detail. Then, based on AHP and fuzzy comprehensive evaluation, an information security risk assessment method is proposed to effectively resolve the quantitative assessment problems of qualitative indicators in the risk assessment. An actual instance shows that the method is of strong rationality and effectiveness, and it can be better applied to information security risk assessment.",,Electronic:978-1-61284-486-2; POD:978-1-61284-485-5,10.1109/ICCSN.2011.6014018,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014018,Analytic Hierarchy Process (AHP);fuzzy comprehensive evaluation;fuzzy relation matrix;information security;risk assessment,Artificial intelligence;Bismuth;TV,computer network security;decision making;educational institutions;fuzzy set theory;information systems;risk management,AHP;analytic hierarchy process;fuzzy comprehensive evaluation;information security risk assessment;information system security;qualitative indicators;risk management;university network system,,0,,10,,no,27-29 May 2011,,IEEE,IEEE Conference Publications
The method of quality management software,A. Kharchenko; I. Galay; V. Yatcyshyn,"National Aviation University, UKRAINE, Kyev, Cosmonaut Komarov Avenue 1",Perspective Technologies and Methods in MEMS Design,20110721,2011,,,82,84,"Were considered the method of process quality management software in its design. The method includes the development of quality requirements based on ISO 25010, and implementation procedures of communication requirements. This allowed to realize the monitoring process as an intermediate product life-cycle and thus ensure compliance with software quality requirements. Communication requirements of the procedure developed based on the method SQFD (Software Quality Funktion Deployment) and the modified Analytic Hierarchy Process. Were developed also ‰ÛÏCASE‰Ûù method which automatizes implementation of these processes.",,Electronic:978-966-2191-18-9; POD:978-1-4577-0639-4,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5960278,Software;quality management;quality model;quality requirements,ISO standards;Instruments;Measurement;Software algorithms;Software quality;Software systems,decision making;software quality,ISO 25010 standard;analytic hierarchy process;process quality management software;product lifecycle;software quality function deployment;software quality requirements,,0,,11,,no,11-14 May 2011,,IEEE,IEEE Conference Publications
The prioritization factors of Enterprise Application Integration (EAI) adoption in Malaysian e-Government,M. Binti Sohimi; W. Faezah Binti Abbas,"Faculty of Computer & Mathematical Sciences, Universiti Teknologi Mara, Selangor, Malaysia",2011 International Conference on Research and Innovation in Information Systems,20120109,2011,,,1,6,"This paper examines the factors that influence Enterprise Application Integration (EAI) adoption in Malaysian government. This research focuses on one of the Malaysian e-Government project which has been selected as the case study for this research. The case study analysis is to investigate the prioritization of EAI factors by using Miles & Huberman [1] scale and Analytic Hierarchy Process (AHP) technique. The study of these factors is based on EAI adoption model in LGA that proposed by Kamal et al. [2]. The results are comprised through a series of tables which illustrated the prioritization for each factors. Therefore, this research outcome provides a guideline for Malaysian public sector for EAI adoption in e-Government project.",2324-8149;23248149,Electronic:978-1-61284-294-3; POD:978-1-61284-295-0,10.1109/ICRIIS.2011.6125710,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6125710,EAI Adoption;e-Government,Barium;Digital signal processing;Electronic government;Interviews;Software,decision making;government data processing,Malaysian public sector;Miles-Huberman scale;analytic hierarchy process technique;e-government;enterprise application integration adoption,,0,,36,,no,23-24 Nov. 2011,,IEEE,IEEE Conference Publications
The research of optimal design of heat exchanger in heat exchanger heat meter,L. Meng; F. Bin,"College of Electronic Information and Control Engineering, Beijing University of Technology, Beijing, 100124, China?","Proceedings of 2011 International Conference on Modelling, Identification and Control",20110804,2011,,,347,350,"In heat exchanger heat meter, the heat exchanger is equivalent to a flowmeter. The flow and heat can be calculated by measuring the inlet and outlet temperature of heat exchanger. It greatly prevents the flowmeter from the problem of inaccuracy and damage which are caused by the quality of water. However, due to the complexity of the heat exchanger model, it is difficult to get an expression between flow and temperature through analytic methods. This makes a great difficulty to design and optimize the structure of heat exchanger. To solve the problem above, we establish a three-dimensional model of heat exchanger by means of CFD numerical simulation software. By solving it in the parallel computing platform, we get the relationship between flow and temperature difference ratio, and optimize the structure of the heat exchanger by means of pattern search method. Simulation results show that after using this optimization method, the performance of heat exchanger has been effectively improved.",,Electronic:978-0-9567157-0-8,10.1109/ICMIC.2011.5973729,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5973729,,Analytical models;Computational modeling;Electron tubes;Fluids;Heating;Numerical models;Temperature measurement,computational fluid dynamics;design engineering;flowmeters;heat exchangers;heat measurement;mechanical engineering computing;numerical analysis,CFD numerical simulation software;flowmeter;heat exchanger;heat meter;optimal design;parallel computing platform,,1,,6,,no,26-29 June 2011,,IEEE,IEEE Conference Publications
The study of supplier selection of vegetable supply chain in Hebei Province,F. Wang; W. Sun,"Business School, Agricultural University of Hebei, Baoding, China",MSIE 2011,20110204,2011,,,116,119,"Purpose of this paper is that the choice of vegetable suppliers provides a strong guarantee for the vegetable supply chain to effectively avoid risks and enhance core competencies in Hebei Province. From four types of the vegetable suppliers and five indicators, the article uses the Analytic Hierarchy Process (AHP) to compare their advantages and disadvantages. The results show that the Agricultural Association is the best suppliers on the vegetable supply chain. This finding can strengthen the construction of vegetable supply chain and improve the vegetables competition in the market.",,Electronic:978-1-4244-8385-3; POD:978-1-4244-8383-9,10.1109/MSIE.2011.5707616,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5707616,Analytic Hierarchy Process (AHP);supplier selection;vegetable supply chain,Accuracy;Contracts;Indexes;Industries;Software systems;Supply chains,agricultural products;decision making;supply chain management,AHP;Hebei province;agricultural association;analytic hierarchy process;vegetable supplier selection;vegetable supply chain,,0,,6,,no,8-11 Jan. 2011,,IEEE,IEEE Conference Publications
TIALA ‰ÛÓ Time series alignment analysis,G. JÌ_ger; F. Battke; K. Nieselt,"Center for Bioinformatics T&#x00FC;bingen, University of Tubingen",2011 IEEE Symposium on Biological Data Visualization (BioVis).,20111205,2011,,,55,61,"The analysis of time series expression data is widely employed for investigating biological mechanisms. Microarrays are often used to generate time series for several different experimental conditions. These time series then need to be compared to each other. For a successful comparison it is necessary to perform a time series alignment because the experiments can differ in the number of time points, as well as in the time points themselves. In this work we propose a novel visual analytics approach for the analysis of multiple time series experiments in parallel. Our time series alignment analysis tool Tiala allows one to align multiple time series experiments and to visually explore the aligned expression profiles. A two- and three-dimensional visualization strategy was implemented that is especially designed to enhance the display of multiple aligned time series expression profiles. Tiala is available as a part of the microarray data analysis software Mayday. Mayday itself is open source software distributed under the terms of the GNU General Public License. It is available from http://www.microarray-analysis.org. We apply our approach to time series showing abiotic stress responses of Arabidopsis thaliana and to data sets from two replicates of the antibiotics producing bacterium Streptomyces coelicolor.",,Electronic:978-1-4673-0004-9; POD:978-1-4673-0003-2,10.1109/BioVis.2011.6094048,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6094048,Multiple time series alignment;expression analysis;microarrays;visual analytics,Data visualization;Gene expression;Image color analysis;Stress;Time series analysis;Visualization,bioinformatics;data analysis;data visualisation;distributed processing;lab-on-a-chip;public domain software;solid modelling;time series,GNU general public license;Mayday;TIALA;abiotic stress response;arabidopsis thaliana;bacterium streptomyces coelicolor;biological mechanism;distributed open source software;microarray data analysis software;three-dimensional visualization strategy;time series alignment analysis tool;time series expression data analysis;visual analytics approach,,1,,17,,no,23-24 Oct. 2011,,IEEE,IEEE Conference Publications
Towards Delivering Analytical Solutions in Cloud: Business Models and Technical Challenges,X. Sun; B. Gao; Y. Zhang; W. An; H. Cao; C. Guo; W. Sun,"IBM Res. - China, Beijing, China",2011 IEEE 8th International Conference on e-Business Engineering,20111215,2011,,,347,351,"Analytical Solutions increasingly play a key role in the modern enterprise business. Currently, such solutions are usually very costly for customer to consume, as the deployment cost is high due to high performance hardware requirement and complex software configuration. Moreover, such on-premises solutions are not suitable for the occasional analytics consumers. On the other hand, Analytical solutions are also hard for solution providers to deliver cost efficient service, since the cost is high in initial customer engagement as well as conducting incremental service. To deliver analytical solutions in a cost-effective way, we propose the idea of ""analytical cloud"", which is designed to provide on-demand decision support, analytical capabilities, and computational resources in a manageable cloud environment. In this paper, we summarize the potential business models supported by this new delivery model and identify the technical issues required to be addressed in such business models.",,Electronic:978-0-7695-4518-9; POD:978-1-4577-1404-7,10.1109/ICEBE.2011.81,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6104640,,Conferences,business data processing;cloud computing;competitive intelligence;decision support systems,analytical capabilities;analytical cloud;analytical solution delivery;business intelligence;business models;computational resources;cost efficient service;customer engagement;enterprise business;incremental service;on-demand decision support;technical challenges,,0,,14,,no,19-21 Oct. 2011,,IEEE,IEEE Conference Publications
Towards efficient resource management for data-analytic platforms,C. Castillo; M. Spreitzer; M. Steinder,"IBM T. J. Watson Research Center, 19 Skyline Drive, Hawthorne, New York 10532, USA",12th IFIP/IEEE International Symposium on Integrated Network Management (IM 2011) and Workshops,20110818,2011,,,73,80,"We present architectural and experimental work exploring the role of intermediate data handling in the performance of MapReduce workloads. Our findings show that: (a) certain jobs are more sensitive to disk cache size than others and (b) this sensitivity is mostly due to the local file I/O for the intermediate data. We also show that a small amount of memory is sufficient for the normal needs of map workers to hold their intermediate data until it is read. We introduce Hannibal, which exploits the modesty of that need in a simple and direct way - holding the intermediate data in application-level memory for precisely the needed time - to improve performance when the disk cache is stressed. We have implemented Hannibal and show through experimental evaluation that Hannibal can make MapReduce jobs run faster than Hadoop when little memory is available to the disk cache. This provides better performance insulation between concurrent jobs.",1573-0077;15730077,Electronic:978-1-4244-9221-3; POD:978-1-4244-9219-0; USB:978-1-4244-9220-6,10.1109/INM.2011.5990676,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5990676,Hadoop;Map-Reduce;disk;performance,Context;Insulation;Monitoring;Reliability;Resource management,cache storage;data handling;middleware;public domain software,Hadoop;Hannibal;MapReduce;MapReduce workloads;application-level memory;data-analytic platforms;disk cache size;intermediate data handling;open source middleware;resource management,,1,,19,,no,23-27 May 2011,,IEEE,IEEE Conference Publications
Towards eliminating configuration errors in cyber infrastructure,S. Narain; S. Malik; E. Al-Shaer,"Telcordia Technologies, Inc, Piscataway, NJ, USA",2011 4th Symposium on Configuration Analytics and Automation (SAFECONFIG),20111226,2011,,,1,2,"It is well-documented that configuration errors account for 50% to 80% of downtime and vulnerabilities in cyber infrastructure. The ConfigAssure suite of tools has been developed to help eliminate these errors. These tools are for requirement specification, configuration synthesis, diagnosis and repair, verification, reconfiguration planning and visualization. These tools are being made available as a web service that is demonstrated.",,Electronic:978-1-4673-0402-3; POD:978-1-4673-0401-6,10.1109/SafeConfig.2011.6111678,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6111678,configuration error;diagnosis;reconfiguration planning;repair;specification;synthesis;visualization,Educational institutions;Engines;Laboratories;Maintenance engineering;Network topology;Security;Visualization,Web services;data visualisation;formal specification;formal verification;software tools,ConfigAssure suite;Web service;configuration error elimination;configuration synthesis;cyber infrastructure;diagnosis;reconfiguration planning;requirement specification;verification;visualization,,0,,10,,no,Oct. 31 2011-Nov. 1 2011,,IEEE,IEEE Conference Publications
Tracking commander's intent in dynamic networks,M. K. Martin; K. M. Carley; P. Sauk; P. Perrin; D. O'Neill; A. Woolley,"Institute for Software Research, Carnegie Mellon University, USA",2011 - MILCOM 2011 Military Communications Conference,20120112,2011,,,1517,1522,"A goal of the Tactical Human Integration of Networked Knowledge (THINK) Army Technology Objective (ATO) is to develop and test a proposed socio-cognitive network analytic method that aims to improve the effective understanding and execution of commander's intent (CI), for a given echelon, based on the interpretation of the operational order (OPORD) and the monitoring of the warfighters' activities, via their communications. Network Text Analysis is used to extract a meta-network model from OPORDs and operational discourse, such as email and chat logs. The contents of this meta-network model are coded using the Battle Management Language (BML), and Dynamic Network Analysis (DNA) techniques are then applied to the extracted data in order to determine the extent to which a Commander's Intent is being accurately followed and to identify points and bases for departure. This will lead to the establishment of new requirements for instrumentation of the net-centric environment for detection of human behavior.",2155-7578;21557578,Electronic:978-1-4673-0081-0; POD:978-1-4673-0079-7,10.1109/MILCOM.2011.6127522,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6127522,BML;Commander's Intent;Human Cognition;Meta-Network;Net-Centric,DNA;Electronic mail;Measurement;Monitoring;Organizations;Semantics;Thesauri,behavioural sciences computing;electronic mail;military communication;military computing;text analysis,Battle Management Language;Dynamic Network Analysis;OPORD;THINK Army Technology Objective;Tactical Human Integration of Networked Knowledge;chat log;commander intent execution;commander intent tracking;commander intent understanding;email;human behavior detection;metanetwork model extraction;military communication;net-centric environment;network text analysis;operational discourse;operational order;sociocognitive network analytic method;warfighter activity monitoring,,0,,7,,no,7-10 Nov. 2011,,IEEE,IEEE Conference Publications
Understanding Boundary Scan test with Trainer 1149,A. Jutman; R. Ubar; S. Devadze; K. Shibin; V. Rosin,"Department of Computer Engineering, Tallinn University of Technology, Tallinn, Estonia",2011 Proceedings of the 22nd EAEEIE Annual Conference (EAEEIE),20120309,2011,,,1,6,"In this paper, we describe a training environment based on multi-functional software system called ‰ÛÏTrainer 1149‰Ûù. It provides simulation and demonstration functionality for learning, research, and development related to IEEE 1149.1 Boundary Scan (BS) standard. The software supports both the analytic and synthetic learning process. Trainer 1149 is the main component of a recent goJTAG initiative that aims at creating of an open-source platform for learning and working with BS. Trainer 1149 provides a cozy graphical design and simulation environment of BS-enabled chips and non-BS clusters. It is also possible to simulate the behaviour of various interconnect faults and inspect them using interactive tools. Using a convenient low-cost USB-JTAG cable, one can test real defects in real hardware. The system is implemented using multi-platform Java environment and distributed as an open-source freeware. Such combination of features is unique for public domain BS software.",,Electronic:978-961-248-281-7; POD:978-1-4673-1150-2,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6165727,Boundary Scan;IEEE 1149.1;JTAG;Trainer 1149;goJTAG,Fault diagnosis;Pins;Registers;Software;Testing;Training;Vectors,IEEE standards;Java;boundary scan testing;computer based training;electronic engineering computing;electronic engineering education;integrated circuit testing;public domain software,IEEE 1149.1 Boundary Scan standard;Join Test Action Group;Trainer 1149 software;USB-JTAG cable;Universal Serial Bus;boundary scan test;graphical design;interactive tool;learning platform;multifunctional software system;multiplatform Java environment;open-source freeware;open-source platform;public domain BS software;simulation environment;training environment,,0,,10,,no,13-15 June 2011,,IEEE,IEEE Conference Publications
Video Analytics: Opportunity or Spoof Story? The State of the Art of Intelligent Video Surveillance,M. Argiolu; F. Bisogni,"Formit Found., Rome, Italy",2011 European Intelligence and Security Informatics Conference,20111027,2011,,,294,294,"In 2010 FORMIT developed the VIEWER project with the support of the Prevention, Preparedness and Consequence Management of Terrorism and other Security Related Risks Programme of the European Commission, DG Home Affairs. The main goals of the project were to realize a recognition of Video Analytics (hereafter VA) software currently on the market; to collect qualitative information and quantitative data on the VA software performances in order to be able to define the state of the art; to address policy recommendations to the European Commission; to address recommendation to the research community on the future desirable development in the sector.",,Electronic:978-0-7695-4406-9; POD:978-1-4577-1464-1,10.1109/EISIC.2011.46,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6061220,Technology performance;Video Analytics;Video Surveillance,Automation;Europe;Intrusion detection;Reliability;Software;Video surveillance,government data processing;terrorism;video surveillance,European Commission;FORMIT;VIEWER project;intelligent video surveillance;security related risks programme;terrorism;video analytics,,0,,,,no,12-14 Sept. 2011,,IEEE,IEEE Conference Publications
Visual analytical approaches to evaluating uncertainty and bias in crowd sourced crisis information,I. Dillingham; J. Dykes; J. Wood,"giCentre, School of Informatics, City University London, UK",2011 IEEE Conference on Visual Analytics Science and Technology (VAST),20111215,2011,,,273,274,"Concerns about verification mean the humanitarian community are reluctant to use information collected during crisis events, even though such information could potentially enhance the response effort. Consequently, a program of research is presented that aims to evaluate the degree to which uncertainty and bias are found in public collections of incident reports gathered during crisis events. These datasets exemplify a class whose members have spatial and temporal attributes, are gathered from heterogeneous sources, and do not have readily available attribution information. An interactive software prototype, and existing software, are applied to a dataset related to the current armed conflict in Libya to identify `intrinsic' characteristics against which uncertainty and bias can be evaluated. Requirements on the prototype are identified, which in time will be expanded into full research objectives.",,Electronic:978-1-4673-0014-8; POD:978-1-4673-0015-5,10.1109/VAST.2011.6102470,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6102470,,Data visualization;Information science;Prototypes;Software;Uncertainty;User-generated content;Visualization,data analysis;data visualisation;emergency services,Libya;bias evaluation;crisis events;crowdsourced crisis information;humanitarian community;interactive software prototype;intrinsic characteristics;spatial attributes;temporal attributes;uncertainty evaluation;visual analytical approach,,0,,16,,no,23-28 Oct. 2011,,IEEE,IEEE Conference Publications
Visual analytics of terrorist activities related to epidemics,E. Bertini; J. BuchmÌ_ller; F. Fischer; S. Huber; T. Lindemeier; F. MaaÌÙ; F. Mansmann; T. Ramm; M. Regenscheit; C. Rohrdantz; C. Scheible; T. Schreck; S. Sellien; F. Stoffel; M. Tautzenberger; M. Zieker; D. A. Keim,"Data Analysis and Visualization Group, University of Konstanz, Germany",2011 IEEE Conference on Visual Analytics Science and Technology (VAST),20111215,2011,,,329,330,"The task of the VAST 2011 Grand Challenge was to investigate potential terrorist activities and their relation to the spread of an epidemic. Three different data sets were provided as part of three Mini Challenges (MCs). MC 1 was about analyzing geo-tagged microblogging (Twitter) messages to characterize the spread of an epidemic. MC 2 required analyzing threats to a computer network using a situational awareness approach. In MC 3 possible criminal and terrorist activities were to be analyzed based on a collection of news articles. To solve the Grand Challenge, insight from each of the individual MCs had to be integrated appropriately.",,Electronic:978-1-4673-0014-8; POD:978-1-4673-0015-5,10.1109/VAST.2011.6102498,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6102498,,Data analysis;Imaging;Rivers;Software;Tag clouds;Terrorism;Visual analytics,computer network security;data analysis;data visualisation;epidemics;medical computing;social networking (online);terrorism,MC 1;MC 2;MC 3;Twitter;VAST 2011 Grand Challenge;computer network threat analysis;criminal activities;epidemic spread;geo-tagged microblogging message analysis;news articles collection;situational awareness approach;terrorist activities;visual analytics,,1,,13,,no,23-28 Oct. 2011,,IEEE,IEEE Conference Publications
Visual analytics tools and theirs application in social networks analysis,D. Randjelovi€à; B. Popovi€à,"Academy fof Criminalistic and Police Studies, Cara Du&#x0161;ana 196, 11080 Belgrade, Serbia",2011 19thTelecommunications Forum (TELFOR) Proceedings of Papers,20120202,2011,,,1340,1343,"The first association on the word ""data"" for common man is a visualization of the alpha-numeric characters and various pictures in the form of a table or chart. Distinction between data, information and knowledge, is one of the primary tasks of information science. The quantity of electronic data in today's information society is steadily increased by using Internet technologies (among other things with the e-government and all it's parts-from commerce, education to public health, also with the emergence of social networks and media in all their forms). It requires visualization of the data for easier understanding and analysis which are obviously necessary. In this paper some of the commercial and non-commercial visual tools and their application in analysis of social networks are considered, with especially interest for theirs application in the field of criminology.",,Electronic:978-1-4577-1498-6; POD:978-1-4577-1499-3,10.1109/TELFOR.2011.6143801,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6143801,Commercial and non-commercial tools;Social networks analysis;Visual analytics;Visualization,Data visualization;Electronic publishing;Information services;Internet;Social network services;Software;Visual analytics,Internet;criminal law;data visualisation;information science;social networking (online),Internet technologies;alpha-numeric characters visualization;commerce;commercial visual tool;criminology;data visualization;e-government;education;information science;noncommercial visual tool;public health;social networks analysis;visual analytics tools,,0,,32,,no,22-24 Nov. 2011,,IEEE,IEEE Conference Publications
Visualization of Parameter Space for Image Analysis,A. J. Pretorius; M. A. Bray; A. E. Carpenter; R. A. Ruddle,"School of Computing, University of Leeds",IEEE Transactions on Visualization and Computer Graphics,20111103,2011,17,12,2402,2411,"Image analysis algorithms are often highly parameterized and much human input is needed to optimize parameter settings. This incurs a time cost of up to several days. We analyze and characterize the conventional parameter optimization process for image analysis and formulate user requirements. With this as input, we propose a change in paradigm by optimizing parameters based on parameter sampling and interactive visual exploration. To save time and reduce memory load, users are only involved in the first step - initialization of sampling - and the last step - visual analysis of output. This helps users to more thoroughly explore the parameter space and produce higher quality results. We describe a custom sampling plug-in we developed for CellProfiler - a popular biomedical image analysis framework. Our main focus is the development of an interactive visualization technique that enables users to analyze the relationships between sampled input parameters and corresponding output. We implemented this in a prototype called Paramorama. It provides users with a visual overview of parameters and their sampled values. User-defined areas of interest are presented in a structured way that includes image-based output and a novel layout algorithm. To find optimal parameter settings, users can tag high- and low-quality results to refine their search. We include two case studies to illustrate the utility of this approach.",1077-2626;10772626,,10.1109/TVCG.2011.253,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065007,Information visualization;image analysis;parameter space;sampling.;visual analytics,Algorithm design and analysis;Image analysis;Information processing;Sampling methods,data visualisation;image sampling;medical image processing,CellProfiler;Paramorama;biomedical image analysis framework;custom sampling plug-in;interactive visual exploration;interactive visualization;parameter optimization process;parameter sampling;parameter space visualization;visual analysis,"Algorithms;Androstadienes;Cell Line;Cell Nucleus;Chromones;Computer Graphics;Computer Simulation;Humans;Image Processing, Computer-Assisted;Morpholines;Software;User-Computer Interface",21,,27,,no,Dec. 2011,,IEEE,IEEE Journals & Magazines
Visualizing an information assurance risk taxonomy,V. Lemieux; B. Endicott-Popovsky; K. Eckler; T. Dang; A. Jansen,"University of British Columbia, USA",2011 IEEE Conference on Visual Analytics Science and Technology (VAST),20111215,2011,,,287,288,"The researchers explore the intersections between Information Assurance and Risk using visual analysis of text mining operations. The methodological approach involves searching for and extracting for analysis those abstracts and keywords groupings that relate to risk within a defined subset of scientific research journals. This analysis is conducted through a triangulated study incorporating visualizations produced using both Starlight and In-Spire visual analysis software. The results are definitional, showing current attitudes within the Information Assurance research community towards risk management strategies, while simultaneously demonstrating the value of visual analysis processes when engaging in sense making of a large body of knowledge.",,Electronic:978-1-4673-0014-8; POD:978-1-4673-0015-5,10.1109/VAST.2011.6102477,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6102477,Information Assurance;Information Security;Risk;Risk Management;Visual Analysis,Abstracts;Availability;Databases;Educational institutions;Security;Taxonomy;Visualization,data analysis;data mining;data visualisation;risk management;security of data,In-Spire visual analysis software;Starlight software;abstracts;information assurance risk taxonomy visualization;keywords groupings;risk management strategies;scientific research journals;text mining operations;visual analysis,,0,,2,,no,23-28 Oct. 2011,,IEEE,IEEE Conference Publications
Vulnerability hierarchies in access control configurations,D. R. Kuhn,"National Institute of Standards and Technology, Gaithersburg, MD 20899, USA",2011 4th Symposium on Configuration Analytics and Automation (SAFECONFIG),20111226,2011,,,1,9,"This paper applies methods for analyzing fault hierarchies to the analysis of relationships among vulnerabilities in misconfigured access control rule structures. Hierarchies have been discovered previously for faults in arbitrary logic formulae [11,10,9,21], such that a test for one class of fault is guaranteed to detect other fault classes subsumed by the one tested, but access control policies reveal more interesting hierarchies. These policies are normally composed of a set of rules of the form ‰ÛÏif [conditions] then [decision]‰Ûù, where [conditions] may include one or more terms or relational expressions connected by logic operators, and [decision] is often 2-valued (‰ÛÏgrant‰Ûù or ‰ÛÏdeny‰Ûù), but may be n-valued. Rule sets configured for access control policies, while complex, often have regular structures or patterns that make it possible to identify generic vulnerability hierarchies for various rule structures such that an exploit for one class of configuration error is guaranteed to succeed for others downstream in the hierarchy. A taxonomy of rule structures is introduced and detection conditions computed for nine classes of vulnerability: added term, deleted term, replaced term, stuck-at-true condition, stuck-at-false condition, negated condition, deleted rule, replaced decision, negated decision. For each configuration rule structure, detection conditions were analyzed for the existence of logical implication relations between detection conditions. It is shown that hierarchies of detection conditions exist, and that hierarchies vary among rule structures in the taxonomy. Using these results, tests may be designed to detect configuration errors, and resulting vulnerabilities, using fewer tests than would be required without knowledge of the hierarchical relationship among common errors. In addition to practical applications, these results may help to improve the understanding of access control policy configurations.",,Electronic:978-1-4673-0402-3; POD:978-1-4673-0401-6,10.1109/SafeConfig.2011.6111679,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6111679,access control;change impact analysis;configuration analysis,Access control;Analytical models;Computer crime;Impedance matching;Taxonomy;Testing,authorisation;set theory;software fault tolerance,access control policy configuration;added term;configuration rule structure;deleted rule;deleted term;detection condition;fault classes;fault hierarchy analysis;generic vulnerability hierarchies;logic operators;logical implication relations;misconfigured access control rule structure;negated condition;negated decision;regular patterns;regular structures;relational expression;replaced decision;replaced term;rule sets;stuck-at-false condition;stuck-at-true condition,,1,,21,,no,Oct. 31 2011-Nov. 1 2011,,IEEE,IEEE Conference Publications
WAM‰ÛÓThe Weighted Average Method for Predicting the Performance of Systems with Bursts of Customer Sessions,D. Krishnamurthy; J. Rolia; M. Xu,"Dept. of Electr. &amp; Comput. Eng., Univ. of Calgary, Calgary, AB, Canada",IEEE Transactions on Software Engineering,20110929,2011,37,5,718,735,"Predictive performance models are important tools that support system sizing, capacity planning, and systems management exercises. We introduce the Weighted Average Method (WAM) to improve the accuracy of analytic predictive performance models for systems with bursts of concurrent customers. WAM considers the customer population distribution at a system to reflect the impact of bursts. The WAM approach is robust with respect to distribution functions, including heavy-tail-like distributions, for workload parameters. We demonstrate the effectiveness of WAM using a case study involving a multitier TPC-W benchmark system. To demonstrate the utility of WAM with multiple performance modeling approaches, we developed both Queuing Network Models and Layered Queuing Models for the system. Results indicate that WAM improves prediction accuracy for bursty workloads for QNMs and LQMs by 10 and 12 percent, respectively, with respect to a Markov Chain approach reported in the literature.",0098-5589;00985589,,10.1109/TSE.2011.65,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5953602,Performance of systems;modeling techniques;operational analysis.;queuing theory,Accuracy;Analytical models;Markov processes;Predictive models;Queueing analysis;Software;Time factors,Markov processes;queueing theory;software performance evaluation;systems analysis,Markov chain approach;analytic predictive performance models;bursty workloads;capacity planning;customer session bursts;heavy tail like distributions;layered queuing models;multitier TPC-W benchmark system;queuing network models;system performance prediction;system sizing;systems management exercises;weighted average method,,4,,41,,no,Sept.-Oct. 2011,,IEEE,IEEE Journals & Magazines
Watershed Reanalysis: Towards a National Strategy for Model-Data Integration,C. Duffy; L. Leonard; G. Bhatt; X. Yu; L. Giles,"Dept. of Civil & Environ. Eng., Pennsylvania State Univ., University Park, PA, USA",2011 IEEE Seventh International Conference on e-Science Workshops,20120116,2011,,,61,65,"Reanalysis or retrospective analysis is the process of re-analyzing and assimilating climate and weather observations with the current modeling context. Reanalysis is an objective, quantitative method of synthesizing all sources of information (historical and real-time observations) within a unified framework. In this context, we propose a prototype for automated and virtualized web services software using national data products for climate reanalysis, soils, geology, terrain and land cover for the purpose of water resource simulation, prediction, data assimilation, calibration and archival. The prototype for model-data integration focuses on creating tools for fast data storage from selected national databases, as well as the computational resources necessary for a dynamic, distributed watershed prediction anywhere in the continental US. In the future implementation of virtualized services will benefit from the development of a cloud cyber infrastructure as the prototype evolves to data and model intensive computation for continental scale water resource predictions.",,Electronic:978-0-7695-4598-1; POD:978-1-4673-0026-1,10.1109/eScienceW.2011.32,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6130732,Climate Reanalysis;Data Analytics;Distributed Hydrologic Model;Numerical Watershed Prediction (NWP);PIHM;Software as a Service (SaaS);Visual Analytics;Web Services,Atmospheric modeling;Computational modeling;Data models;Meteorology;Numerical models;Soil;Web services,Web services;climatology;geology;soil;terrain mapping;water resources,archival;calibration;climate observation;climate reanalysis;cloud cyber infrastructure;computational resources;data assimilation;geology;land cover;model-data integration;national data product;national database;national strategy;reanalyzing;retrospective analysis;soil;terrain;virtualized Web service;virtualized service;water resource prediction;water resource simulation;watershed reanalysis;weather observation,,2,,32,,no,5-8 Dec. 2011,,IEEE,IEEE Conference Publications
"What Time Is It, Eccles?",N. Maiden,,IEEE Software,20110623,2011,28,4,84,85,"Requirements analysts need a new toolbox with both the right tools and the instructions to use them including agile development and user-centered design for techniques such as analysis of Web analytics, wire-framing, and user stories. We can also look to the creativity literature and take techniques such as constraint removal, storytelling, and other worlds.",0740-7459;07407459,,10.1109/MS.2011.86,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5929526,agile;creativity;meaning carriers;requirements;software engineering;techniques,Agile manufacturing;Research and development;Software development management;User centered design,,,,0,,1,,no,July-Aug. 2011,,IEEE,IEEE Journals & Magazines
XQConverter: A System for XML Query Analysis,J. Schejbal; J. Stå«rka; I. Mlynkovå«,"Dept. of Software Eng., Charles Univ. in Prague, Prague, Czech Republic",2011 22nd International Workshop on Database and Expert Systems Applications,20120116,2011,,,129,133,"As XQuery programs became one of the standard instruments of XML manipulation, for the optimization purposes it is important to identify commonly used constructs and patterns. In this paper we describe the design and implementation of a system for the analysis of XQuery programs, XQ Converter, as the extension of our previously described analytic framework Analyzer. The XQuery program is converted to an XML representation which allows to formulate analytical queries in XPath. The analysis is based on the frequency of occurrence of various language constructs.",1529-4188;15294188,Electronic:978-0-7695-4486-1; POD:978-1-4577-0982-1,10.1109/DEXA.2011.90,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6059805,XML;XQuery;analysis;data representation,Context;Generators;Java;Production;Runtime;Syntactics;XML,XML;query processing,XML manipulation;XML query analysis;XML representation;XPath;XQ Converter;XQConverter;XQuery programs;analyzer analytic framework,,0,,17,,no,Aug. 29 2011-Sept. 2 2011,,IEEE,IEEE Conference Publications
A comparison of Fuzzy DNP and SEM in analyzing novel mobile learning technology acceptances by learners,C. Y. Huang,"Dept. of Ind. Educ., Nat. Taiwan Normal Univ., Taipei, Taiwan",2012 International conference on Fuzzy Theory and Its Applications (iFUZZY2012),20130114,2012,,,119,124,"Mobile learning, the learning method which learners can leverage mobile devices to learn everywhere, has become the one of the most potential learning approach as novel mobile devices emerges. However, how the novel mobile learning technology can be accepted by learners was seldom addressed. Meanwhile, due to the unavailability of large number (more than 100) experts for evaluating novel mobile learning devices, the traditional statistical methods, e.g. the structural equation model (SEM), are not appropriate for evaluating the factors influencing the acceptance of novel techniques, a feasible research framework will be very helpful for achieving the evaluation purposes. In this work, the author proposes a novel technology acceptance modeling (TAM) evaluation framework with the fuzzy Decision Making Trial and Evaluation Laboratory (DEMATEL) based Network Process (DNP) to resolve the above mentioned real world problem. The SEM based analytic results based on learners will be introduced for demonstrating the effectiveness of the Fuzzy DNP. An empirical study based on the analysis of factors influencing Taiwanese mobile learners' acceptances of some specific mobile learning software being installed in iPhone will be introduced for demonstrating the feasibility and effectiveness of the proposed method. Based on the analytic results, the Fuzzy DNP based framework can be used for real world technology acceptance analysis of novel mobile learning techniques. Meanwhile, the derived factors which can influence the mobile learning technology acceptances can serve as the basis for educators and marketers for designing and improving the next generation learning devices.",2377-5823;23775823,Electronic:978-1-4673-2056-6; POD:978-1-4673-2057-3,10.1109/iFUZZY.2012.6409686,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6409686,Fuzzy DEMATEL (Decision Making Trial and Evaluation Laboratory);SEM (Structural Equation Model);consumer behavior;mobile learning;technology acceptance model (TAM),Analytical models;Bismuth;Equations;Mathematical model;Mobile communication;Numerical analysis;Pragmatics,computer aided instruction;decision making;fuzzy set theory;mobile computing;social aspects of automation,DEMATEL network processing;SEM;TAM evaluation framework;decision making trial and evaluation laboratory;fuzzy DNP;iPhone;learning approach;mobile learning software;mobile learning technology;statistical method;structural equation model;technology acceptance modeling,,0,,36,,no,16-18 Nov. 2012,,IEEE,IEEE Conference Publications
A Cost-Effective Approach to Delivering Analytics as a Service,X. Sun; B. Gao; L. Fan; W. An,"IBM Res. - China, Beijing, China",2012 IEEE 19th International Conference on Web Services,20120806,2012,,,512,519,"Analytical solutions are considered as increasingly important for modern enterprises. Currently, systematical adoption of analytical solutions is limited to only a small set of large enterprises, as the deployment cost is high due to high performance hardware requirement and expensive analytics software. Moreover, such on-premises solutions are not suitable for the occasional analytics consumers. In order to accelerate the prevalence of analytical solutions, this paper explores the feasibility of leveraging SaaS (Software-as-a-Service) delivery model to provide analytics capabilities as services in a cost-effective way. The main contributions of our work include: (1) proposing a framework to enable enterprise tenants to consume analytics capabilities as services; (2) developing a method to enhance existing analytics platform to support multi-tenancy so that a single software instance can effectively support multiple concurrent tenants; (3) designing an SLA (Service Level Agreement) customization mechanism to satisfy the diverse analytics capability demands of tenants. A prototype system has been developed to evaluate the feasibility of our approach.",,Electronic:978-0-7695-4752-7; POD:978-1-4673-2131-0,10.1109/ICWS.2012.79,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6257847,analytics;multi-tenancy;software as a service,Analytical models;Business;Context;Data models;Predictive models;Servers;Software,cloud computing;enterprise resource planning,SLA;SaaS;cost effective approach;delivering analytics as a Service;expensive analytics software;hardware requirement;service level agreement;software-as-a-service;systematical adoption,,8,,16,,no,24-29 June 2012,,IEEE,IEEE Conference Publications
A Demo Paper: An Analytic Workflow Framework for Green Campus,C. Lee; S. Chaisiri; B. Zoebir; C. Chen; B. S. Lee,"Sch. of Comput. Eng., Nanyang Technol. Univ., Singapore, Singapore",2012 IEEE 18th International Conference on Parallel and Distributed Systems,20130117,2012,,,851,855,"This paper proposes a multi-tenant workflow framework that allows users to create data analytic workflows whose tasks are efficiently scheduled and distributed in cloud computing environment. We provide a demo of an event room assignment (ERA) as a test application of the framework. The ERA dynamically and automatically assigns registered events (e.g., meetings, classes, conferences, etc.) to available rooms meeting the user requirements such as the event size, purpose, reservation period, etc. The assignment will lead to the energy efficiency with respect to the power usage (e.g., lighting, ventilation, devices, etc.), and the energy savings can be achieved without affecting people's comfort. We run the ERA with power consumption data (whose size is approximately 50GB) collected from each of over 200 rooms in a building at Dept. of Engineering, Tokyo University. Through the demonstration, we will show that the proposed framework accelerates the speed of data analysis by providing user-friendly workflow composition and parallel processing features utilizing cloud computing technologies.",1521-9097;15219097,Electronic:978-0-7695-4903-3; POD:978-1-4673-4565-1,10.1109/ICPADS.2012.139,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6413593,Big Data Analytics;Cloud Computing;Workflow Composition,Buildings;Cloud computing;Computer architecture;Educational institutions;Power demand;Real-time systems;XML,cloud computing;data analysis;green computing;human computer interaction;parallel processing;power aware computing;workflow management software,Department of Engineering;ERA;Tokyo University;analytic multitenant workflow framework;cloud computing environment;cloud computing technologies;data analytic workflow;distributed tasks;energy efficiency;event room assignment;green campus;parallel processing;power consumption data;power usage;task scheduling;user requirements;user-friendly workflow composition,,1,,8,,no,17-19 Dec. 2012,,IEEE,IEEE Conference Publications
A framework to improve evaluation in educational games,ÌÅ. Serrano; E. J. Marchiori; ÌÅ. del Blanco; J. Torrente; B. FernÌÁndez-ManjÌ_n,"Dept. of Artificial Intell. & Software Eng., Complutense Univ., Madrid, Spain",Proceedings of the 2012 IEEE Global Engineering Education Conference (EDUCON),20120517,2012,,,1,8,"The evaluation process is key for educator's acceptance of any educational action. The evaluation is challenging in most cases but especially when educational games are used. In educational games if in-game evaluation exist it is usually based on a series of simple goals and whether these goals are achieved (i.e. assessment). But we consider that evaluation can be improved by taking advantage of in-game interaction, such as the user behavior during the game and the type and number of interactions performed by the user while playing. In this paper, we propose an evaluation framework for educational games based on in-game interaction data. We discuss how user interaction data is collected in the most automatic and seamless way possible, how to analyze the data to extract relevant information, and how to present this information in a usable way to educators so they achieve the maximum benefit from the experience. The evaluation framework is implemented as part of the eAdventure educational platform, where it can be used both to improve upon traditional basic assessment methods (i.e. goals, scores & reports) and to provide information to help improve interaction with games (e.g. discovery strategies).",2165-9559;21659559,Electronic:978-1-4673-1456-5; POD:978-1-4673-1457-2; USB:978-1-4673-1455-8,10.1109/EDUCON.2012.6201154,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6201154,Educational video games;Learning Analytics;case study;framework proposal,Adaptation models;Data models;Engines;Games;Graphical user interfaces;Least squares approximation;Semantics,computer aided instruction;computer games;data analysis,data analysis;eadventure educational platform;educational games;evaluation process;in-game interaction;user behavior;user interaction data,,6,,15,,no,17-20 April 2012,,IEEE,IEEE Conference Publications
A multiple criteria decision making approach for E-Learning platform selection: the Primitive Cognitive Network Process,K. K. F. Yuen,"Department of Computer science and Software Engineering, Xi'an Jiaotong-Liverpool University, Suzhou, China","2012 Computing, Communications and Applications Conference",20120220,2012,,,294,298,"E-learning is a learning form supported by information communication technology to facilitate the teachers' teaching and students' learning activities. As there are a number of E-Learning platform providers available in the market, selection of the most suitable E-Learning platform is an essential decision for the learning organization due to its high investments and lasting influence. This paper proposes the Primitive Cognitive Network Process considering multiple criteria and alternatives to address this issue. An illustrated example of E-learning platform selection demonstrates the usability and validity of the proposed approach, with comparing to the Analytic Hierarchy Process (AHP) approach.",,Electronic:978-1-4577-1719-2; POD:978-1-4577-1717-8,10.1109/ComComAp.2012.6154860,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6154860,Analytic Hierarchy Process;E-learning;E-learning Platform;cognitive network process;multiple criteria decision making,Decision making;Educational institutions;Electronic learning;System performance;Vectors,computer aided instruction;decision making;educational technology,AHP;analytic hierarchy process;e-learning platform selection;information communication technology;multiple criteria decision making approach;primitive cognitive network process,,0,,14,,no,11-13 Jan. 2012,,IEEE,IEEE Conference Publications
A multiple-objective workflow scheduling framework for cloud data analytics,O. Udomkasemsub; Li Xiaorong; T. Achalakul,"Computer Engineering Department, King Mongkut's University of Technology Thonburi (KMUTT), Bangkok, Thailand",2012 Ninth International Conference on Computer Science and Software Engineering (JCSSE),20120809,2012,,,391,398,"One of the most important characteristics of a cloud system is elasticity in resources provisioning. Cloud fabric often composes of massive and heterogeneous types of resources allowing the sciences and engineering applications in many domains to collaboratively utilize the infrastructure. As the cloud systems are designed for a large number of users, a large volume of data, and various types of applications, efficient task management is needed for cloud data analytics. One of the popular methods used in task management is to represent a set of tasks with a workflow diagram, which can capture task decomposition, communication between subtasks, and cost of computation and communication. In this paper, we proposed a workflow scheduling framework that can efficiently schedule series workflows with multiple objectives onto a cloud system. Our designed framework uses a meta-heuristics method, called Artificial Bee Colony (ABC), to create an optimized scheduling plan. The framework allows multiple constraints and objectives to be set. Conflicts among objectives can also be resolved using Pareto-based technique. A series of experiments are then conducted to investigate the performance in comparison to the algorithms often used in cloud scheduling. Results show that our proposed method is able to reduce 57% cost and 50% scheduling time within a similar makespan of HEFT/LOSS for a typical scientific workflow like Chimera-2.",,Electronic:978-1-4673-1921-8; POD:978-1-4673-1920-1,10.1109/JCSSE.2012.6261985,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6261985,Artificial Bee Colony;cloud computing;multiple-objective optimization;workflow scheduling,Algorithm design and analysis;Equations;Genetic algorithms;Optimization;Processor scheduling;Schedules;Scheduling,Pareto optimisation;cloud computing;data analysis;resource allocation;scheduling;workflow management software,ABC;Chimera-2;HEFT/LOSS makespan;Pareto-based technique;artificial bee colony;cloud data analytics;cloud fabric;cloud scheduling;computation cost reduction;heterogeneous resources;meta-heuristics method;multiple-objective workflow scheduling framework;resource provisioning process;scheduling plan optimization;scheduling time reduction;scientific workflow;subtask communication cost reduction;task decomposition;task management;workflow diagram,,6,,22,,no,May 30 2012-June 1 2012,,IEEE,IEEE Conference Publications
A New Axes Re-ordering Method in Parallel Coordinates Visualization,L. F. Lu; M. L. Huang; T. H. Huang,"Sch. of Software, Univ. of Technol., Sydney, NSW, Australia",2012 11th International Conference on Machine Learning and Applications,20130110,2012,2,,252,257,"Visualization and interaction of multidimensional data always requires optimized solutions to integrate the display, exploration and analytical reasoning of data into one visual pipeline for human-centered data analysis and interpretation. Parallel coordinate, as one of the popular multidimensional data visualization techniques, is suffered from the visual clutter problem. Though changing the ordering of axis is a straightforward way to address it, optimizing the order of axis is a NP-complete problem. In this paper, we propose a new axes re-ordering method in parallel coordinates visualization: a similarity-based method, which is based on the combination of Nonlinear Correlation Coefficient (NCC) and Singular Value Decomposition (SVD) algorithms. By using this approach, the first remarkable axe can be selected based on mathematics theory and all axis are re-ordered in line with the degree of similarities among them. Meanwhile, we would also propose a measurement of contribution rate of each dimension to reveal the property hidden in the dataset. At last, case studies demonstrate the rationale and effectiveness of our approaches: NCC reordering method can enlarge the mean crossing angles and reduce the amount of polylines between the neighboring axes. It can reduce the computational complexity greatly in comparison with other re-ordering methods.",,Electronic:978-0-7695-4913-2; POD:978-1-4673-4651-1,10.1109/ICMLA.2012.148,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6406759,Multidimensional data visualization;axes re-ordering;nonlinear correlation coefficient;parallel coordinates;singular value decomposition;visual analytics,Acceleration;Clutter;Correlation;Data visualization;Singular value decomposition;Visualization,computational complexity;data analysis;data visualisation;inference mechanisms;optimisation;singular value decomposition,NCC reordering method;NP-complete problem;SVD algorithms;analytical reasoning;axes reordering method;computational complexity;contribution rate measurement;data display;data exploration;human-centered data analysis;human-centered data interpretation;mathematics theory;multidimensional data interaction;multidimensional data visualization technique;nonlinear correlation coefficient;parallel coordinates visualization;polyline reduction;similarity-based method;singular value decomposition;visual clutter problem;visual pipeline,,4,,30,,no,12-15 Dec. 2012,,IEEE,IEEE Conference Publications
A new soft-computing based framework for project management using game theory,T. Bakshi; B. Sarkar; S. K. Sanyal,"Jadavpur University, Kolkata, India","2012 International Conference on Communications, Devices and Intelligent Systems (CODIS)",20130131,2012,,,282,285,"Software project success or failure depends on the ineffective software project management. Success or failure of any project can be attributed incorrect handling of one or more project variables: people, proper technology, proper project scheduling and selection. Among these attributes proper project selection is one of the most vital part of software project management. There exist many uncertainties in project management and current software engineering techniques are unable to eliminate them. So there is huge scope for developing. The current researchers have developed a unique model which is capable to take decision on the field of software project selection. This model has two embedded sub models namely fuzzy AHP(Analytic Hierarchy Process) and strategic game model. Here in the first case experts opinions are considered under fuzzy environment and in the second case, different decisions makers act as players in the game module. Different criteria are taken into consideration for choosing optimal strategy of the players. An elaborated case study is also analyzed for testing the output of the system.",,CD-ROM:978-1-4673-4698-6; Electronic:978-1-4673-4700-6; POD:978-1-4673-4699-3,10.1109/CODIS.2012.6422193,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6422193,Software project management;Strategic game;fuzzy AHP;optimal strategy;project selection,Decision support systems;Intelligent systems,analytic hierarchy process;fuzzy set theory;game theory;software development management,analytic hierarchy process;fuzzy AHP;fuzzy environment;game theory;ineffective software project management;project scheduling;project selection;project variables;soft-computing based framework;software engineering techniques;strategic game model,,0,,15,,no,28-29 Dec. 2012,,IEEE,IEEE Conference Publications
A novel approach to identify problematic call center conversations,M. Abhishek Pandharipande; S. K. Kopparapu,"TCS Innovation Labs - Mumbai, Tata Consultancy Services Limited, Yantra Park, Thane (West), India - 400601",2012 Ninth International Conference on Computer Science and Software Engineering (JCSSE),20120809,2012,,,1,5,"Voice based call centers enable customers to query for information by speaking to agents in the call center. Most often these call conversations are recorded for analysis with the intent of trying to identify things that can help improve the performance of the call center to serve the customer better. Today the recorded conversations are analyzed by humans by listening to call conversations, which is both time consuming, fatigue prone and not accurate. Additionally, humans are able to analyze only a small percentage of the total calls because of economics. In this paper, we propose a visual method to identify problem calls quickly. The idea is to sieve through all the calls and identify problem calls, these calls can then be further analyzed by human. We first model call conversations as a directed graph and then identify a structure associated with a normal call. All call conversations that do not have the structure of a normal call are then classified as being abnormal. In this paper, we use the speaking rate feature to model call conversation because it makes it easy to spot potential problem calls. We have experimented on real call center conversations acquired from a call center and the results are encouraging.",,Electronic:978-1-4673-1921-8; POD:978-1-4673-1920-1,10.1109/JCSSE.2012.6261805,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6261805,Call Conversations;Modeling;Speaking Rate;Speech Analytics;Visual analysis of Speech,Accuracy;Authentication;Business;Humans;Principal component analysis;Speech;Speech recognition,call centres;directed graphs;pattern classification;speech processing,abnormal calls;directed graph;information query;normal calls;performance improvement;problematic call center conversation identification;speaking rate feature;visual method;voice-based call centers,,1,,12,,no,May 30 2012-June 1 2012,,IEEE,IEEE Conference Publications
A novel game theoretic algorithm for project selection under fuzziness,T. Bakshi; S. K. Sanyal; B. Sarkar,"Dept. of Production Engineering, Jadavpur University, Kolkata, India",2012 4th International Conference on Intelligent Human Computer Interaction (IHCI),20130321,2012,,,1,6,"Software project success or failure depends on the ineffective software project management. Success or failure of any project can be attributed incorrect handling of one or more project variables: people, proper technology, proper project scheduling and selection. Among these attributes proper project selection is one of the most vital part of software project management. There exist many uncertainties in project management and current software engineering techniques are unable to eliminate them. So there is huge scope for developing. The current researchers have developed a unique model which is capable to take decision on the field of software project selection. This model has two embedded sub models namely fuzzy AHP (Analytic Hierarchy Process) and strategic game model. Here in the first case experts opinions are considered under fuzzy environment and in the second case, different decisions makers act as players in the game module. Different criteria are taken into consideration for choosing optimal strategy of the players. An elaborated case study is also analyzed for testing the output of the system.",,Electronic:978-1-4673-4369-5; POD:978-1-4673-4367-1,10.1109/IHCI.2012.6481846,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6481846,Software project management;Strategic game;fuzzy AHP;optimal strategy;project selection,Decision support systems;Game theory;Games;Organizations;Project management;Software,analytic hierarchy process;fuzzy set theory;game theory;project management;software engineering;software management,analytic hierarchy process;fuzziness;fuzzy AHP;fuzzy environment;game module;game theoretic algorithm;player optimal strategy;project scheduling;project variable;software engineering technique;software project failure;software project management;software project selection;software project success;strategic game model,,0,,26,,no,27-29 Dec. 2012,,IEEE,IEEE Conference Publications
A Pessimistic Approach for Solving a Multi-criteria Decision Making,N. V. Hieu; L. V. Utkin; D. D. Thang,"Dept. of Software Eng., Da nang Univ. of Technol., Danang, Vietnam",2012 Fourth International Conference on Knowledge and Systems Engineering,20120913,2012,,,121,127,"An extension of the DS/AHP method in the paper. The extension assumes that expert judgments concerning the criteria are often imprecise and incomplete. The proposed extension also uses groups of experts or decision makers for comparing decision alternatives and criteria. However, it does not require assigning favorable values for different groups of decision alternatives and criteria. The computation procedure for processing and aggregating the incomplete information about criteria and decision alternatives is reduced to solving a finite set of linear programming problems. Main results are explained and illustrated by numerical examples.",,Electronic:978-0-7695-4760-2; POD:978-1-4673-2171-6,10.1109/KSE.2012.12,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6299408,,Educational institutions;Finite element methods;Linear programming;Upper bound;Vectors,decision making;inference mechanisms;linear programming,DS-AHP method;Dempster-Shafer theory;analytic hierarchy process;linear programming problem;multicriteria decision making;pessimistic approach,,0,,9,,no,17-19 Aug. 2012,,IEEE,IEEE Conference Publications
A Ray Tracing Simulation of Sound Diffraction Based on the Analytic Secondary Source Model,M. Okada; T. Onoye; W. Kobayashi,"Graduate School of Information Science and Technology, Osaka University, Suita, Osaka, Japan","IEEE Transactions on Audio, Speech, and Language Processing",20120815,2012,20,9,2448,2460,"This paper describes a novel ray tracing method for solving sound diffraction problems. This method is a Monte Carlo solution to the multiple integration in the analytic secondary source model of edge diffraction; it uses ray tracing to calculate sample values of the integrand. The similarity between our method and general ray tracing makes it possible to utilize the various approaches developed for ray tracing. Our implementation employs the OptiX ray tracing engine, which exhibits good acceleration performance on a graphics processor. Two importance sampling methods are derived from different aspects, and they provide an efficient and accurate way to solve the numerically challenging integration. The accuracy of our method was demonstrated by comparing its estimates with the ones calculated by reference software. An analysis of signal-to-noise ratios using an auditory filter bank was performed objectively and subjectively in order to evaluate the error characteristics and perceptual quality. The applicability of our method was evaluated with a prototype system of interactive ray tracing.",1558-7916;15587916,,10.1109/TASL.2012.2203809,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6214586,Acoustic simulation;Monte Carlo method;analytic secondary source model;edge diffraction;importance sampling;ray tracing;uniform theory of diffraction,Analytical models;Diffraction;Monte Carlo methods;Ray tracing,,,,4,,46,,no,Nov. 2012,,IEEE,IEEE Journals & Magazines
A study on channel potential for lightly-doped gate-all-around 6H-SiC nanowire FETs,Ru Han; Man Zhang,"School of Computer Science and Engineering, Northwestern Polytechnical University, Xi'an, China",2012 IEEE International Conference on Electron Devices and Solid State Circuit (EDSSC),20130321,2012,,,1,2,"A continuous and analytic channel potential model for lightly doped gate-all-around (GAA) 6H-SiC nanowire (NW) FETs is developed incorporating the influence of incomplete dopant ionization. By solving the 1D Poisson's equation, and using Lambert-W function, the channel potential and the inversion charge are adequately described from the sub-treshold to the strong inversion.",,Electronic:978-1-4673-5696-1; POD:978-1-4673-5694-7,10.1109/EDSSC.2012.6482888,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6482888,6H-SiC;channel potential;gate-all-around (GAA);lightly doped;nanowire FET,Analytical models;Field effect transistors;Logic gates;Mathematical model;Semiconductor device modeling;Silicon carbide;Temperature,Poisson equation;insulated gate field effect transistors;nanowires;semiconductor device models;semiconductor doping;silicon compounds;wide band gap semiconductors,1D Poisson equation;GAAFET;Lambert-W function;SiC;analytic channel potential model;incomplete dopant ionization;inversion charge;lightly doped gate all around nanowire FET,,0,,4,,no,3-5 Dec. 2012,,IEEE,IEEE Conference Publications
A Survey and Empirical Study of Employee Satisfaction in Growing Small and Micro Businesses,O. Y. Z. An; T. H. Fang,"Econ. & Manage. Dept., Hunan Women's Univ., Changsha, China",2012 Second International Conference on Business Computing and Global Informatization,20121220,2012,,,34,38,"This paper, on the basis of the previous studies and the features of the growing small and micro businesses, presents the evaluation index system of employee satisfaction in these types of companies. It uses an anonymous questionnaire survey approach to survey the enterprise employee satisfaction, and adopts the AHP-Fuzzy method to get statistical analysis on samples, drawing an integrated staff satisfaction of 3.19 points in the growing small and micro businesses, namely"" almost satisfaction"", in which the"" work group satisfaction"" ranks the highest score (4.016 points), and the ""job reward satisfaction"" the lowest (2.571 points). In addition, using the same method to classify and calculate the employee satisfaction, it finds that in the employee age category, staff satisfaction of 30-year-old or over is the highest (3.451), In the light of the working seniority in the enterprise, namely the length of service category, the satisfaction of employees working 2-3 years in the company is the lowest(2.501), according to the staff qualifications category, the satisfaction of employees with lower College education is the lowest (2.932), in family status category, unmarried staff satisfaction is the lowest (2.941), according to the position in hierarchy category, general staff satisfaction is the lowest (2.901 points).",2378-8941;23788941,Electronic:978-0-7695-4854-8; POD:978-1-4673-4469-2,10.1109/BCGIN.2012.16,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6382457,Analytic hierarchy process;Employee satisfaction;Fuzzy comprehensive evaluation;small and micro businesses,Companies;Educational institutions;Indexes;Software reliability,,,,0,,4,,no,12-14 Oct. 2012,,IEEE,IEEE Conference Publications
A Sustainable Building Application Design Based on the mOSAIC API and Platform,V. Stankovski; M. KÌ_nig,"Dept. of Constr. Inf., Univ. of Ljubljana, Ljubljana, Slovenia","2012 Eighth International Conference on Semantics, Knowledge and Grids",20121224,2012,,,249,252,"Building sustanability is an emerging area of research related to the use of renewable energy sources and energy efficiency throughout the buildings lifecycle. For this to happen, it is necessary to collect and analyse an increasing amounts of data from various sources including sensor data. As a result, energy production and use could be optimised based on a number of sustainability criteria. In the present study, the mOSAIC Cloud computing platform and API is used to design an application that would make it possible to store and analyse data about the building stock in Slovenia for various research and commercial purposes. Key parts of the design are: (1) an OWL/RDF knowledge base of high-level concepts describing sustainability aspects of a building including sensor data, CO2 consumption, energy use, and other sustainability parameters, and (2) the mOSAIC platform that takes care about the lower level details related to the application bottlenecks, such as elasticity needed when dealing with the sensor data. In order to connect these two distinct parts an URI referencing scheme is used. At runtime, when running analytic services, various decisions are possible to optimise the data and computationally intensive part of the application.",,Electronic:978-0-4695-4794-0; POD:978-1-4673-2561-5,10.1109/SKG.2012.13,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6391844,OWL/RDF;building sustainability;hybrid cloud;knowledge base,Buildings;Cloud computing;Computational modeling;Knowledge based systems;OWL;Resource description framework;Semantics,application program interfaces;building management systems;cloud computing;data analysis;energy conservation;knowledge representation languages;multi-agent systems;renewable energy sources;sensor fusion;software agents;sustainable development,CO<sub>2</sub> consumption;OWL/RDF knowledge base;Slovenian building stock;URI referencing scheme;building sustainability;buildings lifecycle;data analysis;data collection;energy efficiency;energy production;energy use;mOSAIC API;mOSAIC cloud computing platform;renewable energy sources;sensor data;sustainability criteria;sustainability parameters;sustainable building application design,,1,,19,,no,22-24 Oct. 2012,,IEEE,IEEE Conference Publications
A Trust Model in P2P E-Commerce Systems,Z. Yu; G. c. Shen; H. y. Liu,"Sch. of Inf., Beijing Wuzi Univ., Beijing, China","2012 8th International Conference on Wireless Communications, Networking and Mobile Computing",20130314,2012,,,1,4,"In Peer-to-Peer (P2P) e-commerce systems, peers' features such as heterogeneity, anonymity and autonomy lead to some security problems, such as forging, slandering and collective cheating, which affect the quality of service a lot. A trust model in P2P e-commerce systems based on the recommendation is proposed, each peer in the system has a unique credibility of recommendation, two trust parameters for updating the credibility of recommendation are introduced, namely updating range and updating strength. The trust model proposes an algorithm to update the credibility of recommendation; a peer selects recommendation peers whose evaluation criteria are similar, evaluation criteria of peers are determined through the AHP method(Analytic Hierarchy Process). Simulations show that, the trust model can identify malicious peers, and improve the quality of service in P2P e-commerce systems effectively.",2161-9646;21619646,Electronic:978-1-61284-683-5; POD:978-1-61284-684-2,10.1109/WiCOM.2012.6478380,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6478380,,Analytic hierarchy process;Computational modeling;Educational institutions;Peer-to-peer computing;Quality of service;Security;Software,,,,0,,21,,no,21-23 Sept. 2012,,IEEE,IEEE Conference Publications
A users guide to arc resistant low voltage switchgear & motor control ‰ÛÓ Analytical comparison vs arc flash test results,G. Arce; C. McCollum; J. Jennings; B. Radibratovic,"ABB, San Luis Potosi, Mexico",2012 IEEE-IAS/PCA 54th Cement Industry Technical Conference,20120614,2012,,,1,11,"This paper describes enhanced safety and maintenance features required within LV Motor Control Centers and Switchgear. Enhanced capabilities will include discussion of using NFPA 70E [1], ANSI C37.20.7 [2] and IEEE 1584 [3] in the development of low voltage arc resistance equipment. Highlights will include discussion of arc flash danger to personnel and the solutions used in the development of arc resistant low voltage switchgear and motor control centers. Arc flash software modeling shall be compared with actual testing of equipment per ANSI C37.20.7.",1079-9931;10799931,Electronic:978-1-4673-0286-9; POD:978-1-4673-0284-5,10.1109/CITCON.2012.6215701,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6215701,,,IEEE standards;arcs (electric);electrical safety;occupational safety;switchgear,ANSI C37.20.7;IEEE 1584;NFPA 70E;arc flash software modeling;arc flash test results;arc resistant low voltage switchgear;motor control centers,,0,,10,,no,14-17 May 2012,,IEEE,IEEE Conference Publications
A Work-Centered Visual Analytics Model to Support Engineering Design with Interactive Visualization and Data-Mining,X. Yan; M. Qiao; J. Li; T. W. Simpson; G. M. Stump; X. Zhang,,2012 45th Hawaii International Conference on System Sciences,20120209,2012,,,1845,1854,"To support the knowledge discovery and decision making from large-scale, multi-dimensional, continuous data sets, novel systems of visual analytics need the capability to identify hidden patterns in data that are critical for in-depth analysis. In this paper, we present a work-centered approach to support visual analytics of complex data sets by combining user-centered interactive visualization and data-oriented computational algorithms. We design and implement a specific system prototype, Learning-based Interactive Visualization for Engineering design (LIVE), for engineering designers to handle overwhelming information such as numerous design alternatives generated from automatic simulating software. During the exploration within a ""trade space"" consisting of possible designs and potential solutions, engineering designers want to analyze the data, discover hidden patterns, and identify preferable solutions. The proposed system allows designers to interactively examine large design data sets through visualization and interactively construct data models from automatic data mining algorithms. We expect that our approach can help designers efficiently and effectively make sense of large-scale design data sets and generate decisions. We also report a preliminary evaluation on our system by analyzing a real engineering design problem related to aircraft wing sizing.",1530-1605;15301605,Electronic:978-1-5090-5638-5; POD:978-1-4577-1925-7; USB:978-0-7695-4525-7,10.1109/HICSS.2012.87,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6149110,,Algorithm design and analysis;Clustering algorithms;Computational modeling;Data mining;Data models;Data visualization;Visual analytics,aerospace components;aerospace computing;data analysis;data mining;data visualisation;decision making;design engineering;interactive systems,aircraft wing sizing;automatic simulating software;data mining;data oriented computational algorithms;decision making;design data sets;knowledge discovery;large scale multidimensional continuous data sets;learning based interactive visualization for engineering design;user centered interactive visualization;work centered visual analytics model,,0,1,23,,no,4-7 Jan. 2012,,IEEE,IEEE Conference Publications
"Advancing process modeling, simulation, and analytics in practice",S. M. Sutton,"IBM T. J. Watson Research Center, P.O. Box 218, Yorktown Heights, N.Y. 10598 USA",2012 International Conference on Software and System Process (ICSSP),20120628,2012,,,221,222,"If you take a broad view, there are many ways ahead for software process modeling and simulation. One way to broaden the view is to include not just software processes but also systems and service processes. Another is to address not just technical processes but also business processes. In any scope, impact can be strengthened by combining modeling and simulation with analytics. A way to assure longevity of the field is to solve problems in practice. These afford a variety of ways to make contributions, including both applied and fundamental. Challenges that must be overcome in achieving practical results are getting access to practitioners, acquiring useful data, and getting your solution adopted (among others). More empirical studies and experience reports are needed. But there are many routes by which contributions may flow to and from the field.",,Electronic:978-1-4673-2352-9; POD:978-1-4673-2351-2; USB:978-1-4673-2350-5,10.1109/ICSSP.2012.6225971,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6225971,analytics;modeling;process;simulation,Analytical models;Business;Communities;Context modeling;Portfolios;Programming;Software,software development management,business processes;software process modeling;software simulation;technical processes,,0,,,,no,2-3 June 2012,,IEEE,IEEE Conference Publications
An adaptive parameter space-filling algorithm for highly interactive cluster exploration,Z. Ahmed; C. Weaver,"Sch. of Comput. Sci. &amp; Center for Spatial Anal., Univ. of Oklahoma, Norman, OK, USA",2012 IEEE Conference on Visual Analytics Science and Technology (VAST),20130103,2012,,,13,22,"For a user to perceive continuous interactive response time in a visualization tool, the rule of thumb is that it must process, deliver, and display rendered results for any given interaction in under 100 milliseconds. In many visualization systems, successive interactions trigger independent queries and caching of results. Consequently, computationally expensive queries like multidimensional clustering cannot keep up with rapid sequences of interactions, precluding visual benefits such as motion parallax. In this paper, we describe a heuristic prefetching technique to improve the interactive response time of KMeans clustering in dynamic query visualizations of multidimensional data. We address the tradeoff between high interaction and intense query computation by observing how related interactions on overlapping data subsets produce similar clustering results, and characterizing these similarities within a parameter space of interaction. We focus on the two-dimensional parameter space defined by the minimum and maximum values of a time range manipulated by dragging and stretching a one-dimensional filtering lens over a plot of time series data. Using calculation of nearest neighbors of interaction points in parameter space, we reuse partial query results from prior interaction sequences to calculate both an immediate best-effort clustering result and to schedule calculation of an exact result. The method adapts to user interaction patterns in the parameter space by reprioritizing the interaction neighbors of visited points in the parameter space. A performance study on Mesonet meteorological data demonstrates that the method is a significant improvement over the baseline scheme in which interaction triggers on-demand, exact-range clustering with LRU caching. We also present initial evidence that approximate, temporary clustering results are sufficiently accurate (compared to exact results) to convey useful cluster structure during rapid and protracted interaction.",,Electronic:978-1-4673-4753-2; POD:978-1-4673-4752-5,10.1109/VAST.2012.6400493,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6400493,D.2.2 [Software Engineering]: Design Tools and Techniques ‰ÛÓ [User Interfaces];H.2.3 [Information Systems]: Database Management ‰ÛÓ [Languages];H.5.2 [Information Systems]: Information Interfaces and Presentation ‰ÛÓ [User Interfaces],Algorithm design and analysis;Clustering algorithms;Data visualization;Lenses;Prefetching;Time series analysis;Visualization,cache storage;data visualisation;human computer interaction;pattern clustering;query processing;storage management,K means clustering;LRU caching;Mesonet meteorological data;adaptive parameter space-filling algorithm;best-effort clustering;continuous interactive response time;dynamic query visualization;exact-range clustering;heuristic prefetching technique;interactive cluster exploration;motion parallax;multidimensional clustering;multidimensional data;nearest neighbor calculation;one-dimensional filtering lens;partial query;query computation;time series data;two-dimensional parameter space;user interaction pattern;visual benefit;visualization system;visualization tool,,2,,31,,no,14-19 Oct. 2012,,IEEE,IEEE Conference Publications
An analytic approach for estimation of maximum power point in solar cars,N. Haghdadi; G. Farivar; H. Iman-Eini; F. Miragha,"School of Electrical Engineering, College of Engineering, University of Tehran, Iran",20th Iranian Conference on Electrical Engineering (ICEE2012),20120903,2012,,,575,580,"In this paper a new maximum power point tracker (MPPT) for a moving solar module is proposed. In these cases, multiple peaks are introduced in the PV array curves (due to the shadow effect) and it is difficult to find and track the new MPP, in order to extract maximum PV power. Hence, a simple analytical MPPT approach is introduced which is easy to implement and could overcome the mentioned problem. The proposed method is also fast enough to accurately track the MPP in different environmental circumstances. The feasibility and effectiveness of the proposed method is confirmed using modeling in MATLAB environment.",2164-7054;21647054,Electronic:978-1-4673-1148-9; POD:978-1-4673-1149-6,10.1109/IranianCEE.2012.6292422,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6292422,MPPT;Partial Shading Effect;Photovoltaic;Solar Car,Analytical models;Current measurement;Power capacitors;Q measurement;Software packages;Vehicles,automobiles;estimation theory;maximum power point trackers;modules;solar powered vehicles,MATLAB environment;PV array curve;environmental circumstance;maximum PV power extraction;maximum power point tracker estimation;simple analytical MPPT approach;solar car;solar module,,0,,21,,no,15-17 May 2012,,IEEE,IEEE Conference Publications
An approach to aging assessment of power transformer based on multi-parameters,Y. Li; M. j. Tang; J. b. Deng; G. j. Zhang; S. h. Wang,"State Key Lab of Electrical Insulation and Power Equipment, Xi'an Jiaotong University, Xi'an, 710049, China",2012 IEEE International Conference on Condition Monitoring and Diagnosis,20130124,2012,,,357,360,"As the population of transformers in service increases and becomes older, more attentions are focused on their availability and reliability. Through decades' progress, it is undeniable that considerable advances have been achieved on the techniques of fault diagnosis and condition assessment for oil-immersed transformer, for its great importance in the whole transmission and distribution (T&D) system. However, still insufficient research work focused on how to evaluate transformer's aging condition. This paper represents Aging Index (AI) which is a practical tool that combines the results of routine inspections, and site and laboratory testing to estimate the aging condition. This Aging Index calculation considers not only typical test results such as dissolved gas analysis (DGA), oil quality, furan, and dissipation factor, but also other parameters such as top oil temperature and hot spot temperature estimating. And further, Frequency Domain Spectroscopy (FDS) is also introduced as a novel non-destructive testing technique to estimate remained life expectancy. Moreover, a software system based on transformer's electrical and thermal parameters is developed correspondingly, by using a multi-parameters analytic approach. This system is expected to help to decision-making and replacement planning and to have good application prospects.",,Electronic:978-1-4673-1020-8; POD:978-1-4673-1019-2,10.1109/CMD.2012.6416453,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6416453,aging Index;aging assessment;frequency domain spectroscopy;hot spot temperature;life expectancy;power transformer;temperature,Aging;Oil insulation;Power transformer insulation;Temperature measurement;Testing,ageing;condition monitoring;decision making;fault diagnosis;nondestructive testing;power transformer testing;reliability,aging assessment;aging index;condition assessment;decision-making;dissipation factor;dissolved gas analysis;distribution system;electrical parameters;fault diagnosis;frequency domain spectroscopy;furan;hot spot temperature;laboratory testing;multiparameters analytic approach;nondestructive testing;oil quality;oil-immersed transformer;power transformer;reliability;replacement planning;routine inspections;site testing;thermal parameters;top oil temperature;transmission system,,1,,7,,no,23-27 Sept. 2012,,IEEE,IEEE Conference Publications
"An empirical research on relationship between demand, people and awareness towards training needs: A case study in Malaysia Halal logistics industry",K. M. B. Pahim; S. Jemali; S. J. A. N. S. Mohamad,"Malaysia Institute of Transport (MITRANS), Shah Alam Malaysia","2012 IEEE Business, Engineering & Industrial Applications Colloquium (BEIAC)",20120628,2012,,,246,251,"Recently, the Halal industry has a huge potential for market demand. The rising of Halal has made the consumers to think twice before using any product because it is proven that Halal product can offer good quality, cleanliness, hygienic, safety, authentic and nutritious. Halal has created awareness to the consumers and supplier to use or supply the good according to Halalan Thoyyibban principle. Many logistics companies in Malaysia have adapted Halal in their operation. However, the lack of employees, skills and experience become the major problem to operate Halal in their operations. There are not enough skills and experience workers who can handle the consignment according to the Halal procedure. Hence, training in the Halal logistics industry professional is crucial as the lack of professionalism in the transport and logistics process in the supply chain may cast doubts on the Halal status of a product. The purpose of this research is to present the relationship between the training needs in Halal logistics industry in Malaysia and investigates its relationship with demand, people and awareness. The data collection instruments used was a questionnaire which was administrated to a total sample of 162 respondents from the middle management level in the logistics companies who have the Halal certification by Jabatan Kemajuan Islam Malaysia (JAKIM) or Halal Industry Corporation (HDC). Sample selection was based on purposive sampling. The analysis involved statistical methods using Predictive Analytics Software (PASW) 18.0 such as reliability and validity test and multiple regressions. The result indicated that training needs in Halal logistics industry are related to demand, people and awareness which to explain the significant influence to the Halal logistics industry.",,Electronic:978-1-4673-0426-9; POD:978-1-4673-0425-2,10.1109/BEIAC.2012.6226062,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6226062,3PL;Halal Logistics;Logistics Training;Training Needs and Halal Industries,Companies;Industries;Reliability;Supply chains;Training,food processing industry;logistics,Halal certification;Halal industry corporation;Halal procedure;Halal product;Halal status;Halalan Thoyyibban principle;Jabatan Kemajuan Islam Malaysia;Malaysia Halal logistics industry professional;cleanliness;hygienic;logistics company;logistics process;market demand;middle management level;multiple regression;predictive analytics software;professionalism;purposive sampling;reliability;safety;statistical method;supply chain;validity test,,1,,35,,no,7-8 April 2012,,IEEE,IEEE Conference Publications
An Empirical Study of Bugs in Machine Learning Systems,F. Thung; S. Wang; D. Lo; L. Jiang,"Sch. of Inf. Syst., Singapore Manage. Univ., Singapore, Singapore",2012 IEEE 23rd International Symposium on Software Reliability Engineering,20130404,2012,,,271,280,"Many machine learning systems that include various data mining, information retrieval, and natural language processing code and libraries are used in real world applications. Search engines, internet advertising systems, product recommendation systems are sample users of these algorithm-intensive code and libraries. Machine learning code and toolkits have also been used in many recent studies on software mining and analytics that aim to automate various software engineering tasks. With the increasing number of important applications of machine learning systems, the reliability of such systems is also becoming increasingly important. A necessary step for ensuring reliability of such systems is to understand the features and characteristics of bugs occurred in the systems. A number of studies have investigated bugs and fixes in various software systems, but none focuses on machine learning systems. Machine learning systems are unique due to their algorithm-intensive nature and applications to potentially large-scale data, and thus deserve a special consideration. In this study, we fill the research gap by performing an empirical study on the bugs in machine learning systems. We analyze three systems, Apache Mahout, Lucene, and OpenNLP, which are data mining, information retrieval, and natural language processing tools respectively. We look into their bug databases and code repositories, analyze a sample set of bugs and corresponding fixes, and label the bugs into various categories. Our study finds that 22.6% of the bugs belong to the algorithm/method category, 15.6% of the bugs belong to the non-functional category, and 13% of the bugs belong to the assignment/initialization category. We also report the relationship between bug categories and bug severities, the time and effort needed to fix the bugs, and bug impacts. We highlight several bug categories that deserve attention in future research.",1071-9458;10719458,Electronic:978-0-7695-4888-3; POD:978-1-4673-4638-2,10.1109/ISSRE.2012.22,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405375,,Software reliability,data mining;information retrieval;learning (artificial intelligence);natural language processing;program debugging;software reliability,Apache Mahout;Internet advertising systems;Lucene;OpenNLP;algorithm-intensive code;algorithm-intensive nature;bug categories;bug databases;code repositories;data mining;information retrieval;machine learning systems;natural language processing code;recommendation systems;search engines;software engineering tasks;software mining;system reliability,,8,,42,,no,27-30 Nov. 2012,,IEEE,IEEE Conference Publications
An Evaluation Model in Software Testing Based on AHP,Z. Juan; T. Weiqin; C. Lizhi,"Sch. of Comput. Eng. & Sci., Shanghai Univ., Shanghai, China",2012 IEEE/ACIS 11th International Conference on Computer and Information Science,20120607,2012,,,601,604,"In this paper, a novel software testing evaluation model is proposed for specification based software testing. The proposed model uses Analytic Hierarchy Process (AHP) to analyze the weight of influence each individual function unit software, then the model classifies the defect ratio of each function unit based on the weight of influence derived from step one and calculates the final evaluation result of software testing according to the defect ration and influence degree.",,Electronic:978-0-7695-4694-0; POD:978-1-4673-1536-4,10.1109/ICIS.2012.31,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6211159,AHP;evaluatin model;function point;functional unit;software testing,Analytical models;Computational modeling;Marketing and sales;Software quality;Software testing,decision making;formal specification;program testing,AHP;analytic hierarchy process;defect ratio classification;function unit software;software testing evaluation model;specification based software testing,,1,,12,,no,May 30 2012-June 1 2012,,IEEE,IEEE Conference Publications
An improved analytic expression for write amplification in NAND flash,X. Luojie; B. M. Kurkoski,"School of Information and Software Engineering, University of Electronic Science and Technology of China, Chengdu, China","2012 International Conference on Computing, Networking and Communications (ICNC)",20120312,2012,,,497,501,"Agarwal et al. gave a closed-form expression for write amplification in NAND flash memory by finding the probability of a page being valid over the whole flash memory. This paper gives an improved analytic expression for write amplification in NAND flash memory by finding the probability of a page being invalid in the block selected for garbage collection. The improved expression uses the Lambert W function. Through asymptotic analysis, write amplification is shown to depend on the overprovisioning factor only, consistent with the previous work. Comparison with numerical simulations shows that the improved expression achieves a more accurate prediction of write amplification. For example, when the overprovisioning factor is 0.3, the improved expression gives a write amplification of 2.36 whereas that of the previous work gives 2.17, when the actual value is 2.35.",,Electronic:978-1-4673-0009-4; POD:978-1-4673-0008-7; USB:978-1-4673-0723-9,10.1109/ICCNC.2012.6167472,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6167472,,Analytical models;Ash;Closed-form solutions;Equations;Flash memory;Mathematical model;Stationary state,NAND circuits;amplification;flash memories;probability,Lambert W function;NAND flash memory;asymptotic analysis;garbage collection;write amplification,,4,,9,,no,Jan. 30 2012-Feb. 2 2012,,IEEE,IEEE Conference Publications
An intelligent approach to CAM software selection,Z. Ayag,"Kadir Has University, Kadir Has Campus, Cibali, Fatih, ISTANBUL 34083 Turkey",CCCA12,20130124,2012,,,1,6,"In this paper, an intelligent approach is presented to help the companies select most suitable CAM software for their current and future needs. For this purpose, fuzzy AHP method is used to carry out CAM selection process more effectively, easily and applicable for a company. Shortly, the objectives of the research are; to define a step-by-step approach for an effective CAM software selection. The proposed approach is also illustrated on a case study.",,Electronic:978-1-4673-4695-5; POD:978-1-4673-4694-8,10.1109/CCCA.2012.6417855,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6417855,,Analytic hierarchy process;Companies;Computer aided manufacturing;Indexes;Manufacturing;Software;Vectors,CAD/CAM;analytic hierarchy process;fuzzy set theory;organisational aspects;production engineering computing,CAM software selection;fuzzy AHP method;intelligent approach,,0,,35,,no,6-8 Dec. 2012,,IEEE,IEEE Conference Publications
An optical CDMA method for in-vehicle information service,Chih-Ta Yen; Wen-Bin Chen,"Department of Electrical Engineering, National Formosa University, Yunlin, Taiwan",2012 12th International Conference on ITS Telecommunications,20130131,2012,,,347,351,"This study proposes a new inter-vehicle communication (IVC) system based on optical spectral-amplitude-coding optical code division multiple access (SACOCDMA) systems architecture, then, analysis and calculate the system's performance by using analytic and simulation methods. The important feature of the SAC-OCDMA systems is that multiple access interference (MAI) can be eliminated by code sequences of a fixed in-phase cross-correlation value with balance detection schemes. The advantage of optical CDMA multiplexed method compared with electrical multiplexed method is no electromagnetic interference and security enhanced. The performance of modify prime codes (MPCs) and Walsh-Hadamard codes as signature codes for the SAC-OCDMA systems is analyzed. The simulation results of MPCs in SAC-OCDMA system structures are first presented by commercial simulation obtained using OptiSystem software. The simulation results show that the bit error rate (BER) through use of the MPCs is superior to the conventional Walsh-Hadamard codes, especially when the received effect power is large. The eye diagram also shows that the SAC-OCDMA system with MPCs exhibits a wider opening than the Walsh-Hadamard codes.",,DVD:978-1-4673-3068-8; Electronic:978-1-4673-3070-1; POD:978-1-4673-3071-8,10.1109/ITST.2012.6425195,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6425195,inter-vehicle communication (IVC) system;modify prime codes (MPCs);multiple access interference (MAI);spectral-amplitude-coding optical code division multiple access (SAC-OCDMA),Bit error rate;Magnetic materials;Multiaccess communication;Noise;Optical fibers;Photonic crystals;Receivers,Hadamard codes;Walsh functions;code division multiple access;code division multiplexing;error statistics;interference suppression;spectral analysis,BER;MAI;MPC;OCDMA;OptiSystem software;SAC;Walsh-Hadamard code;balance detection scheme;bit error rate;code sequence;eye diagram;in-phase cross-correlation;intervehicle communication;modify prime code;multiple access interference;optical CDMA method;optical code division multiple access;signature codes;spectral amplitude coding,,1,,10,,no,5-8 Nov. 2012,,IEEE,IEEE Conference Publications
Analysis and prototyping of a low-speed small scale permanent magnet generator for wind power applications,E. Lepa; A. Kilk,"Department of Fundamentals of Electrical Engineering and Electrical Machines, Tallinn University of Technology, Ehitajate tee 5, 19086 Tallinn, Estonia",2012 Electric Power Quality and Supply Reliability,20120802,2012,,,1,6,"Many companies engaged in wind generator development are investigating permanent magnet generators (PMG) mainly because of higher efficiency over a broad range of wind speed and reduced costs of maintenance. One important parameter of PM generators is the primary magnetic flux that has certain influence to no load voltage of the machine. Thus, needs for efficient calculation methods for primary magnetic field is required. Finite element method is one of the most common and efficient way to analyze the magnetic flux of the machine. But on the other hand it is relatively complicated calculation method and it might be incomprehensible. Analytic calculation or calculations based on reluctance network are comprehensive and suitable for engineers-designers. This paper deals with the primary magnetic field analysis and basic winding calculations by an example of 54 slots and 20 poles PM synchronous generator. Modeling of the primary magnetic field distribution by the finite element method in software FEMM is described. Results from the analytic calculations and the finite element method are compared with experimental data of PM prototype generator study.",,Electronic:978-1-4673-1979-9; POD:978-1-4673-1980-5; USB:978-1-4673-1978-2,10.1109/PQ.2012.6256193,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6256193,Primary magnetic field;analytic calculations;permanent magnet;the finite element method,Generators;Magnetic fields;Magnetic flux;Prototypes;Rotors;Stators;Windings,finite element analysis;machine windings;magnetic fields;permanent magnet generators;wind power plants,PM generators;PM synchronous generator;basic winding calculations;finite element method;load voltage;low-speed small scale permanent magnet generator;primary magnetic field distribution;primary magnetic flux;reluctance network;software FEMM;wind generator development;wind power applications;wind speed,,0,,8,,no,11-13 June 2012,,IEEE,IEEE Conference Publications
"Analysis Engine for Automated Health Checks, Early Problem Detection and Advanced Problem Determination",Y. K. Srivastava; A. Abrashkevich,"India Software Lab., IBM India Private Ltd., Bangalore, India",2012 Annual SRII Global Conference,20120924,2012,,,574,581,"With current trends in software industry toward increased complexity of modern software, tight integration of multiple software products, emphasis on software reliability and high-level availability, software support and maintenance costs increase dramatically. It is imperative for businesses to be able to monitor health of their systems making sure that they are performing at top levels, quickly respond to any problems and timely fix them and also be able to perform advanced problem determination to reduce total time for outages that already occurred. Equally important is to prevent problems from occurring based on best practices and knowledge of known problems/issues for specific software products. To achieve these goals, a powerful analysis engine capable of performing comprehensive health checks of customer systems and advanced problem determination based on analysis of customers' data is proposed. It can be used for both proactive and reactive customer support. Such an engine works as a virtual consultant for the end users. It detects potential problems related to customer systems and installed products and provides notifications or alerts proactively, i.e. could be considered as an early detection system. It is also capable of analyzing FFDC (First Failure Data Capture) data after a problem has occurred, comparing the data with well known problems and related symptoms from relevant knowledge databases and providing customers with the results of analysis, found matches of previously recorded problems and recommendations on how to fix the problem at hand. The engine proposed utilizes up to date analytics from subject matter experts and best practices encoded in it. In the present work, a system architecture and design of such an analysis engine is presented. The proposed engine has a low bar of adoption, flexible extensible design and could be easily adopted for any software product. It is able to analyze encoded human knowledge, compare collected customer dat- with available historical data and report problems and issues found along with the relevant recommendations and suggested fixes. More specifically, the engine provides a comprehensive analysis in terms of health checks, best practices compliance check, prerequisites check, end-of-service product check, operating environment and configuration setup check, outage prevention, state comparison, problem determination and others. A case study based on the proposed engine design is presented and discussed in more detail.",2166-0778;21660778,Electronic:978-1-5090-5643-9; POD:978-1-4673-2318-5; USB:978-0-7695-4770-1,10.1109/SRII.2012.70,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6311040,Analysis engine;Automated Health Check;proactive support,Best practices;Computer architecture;Databases;Engines;Maintenance engineering;Servers;Software,DP industry;data analysis;software maintenance;software reliability,FFDC data analysis;advanced problem determination;analysis engine;compliance check;configuration setup check;customer data analysis;customer systems automated health checks;date analytics;early problem detection;end-of-service product check;first failure data capture;high-level availability;operating environment check;outage prevention;prerequisites check;proactive customer support;reactive customer support;software industry;software maintenance costs;software products;software reliability;software support;state comparison;virtual consultant,,0,,9,,no,24-27 July 2012,,IEEE,IEEE Conference Publications
Analytic determination of cogging torque harmonics of brushless permanent magnet machines,G. Bramerdorfer; S. Silber; E. Marth; G. Jungmayr; W. Amrhein,"Institute for Electrical Drives and Power Electronics, Johannes Kepler University, Altenbergerstra&#x00DF;e 69, 4040 Linz, Austria","International Symposium on Power Electronics Power Electronics, Electrical Drives, Automation and Motion",20120813,2012,,,60,65,"In this paper a new analytic method for the determination of possibly arising cogging torque harmonics of brushless permanent magnet machines is presented. Cogging torque is caused by a change of the system's total magnetic energy concerning a rotor revolution for no stator coil(s) excitation. Usually, this change in magnetic energy is very low compared to the torque values for nominal load. The cogging torque calculation using finite element software is very sensitive regarding the quality of the used mesh. Therefore, an analytic consideration of possibly arising cogging torque harmonics is advantageous. Cogging torque harmonics only arising from calculation errors can be determined and the comparison of the calculated and the analytically determined cogging torque spectrum can be used for a qualitative evaluation of the calculation accuracy. With the presented generalized method, it is further even possible to calculate cogging torque harmonics resulting from unbalances, manufacturing errors, etc. Finally, application examples are presented to effectively reduce cogging torque without a decrease of the machine's efficiency by analyzing the cogging torque harmonics depending on the magnetic field harmonics of the permanent magnet excitation.",,Electronic:978-1-4673-1301-8; POD:978-1-4673-1299-8,10.1109/SPEEDAM.2012.6264408,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6264408,Analytic Calculation;Brushless Permanent Magnet Machine;Cogging Torque;Spectral-Field Design,Forging;Harmonic analysis;Magnetic analysis;Permanent magnet machines;Rotors;Stators;Torque,brushless machines;harmonic analysis;mesh generation;permanent magnet machines,brushless permanent magnet machines;cogging torque harmonic analytic determination;cogging torque spectrum;finite element software;magnetic field harmonics;manufacturing errors;permanent magnet excitation;rotor revolution;stator coil excitation;system total magnetic energy,,2,,5,,no,20-22 June 2012,,IEEE,IEEE Conference Publications
Analytic modeling of software coincidence detection in PET,A. Long; P. Xiao; L. Lin; Y. Li; Q. Xie,"Department of Biomedical Engineering, Huazhong University of Science and Technology, Wuhan, Hubei, China",2012 IEEE Nuclear Science Symposium and Medical Imaging Conference Record (NSS/MIC),20130708,2012,,,3093,3096,"PET imaging is based on the detection in coincidence of two back-to-back annihilation photons. Implementation of coincidence detection in PET data acquisition systems usually involve energy discrimination and time discrimination. Depending on which is performed at first, the coincidence detection can be divided into two ways: energy discrimination before time discrimination (ETD) and time discrimination before energy discrimination (TED). The major difference of them is that, certain double coincidences from multiples which will be discarded by TED may be recovered by ETD. Since the extra double coincidences retrieved by ETD contain not only true coincidences but also scatter coincidences and random coincidences, it is hard to tell whether they can improve the final image quality. In this work, an primary analytic model of coincidence detection in PET is proposed, and the noise equivalent count (NEC) of TED and ETD are deduced from the model to estimate the image quality. The validity of the model is evaluated by GATE simulation. The model and simulation results suggested that the ETD has a better sensitivity while suffers from the random coincidences sorted from multiple coincidences. The NEC of TED is superior when the dose is relatively low and/or a narrow coincidence time window is used, while this turns to inverse while dose getting higher and/or a broader coincidence time window is used. This implies that compromises should be made while choosing the coincidence detection method and the choice is rather application dependent.",1082-3654;10823654,DVD:978-1-4673-2029-0; Electronic:978-1-4673-2030-6; POD:978-1-4673-2028-3,10.1109/NSSMIC.2012.6551705,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6551705,Coincidence detection;Energy determination;Positron Emission Tomography (PET);Time determination,,coincidence techniques;data acquisition;positron emission tomography,ETD;GATE simulation;PET;TED;back-to-back annihilation photons;coincidence detection method;data acquisition systems;energy discrimination;energy discrimination before time discrimination;noise equivalent count;software coincidence detection;time discrimination;time discrimination before energy discrimination,,0,,6,,no,Oct. 27 2012-Nov. 3 2012,,IEEE,IEEE Conference Publications
Application DANP with MCDM model to explore smartphone software,C. H. Su; H. L. Tseng; T. Furuzuki; G. H. Tzeng,"Department of Electrical Engineering, Hwa Hsia Institute of Technology, Taipei, Taiwan","The 6th International Conference on Soft Computing and Intelligent Systems, and The 13th International Symposium on Advanced Intelligence Systems",20130422,2012,,,1734,1739,"To understand the behavior of smartphone online application software will be helpful to predict whether the software application would be adopted by the users and to guide the providers to enhance the functions of the software. A wide range of criteria are used to assess smartphone software quality, but most of these criteria have interdependent or interactive characteristics, which can make it difficult to effectively analyze and improve smartphone use intention. The purpose of this study is to address this issue using a hybrid MCDM (multiple criteria decision-making) approach that includes the DEMATEL (decision-making trial and evaluation laboratory), DANP (the DEMATEL-based analytic network process) methods to achieve an optimal solution. By exploring the influential interrelationships between criteria related to mobile communication industry's and related value-added service content providers' reference in the respect of operation. This approach can be used to solve interdependence and feedback problems, allowing for greater satisfaction of the actual needs of mobile communication industries.",,Electronic:978-1-4673-2743-5; POD:978-1-4673-2742-8,10.1109/SCIS-ISIS.2012.6505023,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6505023,DANP (DEMATEL-based analytic network process);MCDM (multiple criteria decision-making);smartphone Application software,,analytic hierarchy process;interactive programming;mobile communication;operations research;smart phones;software quality;telecommunication industry,DANP application;DEMATEL-based analytic network process methods;MCDM model;decision-making trial and evaluation laboratory;feedback problems;hybrid MCDM approach;interactive characteristics;mobile communication industries;multiple criteria decision-making approach;smartphone online application software;smartphone software;smartphone software quality assessment;software functions;value-added service content provider reference,,0,,22,,no,20-24 Nov. 2012,,IEEE,IEEE Conference Publications
Applying Cluster Computing to Enable a Large-scale Smart Grid Stability Monitoring Application,J. Interrante; K. S. Aggour,"Software Sci. & Analytics, GE Global Res., Niskayuna, NY, USA",2012 IEEE 14th International Conference on High Performance Computing and Communication & 2012 IEEE 9th International Conference on Embedded Software and Systems,20121018,2012,,,328,335,"The real-time execution of grid stability monitoring algorithms are critical to enabling a truly smart grid. However, the combination of a high sampling rate for grid monitoring devices, combined with a large number of devices scattered across a grid, result in very high throughput requirements for the execution of these algorithms. Here we define a centralized hardware and software infrastructure to enable the real-time execution of a small signal oscillation detection algorithm using a cluster of commodity nodes. Our research has demonstrated that readings from up to 500 phasor measurement units (PMUs) sampling at 60Hz can be analyzed in real-time by a single 8-core, 2.53GHz machine with 8GB of RAM, and that a cluster of four of these machines can be used to monitor up to 2,000 PMUs in parallel.",,Electronic:978-0-7695-4749-7; POD:978-1-4673-2164-8,10.1109/HPCC.2012.51,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6332191,PMU;Smart Grid;cluster computing;phasor measurement unit;real-time analytics;small signal oscillation detection,Algorithm design and analysis;Engines;Monitoring;Oscillators;Phasor measurement units;Power system stability;Real-time systems,computerised monitoring;oscillations;parallel processing;phasor measurement;power system stability;random-access storage;real-time systems;smart power grids;workstation clusters,RAM;centralized hardware software infrastructure;cluster computing;commodity nodes;frequency 2.53 GHz;frequency 60 Hz;high sampling rate;large-scale smart grid stability monitoring application;parallel PMU;phasor measurement units;real-time execution;small signal oscillation detection algorithm;storage capacity 8 Gbit,,4,,19,,no,25-27 June 2012,,IEEE,IEEE Conference Publications
Applying Learning Analytics in an Open Personal Learning Environment: A Quantitative Approach,E. Koulocheri; A. Soumplis; M. Xenos,"Software Quality Res. Group, Hellenic Open Univ., Patra, Greece",2012 16th Panhellenic Conference on Informatics,20121213,2012,,,290,295,"Every activity in the web leaves a digital trace. Analytics are used to measure all gathered traces along with the activities and the traffic that a user provokes, aiming to exploit the application-under-analysis with an optimal way in terms of time and cost. As a result, the question about how are they applied into learning environment, raises. This paper discusses analytics dedicated to the learning process named learning analytics. After presenting their context, and a literature review regarding this relatively new term, this paper presents a methodology that is applied the academic year 2011-12 on HOU2LEARN Environment, an open educational environment set by Hellenic Open University. This methodology consists of social network analysis along with a set of metrics that is also presented. The procedure and the outcomes of this ongoing research so far are discussed.",,Electronic:978-0-7695-4825-8; POD:978-1-4673-2720-6,10.1109/PCi.2012.48,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6377407,Learning analytics;assessment;data;metrics;social network;visualization,Blogs;Data handling;Educational institutions;Information management;Learning systems;Measurement;Social network services,Internet;computer aided instruction;distance learning;social networking (online),HOU2LEARN environment;Hellenic Open University;Web-based learning;application-under-analysis;learning analytics;learning environment;open educational environment;open personal learning environment;social network analysis,,1,,26,,no,5-7 Oct. 2012,,IEEE,IEEE Conference Publications
Authorship and Documentary Boundary Objects,I. Huvila,"Dept. of ALM, Uppsala Univ., Uppsala, Sweden",2012 45th Hawaii International Conference on System Sciences,20120209,2012,,,1636,1645,"Earlier research on documentary boundary objects has underlined the contextual nature of the process of their emergence. The aim of this paper is to discuss how the process of making and the attribution or non-attribution of authorship affects documentary boundary objects. A better understanding of the making of boundary objects is helpful in understanding why and how particular boundary objects work, and what are their implications. The article proposes an analytic model of four modes of authorship of documentary boundary objects (1. solitary, and 2. emergent authorship, 3. light-weight, and 4. heavy-weight peer-production) based on a review and synthesis of the spectrum of solitary and collaborative practices of creating documentary boundary objects discussed in the literature.",1530-1605;15301605,Electronic:978-1-5090-5638-5; POD:978-1-4577-1925-7; USB:978-0-7695-4525-7,10.1109/HICSS.2012.126,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6149084,authorship;boundary objects;documents,Collaboration;Communities;Complexity theory;Context;Documentation;Production;Software,document handling;groupware;peer-to-peer computing,collaborative practice;documentary boundary objects;emergent authorship mode;heavy-weight peer-production mode;light-weight mode;solitary mode;solitary practice,,0,,46,,no,4-7 Jan. 2012,,IEEE,IEEE Conference Publications
Automatic Bibliometric Analysis of Research Literature in Adult Education,Y. H. Wang; Y. H. Tseng,"Nat. Taiwan Normal Univ., Taipei, Taiwan",2012 Sixth International Conference on Innovative Mobile and Internet Services in Ubiquitous Computing,20120910,2012,,,965,968,"Due to the vast volume of publications in nowadays research environment, methods for analyzing, organizing, and accessing information from large databases are in great need. Through appropriate analytic tools, competitive intelligence can be revealed to enhance the organization or individual performance. This article will show how a free software package called CATAR carry on the analysis about the literature of the Adult Education science. Adult Education Quarterly (AEQ) is a scholarly forum of research and theory in adult education. The field of adult education is broad-based and inter disciplinary, during 1990 to 2010 having 672 papers from the ISI Web of Knowledge database was collected. The data analyzed include the distribution of authors' countries, topical clusters, the most cited references, the productivity rankings, and others to provide information for further study. Our analysis scenario also suggests a feasible way of analyzing other similar data for readers interesting in literature mining applications.",,Electronic:978-0-7695-4684-1; POD:978-1-4673-1328-5,10.1109/IMIS.2012.69,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6296987,Adult Education;Bibliometric;CATAR;Content Analysis;Free Software,Bibliometrics;Couplings;Data mining;Databases;Education;Market research;Productivity,educational administrative data processing;information analysis;public domain software;research and development,CATAR;ISI Web of Knowledge database;adult education quarterly;adult education science;analytic tools;automatic bibliometric analysis;competitive intelligence;free software package;large databases;literature mining applications;research environment;research literature,,0,,10,,no,4-6 July 2012,,IEEE,IEEE Conference Publications
Axial-Flux-Machine Modeling With the Combination of FEM-2-D and Analytical Tools,A. Egea; G. Almandoz; J. Poza; G. Ugalde; A. J. Escalada,"University of Mondrag&#x00F3;n, Mondrag&#x00F3;n, Spain",IEEE Transactions on Industry Applications,20120712,2012,48,4,1318,1326,"This paper deals with the development of analysis tools for axial-flux permanent-magnet machines. Normally, the study of this kind of machine involves three-dimensional (3-D) finite element method (FEM) (FEM-3-D) due to the 3-D nature of the magnetic problem. As it is widely known, the FEM-3-D software could take too much time, and both definition and solving processes of the problem may be very arduous. In this paper, a novel analysis procedure for axial-flux synchronous machines is proposed. This method consists in the combination of 2-D FEM simulations with analytical models based on the Fourier-series theory. The obtained results prove that the proposed method could be a very interesting option in terms of time and accuracy.",0093-9994;00939994,,10.1109/TIA.2012.2199450,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200327,Analytic models;Fourier series;axial-flux machines;finite element method (FEM),Analytical models;Coils;Couplings;Harmonic analysis;Magnetic flux;Shape;Windings,Fourier series;finite element analysis;permanent magnet machines;synchronous machines,3D finite element method;FEM-2D simulations;FEM-3D software;Fourier-series theory;analytical tools;axial-flux permanent-magnet machines;axial-flux synchronous machines;axial-flux-machine modeling;magnetic problem;three-dimensional finite element method,,6,,19,,no,July-Aug. 2012,,IEEE,IEEE Journals & Magazines
Big Data analytics,S. Singh; N. Singh,"Business Analytics Division, IBM India Software Lab (ISL), Pune, India","2012 International Conference on Communication, Information & Computing Technology (ICCICT)",20121231,2012,,,1,4,"In this paper, we explain the concept, characteristics & need of Big Data & different offerings available in the market to explore unstructured large data. This paper covers Big Data adoption trends, entry & exit criteria for the vendor and product selection, best practices, customer success story, benefits of Big Data analytics, summary and conclusion. Our analysis illustrates that the Big Data analytics is a fast-growing, influential practice and a key enabler for the social business. The insights gained from the user generated online contents and collaboration with customers is critical for success in the age of social media.",,Electronic:978-1-4577-2078-9; POD:978-1-4577-2077-2,10.1109/ICCICT.2012.6398180,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6398180,Data-warehousing-as-a Service (DaaS);InfoSphere BigInsights;Massively Parallel Processing (MPP);Omne optimizer & SAND Analytic Platform;ParAccel Analytic Database (PADB);WX2,Computers;Data handling;Data storage systems;Databases;Information management;Media,business data processing;data analysis;data structures;data visualisation;social networking (online),big data analytics;customer success story;entry-&-exit criteria;product selection;social business;social media;unstructured large data;user generated online collaboration;user generated online contents;vendor selection,,18,,9,,no,19-20 Oct. 2012,,IEEE,IEEE Conference Publications
Byte-precision level of detail processing for variable precision analytics,J. Jenkins; E. R. Schendel; S. Lakshminarasimhan; D. A. Boyuka; T. Rogers; S. Ethier; R. Ross; S. Klasky; N. F. Samatova,"North Carolina State Univ., Raleigh, NC, USA","High Performance Computing, Networking, Storage and Analysis (SC), 2012 International Conference for",20130225,2012,,,1,11,"I/O bottlenecks in HPC applications are becoming a more pressing problem as compute capabilities continue to outpace I/O capabilities. While double-precision simulation data often must be stored losslessly, the loss of some of the fractional component may introduce acceptably small errors to many types of scientific analyses. Given this observation, we develop a precision level of detail (APLOD) library, which partitions double-precision datasets along user-defined byte boundaries. APLOD parameterizes the analysis accuracy-I/O performance tradeoff, bounds maximum relative error, maintains I/O access patterns compared to full precision, and operates with low overhead. Using ADIOS as an I/O use-case, we show proportional reduction in disk access time to the degree of precision. Finally, we show the effects of partial precision analysis on accuracy for operations such as k-means and Fourier analysis, finding a strong applicability for the use of varying degrees of precision to reduce the cost of analyzing extreme-scale data.",2167-4329;21674329,Electronic:978-1-4673-0806-9; POD:978-1-4673-0805-2,10.1109/SC.2012.26,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6468527,,Accuracy;Context;Layout;Multiresolution analysis;Vectors;Wavelet transforms,input-output programs;parallel processing;software libraries,APLOD;Fourier analysis;HPC applications;I/O access patterns;I/O bottlenecks;I/O performance tradeoff;bounds maximum relative error;byte-precision level of detail processing;disk access time;double-precision simulation data;partial precision analysis;precision level of detail library;variable precision analytics,,6,,27,,no,10-16 Nov. 2012,,IEEE,IEEE Conference Publications
Calculation of the electromagnetic characteristics of an electrically excited synchronous motor for an EV,M. Strauch; S. Dewenter; A. Binder; K. H. Nam,"Darmstadt University of Technology/ Institute of Electrical Energy Conversion, Germany",2012 IEEE Vehicle Power and Propulsion Conference,20130131,2012,,,1086,1091,"This paper depicts an analytical way for calculating the electromagnetic characteristics and the no-load iron losses of an electrically excited synchronous traction motor with salient poles for an electric vehicle (EV). The calculations are applied on a machine design, of the Department of Electronic and Electrical Engineering of the Pohang University of Science and Technology (POSTECH), Korea. The calculation of the winding parameters such as the coil length, the resistance per phase including the current displacement effect at high frequencies in the stator winding and the stray reactance as well as the rotor pole stray flux are discussed. The magnetic characteristic is calculated, leading to the no-load and the short-circuit characteristic of the machine at a certain frequency. The analytical results are compared with numerical results, obtained by the Finite Element Software JMAG. The last part is about the analytical calculation of the no-load iron losses using the methods of Bertotti and Steinmetz. Here also a numerical result is used as a validation of the analytic approach.",1938-8756;19388756,Electronic:978-1-4673-0954-7; POD:978-1-4673-0953-0,10.1109/VPPC.2012.6422786,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6422786,,Lead;Resistance;Stators,electric vehicles;finite element analysis;stators;synchronous motors;traction motors,Bertotti methods;EV;JMAG finite element software;Steinmetz methods;coil length;current displacement effect;electric vehicle;electrically excited synchronous traction motor;electromagnetic characteristics;machine design;magnetic characteristic;no-load iron losses;rotor pole stray flux;salient poles;short-circuit characteristic;stator winding;stray reactance;winding parameters,,5,,10,,no,9-12 Oct. 2012,,IEEE,IEEE Conference Publications
Call for papers: Special Issue of Tsinghua Science and Technology on Visualization and Computer Graphics,,,Tsinghua Science and Technology,20120928,2012,17,5,606,606,"This special issue invites original articles to address the challenges in the areas of Visualization, Visual Analytics, and Computer Graphics. Topics for suitable articles may cover theoretical and applications of visualization techniques, systems, software, hardware, and user interface issues. All the submissions will go through the blind peer-reviewed process. Authors are invited to submit high quality, original, unpublished research and application papers in all related areas.",,,10.1109/TST.2012.6314536,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6314536,,,,,,0,,,,no,Oct. 2012,,TUP,TUP Journals & Magazines
Change-oriented aircraft fuel burn and emissions assessment methodologies,R. H. Mayer,"Center for Adv. Aviation Syst. Dev., MITRE Corp., McLean, VA, USA","2012 Integrated Communications, Navigation and Surveillance Conference",20120618,2012,,,N5-1,N5-15,"Advancing the National Airspace System (NAS) generally requires research to project the costs and benefits of proposed operational changes that improve system efficiency and environmental performance. Aviation regulatory agencies increasingly rely on aircraft fuel consumption and emissions assessments to support investment decisions needed to adopt new technologies that ensure the best value to the public. The varying scopes of such assessments and often limited availability of data to support evaluations of future change scenarios pose analytic challenges. This paper describes a methodological framework for standardizing benefit estimations. The framework is centered on the concept of operational change, identifies key benefit mechanisms, defines applicable metrics, and presents quantitative fuel burn and emission benefit estimation examples. It aims to support assessments that are based on on-board flight data, surveillance data, and model evaluations in a manner independent of specific implementations of simulation capabilities and software tools. The examples characterize and compare typical operational change scenarios on a parametric basis and rank associated benefit pools of actual changes observed in recent operational improvements including implementations of Performance Based Navigation (PBN) procedures at major airports in the NAS.",2155-4943;21554943,Electronic:978-1-4673-1900-3; POD:978-1-4673-1901-0,10.1109/ICNSurv.2012.6218432,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6218432,,Air traffic control;Aircraft;Aircraft propulsion;Atmospheric modeling;Engines;Fuels;Measurement,air pollution control;aircraft navigation;airports;avionics,NAS;National Airspace System;PBN;aircraft fuel consumption;airports;aviation regulatory agency;change-oriented aircraft fuel burn;emission benefit estimation;emissions assessment methodology;investment decisions;key benefit mechanisms;on-board flight data;performance based navigation procedures;quantitative fuel burn;surveillance data,,3,,14,,no,24-26 April 2012,,IEEE,IEEE Conference Publications
CLOUD 2012: 2012 IEEE 5th International Conference on Cloud Computing [Cover art],,,2012 IEEE Fifth International Conference on Cloud Computing,20120802,2012,,,C4,C4,The following topics are dealt with: cloud computing; admission control; analytics cloud platform; economic models; collaborative cloud computing; cloud performance management; cloud resource management; infrastructure-as-a-service clouds; cloud resource provisioning; virtual server image management; cloud privacy; cloud application deployment; cloud exploitation analysis; cloud SLA management; mobile cloud; cloud application security; cloud data storage; cloud service selection; cloud resource optimization; cloud cost models; cloud architecture; and cloud energy.,2159-6182;21596182,Electronic:978-0-7695-4755-8; POD:978-1-4673-2892-0,10.1109/CLOUD.2012.151,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6253480,,,cloud computing;data privacy;groupware;mobile computing;resource allocation;security of data;software architecture;software management;software performance evaluation;storage management,admission control;analytics cloud platform;cloud SLA management;cloud application deployment;cloud application security;cloud architecture;cloud computing;cloud cost models;cloud data storage;cloud energy;cloud exploitation analysis;cloud performance management;cloud privacy;cloud resource management;cloud resource optimization;cloud resource provisioning;cloud service selection;collaborative cloud computing;economic models;infrastructure-as-a-service clouds;mobile cloud;virtual server image management,,0,,,,no,24-29 June 2012,,IEEE,IEEE Conference Publications
Condition monitoring & fault diagnosis system for Offshore Wind Turbines,O. Bennouna; N. Heraud; Z. Leonowicz,"ESIGELEC / IRSEEM- EA 4353, Rouen- France",2012 11th International Conference on Environment and Electrical Engineering,20120621,2012,,,13,17,"Due to the technological development, the electronic power progress and economic stake, through the use of Wound Rotor Induction Motor (WRIM) has taken more and more places in different domains (transport, energy production, electric drive..,) thanks to their robustness, efficiency and lower costs. Despite the performed work researches and the improvement that has been brought, these machines still remain the potential seats of failures both in stator and rotor levels. Consequently, WRIM faults detection is currently one of the centers of interest of several researches of both academic and industrial laboratories. In fact, this article addresses this problem by the use of Principal Components Analysis (PCA) for faults detection in Offshore Wind Turbine Generator (OWTG). An accurate analytic modeling of healthy and faulted OWTG is suggested to perform the data matrix needed for PCA method. Tests were achieved using a numeric simulator on Matlab/Simulink software. Analysis of OWTG simulation proves the efficiency of PCA method. Several simulation results will be presented and discussed.",,Electronic:978-1-4577-1829-8; POD:978-1-4577-1830-4,10.1109/EEEIC.2012.6221389,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6221389,Diagnosis;Monitoring;OWTG modeling;Principal Components Analysis;Wind turbine,Eigenvalues and eigenfunctions;Fault detection;Principal component analysis;Rotors;Stators;Strontium;Wind turbines,fault diagnosis;induction motors;matrix algebra;offshore installations;power system faults;principal component analysis;rotors;stators;turbogenerators;wind turbines,Matlab/Simulink software;condition monitoring;data matrix;electronic power progress;fault diagnosis system;faults detection;offshore wind turbine generator;principal components analysis;rotor levels;stator levels;wound rotor induction motor,,2,,15,,no,18-25 May 2012,,IEEE,IEEE Conference Publications
Configurable and Extensible Multi-flows for Providing Analytics as a Service on the Cloud,D. P.; P. M. Deshpande; K. Murthy,"IBM Res. - India, Bangalore, India",2012 Annual SRII Global Conference,20120924,2012,,,1,10,"Compared to traditional analytics deployment models, cloud-based solutions for business analytics provide numerous advantages such as reduction of a large upfront infrastructural cost and the efforts to setup an in-house analytics team. Such advantages of cloud-based service delivery make it particularly attractive for small and medium businesses. In spite of these advantages, analytics penetration has been low particularly in developing regions such as India and China due to many other factors. In this paper, we propose pre-packaged configurable workflows for analytics as a means of endearing cloud-based analytics to customers, with a special focus on small and medium businesses in developing regions. We introduce the concept of configurable multi-flows that make it easy for non-technical personnel to use and customize without being aware of the technical details of the various operators involved in the workflow. Multi-flows comprise of an overlap of multiple possible workflows and are easily extensible to include more variations to support the evolving needs of customers non-disruptively and incrementally. We detail a case-study of the Retail sector where an extensive survey of retail businesses in India revealed that configurable pre-packaged workflows may indeed help improve market penetration. We then identify common analytics needs of retail customers, and detail how such tasks can be expressed as configurable multi-flows. Further, we describe a fully functional implementation of our system that supports configurable multi-flows for analytics. Finally, we illustrate the ease-of-use of configurable multi-flows with the use of multiple screenshots.",2166-0778;21660778,Electronic:978-1-5090-5643-9; POD:978-1-4673-2318-5; USB:978-0-7695-4770-1,10.1109/SRII.2012.11,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6310975,analytics;cloud;configurable;retail;smb,Analytical models;Association rules;Business;Data models;Engines;Predictive models;Software,,,,3,,18,,no,24-27 July 2012,,IEEE,IEEE Conference Publications
Connecting the dots in IT service delivery: From operations content to high-level business insights,D. Rosu; W. Cheng; E. E. Jan; N. Ayachitula,"IBM T.J. Watson Research Center, 19 Skyline Dr, Hawthorne, NY, USA","Proceedings of 2012 IEEE International Conference on Service Operations and Logistics, and Informatics",20120820,2012,,,410,415,"IT service delivery relies on intelligent data-driven insights to make strategic decisions. It is a highly complex business with many sub-organizations that focus on different aspects of delivery operations. High-level business insights that emerge from understanding the collective value of all these viewpoints are invaluable to achieving excellent service quality and solid profit margin. However, this is hindered by the inability to integrate data models and taxonomies across business components such as asset management, configuration management, and incident management. Innovative solutions are necessary to effectively ""connect-the-dots"", bridging the gaps between available content and higher-level business insights. In this paper, we describe several real-world business decisions in service delivery and logistics that suffer from this content-model gap. We propose a unified approach to bridge this gap, with an information system component called Business-Knowledge Discovery Component. We discuss key challenges, architectural framework and the text analytic techniques that are involved.",,Electronic:978-1-4673-2401-4; POD:978-1-4673-2400-7,10.1109/SOLI.2012.6273572,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6273572,Analytics;Data model;Logistics and Process Control;Service management,Adaptation models;Business;Databases;Hardware;Robustness;Software;Taxonomy,business data processing;data mining;decision making;information technology;strategic planning,IT service delivery;asset management;business components;business-knowledge discovery component;configuration management;connect-the-dots;content-model gap;data models;high-level business insights;incident management;information system component;intelligent data-driven insights;operations content;strategic decisions;taxonomies;text analytic techniques,,2,,7,,no,8-10 July 2012,,IEEE,IEEE Conference Publications
Cost-aware replication for dataflows,C. Castillo; A. N. Tantawi; D. Arroyo; M. Steinder,"IBM T. J. Watson Research Center, 19 Skyline Drive, Hawthorne, New York 10532",2012 IEEE Network Operations and Management Symposium,20120607,2012,,,171,178,"In this work we are concerned with the cost associated with replicating intermediate data for dataflows in Cloud environments. This cost is attributed to the extra resources required to create and maintain the additional replicas for a given data set. Existing data-analytic platforms such as Hadoop provide for fault-tolerance guarantee by relying on aggressive replication of intermediate data. We argue that the decision to replicate along with the number of replicas should be a function of the resource usage and utility of the data in order to minimize the cost of reliability. Furthermore, the utility of the data is determined by the structure of the dataflow and the reliability of the system. We propose a replication technique, which takes into account resource usage, system reliability and the characteristic of the dataflow to decide what data to replicate and when to replicate. The replication decision is obtained by solving a constrained integer programming problem given information about the dataflow up to a decision point. In addition, we built a working prototype, CARDIO of our technique which shows through experimental evaluation using a real testbed that finds an optimal solution.",1542-1201;15421201,Electronic:978-1-4673-0269-2; POD:978-1-4673-0267-8; USB:978-1-4673-0268-5,10.1109/NOMS.2012.6211896,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6211896,Hadoop;data-availability;dataflows;map-reduce;replication,Availability;Degradation;Fault tolerance;Measurement;Optimization;Silicon,cloud computing;cost reduction;data analysis;data flow analysis;fault tolerant computing;resource allocation;software reliability,Hadoop;cloud environments;constrained integer programming problem;cost aware CARDIO;cost aware replication;data analysis;data utility;dataflow structure;decision point;fault tolerance;intermediate data replication;replication decision;resource usage;system reliability cost minimization,,0,,14,,no,16-20 April 2012,,IEEE,IEEE Conference Publications
Data analytics in the cloud with flexible MapReduce workflows,C. Goncalves; L. Assuncao; J. C. Cunha,Instituto Superior de Engenharia de Lisboa,4th IEEE International Conference on Cloud Computing Technology and Science Proceedings,20130204,2012,,,427,434,"Data analytic applications are characterized by large data sets that are subject to a series of processing phases. Some of these phases are executed sequentially but others can be executed concurrently or in parallel on clusters, grids or clouds. The MapReduce programming model has been applied to process large data sets in cluster and cloud environments. For developing an application using MapReduce there is a need to install/configure/access specific frameworks such as Apache Hadoop or Elastic MapReduce in Amazon Cloud. It would be desirable to provide more flexibility in adjusting such configurations according to the application characteristics. Furthermore the composition of the multiple phases of a data analytic application requires the specification of all the phases and their orchestration. The original MapReduce model and environment lacks flexible support for such configuration and composition. Recognizing that scientific workflows have been successfully applied to modeling complex applications, this paper describes our experiments on implementing MapReduce as sub-workflows in the AWARD framework (Autonomic Workflow Activities Reconfigurable and Dynamic). A text mining data analytic application is modeled as a complex workflow with multiple phases, where individual workflow nodes support MapReduce computations. As in typical MapReduce environments, the end user only needs to define the application algorithms for input data processing and for the map and reduce functions. In the paper we present experimental results when using the AWARD framework to execute MapReduce workflows deployed over multiple Amazon EC2 (Elastic Compute Cloud) instances.",,Electronic:978-1-4673-4510-1; POD:978-1-4673-4511-8; USB:978-1-4673-4509-5,10.1109/CloudCom.2012.6427527,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6427527,Cloud;MapReduce;Text Mining;Workflow,Awards activities;Cloud computing;Computational modeling;Corporate acquisitions;Data models;Programming;Text mining,cloud computing;data analysis;data mining;fault tolerant computing;natural sciences computing;pattern clustering;text analysis;workflow management software,AWARD framework;Amazon Cloud;Amazon EC2;Apache Hadoop;Elastic MapReduce;MapReduce programming model;autonomic workflow activities reconfigurable and dynamic framework;cloud environments;cluster environments;elastic compute cloud;flexible MapReduce workflows;large data set processing;processing phase;scientific workflows;text mining data analytic application,,5,,51,,no,3-6 Dec. 2012,,IEEE,IEEE Conference Publications
Department course selection problem: The primitive cognitive network process approach,K. Kam; Fung Yuen; Manwai Yuen,"Dept. of Comput. Sci. & Software Eng., Xi''an Jiaotong-Liverpool Univ., Suzhou, China","Proceedings of IEEE International Conference on Teaching, Assessment, and Learning for Engineering (TALE) 2012",20121124,2012,,,H1C-15,H1C-17,"The program curriculum in tertiary education has to be reviewed and updated regularly to enhance the education quality regarding the student interests, current industry demands and academic trends. To effectively develop a number of reasonable courses, a comprehensive approach to evaluate and select the suitable courses for students is essential for the program curriculum development. This paper proposes the Primitive Cognitive Network Process for the department course selection strategy considering multiple criteria and alternatives.",,Electronic:978-1-4673-2418-2; POD:978-1-4673-2417-5; USB:978-1-4673-2416-8,10.1109/TALE.2012.6360327,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6360327,analytic hierarchy process;cognitive network process;course development;curriculum design;education management;multiple criteria decision making,Artificial intelligence;Educational institutions;Engineering profession;Market research,cognition;educational administrative data processing;educational courses,academic trends;current industry demands;department course selection problem;department course selection strategy;education quality;multiple criteria;primitive cognitive network process approach;program curriculum development;reasonable courses;student interests;tertiary education,,0,,13,,no,20-23 Aug. 2012,,IEEE,IEEE Conference Publications
Description of the U.S. Geological Survey Geo Data Portal Data Integration Framework,D. Blodgett; N. Booth; T. Kunicki; J. Walker; J. Lucido,"Wisconsin Water Science Center, U.S. Geological Survey, Middleton, United States",IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,20121228,2012,5,6,1687,1691,"The U.S. Geological Survey has developed an open-standard data integration framework for working efficiently and effectively with large collections of climate and other geoscience data. A web interface accesses catalog datasets to find data services. Data resources can then be rendered for mapping and dataset metadata are derived directly from these web services. Algorithm configuration and information needed to retrieve data for processing are passed to a server where all large-volume data access and manipulation takes place. The data integration strategy described here was implemented by leveraging existing free and open source software. Details of the software used are omitted; rather, emphasis is placed on how open-standard web services and data encodings can be used in an architecture that integrates common geographic and atmospheric data.",1939-1404;19391404,,10.1109/JSTARS.2012.2196759,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6212478,Distributed computing;open source software;software standards;standards;web services,Distributed computing;Geospatial analysis;Open source software;Servers;Software standards;Standards;User interfaces;Web services,Web services;data integration;geophysics computing;information retrieval;open systems,US Geological Survey;USGS geo data;Web interface data access;algorithm configuration;catalog datasets;data encoding;data retrieval;geoscience data;metadata;open standard Web services;open standard data integration framework;portal data integration framework,,1,,14,,no,Dec. 2012,,IEEE,IEEE Journals & Magazines
Design and Development of an Adaptive Workflow-Enabled Spatial-Temporal Analytics Framework,X. Li; R. N. Calheiros; S. Lu; L. Wang; H. Palit; Q. Zheng; R. Buyya,"Inst. of High Performance Comput., A*STAR Inst., Singapore, Singapore",2012 IEEE 18th International Conference on Parallel and Distributed Systems,20130117,2012,,,862,867,"Cloud computing is a suitable platform for execution of complex computational tasks and scientific simulations that are described in the form of workflows. Such applications are managed by Workflow Management System (WfMS). Because existing WfMSs are not able to autonomically provision resources to real-time applications and schedule them while supporting fault tolerance and data privacy, we present a highly-scalable workflow-enabled analytics system that manages inter-dependable analytics tasks adaptively with varying operational requirements on a common platform and enables visualization of multidimensional datasets of real world phenomena. In this paper, we present the architecture of such a WfMS and evaluate it in terms of performance for execution of workflows in Clouds. A real world application of climate-associated dengue fever prediction was evaluated on public, private, and hybrid Clouds and experienced effective speedup in all the environments.",1521-9097;15219097,Electronic:978-0-7695-4903-3; POD:978-1-4673-4565-1,10.1109/ICPADS.2012.141,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6413591,cloud computing;data privacy;dynamic resource;fault tolerance;workflow,Cloud computing;Computer architecture;Data privacy;Data visualization;Real-time systems;Resource management;Security,cloud computing;data privacy;real-time systems;workflow management software,WfMS;adaptive workflow enabled spatial temporal analytics framework;cloud computing;multidimensional datasets;real-time applications;scientific simulations,,0,,14,,no,17-19 Dec. 2012,,IEEE,IEEE Conference Publications
Design Principles for Effective Knowledge Discovery from Big Data,E. Begoli; J. Horey,"Comput. Sci. &amp; Eng. Div., Oak Ridge Nat. Lab., Oak Ridge, TN, USA",2012 Joint Working IEEE/IFIP Conference on Software Architecture and European Conference on Software Architecture,20121025,2012,,,215,218,"Big data phenomenon refers to the practice of collection and processing of very large data sets and associated systems and algorithms used to analyze these massive datasets. Architectures for big data usually range across multiple machines and clusters, and they commonly consist of multiple special purpose sub-systems. Coupled with the knowledge discovery process, big data movement offers many unique opportunities for organizations to benefit (with respect to new insights, business optimizations, etc.). However, due to the difficulty of analyzing such large datasets, big data presents unique systems engineering and architectural challenges. In this paper, we present three system design principles that can inform organizations on effective analytic and data collection processes, system organization, and data dissemination practices. The principles presented derive from our own research and development experiences with big data problems from various federal agencies, and we illustrate each principle with our own experiences and recommendations.",,Electronic:978-0-7695-4827-2; POD:978-1-4673-2809-8,10.1109/WICSA-ECSA.212.32,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6337722,Big Data;architecture;design principles,Computer architecture;Data handling;Data storage systems;Data visualization;Information management;Organizations,data mining;research and development;software architecture;systems engineering;very large databases,architectural challenges;associated algorithms;associated systems;big data architectures;big data movement;big data phenomenon;data collection processes;data dissemination practices;federal agency;knowledge discovery process;massive datasets;organizations;research and development;system design principles;system organization;systems engineering;very large data sets,,16,,23,,no,20-24 Aug. 2012,,IEEE,IEEE Conference Publications
Designing a Programmable Wire-Speed Regular-Expression Matching Accelerator,J. V. Lunteren; C. Hagleitner; T. Heil; G. Biran; U. Shvadron; K. Atasu,"IBM Res. - Zurich, Zurich, Switzerland",2012 45th Annual IEEE/ACM International Symposium on Microarchitecture,20130404,2012,,,461,472,"A growing number of applications rely on fast pattern matching to scan data in real-time for security and analytics purposes. The RegX accelerator in the IBM Power Edge of Network<sup>TM</sup> (PowerEN) processor supports these applications using a combination of fast programmable state machines and simple processing units to scan data streams against thousands of regular-expression patterns at state-of-the-art Ethernet link speeds. RegX employs a special rule cache and includes several new micro-architectural features that enable various instruction dispatch and execution options for the processing units. The architecture applies RISC philosophy to special-purpose computing: hardware provides fast, simple primitives, typically performed in a single cycle, which are exploited by an intelligent compiler and system software for high performance. This approach provides the flexibility required to achieve good performance across a wide range of workloads. As implemented in the PowerEN<sup>TM</sup> processor, the accelerator achieves a theoretical peak scan rate of 73.6 Gbit/s, and a measured scan rate of about 15 to 40 Gbit/s for typical intrusion detection workloads.",1072-4451;10724451,Electronic:978-1-4673-4819-5; POD:978-1-4799-1704-4,10.1109/MICRO.2012.49,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6493642,,,cache storage;computer network security;finite state machines;local area networks;pattern matching;program compilers;reduced instruction set computing,Ethernet link speeds;IBM Power Edge of Network processor;PowerEN processor;RISC;RegX accelerator;analytics;cache storage;data stream scanning;grammable state machines;instruction dispatch;instruction execution;intelligent compiler;microarchitectural features;network intrusion detection;pattern matching;peak scan rate;processing units;programmable wire-speed regular-expression matching accelerator design;scan rate measurement;security;system software,,11,1,33,,no,1-5 Dec. 2012,,IEEE,IEEE Conference Publications
Development of virtual lab system through application of fuzzy analytic hierarchy process,Chun Yong Chong; Sai Peck Lee; Teck Chaw Ling,"Faculty of Computer Science and Information Technology, University of Malaya, Malaysia",2012 8th International Conference on Information Science and Digital Content Technology (ICIDT2012),20120816,2012,1,,207,211,"Virtual lab has gained popularity among cloud computing practitioners due to its promise of on-demand provisioning of computer resources. However, lack of standardized guidelines hinders the adoption among educational institutions. Achievements of non-functional requirements possess greater challenge due to their qualitative nature. These requirements can be achieved by applying guidelines during software development. However, priority assessment of non-functional requirements is needed before the selection of suitable guidelines. This paper tries to apply fuzzy analytic hierarchy process in order to aggregate group decisions, as well as dealing with ambiguities of decision makers' judgement. The application of fuzzy analytic approach in this paper had successful create a solid foundation for software developers to identify the essential non-functional requirement in virtual lab environment. Importantly, the result can provide a promising reference model for better understanding of cloud computing in educational sector.",,Electronic:978-8-9886-7870-1; POD:978-1-4673-1288-2,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6269258,non-functional requirement;priority assessment;virtual lab,Abstracts;Analytical models;Artificial intelligence;Computational modeling;Reliability;Security;Usability,cloud computing;computer aided instruction;decision making;educational institutions;formal specification;fuzzy set theory;laboratories,cloud computing;computer resource;decision maker judgement;educational institution;educational sector;fuzzy analytic hierarchy process;group decision;nonfunctional requirement;on-demand provisioning;priority assessment;software developer;software development;virtual lab environment;virtual lab system,,0,,9,,no,26-28 June 2012,,IEEE,IEEE Conference Publications
Does the Cost Function Matter in Bayes Decision Rule?,R. SchlÌ_ter; M. Nussbaum-Thom; H. Ney,"RWTH Aachen University, Aachen",IEEE Transactions on Pattern Analysis and Machine Intelligence,20111219,2012,34,2,292,301,"In many tasks in pattern recognition, such as automatic speech recognition (ASR), optical character recognition (OCR), part-of-speech (POS) tagging, and other string recognition tasks, we are faced with a well-known inconsistency: The Bayes decision rule is usually used to minimize string (symbol sequence) error, whereas, in practice, we want to minimize symbol (word, character, tag, etc.) error. When comparing different recognition systems, we do indeed use symbol error rate as an evaluation measure. The topic of this work is to analyze the relation between string (i.e., 0-1) and symbol error (i.e., metric, integer valued) cost functions in the Bayes decision rule, for which fundamental analytic results are derived. Simple conditions are derived for which the Bayes decision rule with integer-valued metric cost function and with 0-1 cost gives the same decisions or leads to classes with limited cost. The corresponding conditions can be tested with complexity linear in the number of classes. The results obtained do not make any assumption w.r.t. the structure of the underlying distributions or the classification problem. Nevertheless, the general analytic results are analyzed via simulations of string recognition problems with Levenshtein (edit) distance cost function. The results support earlier findings that considerable improvements are to be expected when initial error rates are high.",0162-8828;01628828,,10.1109/TPAMI.2011.163,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5989822,Bayes decision rule;Statistical pattern recognition;classifier design and evaluation;cost/loss function.,Bayesian methods;Cost function;Error analysis;Measurement uncertainty;Speech recognition;Statistical analysis,Bayes methods;pattern recognition,ASR;Bayes decision rule;OCR;POS;automatic speech recognition;distance cost function;error rate symbol;optical character recognition;part-of-speech;pattern recognition;string recognition;symbol sequence,"Algorithms;Bayes Theorem;Computer Simulation;Pattern Recognition, Automated;Speech Recognition Software",4,,16,,no,Feb. 2012,,IEEE,IEEE Journals & Magazines
Dynamic process support based on users' behavior,D. Burkhardt; K. Nazemi,"Inf. Visualization &amp; Visual Analytics, Fraunhofer-Inst. for Comput. Graphics Res., Darmstadt, Germany",2012 15th International Conference on Interactive Collaborative Learning (ICL),20130107,2012,,,1,6,"Nowadays there is a gap between the possibilities and the massively existing data on the one side and the user as main worker on the other side. In different scenarios e.g. search, exploration, analysis and policy-modeling a user has to deal with massive information, but for this work he usually gets a static designed system. So meanwhile data-driven work-processes are increasing in its complexity the support of the users who are working with these data is limited on basic features. Hence this paper describes a concept for a process-supporting approach, which includes relevant aspects of users' behaviors in support him to successfully finish also complex tasks. This will be achieved by a process-based guidance with an automatic tools selection for every process and activity on the one hand. And on the other hand the consideration of expert-level of a user to a single task and process. This expert-level will be classified during each task and process interaction and allow the automatically selection of optimal tools for a concrete task. In final the user gets for every task an automatically initialized user-interface with useful and required tools.",,Electronic:978-1-4673-2427-4; POD:978-1-4673-2425-0; USB:978-1-4673-2426-7,10.1109/ICL.2012.6402079,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6402079,process adaptation;process management;process support;user-centered interaction,Adaptation models;Business;Computers;Context;Data visualization;Process control;Software,behavioural sciences;interactive systems,automatic tool selection;automatically initialized user-interface;data-driven work-processes;dynamic process support;expert-level classification;optimal tool selection;process interaction;process-based guidance;process-supporting approach;user behavior,,1,,19,,no,26-28 Sept. 2012,,IEEE,IEEE Conference Publications
Early effort estimation by AHP: A case study of project metrics in small organizations,Y. Zhang; X. Zhang; X. Zhao; T. Zhang,"Department of Computer Science and Technology, Beijing Electronic Science and Technology Institute, Beijing, P.R. China 100070",2012 IEEE International Conference on Computer Science and Automation Engineering (CSAE),20120820,2012,1,,452,456,"Project effort estimation in the early phase of software development is a significant activity for every software organization to improve the development process. Small organizations need some simple and easily used method to estimate effort because of insufficient resources including human and finance. Analytic Hierarchy Process (AHP), which integrates qualitative and subjective approach with quantitative and objective approach, can meet the need of small organizations. We use the data of three real projects to demonstrate the application of AHP for early project effort estimation. By the analysis of the estimation results, we point that AHP can not only predict the project effort in the early phase of development, but also help managers to find the principal factors that contribute to the project effort in their organizations. In summary, AHP is appropriate for early project effort estimation in a small organization.",,Electronic:978-1-4673-0089-6; POD:978-1-4673-0088-9,10.1109/CSAE.2012.6272636,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6272636,analytic hierarchy process;effort estimation;project metric;small organization,Decision making;Estimation;Measurement;Organizations;Programming;Software;Vectors,decision making;matrix algebra;organisational aspects;project management;software houses;software management;software metrics,AHP;analytic hierarchy process;objective approach;principal factors;project effort estimation;project metrics;qualitative approach;quantitative approach;small-organizations;software development;software organization;subjective approach,,2,,10,,no,25-27 May 2012,,IEEE,IEEE Conference Publications
Empirical analysis of user data in game software development,K. Hullett; N. Nagappan; E. Schuh; J. Hopson,"UC Santa Cruz Santa Cruz, CA, USA",Proceedings of the 2012 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement,20130307,2012,,,89,98,"For several years empirical studies have spanned the spectrum of research from software productivity, quality, reliability, performance to human computer interaction. Analyses have involved software systems ranging from desktop software to telecommunication switching systems. But surprising there has been little work done on the emerging digital game industry, one of the fastest growing domains today. To the best of our knowledge, our work is one of the first empirical analysis of a large commercially successful game system. In this paper, we introduce an analysis of the significant user data generated in the gaming industry by using a successful game: Project Gotham Racing 4. More specifically, due to the increasing ubiquity of constantly connected high-speed internet connections for game consoles, developers are able to collect extensive amounts of data about their games following release. The challenge now is to make sense of that data, and from it be able to make recommendations to developers. This paper presents an empirical case study analyzing the data collected from a released game over a three year period. The results of this analysis include a better understanding of the differences between long-term and short-term players, and the extent to which various options in the game are utilized. This led to recommendations for future development ways to reduce development costs and to keep new players engaged. A secondary goal for this paper is to introduce software game development as a topic of importance to the empirical software engineering community and discuss research results on a key difference area: data analytics on user data to customize user and development experiences.",1949-3770;19493770,Electronic:978-1-4503-1056-7; POD:978-1-4503-1056-7,10.1145/2372251.2372265,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6475400,Game design;Game development;Game metrics,Communities;Games;Industries;Testing;Usability;Vehicles,Internet;computer games;data analysis;human computer interaction;software quality;software reliability,Project Gotham Racing 4;data analytics;desktop software;digital game industry;empirical software engineering community;game consoles;game software development;high-speed internet connections;human computer interaction;long-term players;short-term players;software game development;software productivity;software quality;software reliability;software systems;telecommunication switching systems;user data empirical analysis,,1,,20,,no,20-21 Sept. 2012,,IEEE,IEEE Conference Publications
Energetic-environmental requalification for tourist accommodation facilities by analytic hierarchy process,V. Racioppi; G. Marcarelli; M. Squillante,"Cooperative Institute for the Innovation scrl- ICIE, Naples, Italy",CCCA12,20130124,2012,,,1,6,"Decision problems are often characterized by a great number of alternative actions and by the complexity of relationships between the various factors involved in the process. This leads to the determination of alternative choices in uncertainty conditions. So, the multicriteria decision methods (MCDM) represent a support to the decision maker to rationalize the process and identify the best solution. This paper presents an application of a multicriteria method, the analytic hierarchy process, by using a software package, Expert Choice, for analyzing the problem of an energetic and environmental requalification.",,Electronic:978-1-4673-4695-5; POD:978-1-4673-4694-8,10.1109/CCCA.2012.6417854,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6417854,Analytic Hierarchy Process;eco-substainable requalification;environmental assessments;sensitivity analysis,Analytic hierarchy process;Buildings;Customer satisfaction;Heating;Indexes;Sensitivity analysis;Vectors,analytic hierarchy process;software packages;travel industry,MCDM;analytic hierarchy process;energetic-environmental requalification;expert choice;multicriteria decision methods;software package;tourist accommodation facilities,,1,,11,,no,6-8 Dec. 2012,,IEEE,IEEE Conference Publications
Epistemic signals and emoticons affect kudos,C. Vogel; L. M. Sanchez,"Centre for Computing and Language Studies, Computational Linguistics Group, School of Computer Science and Statistics Trinity College, Trinity College Dublin, Dublin 2, Ireland",2012 IEEE 3rd International Conference on Cognitive Infocommunications (CogInfoCom),20130128,2012,,,517,522,"Our focus is on the interaction between emoticon use and epistemic hedges in the perception of individual contributions to discourse (and posters of those contributions) as deserving of kudos for their input. The communities with English as a lingua franca that we explore consist of self-motivated contributors to user-fora supported by a major multinational with a software technology company. User categories are determined by a few orthogonal classifications: employees, novice users, and experts; recipients of kudos vs. non-recipients of kudos; etc. We explore the interaction between social signals and signals of certainty in content. Among the effects reported are the negative influence of epistemic hedges used in posting on propensity for others in the community to accord kudos to such postings, but a positive influence of the same in interaction with the use of emoticons.",,Electronic:978-1-4673-5188-1; POD:978-1-4673-5187-4; USB:978-1-4673-5186-7,10.1109/CogInfoCom.2012.6422036,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6422036,,,,,,0,,23,,no,2-5 Dec. 2012,,IEEE,IEEE Conference Publications
Establishing an lifelong learning environment using IOT and learning analytics,H. C. Cheng; W. W. Liao,"Department of Information and Management, Chinese Culture University, Taipei, Taiwan",2012 14th International Conference on Advanced Communication Technology (ICACT),20120403,2012,,,1178,1183,"With the trend of the development of global knowledge economy and lifelong learning society, the competition of lifelong learning market becomes increasingly keen. Lifelong learning units need to understand whether the courses they provide are appropriate and their students' learning effectiveness just as advertisers need to understand the market and customers. In this context, learning analytics is extremely important. Traditionally, it used the data of questionnaire after the courses to perform analysis and evaluation, however, the questionnaire survey usually was conducted at the end of the semester, and therefore the immediateness is lower and teachers couldn't adjust the content of the course and learning strategies based on their students' needs immediately. Some researchers used the data collected by LMS to conduct real-time learning analytics, nevertheless, students perform learning not just in classes, they do it when they go to libraries to borrow books, use classrooms to discuss the courses, and utilize Mobile Devices to download data and join in seminar...etc. Therefore, this study tried to combine internet of things (IOT) and the techniques of learning analytics to record and conduct the analysis of students' learning process and further enable them and schools to obtain feedbacks that they need and establish an effective lifelong learning environment.",1738-9445;17389445,Electronic:978-89-5519-163-9; POD:978-1-4673-0150-3,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6174821,IOT;Internet of Things;Learning Analytics;Lifelong Learning,Educational institutions;Internet;Least squares approximation;Mobile handsets;Radiofrequency identification;Software,Internet;computer aided instruction;continuing professional development;educational courses;educational institutions,IOT;Internet of Things;LMS;course content adjustment;data download;global knowledge economy development;learning strategies;libraries;lifelong learning environment;lifelong learning market competition;lifelong learning society;mobile devices;questionnaire survey;real-time learning analytics;schools;student learning process,,1,,12,,no,19-22 Feb. 2012,,IEEE,IEEE Conference Publications
Evaluation of Java-based open source web frameworks with Ajax support,B. Buchner; A. BÌ_ttcher; C. Storch,"Munich University of Applied Sciences, Faculty of Computer Science and Mathematics, Lothstr. 64, D-80335 M&#x00FC;nchen, Germany",2012 14th IEEE International Symposium on Web Systems Evolution (WSE),20121004,2012,,,45,49,"Web frameworks are widely used in web applications to lower development effort and ease maintenance. The large number of Java<sup>1</sup> web frameworks makes it hard to decide for one. A systematic approach is necessary to come to an optimal decision under given requirements and conditions. For this work, 110 Java web frameworks were gathered; 13 of them were analyzed by the Analytic Hierarchy Process (AHP). The result is a systematic approach towards a selection guidance for a Java web framework within a particular project. The list of Java web frameworks and the goal hierarchy can be reused, adopted and extended as required. The process described in this paper enables project leaders and developers to do their own comparisons - it is not limited to evaluation of Java Web frameworks. The AHP, as described here, allows a decision maker to adjust the priorities of criteria and intensities according to his needs in a comprehensible manner.",1550-4441;15504441,Electronic:978-1-4673-3056-5; POD:978-1-4673-3057-2,10.1109/WSE.2012.6320531,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6320531,,Eigenvalues and eigenfunctions;Indexes;Java;Licenses;Software,Internet;Java,Ajax support;Java based open source Web frameworks;Web application;analytic hierarchy process;optimal decision;selection guidance,,1,,6,,no,28-28 Sept. 2012,,IEEE,IEEE Conference Publications
Experiences in Delivering Power System Decision Support Tools over the Web Using Software-as-a-Service (SaaS) Model,A. Pandya; M. Shah; N. Rajagopal; K. V. Prasad,"Tata Consultancy Services (TCS), Mumbai, India",2012 Annual SRII Global Conference,20120924,2012,,,846,849,"Power System Analytics Applications have always been available only as in-premise licensed software -- the use of a SaaS model for delivering the Analytics to the user is a first in the history of the power sector. In this paper, we describe how SaaS based webDNA architecture [1] can be extended to facilitate common power system data repository and associated analytics. We present case studies in delivering two power system decision support applications using this platform. First case study details the experience gained from providing Short Term Load Forecast (webSTLF) service to a leading private electricity distribution company in India. The second case study shares the experiences from delivering a Transmission System Usage Cost and Loss Allocation service (webNetUse) to the electricity regulators and system operators in India. Experience with managing various aspects critical to success of SaaS model like data security, scalability, usability, high availability and disaster recovery is described.",2166-0778;21660778,Electronic:978-1-5090-5643-9; POD:978-1-4673-2318-5; USB:978-0-7695-4770-1,10.1109/SRII.2012.107,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6311073,Power Systems;SaaS;webDNA,Technological innovation,Web services;cloud computing;decision support systems;load forecasting;power system analysis computing;security of data;software architecture,India;SaaS based WebDNA architecture;Web;WebNetUse;data security;disaster recovery;electricity regulators;high availability;in-premise licensed software;loss allocation service;power sector;power system analytics applications;power system data repository;power system decision support tools;private electricity distribution company;scalability;short term load forecast service;software-as-a-service model;system operators;transmission system usage cost;usability;webSTLF,,0,,2,,no,24-27 July 2012,,IEEE,IEEE Conference Publications
Experimental analysis of different optimization techniques on leakage localization using series alignment,Y. Wang; J. Yan; C. Tian; W. Dong; J. Huang,"Urban Natural Resource Analytic, IBM Research - China, Beijing 100193, China","Proceedings of 2012 IEEE International Conference on Service Operations and Logistics, and Informatics",20120820,2012,,,144,149,"Leakage detection and localization system plays crucial role in pipeline technology. Traditionally, negative pressure wave (NPW) based methods have been widely applied for its relatively low construction and maintenance cost. Among these NPW based methods, the slope based method is a major way to estimate leakage position. This paper tries to explore another way in leakage position estimation by aligning two or more NPW signals. In order to provide comprehensive comparison, diverse kinds of optimization methods are used as the optimization engine in searching for the alignment point. Their performance is experimentally compared in multiple NPW alignment tasks to show the strengths and the weaknesses of different kinds optimization algorithms. The experimental results also demonstrate the superiorities of the proposed approach in terms of localization accuracy and algorithm reliability, compared with slope based methods.",,Electronic:978-1-4673-2401-4; POD:978-1-4673-2400-7,10.1109/SOLI.2012.6273520,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6273520,Leakage detection and localization;negative pressure wave;optimization engine;series alignment,Computer aided software engineering;Engines;Optimization methods;Transducers,fault location;leakage currents;optimisation,algorithm reliability;leakage localization;leakage position estimation;localization accuracy;maintenance cost;negative pressure wave;optimization techniques;pipeline technology;series alignment;slope based methods,,0,,13,,no,8-10 July 2012,,IEEE,IEEE Conference Publications
Explicit solution for transcendental equation in power electronics applications,D. G. Castelain; L. O. Seman; A. PÌ©res; S. L. Bertoli; S. V. G. Oliveira,"Regional University of Blumenau - FURB, SC, Brazil",2012 10th IEEE/IAS International Conference on Industry Applications,20130207,2012,,,1,4,This work presents a method to formulate an explicit solution for roots of analytic transcendental equation applied in power electronics circuits. Transcendental equations are extensively used in power electronics modeling and the solution normally adopts numerical approaches. Sometimes the solution is presented by means of graphics or abacus. In this paper a Cauchy's integral theorem based method is presented using only basic concepts of complex integration. This method can be easily applied in mathematical software's or programmable calculators given an exact solution for the problem.,,Electronic:978-1-4673-2411-3; POD:978-1-4673-2412-0,10.1109/INDUSCON.2012.6452903,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6452903,,Accuracy;Calculators;Equations;Integrated circuit modeling;Mathematical model;Numerical models;Power electronics,integral equations;numerical analysis;power electronics,Cauchy's integral theorem based method;abacus presentation;analytic transcendental equation applied;complex integration;graphics presentation;mathematical software;numerical approach;power electronics circuit;programmable calculator,,0,,6,,no,5-7 Nov. 2012,,IEEE,IEEE Conference Publications
Extracting weblog of Siam University for learning user behavior on MapReduce,W. Premchaiswadi; W. Romsaiyud,"Graduate School of Information Technology, Siam University, 38 Petkasem rd., Bangkok, 10160, Thailand",2012 4th International Conference on Intelligent and Advanced Systems (ICIAS2012),20120920,2012,1,,149,154,"MapReduce is a framework that allows developers to write applications that rapidly process and analyze large volumes of data in a massively parallel scale. Moreover, a clickstream is a record of a user's activity on the Internet. Using a clickstream analysis we can collect, analyze, and report aggregate data about which pages visitors visit in what order - and which are the result of the succession of mouse clicks each visitor makes. Clickstream analysis can reveal usage patterns leading to a heightened understanding of users' behavior. In this paper, we introduced a novel and efficient web log mining model for web users clustering. In general, our model consists of three main steps; 1) Computing the similarity measure of any path in a web page, 2) Defining the k-mean clustering for group customerID 3) Generating the report based on the Hadoop MapReduce Framework. Consequently, our experiments were run on real world data derived from weblogs of Siam University at Bangkok, Thailand (www.siam.edu).",,Electronic:978-1-4577-1967-7; POD:978-1-4577-1968-4,10.1109/ICIAS.2012.6306177,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6306177,Clickstream Data;Hadoop Distributed File System (HDFS);K-mean Clustering;MapReduce;Web log Analytics,Artificial intelligence;Computational modeling;Data mining;Educational institutions;File systems;Web pages;Web servers,Web services;computer aided instruction;data analysis;data mining;information retrieval;parallel programming;pattern clustering;public domain software;storage management,Hadoop;Internet;MapReduce;Web log mining model;Web page;Web user clustering;WebLog extraction;clickstream analysis;data analysis;data processing;group customerID;k-mean clustering;similarity measure computing;user behavior learning,,3,,24,,no,12-14 June 2012,,IEEE,IEEE Conference Publications
Forecasting approach of subway tunnels deposition based on ANP-SOM model,Xihua Long; Ningjuan Jia; Wanjun Ye,"Computer College, Xi'an University of Science and Technology, Shanxi China 710054",International Conference on Automatic Control and Artificial Intelligence (ACAI 2012),20130404,2012,,,1073,1076,"The main influencing factors on its nature of complexity and indetermination are analysed deeply in this paper. Super Decisions software of ANP (Network Analytic Hierarchy Process) is used to solve the weights of all the influence factors. Use them as the initial weights to train SOM neural network for the fine-tuning; a new ANP-SOM model which consider to various factors, levels and impact of each other feedback synthetically is pretended. This model is well-adjusted, convergence and precision. It was used in forecasting of Xi'an Tunnel deposition and has achieved a good predicted result.",,,10.1049/cp.2012.1163,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6492770,ANP;Factors influencing;Forecast;SOM neural network;Tunnel deposition,,,,,0,,,,no,3-5 March 2012,,IET,IET Conference Publications
From Business Application Execution to Design Through Model-Based Reporting,T. Holmes,"Software Eng. &amp; Tools, SAP Res., Darmstadt, Germany",2012 IEEE 16th International Enterprise Distributed Object Computing Conference,20121025,2012,,,143,153,"Cross-disciplinary models constitute essential instruments to master complexity. Often it is easier to relate to high-level concepts than to deal with low-level technical details. In model-driven engineering (MDE) models are designated a pivotal role from which systems are generated. As such, MDE enables different stakeholders of business applications to participate in the engineering process. Until now however, MDE does not penetrate phases beyond generation and deployment such as monitoring, analysis, and reporting. To display information from runtime and analytics it would be interesting if reporting could utilize models from design time. Therefore, this paper presents model-based reporting (MbR). Bridging the gap between reporting and design, it enables stakeholders to intuitively specify the reporting through a domain-specific language (DSL) while accelerating development cycles. In non-model-driven settings, MbR can help to introduce models as a first step towards MDE.",1541-7719;15417719,Electronic:978-0-7695-4785-5; POD:978-1-4673-2444-1,10.1109/EDOC.2012.25,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6337245,business applications;end-to-end;model-based;reporting,Abstracts;Business;Context modeling;Correlation;DSL;Data models;Runtime,business data processing;software engineering,DSL;MDE;MbR;business application execution;cross disciplinary models;domain specific language;engineering process;model based reporting;model driven engineering,,1,,24,,no,10-14 Sept. 2012,,IEEE,IEEE Conference Publications
GIS-Based Evaluating and Analysis for Farmland Fertility of Men-hai County,Q. L. Xu; J. H. Yi; K. Yang; L. Y. Li,"Fac. of Tourism & Geographic Sci., Yunnan Normal Univ., Kunming, China",2012 Third Global Congress on Intelligent Systems,20130207,2012,,,373,377,"Through using the ArcGIS software as the platform, and taking DEM analysis, raster calculation and spatial interpolation of spatial analysis technology as the core method, this paper selects Menghai County as the study area, combines with the Delphi method, analytic hierarchy process, fuzzy membership function, farmland Integrated Fertility Index ( IFI ) and other methods, to establish major factors of influence and their weights for the farmland land productivity, as well as built a comprehensive evaluation system and it's spatial computing model. Meanwhile, the analysis such as spatial distribution, area statistics and farmland nutrient analysis for the evaluating results is also discussed in detail. The experimental results and analysis show that, 51575.73 hectares of cultivated land in Menghai County is divided into six grades, which percentages on the total farmland from grade one to grade six respectively are 14.12%, 27.4%, 20.17%, 17.23%, 12.70% and 8.38%, and the level over three grade of cultivated land occupies only less than 60%. As a result, it could be declared that the overall situation of cultivated land fertility is not good, which needs the government to adjust measures to local conditions to maintain the existing quality of cultivated land resources, as well as to enhance the farming ability of cultivated land from level of grade 3, especially grade 4 and 5 to higher level.",2155-6083;21556083,Electronic:978-0-7695-4860-9; POD:978-1-4673-3072-5,10.1109/GCIS.2012.30,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6449557,GIS;IFI;LUCC;evaluation for farmland fertility;fuzzy membership function;spatial analysis;terrain,Analytic hierarchy process;Analytical models;Educational institutions;Geographic information systems;Indexes;Irrigation;Soil,farming;geographic information systems,ArcGIS software;DEM analysis;Delphi method;GIS based evaluation;Men-hai county;Menghai county;analytic hierarchy process;area statistics;comprehensive evaluation system;cultivated land fertility;farming ability;farmland fertility;farmland integrated fertility index;farmland land productivity;farmland nutrient analysis;fuzzy membership function;raster calculation;spatial analysis technology;spatial computing model;spatial distribution;spatial interpolation,,0,,11,,no,6-8 Nov. 2012,,IEEE,IEEE Conference Publications
Goldfish bowl panel: Software development analytics,T. Menzies; T. Zimmermann,"West Virginia University Morgantown, WV, USA",2012 34th International Conference on Software Engineering (ICSE),20120628,2012,,,1032,1033,"Gaming companies now routinely apply data mining to their user data in order to plan the next release of their software. We predict that such software development analytics will become commonplace, in the near future. For example, as large software systems migrate to the cloud, they are divided and sold as dozens of smaller apps; when shopping inside the cloud, users are free to mix and match their apps from multiple vendors (e.g. Google Docs' word processor with Zoho's slide manager); to extend, or even retain, market share cloud vendors must mine their user data in order to understand what features best attract their clients. This panel will address the open issues with analytics. Issues addressed will include the following. What is the potential for software development analytics? What are the strengths and weaknesses of the current generation of analytics tools? How best can we mature those tools?",0270-5257;02705257,Electronic:978-1-4673-1067-3; POD:978-1-4673-1066-6,10.1109/ICSE.2012.6227117,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227117,analytics;empirical software engineering;industry;mining software repositories,Business;Conferences;Data mining;Educational institutions;Programming;Software;Software engineering,cloud computing;data mining;software development management,Google Docs word processor;Zoho slide manager;cloud vendors;data mining;gaming companies;goldfish bowl panel;software development analytics;software systems,,3,,10,,no,2-9 June 2012,,IEEE,IEEE Conference Publications
Guest Editors' Introduction: Studying Professional Software Design,A. Baker; A. van der Hoek; H. Ossher; M. Petre,Visitrend,IEEE Software,20111222,2012,29,1,28,33,"This special issue sets an agenda for research into early software design, and this introduction outlines drivers and issues for that agenda. It argues that looking at software from a design perspective, understanding software as a designed artifact, and considering how design reaches into the whole software life cycle can bring significant benefits both to our understanding of what works in software design and to our approach to tools and practices. The special issue presents outputs from an NSF-funded workshop on 'Studying Professional Software Design' held in 2010 at UC Irvine in which participants analyzed the same professional design sessions from different analytic perspectives. The workshop dialogues provide an example of what's critically needed to drive this research agenda: empirically grounded dialogues between research and practitioners.",0740-7459;07407459,,10.1109/MS.2011.155,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6111365,design;software design;studying professional software design,Product development;Product life cycle management;Software design;Software development management;Software testing;Special issues and sections,,,,6,,15,,no,Jan.-Feb. 2012,,IEEE,IEEE Journals & Magazines
How software engineering can benefit from traditional industries ‰ÛÓ A practical experience report (Invited industrial talk),T. Sprenger,"AdNovum Informatik, Zurich, Switzerland",2012 34th International Conference on Software Engineering (ICSE),20120628,2012,,,1000,1000,"To be competitive in today's market, the IT industry faces many challenges in the development and maintenance of enterprise information systems. Engineering these largescaled systems efficiently requires making decisions about a number of issues. In addition, customers expectations imply continuous software delivery in predictable quality. The operation such systems demands for transparency of the software in regard to lifecycle, change and incident management as well as cost efficiency. Addressing these challenges, we learned how to benefit from traditional industries. Contrary to the fact that the IT business calls itself gladly an industry, the industrialization of software engineering in most cases moves on a rather modest level. Industrialization means not only to build a solution or product on top of managed and well-defined processes, but also to have access to structured information about the current conditions of manufacturing at any time. Comparably with test series and assembly lines of the automobile industry, each individual component and each step from the beginning of manufacturing up to the final product should be equipped with measuring points for quality and progress. Even one step further the product itself, after it has left the factory, should be able to continuously provide analytic data for diagnostic reasons. Information is automatically collected and builds the basic essentials for process control, optimization and continuous improvement of the software engineering process. This presentation shows by means of a practical experience report how AdNovum managed to build its software engineering based on a well-balanced system of processes, continuous measurement and control ‰ÛÓ as well as a healthy portion of pragmatism. We implemented an efficient and predictable software delivery pipeline based on five cornerstones that enables us to ship more than 1500 customer deliveries per year.",0270-5257;02705257,Electronic:978-1-4673-1067-3; POD:978-1-4673-1066-6,10.1109/ICSE.2012.6227249,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227249,,,,,,0,,,,no,2-9 June 2012,,IEEE,IEEE Conference Publications
HPC-VMs: Virtual machines in high performance computing systems,A. Reuther; P. Michaleas; A. Prout; J. Kepner,"Comput. &amp; Analytics Group, MIT Lincoln Lab., Lexington, MA, USA",2012 IEEE Conference on High Performance Extreme Computing,20130110,2012,,,1,6,"The concept of virtual machines dates back to the 1960s. Both IBM and MIT developed operating system features that enabled user and peripheral time sharing, the underpinnings of which were early virtual machines. Modern virtual machines present a translation layer of system devices between a guest operating system and the host operating system executing on a computer system, while isolating each of the guest operating systems from each other. <sup>1</sup> In the past several years, enterprise computing has embraced virtual machines to deploy a wide variety of capabilities from business management systems to email server farms. Those who have adopted virtual deployment environments have capitalized on a variety of advantages including server consolidation, service migration, and higher service reliability. But they have also ended up with some challenges including a sacrifice in performance and more complex system management. Some of these advantages and challenges also apply to HPC in virtualized environments. In this paper, we analyze the effectiveness of using virtual machines in a high performance computing (HPC) environment. We propose adding some virtual machine capability to already robust HPC environments for specific scenarios where the productivity gained outweighs the performance lost for using virtual machines. Finally, we discuss an implementation of augmenting virtual machines into the software stack of a HPC cluster, and we analyze the affect on job launch time of this implementation.",,Electronic:978-1-4673-1576-0; POD:978-1-4673-1577-7,10.1109/HPEC.2012.6408668,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6408668,high performance computing;virtual machines,Cloud computing;Hardware;Operating systems;Productivity;Servers;Virtual machine monitors;Virtual machining,operating systems (computers);virtual machines,HPC cluster;HPC-VM;IBM;MIT;business management systems;email server farms;guest operating system;high performance computing systems;host operating system;operating system features;server consolidation;service migration;service reliability;translation layer;virtual deployment environments;virtual machines,,3,,9,,no,10-12 Sept. 2012,,IEEE,IEEE Conference Publications
Hybrid ANP: Quality attributes decision modeling of a product line architecture design,I. M. Murwantara,"Informatics Department, Universitas Pelita Harapan, Tangerang, Indonesia",2012 2nd International Conference on Uncertainty Reasoning and Knowledge Engineering,20121004,2012,,,30,34,"Architecture design of a software product line includes a lot of decision of how the variability component will be implemented. The decision corresponds to the requirements of the architecture specifications. Furthermore, most of the decision may have dependency between components. However, only some research devoted to address the quality attributes. In this paper, we propose a new approach that hybrid the Analytical Network Process to model the decision, which addressed the quality attributes. In our method, first, the software component derives from the feature model by the domain expert. Then, groups the components to specific quality attributes. Subsequently, models the dependency of quality attributes in the ANP model. We use a eLearning Product Line Architecture to demonstrate the feasibility of our approach.",,Electronic:978-1-4673-1460-2; POD:978-1-4673-1459-6,10.1109/URKE.2012.6319572,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6319572,Analytic Network Process;Architecture Design;Decision Model;Software Product Line,Computer architecture;Decision making;Electronic learning;Reliability;Security;Software;Software architecture,computer aided instruction;decision making;feature extraction;object-oriented methods;software architecture;software quality,analytical network process;decision model;e-learning product line architecture;hybrid ANP model;quality attributes;quality attributes decision modeling;software component;software product line architecture design,,0,,12,,no,14-15 Aug. 2012,,IEEE,IEEE Conference Publications
ICT Solution for Managing Electronic Health Record in India,D. Palasamudram; S. Avinash,"Infosys Ltd., Bangalore, India",2012 Third International Conference on Services in Emerging Markets,20130225,2012,,,65,74,"Government of India is planning to spend 2.5 percent of GDP in Public Healthcare during Twelfth Five Year Plan (2012-2017). It has been well recognized by the Government that IT would play a major role in achieving the intended health goals and improving the overall health status and services in India. Individuals' health data is the key source for Government to assess the healthcare status of the country. This paper proposes an ICT (Information Communication and Technology) solution to capture, manage and track individual's health data in a phased manner. This health data will be exposed to various health management information systems for further usage and analysis. Analytics on data will help in tracking health goals against measurable targets, real time disease surveillance, in pattern analysis and in identifying community needs of healthcare services. Proposed scalable, reliable and highly economical solution is primarily based on open source software and commodity hardware.",,Electronic:978-0-7695-4937-8; POD:978-1-4673-5729-6,10.1109/ICSEM.2012.17,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6468181,Analytics;Disease Surveillance;EHR;Hadoop;Healthcare;ICT;Real Time,Diseases;Government;Pediatrics;Portals;Public healthcare,government;health care;medical information systems,GDP;ICT solution;India government;commodity hardware;electronic health record;health goals;health management information systems;healthcare services;individual health data;measurable targets;open source software;overall health status;pattern analysis;public healthcare;real time disease surveillance,,1,,31,,no,12-15 Dec. 2012,,IEEE,IEEE Conference Publications
Improved Estimation of Weibull Parameters Considering Unreliability Uncertainties,A. Fernandez; M. Vazquez,"EUIT Telecomunicaci&#x00F3;n, Universidad Polit&#x00E9;cnica de Madrid, Spain",IEEE Transactions on Reliability,20120301,2012,61,1,32,40,"We propose a linear regression method for estimating Weibull parameters from life tests. The method uses stochastic models of the unreliability at each failure instant. As a result, a heteroscedastic regression problem arises that is solved by weighted least squares minimization. The main feature of our method is an innovative s-normalization of the failure data models, to obtain analytic expressions of centers and weights for the regression. The method has been Monte Carlo contrasted with Benard's approximation, and Maximum Likelihood Estimation; and it has the highest global scores for its robustness, and performance.",0018-9529;00189529,,10.1109/TR.2011.2168652,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6036203,Heteroscedastic regression;Weibull distributions;life tests;weighted least squares,Approximation methods;Data models;Maximum likelihood estimation;Reliability;Software;Uncertainty,Monte Carlo methods;Weibull distribution;failure analysis;life testing;maximum likelihood estimation;regression analysis;reliability theory;stochastic processes,Benards approximation;Monte Carlo method;Weibull parameter estimation;failure data model;heteroscedastic regression problem;life test;linear regression method;maximum likelihood estimation;s-normalization;stochastic model;unreliability uncertainties;weighted least squares minimization,,1,,30,,no,12-Mar,,IEEE,IEEE Journals & Magazines
Incorporating hardware trust mechanisms in Apache Hadoop: To improve the integrity and confidentiality of data in a distributed Apache Hadoop file system: An information technology infrastructure and software approach,J. C. Cohen; S. Acharya,"Towson University/Hewlett Packard Company, Department of Computer and Information Sciences, Towson, USA",2012 IEEE Globecom Workshops,20130314,2012,,,769,774,"Pairing Apache Hadoop distributed file storage with hardware based trusted computing mechanisms has the potential to reduce the risk of data compromise. With the growing use of Hadoop to tackle big data analytics involving sensitive data, a Hadoop cluster could be a target for data exfiltration, corruption, or modification. By implementing open standards based Trusted Computing technology at the infrastructure and application levels; a novel and robust security posture and protection is presented. An overview of the technologies involved, description of the proposed infrastructure, and potential software integrations are discussed.",2166-0077;21660077,Electronic:978-1-4673-4941-3; POD:978-1-4673-4942-0; USB:978-1-4673-4940-6,10.1109/GLOCOMW.2012.6477672,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6477672,HDFS;Hadoop;Trusted Computing,Computer architecture;Cryptography;File systems;Hardware;Standards,data analysis;distributed processing;storage management;trusted computing,Apache Hadoop distributed file storage;data analytics;data exfiltration;distributed Apache Hadoop file system;hardware based trusted computing mechanisms;hardware trust mechanisms;information technology infrastructure;robust security posture;software integrations,,3,,15,,no,3-7 Dec. 2012,,IEEE,IEEE Conference Publications
Infographics at the Congressional Budget Office,J. A. Schwabish,,2012 IEEE Conference on Visual Analytics Science and Technology (VAST),20130103,2012,,,241,242,"The Congressional Budget Office (CBO) is an agency of the federal government with about 240 employees that provides the U.S. Congress with timely, nonpartisan analysis of important budgetary and economic issues. Recently, CBO began producing static infographics to present its headline stories and to provide information to the Congress in different ways.",,Electronic:978-1-4673-4753-2; POD:978-1-4673-4752-5,10.1109/VAST.2012.6400533,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6400533,data visualization;federal budget;federal government;infographics,Data visualization;Economics;Government;Legislation;Presses;Software,budgeting;computer graphics;government data processing,CBO;Congressional Budget Office;employee;federal government;static infographics,,0,,2,,no,14-19 Oct. 2012,,IEEE,IEEE Conference Publications
InfoMaps : A Session Based Document Visualization and Analysis Tool,S. Kolman; A. S. Dufilie; S. K. Anbalagan; G. Grinstein,"Inst. for Visualization & Perception Res., Univ. of Massachusetts Lowell, Lowell, MA, USA",2012 16th International Conference on Information Visualisation,20120906,2012,,,274,282,"InfoMaps is an information visualization tool designed for personal information management and for supporting data analysis. In this paper we briefly discuss the design of InfoMaps and explain its role in finding relevant information. InfoMaps is tightly coupled with Weave, an open source framework, providing a set of data analysis and visualization tools. Weave's framework is built with session states as its core and this provides InfoMaps the ability to store the entire user's interactions as well as visualization layouts. We discuss the implications of using the Weave framework with InfoMaps and its relevance to the field of information retrieval and visual analytics.",1550-6037;15506037,Electronic:978-0-7695-4771-8; POD:978-1-4673-2260-7,10.1109/IV.2012.54,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6295826,infomaps;information retrieval;viri;visual information retrieval interface,Browsers;Data visualization;Image color analysis;Layout;Servers;Visualization;Weaving,data analysis;data visualisation;design engineering;document handling;public domain software,InfoMaps design;Weave framework;data analysis;information retrieval;information visualization tool;open source framework;personal information management;session based document analysis tool;session based document visualization;visual analytics,,0,,29,,no,11-13 July 2012,,IEEE,IEEE Conference Publications
Information needs for software development analytics,R. P. L. Buse; T. Zimmermann,"The University of Virginia, USA",2012 34th International Conference on Software Engineering (ICSE),20120628,2012,,,987,996,"Software development is a data rich activity with many sophisticated metrics. Yet engineers often lack the tools and techniques necessary to leverage these potentially powerful information resources toward decision making. In this paper, we present the data and analysis needs of professional software engineers, which we identified among 110 developers and managers in a survey. We asked about their decision making process, their needs for artifacts and indicators, and scenarios in which they would use analytics. The survey responses lead us to propose several guidelines for analytics tools in software development including: Engineers do not necessarily have much expertise in data analysis; thus tools should be easy to use, fast, and produce concise output. Engineers have diverse analysis needs and consider most indicators to be important; thus tools should at the same time support many different types of artifacts and many indicators. In addition, engineers want to drill down into data based on time, organizational structure, and system architecture.",0270-5257;02705257,Electronic:978-1-4673-1067-3; POD:978-1-4673-1066-6,10.1109/ICSE.2012.6227122,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227122,,Complexity theory;Decision making;Guidelines;Measurement;Programming;Software;Software engineering,data analysis;software engineering,data analysis;data rich activity;decision making;organizational structure;powerful information resources;software development analytics;software engineers;sophisticated metrics;system architecture,,28,,44,,no,2-9 June 2012,,IEEE,IEEE Conference Publications
Innovation-Related Enterprise Semantic Search: The INSEARCH Experience,R. Basili; A. Stellato; P. Daniele; P. Salvatore; J. Wurzer,"Dept. of Enterprise Eng., Univ. of Roma, Rome, Italy",2012 IEEE Sixth International Conference on Semantic Computing,20121025,2012,,,194,201,"Innovation is a crucial process for enterprizes and pushes for strict requirements towards semantic technologies. Large scale and timely search processes on the Web are here often involved in different business analytics tasks. In the European project INSEARCH, an advanced information retrieval system has been developed integrating robust semantic technologies and industry-standard software architectures for Web monitoring and alerting, proactive search as well as personalized domain-specific classification and ranking functionalities.",,Electronic:978-0-7695-4859-3; POD:978-1-4673-4433-3,10.1109/ICSC.2012.61,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6337104,Enterprise Innovation;Intelligent Document Management;Language Processing;Semantic Enterprise Search,Patents;Resource description framework;Semantics;Servers;Standards;Technological innovation;Vectors,Internet;business data processing;information retrieval;pattern classification;software architecture;standards,European project INSEARCH experience;Web alerting;Web monitoring;advanced information retrieval system;business analytics tasks;industry-standard software architectures;innovation-related enterprise semantic search;personalized domain-specific classification;proactive search;ranking functionality;robust semantic technologies,,0,,20,,no,19-21 Sept. 2012,,IEEE,IEEE Conference Publications
Intelligent automation and controls of power industry Microgrid solutions,R. Chudgar; J. Jennings,"Power Analytics, 9208 Falls of Neuse Rd, Suite 215, Raleigh, NC 27615, USA",World Automation Congress 2012,20121004,2012,,,1,5,"Power is one of the fundamentals in developing and developed nations world-wide. Over 80% of the world's population does not have reliable power. Without guaranteed power, nations quickly lose capabilities, efficiencies, and other critical infrastructure to their people and livelihood. The barrier for reliable power, in the past was the large investment requirement for centralized bulk generation. Additionally, the belief was that without large centralized power, the nation would not get efficiencies needed to provide reliable power. Across the world today, still many countries have inconsistent power and see rolling brown or black outs hourly or daily. At best developed countries have, reliable and secure power that is centrally managed through a bulk power grid and sometime governed by central government or large corporations. However with the advancement of distributed generation, substation automation, Microgrid (‰ÛÏMG‰Ûù) control software, the industry is going through a fundamental change. The change is bringing on smaller Microgrid pockets, smaller generation due with less capital investment, and higher efficiency solutions that can compete neck and neck with large bulk generation. All of these items, while critical for the new ‰ÛÏWorld Power Infrastructure‰Ûù still lack some basic building blocks for the transformation to take its final shape. This paper will describe some of the concepts, barriers, lessons learned, and next steps to move from either unreliable Power structures or Central Bulk Power to Microgrid solutions with Distributed Automated Intelligent Power solutions.",2154-4824;21544824,Electronic:978-1-889334-47-9; POD:978-1-4673-4497-5,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6320995,Automation;Controls;Microgrid;Power Generation;Virtual Power plants;co-optimization;communications;distributed generation;frequency droop,,,,,0,,9,,no,24-28 June 2012,,IEEE,IEEE Conference Publications
Interactive visual exploration of simulator accuracy: A case study for stochastic simulation algorithms,M. Luboschik; S. Rybacki; R. Ewald; B. Schwarze; H. Schumann; A. M. Uhrmacher,"Albert Einstein Str. 22, University of Rostock, 18059 Rostock, Germany",Proceedings of the 2012 Winter Simulation Conference (WSC),20130221,2012,,,1,12,"Visual Analytics offers various interesting methods to explore high dimensional data interactively. In this paper we investigate how it can be applied to support experimenters and developers of simulation software conducting simulation studies. In particular the usage and development of approximate simulation algorithms poses several practical problems, e.g., estimating the impact of algorithm parameters on accuracy or detecting faulty implementations. To address some of those problems, we present an approach that allows to relate configurations and accuracy visually and exploratory. The approach is evaluated by a brief case study, focusing on the accuracy of Stochastic Simulation Algorithms.",0891-7736;08917736,Electronic:978-1-4673-4782-2; POD:978-1-4673-4779-2; USB:978-1-4673-4781-5,10.1109/WSC.2012.6465190,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6465190,,Accuracy;Analytical models;Computational modeling;Data models;Data visualization;Trajectory;Visualization,algorithm theory;data visualisation;simulation,high dimensional data;interactive visual exploration;simulation software;simulator accuracy;stochastic simulation algorithm;visual analytics,,1,,33,,no,9-12 Dec. 2012,,IEEE,IEEE Conference Publications
Interfstud electromagnetic interference software: An accurate evaluation of current distribution in soil and in underground pipelines,D. D. Micu; L. Czumbil; M. PrÅÁa; K. KasaÅÁ-LaÅ_eti€à,"Electrical Engineering Department, Technical University of Cluj-Napoca, Romania",International Symposium on Electromagnetic Compatibility - EMC EUROPE,20121231,2012,,,1,5,"A user friendly software application was developed to study the electromagnetic interference problem between high voltage power lines and underground metallic pipelines. The dedicated developed InterfStud software was based also on the development of the magnetic vector potential and current distribution within the soil formulas. The best way to investigate the soil behavior as a conducting media is to determine the current distribution within the soil. New analytical formulas for the induced magnetic vector potential and current distribution in soil are derived. The determined formulas contain semi-infinite integral terms which will be evaluated. We might seek approximations of the semi-infinite integrals by replacing an exponential or algebraic function, the objective being to permit analytic integration. Since there is no good systematic method for making these replacements, their success depends directly on the intuition and ingenuity, taking into account that in practice the integrand has limited accuracy.",2325-0356;23250356,Electronic:978-1-4673-0717-8; POD:978-1-4673-0718-5,10.1109/EMCEurope.2012.6396894,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6396894,electromagnetic field;high voltage power line;metallic pipelines;semi-infinite integrals,Conductors;Current density;Earth;Pipelines;Software;Soil;Vectors,algebra;current distribution;electromagnetic interference;high-voltage engineering;integral equations;integration;pipelines;power engineering computing;power transmission lines;soil,Interfstud electromagnetic interference software;algebraic function;analytic integration;conducting media;current distribution evaluation;electromagnetic interference problem;exponential function;high voltage power line;integrand;magnetic vector potential;semiinfinite integral term;soil behavior;soil current distribution;underground metallic pipeline;underground pipelines;user friendly software application,,0,,8,,no,17-21 Sept. 2012,,IEEE,IEEE Conference Publications
Introduction to Geometric Processing through Optimization,G. Taubin,Brown University,IEEE Computer Graphics and Applications,20120810,2012,32,4,88,94,"As an introduction to the field, this article shows how to formulate several geometry-processing operations to solve systems of equations in the&amp;amp;#x201C;least-squares&amp;amp;#x201D; sense. The equations are derived from local geometric relations using elementary concepts from analytic geometry, such as points, lines, planes, vectors, and polygons. Simple and useful tools for interactive polygon mesh editing result from the most basic descent strategies to solve these optimization problems. Throughout the article, the author develops the mathematical formulations incrementally, keeping in mind that the objective is to implement simple software for interactive editing applications that works well in practice. Readers can implement higher-performance versions of these algorithms by replacing the simple solvers proposed here with more advanced ones.",0272-1716;02721716,,10.1109/MCG.2012.80,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6265062,Geometry;Mesh networks;Optimization;Smoothing methods;geometric optimization;mesh;polygon;smoothing,Geometry;Mesh networks;Optimization;Smoothing methods,,,,1,,,,no,July-Aug. 2012,,IEEE,IEEE Journals & Magazines
Investigating network traffic through compressed graph visualization: VAST 2012 Mini Challenge 2 award: ‰ÛÏGood adaptation of graph analysis techniques‰Ûù,L. Shi; Q. Liao; C. Yang,"State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences",2012 IEEE Conference on Visual Analytics Science and Technology (VAST),20130103,2012,,,279,280,"Compressed Graph Visualization is a visual analytics method to scale the traditional node-link representation to huge graphs. This paper introduces its visualization, data processing and visual analytics process in solving Mini-Challenge 2 of VAST 2012 contest.",,Electronic:978-1-4673-4753-2; POD:978-1-4673-4752-5,10.1109/VAST.2012.6400517,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6400517,,,,,,1,,2,,no,14-19 Oct. 2012,,IEEE,IEEE Conference Publications
iPresage: An innovative patent landscaping tool,V. Avasarala; P. Bonissone,"Machine Learning Lab, GE Global Research, Schenectady, NY, USA",2012 IEEE Congress on Evolutionary Computation,20120802,2012,,,1,7,"iPresage is a web-based interactive tool for rapid and scalable patent landscaping. iPresage relies on sophisticated text mining, statistical machine learning and computational intelligence algorithms to allow analysts to mine large patent databases and evaluate whitespaces and temporal trends. This paper describes iPresage algorithms in detail and showcases iPresage functionality, using an illustrative example.",1089-778X;1089778X,Electronic:978-1-4673-1509-8; POD:978-1-4673-1510-4,10.1109/CEC.2012.6256503,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6256503,Bayesian Models;Evolutionary Computation;Latent Dirchlet Allocation;Multi-Dimensional Scaling,Algorithm design and analysis;Evolutionary computation;Patents;Resource management;Software;Vectors;Visualization,Internet;data mining;database management systems;learning (artificial intelligence);patents;statistical analysis;text analysis,Web-based interactive tool;computational intelligence algorithms;iPresage algorithms;iPresage functionality;innovative patent landscaping tool;large patent database mining;rapid patent landscaping;scalable patent landscaping;sophisticated text mining;statistical machine learning;temporal trends,,0,,13,,no,10-15 June 2012,,IEEE,IEEE Conference Publications
Keynote abstracts,I. Stojmenovic; Jian Pei; K. Iwama; Zhiwei Xu; Wanlei Zhou,"Univ. of Ottawa, Ottawa, ON, Canada","2012 13th International Conference on Parallel and Distributed Computing, Applications and Technologies",20130909,2012,,,xxii,xxviii,The paper provides a list of keynote speeches that involves the following: IoT/CPS with Sensors and Robots: Actuation Challenges; Data Analytics as Challenging Parallel and Distributed Computing Applications; Parameterized Testability; New Information Technology Utilizing Human-Cyber-Physical Resources; Dealing with Worm Propagation: Modelling and Defence Strategies.,2379-5352;23795352,Electronic:978-0-7695-4879-1; POD:978-1-4673-5704-3,10.1109/PDCAT.2012.138,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6589231,,,Internet of Things;actuators;data analysis;distributed processing;invasive software;robots;sensors,CPS;Internet of Things;IoT;actuation challenge;cyber-physical system;data analytics;distributed computing;information technology;parallel computing;parameterized testability;robot;sensors;worm defence strategy;worm modelling strategy;worm propagation,,0,,,,no,14-16 Dec. 2012,,IEEE,IEEE Conference Publications
"Large Scale Video Analytics: On-demand, iterative inquiry for moving image research",V. Kuhn; R. Arora; A. Craig; K. Franklin; M. Simeone; D. Bock; L. Marini,"USC, Los Angeles, CA, USA",2012 IEEE 8th International Conference on E-Science,20130110,2012,,,1,5,"Video is exploding as a means of communication and expression, and the resultant archives are massive, disconnected datasets. Thus, scholars' ability to research this crucial aspect of contemporary culture is severely hamstrung by limitations in semantic image retrieval, incomplete metadata, and the lack of a precise understanding of the actual content of any given archive. Our aim in the Large Scale Video Analytics (LSVA) project is to address obstacles in both image-retrieval and research that uses extreme-scale archives of video data that employs a human-machine hybrid process for analyzing moving images. We propose an approach that 1) places more interpretive power in the hands of the human user through novel visualizations of video data, and 2) uses a customized on-demand configuration that enables iterative queries.",,Electronic:978-1-4673-4466-1; POD:978-1-4673-4467-8,10.1109/eScience.2012.6404446,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6404446,High Performance Computing;Image Edge Detection;Image Retrieval;Multimedia Databases;Software;Visualization,Biomedical imaging;Data visualization;Films;Image retrieval;Motion pictures;Streaming media,image retrieval;iterative methods;video signal processing,LSVA;disconnected datasets;image research;image retrieval;incomplete metadata;interpretive power;iterative inquiry;iterative queries;large scale video analytics;semantic image retrieval;video data;video data visualizations,,1,,11,,no,8-12 Oct. 2012,,IEEE,IEEE Conference Publications
Link datarate based admission control in wireless networks,K. Narendran; R. M. Karthik; K. M. Sivalingam,"Centre of Excellence in Wireless Technology, Chennai, 600113, India",2012 IEEE International Conference on Advanced Networks and Telecommunciations Systems (ANTS),20130606,2012,,,38,43,"This paper presents a link data-rate based admission control mechanism for a wireless network using a transmit power allocation algorithm. The admission control mechanism uses the nodes' link datarate requirements as an input parameter. Transmit power control enables the nodes to set their power levels such that the required link datarate requirements are met while minimizing the interference caused to other transmissions. The link datarate requirement of a node is converted to its signal strength requirement that is subsequently used for admission control. We establish the relationship between key parameters in admission control such as signal strength requirement, link datarate requirement, number of nodes in the system etc. The proposed algorithm is studied using analytic and simulation techniques to show that it enables the nodes to meet their respective link datarate requirements.",2153-1676;21531676,Electronic:978-1-4673-5132-4; POD:978-1-4673-5130-0,10.1109/ANTS.2012.6524225,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6524225,,,minimisation;power control;radiofrequency interference;telecommunication congestion control;wireless channels,admission control;interference minimization;nodes link datarate requirements;power allocation;power control;signal strength requirement;wireless networks,,1,,10,,no,16-19 Dec. 2012,,IEEE,IEEE Conference Publications
Log Analytics for Dependable Enterprise Telephony,C. Chen; N. Singh; S. Yajnik,"Google Inc., Mountain View, CA, USA",2012 Ninth European Dependable Computing Conference,20120611,2012,,,94,101,"Enterprise telephony servers for large enterprises in business sectors like finance and healthcare, are complex software systems that require at least five nines of availability. Such systems need guaranteed service with minimal service disruption during failures and are built with availability designed into various aspects of operation. One of the approaches used to improve availability is detecting and predicting failures through analysis of system traces. System trace/debug logs are very textual in nature and as such are not amenable to a fully automated analysis. In this work we develop a log analysis technique for analyzing trace logs of large enterprise telephony systems. We analyze the logs to define normal and failure operational states of a running system and then categorize the current state of the system based on previously learned states. Our analysis shows that the technique is successful in correctly identifying the failure states of the system.",,Electronic:978-0-7695-4671-1; POD:978-1-4673-0938-7,10.1109/EDCC.2012.14,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6214764,,Availability;IP networks;Manuals;Servers;Software;Telephony;Time series analysis,business communication;computer telephony integration,business sectors;complex software systems;debug logs;dependable enterprise telephony;enterprise telephony servers;finance;healthcare;log analytics;system trace analysis,,2,,19,,no,8-11 May 2012,,IEEE,IEEE Conference Publications
Low Power Surge: Advanced Base-Station Demand Impact on RF Components [MicroBusiness],S. Entwistle; A. Anwar; C. Taylor; E. Higham,"Strategy Analytics in Milton Keynes, U.K.",IEEE Microwave Magazine,20120312,2012,13,2,14,21,"The cellular base-station market is entering a time of considerable change. Despite the worldwide macroeconomic outlook, conditions are favorable for continuing growth, particularly in emerging markets, with the market for base-station radio components set to top US$5.4 billion worldwide by 2015. Mobile service providers and equipment vendors are moving toward smaller cells and improving spectral efficiency with new air interfaces such as LTE. Advances in technology are opening up the market to a host of new players, posing a substantial threat to the more established semiconductor firms and subsystem equipment vendors that have dominated this space for so long. New semiconductor materials and techniques for improving linearity and efficiency could potentially serve as the Trojan horses that allow new players to enter the base-station component market and undermine the current market leaders.",1527-3342;15273342,,10.1109/MMM.2011.2181604,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6167601,,"Base stations;Cellular networks;Components, packaging, and manufacturing technology;Economics;Marketing and sales;Supply and demand;Surge protection;Telecommunication services",cellular radio;invasive software;macroeconomics;radio equipment;radio stations;semiconductor industry,Trojan horses;base station component market;base station radio components;cellular base station market;equipment vendors;macroeconomic;mobile service providers;semiconductor firms,,0,,1,,no,March-April 2012,,IEEE,IEEE Journals & Magazines
MAAP: Mission Assurance Analytics Platform,T. LlansÌ_; P. A. Hamilton; M. Silberglitt,"Johns Hopkins University, Applied Physics Laboratory, Laurel, Maryland 20723, USA",2012 IEEE Conference on Technologies for Homeland Security (HST),20130214,2012,,,549,555,"This paper describes the Mission Assurance Analytics Platform (MAAP), an open, experimental software framework that provides analysts with an environment for systematically studying the link between cyber attack and the resulting impact on operational missions that are supported by a cyber system. MAAP directly informs both risk decisions and mitigation prioritization.",,Electronic:978-1-4673-2709-1; POD:978-1-4673-2708-4,10.1109/THS.2012.6459908,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6459908,Analytics;Architecture;Cyber;Framework;MBA;MIRA;Mission;Mission Assurance;Mitigations;Risk,Analytical models;Computer architecture;Data models;Data visualization;Databases;Heating;Security,risk analysis;security of data,MAAP;cyber attack;cyber system;experimental software framework;mission assurance analytics platform;open experimental software framework;operational missions;risk decisions;risk mitigation prioritization,,3,,8,,no,13-15 Nov. 2012,,IEEE,IEEE Conference Publications
Malware Analysis and attribution using Genetic Information,A. Pfeffer; C. Call; J. Chamberlain; L. Kellogg; J. Ouellette; T. Patten; G. Zacharias; A. Lakhotia; S. Golconda; J. Bay; R. Hall; D. Scofield,"Charles River Analytics, USA",2012 7th International Conference on Malicious and Unwanted Software,20130214,2012,,,39,45,"As organizations become ever more dependent on networked operations, they are increasingly vulnerable to attack by a variety of attackers, including criminals, terrorists and nation states using cyber attacks. New malware attacks, including viruses, Trojans, and worms, are constantly and rapidly emerging threats. However, attackers often reuse code and techniques from previous attacks. Both by recognizing the reused elements from previous attacks and by detecting patterns in the types of modification and reuse observed, we can more rapidly develop defenses, make hypotheses about the source of the malware, and predict and prepare to defend against future attacks. We achieve these objectives in Malware Analysis and Attribution using Genetic Information (MAAGI) by adapting and extending concepts from biology and linguistics. First, analyzing the ‰ÛÏgenetics‰Ûù of malware (i.e., reverse engineered representations of the original program) provides critical information about the program that is not available by looking only at the executable program. Second, the evolutionary process of malware (i.e., the transformation from one species of malware to another) can provide insights into the ancestry of malware, characteristics of the attacker, and where future attacks might come from and what they might look like. Third, functional linguistics is the study of the intent behind communicative acts; its application to malware characterization can support the study of the intent behind malware behaviors. To this point in the program, we developed a system that uses a range of reverse engineering techniques, including static, dynamic, behavioral, and functional analysis that clusters malware into families. We are also able to determine the malware lineage in some situations. Using behavioral and functional analysis, we are also able to identify a number of functions and purposes of malware.",,DVD:978-1-4673-4878-2; Electronic:978-1-4673-4879-9; POD:978-1-4673-4880-5,10.1109/MALWARE.2012.6461006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6461006,,Clustering algorithms;Genetics;Malware;Reverse engineering;Semantics;Software;Software algorithms,genetic algorithms;invasive software;reverse engineering,MAAGI;attackers;criminals;critical information;cyber attacks;executable program;functional linguistics;malware analysis and attribution using genetic information;malware species;nation states;networked operations;reverse engineering techniques;terrorists,,2,,13,,no,16-18 Oct. 2012,,IEEE,IEEE Conference Publications
Measuring influence in social networks using a network amplification score - an analysis using cloud computing,A. R. Hussain; M. A. Hameed; S. F. Sayeedunnissa,"R&amp;D, Host Analytics Software Pvt. Ltd., Hyderabad, India",2012 12th International Conference on Hybrid Intelligent Systems (HIS),20130128,2012,,,396,401,"Information shared on social networks can be attributed to various factors such as common interests, innovation in areas of job role, etc. The propagation of information or its reach in a social network is directly related to the influence of the author. Traditional influence scoring algorithms rely only on the number of direct connections of a user or on a ratio of their connections. As proved by recent research, a focus solely on the connection between users skews an understanding of how influence operates and follows. In this paper we differentiate between passive and active information. Information is classified as active if it promotes engagement through conversation and is thus exposed to more members, increasing the reach of information. Using a large amount of social network data we analyze the different components of information generated by a user, its propagation within the network and derive a metric for calculating the user's influence score. This influence score considers both the passive content and the active conversation aspect of the shared information. The score relies on the network metric degree of centrality. The measure of inbound and outbound information for 2 levels in the network, provide insight to the amplification of the reach of the shared information.",,Electronic:978-1-4673-5116-4; POD:978-1-4673-5114-0,10.1109/HIS.2012.6421367,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6421367,cloud computing;conversation;influence;social network;twitter,Decision support systems;Helium;Hybrid intelligent systems,cloud computing;social networking (online);social sciences computing,active conversation;active information;cloud computing;influence;influence scoring algorithms;network amplification score;network metric degree;passive content;passive information;social network data,,0,,17,,no,4-7 Dec. 2012,,IEEE,IEEE Conference Publications
Memory Access Control in Multiprocessor for Real-Time Systems with Mixed Criticality,H. Yun; G. Yao; R. Pellizzoni; M. Caccamo; L. Sha,"Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA",2012 24th Euromicro Conference on Real-Time Systems,20120806,2012,,,299,308,"Shared resource access interference, particularly memory and system bus, is a big challenge in designing predictable real-time systems because its worst case behavior can significantly differ. In this paper, we propose a software based memory throttling mechanism to explicitly control the memory interference. We developed analytic solutions to compute proper throttling parameters that satisfy schedulability of critical tasks while minimize performance impact caused by throttling. We implemented the mechanism in Linux kernel and evaluated isolation guarantee and overall performance impact using a set of synthetic and real applications.",1068-3070;10683070,Electronic:978-0-7695-4739-8; POD:978-1-4673-2032-0,10.1109/ECRTS.2012.32,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6257581,,Delay;Equations;Interference;Mathematical model;Memory management;Real time systems;Time factors,Linux;authorisation;multiprocessing programs;operating system kernels;real-time systems;scheduling;storage management,Linux kernel;memory access control;memory bus;mixed criticality;multiprocessor;real-time systems;resource access interference;schedulability;software based memory throttling mechanism;system bus,,32,,23,,no,11-13 July 2012,,IEEE,IEEE Conference Publications
Modeling and simulation based study for on-line detection of partial discharge of solid dielectric,V. K. Fasil; S. Karmakar,"Department of Electrical Engineering, National Institute of Technology, Rourkela-769008, India",2012 IEEE 10th International Conference on the Properties and Applications of Dielectric Materials,20121004,2012,,,1,5,"Nowadays electric utilities are facing major problems due to the ageing and deterioration of high voltage (HV) power equipments in their operating service period. There are several solid materials are used in high voltage power system equipments for insulation purpose. The insulators used in HV power equipment always have a small amount of impurity inside it. The impurity is mainly in the form of solid, gas or liquid. In most cases the impurity is in the form of air bubbles (void) which creates a weak zone inside the insulator. Therefore, this void is the reason for the occurrence of partial discharge in high voltage power equipments while sustaining the high voltage. Ageing and deterioration is mainly occurs due to the presence of partial discharge in such insulator used in the high voltage power equipments. The presence of partial discharge for a long period of time is also causes the insulation failure of high voltage equipments used in power system. Therefore, the partial discharge detection and measurement is necessary for prediction and reliable operation of insulation in high voltage power equipments. In this work, to study the on-line detection of partial discharge an epoxy resin is taken as a solid dielectric for simulating and modeling purpose. This epoxy resin with small impurity (air bubble) under high voltage stress creates a source of partial discharge inside the dielectric. The generated partial discharge is continuously detected and monitored by using LabVIEW software. Simulation of real time detection, de-noising and different analytic techniques of partial discharge signal by using LabVIEW software is proposed which gives the real time visualization of partial discharge signal produced inside the high voltage power equipment.",2160-9225;21609225,Electronic:978-1-4673-2851-7; POD:978-1-4673-2852-4,10.1109/ICPADM.2012.6318904,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6318904,Online monitoring;Partial discharge;Solid dielectric,Dielectrics;Discharges (electric);Fault diagnosis;Materials;Partial discharges;Voltage measurement,ageing;bubbles;partial discharges;power apparatus;power engineering computing;virtual instrumentation;voids (solid),LabVIEW software;ageing;air bubbles;deterioration;electric utilities;high voltage power system equipments;insulation failure;modeling based study;on-line detection;simulation based study;solid dielectric;solid dielectric partial discharge;void,,0,,7,,no,24-28 July 2012,,IEEE,IEEE Conference Publications
Modeling of radiowave propagation in tunnels,J. Boksiner; C. Chrysanthos; J. Lee; M. Billah; T. Bocskor; D. Barton; J. Breakall,"Space &amp; Terrestrial Commun. Directorate (S&amp;TCD), US Army RDECOM CERDEC, Aberdeen Proving Ground, MD, USA",MILCOM 2012 - 2012 IEEE Military Communications Conference,20130128,2012,,,1,6,"We investigated propagation of radio frequency (RF) waves in tunnels. An understanding of RF propagation in tunnels is needed to plan for optimal radio communication system performance. We considered tunnels with a square cross section and right-angle junctions. We developed an analytic model of tunnel propagation by considering multiple reflections from tunnel walls and diffraction around junction corners. A comparison of model results with measurements shows a strong correlation between model predictions and measured data. Finally, we briefly describe a software tool that helps users visualize field strength of the propagating field in tunnels.",2155-7578;21557578,Electronic:978-1-4673-1731-3; POD:978-1-4673-1729-0,10.1109/MILCOM.2012.6415864,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6415864,,Antennas;Diffraction;Gain;Junctions;Radio frequency;Receivers,radiocommunication;radiowave propagation;tunnels,data visualization;junction corners;optimal radio communication system performance;propagating fields;radio frequency wave propagation;radiowave propagation modeling;right-angle junctions;software tool;square cross section;tunnel propagation;tunnel walls;tunnels,,1,,13,,no,Oct. 29 2012-Nov. 1 2012,,IEEE,IEEE Conference Publications
Monitoring and Control System used in Microalgae Crop,L. M. Beltran; C. L. Garzon-Castro; F. Garces; M. Moreno,"Univ. de La Sabana, Chia, Colombia",IEEE Latin America Transactions,20120820,2012,10,4,1993,1998,"This paper presents a monitoring and control system developed with the purpose to increase the reproducibility of microalgae production processes, achieving more effectiveness of casual-analytic physiologic research, the verification of process models and the development of bioprocess. It also controls of form web the following variables: temperature and photoperiod, the equipment was tested on a Haematococcus pluvialis crop. The settings of experimentation were: photoperiod 18/6 (light hours/dark) given by the extensions of blue SMD LEDs (380<;ëÈ<;470 nm), the agitation speed of 720 RPM and temperature of 24å¡C. The settings for the controlled crop were the same except the light that, for this case, was natural. It was found that the system is able to control automatically the process variables permitting the continuous monitoring of the experiments, removing barriers of time and place. At an institutional level, the system has brought benefits allowing the decongestion of places as saving time for both students and researchers. At a level of bioprocesses of researching, it was determined that both kinetic of microalgae growth and quantity of astaxanthin that is produced are incremented when the crop is illuminating by short wavelength, blue light.",1548-0992;15480992,,10.1109/TLA.2012.6272485,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6272485,bioprocess;computer applications;microalgae;monitoring and control system,Agriculture;Biomedical monitoring;Control systems;Hardware;Light emitting diodes;Monitoring;Software,biotechnology;crops;microorganisms;production equipment;temperature control,Haematococcus pluvialis crop;agitation speed;astaxanthin;bioprocess development;casual-analytic physiologic research;control system;crop illumination;microalgae crop;microalgae growth;microalgae production processes;monitoring system;photoperiod variable;production equipment;temperature 24 degC;temperature variable,,0,,,,no,12-Jun,,IEEE,IEEE Journals & Magazines
MSR 2012 keynote: Software analytics in practice ‰ÛÓ Approaches and experiences,D. Zhang,Microsoft Research Asia,2012 9th IEEE Working Conference on Mining Software Repositories (MSR),20120625,2012,,,1,1,Summary form only given. A list of the plenary sessions speakers and tracks is given. Following that are abstracts for all full papers published on the original conference proceedings CD.,2160-1852;21601852,Electronic:978-1-4673-1761-0; POD:978-1-4673-1760-3,10.1109/MSR.2012.6224292,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6224292,,,,,,1,,10,,no,2-3 June 2012,,IEEE,IEEE Conference Publications
Multi-level Layout Optimization for Efficient Spatio-temporal Queries on ISABELA-compressed Data,Z. Gong; S. Lakshminarasimhan; J. Jenkins; H. Kolla; S. Ethier; J. Chen; R. Ross; S. Klasky; N. F. Samatova,"North Carolina State Univ., Raleigh, NC, USA",2012 IEEE 26th International Parallel and Distributed Processing Symposium,20120816,2012,,,873,884,"The size and scope of cutting-edge scientific simulations are growing much faster than the I/O subsystems of their runtime environments, not only making I/O the primary bottleneck, but also consuming space that pushes the storage capacities of many computing facilities. These problems are exacerbated by the need to perform data-intensive analytics applications, such as querying the dataset by variable and spatio-temporal constraints, for what current database technologies commonly build query indices of size greater than that of the raw data. To help solve these problems, we present a parallel query-processing engine that can handle both range queries and queries with spatio-temporal constraints, on B-spline compressed data with user-controlled accuracy. Our method adapts to widening gaps between computation and I/O performance by querying on compressed metadata separated into bins by variable values, utilizing Hilbert space-filling curves to optimize for spatial constraints and aggregating data access to improve locality of per-bin stored data, reducing the false positive rate and latency bound I/O operations (such as seek) substantially. We show our method to be efficient with respect to storage, computation, and I/O compared to existing database technologies optimized for query processing on scientific data.",1530-2075;15302075,Electronic:978-0-7695-4675-9; POD:978-1-4673-0975-2,10.1109/IPDPS.2012.83,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6267895,,Bandwidth;Computational modeling;Indexes;Layout;Organizations;Query processing;Splines (mathematics),Hilbert spaces;data compression;database indexing;input-output programs;natural sciences computing;optimisation;parallel databases;query processing;software performance evaluation;splines (mathematics),B-spline compressed data;Hilbert space-filling curves;I-O subsystems;ISABELA-compressed data;data access aggregation;database technologies;multilevel layout optimization;parallel query-processing engine;query indices;scientific simulations;spatio-temporal queries,,4,,41,,no,21-25 May 2012,,IEEE,IEEE Conference Publications
Multi-object tracking coprocessor for multi-channel embedded DVR systems,S. Kim; B. j. Lee; J. w. Jeong; M. j. Lee,"LG Production Eng. Res. Inst., LG Electron., Pyeongtaek, South Korea",IEEE Transactions on Consumer Electronics,20130124,2012,58,4,1366,1374,"In this paper, the architecture of a video analytics coprocessor is proposed for multi-channel embedded digital video recorder (DVR) systems. A reference video analytics algorithm is proposed for multi-object tracking and is divided into independent processing steps based on data flow. Each step is designed in hardware or software considering its computational complexity and required system resources. Pixelwise processing requiring a large amount of computational resources, such as frame difference and background modeling, are designed as hardware with embedded direct memory access (DMA) controllers. A single-pass connected component labeling (CCL) is designed as a hardware targeting real-time processing of stream input. High-level tasks such as object filtering, frame-based control of hardware modules, and communication with an external host are designed with software on an embedded processor. Object tracking and event detection are designed with software on a host processor. Considering both the bandwidth required for frame processing and the bandwidth available by memory buses, the architecture of a 4-channel video analytics coprocessor is explored. It is finally implemented on a field-programmable gate arrays (FPGA) device, integrated into a conventional DVR system, and verified as to its functions and performance. It can provide video analysis functions to conventional DVR system-on-chip (SoC), and can lessen the cost of real-time video monitoring at remote monitoring centers.",0098-3063;00983063,,10.1109/TCE.2012.6415008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6415008,background model;digital video recorder;object tracking;video analytics,Algorithm design and analysis;Computer architecture;Coprocessors;Monitoring;Real-time systems;Streaming media;System-on-a-chip,computational complexity;coprocessors;field programmable gate arrays;file organisation;system-on-chip;video recording,4-channel video analytic coprocessor;DVR system-on-chip;FPGA device;background modeling;computational complexity;computational resources;embedded DMA controllers;embedded direct memory access controllers;embedded processor;event detection;field programmable gate arrays;frame difference;frame processing;frame-based control;hardware modules;high-level tasks;multichannel embedded DVR systems;multichannel embedded digital video recorder systems;multiobject tracking coprocessor;object filtering;pixelwise processing;real-time processing;real-time video monitoring;reference video analytic algorithm;remote monitoring centers;single-pass CCL;single-pass connected component labeling;video analytic coprocessor,,3,,16,,no,12-Nov,,IEEE,IEEE Journals & Magazines
Neuro Analytical hierarchy process (NAHP) approach for CAD/CAM/CIM tool selection in the context of small manufacturing industries,D. R. Kalbande; S. Khan; G. T. Thampi,"S.P.I.T., Mumbai, India","2012 International Conference on Communication, Information & Computing Technology (ICCICT)",20121231,2012,,,1,6,"The purpose of this paper is to provide a methodology to assist small automobile manufacturing companies of India in selecting CAD/CAM system. The use of these tools in design and manufacturing of automobile applications makes it possible to remove much of the tedium and manual labor work involved. They allow complex designs to be made quicker and accurately. Many companies are using obsolete packages to design and manufacture automobiles. Since technology is changing rapidly and most companies cannot update with the technology advancements. They need to know what CAD/CAM packages are available and how to select one will help the companies. The selection of CAD/CAM/CIM tool/package is multi criteria, multi attribute, decision making process which involve many factors that can be solved using NAHP (Neuro Analytical Hierarchy Process). In NAHP, we are combining the power of AHP and Neural Network. To accomplish this purpose, data have been collected about current CAD/CAM systems. Important criteria for system selection and parameters for selection have also been identified and prioritized. NAHP is used as decision making technique to identify and prioritize important factors for selection of CAD/CAM/CIM system for small automobile manufacturing companies of India.",,Electronic:978-1-4577-2078-9; POD:978-1-4577-2077-2,10.1109/ICCICT.2012.6398139,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6398139,AHP;Multi-Criteria Decision Making (MCDM);NAHP;Technology Selection,Companies;Computer aided manufacturing;Computers;Design automation;Manufacturing;Software;Training,CAD/CAM;analytic hierarchy process;automobile industry;computer integrated manufacturing;neural nets;operations research;small-to-medium enterprises,AHP;CAD/CAM system;CAD/CAM/CIM tool selection;CAD/CAM/CIM tool/package;India;NAHP;automobile manufacturing companies;decision making process;decision making technique;manual labor work;manufacturing industries;multiattribute selection;multicriteria selection;neural network;neuro analytical hierarchy process;selection parameters;system selection;tedium labor work,,0,,5,,no,19-20 Oct. 2012,,IEEE,IEEE Conference Publications
Next generation emergency management common operating picture software/systems (COPSS),R. E. Balfour,"V.C.O.R.E. Solutions LLC, Bethpage, NY, USA","2012 IEEE Long Island Systems, Applications and Technology Conference (LISAT)",20120625,2012,,,1,6,"In the state-of-the-art Command Center at the Morrelly Homeland Security Center in Bethpage NY, VCORE Solutions has integrated, demonstrated, tested, deployed, and is operating powerful Emergency Management Common Operating Picture Software/Systems (COPSS). This Regional COPSS demonstrates in an operational environment the next generation of emergency management situational awareness, command & control, and information sharing in a natural, easy to use and understand four-dimensional (4D) common operating picture. This Regional COPSS is based on patented fourDscapeå¨ software technology, developed over the past decade by Long Island, NY-based Balfour Technologies. This powerful fourDscape augmented virtual reality technology has been effectively applied to local and regional emergency management operations and can be deployed nationally to deliver comprehensive situational awareness in support of safety, security and emergency preparedness, prevention, mitigation, response and recovery operations at all levels. A fourDscape browser/server-based COPSS is designed as an open, multi-layered service/resource oriented networked architecture (SOA/ROA, i.e. ‰ÛÏcloud‰Ûù) capable of (1) integrating and managing a multitude of disparate data sources of all types (including live and static data feeds); (2) interoperability with numerous other vendors information systems, COPs, information sharing frameworks, notification and alerting systems, analytics, etc.; and (3) sharing and passing information and comprehensive, timely situational awareness between first responders at the incident site, incident commanders, and emergency managers and decision makers at local, regional and national emergency operations centers across the country. And consistent with the recent Presidential Policy Directive on National Preparedness (PPD-8), this fourDscape COPSS capabilities and framework represents currently operational technology that can achieve an integrated, la- ered, and all-of-Nation [capabilities-based] preparedness approach that optimizes the use of available resources. Next Generation COPSS such as fourDscape need to be open and scalable to facilitate global information sharing; deliver information in an easy-to-understand augmented virtual reality common operating picture; be easy-to-use walk-up technology with a full complement of embedded training/simulation capability; provide for effective information assurance; and be compliant and effective in executing national preparedness goals. All this can and will be achieved by next generation COPSS.",,Electronic:978-1-4577-1343-9; POD:978-1-4577-1342-2,10.1109/LISAT.2012.6223101,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6223101,common operating picture;emergency response;fourDscape;situational awareness;telepresence;virtual reality,Analytical models;Browsers;Feeds;Next generation networking;Sensors;Solid modeling;Virtual reality,augmented reality;cognition;command and control systems;data integration;decision making;electronic data interchange;emergency services;open systems;optimisation;service-oriented architecture,4D common operating picture;COPSS;NY-based Balfour technology;PPD-8;Presidential Policy Directive on National Preparedness;ROA;SOA;augmented virtual reality;command and control system;common operating picture software-system;data integration;data source management;decision making;embedded training;emergency preparedness;fourDscape browser-server;fourDscapeå¨ software technology;global information sharing;information assurance;information passing;interoperability;local emergency operation;multilayered service;national emergency operation;national preparedness goal execution;next generation emergency management;open architecture;operational environment;optimization;recovery operation;regional emergency operation;resource oriented architecture;safety;security;service oriented architecture;situational awareness;vendor information systems;walk-up technology,,3,1,5,,no,4-4 May 2012,,IEEE,IEEE Conference Publications
On analytic model of multiple vias for high-speed printed circuit board and electric band-gap structures,L. Zhang; G. Pan; Z. Guo,"Arizona State University, Tempe, AZ, USA",2012 IEEE 21st Conference on Electrical Performance of Electronic Packaging and Systems,20130211,2012,,,311,314,"We developed a full-wave formulation to model massive number of vias in high-speed printed circuit board (PCB), through silicon via (TSV) and electric band-gap (EBG) structures. This analytic method employs the equivalent magnetic frill array, Galerkin's procedure, image theory and Fourier transform to simplify the problem from a 3D configuration into a 2D frame. Based on Bessel's functions and addition theorem, the final matrix equation is formulated analytically without using any numerical techniques. The new method is purely from the boundary conditions. Consequently, it is simple, versatile, efficient and accurate. Numerical examples demonstrate good agreement between our analytical solution and commercial software (HFSS) for through silicon and PCB vias. The model is also used to study the EBG wall and cavity, for leakage fields.",2165-4107;21654107,Electronic:978-1-4673-2538-7; POD:978-1-4673-2539-4; USB:978-1-4673-2537-0,10.1109/EPEPS.2012.6457904,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6457904,,Cavity resonators;Insertion loss;Magnetic resonance imaging;Metamaterials;Periodic structures;Silicon;Through-silicon vias,Bessel functions;Fourier transforms;Galerkin method;integrated circuit modelling;matrix algebra;photonic band gap;printed circuits;three-dimensional integrated circuits,2D frame;Bessel functions;EBG structures;EBG wall;Fourier transform;Galerkin procedure;HFSS software;PCB vias;TSV model;addition theorem;electric band-gap structures;equivalent magnetic frill array;full-wave formulation;high-speed printed circuit board;image theory;leakage fields;matrix equation;multiple vias analytic model;through silicon via model,,0,,7,,no,21-24 Oct. 2012,,IEEE,IEEE Conference Publications
"On the path to sustainable, scalable, and energy-efficient data analytics: Challenges, promises, and future directions",S. Lakshminarasimhan; P. Kumar; Wei-keng Liao; A. Choudhary; V. Kumar; N. F. Samatova,"North Carolina State University, Raleigh, USA",2012 International Green Computing Conference (IGCC),20121004,2012,,,1,6,"As scientific data is reaching exascale, scalable and energy efficient data analytics is quickly becoming a top notch priority. Yet, a sustainable solution to this problem is hampered by a number of technical challenges that get exacerbated with the emerging hardware and software technology trends. In this paper, we present a number of recently created ‰ÛÏsecret sauces‰Ûù that promise to address some of these challenges. We discuss transformative approaches to efficient data reduction, analytics-driven query processing, scalable analytical kernels, approximate analytics, among others. We propose a number of future directions that could be pursued on the path to sustainable data analytics at scale.",,Electronic:978-1-4673-2154-9; POD:978-1-4673-2155-6; USB:978-1-4673-2153-2,10.1109/IGCC.2012.6322265,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6322265,,Algorithm design and analysis;Approximation algorithms;Data mining;Indexes;Kernel;Software algorithms;Throughput,data analysis;data reduction;energy conservation;natural sciences computing;query processing,analytics-driven query processing;approximate analytics;data reduction;energy-efficient data analytics;exascale data analytics;hardware technology trends;scalable analytical kernels;scalable data analytics;scientific data;software technology trends;sustainable data analytics,,1,,17,,no,4-8 June 2012,,IEEE,IEEE Conference Publications
Optimizing an SPT-tree for visual analytics,C. Gramazio; R. Chang,,2012 IEEE Conference on Visual Analytics Science and Technology (VAST),20130103,2012,,,219,220,"Despite the extensive work done in the scientific visualization community on the creation and optimization of spatial data structures, there has been little adaptation of these structures in visual analytics and information visualization. In this work we present how we modify a space-partioning time (SPT) tree - a structure normally used in direct-volume rendering - for geospatial-temporal visualizations. We also present optimization techniques to improve the traversal speed of our structure through locational codes and bitwise comparisons. Finally, we present the results of an experiment that quantitatively evaluates our modified SPT tree with and without our optimizations. Our results indicate that retrieval was nearly three times faster when using our optimizations, and are consistent across multiple trials. Our finding could have implications for performance in using our modified SPT tree in large-scale geospatial temporal visual analytics software.",,Electronic:978-1-4673-4753-2; POD:978-1-4673-4752-5,10.1109/VAST.2012.6400544,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6400544,,Communities;Data visualization;Geospatial analysis;Indexing;Octrees;Optimization;Visual analytics,data visualisation;rendering (computer graphics);tree data structures;trees (mathematics),SPT-tree optimization;bitwise comparisons;direct-volume rendering;geospatial temporal visual analytics software;geospatial-temporal visualizations;information visualization;locational codes;scientific visualization community;space-partioning time tree;spatial data structures;traversal speed improvement,,0,,6,,no,14-19 Oct. 2012,,IEEE,IEEE Conference Publications
P-Tracer: Path-Based Performance Profiling in Cloud Computing Systems,H. Mi; H. Wang; H. Cai; Y. Zhou; M. R. Lyu; Z. Chen,"Nat. Lab. for Parallel &amp; Distrib. Process., Nat. Univ. of Defense Technol., Changsha, China",2012 IEEE 36th Annual Computer Software and Applications Conference,20121110,2012,,,509,514,"In large-scale cloud computing systems, the growing scale and complexity of component interactions pose great challenges for operators to understand the characteristics of system performance. Performance profiling has long been proved to be an effective approach to performance analysis; however, existing approaches do not consider two new requirements that emerge in cloud computing systems. First, the efficiency of the profiling becomes of critical concern; second, visual analytics should be utilized to make profiling results more readable. To address the above two issues, in this paper, we present P-Tracer, an online performance profiling approach specifically tailored for large-scale cloud computing systems. P-Tracer constructs a specific search engine that adopts a proactive way to process performance logs and generates particular indices for fast queries; furthermore, PTracer provides users with a suite of web-based interfaces to query statistical information of all kinds of services, which helps them quickly and intuitively understand system behavior. The approach has been successfully applied in Alibaba Cloud Computing Inc. to conduct online performance profiling both in production clusters and test clusters. Experience with one real-world case demonstrates that P-Tracer can effectively and efficiently help users conduct performance profiling and localize the primary causes of performance anomalies.",0730-3157;07303157,Electronic:978-1-5090-5637-8; POD:978-1-4673-1990-4; USB:978-0-7695-4736-7,10.1109/COMPSAC.2012.69,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6340205,Performance profiling;performance anomaly;visual analytics,Cloud computing;Instruments;Postal services;Production;Search engines;Shape;System performance,cloud computing;data analysis;data visualisation;pattern clustering;program diagnostics;query processing;search engines;software performance evaluation;statistical analysis;user interfaces,Alibaba Cloud Computing Inc;P-tracer;Web-based interfaces;component interactions;index generation;large-scale cloud computing systems;online performance profiling approach;path-based performance profiling;performance analysis;performance logs;production clusters;profiling efficiency;search engine;statistical information querying;test clusters;visual analytics,,4,,17,,no,16-20 July 2012,,IEEE,IEEE Conference Publications
Parallel and pipelined processing of large-scale mobile comminucation data using hadoop open-source framework,M. Koca; I. Ari; U. KoÌ_ak; O. Ìàal€±kuÅÙ; C. Sezgin,"Bilgisayar M&#x00FC;hendisli&#x011F;i B&#x00F6;l&#x00FC;m&#x00FC;, &#x00D6;zyegin &#x00DC;niversitesi, Turkey",2012 20th Signal Processing and Communications Applications Conference (SIU),20120528,2012,,,1,4,"The fast increase in mobile device and bandwidth usage is generating big workloads on the IT infrastructures of mobile service providers and increasing management costs. These providers collect log files continuously and use these logs for billing, operational and marketing purposes. In this paper, we describe the design, implementation and efficient parallel processing of large-scale mobile logs using the open-source Hadoop-based low-cost private cloud system for near real-time analytics. We find that batching of small files, parallel loading and pipelining of different workloads by overlapping their disk-and-CPU intensive phases can have significant performance benefits. Optimizations were performed in the light of these findings. Our web-based interface helps users explore progress and performance of their workloads.",2165-0608;21650608,Electronic:978-1-4673-0056-8; POD:978-1-4673-0055-1; USB:978-1-4673-0054-4,10.1109/SIU.2012.6204832,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6204832,,Abstracts;Erbium;Loading;Privacy;Receivers,batch processing (computers);cloud computing;file organisation;invoicing;marketing;mobile communication;mobile computing;parallel processing;pipeline processing;public domain software;records management;user interfaces,IT infrastructures;Web-based interface;bandwidth usage;billing;disk-and-CPU intensive phase overlapping;file batching;large-scale mobile communication data;log files;marketing;mobile device;mobile service providers;open source Hadoop-based low-cost private cloud system;optimization;parallel loading;parallel processing;pipelined processing,,0,,1,,no,18-20 April 2012,,IEEE,IEEE Conference Publications
Partial cuts in attack graphs for cost effective network defence,R. Sawilla; D. Skillicorn,"Defence R&D Canada, Ottawa, ON, Canada",2012 IEEE Conference on Technologies for Homeland Security (HST),20130214,2012,,,291,297,"Because of increasing vulnerabilities, maturing attack tools, and increasing dependence on computer network infrastructure, tools to support network defenders are essential. Course-of-action recommendation research has often assumed a goal of perfect network security. In reality, network administrators balance security with usability and so tolerate vulnerabilities and imperfect security. We provide realistic course-of-action decision support for network administrators by minimizing connectivity in attack graphs, by optimizing network configuration changes to separate defence goals from attackers as much as possible, even when complete security is impractical. We introduce vertex closures and closure-relation graphs in AND/OR digraphs as the underlying framework. Computing an optimal course-of-action is NP-hard but we design a polynomial-time greedy algorithm that almost always produces an optimal solution.",,Electronic:978-1-4673-2709-1; POD:978-1-4673-2708-4,10.1109/THS.2012.6459864,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6459864,attack analytic tools;cyber security;decision support systems,Algorithm design and analysis;Communities;Complexity theory;Force;Security;Software;Vectors,computational complexity;computer network security;directed graphs;greedy algorithms;optimisation,AND/OR digraphs;NP-hard problem;attack graphs;closure-relation graphs;computer network infrastructure;cost effective network defence;course-of-action recommendation research;network defenders;network security;partial cuts;polynomial-time greedy algorithm;vertex closures,,4,,15,,no,13-15 Nov. 2012,,IEEE,IEEE Conference Publications
Performance debugging in the large via mining millions of stack traces,,,,,2012,,,,,"Given limited resource and time before software release, development-site testing and debugging become more and more insufficient to ensure satisfactory software performance. As a counterpart for debugging in the large pioneered by the Microsoft Windows Error Reporting (WER) system focusing on crashing/hanging bugs, performance debugging in the large has emerged thanks to available infrastructure support to collect execution traces with performance issues from a huge number of users at the deployment sites. However, performance debugging against these numerous and complex traces remains a significant challenge for performance analysts. In this paper, to enable performance debugging in the large in practice, we propose a novel approach, called StackMine, that mines callstack traces to help performance analysts effectively discover highly impactful performance bugs (e.g., bugs impacting many users with long response delay). As a successful technology-transfer effort, since December 2010, StackMine has been applied in performance-debugging activities at a Microsoft team for performance analysis, especially for a large number of execution traces. Based on real-adoption experiences of StackMine in practice, we conducted an evaluation of StackMine on performance debugging in the large for Microsoft Windows 7. We also conducted another evaluation on a third-party application. The results highlight substantial benefits offered by StackMine in performance debugging in the large for large-scale software systems.",,,,http://dl.acm.org/citation.cfm?id=2337241&CFID=696538919&CFTOKEN=83912867,,,,,,,,,,yes,,,,
Performance evaluation of node in cloud storage,K. Haung; Z. Ma; L. Sun,"Zhengzhou Information Science and Technology Institute, Zhengzhou, 450004, China","2012 2nd International Conference on Consumer Electronics, Communications and Networks (CECNet)",20120517,2012,,,2739,2744,"There is a method to evaluate the performance of the node storage the replica in the cloud storage, under this method we can get better node when we place the replica according to user requirement for service. First, establish the model of the performance of the node storage the replica using the analytic hierarchy process. Then, obtain the weighting of each factor in model using the combination weight. At last, the valuations of nodes are calculated and the rank of nodes is given by grey comprehensive evaluation. Experiment results show that the proposed method could provide more reasonable replica strategies effectively.",,DVD:978-1-4577-1413-9; Electronic:978-1-4577-1415-3; POD:978-1-4577-1414-6,10.1109/CECNet.2012.6201473,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6201473,analytic hierarchy process;cloud storage;combination weights;grey comprehensive evaluation;node;replica placement,Availability;Bandwidth;Cloud computing;Entropy;Indexes;Quality of service,cloud computing;decision making;software performance evaluation;storage management,analytic hierarchy process;cloud storage;combination weight;grey comprehensive evaluation;node storage performance evaluation;replica placement;user requirement,,0,,10,,no,21-23 April 2012,,IEEE,IEEE Conference Publications
Performance Evaluation of Yahoo! S4: A First Look,J. Chauhan; S. A. Chowdhury; D. Makaroff,"Dept. of Comput. Sci., Univ. of Saskatchewan, Saskatoon, SK, Canada","2012 Seventh International Conference on P2P, Parallel, Grid, Cloud and Internet Computing",20121129,2012,,,58,65,"Processing large data sets has been dominated recently by the map/reduce programming model [1], originally proposed by Google and widely adopted through the Apache Hadoop1 implementation. Over the years, developers have identified weaknesses of processing data sets in batches as in MapReduce and have proposed alternatives. One such alternative is continuous processing of data streams. This is particularly suitable for applications in online analytics, monitoring, financial data processing and fraud detection that require timely processing of data, making the delay introduced by batch processing highly undesirable. This processing paradigm has led to the development of systems such as Yahoo! S4 [2] and Twitter Storm.2 Yahoo! S4 is a general-purpose, distributed and scalable platform that allows programmers to easily develop applications for processing continuous unbounded streams of data. As these frameworks are quite young and new, there is a need to understand their performance for real time applications and find out the existing issues in terms of scalability, execution time and fault tolerance. We did an empirical evaluation of one application on Yahoo! S4 and focused on the performance in terms of scalability, lost events and fault tolerance. Findings of our analyses can be helpful towards understanding the challenges in developing stream-based data intensive computing tools and thus providing a guideline for the future development.",,Electronic:978-0-7695-4841-8; POD:978-1-4673-2991-0,10.1109/3PGCIC.2012.55,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6362950,Performance;Stream-based Data Intensive computing;Yahoo! S4,Computational modeling;Data models;Data processing;Fault tolerance;Fault tolerant systems;Real-time systems;Scalability,data handling;distributed programming;public domain software;software fault tolerance;software performance evaluation,Apache Hadoop1 implementation;Google;MapReduce programming model;Twitter Storm;Yahoo! S4 performance evaluation;batch processing;continuous unbounded data stream processing;general-purpose distributed scalable platform;online analytics;online financial data processing;online fraud detection;online monitoring;real time applications;stream-based data intensive computing tools,,4,,11,,no,12-14 Nov. 2012,,IEEE,IEEE Conference Publications
Performance indicators in software project monitoring: Balanced scorecard approach,L. Kazi; B. Radulovic; Z. Kazi,"University of Novi Sad, Technical faculty &#x201C;Mihajlo Pupin&#x201D;, Zrenjanin, Serbia",2012 IEEE 10th Jubilee International Symposium on Intelligent Systems and Informatics,20121025,2012,,,19,25,"Balanced scorecard is one of most used methodologies for performance measurement in enterprises and non-profit organizations. It presents a general framework that has been applied more precisely by many scientists and practitioners. It has been adapted to particular performance measurement systems. These monitoring systems are based on collecting data and analysis of these data. This paper presents overview of implementing balanced scorecard in IT project management. Framework for software project success monitoring, based on PRINCE2 methodology and balanced scorecard is proposed. Key performance indicators as measures to be performed upon data collected during software project implementation are defined. This way, analytics to be performed upon data according to specified key performance indicators, needed for decision support is enabled.",1949-047X;1949047X,Electronic:978-1-4673-4750-1; POD:978-1-4673-4751-8; USB:978-1-4673-4749-5,10.1109/SISY.2012.6339539,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6339539,,Adaptation models;Measurement;Monitoring;Organizations;Project management;Software,decision support systems;organisational aspects;software management;software performance evaluation,IT project management;PRINCE2 methodology;balanced scorecard approach;decision support;nonprofit organizations;performance indicators;performance measurement systems;software project implementation;software project monitoring;software project success monitoring,,1,,25,,no,20-22 Sept. 2012,,IEEE,IEEE Conference Publications
"Platform 2012, a many-core computing accelerator for embedded SoCs: Performance evaluation of visual analytics applications",D. Melpignano; L. Benini; E. Flamand; B. Jego; T. Lepley; G. Haugou; F. Clermidy; D. Dutoit,"STMicroelectronics - AST, Grenoble, France",DAC Design Automation Conference 2012,20120719,2012,,,1137,1142,"P2012 is an area- and power-efficient many-core computing accelerator based on multiple globally asynchronous, locally synchronous processor clusters. Each cluster features up to 16 processors with independent instruction streams sharing a multi-banked one-cycle access L1 data memory, a multi-channel DMA engine and specialized hardware for synchronization and aggressive power management. P2012 is 3D stacking ready and can be customized to achieve extreme area and energy efficiency by adding domain-specific HW IPs to the cluster. The first P2012 SoC prototype in 28nm CMOS will sample in Q3, featuring four 16-processor clusters, a 1MB L2 memory and delivering 80GOPS (with 32 bit single precision floating point support) in 18mm<sup>2</sup> with 2W power consumption (worst-case). P2012 can run standard OpenCL‰ã¢ and proprietary Native Programming Model SW components to achieve the highest level of control on application-to-resource mapping. A dedicated version of the OpenCV vision library is provided in the P2012 SW Development Kit to enable visual analytics acceleration. This paper will discuss preliminary performance measurements of common feature extraction and tracking algorithms, parallelized on P2012, versus sequential execution on ARM CPUs.",0738-100X;0738100X,Electronic:978-1-4503-1199-1; POD:978-1-4503-1199-1,10.1145/2228360.2228568,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6241648,3D stacking;Low-power;SoC;computer vision;feature extraction;many-core;process aware,Acceleration;Computer architecture;Fabrics;Hardware;Programming;Software;System-on-a-chip,CMOS memory circuits;embedded systems;energy conservation;feature extraction;file organisation;microprocessor chips;multiprocessing systems;parallel architectures;performance evaluation;power aware computing;synchronisation;system-on-chip,16-processor clusters;3D stacking;ARM CPU;CMOS;L2 memory;OpenCV vision library;P2012;P2012 SW Development Kit;P2012 SoC prototype;application-to-resource mapping;area-efficient many-core computing accelerator;domain-specific HW IP;embedded SoC;energy efficiency;feature extraction;instruction streams;multibanked one-cycle access L1 data memory;multichannel DMA engine;multiple globally asynchronous locally synchronous processor clusters;native programming model;performance evaluation;performance measurements;power 2 W;power management;power-efficient many-core computing accelerator;sequential execution;size 28 nm;synchronization;tracking algorithms;visual analytics applications,,60,,16,,no,3-7 June 2012,,IEEE,IEEE Conference Publications
Privacy-Preserving Layer over MapReduce on Cloud,X. Zhang; C. Liu; S. Nepal; W. Dou; J. Chen,"Fac. of Eng. & IT, Univ. of Technol. Sydney, Sydney, NSW, Australia",2012 Second International Conference on Cloud and Green Computing,20130214,2012,,,304,310,"Cloud computing provides powerful and economical infrastructural resources for cloud users to handle ever-increasing Big Data with data-processing frameworks such as MapReduce. Based on cloud computing, the MapReduce framework has been widely adopted to process huge-volume data sets by various companies and organizations due to its salient features. Nevertheless, privacy concerns in MapReduce are aggravated because the privacy-sensitive information scattered among various data sets can be recovered with more ease when data and computational power are considerably abundant. Existing approaches employ techniques like access control or encryption to protect privacy in data processed by MapReduce. However, such techniques fail to preserve data privacy cost-effectively in some common scenarios where data are processed for data analytics, mining and sharing on cloud. As such, we propose a flexible, scalable, dynamical and costeffective privacy-preserving layer over the MapReduce framework in this paper. The layer ensures data privacy preservation and data utility under the given privacy requirements before data are further processed by subsequent MapReduce tasks. A corresponding prototype system is developed for the privacy-preserving layer as well.",,Electronic:978-0-7695-4864-7; POD:978-1-4673-3027-5,10.1109/CGC.2012.43,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6382833,Big Data;MapReduce;cloud computing;framework;privacy preservation,Cloud computing;Cryptography;Data handling;Data privacy;Data storage systems;Information management;Privacy,cloud computing;data privacy;parallel processing,MapReduce framework;access control;big data;cloud computing;data analytics;data mining;data sharing;encryption;huge-volume data processing;privacy protection;privacy requirement;privacy-preserving layer;privacy-sensitive information,,2,,42,,no,1-3 Nov. 2012,,IEEE,IEEE Conference Publications
Private cloud delivery model for supplying centralized analytics services,L. C. Yarter,"IBM Chief Information Office, Maryland, NY, USA",IBM Journal of Research and Development,20121115,2012,56,6,10:01,10:06,"Business analytics has become vital for businesses to achieve their near-term and strategic goals. Historically, large enterprises have used a decentralized information technology delivery model to meet reporting and analytics needs within each business unit. Rising demand for analytics, combined with the cost of decentralized delivery, has driven businesses to look at alternative delivery patterns. Chief information officers (CIOs) have a renewed interest in considering centralized, virtualized services‰ÛÓgiven their desire to reduce expenses without reduced business flexibility. The Chief Information Office of IBM has pioneered a private cloud delivery model for delivering analytics tooling to internal users (IBM employees), providing centralization and standardization of service. Through evaluation of analytics usage patterns, IBM developed a common analytics services strategy to reduce expenses and solution deployment time while improving business agility and insights. IBM deployed a share-all private cloud model (in which users deploy reports on the same instance of the hardware and software) on an extensible Linuxå¨ on the IBM System zå¨ infrastructure to deliver ubiquitous analytics service to every business process area. The IBM software-as-a-service analytics delivery model allowed its more than 190,000 business users to maintain user or user-group solution autonomy while providing access to cost-effective ($25 million savings over five years), timely commodity services.",0018-8646;00188646,,10.1147/JRD.2012.2216331,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6353964,,,,,,0,,,,no,Nov.-Dec. 2012,,IBM,IBM Journals & Magazines
Quantifying the performance impact of overbooking virtualized resources,D. Hoeflin; P. Reeser,"AT&T Labs, Middletown, NJ 07748 USA",2012 IEEE International Conference on Communications (ICC),20121129,2012,,,5523,5527,"Cloud services are compelling to customers from many perspectives, but the immense benefits come at a price. Along with reliability and security, performance is the Achilles heel of Cloud services. In order to make Cloud services profitable, the Cloud service provider obviously must overbook physical resources. But at what level of resource overbooking (OB) does performance begin to suffer? The answer, of course, depends on many factors. In this paper, we present a simple analytic model to quantify the performance impact of overbooking virtualized resources as a function of the relevant environment and usage parameters. We then validate those analytic modeling results against simulation, lab tests, and field data. Finally, we propose a simple means for measuring the model parameters in the field, in order to use the analytic model to determine allowable OB factors (engineering rules) while still meeting performance constraints and service-level agreements.",1550-3607;15503607,Electronic:978-1-4577-2053-6; POD:978-1-4577-2052-9; USB:978-1-4577-2051-2,10.1109/ICC.2012.6364669,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6364669,Cloud;QoS;SLA;VM;overbooking;performan;virtualization,Aggregates;Analytical models;Computational modeling;Monitoring;Servers;Throughput;Time factors,Web services;cloud computing;pricing;security of data;software reliability;virtual machines;virtualisation,OB;cloud service provider;model parameter;overbooking virtualized resource;performance constraint;pricing;reliability;resource overbooking;security;service level agreement,,4,,10,,no,10-15 June 2012,,IEEE,IEEE Conference Publications
Research and application of a karst ground collapse risk assessment information system in Shenzhen Universiade Center (China),X. Li; G. Xu,"Engineering Faculty, China University of Geosciences Wuhan, Hubei, 430074, China",2012 IEEE Symposium on Robotics and Applications (ISRA),20120618,2012,,,540,543,"To ensure the project safety of Shenzhen Universiade Center and normal use of the facilities, it's necessary to study the distribution rules of karst and predict the risks of Karst ground Collapse to provide a reference for prevention and control measures. With the support of MAPGIS software, a risk assessment information system of karst ground collapse in Shenzhen Universiade Center was researched and developed, which realized the automated process from basic data input to the result assessment of karst ground collapse. System functions include data input and output, data query, database generation assessment, model parameter calculations assessment, model features assessment, and result mapping assessment. The paper analyzed all possible factors that may affect karst ground collapse and obtained six important factors - distribution of dissoluble rock, cover thickness, groundwater level, karst development degree, building importance and structure. The weight coefficients of three grades of indexes were obtained by using AHP to analyze the six factors. The site was divided into 6843 grids by the size of 10m * 10m. The assessment results showed that 1.461% of the site is high-risk area, 8.666% is relatively higher risk area, 23.805% is less dangerous area, and 66.068% is low-risk area. Such assessment results can be used as reference for follow-up processing of the site.",,DVD:978-1-4673-2206-5; Electronic:978-1-4673-2207-2; POD:978-1-4673-2205-8,10.1109/ISRA.2012.6219244,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6219244,Analytic Hierarchy Process (AHP);Collapse;GIS;Karst;Shenzhen Universiade Center,Buildings;Data models;Indexes;Risk management;Rocks;Surface treatment,decision making;disasters;geographic information systems;geophysical techniques;geophysics computing;risk management,AHP;Chinese Shenzhen Universiade Center;MAPGIS software;analytic hierarchy process;building importance;building structure;cover thickness;data input;data output;data query;database generation assessment;dissoluble rock factor distribution;groundwater level;high-risk area;index grades;karst development degree;karst distribution rules;karst ground collapse risk assessment information system;low-risk area;model feature assessment;model parameter calculation assessment;project safety;result mapping assessment;risk prediction;weight coefficients,,0,,5,,no,3-5 June 2012,,IEEE,IEEE Conference Publications
Research of School-Enterprise Research Evaluation Information Mining Technology Based on Analytical Hierarchy Process (AHP) and Genetic Algorithm (GA),B. Song; J. Yu,"Sch. of Comput. Sci. &amp; Software, Tianjin Polytech. Univ., Tianjin, China",2012 Fourth International Conference on Multimedia Information Networking and Security,20130110,2012,,,721,724,"This paper describes the research development levels of home and abroad, and makes a mining research about Chinese school-enterprise research evaluation information. In consideration of the evaluation index system of school-enterprise research, data mining model is established in the comprehensive use of the improved genetic algorithm based on the AHP analytic hierarchy process (AHP). The paper also makes scientific quantitative analysis on the teach-research indexes to each other, the quantitative index system provides a broad prospect for teaching and research work. In order to solve the implementation of school-enterprise research mechanism the problem that is badly in need of solving in current higher vocational education in our country.",2162-8998;21628998,Electronic:978-0-7695-4852-4; POD:978-1-4673-3093-0,10.1109/MINES.2012.170,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405799,Analytic Hierarchy Process;Genetic Algorithm;Quantitative Analysis and Mining;School-enterprise Research Information,Educational institutions;Genetic algorithms;Indexes;Optimization;Sociology;Statistics,analytic hierarchy process;data mining;educational administrative data processing;educational institutions;further education;genetic algorithms;research and development;teaching;vocational training,AHP;Chinese school-enterprise research evaluation information;GA;analytical hierarchy process;data mining model;evaluation index system;higher vocational education;improved genetic algorithm;quantitative index system;research development levels;school-enterprise research evaluation information mining technology;teach-research indexes;teaching,,1,,5,,no,2-4 Nov. 2012,,IEEE,IEEE Conference Publications
Research on Comprehensive Evaluation of the Students' Vocational Ability Based on AHP-FUZZY,J. Liu; Y. Pan; J. Yang,"Acad. Affairs Office, Tianjin Inst. of Software Eng., Tianjin, China",2012 International Conference on Control Engineering and Communication Technology,20130117,2012,,,791,793,"In the new situation of implementing innovative talent training mode and joint education between colleges and enterprises, colleges are focusing on comprehensively improving students' vocational skills and quality and realizing students' vocational management evaluation of students' vocational ability is a comprehensive evaluation which includes students' learning ability and vocational quality. Colleges can combine fuzzy comprehensive evaluation and AHP through building students' comprehensive evaluation system in the vocational training mode. AHP is used to identify evaluation target and index weights and fussy comprehensive evaluation is used for the overall evaluation of students' vocational ability. According to the evaluation result, colleges can improve their students' inspiration and training models.",,Electronic:978-0-7695-4881-4; POD:978-1-4673-4499-9,10.1109/ICCECT.2012.242,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6413992,Analytic Hierarchy Process;Comprehensive Evaluation;Students' Vocational Ability,Analytic hierarchy process;Buildings;Educational institutions;Indexes;Software engineering;Training,analytic hierarchy process;education;fuzzy set theory,AHP-FUZZY;comprehensive evaluation;fuzzy comprehensive evaluation;joint education;students vocational ability;students vocational skills;talent training mode,,0,,4,,no,7-9 Dec. 2012,,IEEE,IEEE Conference Publications
Research on the comprehensive evaluation of patient satisfaction based on the attribute reduction and AHP,Chen Lihua; Hou Kaihu; Sun Shaopeng; Ji Yunhai,"Department of Industrial Engineering, Kunming University of Science and Technology, China",ICSSSM12,20120730,2012,,,389,392,"Focusing on the systematicness and validity of the evaluation system of the patient satisfaction, large number of factors impacting the patient satisfaction were considered comprehensively, the analytical method combining the Attribute Reduction in Rough Set with AHP was adopted. First, the evaluation factors were analyzed to establish the evaluation hierarchical model of the patient satisfaction, the indexes were reduced by Attribute Reduction based on the information quantity; then, the significance of attribute was used to determine the index weight; lastly, the total sequence weight of sub-criteria was calculated by AHP method and the comprehensive evaluation value was obtained. This model was used to evaluate and analyze the five hospitals in one area and the validity of this method was tested. This method reduces the amount of calculation, improves the computational efficiency and provides a method of quantitative analysis.",2161-1890;21611890,Electronic:978-1-4577-2025-3; POD:978-1-4577-2024-6,10.1109/ICSSSM.2012.6252261,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6252261,Attribute Reduction;index weight;patient satisfaction;the significance,Analytical models;Biomedical imaging;Hospitals;Indexes;Software reliability,decision making;hospitals;patient care;rough set theory,AHP;analytic hierarchy process;attribute reduction;comprehensive evaluation system;computational efficiency improvement;evaluation hierarchical model;index weight determination;information quantity;patient satisfaction;rough set;subcriteria total sequence weight,,0,,,,no,2-4 July 2012,,IEEE,IEEE Conference Publications
Research on VAF of IFPUG Method Based on Fuzzy Analytic Hierarchy Process,H. Peng; G. X. Yang; L. Cai,"Sch. of Inf. Sci. &amp; Eng., East China Univ. of Sci. &amp; Technol., Shanghai, China",2012 IEEE/ACIS 11th International Conference on Computer and Information Science,20120607,2012,,,593,597,"As the weight of the general system characteristics (GSC) in IFPUG method is the same in the different types of software systems. To solve the problem, this article uses the Fuzzy AHP (Analytic Hierarchy Process) approach to assign weight to the fourteen general system characteristics, and gives the improved formula of VAF. It makes IFPUG method more suitable to these other types of software systems besides the information management system. An example of functional size measurement is given to illustrate the feasibility of this method. And it also illustrates that the fuzzy AHP approach can get a better accuracy of the function point counting, and reduces the deviation.",,Electronic:978-0-7695-4694-0; POD:978-1-4673-1536-4,10.1109/ICIS.2012.93,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6211158,Fuzzy AHP approach;IFPUG method;Value adjustment factor,Estimation;Real time systems;Size measurement;Software measurement;Software systems;Vectors,decision making;information management;software development management,IFPUG method;International Function Point User Group;VAF formula;deviation reduction;function point counting;functional size measurement;fuzzy AHP;fuzzy analytic hierarchy process;general system characteristics;information management system;software systems;value adjustment factor;weight assignment,,2,,10,,no,May 30 2012-June 1 2012,,IEEE,IEEE Conference Publications
Robust Decision Engineering: Collaborative Big Data and its application to international development/aid,S. Chan; W. Rhodes; C. Atencio; C. Kuo; B. Ranalli; A. Miao; S. Sala; S. Serene; R. Helbling; S. Rumbley; M. Clement; L. Sokol; L. Gary,"Prince of Wales Fellows at the MIT International Development Initiative, Massachusetts Institute of Technology, Cambridge, USA","8th International Conference on Collaborative Computing: Networking, Applications and Worksharing (CollaborateCom)",20130207,2012,,,597,604,"Much of the research that goes into Big Data, and specifically on Collaborative Big Data, is focused upon questions, such as: how to get more of it? (e.g., åá participatory mechanisms, social media, geo-coded data from personal electronic devices) and åá how to handle it? (e.g., how to ingest, sort, store, and link up disparate data sets). A question that receives far less attention is that of Collaborative analysis of Big Data; how can a multi-disciplinary layered analysis of Big Data be used to support robust decisions, especially in a collaborative setting, and especially under time pressure? The robust Decision Engineering required can be achieved by employing an approach related to Network Science, that we call Relationship Science. In Relationship Science, our methodological framework, karassian netchain analysis (KNA), is utilized to ascertain islands of stability or positive influence dominating sets (PIDS), so that a form of annealed resiliency or latent stability is achieved, thereby mitigating against unintended consequences, elements of instability, and ‰ÛÏperfect storm‰Ûù crises lurking within the network.",,Electronic:978-1-936968-68-8; POD:978-1-4673-2740-4,10.4108/icst.collaboratecom.2012.250715,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6450957,"3D;5D;Aegis System;Annealed Resiliency;Better Decisions;Big Compute;Big Insights;Bigger Data;Brittleness;Cascading Failure;Civil Society;Collaborative Big Data;Common Operating Picture;Complexity Ceiling;Complexity Theory;Compressed Decision Cycles;Computational Intelligence;Condition-Creating;Content Analytics, Entity Resolution, Predictive Analytics;Cyber-Physical Supply Chain;Decision Engineering Science;Decision-Making;Democratic Governance;Dualistic Actors;Faster Decisions;Fifth Column;Flash Mobs;Gestaltian Closure;High Adaptation Cycles;High Performance Computing;Insider Threats;Intelligent Decisions;Islands of Stability;Karassian Netchain Analysis;Latent Stability;Layered Analytics;Local Community Structures;Memes;Motifs;Network Science;Network Shapes;Participatory Revolution;Perfect Storm Crises;Positive Influence Dominating Sets;Provenanced/Pedigreed Data;Relationship Manager;Relationship Science;Robust Decision Engineering;Sandpile Effect;Science of Development;Selection Bias;Sentiment Analysis;Smart Power Times;Social Complexity Science;Social Influence Network;Sparse Data;Sparse Networks;Unintended Consequences;Velocity, Volume, and Vectors of Big Data",Lead;Pattern matching;Pediatrics;Robustness;Stability analysis;Storms,data analysis;decision support systems;groupware;network theory (graphs),KNA;PIDS;annealed resiliency;collaborative analysis;collaborative big data;international aid;international development;karassian netchain analysis;latent stability;network science;perfect storm crises;positive influence dominating sets;relationship science;robust decision engineering;stability islands,,1,,41,,no,14-17 Oct. 2012,,IEEE,IEEE Conference Publications
RPig: A scalable framework for machine learning and advanced statistical functionalities,M. Wang; S. B. Handurukande; M. Nassar,"Network Management Lab, Ericsson Ireland",4th IEEE International Conference on Cloud Computing Technology and Science Proceedings,20130204,2012,,,293,300,In many domains such as Telecom various scenarios necessitate the processing of large amounts of data using statistical and machine learning algorithms. A noticeable effort has been made to move the data management systems into MapReduce parallel processing environments such as Hadoop and Pig. Nevertheless these systems lack the features of advanced machine learning and statistical analysis. Frame-works such as Mahout on top of Hadoop support machine learning but their implementations are at the preliminary stage. For example Mahout does not provide Support Vector Machine (SVM) algorithms and it is difficult to use. On the other hand traditional statistical software tools such as R containing comprehensive statistical algorithms for advanced analysis are widely used. But such software can only run on a single computer and therefore it is not scalable. In this paper we propose an integrated solution RPig which takes the advantages of R (for machine learning and statistical analysis capabilities) and parallel data processing capabilities of Pig. The RPig framework offers a scalable advanced data analysis solution for machine learning and statistical analysis. Analysis jobs can be easily developed with RPig script in high level languages. We describe the design implementation and an eclipse-based RPigEditor for the RPig framework. Using application scenarios from the Telecom domain we show the usage of RPig and how the framework can significantly reduce the development effort. The results demonstrate the scalability of our framework and the simplicity of deployment for analysis jobs.,,Electronic:978-1-4673-4510-1; POD:978-1-4673-4511-8; USB:978-1-4673-4509-5,10.1109/CloudCom.2012.6427480,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6427480,Analytic;Big data;Design;MapReduce;Pig;R,,authoring languages;data analysis;learning (artificial intelligence);parallel processing;software tools;statistical analysis;telecommunication computing,Hadoop;MapReduce parallel processing environments;RPig script;Telecom domain;advanced statistical functionality;comprehensive statistical algorithms;data management systems;eclipse-based RPigEditor;high level languages;machine learning;scalable advanced data analysis solution;scalable framework;statistical analysis;statistical software tools,,1,,22,,no,3-6 Dec. 2012,,IEEE,IEEE Conference Publications
Scalable complex graph analysis with the knowledge discovery toolbox,A. Lugowski; A. BuluÌ_; J. R. Gilbert; S. Reinhardt,"University of California, Santa Barbara, USA","2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",20120830,2012,,,5345,5348,"The Knowledge Discovery Toolbox (KDT) enables domain experts to perform complex analyses of huge datasets on supercomputers using a high-level language without grappling with the difficulties of writing parallel code, calling parallel libraries, or becoming a graph expert. KDT delivers competitive performance from a general-purpose, reusable library for graphs on the order of 10 billion edges and greater. We describe our approach for supporting arbirary vertex and edge attributes, in-place graph filtering, and graph traversal using pre-defined access patterns.",1520-6149;15206149,Electronic:978-1-4673-0046-9; POD:978-1-4673-0045-2; USB:978-1-4673-0044-5,10.1109/ICASSP.2012.6289128,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6289128,filter;graph analytics;knowledge discovery;scalability;semantic graph,Algorithm design and analysis;Clustering algorithms;Libraries;Semantics;Sparse matrices;Vectors,data mining;graph theory;high level languages;information filtering;network theory (graphs);program compilers;software reusability,KDT;arbirary edge attributes;arbirary vertex attributes;general-purpose reusable library;graph traversal;high-level language;in-place graph filtering;knowledge discovery toolbox;pre-defined access patterns;scalable complex graph analysis;semantic graphs;supercomputers,,2,1,5,,no,25-30 March 2012,,IEEE,IEEE Conference Publications
Scalable performance of ScaleGraph for large scale graph analysis,M. Dayarathna; C. Houngkaew; H. Ogata; T. Suzumura,"Department of Computer Science, Tokyo Institute of Technology",2012 19th International Conference on High Performance Computing,20130425,2012,,,1,9,"Scalable analysis of massive graphs has become a challenging issue in high performance computing environments. ScaleGraph is an X10 library aimed for large scale graph analysis scenarios. This paper evaluates scalability of ScaleGraph library for degree distribution calculation, betweeness centrality, and spectral clustering algorithms. We make scalability evaluation by analyzing a synthetic Kronecker graph with 40.3 million edges (for all the three algorithms), and a real social network with 69 million edges (for degree distribution calculation) on Tsubame 2.0 distributed memory environment.",,Electronic:978-1-4673-2371-0; POD:978-1-4673-2372-7,10.1109/HiPC.2012.6507498,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6507498,HPCS;PGAS;X10;distributed computing;large graph analytics;programming techniques;reusable libraries,,distributed memory systems;parallel processing;software libraries,ScaleGraph library;Tsubame distributed memory environment;X10 library;degree distribution calculation;high performance computing environments;large scale graph analysis;massive graphs;scalability evaluation;scalable performance;social network;spectral clustering algorithms;synthetic Kronecker graph,,3,,61,,no,18-22 Dec. 2012,,IEEE,IEEE Conference Publications
Semi-infinite integral implementation in the development steps of Interfstud electromagnetic interference software,D. D. Micu; L. Czumbil; G. C. Christoforidis; T. Papadopoulos,"Electrical Engineering Department, Technical University of Cluj-Napoca, Cluj-Napoca, Romania",2012 47th International Universities Power Engineering Conference (UPEC),20121231,2012,,,1,6,"A user friendly software application was developed to study the electromagnetic interference problem between high voltage power lines and underground metallic pipelines. A hybrid method presented in previous papers is implemented in the software to calculate the induced A.C. voltage in pipeline. The dedicated developed InterfStud software was based on induced magnetic vector potential and ground correction terms evaluation. New analytical formulas for the induced magnetic vector potential and ground correction terms for self and mutual impedances are derived. The determined formulas contain semi-infinite integral terms which will be evaluated. We might seek approximations of the semi-infinite integrals by replacing an exponential or algebraic function, the objective being to permit analytic integration. Since there is no good systematic method for making these replacements, their success depends directly on the intuition and ingenuity, taking into account that in practice the integrand has limited accuracy.",,Electronic:978-1-4673-2856-2; POD:978-1-4673-2854-8; USB:978-1-4673-2855-5,10.1109/UPEC.2012.6398640,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6398640,electromagnetic field;high voltage power line;metallic pipelines;semi-infinite integrals,Earth;Equations;Magnetic domains;Pipelines;Software;Soil;Vectors,approximation theory;electromagnetic interference;pipelines;power cables;power engineering computing,Interfstud electromagnetic interference software;algebraic function;exponential function;ground correction term evaluation;high voltage power lines;induced AC voltage;induced magnetic vector potential;mutual impedances;self-impedances;semiinfinite integral terms;underground metallic pipelines,,2,,11,,no,4-7 Sept. 2012,,IEEE,IEEE Conference Publications
"Setting up, managing and using a complex Business Intelligence platform",M. Boero; M. E. Jackson,"CSI-Piemonte/Direzione Governo e Gestione, Turin, Italy",2012 Proceedings of the 35th International Convention MIPRO,20120716,2012,,,1624,1628,"CSI-Piemonte (Consortium for Information Systems in Piedmont) builds and manages Information Systems for public authorities in Italy in the Piedmont region. The consortium has adopted a Business Intelligence (BI) platform made up of SASå¨ and SAP BusinessObjectå¨ solutions. The company has been working with SASå¨ for 30 years and nowadays it has a BI Competency Centre (BICC), set up in order to use and manage the platform as its optimal level; the BICC's roots date back to the 80s. In this platform both hardware and software of all customers converge on a single high reliable infrastructure based on SASå¨ 9.2 and SAP BusinessObjectå¨ XI and the SASå¨ component is designed as a Grid Platform. On this architecture CSI-Piemonte design a wide range of BI applications to support the Piedmont local institutions. Applications cover all BI use cases, taking into account also Data Quality and Data Mining issues. The platform includes Data Integration, Data Quality, Data Mining, other Analytics, Query and Reporting and Master Data Management tools. The paper focuses on the overall experience of setting up and managing a BI platform of such complexity and on a success case study on e-government.",,DVD:978-953-233-072-4; Electronic:978-953-233-068-7; POD:978-1-4673-2577-6,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6240911,,Bismuth;Data models;Databases;Information systems;Procurement;Synthetic aperture sonar,competitive intelligence;data integration;data mining;government data processing;public information systems,BI competency centre;BICC;CSI-Piemonte;Consortium for Information Systems in Piedmont;Italy;Piedmont local institutions;SAP BusinessObject solutions;SAS BusinessObject solutions;analytics tool;business intelligence platform;data integration;data mining issues;data quality;e-government;grid platform;master data management tools;public authorities;query tool;reporting tools,,0,,4,,no,21-25 May 2012,,IEEE,IEEE Conference Publications
Shared disk big data analytics with Apache Hadoop,A. Mukherjee; J. Datta; R. Jorapur; R. Singhvi; S. Haloi; W. Akram,"Symantec Corporation ICON, Baner Road, Pune - 411021, India",2012 19th International Conference on High Performance Computing,20130425,2012,,,1,6,"Big Data is a term applied to data sets whose size is beyond the ability of traditional software technologies to capture, store, manage and process within a tolerable elapsed time. The popular assumption around Big Data analytics is that it requires internet scale scalability: over hundreds of compute nodes with attached storage. In this paper., we debate on the need of a massively scalable distributed computing platform for Big Data analytics in traditional businesses. For organizations which don't need a horizontal., internet order scalability in their analytics platform., Big Data analytics can be built on top of a traditional POSIX Cluster File Systems employing a shared storage model. In this study., we compared a widely used clustered file system: VERITAS Cluster File System (SF-CFS) with Hadoop Distributed File System (HDFS) using popular Map-reduce benchmarks like Terasort., DFS-IO and Gridmix on top of Apache Hadoop. In our experiments VxCFS could not only match the performance of HDFS., but also outperformed in many cases. This way., enterprises can fulfill their Big Data analytics need with a traditional and existing shared storage model without migrating to a different storage model in their data centers. This also includes other benefits like stability & robustness., a rich set of features and compatibility with traditional analytics applications.",,Electronic:978-1-4673-2371-0; POD:978-1-4673-2372-7,10.1109/HiPC.2012.6507520,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6507520,Analytics;BigData;Cloud;Clustered File Systems;Hadoop,,data analysis;storage management,Apache Hadoop;DFS-IO;Gridmix;HDFS;Hadoop Distributed File System;Internet scale scalability;Map-reduce benchmark;POSIX Cluster File System;SF-CFS;Terasort;VERITAS Cluster File System;clustered file system;data capture;data center;data management;data processing;data storage;massively scalable distributed computing platform;shared disk Big Data analytics;shared storage model,,1,,10,,no,18-22 Dec. 2012,,IEEE,IEEE Conference Publications
Simulations of a dynamical system model for electronic circuits,A. A. Marins; D. M. Lyra-Leite; J. P. C. L. da Costa,"Digital Signal Processing Group, Department of Electrical Engineering, University of Bras&#x00ED;lia, Brazil",2012 Workshop on Engineering Applications,20120621,2012,,,1,6,"Models and numerical simulations are used to understand concepts of electronic circuits as well as to evaluate their performance without the actual need to build them. Today, they are mostly implemented by softwares that rely on numerical solutions of analytic models, which usually require a great amount of computational resources. In this paper, we propose an alternative particle dynamical system to model electronic circuits. Its main features are a very simple evolution rule, which resembles the principles of classical electrodynamics, and purely deterministic scenarios. A simulation method is also proposed to simulate such model in order to allow the easy comprehension of electronic properties and concepts. Also, as we show in this paper, simulations of the model succeeds in displaying electron tunneling events, even though no particle is defined in terms of quantum mechanics.",,Electronic:978-1-4673-0870-0; POD:978-1-4673-0871-7,10.1109/WEA.2012.6220090,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6220090,,Charge carriers;Integrated circuit modeling;Lattices;Logic gates;Tiles;Tin;Vectors,circuit simulation;network analysis;numerical analysis;tunnelling,analytic models;classical electrodynamics;dynamical system model;electron tunneling events;electronic circuit simulation model;electronic properties;numerical simulations;particle dynamical system;quantum mechanics,,1,,8,,no,2-4 May 2012,,IEEE,IEEE Conference Publications
Simulative Analyses of the Rotor for Magnetically Suspended Flywheel with Vernier Gimballing Capacity,J. Tang; X. Han,"Sci. &amp; Technol. on Initial Lab., Beihang Univ., Beijing, China",2012 Third International Conference on Digital Manufacturing & Automation,20120913,2012,,,776,779,"Magnetically suspended flywheels provide a number of attractive advantages over ball bearing wheels. As an important executive component of satellite attitude control system, the improvement of flywheel system is spurred by high performance satellite attitude control system. The general situation of the flywheel with vernier gimballing capacity is introduced. To reduce power consumption and augment the outputting torque of the existing magnetically suspended flywheel when the rotor is tilting, a innovative 5DOFs magnetically suspended flywheel with vernier gimballing capacity is proposed also its composition and working principle are introduced briefly. As the most important part of the flywheel, the rotor is analyzed by finite element analysis software ANSYS-Workbench, including mode, harmonic vibration and random vibration simulative analyses. The analytic results indicate that structural design of the flywheel is reasonable. The analytical method takes an important part in designing the flywheel which is not only factualistic but also effective..",,Electronic:978-0-7695-4772-5; POD:978-1-4673-2217-1,10.1109/ICDMA.2012.182,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6298631,5DOFs;Magnetically Suspended Flywheel;Simulative Analyses;Vernier Gimballing,Flywheels;Harmonic analysis;Lorentz covariance;Magnetic levitation;Rotors;Torque;Vibrations,aircraft control;attitude control;ball bearings;finite element analysis;flywheels;magnetic bearings;magnetic fluids;rotors;vibrations,ANSYS-Workbench;DOF;Vernier gimballing capacity;ball bearing wheels;degree of freedom;finite element analysis software;flywheel structural design;harmonic vibration;high performance satellite attitude control system;magnetically suspended flywheel system;power consumption;random vibration simulative analysis;rotor simulative analysis,,0,,6,,no,July 31 2012-Aug. 2 2012,,IEEE,IEEE Conference Publications
Single-Antenna Coherent Detection of Collided FM0 RFID Signals,A. Bletsas; J. Kimionis; A. G. Dimitriou; G. N. Karystinos,"Telecommunications Laboratory, Electronic and Computer Engineering Dept., Technical Univ. of Crete, Chania 73100, Greece",IEEE Transactions on Communications,20120309,2012,60,3,756,766,"This work derives and evaluates single-antenna detection schemes for collided radio frequency identification (RFID) signals, i.e. simultaneous transmission of two RFID tags, following FM0 (biphase-space) encoding. In sharp contrast to prior art, the proposed detection algorithms take explicitly into account the FM0 encoding characteristics, including its inherent memory. The detection algorithms are derived when error at either or only one out of two tags is considered. It is shown that careful design of one-bit-memory two-tag detection can improve bit-error-rate (BER) performance by 3dB, compared to its memoryless counterpart, on par with existing art for single-tag detection. Furthermore, this work calculates the total tag population inventory delay, i.e. how much time is saved when two-tag detection is utilized, as opposed to conventional, single-tag methods. It is found that two-tag detection could lead to significant inventory time reduction (in some cases on the order of 40%) for basic framed-Aloha access schemes. Analytic calculation of inventory time is confirmed by simulation. This work could augment detection software of existing commercial RFID readers, including single-antenna portable versions, without major modification of their RF front ends.",0090-6778;00906778,,10.1109/TCOMM.2011.020612.110212,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6150982,FM0 coding;Gen2;RFID;collision detection,Backscatter;Delay;Encoding;Error probability;Radiofrequency identification;Receivers;Vectors,access protocols;antennas;phase coding;radiofrequency identification;signal detection,biphase-space encoding;bit-error-rate performance;collided FM0 RFID signal detection;framed-Aloha access schemes;inventory time reduction;one-bit-memory two-tag detection design;radiofrequency identification;single-antenna coherent detection scheme;single-tag detection;total tag population inventory delay,,14,,28,,no,12-Mar,,IEEE,IEEE Journals & Magazines
Sleep and activity monitoring for Returning Soldier Adjustment Assessment,T. Yardibi; D. Cleary; J. Wood; M. Stachura; E. Wood; A. Dicks,"Software Sciences and Analytics Organization at General Electric Global Research, Niskayuna, NY 12309, USA",2012 Annual International Conference of the IEEE Engineering in Medicine and Biology Society,20121110,2012,,,2144,2148,"This paper describes the development of unobtrusive room sensors to discover relationships between sleep quality and the clinical assessments of combat soldiers suffering from post-traumatic stress disorder (PTSD) and mild traumatic brain injury (TBI). We consider the use of a remote room sensor unit composed of a Doppler radar, light, sound and other room environment sensors. We also employ an actigraphy watch. We discuss sensor implementation, radar data analytics and preliminary results using real data from a Warrior Transition Battalion located in Fort Gordon, GA. Two radar analytical approaches are developed and compared against the actigraphy watch estimates - one, emphasizing system knowledge; and the other, clustering on several radar signal features. The radar analytic algorithms are able to estimate sleep periods, signal absence and restlessness in the bed. In our test cases, the radar estimates are shown to agree with the actigraphy watch. PTSD and mild-TBI soldiers do often show signs of sporadic and restless sleep. Ongoing research results are expected to provide further insight.",1094-687X;1094687X,Electronic:978-1-4577-1787-1; POD:978-1-4244-4119-8; USB:978-1-4244-4120-4,10.1109/EMBC.2012.6346385,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6346385,,Brain injuries;Doppler radar;Measurement;Monitoring;Radar measurements;Sensors,behavioural sciences;biomedical measurement;brain;injuries;neurophysiology;patient monitoring;remote sensing by radar;sleep,Doppler radar sensor;PTSD;Warrior Transition Battalion;actigraphy watch;activity monitoring;combat soldiers;light sensor;mild TBI;mild traumatic brain injury;post traumatic stress disorder;radar analytical approach;radar data analytics;radar signal feature clustering;remote room sensor unit;returning soldier adjustment assessment;room environment sensor;sleep monitoring;sleep quality;sound sensor;system knowledge;unobtrusive room sensors,"Actigraphy;Adult;Brain Injuries;Humans;Male;Middle Aged;Military Personnel;Monitoring, Ambulatory;Point-of-Care Systems;Polysomnography;Radar;Reproducibility of Results;Sensitivity and Specificity;Sleep Stages;Stress Disorders, Post-Traumatic;Young Adult",0,,10,,no,Aug. 28 2012-Sept. 1 2012,,IEEE,IEEE Conference Publications
Smart city surveillance: Leveraging benefits of cloud data stores,S. Dey; A. Chakraborty; S. Naskar; P. Misra,"Innovation Labs, Tata Consultancy Services Ltd., Kolkata, India",37th Annual IEEE Conference on Local Computer Networks - Workshops,20130131,2012,,,868,876,"The smart cities of future need to have a robust and scalable video surveillance infrastructure. In addition it may also make use of citizen contributed video feeds, images and sound clips for surveillance purposes. Multimedia data from various sources need to be stored in large scalable data stores for compulsory retention period, on-line, off-line analytics and archival. Multimedia feeds related to surveillance are voluminous and varied in nature. Apart from large multimedia files, events detected using video analytics and associated metadata needs to be stored. The underlying data storage infrastructure therefore needs to be designed for mainly continuous streaming writes from video cameras and some variety in terms of I/O sizes, read-write mix, random vs. sequential access. As of now, the video surveillance storage domain is mostly dominated by iSCSI based storage systems. Cloud based storage is also provided by some vendors. Taking in account the need for scalability, reliability and data center cost minimization, it is worth investigating if large scale video surveillance backend can be integrated to the open source cloud based data stores available in the ‰ÛÏbig data‰Ûù trend. We developed a multimedia surveillance backend system architecture based on the Sensor Web Enablement framework and cloud based ‰ÛÏkey-value‰Ûù stores. Our framework gets data from camera/ edge device simulators, splits media files and metadata and stores those in a segregated way in cloud based data stores hosted on Amazons EC2. We have benchmarked performances of a few cloud based key-value stores under large scale video surveillance workload and demonstrated that those perform satisfactorily, bringing in inherent scalability and reliability of a cloud based storage system to a video surveillance system for a smart safe city. With a case study of the storage of video surveillance system, we show in this paper that with the availability of several cloud based d- stributed data stores and benchmarking tools, an application's data management needs can be served using hybrid cloud based data stores and selection of such stores can be facilitated using benchmark tools if the application workload characteristics are known.",,Electronic:978-1-4673-2129-7; POD:978-1-4673-2130-3,10.1109/LCNW.2012.6424076,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6424076,Cloud database;benchmarking;city surveillance;smart city;ycsb,Cameras;Cities and towns;Databases;Media;Streaming media;Video surveillance,cloud computing;multimedia computing;public domain software;video surveillance,Amazons EC2;IO sizes;citizen contributed video feeds;cloud based storage;cloud data stores;iSCSI;key-value stores;multimedia data;multimedia feeds;multimedia surveillance backend system architecture;open source cloud;random access;read-write mix;sensor Web enablement framework;sequential access;smart city surveillance;sound clips;video analytics;video surveillance infrastructure,,7,,18,,no,22-25 Oct. 2012,,IEEE,IEEE Conference Publications
Smart Traffic Cloud: An Infrastructure for Traffic Applications,W. Q. Wang; X. Zhang; J. Zhang; H. B. Lim,"Intell. Syst. Centre, Nanyang Technol. Univ., Singapore, Singapore",2012 IEEE 18th International Conference on Parallel and Distributed Systems,20130117,2012,,,822,827,"With rapid development of sensor technologies and wireless network infrastructure, research and development of traffic related applications, such as real time traffic map and on-demand travel route recommendation have attracted much more attentions than ever before. Both archived and real-time data involved in these applications could potentially be very big, depending on the number of deployed sensors. Emerging Cloud infrastructure can elastically handle such big data and conveniently providing nearly unlimited computing and storage resources to hosted applications, to carry out analysis not only for long-term planning and decision making, but also analytics for near real-time decision support. In this paper, we propose Smart Traffic Cloud, a software infrastructure to enable traffic data acquisition, and manage, analyze and present the results in a flexible, scalable and secure manner using a Cloud platform. The proposed infrastructure handles distributed and parallel data management and analysis using ontology database and the popular Map-Reduce framework. We have prototyped the infrastructure in a commercial Cloud platform and we developed a real-time traffic condition map using data collected from commuters' mobile phones.",1521-9097;15219097,Electronic:978-0-7695-4903-3; POD:978-1-4673-4565-1,10.1109/ICPADS.2012.134,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6413598,Participatory sensing;cloud computing;data management;software architecture,Distributed databases;Global Positioning System;Ontologies;Real-time systems;Roads;Smart phones;Vehicles,cloud computing;data handling;ontologies (artificial intelligence);parallel processing,Map-Reduce framework;hosted applications;mobile phones;ontology database;parallel data analysis;parallel data management;real time traffic map;sensor technologies;smart traffic cloud;storage resources;traffic applications infrastructure;traffic related applications;wireless network infrastructure,,5,,17,,no,17-19 Dec. 2012,,IEEE,IEEE Conference Publications
SnapShot: Visualization to Propel Ice Hockey Analytics,H. Pileggi; C. D. Stolper; J. M. Boyle; J. T. Stasko,Georgia Institute of Technology,IEEE Transactions on Visualization and Computer Graphics,20121008,2012,18,12,2819,2828,"Sports analysts live in a world of dynamic games flattened into tables of numbers, divorced from the rinks, pitches, and courts where they were generated. Currently, these professional analysts use R, Stata, SAS, and other statistical software packages for uncovering insights from game data. Quantitative sports consultants seek a competitive advantage both for their clients and for themselves as analytics becomes increasingly valued by teams, clubs, and squads. In order for the information visualization community to support the members of this blossoming industry, it must recognize where and how visualization can enhance the existing analytical workflow. In this paper, we identify three primary stages of today's sports analyst's routine where visualization can be beneficially integrated: 1) exploring a dataspace; 2) sharing hypotheses with internal colleagues; and 3) communicating findings to stakeholders.Working closely with professional ice hockey analysts, we designed and built SnapShot, a system to integrate visualization into the hockey intelligence gathering process. SnapShot employs a variety of information visualization techniques to display shot data, yet given the importance of a specific hockey statistic, shot length, we introduce a technique, the radial heat map. Through a user study, we received encouraging feedback from several professional analysts, both independent consultants and professional team personnel.",1077-2626;10772626,,10.1109/TVCG.2012.263,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327288,Visual knowledge discovery;human computer interaction;hypothesis testing;visual evidence;visual knowledge representation,Data visualization;Games;Human computer interaction;Knowledge discovery;Sports equipment,data analysis;data mining;data visualisation;sport;statistical analysis,R software;SAS software;SnapShot;Stata software;analytical workflow;dataspace exploration;dynamic games;game data;hockey intelligence gathering;hockey statistics;hypothesis sharing;ice hockey analytics;independent consultants;information visualization;internal colleagues;knowledge discovery;professional ice hockey analyst;professional team personnel;radial heat map;shot data display;shot length;sports analysis;stakeholders;statistical software package,0,9,,28,,no,Dec. 2012,,IEEE,IEEE Journals & Magazines
Software analytics in practice and its implications for education and training,Dongmei Zhang,,2012 IEEE 25th Conference on Software Engineering Education and Training,20120723,2012,,,xvi,xvi,"Summary form only given. Software analytics is to enable software practitioners to perform data exploration and analysis in order to obtain insightful and actionable information for data-driven tasks around software and services. In this talk, based on the success of technology transfer on software analytics at Microsoft Research Asia, I will share our experiences in carrying out successful technology transfers mainly including (1) incorporation of a broad spectrum of domain knowledge and expertise, e.g., management, machine learning, large-scale data processing and computing, and information visualization; and (2) investigation into how practitioners take actions on the produced information, and providing effective support for such information-based action taking. I will talk about the implications of our experiences for software engineering education and training, such as skill-set requirements, curriculum development, and academia-Industry collaboration.",1093-0175;10930175,Electronic:978-0-7695-4693-3; POD:978-1-4673-1592-0,10.1109/CSEET.2012.5,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6244998,,,computer science education;data analysis;data visualisation;learning (artificial intelligence);software engineering,Microsoft Research Asia;academia-Industry collaboration;curriculum development;data analysis;data computing;data exploration;data-driven task;domain expertise;domain knowledge;information visualization;information-based action taking;large-scale data processing;machine learning;management;skill-set requirement;software analytics;software engineering education;software engineering training;technology transfer,,0,,,,no,17-19 April 2012,,IEEE,IEEE Conference Publications
Software analytics in practice: Mini tutorial,D. Zhang; T. Xie,"Microsoft Research Asia, Beijing, China",2012 34th International Conference on Software Engineering (ICSE),20120628,2012,,,997,997,"Summary form only given. A huge wealth of various data exists in the software development process, and hidden in the data is information about the quality of software and services as well as the dynamics of software development. With various analytic and computing technologies, software analytics is to enable software practitioners to performance data exploration and analysis in order to obtain insightful and actionable information for data-driven tasks around software and services [1].",0270-5257;02705257,Electronic:978-1-4673-1067-3; POD:978-1-4673-1066-6,10.1109/ICSE.2012.6227121,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227121,,,,,,6,,6,,no,2-9 June 2012,,IEEE,IEEE Conference Publications
Software as an engineering material: How the affordances of programming have changed and what to do about it (Invited industrial talk),K. Braithwaite,"Z&#x00FC;hlke Engineering, London, UK",2012 34th International Conference on Software Engineering (ICSE),20120628,2012,,,998,998,"Summary form only given. A huge wealth of various data exist in the practice of software development. Further rich data are produced by modern software and services in operation, many of which tend to be data-driven and/or data-producing in nature. Hidden in the data is information about the quality of software and services and the dynamics of software development. Software analytics is to develop and apply data exploration and analysis technologies, such as pattern recognition, machine learning, and information visualization, on software data to obtain insightful and actionable information for modern software and services. This tutorial presents latest research and practice on principles, techniques, and applications of software analytics in practice, highlighting success stories in industry, research achievements that are transferred to industrial practice, and future research and practice directions in software analytics. The attendees can acquire the skills and knowledge needed to perform industrial research or conduct industrial practice in the field of software analytics and to integrate analytics in their own industrial research, practice, and training.",0270-5257;02705257,Electronic:978-1-4673-1067-3; POD:978-1-4673-1066-6,10.1109/ICSE.2012.6227251,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227251,,,,,,0,,,,no,2-9 June 2012,,IEEE,IEEE Conference Publications
Software requirement prioritization using fuzzy multi-attribute decision making,A. Ejnioui; C. E. Otero; A. A. Qureshi,"Information Technology, University of South Florida, Lakeland, Florida, USA",2012 IEEE Conference on Open Systems,20130124,2012,,,1,6,"Although many approaches have been proposed to prioritize requirements in software projects, almost none has been widely adopted. This is mostly due to their complexity, time commitment, lack of consistency, or implementation difficulties. This paper proposes a novel approach to do so that is practical, easy to implement and can show a reasonable level of consistency. In addition, it takes in consideration the imprecise nature of requirements and quality attributes by modeling the latter as fuzzy variables. The problem of prioritizing requirements is formulated as a fuzzy multi-attribute decision problem in which the expected value operator is used to rank the alternatives listed in the problem formulation. This approach can be easily extended to include other quality attributes as well as customized to fit the needs of most software projects.",,Electronic:978-1-4673-1046-8; POD:978-1-4673-1044-4,10.1109/ICOS.2012.6417646,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6417646,expected operator;fuzzy simulation;multi-attribute decision making;requirements engineering;requirements prioritization;software quality,Analytic hierarchy process;Computational modeling;Software;Software engineering;Total quality management;Vectors,decision making;formal specification;fuzzy set theory;project management;software quality;systems analysis,consistency level;expected value operator;fuzzy multiattribute decision making;fuzzy variables;implementation difficulties;quality attributes;software projects;software requirement prioritization;time commitment,,0,,31,,no,21-24 Oct. 2012,,IEEE,IEEE Conference Publications
Software Solution of Delay Differential Equations,J. KrÌ_; V. NovotnÌÁ; J. Luhan,"Dept. of Inf., BUT, Brno, Czech Republic",2012 Third World Congress on Software Engineering,20130110,2012,,,194,197,"Methods used for ordinary differential equations cannot generally be used to solve delay differential equation, which is reflected in the choice of suitable software. The paper shows possibilities of software solution of delay differential equations. The aim of the paper is to present the possibilities of current software packages and programme systems (e.g. Matlab, Maple, R). The paper further shows how the aforementioned equations can be used in solutions of dynamical models. The conclusion of the papers demonstrates a solution of a specific dynamical model - the Phillips curve applied to the Czech Republic. Setting up the model required the use of analytic and synthetic methods, dynamical modelling and solving the system of two delay differential equations. In the conclusion the authors claim that although the quality of the available software, suitable for solving delay differential equations, is not as good as that of software used for solving ordinary differential equations, such software that meets basic requirements can be found. Therefore such software can be used as a supportive tool for exact modelling methods in practice.",,Electronic:978-0-7695-4863-0; POD:978-1-4673-4546-0,10.1109/WCSE.2012.47,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6394952,Maple;Phillips curve;delay differential equation;software,Biological system modeling;Delay;Differential equations;Economics;Mathematical model;Software;Unemployment,difference equations;mathematics computing,Czech Republic;Phillips curve;delay differential equations;ordinary differential equations;programme systems;software packages;software solution,,0,,21,,no,6-8 Nov. 2012,,IEEE,IEEE Conference Publications
Software-as-a-Service evaluation in cloud paradigm: Primitive cognitive network process approach,K. K. F. Yuen,"Department of Computer Science and Software Engineering, Xi'an Jiaotong-Liverpool University, Suzhou, China","2012 IEEE International Conference on Signal Processing, Communication and Computing (ICSPCC 2012)",20121022,2012,,,119,124,"With rapid growth of Software-as-a-Services (SaaS) products in cloud paradigm, evaluation of SaaS product is essential for an enterprise to purchase a software service whilst there are a number of alternatives. This paper proposes the primitive cognitive network process (P-CNP) approach to measure the SaaS products in multi-criteria decision making aspect. The P-CNP is the rectified approach of the Analytic Hierarchy Process (AHP) in the aspects of paired interval scale and the corresponding mathematical development. The proposed approach can support the business decision maker to select the best cloud service product through user experiences and preferences.",,Electronic:978-1-4673-2193-8; POD:978-1-4673-2192-1,10.1109/ICSPCC.2012.6335719,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6335719,Analytic Hierarchy Process (AHP);Cloud Computing;Multi-Criteria Decision Making;Software Vendor Selection;Software-as-a-Service,Cloud computing;Customer relationship management;Vectors,cloud computing;decision making,AHP;P-CNP approach;SaaS products;Software-as-a-Service evaluation;analytic hierarchy process;business decision maker;cloud paradigm;cloud service product;mathematical development;multicriteria decision making aspect;primitive cognitive network process;software service,,2,,14,,no,12-15 Aug. 2012,,IEEE,IEEE Conference Publications
STINGER: High performance data structure for streaming graphs,D. Ediger; R. McColl; J. Riedy; D. A. Bader,"Georgia Inst. of Technol., Atlanta, GA, USA",2012 IEEE Conference on High Performance Extreme Computing,20130110,2012,,,1,5,"The current research focus on ‰ÛÏbig data‰Ûù problems highlights the scale and complexity of analytics required and the high rate at which data may be changing. In this paper, we present our high performance, scalable and portable software, Spatio-Temporal Interaction Networks and Graphs Extensible Representation (STINGER), that includes a graph data structure that enables these applications. Key attributes of STINGER are fast insertions, deletions, and updates on semantic graphs with skewed degree distributions. We demonstrate a process of algorithmic and architectural optimizations that enable high performance on the Cray XMT family and Intel multicore servers. Our implementation of STINGER on the Cray XMT processes over 3 million updates per second on a scale-free graph with 537 million edges.",,Electronic:978-1-4673-1576-0; POD:978-1-4673-1577-7,10.1109/HPEC.2012.6408680,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6408680,,Data structures;Instruction sets;Multicore processing;Optimization;Parallel processing;Semantics;Synchronization,data structures;graph theory;multiprocessing systems,Cray XMT family;Intel multicore servers;STINGER;algorithmic optimizations;architectural optimizations;big data problems;deletion;graph data structure;high performance data structure;insertion;semantic graphs;skewed degree distributions;spatio-temporal interaction network and graph extensible representation;streaming graphs;update,,10,,10,,no,10-12 Sept. 2012,,IEEE,IEEE Conference Publications
Stock Market Volatility Prediction: A Service-Oriented Multi-kernel Learning Approach,F. Wang; L. Liu; C. Dou,"State Key Lab. of Software Eng., Wuhan Univ., Wuhan, China",2012 IEEE Ninth International Conference on Services Computing,20120823,2012,,,49,56,"Stock market is an important and active part of nowadays financial markets. Stock time series volatility analysis is regarded as one of the most challenging time series forecasting due to the hard-to-predict volatility observed in worldwide stock markets. In this paper we argue that the stock market state is dynamic and invisible but it will be influenced by some visible stock market information. Existing research on financial time series analysis and stock market volatility prediction can be classified into two categories: in depth study of one market factor on the stock market volatility prediction or prediction by combining historical price fluctuations with either trading volume or news. In this paper we present a service-oriented multi-kernel based learning framework (MKL) for stock volatility analysis. Our MKL service framework promotes a two-tier learning architecture. In the top tier, we develop a suite of data preparation and data transformation techniques to provide a source-specific modeling, which transforms and normalizes a source specific input dataset into the MKL ready data representation. Then we apply data alignment techniques to prepare the datasets from multiple information sources based on the classification model we choose for cross-source correlation analysis. In the next tier, we develop model integration methods to perform three analytic tasks: (i) building one sub-kernel per source, (ii) learning and tuning the weights for sub-kernels through weight adjustment methods and (iii) performing multi-kernel based cross-correlation analysis of market volatility. To validate the effectiveness of our service oriented MKL approach, we performed experiments on HKEx 2001 stock market datasets with three important market information sources: historical prices, trading volumes and stock related news articles. Our experiments show that 1) multi-kernel learning method has a higher degree of accuracy and a lower degree of false prediction, compared to exis- ing single kernel methods; and 2) integrating both news and trading volume data with historical stock price information can significantly improve the effectiveness of stock market volatility prediction, compared to many existing prediction methods.",,Electronic:978-07695-4753-4; Electronic:978-1-4673-3049-7; POD:978-1-4673-3049-7,10.1109/SCC.2012.35,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6274126,multi-data source integration;multiple kernel learning;stock prediction;support vector machine,Correlation;Data mining;Data models;Kernel;Predictive models;Stock markets;Time series analysis,data structures;forecasting theory;learning (artificial intelligence);service-oriented architecture;stock markets;time series,MKL;classification model;cross-source correlation analysis;data preparation;data representation;data transformation;financial markets;financial time series analysis;historical stock price information;service-oriented multi-kernel learning approach;source specific input dataset;stock market information;stock market volatility prediction;stock time series volatility;time series forecasting;two-tier learning architecture,,8,,5,,no,24-29 June 2012,,IEEE,IEEE Conference Publications
Streamlining Service Levels for IT Infrastructure Support,G. K. Palshikar; M. Mudassar; H. M. Vin; M. Natu,"Tata Res. Dev. &amp; Design Centre, Tata Consultancy Services Ltd., Pune, India",2012 IEEE 12th International Conference on Data Mining Workshops,20130110,2012,,,309,316,"For IT Infrastructure Support (ITIS), it is crucial to identify opportunities for reducing service costs and improving service quality. We focus on streamlining service levels i.e., finding right resolution level for each ticket, to reduce time, efforts and cost for ticket handling, without affecting workloads and user satisfaction. We formalize this problem and present two statistics-based search algorithms for identifying problems suitable for left-shift (from expensive, expertise intensive L2 level to cheaper, simpler L1 level) and right-shift (from L1 to L2). The approach is domain-driven: it produces directly usable and often novel results, without any trial-and error experimentation, along with detailed justifications and predicted impacts. This helps in acceptance among end-users and more active use of the results. We discuss one real-life case-study of results produced by the algorithms.",2375-9232;23759232,Electronic:978-1-4673-5164-5; POD:978-1-4799-1707-5,10.1109/ICDMW.2012.118,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6406456,Customer Support;Domain-driven Data-mining;IT infrastructure Support;ITIL;Service quality;Support Analytics;Ticket routing;Workforce management,Algorithm design and analysis;Business;Data mining;Databases;Prediction algorithms;Software;Training,business data processing;cost reduction;data mining;resource allocation;search problems;text analysis,IT infrastructure support;ITIS;data mining;domain-driven approach;end-user acceptance;left-shift;right-shift;service cost reduction;service level management process;service quality improvement;statistics-based search algorithms;ticket handling cost reduction;time reduction;user satisfaction,,1,,17,,no,10-10 Dec. 2012,,IEEE,IEEE Conference Publications
Study on Innovative Capability Evaluation of Knowledge-innovation Talents Based on AHP: A Case Study on Henan Province,Y. Cui,"Bus. Sch., Hohai Univ., Nanjing, China",2012 Second International Conference on Business Computing and Global Informatization,20121220,2012,,,562,564,"Knowledge-innovation talents, as the core element of competition and technological development, are the first strategic resources of the national and regional development. Then, the innovation capacity of technological innovators is the fundamental factor which influences the effect of the technology and knowledge innovators and is also the research focus of this essay. According to the AHP, the paper built the evaluation model for the innovation capacity of technological innovators of Henan Province.",2378-8941;23788941,Electronic:978-0-7695-4854-8; POD:978-1-4673-4469-2,10.1109/BCGIN.2012.152,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6382594,AHP Method;Capability Evaluation;Innovative Capability;Knowledge-innovation Talents,Cognition;Educational institutions;Software;Standards;Technological innovation,analytic hierarchy process;innovation management;knowledge management;regional planning,AHP;Henan Province;analytic hierarchy process;competition development;innovation capacity;innovative capability evaluation;knowledge-innovation talents;national development;regional development;technological development,,0,,7,,no,12-14 Oct. 2012,,IEEE,IEEE Conference Publications
Supporting sustainability with software ‰ÛÓ An industrial perspective (Keynote),F. D. Clesle,"SAP, Germany",2012 34th International Conference on Software Engineering (ICSE),20120628,2012,,,962,962,"Summary form only given. TechnoAware research and develops technologies and solutions for ambient intelligence. Established in 2003 TechnoAware was born from the experiences and competencies of the ISIP40 research group of the University of Genova. This research group is studying and implementing video analytics algorithms since 1985 and is considered nowadays one of the major actors in this filed worldwide. Entirely made up by researchers and experts in the video analytics field, TechnoAware main principles are: proprietary technologies (highly customizable and modular solutions), scientific competencies (high quality level and performances), continuous research and technological innovation (cutting edge products).",0270-5257;02705257,Electronic:978-1-4673-1067-3; POD:978-1-4673-1066-6,10.1109/ICSE.2012.6227254,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227254,,,,,,0,,,,no,2-9 June 2012,,IEEE,IEEE Conference Publications
Synchronous Parallel Processing of Big-Data Analytics Services to Optimize Performance in Federated Clouds,G. Jung; N. Gnanasambandam; T. Mukherjee,"Xerox Res. Center Webster, Webster, MA, USA",2012 IEEE Fifth International Conference on Cloud Computing,20120802,2012,,,811,818,"Parallelization of big-data analytics services over a federation of heterogeneous clouds has been considered to improve performance. However, contrary to common intuition, there is an inherent tradeoff between the level of parallelism and the performance for big-data analytics principally because of a significant delay for big-data to get transferred over the network. The data transfer delay can be comparable or even higher than the time required to compute data. To address the aforementioned tradeoff, this paper determines: (a) how many and which computing nodes in federated clouds should be used for parallel execution of big-data analytics; (b) opportunistic apportioning of big-data to these computing nodes in a way to enable synchronized completion at best-effort performance; and (c) sequence of apportioned, different sizes of big-data chunks to be computed in each node so that transfer of a chunk is overlapped as much as possible with the computation of the previous chunk in the node. In this regard, Maximally Overlapped Bin-packing driven Bursting (MOBB) algorithm is proposed, which improve the performance by up to 60% against existing approaches.",2159-6182;21596182,Electronic:978-0-7695-4755-8; POD:978-1-4673-2892-0,10.1109/CLOUD.2012.108,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6253583,big-data analytics;federated clouds;parallelization,Computational modeling;Data mining;Delay;Estimation;Parallel processing;Sorting;Synchronization,bin packing;cloud computing;data analysis;software performance evaluation,MOBB algorithm;big-data analytics services;big-data chunks;computing nodes;data transfer delay;federated clouds;heterogeneous clouds;maximally overlapped bin-packing driven bursting algorithm;opportunistic apportioning;parallel execution;performance improvement;performance optimization;synchronous parallel processing,,14,,22,,no,24-29 June 2012,,IEEE,IEEE Conference Publications
Teaching and Training for Software Analytics,D. Zhang; Y. Dang; S. Han; T. Xie,"Microsoft Res. Asia, Beijing, China",2012 IEEE 25th Conference on Software Engineering Education and Training,20120723,2012,,,92,92,"Software analytics is to enable software practitioners to perform data exploration and analysis in order to obtain insightful and actionable information for data-driven tasks around software and services. When applying analytic technologies in practice of software analytics, one should incorporate (1) a broad spectrum of domain knowledge and expertise, e.g., management, machine learning, large-scale data processing and computing, and information visualization; and (2) investigate how practitioners take actions on the produced information, and provide effective support for such information-based action taking. This tutorial instructs materials to equip participants with skills and knowledge of conducting software analytics along with teaching and training students and practitioners for software analytics in university or industrial settings.",1093-0175;10930175,Electronic:978-0-7695-4693-3; POD:978-1-4673-1592-0,10.1109/CSEET.2012.14,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6245015,software analytics;software engineering education and training;technology transfer,Conferences;Educational institutions;Machine learning;Software;Software engineering;Training;Tutorials,computer science education;data analysis;data visualisation;learning (artificial intelligence);program diagnostics;teaching;training,analytic technology;data analysis;data exploration;data-driven task;domain expertise;domain knowledge;information visualization;information-based action taking;large-scale data processing;machine learning;management;software analytics;software practitioners;student training;teaching,,0,,3,,no,17-19 April 2012,,IEEE,IEEE Conference Publications
The effect of aggregation and defuzzification method selection on the risk level calculation,E. TÌ_th-Laufer; M. TakÌÁcs,"Doctoral School, &#x00D3;buda University, Budapest, Hungary",2012 IEEE 10th International Symposium on Applied Machine Intelligence and Informatics (SAMI),20120531,2012,,,131,136,"In this paper a fuzzy logic-based hierarchical multilevel risk calculation model will be introduced with different model parameters. On each occasion when a fuzzy-based simulation model is constructed, the appropriate aggregation and defuzzification method must be chosen. It is very difficult because it cannot be said generally, which is the best method, its depends on the current application. The model presented in the paper is a model for risk calculation of physical exercise, and it was constructed in Simulink - Fuzzy Logic Toolbox environment with Mamdani-type fuzzy evaluation and different aggregation and defuzzification operators. The test was performed for several typical groups of the patients. The results are compared with a previously implemented Analytic Hierarchy Process with Fuzzy Comprehensive Evaluation based model with similar purposes. The result of the comparison has been analyzed and the best methods have been selected.",,Electronic:978-1-4577-0197-9; POD:978-1-4577-0196-2,10.1109/SAMI.2012.6208943,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6208943,,Application software;Data models;Educational institutions;Fuzzy logic;Informatics;Mathematical model;Moment methods,decision making;fuzzy logic;health care;risk analysis,Mamdani-type fuzzy evaluation;Simulink;analytic hierarchy process;defuzzification method selection;defuzzification operators;fuzzy comprehensive evaluation;fuzzy logic toolbox environment;fuzzy logic-based hierarchical multilevel risk calculation;fuzzy-based simulation model;physical exercise;risk level calculation,,3,,12,,no,26-28 Jan. 2012,,IEEE,IEEE Conference Publications
The modeling and optimization of software engineering processes,A. Harchenko; I. Bodnarchuk; V. Yatcyshyn,,"Proceedings of International Conference on Modern Problem of Radio Engineering, Telecommunications and Computer Science",20120507,2012,,,326,326,"The using of the method of insertion programming and Saaty Analytic Hierarchy Process (AHP) is described for formalization and optimization of software engineering processes. The procedure of formalized representation of requirements specification for software system (SWS) is shown, and the algorithm of optimal selection of SWS architecture in case of large quantity of quality parameters is presented too.",,Electronic:978-617-607-138-9; POD:978-1-4673-0283-8,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6192592,Analytic Hierarchy Process;architecture;base protocols;formalization;requirements specification;software system,Minimization;Service oriented architecture,decision making;formal specification;logic programming;software engineering,AHP;SWS architecture optimal selection;Saaty analytic hierarchy process;insertion programming method;quality parameters;software engineering processes formalization;software engineering processes modeling;software engineering processes optimization;software system,,1,,4,,no,21-24 Feb. 2012,,IEEE,IEEE Conference Publications
The Software Quality Evaluation Method Based on Software Testing,L. Wen-Hong; W. Xin,"Beijing Inst. of Tracking & Telecommun. Technol., Beijing, China",2012 International Conference on Computer Science and Service System,20121231,2012,,,1467,1471,"In order to improve the effectiveness, visibility and specification of the TT&C software testing, this paper made an in-depth study according to its characteristics. At first, this paper presented a quality assessment model with high reliability and real-time demands getting idea from analytic hierarchy process (AHP). Then, the paper brought forward a dedicated simulation test environment and a generating method of software testing cases based on fault tree analysis (FTA). Next, the paper defined the software testing procedure learning from CMMI demands. At last, the paper gave the performance of quantitative assessment results in the form of a radar chart. Practice has proved that the given model of this paper can represent the software quality objectively, the generating method can improve the sufficiency of software testing cases effectively, and the defined procedure can ensure the specification of software testing availably.",,Electronic:978-0-7695-4719-0; POD:978-1-4673-0721-5,10.1109/CSSS.2012.369,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6394607,Software testing;software process management;software quality evaluation,Data models;Measurement;Planning;Software quality;Software testing,Capability Maturity Model;fault trees;program testing;software quality,AHP;CMMI demands;FTA;TT&C software testing;analytic hierarchy process;dedicated simulation test environment;fault tree analysis;generating method;quality assessment model;radar chart;software quality evaluation method;software specification;software testing procedure learning,,0,,3,,no,11-13 Aug. 2012,,IEEE,IEEE Conference Publications
Towards Quality Aware Collaborative Video Analytic Cloud,J. Lee; T. Feng; W. Shi; A. Bedagkar-Gala; S. K. Shah; H. Yoshida,"Dept. of Comput. Sci., Univ. of Houston, Houston, TX, USA",2012 IEEE Fifth International Conference on Cloud Computing,20120802,2012,,,147,154,"As cloud diversifies into different application fields, understanding and characterizing the specific work load sand application requirements play important roles in the design of efficient cloud infrastructure and system software support. Video analytic is a rapidly advancing field and it is widely used in many application domains (i.e., health, medical care, surveillance, and defense). To support video analytic applications efficiently in cloud, one has to overcome many challenges such as lack of understanding of the relationship and trade off between analytic performance metrics and resource requirements. Furthermore, cloud computing has grown from the early model of resource sharing to data sharing and workflow sharing. To address the challenges and to lever age emerging trends, we propose and experiment with a domain specific cloud environment for video analytic applications. We design a cloud infrastructure framework for sharing video data, analytic software, and workflow. In addition, we create a video analytic quality aware resource plan model to guarantee users QoS and optimize usage of resources based on predictive knowledge of video analytic softwares performance metrics and a resource planning model that optimizes the overall analytic service quality under users constraints (i.e., time and cost).The predictive knowledge is represented as input and analytic software specific predictors. The experimental results show that the video analytic quality aware resource planning model can balance the tradeoff between analytic quality and resource requirements, and achieve optimal or near-optimal planning for video analytic workloads with constraints in a resource shared environment. Simulation studies show that resource planning results using ground truth and video analytic performance predictions are very similar, which indicates that our analytic quality/resource predictors are very accurate.",2159-6182;21596182,Electronic:978-0-7695-4755-8; POD:978-1-4673-2892-0,10.1109/CLOUD.2012.141,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6253500,Cloud Computing;Planning;Quality Prediction;Video Analytic,Algorithm design and analysis;Analytical models;Measurement;Planning;Prediction algorithms;Software;Streaming media,cloud computing;groupware;knowledge representation;quality of service;resource allocation;software metrics;software performance evaluation;video signal processing,analytic performance metrics;application requirement;cloud computing;cloud environment;cloud infrastructure design;defense;health;medical care;overall analytic service quality optimization;predictive knowledge representation;quality aware collaborative video analytic cloud;resource plan model;resource planning model;resource predictors;resource requirements;resource shared environment;resource sharing;resource usage optimization;surveillance;system software support;user constraint;users QoS guarantee;video analytic application;video analytic software performance metrics;video analytic workload;video data sharing;workflow sharing,,5,,29,,no,24-29 June 2012,,IEEE,IEEE Conference Publications
Tracking aggregate vs. individual gaze behaviors during a robot-led tour simplifies overall engagement estimates,H. Knight; R. Simmons,"Carnegie Mellon's Robotics Institute, 5000 Forbes Avenue, Pittsburgh, PA",2012 7th ACM/IEEE International Conference on Human-Robot Interaction (HRI),20120730,2012,,,175,176,"As an early behavioral study of what non-verbal features a robot tourguide could use to analyze a crowd, personalize an interaction and maintain high levels of engagement, we analyze participant gaze statistics in response to a robot tour guide's deictic gestures. There were thirty-seven participants overall, split into nine groups of three to five people each. In groups with the lowest engagement levels aggregate gaze response to the robot's pointing gesture involved the fewest total glance shifts, least time spent looking at indicated object and no intra-participant gaze. Our diverse participants had overlapping engagement ratings within their group, and we found that a robot that tracks group rather than individual analytics could capture less noisy and often stronger trends relating gaze features to self-reported engagement scores. Thus we have found indications that aggregate group analysis captures more salient and accurate assessments of overall humans-robot interactions, even with lower resolution features.",2167-2121;21672121,Electronic:978-1-4503-1063-5; POD:978-1-4503-1063-5,10.1145/2157689.2157742,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6249512,Human tracking;Humans-robot interaction;Low resolution sensing;Social dynamics modeling,Aggregates;Heating;Humans;Robot sensing systems;Software;Vents,human-robot interaction;mobile robots,aggregate group analysis;aggregate tracking;humans-robot interactions;individual gaze behaviors;nonverbal features;overall engagement estimates;overlapping engagement ratings;participant gaze statistics;robot pointing gesture;robot tour guide deictic gestures;robot-led tour,,1,,5,,no,5-8 March 2012,,IEEE,IEEE Conference Publications
TSVHOA-traffic sensitive Vertical Handoff algorithm for best quality network technology,A. Bhuvaneswari; E. G. D. P. Raj,"Dept. of Computer Science, Cauvery College for Women, Trichy, Tamilnadu, India",International Conference on Software Engineering and Mobile Application Modelling and Development (ICSEMA 2012),20130701,2012,,,1,6,"Growth in the field of wireless communication technology has captivated the user to make use of it in recent years. The user expects more supporting services to access the best available network which satisfies user preferences and application requirements. To fulfill the requirements of the user, the next generation mobile terminals have multiple interface technologies, which allow data to be received over multiple system bearers with different characteristics. During user mobility, there is a need to have vertical handoff solution to have seamless communication. The main issue is how to select the best available network to meet requirements of user, network and application during handoff. This paper proposes an algorithm TSVHOA for Vertical Handoff decision to select the best quality network technology based on the traffic type using Analytical Hierarchical Process (AHP). Also, the algorithm is simulated with respect to different network technologies like WiFi, WiMAX and WCDMA to select the best one.",,,10.1049/ic.2012.0159,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6549327,AHP;TSVHOA;Vertical Handoff;WCDMA;WiFi;WiMAX,,analytic hierarchy process;mobility management (mobile radio);radio networks;telecommunication traffic,AHP;TSVHO;WCDMA;WiFi;WiMAX;analytical hierarchical process;best quality network technology;next generation mobile terminals;traffic sensitive vertical handoff algorithm;traffic type;user mobility;wireless communication technology,,0,,,,no,19-21 Dec. 2012,,IET,IET Conference Publications
Two Visualization Tools for Analyzing Agent-Based Simulations in Political Science,R. J. Crouser; D. Kee; D. Jeong; R. Chang,Tufts University,IEEE Computer Graphics and Applications,20111222,2012,32,1,67,77,"Agent-based modeling has become a key technique for modeling and simulating dynamic, complicated behaviors in the social and political sciences. Although many robust toolkits for developing and running these simulations exist, systems that support analysis of their results are few and tend to be overly general. So, social scientists have had difficulty interpreting the results of their increasingly complex simulations. To help bridge this gap between data generation and interpretation, researchers collaborated with political science analysts to design two tools for interactive data exploration and domain-specific data analysis. Testing by the analysts validated that these tools provided an efficient framework to explore individual trajectories and the relationships between variables. The tools also supported hypothesis generation by enabling analysts to group simulations according to multidimensional similarity and drill down to investigate further.",0272-1716;02721716,,10.1109/MCG.2011.90,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6051409,agent-based simulation;computer graphics;graphics and multimedia;political science;visual analytics systems,Analytical models;Computational modeling;Data analysis;Data models;Data visualization;Image color analysis,data analysis;data visualisation;digital simulation;politics;software agents,agent-based modeling;agent-based simulation;data generation;data interpretation;domain-specific data analysis;hypothesis generation;interactive data exploration;multidimensional similarity;political science;visualization tool,,1,,10,,no,Jan.-Feb. 2012,,IEEE,IEEE Journals & Magazines
Undercovering research trends: Network analysis of keywords in scholarly articles,A. Duvvuru; S. Kamarthi; S. Sultornsanee,"Department of Mechanical and Industrial Engineering, Northeastern University, Boston, Massachusetts, USA",2012 Ninth International Conference on Computer Science and Software Engineering (JCSSE),20120809,2012,,,265,270,"In this study we present a network approach to uncovering trends in an area of research by analyzing keywords appearing in scholarly articles. We represent keywords as nodes. We link a pair of keywords if they appear in the same article. Each link is assigned a weight, representing the number of co-occurrences of the pair in different articles. We perform a statistical and visual analysis of the network's structural and temporal characteristics. These characteristics or patterns provide a broad understanding of how keywords are organized and research areas evolved over time. Our findings show that keywords organize themselves into three categories: topical keywords, complimentary keywords and, diverse keywords. Through comparative analysis of the networks built from articles published in two successive time windows, we are able to detect some interesting keyword patterns and emerging areas. Results from this analysis can be used for identifying emerging research areas or updating academic programs and course curricula. To demonstrate the approach we use keywords from the European Journal of Operational Research.",,Electronic:978-1-4673-1921-8; POD:978-1-4673-1920-1,10.1109/JCSSE.2012.6261963,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6261963,Complex networks;Data analysis;Information extraction;Research trends;Visual analytics;Visualization,Color;Educational institutions;Job shop scheduling;Organizations;Statistical analysis;Visualization,data analysis;data visualisation;information retrieval;statistical analysis,European Journal of Operational Research;academic programs;complimentary keywords;course curricula;diverse keywords;keyword network analysis;keyword pattern detection;network structural characteristics;network temporal characteristics;research trend undercovering;scholarly articles;statistical analysis;topical keywords;visual analysis,,2,,,,no,May 30 2012-June 1 2012,,IEEE,IEEE Conference Publications
User-driven cloud transportation system for smart driving,M. Ma; Y. Huang; C. H. Chu; P. Wang,"School of Electronics Engineering and Computer Science Peking University, Beijing, China",4th IEEE International Conference on Cloud Computing Technology and Science Proceedings,20130204,2012,,,658,665,"Intelligent transportation systems (ITS) have emerged as an efficient and effective way of alleviating the traffic congestion and improving the performance of transportation systems. Key challenges of ITS in recent years include the pervasive data collection, data security, privacy preserving, large volume data processing, and intelligent analytics. These challenges lead to a revolution in ITS development by leveraging the crowdsourcing scheme and cloud computing architecture. In this paper, we propose a user-driven Cloud Transportation system (CTS) which employs a scheme of user-driven crowdsourcing to collect user data for traffic model construction and congestion prediction including data collection, filtering, modeling, intelligent computation and publish. We describe in details the application scenario, system architecture, and core CTS services model. To verify the feasibility of our approach, we have developed a prototype system which elaborated the cloud architecture and other implementation details. This paper aims to inspire further research of user-driven CTS on intelligent data processing model for smarter utilization of transportation infrastructure.",,Electronic:978-1-4673-4510-1; POD:978-1-4673-4511-8; USB:978-1-4673-4509-5,10.1109/CloudCom.2012.6427600,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6427600,Cloud Computing;Cloud Trasoortation Svstem (CTS);Smart Driving;User-Driven,Cloud computing;Computational modeling;Computer architecture;Data models;Servers;Vehicles,automated highways;cloud computing;data acquisition;data analysis;data models;data privacy;information filtering;road traffic;security of data,CTS;ITS;cloud computing architecture;cloud transportation system;data filtering;data modeling;data privacy;data processing;data publish;data security;intelligent analytics;intelligent data processing;intelligent transportation system;pervasive data collection;smart driving;traffic congestion prediction;traffic model construction;transportation infrastructure;user driven crowdsourcing,,1,,41,,no,3-6 Dec. 2012,,IEEE,IEEE Conference Publications
Using Analytic Network Process to analyze influencing factors of project complexity,Q. h. He; L. Luo; J. Wang; Y. k. Li; L. Zhao,"Sch. of Econ. &amp; Manage., Tongji Univ., Shanghai, China",2012 International Conference on Management Science & Engineering 19th Annual Conference Proceedings,20130117,2012,,,1781,1786,"The management of project complexity has become an important part in the project management, being critical to the success of the large complex project. According to literature review and questionnaire survey, the Analytical Network Process (ANP) method is used to measure the influencing factors of project complexity and Super Decisions (SD) software is used to calculate weights of influencing factors, so as to identify the key influencing factors to manage projects better. Results found that cross-organizational interdependence, multiple stakeholders, number of organizational structure hierarchy, project team's trust and diversity of technology are the five key factors which have the biggest influence on the project complexity. This research provides the scientific support for the practice of project management, which has theory direction significance for mega and complex project management.",2155-1847;21551847,Electronic:978-1-4673-3014-5; POD:978-1-4673-3015-2,10.1109/ICMSE.2012.6414413,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6414413,ANP;influencing factors;project complexity;super decisions,Complexity theory;Cultural differences;Indexes;Organizations;Project management;Software;Uncertainty,computational complexity;decision support systems;organisational aspects;project management,ANP;SD;analytic network process;cross-organizational interdependence;project complexity;project management;scientific support;super decisions software,,0,,20,,no,20-22 Sept. 2012,,IEEE,IEEE Conference Publications
Using Pregel-like Large Scale Graph Processing Frameworks for Social Network Analysis,L. Quick; P. Wilkinson; D. Hardcastle,"Gov. Commun. Headquarters, Cheltenham, UK",2012 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining,20130204,2012,,,457,463,"Pregel is a system for large scale graph processing developed at Google. It provides a scalable framework for running graph analytics on clusters of commodity machines. In this paper, we present several important undirected graph algorithms for social network analysis which fit within this framework. We discuss various graph componentisation methods, diameter estimation, degrees of separations, along with triangle, k-core and k-truss finding and computing clustering coefficients. Finally we present some experimental results using our own implementation of the Pregel framework, and examine key features of the general framework and algorithmic design.",,Electronic:978-0-7695-4799-2; POD:978-1-4673-2497-7,10.1109/ASONAM.2012.254,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6425724,,Algorithm design and analysis;Clustering algorithms;Computational modeling;Estimation;Open source software;Random access memory;Social network services,graph theory;pattern clustering;social networking (online),Google;Pregel-like large scale graph processing frameworks;algorithmic design;clustering coefficients;commodity machines;degrees of separations;diameter estimation;general framework;graph componentisation methods;k-core finding;k-truss finding;social network analysis;triangle finding;undirected graph algorithms,,1,,28,,no,26-29 Aug. 2012,,IEEE,IEEE Conference Publications
Using technical debt data in decision making: Potential decision approaches,C. Seaman; Y. Guo; N. Zazworka; F. Shull; C. Izurieta; Y. Cai; A. VetrÌ_,"Information Systems Department, UMBC, Baltimore, MD, USA",2012 Third International Workshop on Managing Technical Debt (MTD),20120628,2012,,,45,48,"The management of technical debt ultimately requires decision making - about incurring, paying off, or deferring technical debt instances. This position paper discusses several existing approaches to complex decision making, and suggests that exploring their applicability to technical debt decision making would be a worthwhile subject for further research.",,Electronic:978-1-4673-1749-8; POD:978-1-4673-1748-1,10.1109/MTD.2012.6225999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6225999,Analytic Hierarchy Process;cost-benefit analysis;decision making;options;portfolio management;technical debt,Decision making;Investments;Maintenance engineering;Mathematical model;Portfolios;Software;USA Councils,decision making;software cost estimation;software development management,analytic hierarchy process;cost-benefit analysis;decision making;portfolio management;technical debt data;technical debt management,,27,,13,,no,5-5 June 2012,,IEEE,IEEE Conference Publications
Using the fuzzy analytic hierarchy process to the balanced scorecard: A case study for the elementary and secondary schools' information department of south Taiwan,Yi-Hui Liang,"Department of Information Management, I-SHOU University, Kaohsiung 84001, Taiwan, R.O.C.",2012 International Conference on Machine Learning and Cybernetics,20121124,2012,4,,1424,1428,"The purpose of this study is to establish balanced scorecard (BSC) in performance measurement of elementary and secondary schools' MIS Department. We take a broader definition of elementary and secondary schools' MIS Department as ‰ÛÏan assembly which brings forth some specific functional activities to fulfill the task of MIS.‰Ûù BSC used as a measurement tool to assess study subjects, according to its strategy and goal formed by its assignment property, can be divided into four dimensions: finance, customer, inter process, learning and growth, which can provide us with a timely, efficient, flexible, simple, accurate, and highly overall reliable measurement tool. In order to extract the knowledge and experience from related experts to pick out important evaluation criteria and opinion, this study combines fuzzy theory and the analytical hierarchy process (AHP) to calculate the weights. After completing weighted calculation of every dimension and indicator, the BSC model is thus established. The findings of this study show that the indicator weightings between and among all the levels are not the same, rather there exists certain amount of differences. The degrees of attention drawing in order of importance among all dimensions are customer, financial, internal process and learning and growth dimension. After comprehensively analyzing indictors of performance measurement included in every level, the highly valued top five indictors are, when conducting dimension performance measurement in elementary and secondary schools' MIS Department, ‰ÛÏRationalize software and hardware and maintenance expenses,‰Ûù ‰ÛÏBudget satisfy and control,‰Ûù ‰ÛÏQuick response and handling,‰Ûù ‰ÛÏImprove service quality,‰Ûù and ‰ÛÏHigh effective information system‰Ûù.",2160-133X;2160133X,Electronic:978-1-4673-1487-9; POD:978-1-4673-1484-8,10.1109/ICMLC.2012.6359574,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6359574,AHP;Balanced scorecard;Fuzzy;MIS;Performance measurement,Abstracts;Analytical models;Educational institutions;Information systems;Lead;Performance evaluation,analytic hierarchy process;educational institutions;fuzzy systems,BSC;balanced scorecard;evaluation criteria;fuzzy analytic hierarchy process;indicator weightings;knowledge extraction;school information department;schools MIS Department,,0,,16,,no,15-17 July 2012,,IEEE,IEEE Conference Publications
Validation and optimization of modular railgun model,Y. Hu; P. Ma; M. Yang; Z. Wang,"Control and Simulation Center, Harbin Institute of Technology, 150001, China",2012 16th International Symposium on Electromagnetic Launch Technology,20121008,2012,,,1,6,"Considering the electromagnetic railgun structure features, an applied model has been established by modularization modeling method. Because model validation plays a key role in system simulation for analysis and optimization of railgun, a novel validation method is proposed in this paper. By utilizing the railgun dynamic test and predicted data, the simulation model is validated based on theil's inequality coefficient (TIC) method. To execute sufficient validation of the model and improve reliability of the validation results, similarity theory is adopted to analyze the credibility of simulation model by comparing the characteristics of the railgun. Inspired by analytic hierarchy process (AHP) for multiple attribute decision making, we define the muzzle velocity, current peak and discharging time as evaluation index for assessing the credibility of the simulation model and then evaluation structure is established. The final synthesis results demonstrate the feasibility of the adopted validation method and the validity of the modular model. The whole system model is run under MATLAB software platform, which is divided into several main components encapsulated into classes by object-oriented technology. On the basis of the validated model, an optimization pursuing highest velocity with the constraint of the current peak and the residual current in the rail is carried out to find a set of optimal parameters for the railgun using genetic algorithm.",,Electronic:978-1-4673-0305-7; POD:978-1-4673-0306-4,10.1109/EML.2012.6325055,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6325055,analytic hierarchy process;electromagnetic railgun;genetic algorithm;model validation;modularized design;optimization;similarity theory,Analytical models;Mathematical model;Object oriented modeling;Optimization;Projectiles;Railguns;Rails,decision making;discharges (electric);genetic algorithms;railguns;reliability,AHP;MATLAB software platform;TIC method;Theil inequality coefficient;analytic hierarchy process;current peak;discharging time;electromagnetic railgun structure features;evaluation index;genetic algorithm;model validation;modular railgun model;modularization modeling method;multiple attribute decision making;muzzle velocity;object-oriented technology;railgun analysis;railgun dynamic test;railgun optimization;reliability;residual current;similarity theory;system simulation;validation method,,5,,15,,no,15-19 May 2012,,IEEE,IEEE Conference Publications
VAST 2012 Mini-Challenge 2: Chart- and Matrix-based approach to network operations forensics,J. Hildenbrand; D. I. Paval; P. Thapa; C. Rohrdantz; F. Mansmann; E. Bertini; T. Schreck,"University of Konstanz, Germany",2012 IEEE Conference on Visual Analytics Science and Technology (VAST),20130103,2012,,,287,288,"We report the approach and results on the VAST 2012 MiniChallenge 2: Bank of Money Regional Office Network Operations Forensics. Using commercial data mining, visualization and database software such as KNIME, Tableau and MySQL as well as a custom-written source vs. destination IP pixel matrix, our team of students identified suspicious IRC traffic, an attack on the firewall, a drop in the firewall connections, an attempt for sensitive information exchange and a possible Distributed Denial-of-Service attack executed partly from a host within the bank network.",,Electronic:978-1-4673-4753-2; POD:978-1-4673-4752-5,10.1109/VAST.2012.6400513,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6400513,,,,,,0,,1,,no,14-19 Oct. 2012,,IEEE,IEEE Conference Publications
VAST Challenge 2012: Visual analytics for big data,K. Cook; G. Grinstein; M. Whiting; M. Cooper; P. Havig; K. Liggett; B. Nebesh; C. L. Paul,Pacific Northwest National Laboratory,2012 IEEE Conference on Visual Analytics Science and Technology (VAST),20130103,2012,,,251,255,"The 2012 Visual Analytics Science and Technology (VAST) Challenge posed two challenge problems for participants to solve using a combination of visual analytics software and their own analytic reasoning abilities. Challenge 1 (C1) involved visualizing the network health of the fictitious Bank of Money to provide situation awareness and identify emerging trends that could signify network issues. Challenge 2 (C2) involved identifying the issues of concern within a region of the Bank of Money network experiencing operational difficulties utilizing the provided network logs. Participants were asked to analyze the data and provide solutions and explanations for both challenges. The data sets were downloaded by nearly 1100 people by the close of submissions. The VAST Challenge received 40 submissions with participants from 12 different countries, and 14 awards were given.",,Electronic:978-1-4673-4753-2; POD:978-1-4673-4752-5,10.1109/VAST.2012.6400529,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6400529,Visual analytics;contest;evaluation;human information interaction;metrics;sense making,,,,,5,,5,,no,14-19 Oct. 2012,,IEEE,IEEE Conference Publications
VDQAM: A toolkit for database quality evaluation based on visual morphology,D. Teng; H. Yang; C. Ma; H. Wang,"Inst. of Software, Beijing, China",2012 IEEE Conference on Visual Analytics Science and Technology (VAST),20130103,2012,,,245,246,"Data quality evaluation is one of the most critical steps during the data mining processes. Data with poor quality often leads to poor performance in data mining, low efficiency in data analysis, wrong decision which bring great economic loss to users and organizations further. Although many researches have been carried out from various aspects of the extracting, transforming, and loading processes in data mining, most researches pay more attention to analysis automation than to data quality evaluation. To address the data quality evaluation issues, we propose an approach to combine human beings' powerful cognitive abilities in data quality evaluation with the high efficiency ability of computer, and develop a visual analysis method for data quality evaluation based on visual morphology.",,Electronic:978-1-4673-4753-2; POD:978-1-4673-4752-5,10.1109/VAST.2012.6400531,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6400531,Database Quality;Interactive Visualization;Visual Analysis,Data analysis;Data mining;Data visualization;Morphology;Visual databases;Visualization,data mining;data visualisation;database management systems,VDQAM;cognitive ability;data analysis;data mining;data quality evaluation;database quality evaluation;extracting process;loading process;transforming process;visual analysis method;visual morphology,,0,,7,,no,14-19 Oct. 2012,,IEEE,IEEE Conference Publications
Video games as a medium for software education,B. R. C. Marques; S. P. Levitt; K. J. Nixon,"School of Electrical and Information Engineering, The University of the Witwatersrand, Johannesburg, South Africa",2012 IEEE International Games Innovation Conference,20121015,2012,,,1,4,"Educational video games may be used as a medium for software visualisation and visual programming to provide highly enjoyable, self-motivating and inquiry-based pedagogical tools. An educational game has been developed and tested on university-level students in three iterations. Players are required to solve puzzles by programming the solutions; the game introduces syntax, conditional statements and logical operators. An integrated analytics system is used to store the time taken, the number of lines of code, and players' solutions to each level. Qualitative feedback indicates that the tool is very easy to learn because of the help system and user interface. A software quiz was administered before and after participants played the game. When tested on 14 applied computing students (who had formal exposure to programming), there was no increase in the average grade. In a group of 32 electrical engineering students (who had no exposure to programming at university), the game helped about 60% of participants increase their grade, by an average of 11%.",2166-6741;21666741,Electronic:978-1-4673-1358-2; POD:978-1-4673-1359-9,10.1109/IGIC.2012.6329850,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6329850,,Data visualization;Educational institutions;Games;Programming profession;Software;Visualization,computational linguistics;computer aided instruction;computer games;computer science education;further education;program visualisation;teaching;user interfaces,computing students;conditional statements;educational video game;electrical engineering students;help system;highly enjoyable pedagogical tool;inquiry-based pedagogical tool;integrated analytics system;logical operators;puzzle solving;qualitative feedback;self-motivating pedagogical tool;software education;software quiz;software visualisation;solution programming;syntax;university-level student;user interface;visual programming,,0,,14,,no,7-9 Sept. 2012,,IEEE,IEEE Conference Publications
Virtual floortime using games to engage children with Autism Spectrum Disorder,J. Sarachan,"Commun./Journalism &amp; Digital Cultures &amp; Technol., St. John Fisher Coll., Rochester, NY, USA",2012 IEEE International Games Innovation Conference,20121015,2012,,,1,4,"1 in 88 children is diagnosed with Autism Spectrum Disorder (ASD). Specific software, virtual worlds, and games may be used to improve social and language skills. These tools may be combined with the DIR/Floortime therapeutic model that encourages parents, teachers, and therapists to engage these children through their own (sometimes limited) interests. Applying this framework, this paper suggests the creation of a workshop modeled after the Computer Clubhouse, as developed at the Massachusetts Institute of Technology, which would take advantage of the children's sometimes above-average analytic and visual abilities. Participants would produce their own video games with Scratch or other age-appropriate tool. In this way, children on the autism spectrum could pursue their interests in computers and games while strengthening their creativity and problem-solving skills: areas that are sometimes difficult for those with the disorder.",2166-6741;21666741,Electronic:978-1-4673-1358-2; POD:978-1-4673-1359-9,10.1109/IGIC.2012.6329852,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6329852,Asperger's;Autism;constructivism;special interest area;video game creation,Autism;Computational modeling;Computers;Conferences;Education;Games;Visualization,computer games;medical disorders;patient treatment,Computer Clubhouse;DIR-floortime therapeutic model;Massachusetts Institute of Technology;autism spectrum disorder;children engagement;developmental-individual difference-relationship-based therapy;language skills;problem-solving skills;social skills;video games;virtual floortime;virtual worlds,,0,,43,,no,7-9 Sept. 2012,,IEEE,IEEE Conference Publications
Virtual Time Integration of Emulation and Parallel Simulation,D. Jin; Y. Zheng; H. Zhu; D. M. Nicol; L. Winterrowd,"Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA",2012 ACM/IEEE/SCS 26th Workshop on Principles of Advanced and Distributed Simulation,20120920,2012,,,201,210,"A high fidelity testbed for large-scale system analysis requires emulation to represent the execution of critical software, and simulation to model an extensive ensemble of background computation and communication. We leverage prior work showing that large numbers of virtual environments may be emulated on a single host, and that the time stamped interactions between them can be mapped to virtual time, and we leverage existing work on simulation of large-scale communication networks. The present paper brings these concepts together, marrying the scale emulation framework OpenVZ (modified earlier to operate in virtual time) with a scalable network simulator S3F. Our algorithmic contributions lay in the design and management of virtual time as it transitions from emulation, to simulation, and back. In particular, inescapable uncertainties in emulation behavior force us to explicitly set and reset timestamps so as to avoid either emulator or simulator having to deal with a packet arriving in its logical past. We provide analytic bounds and empirical evidence that the error introduced in resetting timestamps is small. Finally, we present a case-study using this capability, of a cyber-attack with the smart power grid communication infrastructure.",1087-4097;10874097,Electronic:978-0-7695-4714-5; POD:978-1-4673-1797-9,10.1109/PADS.2012.49,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6305913,network emulation;parallel discrete event simulation;virtual time,Clocks;Computational modeling;Emulation;Load modeling;Mathematical model;Synchronization;System analysis and design,parallel processing;security of data,algorithmic contributions;analytic bounds;background communication;background computation;critical software;cyber-attack;empirical evidence;emulation behavior force;high fidelity testbed;large-scale system analysis;packet arriving;parallel simulation;scalable network simulator S3F;scale emulation framework OpenVZ;smart power grid communication infrastructure;timestamps resetting;virtual environments;virtual time integration,,9,1,22,,no,15-19 July 2012,,IEEE,IEEE Conference Publications
Visual analytics for the big data era ‰ÛÓ A comparative review of state-of-the-art commercial systems,L. Zhang; A. Stoffel; M. Behrisch; S. Mittelstadt; T. Schreck; R. Pompl; S. Weber; H. Last; D. Keim,"Univ. of Konstanz, Konstanz, Germany",2012 IEEE Conference on Visual Analytics Science and Technology (VAST),20130103,2012,,,173,182,"Visual analytics (VA) system development started in academic research institutions where novel visualization techniques and open source toolkits were developed. Simultaneously, small software companies, sometimes spin-offs from academic research institutions, built solutions for specific application domains. In recent years we observed the following trend: some small VA companies grew exponentially; at the same time some big software vendors such as IBM and SAP started to acquire successful VA companies and integrated the acquired VA components into their existing frameworks. Generally the application domains of VA systems have broadened substantially. This phenomenon is driven by the generation of more and more data of high volume and complexity, which leads to an increasing demand for VA solutions from many application domains. In this paper we survey a selection of state-of-the-art commercial VA frameworks, complementary to an existing survey on open source VA tools. From the survey results we identify several improvement opportunities as future research directions.",,Electronic:978-1-4673-4753-2; POD:978-1-4673-4752-5,10.1109/VAST.2012.6400554,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6400554,H.4 [Information Systems]: INFORMATION SYSTEMS APPLICATIONS;K.1 [Computing Milieux];THE COMPUTER INDUSTRY ‰ÛÓ Markets,Analytical models;Bismuth;Data handling;Data models;Data visualization;Software;Visual analytics,data visualisation;public domain software,IBM;SAP;open source toolkit;software company;software vendor;visual analytics system development;visualization technique,,10,,29,,no,14-19 Oct. 2012,,IEEE,IEEE Conference Publications
Visual Data Analysis as an Integral Part of Environmental Management,J. Meyer; E. W. Bethel; J. L. Horsman; S. S. Hubbard; H. Krishnan; A. Romosan; E. H. Keating; L. Monroe; R. Strelitz; P. Moore; G. Taylor; B. Torkian; T. C. Johnson; I. Gorton,Lawrence Berkeley National Laboratory,IEEE Transactions on Visualization and Computer Graphics,20121008,2012,18,12,2088,2094,"The U.S. Department of Energy's (DOE) Office of Environmental Management (DOE/EM) currently supports an effort to understand and predict the fate of nuclear contaminants and their transport in natural and engineered systems. Geologists, hydrologists, physicists and computer scientists are working together to create models of existing nuclear waste sites, to simulate their behavior and to extrapolate it into the future. We use visualization as an integral part in each step of this process. In the first step, visualization is used to verify model setup and to estimate critical parameters. High-performance computing simulations of contaminant transport produces massive amounts of data, which is then analyzed using visualization software specifically designed for parallel processing of large amounts of structured and unstructured data. Finally, simulation results are validated by comparing simulation results to measured current and historical field data. We describe in this article how visual analysis is used as an integral part of the decision-making process in the planning of ongoing and future treatment options for the contaminated nuclear waste sites. Lessons learned from visually analyzing our large-scale simulation runs will also have an impact on deciding on treatment measures for other contaminated sites.",1077-2626;10772626,,10.1109/TVCG.2012.278,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327213,Visual analytics;data management;environmental management;high-performance computing;parallel rendering,Computational modeling;Data models;Data visualization;Environmental management;Google;Monitoring;Pollution measurement;Visual analytics,contamination;data visualisation;environmental science computing;parallel processing;waste management,DOE/EM;contaminant transport;decision-making process;environmental management;high-performance computing;large-scale simulation;nuclear contaminant;nuclear waste site;parallel processing;visual data analysis;visualization software,,0,,8,,no,Dec. 2012,,IEEE,IEEE Journals & Magazines
Visual Readability Analysis: How to Make Your Writings Easier to Read,D. Oelke; D. Spretke; A. Stoffel; D. A. Keim,"University of Konstanz, Konstanz",IEEE Transactions on Visualization and Computer Graphics,20120315,2012,18,5,662,674,"We present a tool that is specifically designed to support a writer in revising a draft version of a document. In addition to showing which paragraphs and sentences are difficult to read and understand, we assist the reader in understanding why this is the case. This requires features that are expressive predictors of readability, and are also semantically understandable. In the first part of the paper, we, therefore, discuss a semiautomatic feature selection approach that is used to choose appropriate measures from a collection of 141 candidate readability features. In the second part, we present the visual analysis tool VisRA, which allows the user to analyze the feature values across the text and within single sentences. Users can choose between different visual representations accounting for differences in the size of the documents and the availability of information about the physical and logical layout of the documents. We put special emphasis on providing as much transparency as possible to ensure that the user can purposefully improve the readability of a sentence. Several case studies are presented that show the wide range of applicability of our tool. Furthermore, an in-depth evaluation assesses the quality of the measure and investigates how well users do in revising a text with the help of the tool.",1077-2626;10772626,,10.1109/TVCG.2011.266,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6051432,Document and text processing;feature evaluation and selection.,Correlation;Length measurement;Navigation;Training data;Visual analytics;Vocabulary,learning (artificial intelligence);text analysis,VisRA;document processing;draft version;semiautomatic feature selection approach;text processing;visual analysis tool;visual readability analysis;visual representations,"Books;Comprehension;Computer Graphics;Databases, Factual;Humans;Image Processing, Computer-Assisted;Linguistics;Reading;Software;Writing",6,,31,,no,12-May,,IEEE,IEEE Journals & Magazines
Visual Tracing for the Eclipse Java Debugger,B. Alsallakh; P. Bodesinsky; A. Gruber; S. Miksch,"Centre of Visual Analytics Sci. & Technol. (CVAST), Vienna Univ. of Technol., Vienna, Austria",2012 16th European Conference on Software Maintenance and Reengineering,20120405,2012,,,545,548,"In contrast to stepping, tracing is a debugging technique that does not suspend the execution. This technique is more suitable for debugging programs whose correctness is compromised by the suspension of execution. In this work we present a tool for visually tracing Java programs in Eclipse. Trace point hits are collected on a per-instance basis. This enables finding out which trace points were hit for which objects at which time. The interactive visualization provides detailed information about the hits such as thread, stack trace, and assigned values. We implemented the tool as an Eclipse plug in that integrates with other features of Eclipse Java debugger. In an informal evaluation, developers appreciated the utility of our method as a solution in the middle between full tracing and stop-and-go debugging. They suggested scenarios in which our tool can help them in debugging and understanding their programs.",1534-5351;15345351,Electronic:978-0-7695-4666-7; POD:978-1-4673-0984-4,10.1109/CSMR.2012.72,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6178940,Program Tracing;Software Visualization;Visual Debugging,Data visualization;Debugging;History;Java;Message systems;Runtime;Visualization,Java;data visualisation;interactive systems;program debugging,Eclipse Java debugger;Eclipse plugin;Java programs;debugging technique;informal evaluation;interactive visualization;stop-and-go debugging programs;visual tracing,,2,,8,,no,27-30 March 2012,,IEEE,IEEE Conference Publications
Visualizing Arrays in the Eclipse Java IDE,B. Alsallakh; P. Bodesinsky; S. Miksch; D. Nasseri,"Centre of Visual Analytics Sci. & Technol. (CVAST), Vienna Univ. of Technol., Vienna, Austria",2012 16th European Conference on Software Maintenance and Reengineering,20120405,2012,,,541,544,"The Eclipse Java debugger uses an indented list to view arrays at runtime. This visualization provides limited insight into the array. Also, it is cumbersome and time-consuming to search for certain values at an unknown index. We present a new Eclipse plug in for visualizing large arrays and collections while debugging Java programs. The plug in provides three views to visualize the data. These views are designed to support different tasks more efficiently. A tabular view gives detailed information about the elements in the array, such as the value of their field variables. A line chart aims to depict the values of a numerical field over the array. Lastly, bar charts and histograms show how the values of a field are distributed. We show how these views can be used to explore linear data structures and hashes from the Collections Framework. The plug in features tight integration with the Eclipse IDE, and is freely available as an open-source project. Developers' feedback confirmed the utility of the plug in to explore large arrays in real-world scenarios.",1534-5351;15345351,Electronic:978-0-7695-4666-7; POD:978-1-4673-0984-4,10.1109/CSMR.2012.71,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6178939,Data Structure Visualization;Eclipse Plugin;Visual Debugging,Arrays;Data visualization;Debugging;Histograms;Java;Visualization,Java;data structures;data visualisation;program debugging;programming environments,Eclipse Java IDE;Eclipse Java debugger;Eclipse plug in;Java program debugging;array visualization;bar chart;collections framework;data visualization;hash;histogram;line chart;linear data structure;open-source project;tabular view,,2,,9,,no,27-30 March 2012,,IEEE,IEEE Conference Publications
Weather influence on alarm occurrence in home telemonitoring of heart failure patients,M. Vukovi€à; M. Drobics; D. Hayn; G. Schreier; H. Lohninger; F. Rattay,"AIT Austrian Institute of Technology, Safety and Security Department, Vienna, Austria",2012 Computing in Cardiology,20130128,2012,,,525,528,"The study investigated weather influences on occurrence of alarm conditions among heart failure patients subjected to home telemonitoring. The telemonitored patients were located in the vicinities of five Austrian cities: Vienna, Graz, Innsbruck, Klagenfurt and Linz. The associated daily weather conditions were obtained from the Austrian Central Institute for Meteorology and Geodynamics. The investigations included correlations between patient's vital signs: systolic and diastolic blood pressure, heart rate, and weight, and weather conditions: air temperature, humidity and atmospheric pressure. GNU-R statistical software was used for the analysis. The results show statistically significant differences in measured blood pressure between the days with high thermal stress, particularly with falling temperatures, cold stress days. At the same time, blood pressure was associated with the highest number of patient alarm conditions requiring medical response. Including weather data within the home telemonitoring alarm generation systems offers potential for enhancing decision support towards prevention or minimization of the occurrence of adverse events.",0276-6574;02766574,Electronic:978-1-4673-2077-1; POD:978-1-4673-2076-4,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6420446,,Blood pressure;Correlation;Meteorology;Stress;Temperature distribution;Temperature measurement;Thermal stresses,alarm systems;cardiology;decision support systems;diseases;haemodynamics;humidity;medical computing;meteorology;patient monitoring;telemedicine,GNU-R statistical software;air temperature;alarm occurrence;atmospheric pressure;decision support;diastolic blood pressure;heart failure;heart rate;home telemonitoring;humidity;systolic blood pressure;thermal stress;weather influences,,0,,13,,no,9-12 Sept. 2012,,IEEE,IEEE Conference Publications
Wireless sensor network security visualization,E. Karapistoli; A. A. Economides,"Comput. Networks & Telematics Applic. Lab., Univ. of Macedonia, Thessaloniki, Greece",2012 IV International Congress on Ultra Modern Telecommunications and Control Systems,20130214,2012,,,850,856,"Security is becoming a major concern for many mission-critical applications wireless sensor networks (WSNs) are envisaged to support. This is because WSNs are susceptible to various types of attacks or to node compromises that exploit known and unknown vulnerabilities of protocols, software and hardware, and threaten the security, integrity, authenticity, and availability of data that resides in these networked systems. While various security mechanisms have been proposed for these networks dealing with either MAC layer or network layer security issues, or key management problems, the security benefits that can be obtained from an upper visualization layer have not been adequately considered in their design. In this paper, we explore the issues and concerns surrounding the application of visual analysis for wireless sensor network security purposes. This paper focuses on several distinct advantages information visualization and visual analytics can offer in the security domain. In addition, this paper reviews security visualization tools that are available to network security analysts. Finally, it concludes by identifying challenges for this new area of research.",2157-0221;21570221,Electronic:978-1-4673-2017-7; POD:978-1-4673-2016-0,10.1109/ICUMT.2012.6459781,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6459781,,Data visualization;Monitoring;Network topology;Security;Visual analytics;Wireless sensor networks,access protocols;data visualisation;public key cryptography;telecommunication computing;telecommunication security;wireless sensor networks,MAC layer;WSN security mechanisms;data authenticity;data security;information visualization;key management problems;mission-critical applications wireless sensor networks;network layer security;network security analysts;networked systems;protocols;security domain;security visualization tools;upper visualization layer;visual analysis;wireless sensor network security visualization,,2,,48,,no,3-5 Oct. 2012,,IEEE,IEEE Conference Publications
Workflow framework to support data analytics in cloud computing,S. Chaisiri; Z. Bong; C. Lee; B. S. Lee; P. Sessomboon; T. Saisillapee; T. Achalakul,"Nanyang Technological University, Singapore",4th IEEE International Conference on Cloud Computing Technology and Science Proceedings,20130204,2012,,,610,613,"This paper reports on the development of the Cloud Oriented Data Analytics (CODA) framework which has functions for composing, managing, and processing workflows for data analytics in cloud computing. The framework provides a number of reusable software components for data analytics to users which can be composed as workflows through well-known workflow composers, e.g., RapidMiner, Taverna, and JOpera. In particular, workflow scheduling, workflow recommendation, resource provisioning, resource monitoring, data locality, and security for the workflow computation are addressed by the framework. By using the framework, we demonstrate that workflows can be easily composed and processed in cloud computing. By coordinating the submitted workflows, we can obtain a significant improvement in performance.",,Electronic:978-1-4673-4510-1; POD:978-1-4673-4511-8; USB:978-1-4673-4509-5,10.1109/CloudCom.2012.6427489,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6427489,,Cloud computing;Data handling;Data storage systems;Information management;Servers;XML,cloud computing;data analysis;resource allocation;software reusability;workflow management software,CODA framework;JOpera;RapidMiner;Taverna;cloud computing;cloud oriented data analytics framework;data analytics;data locality;data security;resource monitoring;resource provisioning;software component reusability;workflow composers;workflow composition;workflow computation;workflow framework;workflow management;workflow processing;workflow recommendation;workflow scheduling,,3,1,15,,no,3-6 Dec. 2012,,IEEE,IEEE Conference Publications
1st International workshop on data analysis patterns in software engineering (DAPSE 2013),C. Bird; T. Menzies; T. Zimmermann,"Microsoft Research, USA",2013 35th International Conference on Software Engineering (ICSE),20130926,2013,,,1517,1518,"Data scientists in software engineering seek insight in data collected from software projects to improve software development. The demand for data scientists with domain knowledge in software development is growing rapidly and there is already a shortage of such data scientists. Data science is a skilled art with a steep learning curve. To shorten that learning curve, this workshop will collect best practices in form of data analysis patterns, that is, analyses of data that leads to meaningful conclusions and can be reused for comparable data. In the workshop we compiled a catalog of such patterns that will help experienced data scientists to better communicate about data analysis. The workshop was targeted at experienced data scientists and researchers and anyone interested in how to analyze data correctly and efficiently in a community accepted way.",0270-5257;02705257,Electronic:978-1-4673-3076-3; POD:978-1-4673-3075-6,10.1109/ICSE.2013.6606765,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606765,big data;business intelligence;data mining;data science;machine learning;predictive analytics;smart data;software analytics;software engineering;software intelligence,Conferences;Data analysis;Data mining;Sociology;Software;Software engineering;Statistics,,,,0,,,,no,18-26 May 2013,,IEEE,IEEE Conference Publications
3D documentation for the Conservation and Restoration of contemporary works of art: The sculptures of Maurizio Savini,L. Baratin; S. Zuliani,"DiSBEF, Univ. of Urbino, Urbino, Italy",2013 Digital Heritage International Congress (DigitalHeritage),20140220,2013,1,,777,777,"As part of its five-year university course ""Conservation and Restoration of Cultural Heritage"", ""Carlo Bo"" Urbino University has introduced a study programme involving the sculptures of the contemporary artists, which are particularly significant in terms of three-dimensional experimentation for the documentation and conservation of the works. In the case, the work of MauriníÅo Savini belongs to a contemporary artistic context - where matter becomes increasingly ephemeral and streamlined - making it more important than ever to have a scientific analytic tool that ensures good acquisition. The fundamental problem is once again, in the work of art itself, and during acquisition, that of making a distinction between what is relevant or irrelevant, identifying information that is or will become fundamental in time due to its specificity and the choice of documentation techniques that best meet these needs. The importance of correct data acquisition thus becomes a fundamental stage for understanding how to manage the project, and is in fact a pre-diagnosis of the work of art as well as a trace for future conservation works. The documentation of works of art like those in question must necessarily include acquisition systems that are able to record objects with complex and varied volumes with extreme precision. The development of hardware and software for three-dimensional acquisition makes it possible to diversify results, according to the morphological characteristics of the objects analysed. .ne of the most important aspects of this work is the methodology used to catalogue the individual items and their positioning in space, and to evaluate the state of conservation on 3D models.",,Electronic:978-1-4799-3170-5; POD:978-1-4799-3171-2; USB:978-1-4799-3169-9,10.1109/DigitalHeritage.2013.6743845,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6743845,3D documentation;conservation;contemporary works of art,,art;data acquisition;educational courses,3D documentation;Maurizio Savini sculptures;contemporary works of art;data acquisition;documentation techniques;scientific analytic tool;study programme;three-dimensional experimentation,,0,,,,no,Oct. 28 2013-Nov. 1 2013,,IEEE,IEEE Conference Publications
3D visual analytics for quality control in engineering,M. Klemen€çi€à; K. Skala,"University of Zagreb, Faculty of Graphic Arts, (PhD study) Getaldi&#x0107;eva 2, 10000, Croatia","2013 36th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)",20130916,2013,,,258,262,"This article will focus on 3D visual analytics for quality control in engineering. The world today is a busy place with a rapidly increasing amount of data to be dealt with every day. Problems are encountered when most of the data are stored without filtering and refinement for later use. Industry has raised the demands for the high technological performance of final products, such as short production time, low manufacturing costs and overall product quality. Digitizing in the real-world has various application domains, and is of vital importance when it comes to methods involving industrial quality assurance. The process of the rapid development of products depends on new technologies, such as 3D scanning, 3D printing and prototyping. Detailed digitizing can provide more product information, making it easier to locate the causes of inaccuracies and optimize production. These possibilities help us check and improve tools and gadgets, control the form of prototypes and test series in production optimization, quality assurance of serial production etc. Quality and rapid digitizing using these systems makes copying easy and thus accelerates serial production, which is why these procedures are used by numerous companies. This paper explores and analyzes existing technology and devices used in industrial quality assurance, and provides a brief review of current options and application opportunities.",,Electronic:978-953-233-073-1; POD:978-0-7695-4695-7; POD:978-953-233-073-1,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6596263,,Data visualization;Materials;Production;Software;Three-dimensional displays;Visual analytics,data analysis;data visualisation;production engineering computing;quality assurance;quality control,3D printing;3D scanning;3D visual analytics;application opportunities;engineering;high technological performance;industrial quality assurance;manufacturing costs;product quality;production optimization;production time;prototyping;quality control;serial production,,0,,3,,no,20-24 May 2013,,IEEE,IEEE Conference Publications
A closer look at bugs,T. Dal Sassc; M. Lanza,"REVEAL @ Fac. of Inf., Univ. of Lugano, Lugano, Switzerland",2013 First IEEE Working Conference on Software Visualization (VISSOFT),20131031,2013,,,1,4,"The evolution of non-trivial software systems is accompanied by unexpected behaviour and side-effects, referred as bugs or defects. These defects are reported to and stored in bug tracking systems, which contain descriptions of the problems that have been encountered. However, bug tracking systems store and present bug reports in textual form, which makes their understanding dispersive and unintuitive. We present an approach to display bug reports through a web-based visual analytics platform, named in*Bug. in*Bug allows users to navigate and inspect the vast information space created by bug tracking systems, with the goal of easing the comprehension of bug reports in detail and also obtain an understanding ‰ÛÏin the large‰Ûù of how bugs are reported with respect to one system or to an entire software ecosystem.",,Electronic:978-1-4799-1457-9; POD:978-1-4799-1455-5,10.1109/VISSOFT.2013.6650542,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6650542,,Computer bugs;Data visualization;Ecosystems;Joining processes;Software;Visual analytics,program debugging;software maintenance;tracking,Web-based visual analytics platform;bug reports;bug tracking systems;in*Bug;nontrivial software system evolution;software ecosystem,,1,,9,,no,27-28 Sept. 2013,,IEEE,IEEE Conference Publications
A comparative study of enterprise and open source big data analytical tools,U. Chandrasekhar; A. Reddy; R. Rath,"School of Information Technology and Engineering VIT University Vellore, India",2013 IEEE Conference on Information & Communication Technologies,20130715,2013,,,372,377,"In this paper, we bring forward a comparative study between the revolutionary enterprise big data analytical tools and the open source tools for the same. The Transaction Processing Council (TPC) has established a few benchmarks for measuring the potential of software and its use. We use similar benchmarks to study the tools under discussion. We try to cover as many different platforms for big data analytics and compare them based on computing environment, amount of data that can be processed, decision making capabilities, ease of use, energy and time consumed, and the pricing.",,Electronic:978-1-4673-5758-6; POD:978-1-4673-5759-3,10.1109/CICT.2013.6558123,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6558123,Big data;Hadoop;MapReduce;SQL;analytical tools;business intelligence;enterprise;metadata;open source;reliability;security,Companies;Data handling;Data storage systems;Google;Green products;Information management,business data processing;data analysis;public domain software;software metrics;transaction processing,TPC;decision making capability;energy consumption;enterprise source big data analytical tool;open source tool;pricing;time consumption;transaction processing council,,3,,18,,no,11-12 April 2013,,IEEE,IEEE Conference Publications
A Data Intensive Statistical Aggregation Engine: A Case Study for Gridded Climate Records,D. Chapman; T. A. Simon; P. Nguyen; M. Halem,"Comput. Sci. & Electr. Eng. Dept., Univ. of Maryland Baltimore County (UMBC), Baltimore, MD, USA","2013 IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum",20131031,2013,,,2157,2164,"Satellite derived climate instrument records are often highly structured and conform to the ""Data-Cube"" topology. However, data scales on the order of tens to hundreds of Terabytes make it more difficult to perform the rigorous statistical aggregation and analytics necessary to investigate how our climate is changing over time and space. It is especially cumbersome to supply the full derivation (provenance) of this analysis, as is increasingly required by scientific conferences and journals. In this paper, we address our approach toward the creation of a 55 Terabyte decadal record of Outgoing Long wave Spectrum (OLS) from the NASA Atmospheric Infrared Sounder (AIRS), and describe our open source data-intensive statistical aggregation engine ""Gridderama"" intended primarily for climate trend analysis, and may be applicable to other aggregation problems involving large structured datasets.",,Electronic:978-0-7695-4979-8; POD:978-1-4799-1372-5,10.1109/IPDPSW.2013.87,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6651122,Aggregation;Big-data;Gridderama;Scientific;Workflow,Arrays;Engines;Instruments;Market research;Meteorology;NASA;Runtime environment,artificial satellites;data handling;geophysics computing;parallel processing;public domain software;statistical analysis;topology,AIRS;Gridderama;NASA Atmospheric Infrared Sounder;OLS;Outgoing Longwave Spectrum;Satellite derived climate instrument records;climate trend analysis;data intensive statistical aggregation engine;data-cube topology;gridded climate records;large structured datasets;open source data-intensive statistical aggregation engine;rigorous statistical aggregation;rigorous statistical analytics,,0,,24,,no,20-24 May 2013,,IEEE,IEEE Conference Publications
A Dynamic Data Placement Scheme for Hadoop Using Real-time Access Patterns,V. P. Poonthottam; S. D. Madhu Kumar,"Department of Computer Science and Engineering, NIT Calicut, Kerala, INDIA","2013 International Conference on Advances in Computing, Communications and Informatics (ICACCI)",20131021,2013,,,225,229,"Hadoop has become a popular platform for largescale data analytics. In this paper, we identify a major performance bottleneck of Hadoop-its lack of ability to place data near to its required users. This new Data Placement scheme using access patterns will improve the performance of Hadoop. With this strategy the data will be placed nearer to the required users thereby achieving optimization in access time and bandwidth. Finally, the simulation experiments indicate that our strategy behaves much better than the HDFS blocks placement.",,Electronic:978-1-4673-6217-7; POD:978-1-4799-1664-1,10.1109/ICACCI.2013.6637175,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6637175,Access pattern;HDFS;Hadoop;Load Optimization;block placement,Bandwidth;Heart beat;Informatics;Pipelines;Real-time systems;Servers;Software,data analysis;parallel processing,HDFS blocks placement;Hadoop distributed file system;access time optimization;bandwidth optimization;dynamic data placement scheme;large scale data analytics;realtime access patterns,,1,,10,,no,22-25 Aug. 2013,,IEEE,IEEE Conference Publications
A fast and hardware mimicking analytic CT simulator,H. Ghadiri; A. Rahmim; M. B. Shiran; H. Soltanian-Zadeh; M. R. Ay,"Dept. of Med. Phys. & Biomed. Eng., Tehran Univ. of Med. Sci., Tehran, Iran",2013 IEEE Nuclear Science Symposium and Medical Imaging Conference (2013 NSS/MIC),20140612,2013,,,1,5,"Different algorithms have been utilized for x-ray computed tomography (CT) simulation based on Monte Carlo technique, analytic calculation, or combination of them. Software packages based on Monte Carlo algorithm provide sophisticated calculations but the time consuming nature of them limits its applicability. Analytic calculation for CT simulation has been also evaluated in recent years. Due to ignoring basic physical processes, analytic methods have limited applications. In this study, a hardware mimicking algorithm has been developed to accurately model the CT imaging chain using analytic calculation. The model includes x-ray spectrum generation according to the pre-defined scanning protocol. The detector is designed to acquire the data either in integral or spectral modes. CT geometry can be used as parallel or fan beam with different sizes. Poisson noise model was applied to the acquired projection data. Varieties of projection-based computerized phantoms have been designed and implemented in the simulator. CT number and background noise of the simulated images have been compared with experimental data. On average, the relative difference between simulated and experimental HUs are 8.3%, 7.5%, and 8.0% for bone; 12.1%, 10.3%, and 7.8% for contrast agent; and 16.6%, 3.6%, and 5.2% for the background at 80 kVp/500 mAs, 120 kVp/250 mAs, and 140 kVp/125 mAs, respectively. The relative difference between simulated and experimental noise values vary between 2% to slightly less than 26%. For scanning and image generation with a computer equipped with Intel Core2 Quad CPU and 2.0 GB of RAM, the simulator takes about 32 seconds for generating a 512ÌÑ512 single slice image when it is adjusted to acquire 900 projection angles with 20 mm slice thickness and 140kVp/200 mAs scanning protocol. The simulation time is independent of photon intensity.",1082-3654;10823654,Electronic:978-1-4799-0534-8; POD:978-1-4799-0532-4,10.1109/NSSMIC.2013.6829157,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6829157,,Analytical models;Computational modeling;Computed tomography;Detectors;Noise;Phantoms;X-ray imaging,Monte Carlo methods;computerised tomography;phantoms,Monte Carlo technique;Poisson noise model;X-ray computed tomography;X-ray spectrum generation;analytic calculation;contrast agent;hardware mimicking analytic CT simulator;photon intensity;projection based computerized phantoms;scanning protocol,,0,,6,,no,Oct. 27 2013-Nov. 2 2013,,IEEE,IEEE Conference Publications
A formal model for verifying stealthy attacks on state estimation in power grids,M. A. Rahman; E. Al-Shaer; M. A. Rahman,"Dept. of Software & Inf. Syst., Univ. of North Carolina at Charlotte, Charlotte, NC, USA",2013 IEEE International Conference on Smart Grid Communications (SmartGridComm),20131219,2013,,,414,419,"The power system state estimation is very important for maintaining the power system securely, reliably, and efficiently. An attacker can compromise meters or communication systems and introduce false measurements, which can evade existing bad data detection algorithms and lead to incorrect state estimation. This kind of stealthy attack is well-known as Undetected False Data Injection (UFDI) attack. However, attackers usually have different constraints with respect to knowledge, capabilities, resources, and attack targets. These attack attributes are important to consider in order to know the potential attack vectors. In this paper, we propose a formal model for UFDI attack verification in order to provide security analytics for power grid state estimation. Our model formalizes the grid information and different constraints, particularly with respect to attackers' point of view. The solution to the model provides an attack vector, when it exists, by satisfying the given constraints. We demonstrate our UFDI attack verification model with the help of an example. We evaluated our proposed model by running experiments on different IEEE test systems and we found that our model is very efficient in solving problems with hundreds of buses.",,Electronic:978-1-4799-1526-2; POD:978-1-4799-1525-5,10.1109/SmartGridComm.2013.6687993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6687993,,Admittance;Equations;Mathematical model;Power grids;Power measurement;State estimation;Substations,formal verification;power grids;power meters;power system security;power system state estimation,IEEE test systems;UFDI attack verification;attack vectors;bad data detection;communication systems;false measurements;formal model;grid information;meter systems;power grids;power system state estimation;stealthy attack verification;undetected false data injection attack,,4,,14,,no,21-24 Oct. 2013,,IEEE,IEEE Conference Publications
A hybrid method for word segmentation with English-Vietnamese bilingual text,Quoc Hung Ngo; Dinh Dien; W. Winiwarter,"Comput. Sci. Fac., Univ. of Inf. Technol., Ho Chi Minh City, Vietnam","2013 International Conference on Control, Automation and Information Sciences (ICCAIS)",20140127,2013,,,48,52,"This paper proposes a hybrid approach for Vietnamese word segmentation. The approach combines a dictionary-based method and a machine learning method to detect word boundaries in Vietnamese text by comparing English-Vietnamese pairs. We also point out several characteristics of Vietnamese which affect the Vietnamese word segmentation task and word alignment of English-Vietnamese text. Moreover, we built an English-Vietnamese bilingual corpus with nearly 10 million words, namely EVBCorpus, while a part of EVBNews has been manually segmented at the word level. We evaluate the performance of our approach by comparing its word segmentation results on this corpus. Our hybrid approach achieves 97% accuracy on the EVBNews corpus.",,Electronic:978-1-4799-0572-0; POD:978-1-4799-0571-3,10.1109/ICCAIS.2013.6720528,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6720528,,Accuracy;Conferences;Dictionaries;Hidden Markov models;Software;Support vector machines;Training,natural language processing;text analysis,EVBCorpus;EVBNews;English-Vietnamese bilingual corpus;English-Vietnamese bilingual text;English-Vietnamese pair comparison;English-Vietnamese text word alignment;Vietnamese word segmentation;dictionary-based method;hybrid method;machine learning method;word boundary detection,,0,,18,,no,25-28 Nov. 2013,,IEEE,IEEE Conference Publications
A MapReduce Based Approach of Scalable Multidimensional Anonymization for Big Data Privacy Preservation on Cloud,X. Zhang; C. Yang; S. Nepal; C. Liu; W. Dou; J. Chen,"Fac. of Eng. & IT, Univ. of Technol. Sydney, Sydney, NSW, Australia",2013 International Conference on Cloud and Green Computing,20131219,2013,,,105,112,"The massive increase in computing power and data storage capacity provisioned by cloud computing as well as advances in big data mining and analytics have expanded the scope of information available to businesses, government, and individuals by orders of magnitude. Meanwhile, privacy protection is one of most concerned issues in big data and cloud applications, thereby requiring strong preservation of customer privacy and attracting considerable attention from both IT industry and academia. Data anonymization provides an effective way for data privacy preservation, and multidimensional anonymization scheme is a widely-adopted one among existing anonymization schemes. However, existing multidimensional anonymization approaches suffer from severe scalability or IT cost issues when handling big data due to their incapability of fully leveraging cloud resources or being cost-effectively adapted to cloud environments. As such, we propose a scalable multidimensional anonymization approach for big data privacy preservation using Map Reduce on cloud. In the approach, a highly scalable median-finding algorithm combining the idea of the median of medians and histogram technique is proposed and the recursion granularity is controlled to achieve cost-effectiveness. Corresponding MapReduce jobs are dedicatedly designed, and the experiment evaluations demonstrate that with our approach, the scalability and cost-effectiveness of multidimensional scheme can be improved significantly over existing approaches.",,Electronic:978-0-7695-5114-2; POD:978-1-4799-1362-6,10.1109/CGC.2013.24,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6686016,MapReduce;big data;cloud computing;multidimensional anonymization;privacy preservation,Cloud computing;Data handling;Data privacy;Data storage systems;Information management;Privacy;Scalability,Big Data;cloud computing;data protection,MapReduce;big data privacy preservation;cloud computing;cost-effectiveness;data anonymization;histogram technique;median of medians technique;privacy protection;recursion granularity;scalable median-finding algorithm;scalable multidimensional anonymization approach,,4,,36,,no,Sept. 30 2013-Oct. 2 2013,,IEEE,IEEE Conference Publications
A novel visual analytics approach for clustering large-scale social data,Z. Wang; C. Chen; J. Zhou; J. Liao; W. Chen; R. Maciejewski,"State Key Lab. of CAD&CG, Zhejiang Univ., Hangzhou, China",2013 IEEE International Conference on Big Data,20131223,2013,,,79,86,"Social data refers to data individuals create that is knowingly and voluntarily shared by them and is an exciting avenue into gaining insight into interpersonal behaviors and interaction. However, such data is large, heterogeneous and often incomplete, properties that make the analysis of such data extremely challenging. One common method of exploring such data is through cluster analysis, which can enable analysts to find groups of related users, behaviors and interactions. This paper presents a novel visual analysis approach for detecting clusters within large-scale social networks by utilizing a divide-analyze-recombine scheme that sequentially performs data partitioning, subset clustering and result recombination within an integrated visual interface. A case study on a microblog messaging data (with 4.8 millions users) is used to demonstrate the feasibility of this approach and comparisons are also provided to illustrate the performance benefits of this approach with respect to existing solutions.",,Electronic:978-1-4799-1293-3; POD:978-1-4799-1294-0,10.1109/BigData.2013.6691718,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6691718,Cluster Analysis;Divide and Recombine;K-means;Visual Analysis,Algorithm design and analysis;Clustering algorithms;Educational institutions;Partitioning algorithms;Sensitivity;Vectors;Visualization,data analysis;data visualisation;pattern clustering;social networking (online);user interfaces,cluster analysis;data partitioning;divide-analyze-recombine scheme;integrated visual interface;interpersonal behaviors;large-scale social data clustering;large-scale social networks;microblog messaging data;result recombination;subset clustering;visual analysis approach;visual analytics approach,,2,,24,,no,6-9 Oct. 2013,,IEEE,IEEE Conference Publications
"A Precision Information Environment (PIE) for emergency responders: Providing collaborative manipulation, role-tailored visualization, and integrated access to heterogeneous data",R. Kilgore; A. Godwin; A. Davis; C. Hogan,"Cognitive Systems Division, Charles River Analytics, Cambridge, USA",2013 IEEE International Conference on Technologies for Homeland Security (HST),20140102,2013,,,766,771,"During a crisis, emergency responders must rapidly integrate information from many separate sources to satisfy their role-specific needs and to make time-sensitive decisions. Responders currently receive this information through numerous software applications and face the challenge of integrating this heterogeneous data into an all-encompassing picture. Individual responders with distinct roles, such as police, fire, and EMS, often have very different information needs, but existing tools do not provide individual tailoring of workspaces to support this need. As responders communicate using text, voice, or other multimodal collaboration systems, important details can also be lost or become stale over time. This paper describes our approach to developing a Precision Information Environment (PIE) that: (1) streamlines access to multiple information resources by fusing heterogeneous information for presentation through a single access point; (2) supports role-tailorable understanding of the unified data sources through a flexible workspace; and (3) supports collaboration between teams of local and distributed responders by providing a work environment that allows teams to share and manipulate dynamic data sources in real time. We also describe our initial results from a usability evaluation of the system with subject matter experts.",,CD-ROM:978-1-4799-3963-3; Electronic:978-1-4799-1535-4; POD:978-1-4799-3964-0,10.1109/THS.2013.6699100,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6699100,Emergency Operations;Geospatial and Temporal Integration;Human-Centered Design;Information Visualization;Meta-Information Visualization;Multimodal Collaboration;Precision Information Environment;Visualization Ontology,Collaboration;Data visualization;Emergency services;Hospitals;Information services;Media;Ontologies,data integration;data visualisation;emergency management;graphical user interfaces;groupware,PIE;access point;collaborative manipulation;crisis;data visualization;distributed responders teams;dynamic data source manipulation;dynamic data source sharing;emergency responders;flexible workspace;heterogeneous data integration;heterogeneous information fusion;information integration;information resources;integrated heterogeneous data access;local responders teams;multimodal collaboration systems;precision information environment;software applications;subject matter experts;system usability evaluation;text system;time-sensitive decision making;unified data sources;voice system,,0,,14,,no,12-14 Nov. 2013,,IEEE,IEEE Conference Publications
A research of critical factors in the the enterprise adoption of cloud service,C. Y. Hsu; H. I. Wang,"Dept. of Manage. Inf. Syst., Overseas Chinese Univ., Taichung, Taiwan",2013 International Joint Conference on Awareness Science and Technology & Ubi-Media Computing (iCAST 2013 & UMEDIA 2013),20140313,2013,,,465,469,"Cloud is essentially a flexible and scalable model for the way IT services are delivered and consumed. There are lots of advantages to using cloud computing for international companies. One of the major ones is the flexibility that it offers. Cloud computing means that users can access the files and data that they need even when they're working remotely. Since the popularity of cloud service, businesses are looking for cloud solutions to solve some of their biggest business and technology challenges: reducing costs, creating new levels of efficiency, and facilitating innovative business models. To understand what that means to the business, the benefits and potential risks of migrating to cloud services need to be carefully considered. The suitable business model is therefore becoming the focus as adopting the cloud service. The study is based on the Critical Factor Index with AHP method which had here a different function than it usually does. Basically all the other results given by the Critical Factor Index are based on a questionnaire inside the companies. In this case, the study was carried out by using external experts. The research outcome will conclude the critical success factors and new business model of cloud service adoption. We also discuss the factors that make cloud computing an attractive option for enterprises.",,Electronic:978-1-4799-2364-9; POD:978-1-4799-4400-2,10.1109/ICAwST.2013.6765485,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6765485,AHP;Business model;Cloud service;Critical success factors,Analytic hierarchy process;Cloud computing;Companies;Electronic government;Software as a service,analytic hierarchy process;business data processing;cloud computing;cost reduction,AHP method;IT services;cloud computing;cloud service;cost reduction;critical factor index;critical success factors;efficiency level;enterprise adoption;innovative business models;international companies,,0,,20,,no,2-4 Nov. 2013,,IEEE,IEEE Conference Publications
A Retrospective Study of Software Analytics Projects: In-Depth Interviews with Practitioners,A. T. Misirli; B. Caglayan; A. Bener; B. Turhan,University of Oulu,IEEE Software,20130903,2013,30,5,54,61,"Software analytics guide practitioners in decision making throughout the software development process. In this context, prediction models help managers efficiently organize their resources and identify problems by analyzing patterns on existing project data in an intelligent and meaningful manner. Over the past decade, the authors have worked with software organizations to build metric repositories and predictive models that address process-, product-, and people-related issues in practice. This article shares their experience over the years, reflecting the expectations and outcomes both from practitioner and researcher viewpoints.",0740-7459;07407459,,10.1109/MS.2013.93,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6547143,Decision making;Estimation;Predictive models;Software analytics;Software development;defect prediction;effort estimation;interviews;software analytics,Decision making;Estimation;Predictive models;Software analytics;Software development,software development management;software metrics,metric repositories;predictive model;software analytics projects;software development process,,2,,11,,no,Sept.-Oct. 2013,,IEEE,IEEE Journals & Magazines
A Scalable Distributed Framework for Efficient Analytics on Ordered Datasets,J. Yin; Y. Liao; M. Baldi; L. Gao; A. Nucci,,2013 IEEE/ACM 6th International Conference on Utility and Cloud Computing,20140505,2013,,,131,138,"One of the most common datasets used by many corporations to gain business intelligence is event log files. Oftentimes, the records in event log files are temporally ordered, and need to be grouped by user ID with the temporal ordering preserved to facilitate mining user behaviors. This kind of analytical workload, here referred to as Relative Order-preserving based Grouping (RE-ORG), is quite common in big data analytics. Using MapReduce/Hadoop for executing RE-ORG tasks on ordered datasets is not efficient due to its internal sort-merge mechanism. In this paper, we propose a distributed framework that adopts an efficient group-order-merge mechanism to provide faster execution of RE-ORG tasks. We demonstrate the advantage of our framework by comparing its performance with Hadoop through extensive experiments on real-world datasets. The evaluation results show that our framework can achieve up to 6.3ÌÑ speedup over Hadoop in executing RE-ORG tasks.",,Electronic:978-0-7695-5152-4; POD:978-1-4799-2574-2,10.1109/UCC.2013.35,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6809349,Hadoop;MapReduce;big data analytics;distributed framework;ordered dataset,Business;Indexes;Instruments;Merging;Open source software;Programming;Sorting,Big Data;competitive intelligence;data analysis;distributed databases;real-time systems,MapReduce/Hadoop;RE-ORG;big data analytics;business intelligence;event log files;group-order-merge mechanism;ordered dataset analytics;real-world datasets;relative order-preserving based grouping;scalable distributed framework;temporal ordering;user behavior mining,,0,,28,,no,9-12 Dec. 2013,,IEEE,IEEE Conference Publications
A Software Suite for Efficient Use of the European Qualifications Framework in Online and Blended Courses,B. Florian-Gaviria; C. Glahn; R. Fabregat Gesa,"Universidad del Valle, Cali",IEEE Transactions on Learning Technologies,20130905,2013,6,3,283,296,"Since introduction of the European qualifications framework (EQF) as one instrument to bridge from learning institutions to competence driven lifelong learning, it remains a challenge for instructors and teachers in higher education to make efficient use of this framework for designing, monitoring, and managing their lessons. This paper presents a software suite for enabling teachers to make better use of EQF in their teaching. The software suite extends course design based on well-defined learning outcomes, monitoring performance and competence acquisition according to the EQF levels, assessment using scoring rubrics of EQF levels and competences in a 360-degree feedback, as well as visualizations of learning analytics and open student models in dashboards for different social perspectives in social planes. This paper includes a case study with 20 teachers who used the software suite in all phases of the course lifecycle for three programming courses. The results show that integrated applications for adopting the EQF in teaching practice are strongly needed. These results also show that the suite can assist teachers in creating contextual awareness, kindling reflection, understanding students and course progress, and inferring patterns of success and failure in competences development.",1939-1382;19391382,,10.1109/TLT.2013.18,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6518105,360-degree feedback;EQF;Instructor interfaces;integration and modeling;learning analytics;personalized e-learning;social learning techniques;system architectures;systems specification methodology,Computer architecture;Data models;Europe;Monitoring;Qualifications;Software,computer aided instruction;educational courses;further education;teaching,360-degree feedback;EQF;European qualifications framework;blended course;competence driven lifelong learning;course design;course lifecycle;higher education;learning analytics visualization;learning institution;learning outcome;online course;open student model;programming course;social perspective;software suite;student competences development;teaching practice,,5,,35,,no,July-Sept. 2013,,IEEE,IEEE Journals & Magazines
A triple-mode ring dielectric resonator band-pass filter using substrate integrated waveguide (SIW),D. D. Zhang; L. Zhou; J. F. Mao; W. Y. Yin,"Key Lab. of Minist. of Educ. of Design & Electromagn. Compatibility of High Speed Electron. Syst., Shanghai Jiao Tong Univ., Shanghai, China",2013 European Microwave Conference,20131223,2013,,,163,166,"This paper presents a design of triple mode ring dielectric resonator band-pass filter. The first mode TM<sub>010</sub> and the second degenerated modes TM<sub>110</sub> were excited. Instead of using conventional metal enclosures, substrate integrated waveguide (SIW) was used for easily integrating with other microwave circuits. Analytic equations were derived to determine the resonant frequencies and the quality factor of the ring dielectric resonator filter. It has been found that the calculated results agreed with simulated ones by using EM software. To develop a triple mode ring dielectric resonator filter, novel input and output structures were realized. The measured insertion of the filter is about 0.6dB with 3-dB fractional bandwidth of 9.8% obtained at operating frequency 2.1GHz. This proposed filter configuration effectively reduced the cost of assembly and the difficulty of integration associated with other circuits. The concept is very attractive for using in low cost, low insertion loss and high Q production of wireless filter application.",,Electronic:978-2-87487-031-6; POD:978-1-4799-0264-4,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6686616,High Q factor;Substrate Integrated Waveguide (SIW);band-pass filter;dielectric resonator;triple mode,Band-pass filters;Dielectrics;Equations;Microwave filters;Q-factor;Resonator filters;Substrates,dielectric resonator filters;substrate integrated waveguides,EM software;SIW;analytic equations;filter configuration;fractional bandwidth;high Q production;low insertion loss;microwave circuits;quality factor;resonant frequencies;substrate integrated waveguide;triple mode ring dielectric resonator band pass filter;wireless filter application,,0,,9,,no,6-10 Oct. 2013,,IEEE,IEEE Conference Publications
A Trust Evaluation Method for Supplier Selection,X. Qiu; L. Cao; P. Li; L. Zhao,"Beijing Key Lab. of Network Syst. Archit. & Convergence, Beijing, China","2013 12th IEEE International Conference on Trust, Security and Privacy in Computing and Communications",20131212,2013,,,1498,1503,"Trust and reputation systems play important roles in supplier selection, a topic that has been widely investigated from a business and operation point of view. However, we still lack of effective studies on supplier selection with security as the optimal target, which is a very important factor for Information and Communication Technology (ICT) systems. In response, this paper outlines a method that enables people to evaluate the relative and objective trustworthiness of the alternative suppliers. Our analysis is based solely on the original data of the vulnerabilities publicly available from OSVDB and NVD, which are impossible to tamper with. They are then interpreted by an approach that combines the Analytic Hierarchy Process (AHP) and objective analysis that enables the inference of relative trust valuations based on different evaluation indexes. A case study of five well known vendors is demonstrated with qualitative comparison based on visualization of data.",2324-898X;2324898X,Electronic:978-0-7695-5022-0; POD:978-1-4799-1444-9,10.1109/TrustCom.2013.182,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6681006,Analytic Hierarchy Process;NVD;OSVDB;Trust evaluation;supplier selection,Analytic hierarchy process;Computer architecture;Databases;History;Measurement;Security,analytic hierarchy process;data visualisation;public domain software;supply chain management;trusted computing,ICT systems;NVD;OSVDB;analytic hierarchy process;data visualization;information and communication technology systems;national vulnerability database;open source vulnerability database;supplier selection;trust evaluation method;trustworthiness evaluation,,0,,11,,no,16-18 July 2013,,IEEE,IEEE Conference Publications
A Universal Storage Architecture for Big Data in Cloud Environment,Q. Zhang; Z. Chen; A. Lv; L. Zhao; F. Liu; J. Zou,"Sch. of Software Technol., Dalian Univ. of Technol., Dalian, China","2013 IEEE International Conference on Green Computing and Communications and IEEE Internet of Things and IEEE Cyber, Physical and Social Computing",20131212,2013,,,476,480,"With the rapid development of the Internet of Things and Electronic Commerce, we have entered the era of big data. The characteristics, such as great amount and heterogeneousity, of big data bring the challenge to the storage and analytics. The paper presented a universal storage architecture for big data in cloud environment. We use clustering analysis to divide the cloud nodes into multiple clusters according to the communication cost between different nodes. The cluster with the strongest computing power is selected to provide the universal storage and query interface for users. Each of other clusters is responsible for storing the data of a particular model, such as relational data, key-value data, and document data and so on. Experiments show that our architecture can store all kinds of heterogeneous big data and provide users with unified storage and query interface for big data easily and quickly.",,Electronic:978-0-7695-5046-6; POD:978-1-4799-0631-4,10.1109/GreenCom-iThings-CPSCom.2013.96,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6682110,big datat;data model;storage architecture,Computer architecture;Data handling;Data models;Data storage systems;Educational institutions;Information management;Internet,Big Data;cloud computing;data analysis;pattern clustering;query processing;storage management,Internet of Things;cloud environment;cloud nodes;clustering analysis;communication cost;data analytics;data storage;document data;electronic commerce;heterogeneous Big Data;key-value data;multiple clusters;query interface;relational data;universal storage architecture,,2,,22,,no,20-23 Aug. 2013,,IEEE,IEEE Conference Publications
A Web service based application serving vegetation condition indices for Flood Crop Loss Assessment,B. Zhang; L. Di; G. Yu; Y. Shao; R. Shrestha; L. Kang,"Center for Spatial Information Science and Systems, George Mason University, 4087 University Drive, Fairfax, VA 22030, USA",2013 Second International Conference on Agro-Geoinformatics (Agro-Geoinformatics),20131007,2013,,,215,220,"Vegetation condition assessment is very useful and helpful for researchers and decision makers to evaluate crop loss and value, and identify and manage risks in the flood hazard areas. Crop responses to flooding vary with crop types, crop growing stages, soil characteristics, weather condition, flood duration and depth, etc. How to measure and understand crop response is a challenging and important research topic in agriculture. The availability and integration of high spatial and temporal resolution remote sensing data facilitates crop type identification, crop condition monitoring, soil moisture measurement, crop yield estimation, and crop damage evaluation. Remote sensing based vegetation condition indices are widely used by researchers in these fields. In this paper, an integrated Web geospatial application named Remote-sensing-based Flood Crop Loss Assessment Service System (RF-CLASS) is developed to automate the ‰ÛÏdata-information-knowledge-decision‰Ûù process of downloading near real time 250m resolution MODIS land surface reflectance data from NASA website, re-projecting, reformatting, and mosaicking these data using geospatial software packages, calculating various daily, weekly, and bi-weekly vegetation condition indices through efficient computing method, visualizing and analyzing these indices in an interactive way, assessing crop progress and condition, and evaluating crop damages in the flood areas. Currently, the vegetation indices for the whole continental United States from the year of 2000 have been generated routinely. These indices and other geospatial data like the boundary layers, the road layers, and the latest cropland data layer (CDL) have been served in the prototype system of RF-CLASS. RF-CLASS not only provides basic map operations, area of interest definition, geospatial data customization and downloading, and geospatial analytics functions for decision making through its interactive and intuitive user interface, but also o- fers standard Web services to query, visualize, disseminate, and analyze various types of vegetation condition indices for integration in other applications or invocation in scientific workflows.",,Electronic:978-1-4799-0868-4; POD:978-1-4799-0867-7,10.1109/Argo-Geoinformatics.2013.6621910,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6621910,Cropland Data Layer;MODIS;flood;remote sensing;vegetation condition indices,Agriculture;Floods;Geospatial analysis;Indexes;Remote sensing;Standards;Vegetation mapping,Web services;agricultural engineering;crops;floods;software packages;vegetation mapping,CDL;MODIS land surface reflectance data;NASA Website;Remote-sensing-based Flood Crop Loss Assessment Service System;Web service;area-of-interest definition;basic map operations;boundary layers;continental United States;crop condition assessment;crop damage evaluation;crop progress assessment;cropland data layer;data mosaicking;data re-projection;data reformatting;data-information-knowledge-decision process;geospatial analytics functions;geospatial data customization;geospatial data downloading;geospatial software packages;indices analysis;indices visualization;integrated Web geospatial application;road layers;user interface;vegetation condition indices,,1,,14,,no,12-16 Aug. 2013,,IEEE,IEEE Conference Publications
A Web-Based Hybrid System for Evaluating Marketing and e-Commerce Web Site Performance,S. Li; J. Z. Li; S. S. L. Li,"Westminster Bus. Sch., Univ. of Westminster, London, UK",2013 Fourth International Conference on Digital Manufacturing & Automation,20130916,2013,,,94,97,"In the globalised markets with changing customer demands, linking manufacturing or production with sound marketing strategies and effective marketing performance management becomes increasingly important. In this paper, a Web-based hybrid system, WebMarP (created by the authors), for evaluating marketing and e-commerce Web site performance is introduced. The proposed novel approach integrates the strengths of the analytic hierarchy process, Web-based expert system, online fuzzy rules and graphical displays. The software system architecture is outlined with input and output screen samples illustrated. Initial evaluation work is also reported with evaluation findings briefly presented.",,Electronic:978-0-7695-5016-9; POD:978-1-4799-0325-2,10.1109/ICDMA.2013.23,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6597941,Analytic Hierarchy Process;E-commerce Web Site Performance;Fuzzy Logic;Marketing Performance;Social Media Marketing Performance;Web-Based Expert System,Automation;Manufacturing,Web sites;decision making;electronic commerce;expert systems;marketing;software architecture,Web-based expert system;Web-based hybrid system;WebMarP system;analytical hierarchy process;customer demand;e-commerce Web site performance;electronic commerce;fuzzy rule;graphical display;marketing Web site performance;software system architecture,,0,,6,,no,29-30 June 2013,,IEEE,IEEE Conference Publications
A Workflow Framework for Big Data Analytics: Event Recognition in a Building,C. Chen; X. Yang; B. Zoebir; S. Chaisiri; B. S. Lee,"Sch. of Comput. Eng., Nanyang Technol. Univ., Singapore, Singapore",2013 IEEE Ninth World Congress on Services,20131107,2013,,,21,28,"This paper studies event recognition in a building based on the patterns of power consumption. It is a big challenge to identify what kinds of events happened in a building without additional devices such as camera and motion sensors, etc. Instead, we learn when and how the events happened from the historical record of power consumption and apply the lesson into the design of an event recognition system (ERS). The ERS will find out abnormal power usage to avoid wasting power, which leads to the energy savings in a building. The ERS involves big data analytics with a large size of dataset collected in a real time. Such a data intensive system is usually viewed as a workflow. A workflow management is a significant task of the system requiring data analysis in terms of the system scalability to maintain high throughput or fast speed analysis. We propose a workflow framework that allows users to perform remote and parallel workflow execution, whose tasks are efficiently scheduled and distributed in cloud computing environment. We run the ERS as a target system for the proposed framework with power consumption data (whose size is approximately 20GB or more) collected from each of over 240 rooms in a building at Dept. of Engineering, Tokyo University in 2011. We show that the proposed framework accelerates the speed of data analysis by providing scaling infrastructure and parallel processing feature utilizing cloud computing technologies. We also share our experience and results on the big data analytics and discuss how the studies contribute to achieve Green Campus.",2378-3818;23783818,Electronic:978-0-7695-5024-4; POD:978-1-4799-0412-9,10.1109/SERVICES.2013.29,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6655671,big data analytics;event recognition system;green building;workflow framework,Buildings;Cloud computing;Correlation;Lighting;Plugs;Power demand;XML,building management systems;cloud computing;data analysis;green computing;parallel processing;power consumption;power engineering computing;workflow management software,Department of Engineering;ERS;Tokyo University;abnormal power usage;big data analytics;building energy savings;building event recognition;camera;cloud computing environment;cloud computing technology;data analysis;data intensive system;distributed task;event recognition system;green campus;motion sensors;parallel processing;parallel workflow execution;power consumption data;power consumption historical record;power consumption pattern;power waste;remote workflow execution;scaling infrastructure;system scalability;task scheduling;workflow framework;workflow management,,1,,20,,no,June 28 2013-July 3 2013,,IEEE,IEEE Conference Publications
Accelerating Batch Analytics with Residual Resources from Interactive Clouds,R. B. Clay; Z. Shen; X. Ma,"Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC, USA","2013 IEEE 21st International Symposium on Modelling, Analysis and Simulation of Computer and Telecommunication Systems",20140203,2013,,,414,423,"The popularity of cloud-based interactive computing services (e.g., virtual desktops) brings new management challenges. Each interactive user leaves abundant but fluctuating residual resources while being intolerant to latency, precluding the use of aggressive VM consolidation. In this paper, we present the Resource Harvester for Interactive Clouds (RHIC), an autonomous management framework that harnesses dynamic residual resources aggressively without slowing the harvested interactive services. RHIC builds ad-hoc clusters for running throughput-oriented ""background"" workloads using a hybrid of residual and dedicated resources. These hybrid clusters offer significant gains over normal dedicated clusters: 20-40% cost and 20-29% energy savings in our test bed. For a given background job, RHIC intelligently discovers and maintains the ideal cluster size and composition, to meet user-specified goals such as cost/energy minimization or deadlines. RHIC employs black-box workload performance modeling, requiring only system-level metrics and incorporating techniques to improve modeling accuracy with bursty and heterogeneous residual resources. We demonstrate the effectiveness and adaptivity of our RHIC prototype with two parallel data analytics frameworks, Hadoop and HBase. Our results show that RHIC finds near-ideal cluster sizes and compositions across a wide range of workload/goal combinations.",1526-7539;15267539,Electronic:978-0-7695-5102-9; POD:978-1-4799-1209-4,10.1109/MASCOTS.2013.63,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6730798,Adaptive systems;Distributed computing;Performance analysis,Adaptation models;Availability;Bandwidth;Measurement;Predictive models;Productivity;Virtual machine monitors,cloud computing;interactive systems;power aware computing;resource allocation;software performance evaluation;virtual machines;workstation clusters,HBase;Hadoop;RHIC prototype;ad-hoc clusters;aggressive VM consolidation;autonomous management framework;batch analytics acceleration;black- box workload performance modeling;cloud-based interactive computing services;dedicated resources;dynamic residual resources;energy minimization;ideal cluster composition;ideal cluster size;parallel data analytics frameworks;resource harvester for interactive clouds;system-level metrics;throughput-oriented background workloads;virtual desktops,,7,,25,,no,14-16 Aug. 2013,,IEEE,IEEE Conference Publications
Accelerating Join Operation for Relational Databases with FPGAs,R. J. Halstead; B. Sukhwani; H. Min; M. Thoennes; P. Dube; S. Asaad; B. Iyer,"Dept. Comput. Sci., Univ. of California, Riverside, Riverside, CA, USA",2013 IEEE 21st Annual International Symposium on Field-Programmable Custom Computing Machines,20130624,2013,,,17,20,"In this paper, we investigate the use of field programmable gate arrays (FPGAs) to accelerate relational joins. Relational join is one of the most CPU-intensive, yet commonly used, database operations. Hashing can be used to reduce the time complexity from quadratic (naiíöve) to linear time. However, doing so can introduce false positives to the results which must be resolved. We present a hash-join engine on FPGA that performs hashing, conflict resolution, and joining on a PCIe-attached system, achieving greater than 11x speedup over software.",,Electronic:978-0-7695-4969-9; POD:978-1-4673-6005-0,10.1109/FCCM.2013.17,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6545988,FPGA;Relational database;analytics;database join;hardware aceleration,Benchmark testing;Databases;Field programmable gate arrays;Pipelines;Probes;Software;Vectors,computational complexity;field programmable gate arrays;peripheral interfaces;relational databases,CPU-intensive;FPGA;PCIe-attached system;conflict resolution;false positives;field programmable gate arrays;hash-join engine;linear time;relational databases;relational joins;time complexity,,6,,16,,no,28-30 April 2013,,IEEE,IEEE Conference Publications
AHP-based prioritization of microgrid generation plans considering resource uncertainties,S. S. Mousavi-Seyedi; F. Aminifar; A. Rahimikian; S. Rezayi,"Sch. of Electr. & Comput. Eng., Univ. of Tehran, Tehran, Iran",2013 Smart Grid Conference (SGC),20140210,2013,,,63,68,"The growing focus on the energy efficiency promotion, deployment of renewable energy resources, and serving the electricity in more reliable ways have recently resulted in a remarkable attention on microgrids. Among different electricity generation options available for a microgrid, the one compromising both technical and economical aspects is of a great desire. Taking into account the existing uncertainties associated with energy resources, this paper evaluates various distributed generation (DG) configuration plans for a microgrid using analytical hierarchical process (AHP) approach. A variety of DG resources including photovoltaic and wind generation are incorporated in the analysis. Meanwhile, the uncertainties associated with solar radiation, wind speed, and fuel prices are considered. For the sake of numerical analysis, DG configuration plans are generated with the HOMER software; thereafter, the plans are assessed with the Expert Choice software that supports the AHP approach.",,Electronic:978-1-4799-3040-1; POD:978-1-4799-3041-8,10.1109/SGC.2013.6733800,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6733800,,Batteries;Cogeneration;Indexes;Inverters;Microgrids;Software;Uncertainty,analytic hierarchy process;distributed power generation;numerical analysis;photovoltaic power systems;power generation planning;wind power plants,AHP-based prioritization approach;DG resources;HOMER software;analytical hierarchical process;distributed generation configuration plans;electricity generation;energy efficiency promotion;expert choice software;fuel prices;microgrid generation plans;numerical analysis;photovoltaic generation;renewable energy resources;resource uncertainties;solar radiation;wind generation;wind speed,,0,,19,,no,17-18 Dec. 2013,,IEEE,IEEE Conference Publications
An analysis of contribution rates in relation with funding structurization in formulating the Indonesian National Standards (SNIS),I. P. Sari; A. Bakhtiar; A. Arvianto,"Nat. Stand. Agency of Indonesia (BSN), Univ. of Geneva, Geneva, Indonesia",2013 8th International Conference on Standardization and Innovation in Information Technology (SIIT),20140320,2013,,,1,7,"Standard formulation/development processes/stages are important to be analyzed as they determine whether a standard is acceptable in the market and/or able to meet the stakeholders' expectation. This research aims to identify the contribution rates of the SNI (Indonesian National Standard) formulation stages. The order of SNI formulation stages based on contribution rate is compared with that based on funding structurization to analyze the relations between them. Analytical Hierarchy Process (AHP) method is considered effective and efficient to measure contribution rate to evaluate the SNI formulation stages while activity-based costing (ABC) technique is used to identify the funding structurization. The research concludes that the existing SNI formulation stages are still relevant to be used. The rate contribution shows differences between stages but not significant. The stage of publication has the largest contribution rate (0.197703) while the stage of concept drafting has the largest funding structurization (USD 624.). There are differences in the order of importance of SNI formulation stages between the contribution rates and funding structurization. Moreover, this research also concludes that priority setting can be used to monitor the allocation of limited resources in order to be more effective. Thus, contribution rate analysis can become a tool to implement it. Also, the funding structurization can be used as a basis to create an equation to determine standard formulation costs that include fixed and variable costs. Further research can be conducted to analyze the relationship of contribution rate with other resources such as time requirement and human resources in the SNI formulation stages.",,Electronic:978-1-4799-3735-6; POD:978-1-4799-3736-3,10.1109/SIIT.2013.6774581,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6774581,AHP;Contribution rate;SNI;funding structurization,Costing;Decision making;Educational institutions;Meetings;Software;Standards,activity based costing;analytic hierarchy process;standards,ABC technique;AHP method;Indonesian National Standards;SNI formulation stages;activity-based costing;analytical hierarchy process;concept drafting stage;contribution rates;fixed costs;funding structurization;human resources;publication stage;standard formulation costs;time requirement;variable costs,,0,,16,,no,24-26 Sept. 2013,,IEEE,IEEE Conference Publications
An Analytic Framework for Frame-Level Dependent Bit Allocation in Hybrid Video Coding,C. Pang; O. C. Au; F. Zou; J. Dai; X. Zhang; W. Dai,"Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Kowloon, Hong Kong",IEEE Transactions on Circuits and Systems for Video Technology,20130531,2013,23,6,990,1002,"In this paper, we address the frame-level dependent bit allocation (DBA) problem in hybrid video coding. In most existing methods, the DBA solution is achieved at the expense of high, sometimes even unbearable, computational complexity because of the multipass coding involved. Motivated by this, we propose a model-based approach as an attempt to solve this problem analytically. Leveraging the predictive nature in hybrid video coding, we develop a novel interframe dependency model (IFDM), which enables a quantitative measure of the coding dependency between the current frame and its reference frame. Based on the IFDM, the buffer-constrained frame-level DBA problem is carefully formulated. Finally, the model-based DBA method called IFDM-DBA is derived, in which successive convex approximation techniques are employed to convert the original optimization problem into a series of convex optimization problem s of which the optimal solutions can be obtained efficiently. Experimental results suggest that the proposed IFDM-DBA method can achieve up to a 23% bitrate reduction over the JM reference software of H.264.",1051-8215;10518215,,10.1109/TCSVT.2013.2244795,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6428636,Convex optimization;dependent bit allocation (DBA);hybrid video coding;interframe dependency model (IFDM),Bit rate;Computational modeling;Decoding;Discrete cosine transforms;Encoding;Video coding;Video sequences,computational complexity;convex programming;video coding,H.264;IFDM-DBA;JM reference software;analytic framework;buffer-constrained frame-level DBA problem;coding dependency;computational complexity;convex optimization;dependent bit allocation;hybrid video coding;interframe dependency model;multipass coding;reference frame;successive convex approximation,,1,,27,,no,13-Jun,,IEEE,IEEE Journals & Magazines
An architecture for efficiently establishing the classroom cloud,J. W. Rau; Y. S. Chen,"Department of Electrical Engineering, Yuan Ze University, Chung-Li 32003, Taoyuan, Taiwan, ROC",2013 International Conference on Machine Learning and Cybernetics,20140908,2013,3,,1418,1422,"Cloud computing is the key for Big Data analytics. Apache Hadoop makes it feasible to process the immense data set. An experimental environment with flexibility is required to do education and research with Big Data. However, this solution brings with the challenges that are how to manage large number of computers in the Hadoop cluster, and how to reuse the idle computer resources. In this paper, we propose the classroom cloud infrastructure to overcome the challenges. With this infrastructure, we can reuse the idle computer resources in the classroom for education and research. The case study of such a classroom cloud implemented in YZU is presented and discussed to help reader who can follow architecture to set up their personal Hadoop experimental environment in the classroom.",2160-133X;2160133X,Electronic:978-1-4799-0260-6; POD:978-1-4799-0259-0,10.1109/ICMLC.2013.6890805,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6890805,Classroom Cloud;Computer resource management;Hadoop;Virtual machine,Cloud computing;Computer architecture;Computers;Hardware;Servers;User interfaces;Virtual machining,Big Data;cloud computing;computer aided instruction;data analysis;public domain software,Apache Hadoop cluster;Big Data analytics;YZU;classroom cloud infrastructure;cloud computing;data set;education;idle computer resources;personal Hadoop experimental environment;research,,2,,14,,no,14-17 July 2013,,IEEE,IEEE Conference Publications
An effect evaluation method for IMS SIP flooding attacks based on fuzzy comprehensive evaluation,Yanzan Guo; Xinsheng Ji; Caixia Liu,"Nat. Digital Switching Syst. Eng. Technol. R&D Center, Zhengzhou, China",2013 IEEE 4th International Conference on Software Engineering and Service Science,20130930,2013,,,973,976,"To quantitatively understand the effects of SIP flooding attacks in IMS, the paper introduces an evaluation method for attack effect based on fuzzy comprehensive evaluation. The paper gets the evaluation indexes by analyzing the possible attack flow, and introduces analytic hierarchy process (AHP) to compute the indexes' weight, and makes fuzzy comprehensive evaluation for attacks. To reduce the bias brought by a single fuzzy operator's properties, the paper selects several operators to complement with each other. Finally, it is validated that the evaluation method is able to distinguish the attack effects resulted by different SIP flooding attacks effectively.",2327-0586;23270586,Electronic:978-1-4673-5000-6; POD:978-1-4673-4999-4,10.1109/ICSESS.2013.6615468,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6615468,Fuzzy comprehensive evaluation;IMS (IP Multimedia Subsystem);SIP flooding attack effects,Authentication,IP networks;analytic hierarchy process;computer network security;fuzzy set theory;multimedia communication;signalling protocols,ARP;IMS SIP flooding attacks;IP multimedia subsystem;analytic hierarchy process;attack effect evaluation method;evaluation index;fuzzy comprehensive evaluation;fuzzy operator properties;index weight,,0,,12,,no,23-25 May 2013,,IEEE,IEEE Conference Publications
An End-to-End QoS Mapping Approach for Cloud Service Selection,R. Karim; C. Ding; A. Miri,"Dept. of Comput. Sci., Ryerson Univ., Toronto, ON, Canada",2013 IEEE Ninth World Congress on Services,20131107,2013,,,341,348,"In order to select and rank the best services in a cloud computing environment, the end-to-end quality of service (QoS) values of cloud services have to be computed. For a new SaaS provider, the deployment of its software application in the cloud is a challenging job. It has to find a hosting service (IaaS) that hosts its service. The primary goal of the SaaS provider is to make its service at the top of the ranked list of cloud services returned to end users through satisfying their QoS requirements. In this paper, we propose a mechanism to map the users' QoS requirements of cloud services to the right QoS specifications of SaaS then map them to best IaaS service that offers the optimal QoS guarantees. Then together SaaS and IaaS services can provide the best service offer to end users. As a result of the mapping, the end-to-end QoS values can be calculated. We propose a set of rules to perform the mapping process. We hierarchically model the QoS specifications of cloud services using the Analytic Hierarchy Process (AHP) method. The AHP based model helps to facilitate the mapping process across the cloud layers, and to rank the candidate cloud services for end users. We use a case study to illustrate and validate our solution approach.",2378-3818;23783818,Electronic:978-0-7695-5024-4; POD:978-1-4799-0412-9,10.1109/SERVICES.2013.71,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6655719,SaaS; IaaS; QoS mapping; QoS-based web Service Selection; Cloud Computing; Analytic Hierarchy Process,Cloud computing;Measurement;Quality of service;Reliability;Security;Software as a service;Usability,analytic hierarchy process;cloud computing;formal specification;quality of service,AHP based model;IaaS;QoS specifications;SaaS provider;analytic hierarchy process method;candidate cloud service ranking;cloud computing environment;cloud layers;cloud service selection;end-to-end QoS mapping approach;end-to-end quality of service values;optimal QoS guarantees,,8,,24,,no,June 28 2013-July 3 2013,,IEEE,IEEE Conference Publications
An Open Source Simulator for IEEE1588 Time Synchronization over 802.11 Networks,Y. Huang; Y. Yang; T. Li; X. Dai,"Sch. of Electron. & Inf. Eng., Southwest Univ., Chongqing, China",2013 European Modelling Symposium,20140331,2013,,,560,565,"As a key technique in distributed systems, time synchronization plays an important role in time sensitive modern industrial wireless local area networks (WLANs). Based on time packet exchange technology, the IEEE 1588 precise time protocol (PTP) achieves high precise and low power consumption time synchronization in wired Ethernet. In WLANs, however, it is difficult for the wireless sensor nodes to acquire accurate timestamps due to their limited resources. Moreover, compared to wired Ethernet, WLANs suffer more from wireless channel sharing, fading and packet collisions, which leads to overwhelming transmission delay jitters. Therefore, analytic solution to PTP's performance in WLANs is very difficult and it is of importance to evaluate PTP's performance by realistic simulation. In this paper, based on the open source OMNeT++ simulation engine, we present the simulator we developed for PTP time synchronization in 802.11 WLANs. The behavior of the PTP time synchronization, the simulation results and the factors that affect the PTP performance are presented and evaluated.",,CD-ROM:978-1-4799-2577-3; Electronic:978-1-4799-2578-0; POD:978-1-4799-2579-7,10.1109/EMS.2013.93,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6779905,IEEE 1588;OMNeT++;industrial WLAN;time synchronization,Clocks;Delays;Protocols;Standards;Synchronization;Uncertainty;Wireless communication,IEEE standards;protocols;public domain software;synchronisation;wireless LAN,802.11 WLAN;Ethernet;IEEE 1588 precise time protocol;IEEE1588 time synchronization;PTP;fading channel;modern industrial wireless local area networks;open source OMNeT++ simulation engine;open source simulator;packet collisions;time packet exchange technology;transmission delay jitters;wireless channel sharing;wireless sensor nodes,,0,,19,,no,20-22 Nov. 2013,,IEEE,IEEE Conference Publications
Analysis of electromechanical interface model for liquid floated micro-gyroscope,Mingyuan Ren; Xiaowei Liu; Wen Zuo; Zhigang Mao,"MEMS center, Harbin Institute of Technology, CHINA",The 8th Annual IEEE International Conference on Nano/Micro Engineered and Molecular Systems,20130718,2013,,,799,802,"This paper presents an electromechancial interface model of liquid floated micro-gyroscope, which can analyze sensing capacitors and resistance of gyroscope. The impacts of them are analyzed and simulated with the simulation software of circuit. The analytic results indicate that the reasonable interface circuit can substantially remove the impacts of these parasitic capacitances, increase the signal noise ratio. To ensure the high resolution, the model of liquid floated micro-gyroscope with ANSOFT conducted. The results show the transfer characteristics of the sensor and nonlinear error of transfer characteristics.",,Electronic:978-1-4673-6352-5; POD:978-1-4673-6350-1; USB:978-1-4673-6351-8,10.1109/NEMS.2013.6559846,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6559846,electromechanical interface model;gyroscopet;liquid floated,Capacitors;Gyroscopes;Integrated circuit modeling;Liquids;Magnetic liquids;Mathematical model;Rotors,gyroscopes;microsensors,ANSOFT;capacitors;electromechanical interface model;gyroscope resistance;high resolution;liquid floated microgyroscope;nonlinear error;parasitic capacitances;reasonable interface circuit;sensors;signal noise ratio;transfer characteristics,,0,,2,,no,7-10 April 2013,,IEEE,IEEE Conference Publications
Analysis on the Scheduling Problem in Transparent Computing,R. Ju; Z. Yaoxue; C. Jianer,"Coll. of Inf. Sci. & Eng., Central South Univ., Changsha, China",2013 IEEE 10th International Conference on High Performance Computing and Communications & 2013 IEEE International Conference on Embedded and Ubiquitous Computing,20140612,2013,,,1832,1837,"Transparent computing has received increasing attention recently. As a variation and implementation of cloud computing, scheduling is destined to be one of the most hot topics for performance optimization. In this paper, we focus on a typical transparent computing platform, where a number of clients are connected with a server cluster in a gigabit LAN. Due to the NP-completeness of the scheduling problem on parallel processors, the optimal scheduling solution of TC can not be achieved in polynomial time. To find an efficient scheduling algorithm, We present two approximation algorithms and analysed their upper bounds theoretically. Though the analytic results are less than satisfactory, the practical performances of the proposed algorithms are demonstrated to be acceptable by extensive simulations.",,Electronic:978-0-7695-5088-6; POD:978-1-4799-0973-5,10.1109/HPCC.and.EUC.2013.263,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6832145,NP-complete;optimization;scheduling;transparent computing,Algorithm design and analysis;Optimal scheduling;Optimized production technology;Scheduling;Scheduling algorithms;Servers,approximation theory;cloud computing;computational complexity;file servers;local area networks;optimisation;scheduling;software performance evaluation,approximation algorithms;cloud computing;gigabit LAN;performance optimization;scheduling problem NP-completeness;server cluster;transparent computing platform,,0,,15,,no,13-15 Nov. 2013,,IEEE,IEEE Conference Publications
Analytic Hierarchy process selection for batteries storage technologies,F. Ben Ammar; I. H. Hafsa; F. Hammami,Laboratoire M.M.A - INSAT - Centre Urbain Nord BP N&#x00B0;676 1080 Tunis - Universit&#x00E9; de Carthage,2013 International Conference on Electrical Engineering and Software Applications,20130815,2013,,,1,6,"The objective of this study is to select the most appropriate battery technology for photovoltaic application. However, the selection criteria and the diversity of technologies make choice difficult. So, we focus on the Analytic Hierarchy Process which is among the most widely used and has been applied in several multicriteria decision making domains.",,Electronic:978-1-4673-6301-3; POD:978-1-4673-6302-0,10.1109/ICEESA.2013.6578374,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6578374,AHP;Battery technology;multi criteria;selection,Analytic hierarchy process;Batteries;Discharges (electric);Lead;Vectors,analytic hierarchy process;battery storage plants;photovoltaic power systems,analytic hierarchy process selection;battery storage technology;multicriteria decision making domain;photovoltaic application,,0,,17,,no,21-23 March 2013,,IEEE,IEEE Conference Publications
Analytic results for efficient computation of the Nuttall-Q and incomplete Toronto functions,P. C. Sofotasios; K. Ho-Van; T. D. Anh; H. D. Quoc,"Sch. of Electron. & Electr. Eng., Univ. of Leeds, Leeds, UK",2013 International Conference on Advanced Technologies for Communications (ATC 2013),20140102,2013,,,420,425,"This work is devoted to the derivation of novel analytic results for special functions which are particularly useful in wireless communication theory. Capitalizing on recently reported series representations for the Nuttall Q-function and the incomplete Toronto function, we derive closed-form upper bounds for the corresponding truncation error of these series as well as closed-form upper bounds that under certain cases become accurate approximations. The derived expressions are tight and their algebraic representation is rather convenient to handle analytically and numerically. Given that the Nuttall-Q and incomplete Toronto functions are not built-in in popular mathematical software packages, the proposed results are particularly useful in computing these functions when employed in applications relating to natural sciences and engineering, such as wireless communication over fading channels.",2162-1020;21621020,Electronic:978-1-4799-1089-2; POD:978-1-4799-1087-8,10.1109/ATC.2013.6698148,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6698148,,Approximation methods;Equations;Fading;Finite wordlength effects;Software packages;Telecommunications;Upper bound,approximation theory;fading channels,Nuttall-Q function;algebraic representation;closed-form upper bounds;engineering;fading channels;incomplete Toronto function;natural sciences;truncation error;wireless communication theory,,4,,43,,no,16-18 Oct. 2013,,IEEE,IEEE Conference Publications
Analytical analysis of magnetic field of eddy-current driver based on equivalent current method,Tongyu Shi; D. Wang; Z. Liu; Songning Shi,"Sch. of Inf. Sci. & Eng., Northeastern Univ., Shenyang, China","Proceedings 2013 International Conference on Mechatronic Sciences, Electric Engineering and Computer (MEC)",20140828,2013,,,3321,3325,"According to the structure parameters of the eddy-current driver, this paper established the magnetic field analytical model of eddy-current driver by utilizing the method of equivalent surface current. We calculated and analyzed the magnetic field of the eddy-current driver with the help of the MATLAB simulation software. The purpose was to explore the regularity of analytical model of magnetic field. Then the result was compared with that of finite element method and measured values. The compared result showed that the space distribution of the axial magnetic field on the surface of copper conductor plate of eddy current drive was periodically distributed along the circumferential Angle. The error of analytic method is less than that of the numerical solution method.",,CD-ROM:978-1-4799-2563-6; Electronic:978-1-4799-2565-0; POD:978-1-4799-2566-7,10.1109/MEC.2013.6885590,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6885590,analytical analysis;eddy-current driver;equivalent surface current;magnetic field,Copper;Eddy currents;Educational institutions;Magnetic fields;Magnetic flux density;Permanent magnets;Rotors,copper;digital simulation;drives;eddy currents;magnetic fields;mechanical engineering computing,MATLAB simulation software;analytical analysis;axial magnetic field;copper conductor plate;eddy-current driver;equivalent current method;equivalent surface current;magnetic field analytical model;space distribution;structure parameters,,0,,14,,no,20-22 Dec. 2013,,IEEE,IEEE Conference Publications
Analytical modelling of the current (I)-voltage (V) characteristics of sub-micron gate-length ion-implanted GaAs MESFETs under dark and illuminated conditions,S. Tripathi; S. Jit,"Centre for Research in Microelectronics (CRME), Department of Electronics Engineering, Indian Institute of Technology, (Banaras Hindu University), Varanasi 221005, India","IET Circuits, Devices & Systems",20130613,2013,7,1,42,60,"An analytical model for current (I)-voltage (V) characteristics of a short-channel ion-implanted GaAs MESFET has been presented for dark and illuminated conditions. For the sake of simplicity, the non-analytic (i.e. non-integral) Gaussian doping function commonly considered for the channel doping of an ion-implanted GaAs metal semiconductor field effect transistor (MESFET) has been replaced by an analytic Gaussian-like doping profile in the vertical direction. The device uses an indium-tin oxide-based Schottky gate through which an optical radiation of 0.87 ë_m wavelength is coupled from an external source into the device to modulate the I-V characteristics of the short-gate length GaAs MESFET. The coupled light generates electron-hole pairs in the active channel region below the gate and develops a photovoltage across the Schottky gate-channel junction and modulates the device characteristics. This study also includes the modelling of this photovoltaic effect by taking the short-gate length effects into consideration. The developed model includes the effects of doping profile and device parameters on the drain current of the short-channel ion-implanted GaAs MESFETs under dark and illuminated conditions of operations. The accuracy of the proposed model is extensively verified by comparing the theoretically predicted results with numerical simulation data obtained by using the commercially available ATLASTM device simulation software.",1751-858X;1751858X,,10.1049/iet-cds.2012.0145,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6531070,,,,,,0,,,,no,Jan. 2013,,IET,IET Journals & Magazines
Analytical performance analysis of mesh network-on-chip based on network calculus,N. Moussa; R. Tourki,,"2013 International Conference on Control, Decision and Information Technologies (CoDIT)",20131223,2013,,,325,329,"The design of on chip interconnection architecture (NoC) should carefully take on consideration both hardware and communication constraints in order to built up a system that meets quality of service requirements. In NoC architecture, the on chip switch available hardware and software resources drive up the global performances of communication processes. Therefore it is crucial, before the physical design process, to carry out the required capacities such as buffer depth and management-tasks of a flit. In fact, one of the most critical parameters that can affect communication characteristics are the available memory space in addition to flit-time processing according to a given scheduling approach. This paper deals with these concepts. It presents a study of NoC switch using Network Calculs (NC) theory. It provides an analytic model of the internal on chip switch architecture to study the performance with a mathematical approach. This helps to specify the best physical and logical characteristics that can achieve enhanced performances.",,Electronic:978-1-4673-5549-0; POD:978-1-4673-5548-3,10.1109/CoDIT.2013.6689565,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6689565,Network Calculs(NC);Network on chip (NoC);QualitÌ© de Service (QoS),Computer architecture;Delays;Equations;Mathematical model;Multiplexing;Routing;Switches,calculus of communicating systems;network-on-chip;performance evaluation,NoC architecture;NoC switch;chip interconnection architecture;communication processes;flit time processing;hardware resources;management tasks;memory space;mesh network on chip;network calculus theory;on chip switch;performance analysis;scheduling;software resources,,0,,13,,no,6-8 May 2013,,IEEE,IEEE Conference Publications
Analytics for Product Planning: In-Depth Interview Study with SaaS Product Managers,F. Fotrousi; K. Izadyan; S. A. Fricker,"Sch. of Comput., Blekinge Inst. of Technol., Karlskrona, Sweden",2013 IEEE Sixth International Conference on Cloud Computing,20140217,2013,,,871,879,"SaaS cloud computing, in contrast to packaged products, enables permanent contact between users of a software product and the product-owning company. When planning the development and evolution of a software product, a product manager depends on reliable information about feature attractiveness. So far, planning decisions were based on stakeholder opinion and the customer's willingness to buy. Whether or not a feature actually is used was out of consideration. Analytics that measure the interaction between users and the SaaS gives product managers unprecedented access to information about product usage. To understand whether and how SaaS analytics can be used for product planning decision, we performed 17 in-depth interviews with experienced managers of SaaS products and analyzed the results analyzed with a mixed-method strategy. The empirical results characterize the relevance of a broad range of analytics for product planning decisions, and the strengths and limitations of an analytics-based product planning approach.",2159-6182;21596182,Electronic:978-0-7695-5028-2; POD:978-1-4799-0490-7,10.1109/CLOUD.2013.33,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6740237,SaaS;Software product management;analytics;decision-making;product planning,Business;Interviews;Planning;Quality of service;Software as a service;Usability,DP industry;cloud computing;customer services;production planning;software management;software reliability,SaaS analytics;SaaS cloud computing;SaaS product managers;analytics-based product planning approach;customer willingness;mixed-method strategy;packaged products;product planning decision;product-owning company;reliable information;software product,,2,,66,,no,June 28 2013-July 3 2013,,IEEE,IEEE Conference Publications
"Analytics over Big Data: Exploring the Convergence of DataWarehousing, OLAP and Data-Intensive Cloud Infrastructures",A. Cuzzocrea,"ICAR, Univ. of Calabria, Rende, Italy",2013 IEEE 37th Annual Computer Software and Applications Conference,20131031,2013,,,481,483,"This paper explores the convergence of Data Warehousing, OLAP and data-intensive Cloud Infrastructures in the context of so-called analytics over Big Data. The paper briefly reviews some state-of-the-art proposals, highlights open research issues and, finally, it draws possible research directions in this scientific field.",,Electronic:978-0-7695-4986-6; POD:978-1-4673-6494-2,10.1109/COMPSAC.2013.152,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6649871,Big Data; Data Warehousing and OLAP over Big Data; Data-Intensive Cloud Infrastructures,Bismuth;Context;Convergence;Data handling;Data storage systems;Information management;Warehousing,cloud computing;data mining;data warehouses,OLAP;big data analytics;data warehousing;data-intensive cloud infrastructures,,5,,11,,no,22-26 July 2013,,IEEE,IEEE Conference Publications
Animating signing avatar using descriptive sign language,N. Ben Yahia; M. Jemni,"High School of Sciences and Techniques of Tunis Research Laboratory: Information Technology and Communication & Electrical Engineering (Latice) Tunis, Tunisia",2013 International Conference on Electrical Engineering and Software Applications,20130815,2013,,,1,5,"This paper proposes an interactive manipulation of signing avatar (complex human's figures) by means of a descriptive sign language. The objective is to improve the human-computer communication, to generate precise signs for Deaf community, to control and animate the virtual character. This approach has been developed to manipulate robots through several analytic and numerical resolution methods. These methods have been applied to the virtual character for better monitoring and to ensure a realistic animation in real time.",,Electronic:978-1-4673-6301-3; POD:978-1-4673-6302-0,10.1109/ICEESA.2013.6578385,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6578385,Animation;Avatar;Gesture;Sign language,Animation;Assistive technology;Avatars;Gesture recognition;Joints;Kinematics;Mathematical model,avatars;computer animation;human computer interaction;numerical analysis,analytic method;deaf community;descriptive sign language;human-computer communication;interactive signing avatar manipulation;numerical resolution method;robot manipulation;signing avatar animation;virtual character,,0,,31,,no,21-23 March 2013,,IEEE,IEEE Conference Publications
Aperture: An Open Web 2.0 Visualization Framework,D. Jonker; S. Langevin; N. Bozowsky; W. Wright,,2013 46th Hawaii International Conference on System Sciences,20130318,2013,,,1485,1494,"Aperture is an open, adaptable and extensible Web 2.0 visualization framework, designed to produce visualizations for analysts and decision makers in any common web browser. Aperture utilizes a novel layer based approach to visualization assembly, and a data mapping API that simplifies the process of adaptable transformation of data and analytic results into visual forms and properties. This common visual layer and data mapping API, combined with core elements such as contextually derivable color palettes, layout and symbol ontology services is designed to enable highly creative and expressive visual analytics, rapidly and with less effort. This paper introduces the Aperture framework, describing key features of the programming API and reference implementation, presents example use cases, and proposes an approach for measuring technical performance metrics for software development, and operational performance metrics for visualization support of analysis and decision making.",1530-1605;15301605,Electronic:978-1-5090-5646-0; POD:978-1-4673-5933-7; USB:978-0-7695-4892-0,10.1109/HICSS.2013.96,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6480018,agile;big data;framework;toolkit;visual analytics;visualization,Apertures;Assembly;Data visualization;Image color analysis;Visual analytics,Internet;application program interfaces;data analysis;data visualisation;decision making;online front-ends;ontologies (artificial intelligence);software metrics;software performance evaluation,Open Web 2.0 visualization framework;Web browser;adaptable data transformation;aperture framework;contextually derivable color palettes;creative visual analytics;data mapping API;decision making;expressive visual analytics;layer based approach;operational performance metrics;programming API;reference implementation;software development;symbol ontology services;technical performance metrics;visual layer;visualization assembly,,2,,27,,no,7-10 Jan. 2013,,IEEE,IEEE Conference Publications
App Analytic: A Study on Correlation Analysis of App Ranking Data,S. Y. Ihm; W. K. Loh; Y. H. Park,"Dept. of Multimedia Sci., Sookmyung Women's Univ., Seoul, South Korea",2013 International Conference on Cloud and Green Computing,20131219,2013,,,561,563,"In recent years, app store and android market have experienced a significant growth in terms of app numbers. Since we discover 85% of apps through the ranks, it is important to develop effective app ranking analyzing tools. In this paper, we present a method called App Analytic. In our method, we explore correlations of app ranking data about popular social networking sites. Specifically, we analyze correlations between various characteristics of social networking sites on Internet and android market. The results of correlation analysis reveal that there is a strong positive correlation of the number of app downloads with the number of registered users and page rank. We also provide an in-depth analysis on the major factors that impact the correlations.",,Electronic:978-0-7695-5114-2; POD:978-1-4799-1362-6,10.1109/CGC.2013.95,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6686087,R statistical software;app ranking data;correlation analysis,Androids;Correlation;Humanoid robots;Internet;Smart phones;Social network services;Software,Android (operating system);Internet;data analysis;social networking (online),App Analytic;App ranking analyzing tools;App ranking data;App store;Internet;android market;correlation analysis;social networking sites,,0,1,4,,no,Sept. 30 2013-Oct. 2 2013,,IEEE,IEEE Conference Publications
Application management services analytics,Y. Li; T. H. Li; R. Liu; J. Yang; J. Lee,"IBM T. J. Watson Research Center, Yorktown Heights, NY, U.S.A","Proceedings of 2013 IEEE International Conference on Service Operations and Logistics, and Informatics",20130926,2013,,,366,371,"Enterprises often maintain many IT applications to support their business. Application Management Services (AMS) aim to maintain high levels of service quality and availability by restoring normal application service operations and minimizing negative business impact. In this paper, we present the AMS Analytics System for improving the productivity and quality of delivery for AMS practices. Issues regarding IT applications are formally referred as IT incidents or tickets, which are an important vehicle for measuring quality of AMS. IT incident ticket analytics, an important component of of the analytics system, measures workload variability, resource productivity and delivery performance using algorithms from statistics, queuing theory, data clustering and signal processing. The AMS Analytics System provides a standardized, integrated analytics platform supporting AMS delivery. It is built on a Web platform using a set of standard open stack software, enhanced with advanced analytics. Since its initial release, we have applied the AMS Analytics System to several dozens of real-world enterprise users, receiving very positive feedback.",,Electronic:978-1-4799-0530-0; POD:978-1-4799-0528-7,10.1109/SOLI.2013.6611442,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6611442,Application management services;IT incident analysis;IT incident tickets;IT incidents;incident management,Data models;Maintenance engineering;Market research;Productivity;Resource management;Standards,business data processing;information technology;productivity,AMS analytics system;AMS delivery;AMS practices;IT applications;IT incident ticket analytics;Web platform;application management services analytics;data clustering;delivery performance;enterprises;negative business impact;normal application service operations;productivity;quality of delivery;queuing theory;service quality;signal processing;standard open stack software;standardized integrated analytics platform;statistics;workload variability,,3,,11,,no,28-30 July 2013,,IEEE,IEEE Conference Publications
Approximate computing: Energy-efficient computing with good-enough results,A. Raghunathan; K. Roy,"Purdue University, West Lafayette, IN 47906",2013 IEEE 19th International On-Line Testing Symposium (IOLTS),20130919,2013,,,258,258,"Summary form only given. With the explosion in digital data, computing platforms are increasingly being used to execute applications (such as web search, data analytics, sensor data processing, recognition, mining, and synthesis) for which ‰ÛÏcorrectness‰Ûù is defined as producing results that are good enough, or of sufficient quality. Such applications invariably demonstrate a high degree of inherent resilience to their underlying computations being executed in an approximate manner. This inherent resilience is due to several factors including redundancy in the input data, the statistical nature of the computations themselves, and the acceptability (often, inevitability) of less-than-perfect results. Approximate computing is an approach to designing systems that are more efficient, by leveraging the inherent resilience of applications. We will outline a range of approximate computing techniques that we have developed from software to architecture to circuits, which have shown promising results. We conclude with a discussion of some of the challenges that need to be addressed to facilitate a broader adoption of approximate computing.",1942-9398;19429398,Electronic:978-1-4799-0664-2; POD:978-1-4799-0663-5; USB:978-1-4799-0662-8,10.1109/IOLTS.2013.6604092,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6604092,,Data mining;Data processing;Educational institutions;Energy efficiency;Explosions;Resilience;Web search,power aware computing;statistical analysis,approximate computing techniques;digital data;energy-efficient computing;good-enough results;input data redundancy;statistical computation nature,,1,,,,no,8-10 July 2013,,IEEE,IEEE Conference Publications
Archetypical motion: Supervised game behavior learning with Archetypal Analysis,R. Sifa; C. Bauckhage,Game Analytics and University of Bonn,2013 IEEE Conference on Computational Inteligence in Games (CIG),20131017,2013,,,1,8,"The problem of creating believable game AI poses numerous challenges for computational intelligence research. A particular challenge consists in creating human-like behaving game bots by means of applying machine learning to game-play data recorded by human players. In this paper, we propose a novel, biologically inspired approach to behavior learning for video games. Our model is based on the idea of movement primitives and we use Archetypal Analysis to determine elementary movements from data in order to represent any player action in terms of convex combinations of archetypal motions. Given these representations, we use supervised learning in order to create a system that is able to synthesize appropriate motion behavior during a game. We apply our model to teach a first person shooter game bot how to navigate in a game environment. Our results indicate that the model is able to simulate human-like behavior at lower computational costs than previous approaches.",2325-4270;23254270,Electronic:978-1-4673-5311-3; POD:978-1-4673-5310-6,10.1109/CIG.2013.6633609,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6633609,,Biological system modeling;Computational modeling;Games;History;Training;Vectors,computer games;learning (artificial intelligence);software agents,archetypal analysis;archetypal motion;archetypical motion;believable game AI;biologically inspired approach;computational intelligence;convex combination;elementary movement determination;first person shooter game bot teaching;game environment navigation;game-play data;human players;human-like behaving game bot;machine learning;motion behavior synthesis;movement primitives;player action representation;supervised game behavior learning;supervised learning;video game,,6,,22,,no,11-13 Aug. 2013,,IEEE,IEEE Conference Publications
Architecture Security Evaluation Method Based on Security of the Components,C. Du; X. Li; H. Shi; J. Hu; R. Feng; Z. Feng,"Dept. of Comput. Sci. & Technol., Tianjin Univ., Tianjin, China",2013 20th Asia-Pacific Software Engineering Conference (APSEC),20140428,2013,1,,523,528,"This paper presents a quantitative architecture security evaluation method to identify potential risks of an architecture. The method is based on security of the architecture components. In this method, components of the architecture are classified and their security measures are identified according to component function and architecture level. Then, an integration process applies analytic hierarchy process (AHP) and fuzzy evaluation analysis to determine quantitative and qualitative factors in evaluating the security of components. These factors are used to obtain security conclusions of the architecture. The experiment shows that the method not only improves efficiency of the evaluation, but also makes security evaluation process more objective and accurate.",1530-1362;15301362,CD-ROM:978-1-4799-2143-0; Electronic:978-1-4799-2144-7; POD:978-1-4799-2145-4,10.1109/APSEC.2013.75,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6805446,AHP;Fuzzy evaluation analysis;architecture;component;security evaluation,Access control;Authentication;Business;Computer architecture;Encryption;Vectors,analytic hierarchy process;fuzzy set theory;object-oriented programming;safety-critical software;software architecture,AHP;analytic hierarchy process;architecture level;component function;efficiency improvement;fuzzy evaluation analysis;integration process;qualitative factors;quantitative architecture security evaluation method;quantitative factors;risk identification;security measures,,0,,16,,no,2-5 Dec. 2013,,IEEE,IEEE Conference Publications
Architecture-Based Adaptivity Support for Service Oriented Scientific Workflows,Y. Liu; I. Gorton; A. Wynne,"Concordia Univ., Montreal, QC, Canada",2013 IEEE Seventh International Symposium on Service-Oriented System Engineering,20130610,2013,,,309,314,"Adaptivity is the ability of a program to change its behavior automatically according to its context. Programs over multiple scientific workflows and analytic domains have similar needs of adaptivity to handle data intensive computing. These include dynamically selecting analytical models or processes according to data sets at runtime, handling exceptions to make long running workflows reliable, and reducing large volumes of data to a suitable form for visualization. Architecture support to reuse these adaptive techniques across different scientific domains helps to enhance the workflows' efficiency, extensibility and reliability. In this paper, a service oriented architecture framework is presented to compose adaptive scientific workflows. This framework has a core of service components and mechanisms that eases the interoperation between disparate workflows and programs that encapsulate the adaptive control logic. The uses of this framework are demonstrated by scientific workflow scenarios.",,Electronic:978-0-7695-4944-6; POD:978-1-4673-5659-6,10.1109/SOSE.2013.37,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6525537,SOA;adaptive software;scientific workflow,Adaptation models;Adaptive control;Buildings;Connectors;Meteorology;Monitoring;Pipelines,service-oriented architecture;software reliability,adaptive control logic;adaptive scientific workflow;architecture-based adaptivity support;data intensive computing;service oriented architecture;service oriented scientific workflow,,0,,14,,no,25-28 March 2013,,IEEE,IEEE Conference Publications
Are Software Analytics Efforts Worthwhile for Small Companies? The Case of Amisoft,R. Robbes; R. Vidal; M. C. Bastarrica,University of Chile,IEEE Software,20130903,2013,30,5,46,53,"Amisoft, a Chilean software company with 43 employees, successfully uses software analytics in its projects. These support a variety of strategic and tactical decisions, resulting in less overwork of employees. However, the analytics done at Amisoft are very different from the ones used in larger companies.",0740-7459;07407459,,10.1109/MS.2013.92,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6544520,Amisoft;Companies;Product development;Software analytics;analytics viability;small businesses;software analytics,Companies;Product development;Software analytics,project management;small-to-medium enterprises;software development management;software houses;software maintenance;software metrics,Amisoft;Chilean software company;employee overwork;small companies;software analytics;strategic decision;tactical decision,,1,,8,,no,Sept.-Oct. 2013,,IEEE,IEEE Journals & Magazines
Assessment of student's learning style and engagement in traditional based software engineering education,N. Pratheesh; T. Devi,"Department of Computer Applications School of Computer Science and Engineering Bharathiar University, Coimbatore - 641 046, India",2013 International Conference on Intelligent Interactive Systems and Assistive Technologies,20130923,2013,,,25,31,"Software Engineering courses are nucleus elements of the Computer Science syllabus. While the main aspire of such courses is to give students practical industry-relevant ‰ÛÏsoftware engineering in the large‰Ûù familiarity, frequently such courses plummet short of this significant intention owed to be short of industrial experience and support infrastructure, degenerating the course into ‰ÛÏone big coding assignment‰Ûù. Therefore, it is obligatory to design and develop improved infrastructure support for teaching or taking such courses. This would benefit instructors and students communities in computer science atmosphere. Learning analytics lend a hand to the learners to enhance their learning tricks. This was scrutinized among the software engineering students how the learning style and learning engagement sways in gathering knowledge. This paper discusses the importance of learning analytics in software engineering education especially the learning style, learning engagement and its influences in this domain.",,Electronic:978-1-4799-0476-1; POD:978-1-4799-0477-8,10.1109/IISAT.2013.6606434,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606434,Collaborative Learning;Learning Analytics;Learning Style;Social Learning Analytics;Software Engineering Education,Computer science;Education;Industries;Knowledge engineering;Software;Software engineering;Standards,computer science education;educational courses;software engineering,computer science atmosphere;computer science syllabus;industrial experience;learning analytics;learning engagement;software engineering courses;software engineering education;software engineering students;student learning style,,0,,24,,no,2-3 Aug. 2013,,IEEE,IEEE Conference Publications
Assisting developers of Big Data Analytics Applications when deploying on Hadoop clouds,W. Shang; Z. M. Jiang; H. Hemmati; B. Adams; A. E. Hassan; P. Martin,"Software Analysis and Intelligence Lab (SAIL), School of Computing, Queen's University, Kingston, Canada",2013 35th International Conference on Software Engineering (ICSE),20130926,2013,,,402,411,"Big data analytics is the process of examining large amounts of data (big data) in an effort to uncover hidden patterns or unknown correlations. Big Data Analytics Applications (BDA Apps) are a new type of software applications, which analyze big data using massive parallel processing frameworks (e.g., Hadoop). Developers of such applications typically develop them using a small sample of data in a pseudo-cloud environment. Afterwards, they deploy the applications in a large-scale cloud environment with considerably more processing power and larger input data (reminiscent of the mainframe days). Working with BDA App developers in industry over the past three years, we noticed that the runtime analysis and debugging of such applications in the deployment phase cannot be easily addressed by traditional monitoring and debugging approaches. In this paper, as a first step in assisting developers of BDA Apps for cloud deployments, we propose a lightweight approach for uncovering differences between pseudo and large-scale cloud deployments. Our approach makes use of the readily-available yet rarely used execution logs from these platforms. Our approach abstracts the execution logs, recovers the execution sequences, and compares the sequences between the pseudo and cloud deployments. Through a case study on three representative Hadoop-based BDA Apps, we show that our approach can rapidly direct the attention of BDA App developers to the major differences between the two deployments. Knowledge of such differences is essential in verifying BDA Apps when analyzing big data in the cloud. Using injected deployment faults, we show that our approach not only significantly reduces the deployment verification effort, but also provides very few false positives when identifying deployment failures.",0270-5257;02705257,Electronic:978-1-4673-3076-3; POD:978-1-4673-3075-6,10.1109/ICSE.2013.6606586,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606586,Big-Data Analytics Application;Cloud Computing;Hadoop;Log Analysis;Monitoring and Debugging,Context;Data handling;Data storage systems;Information management;Joining processes;Keyword search;Programming,cloud computing;data analysis;formal verification;parallel processing;program debugging;public domain software;software fault tolerance;system monitoring,Hadoop clouds;Hadoop-based BDA Apps;big data analysis;big data analytics applications;cloud deployments;deployment verification effort reduction;developer assistance;execution log abstraction;execution sequence recovery;parallel processing frameworks;software applications,,14,,26,,no,18-26 May 2013,,IEEE,IEEE Conference Publications
Attitude Control of Five Degrees of Freedom Air-Bearing Platform Based on Fractional Order Sliding Mode,D. Liwei; S. Shenmin; G. Yong,"Center for Control Theor. & Guidance Technol., Harbin Inst. of Technol., Harbin, China","2013 Third International Conference on Instrumentation, Measurement, Computer, Communication and Control",20140623,2013,,,1530,1534,"Spacecraft attitude determination and control hard verification and software development can be simultaneously simulated with five degrees of freedom air-bearing spacecraft simulator. In this paper, a new robust fractional order sliding mode controller is proposed for simulator attitude control with control input limited amplitude under considering gravity unbalance torque disturbance and actuator assembling error. The attitude system can convergence to fractional order sliding mode surface within the finite time, which is proven by using fractional order Lyapunov stability theory. Furthermore, numerical simulations are also included to reinforce the analytic results and to validate the excellent effect of the new robust fractional sliding mode controller.",,Electronic:978-0-7695-5122-7; POD:978-1-4799-1393-0,10.1109/IMCCC.2013.341,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6840731,air-bearing platform;fractional calculus;fractional order sliding;spacecraft attitude,Angular velocity;Attitude control;Gravity;Sliding mode control;Space vehicles;Torque,Lyapunov methods;actuators;aircraft control;attitude control;numerical analysis;robust control;variable structure systems,actuator assembling error;amplitude;attitude system;finite time;five degrees of freedom air-bearing spacecraft simulator;fractional order Lyapunov stability theory;gravity unbalance torque disturbance;numerical simulations;robust fractional order sliding mode controller;simulator attitude control;software development;spacecraft attitude determination;verification,,0,,12,,no,21-23 Sept. 2013,,IEEE,IEEE Conference Publications
Backtrack-Based Failure Recovery in Distributed Stream Processing,Q. Chen; M. Hsu; M. Castellanos,"HP Labs., Palo Alto, CA, USA","2013 14th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing",20130916,2013,,,261,266,"Since stream analytics is treated as a kind of cloud service, there exists a pressing need for its reliability and fault-tolerance. In a streaming process, the parallel and distributed tasks are chained in a graph-structure with each task transforming a stream to a new stream, the transaction property guarantees the streaming data, called tuples, to be processed in the order of their generation in every dataflow path, with each tuple processed once and only once. The failure recovery of a task allows the previously produced results to be corrected for eventual consistency, which is different from the instant consistency of global state enforced by the failure recovery of general distributed systems, and therefore presents new technical challenges. Transactional stream processing typically requires every task to checkpoint its execution state, and when it is restored from a failure, to have the last state recovered from the checkpoint and missing tuple re-acquired and processed. Currently there exist two kind approaches: one treats the whole process as a single transaction, and therefore suffers from the loss of intermediate results during failures, the other relies on the receipt of acknowledgement (ACK) to decide whether moving forward to emit the next resulting tuple or resending the current one after timeout, on the per-tuple basis, thus incurs extremely high latency penalty. In contradistinction to the above, we propose the backtrack mechanism for failure recovery, which allows a task to process tuples continuously without waiting for ACKs and without resending tuples in the failure-free case, but to request (ASK) the source tasks to resend the missing tuples only when it is restored from a failure which is a rare case thus has limited impact on the overall performance. We have implemented the proposed mechanisms on Fontainebleau, the distributed stream analytics infrastructure we developed on top of Storm. As a principle, we ensure all the transactional proper- ies to be system supported and transparent to users. Our experience shows that the ASK-based recovery mechanism significantly outperforms the ACK-based one.",,Electronic:978-0-7695-5005-3; POD:978-1-4799-0371-9,10.1109/SNPD.2013.36,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6598475,Dataflow transaction;Stream processing,Abstracts;Amplitude shift keying;Checkpointing;Cognition;Semantics;Servers;Storms,cloud computing;distributed processing;software fault tolerance,ASK-based recovery mechanism;Storm;acknowledgement receipt;backtrack-based failure recovery;cloud service;distributed stream processing;graph structure;stream analytics;transaction property;tuple processing,,0,,18,,no,1-3 July 2013,,IEEE,IEEE Conference Publications
Big Data -- Opportunities and Challenges Panel Position Paper,E. Bertino,"CS Dept., Purdue Univ., West Lafayette, IN, USA",2013 IEEE 37th Annual Computer Software and Applications Conference,20131031,2013,,,479,480,This paper summarizes opportunities and challenges of big data. It identifies important research directions and includes a number of questions that have been debated by the panel.,,Electronic:978-0-7695-4986-6; POD:978-1-4673-6494-2,10.1109/COMPSAC.2013.143,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6649870,data analytics;data management;data privacy;data security,Data handling;Data integration;Data mining;Data security;Data storage systems;Information management;Medical services,data handling,big data challenges;big data opportunities,,2,,6,,no,22-26 July 2013,,IEEE,IEEE Conference Publications
"BioExtract Server, a Web-based workflow enabling system, leveraging iPlant collaborative resources",C. M. Lushbough; E. Gnimpieba; R. Dooley,"Comput. Sci. Dept., Univ. of South Dakota, Vermillion, SD, USA",2013 IEEE International Conference on Cluster Computing (CLUSTER),20140109,2013,,,1,3,"In order to handle the vast quantities of biological data generated by high-throughput experimental technologies, the BioExtract Server (bioextract.org) has leveraged iPlant Collaborative (www.iplantcollaborative.org) functionality to help address big data storage and analysis issues in the bioinformatics field. The BioExtract Server is a Web-based, workflow-enabling system that offers researchers a flexible environment for analyzing genomic data. It provides researchers with the ability to save a series of BioExtract Server tasks (e.g. query a data source, save a data extract, and execute an analytic tool) as a workflow and the opportunity for researchers to share their data extracts, analytic tools and workflows with collaborators. The iPlant Collaborative is a community of researchers, educators, and students working to enrich science through the development of cyberinfrastructure - the physical computing resources, collaborative environment, virtual machine resources, and interoperable analysis software and data services - that are essential components of modern biology. The iPlant Agave API (Agave), developed through the iPlant Collaborative, is a hosted, Software-as-a-Service resource providing access to a collection of High Performance Computing (HPC) and Cloud resources [6]. Leveraging Agave, the BioExtract Server gives researchers easy access to multiple high performance computers and delivers computation and storage as dynamically allocated resources via the Internet.",1552-5244;15525244,Electronic:978-1-4799-0898-1; POD:978-1-4799-0897-4; USB:978-1-4799-0896-7,10.1109/CLUSTER.2013.6702692,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6702692,BioExtract Server;Bioinformatic workflows;iPlant Data Store;iPlant Foundation API,Europe;Registers;Servers,Internet;application program interfaces;bioinformatics;cloud computing;data analysis;genomics;groupware;parallel processing;resource allocation;storage management;workflow management software,BioExtract server;HPC;Internet;Web-based workflow enabling system;analytic tools;bioextract.org;bioinformatics field;biological data;cloud resources;collaborative environment;cyberinfrastructure;data extracts;data services;data storage;experimental technology;genomic data analysis;high performance computing;iPlant Agave API;iPlant collaborative functionality;iPlant collaborative resources;interoperable analysis software;modern biology;physical computing resources;resource allocation;software-as-a-service resource;virtual machine resources,,0,,8,,no,23-27 Sept. 2013,,IEEE,IEEE Conference Publications
Biological tissues dispersivity and power loss density in transcranial magnetic stimulation,K. Porzig; H. Brauer; H. Toepfer,"Dept. of Adv. Electromagn., Ilmenau Univ. of Technol., Ilmenau, Germany","2013 21st International Conference on Software, Telecommunications and Computer Networks - (SoftCOM 2013)",20131121,2013,,,1,5,"Power loss density and energy converted into heat considering dispersive biological tissue was investigated in the framework of transcranial magnetic stimulation (TMS). The solutions were obtained by applying an analytic method of Eaton [1] and a finite-element method (FEM). A commercial available figure-of-8 coil with a biphasic current pulse operates as the source of excitation. The calculations were performed in the frequency domain by using 1000 complex harmonics at different frequencies in order to reconstruct the transient signal in the excitation coil. The displacement current density was taken into account to provide an accurate estimate of the total energy. The human head was modeled as a homogeneous isotropic dispersive volume conductor consisting of grey matter (GM). The dielectric properties of GM were calculated in dependence on the frequency by means of the Cole-Cole model from Gabriel et al. [2]. The induced electric field as well as the current density are compared against those obtained in the non-dispersive case. The results revealed an increased magnitude of the peak value of the current density by 22.3% compared to the non-dispersive case. However, the induced electric field was not influenced by tissues dispersivity. Finally it was shown that the frequency dependent biological tissue affects the time development of the power loss density but has only minor effects on the total energy converted into heat.",,Electronic:978-953-290-040-8; POD:978-1-4799-1122-6,10.1109/SoftCOM.2013.6671857,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6671857,dispersive material;finite-element method;transcranial magnetic stimulation,,biological tissues;current density;finite element analysis;medical signal processing;signal reconstruction;transcranial magnetic stimulation,Cole-Cole model;Eaton analytic method;FEM;GM;TMS;biological tissues dispersivity;biphasic current pulse;current density;dielectric property;displacement current density;excitation coil;figure-of-8 coil;finite-element method;frequency dependent biological tissue;grey matter;homogeneous isotropic dispersive volume conductor;human head;induced electric field;power loss density;transcranial magnetic stimulation;transient signal reconstruction,,0,,10,,no,18-20 Sept. 2013,,IEEE,IEEE Conference Publications
Blending SQL and NewSQL Approaches: Reference Architectures for Enterprise Big Data Challenges,K. A. Doshi; T. Zhong; Z. Lu; X. Tang; T. Lou; G. Deng,,2013 International Conference on Cyber-Enabled Distributed Computing and Knowledge Discovery,20131219,2013,,,163,170,"As it becomes ever more pervasively engaged in data driven commerce, a modern enterprise becomes increasingly dependent upon reliable and high speed transaction services. At the same time it aspires to capitalize upon large inflows of information to draw timely business insights and improve business results. These two imperatives are frequently in conflict because of the widely divergent strategies that must be pursued: the need to bolster on-line transactional processing generally drives a business towards a small cluster of high-end servers running a mature, ACID compliant, SQL relational database, while high throughput analytics on massive and growing volumes of data favor the selection of very large clusters running non-traditional (NoSQL/NewSQL) databases that employ softer consistency protocols for performance and availability. This paper describes an approach in which the two imperatives are addressed by blending the two types (scale-up and scale-out) of data processing. It breaks down data growth that enterprises experience into three classes-Chronological, Horizontal, and Vertical, and picks out different approaches for blending SQL and NewSQL platforms for each class. To simplify application logic that must comprehend both types of data platforms, the paper describes two new capabilities: (a) a data integrator to quickly sift out updates that happen in an RDBMS and funnel them into a NewSQL database, and (b) extensions to the Hibernate-OGM framework that reduce the programming sophistication required for integrating HBase and Hive back ends with application logic designed for relational front ends. Finally the paper details several instances in which these approaches have been applied in real-world, at a number of software vendors with whom the authors have collaborated on design, implementation and deployment of blended solutions.",,Electronic:978-0-7695-5106-7; POD:978-1-4799-1327-5,10.1109/CyberC.2013.34,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6685675,Big Data;Clustering;Databases;NewSQL;NoSQL;SQL,Companies;Complexity theory;Computer architecture;Databases;Real-time systems;Software,Big Data;SQL;business data processing;data analysis;data integration;relational databases,HBase backends;Hibernate-OGM framework;Hive backends;NewSQL database;RDBMS;SQL;application logic;chronological data growth;data integrator;data processing;enterprise big data challenges;high throughput data analytics;horizontal data growth;on-line transactional processing;reference architectures;relational front ends;software vendors;vertical data growth,,0,,27,,no,10-12 Oct. 2013,,IEEE,IEEE Conference Publications
Branching strategies based on Social Networks,N. Kerzazi,"Department of Research & Development, Payza, Montreal, Canada",2013 1st International Workshop on Release Engineering (RELENG),20130926,2013,,,25,28,"Effective code branching strategy must be adapted to the unique needs of each organization. Teams and workflows organization as well as software architecture should be reflected in the branching strategies to maximize productivity and to minimize development risks. When conceptualized carefully, proper branching structure produces superior results. This paper proposes an analytic approach for adapting structure of branches based on Social Network Analysis to find out Branch-dependencies. The article provides context-based scenarios of successful application of such branching strategies in different situations.",,Electronic:978-1-4673-6441-6; POD:978-1-4673-6442-3,10.1109/RELENG.2013.6607693,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6607693,Branch-Dependency;Branching Structure;Network Matrix;Release Cycles;Release Engineering,Collaboration;Computer bugs;Organizations;Productivity;Social network services;Software;Team working,productivity;software architecture;software management,branch dependencies;branching structure;code branching strategy;development risk minimization;productivity maximization;social network analysis;software architecture,,0,,14,,no,20-20 May 2013,,IEEE,IEEE Conference Publications
Bringing big analytics to the masses,N. Leavitt,Leavitt Communications,Computer,20130124,2013,46,1,20,23,Big data analytics is valuable to many companies but has been too complex and expensive for smaller businesses. This is beginning to change.,0018-9162;00189162,,10.1109/MC.2013.9,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6419709,BigML;CloudVertical;Cloudability;Cloudyn;Continuuity;GoodData;Hadoop;InsightsOne;Newvem;QlickTech;Rackspace;RightScale;Splunk;Suvola;Tableau Software;Tidemark;Uptime Software;WibiData;analytics;big data;cloud computing,,business data processing;data analysis,big data analytics;smaller businesses,,1,,,,no,Jan. 2013,,IEEE,IEEE Journals & Magazines
BugMap: a topographic map of bugs,,,,,2013,,,,,"A large and complex software system could contain a large number of bugs. It is desirable for developers to understand how these bugs are distributed across the system, so they could have a better overview of software quality. In this paper, we describe BugMap, a tool we developed for visualizing large-scale bug location information. Taken source code and bug data as the input, BugMap can display bug localizations on a topographic map. By examining the topographic map, developers can understand how the components and files are affected by bugs. We apply this tool to visualize the distribution of Eclipse bugs across components/files. The results show that our tool is effective for understanding the overall quality status of a large-scale system and for identifying the problematic areas of the system.",,,,http://dl.acm.org/citation.cfm?id=2494582&CFID=696538919&CFTOKEN=83912867,,,,,,,,,,yes,,,,
Building value chain through actionbale benchmarking for sustainability and excellence,S. Dupada; R. K. Gedela; R. C. Aryasri; R. Acharya,Tech Mahindra,2013 2nd International Conference on Information Management in the Knowledge Economy,20141002,2013,,,24,30,"Knowledge scarcity has been, and will continue to be the de-facto pain point to almost every organization globally, more specifically to Information Technology (IT) services organizations. Several initiatives, however, has been underwent in many organizations including knowledge transformation, knowledge re-engineering etc., to support the new business models for sustainability. Unfortunately, the knowledge scarcity journey is neither the beginning nor the end. Therefore creating knowledge based value chains are absolutely critical for organizational sustainability and excellence. One big question remains: how do we create value for existence? Our research reveals that autonomous value creation organizational strategy will fail to deliver the promise if is designed and implemented without data integration and analytics. We, the authors, attempt to leverage our experiences, from in-the-field and research work, and propose Generic Benchmarking Integrated Innovation Framework (GBMIIF), to transform insights-to actions-to results, in making the strategy work. What differentiates this paper from other is, however, to provide process intelligence to make informed decisions using the framework to align organizational objectives. Central to the factors is analytics with research findings.",,Electronic:978-81-920249-7-4; POD:978-1-4673-4535-4,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6915068,Analytics;Benchmarking;Data Management;Human Resource Management;Strategy,Benchmark testing;Context;Investment;Organizations;Recruitment;Software,benchmark testing;knowledge management;organisational aspects;sustainable development;value engineering,GBMIIF;IT service organizations;actionable benchmarking;autonomous value creation organizational strategy;business models;data analytics;data integration;generic benchmarking integrated innovation framework;information technology service organizations;knowledge based value chains;knowledge re-engineering;knowledge scarcity;knowledge transformation;organizational objectives;process intelligence;sustainability,,0,,11,,no,19-20 Dec. 2013,,IEEE,IEEE Conference Publications
Business intelligence solutions in healthcare a case study: Transforming OLTP system to BI solution,O. T. Ali; A. B. Nassif; L. F. Capretz,"Electrical and Computer Engineering, The University of Western Ontario, London, Canada, N6A 5B9",2013 Third International Conference on Communications and Information Technology (ICCIT),20130815,2013,,,209,214,"Healthcare environment is growing to include not only the traditional information systems, but also a business intelligence platform. For executive leaders, consultants, and analysts, there is no longer a need to spend hours in design and develop of typical reports or charts, the entire solution can be completed through using Business Intelligence ‰ÛÏBI‰Ûù software. This paper discusses current state-of-the-art B.I components (tools) and outlines hospitals advances in their businesses by using B.I solutions through focusing on inter-relationship of business needs and the IT technologies. We also present a case study that illustrates of transforming a traditional online transactional processing (OLTP) system towards building an online analytical processing (OLAP) solution.",,Electronic:978-1-4673-5307-6; POD:978-1-4673-5306-9; USB:978-1-4673-5305-2,10.1109/ICCITechnology.2013.6579551,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6579551,business intelligence;data analytics;data mining;data warehouse;decision support;healthcare informatics,Bismuth;Buildings;Business;Data mining;Data warehouses;Databases;Medical services,competitive intelligence;data mining;health care;hospitals;transaction processing,BI software;IT technology;OLAP solution;OLTP system;OLTP system transformation;business intelligence solution;healthcare;online analytical processing solution;online transactional processing system,,0,,15,,no,19-21 June 2013,,IEEE,IEEE Conference Publications
Catching the wave: Big data in the classroom,C. J. Romanowski; R. K. Raj,"Center for Multidiscipl. Studies, Rochester Inst. of Technol., Rochester, NY, USA",2013 IEEE Frontiers in Education Conference (FIE),20131219,2013,,,405,406,"Many diverse domains-in the sciences, engineering, healthcare, and homeland security-have been grappling with the analysis of ‰ÛÏBig Data,‰Ûù which has become shorthand to represent extremely large amounts of diverse types of data. A recent Gartner report predicts that around 4.4 million IT jobs globally will be created by 2015 to support Big Data, with 1.9 million of those jobs in the United States. Therefore, understanding approaches and techniques for handling and analyzing Big Data from diverse domains has become crucial for not only in computing but also engineering students. The mini-workshop will make use of active and collaborative learning exercises to introduce faculty in computer science, software engineering, and other disciplines to concepts and techniques involved in managing and analyzing Big Data. Approaches for incorporating Big Data into the engineering and computing curricula will also be presented.",0190-5848;01905848,Electronic:978-1-4673-5261-1; POD:978-1-4673-5259-8; USB:978-1-4673-5260-4,10.1109/FIE.2013.6684855,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6684855,big data;data analytics;data mining;data-intensive applications,Computer science;Conferences;Data handling;Data mining;Data storage systems;Education;Information management,Big Data;computer science education;data analysis;software engineering,Big Data analysis;Big Data management;collaborative learning;computer science;computing curricula;engineering curricula;engineering students;software engineering,,0,,4,,no,23-26 Oct. 2013,,IEEE,IEEE Conference Publications
Cloud Bank Model Based on AHP Resource Scheduling Strategy Research,H. Li; S. Jia; F. Yang; R. Wang,"Sch. of Software, Yunnan Univ., Kunming, China",2013 International Conference on Cloud Computing and Big Data,20140526,2013,,,493,496,"Under the circumstances of the cloud bank model, this paper studies resource scheduling problem on cloud computing. Considering the the user's needs for optimal allocation of resources and the shortest time to complete the total task, cloud computing resource scheduling of analytic hierarchy process model (AHP) is established. Under this model, conduct simulation experiment through the CloudSim platform. Results show that the AHP algorithm under the model, while protecting users' (quality of service) QoS, is even shorter than the general algorithm in completing the total task time. It is an effective resource scheduling algorithm.",,Electronic:978-1-4799-2830-9; POD:978-1-4799-2831-6,10.1109/CLOUDCOM-ASIA.2013.49,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6821038,AHP model;cloud bank;resource scheduling,Cloud computing;Computational modeling;Processor scheduling;Quality of service;Resource management;Scheduling;Vectors,analytic hierarchy process;cloud computing;quality of service;scheduling,AHP resource scheduling strategy research;CloudSim platform;QoS;analytic hierarchy process model;cloud bank model;cloud computing;optimal allocation;quality of service,,0,,12,,no,16-19 Dec. 2013,,IEEE,IEEE Conference Publications
Cloud Based Big Data Analytics for Smart Future Cities,Z. Khan; A. Anjum; S. L. Kiani,"Univ. of the West of England, Bristol, UK",2013 IEEE/ACM 6th International Conference on Utility and Cloud Computing,20140505,2013,,,381,386,"ICT is becoming increasingly pervasive to urban environments and providing the necessary basis for sustainability and resilience of the smart future cities. Often ICT tools for a smart city deal with different application domains e.g. land use, transport, energy, and rarely provide an integrated information perspective to deal with sustainability and socioeconomic growth of the city. Smart cities can benefit from such information using Big, and often real-time cross-thematic, data collection, processing, integration and sharing through inter-operable services deployed in a Cloud environment. However, such information utilisation requires appropriate software tools, services and technologies to collect, store, analyse and visualise large amounts of data from the city environment, citizens and various departments and agencies at city scale. This paper presents a theoretical perspective on the smart cities focused Big data processing and analysis by proposing a Cloud-based analysis service that can be further developed to generate information intelligence and support decision-making in smart future cities context.",,Electronic:978-0-7695-5152-4; POD:978-1-4799-2574-2,10.1109/UCC.2013.77,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6809436,,Cities and towns;Cloud computing;Data handling;Data storage systems;Decision making;Information management;Smart phones,Big Data;cloud computing;data integration;open systems;socio-economic effects;sustainable development,ICT tools;city environment;cloud-based Big Data analytics;cloud-based analysis service;cross-thematic data;data analysis;data collection;data integration;data processing;data sharing;data storage;data visualisation;decision-making;information intelligence generation;integrated information;interoperable services;pervasive environments;smart future city resilience;smart future city sustainability;socioeconomic growth;software services;software technologies;software tools;sustainability;urban environments,,6,,28,,no,9-12 Dec. 2013,,IEEE,IEEE Conference Publications
Cloud-Based Software Platform for Big Data Analytics in Smart Grids,Y. Simmhan; S. Aman; A. Kumbhare; R. Liu; S. Stevens; Q. Zhou; V. Prasanna,"Univ. of Southern California, Las Angeles, CA, USA",Computing in Science & Engineering,20131114,2013,15,4,38,47,"This article focuses on a scalable software platform for the Smart Grid cyber-physical system using cloud technologies. Dynamic Demand Response (D<sup>2</sup>R) is a challenge-application to perform intelligent demand-side management and relieve peak load in Smart Power Grids. The platform offers an adaptive information integration pipeline for ingesting dynamic data; a secure repository for researchers to share knowledge; scalable machine-learning models trained over massive datasets for agile demand forecasting; and a portal for visualizing consumption patterns, and validated at the University of Southern California's campus microgrid. The article examines the role of clouds and their tradeoffs for use in the Smart Grid Cyber-Physical Sagileystem.",1521-9615;15219615,,10.1109/MCSE.2013.39,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6475927,Dynamic Demand Response;big data analytics;cloud computing;cyber-physical systems;machine learning;scientific computing;smart grid;software platform;stream processing;workflows,Big data;Cloud computing;Data handling;Information management;Microgrids;Optimization;Scientific computing;Smart grids,cloud computing;learning (artificial intelligence);power engineering computing;smart power grids,D<sup>2</sup>R;Smart Grid Cyber-Physical Sagileystem;University of Southern California;agile demand forecasting;big data analytics;campus microgrid;cloud-based software platform;dynamic demand response;information integration pipeline;ingesting dynamic data;intelligent demand-side management;scalable machine-learning models;scalable software platform;smart grid cyber-physical system;smart grids;smart power grids,,35,,12,,no,July-Aug. 2013,,IEEE,IEEE Journals & Magazines
CODEMINE: Building a Software Development Data Analytics Platform at Microsoft,J. Czerwonka; N. Nagappan; W. Schulte; B. Murphy,Microsoft,IEEE Software,20130626,2013,30,4,64,71,"The scale and speed of today's software development efforts impose unprecedented constraints on the pace and quality of decisions made during planning, implementation, and postrelease maintenance and support for software. Decisions during the planning process include level of staffing and choosing a development model given the scope of a project and timelines. Tracking progress, course correcting, and identifying and mitigating risks are key in the development phase, as are monitoring aspects of and improving overall customer satisfaction in the maintenance and support phase. Availability of relevant data can greatly increase both the speed and likelihood of making a decision that leads to a successful software system. This article outlines the process Microsoft has gone through developing CODEMINE--a software development data analytics platform for collecting and analyzing engineering process data&mdash;its constraints, and pivotal organizational and technical choices.",0740-7459;07407459,,10.1109/MS.2013.68,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6509369,and software analytics;code quality;metrics;mining;reliability;software repositories,Analytical models;Computer architecture;Computer bugs;Data analysis;Data models;Software architecture;Software development;Software quality,data analysis;planning;risk management;software maintenance,CODEMINE;Microsoft;course correction;course identification;development model;engineering process data analysis;engineering process data collection;progress tracking;risk mitigation;software development data analytics platform;software implementation;software planning;software postrelease maintenance;staffing level,,13,,8,,yes,July-Aug. 2013,,IEEE,IEEE Journals & Magazines
Collaborative Analytics with Genetic Programming for Workflow Recommendation,C. S. Chong; T. Zhang; K. K. Lee; G. G. Hung; Terence; B. S. Lee,"Inst. of High Performance Comput., A*STAR, Singapore, Singapore","2013 IEEE International Conference on Systems, Man, and Cybernetics",20140127,2013,,,657,662,"Formulation of appropriate data analytics workflows requires intricate knowledge and rich experiences of data analytics experts. This problem is further compounded by continuous advancement and improvement in analytical algorithms. In this paper, a generic non-domain specific solution for the creation of appropriate workflows targeted at supervised learning problems is proposed. Our adaptive workflow recommendation engine based on collaborative analytics matches analytics needs with relevant workflows in repository. It is capable of picking workflows with better performance as compared to randomly selected workflows. The recommendation engine is now augmented by a workflow optimizer that applies genetic programming to further improve the recommended workflows through iterative evolution, leading to better alternative workflows. This unique Collaborative Analytics Recommender System is tested on seven UCI benchmark datasets. It is shown that the final workflows produced by the system could closely approximate, in terms of accuracy, the best workflows that analytics experts could possibly design.",1062-922X;1062922X,Electronic:978-1-4799-0652-9; POD:978-1-4799-0650-5,10.1109/SMC.2013.117,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6721870,Workflow recommendation;collaborative analytics;genetic programming,Accuracy;Benchmark testing;Breast cancer;Classification algorithms;Collaboration;Engines;Training,data analysis;genetic algorithms;groupware;learning (artificial intelligence);recommender systems;workflow management software,UCI benchmark datasets;adaptive workflow recommendation engine;analytical algorithm;collaborative analytics recommender system;data analytics expert;data analytics workflow formulation;generic nondomain specific solution;genetic programming;supervised learning problems;workflow optimizer,,1,,20,,no,13-16 Oct. 2013,,IEEE,IEEE Conference Publications
Compact Wideband Corrugated Feedhorns With Ultra-Low Sidelobes for Very High Performance Antennas and Quasi-Optical Systems,J. E. McKay; D. A. Robertson; P. A. S. Cruickshank; R. I. Hunter; D. R. Bolton; R. J. Wylde; G. M. Smith,"Millimetre Wave and High-Field EPR Group, SUPA, School of Physics and Astronomy, University of St. Andrews, Fife, Scotland",IEEE Transactions on Antennas and Propagation,20130403,2013,61,4,1714,1721,"The corrugated or scalar feedhorn has found many applications in millimeter wave and sub-millimeter wave systems due to its high beam symmetry, relatively low sidelobe levels and strong coupling to the fundamental mode Gaussian beam. However, for applications such as millimeter wave cosmology, space-based experiments, or even high performance imaging, there is a generic requirement to reduce the size of horns whilst maintaining very high levels of performance. In this paper we describe a general analytic methodology for the design of compact dual-profiled corrugated horns with extremely low sidelobe levels. We demonstrate that it is possible to achieve -50 dB sidelobe levels, over wide bandwidths with short horns, which we believe represents state-of-the-art performance. We also demonstrate experimentally a simple scalar design that operates over wide bandwidths and can achieve sidelobes of better than -40 dB, whilst maintaining a frequency independent phase center. This design methodology has been validated experimentally by the successful manufacture and characterization of feedhorns at 94 GHz and 340 GHz for both radar and quasi-optical instrumentation applications.",0018-926X;0018926X,,10.1109/TAP.2013.2243097,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6420894,Beam coupling;Gaussian beams;corrugated feedhorn;quasi-optics;ultra-low sidelobes,Antenna measurements;Apertures;Bandwidth;Couplings;Imaging;Radar imaging;Software,antenna feeds;broadband antennas;horn antennas;millimetre wave antennas,compact dual-profiled corrugated horns;compact wideband corrugated feedhorns;fundamental mode Gaussian beam;millimeter wave;millimeter wave cosmology;quasi-optical instrumentation applications;quasi-optical systems;scalar feedhorn;spacebased experiments;submillimeter wave systems;ultra-low sidelobes;very high performance antennas,,10,,19,,no,13-Apr,,IEEE,IEEE Journals & Magazines
Comparing optimum operation of Pulse Width-Pulse Frequency and Pseudo-Rate modulators in spacecraft attitude control subsystem employing thruster,M. Navabi; H. Rangraz,"New Technology Faculty, Shahid Beheshti University, GC, Iran",2013 6th International Conference on Recent Advances in Space Technologies (RAST),20130815,2013,,,625,630,"The Aim of this paper is to verify which modulator will extend the life time of a spacecraft attitude control system utilizing thrusters. For this purpose, the optimal region of operation for Pulse Width-Pulse Frequency modulator and Pseudo-Rate modulator is defined where they are used as a compensator in the control subsystem. The optimal region is where the modulator behavior is close to linear and simultaneously, fuel consumption and thruster activity is at the lowest. Because of nonlinear nature of the modulator, an analytic approach is difficult so as an alternative, system simulations are carried out. Optimal region of parameters is determined through Static and dynamic analysis. Modulators are compared by employing them in a spacecraft model for minimum fuel use once for same parameters and once for their optimal parameters. Results give valuable information about control system fuel usage and reliability and effects of modulators in control effort. Trustworthiness and accuracy of results are increased by employing nonlinear attitude dynamics in system simulations. Results can be utilized for practical designs in order to select proper modulator for different missions.",,Electronic:978-1-4673-6396-9; POD:978-1-4673-6395-2,10.1109/RAST.2013.6581286,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6581286,Optimum Operation;PRM modulator;PWPF modulator;attitude control subsystem;thruster,Attitude control;Firing;Frequency modulation;Fuels;Nonlinear dynamical systems;Software packages,attitude control;nonlinear control systems;optimal control;space vehicles,compensator;control system fuel usage;dynamic analysis;fuel consumption;nonlinear attitude dynamics;optimum operation;pseudorate modulator;pulse width-pulse frequency modulator;spacecraft attitude control subsystem;static analysis;thruster activity,,0,,6,,no,12-14 June 2013,,IEEE,IEEE Conference Publications
Composing hierarchical stochastic model from SysML for system availability analysis,F. Machida; J. Xiang; K. Tadano; Y. Maeno,"NEC Knowledge Discovery Res. Labs., Kawasaki, Japan",2013 IEEE 24th International Symposium on Software Reliability Engineering (ISSRE),20140102,2013,,,51,60,"Comprehensive analytic model for system availability analysis often confronts the largeness issue where a system designer cannot easily handle the model and the solution is not given in a feasible solution time. Hierarchical decomposition of a large state-space model gives a promising solution to the largeness issue when the model is decomposable. However, the decomposability of analytic model is not always manually tractable especially when the model is generated in an automated manner. In this paper, we propose an automated model composition technique from a system design to a hierarchical stochastic model which is the judicious combination of combinatorial and state-space models. In particular, from SysML-based system specifications, a top-level fault tree and associated stochastic reward nets are automatically generated in hierarchical manner. The obtained hierarchical stochastic model can be solved analytically considerably faster than monolithic state-space models. Through an illustrative example of three-tier web application system on a virtualized infrastructure, the accuracy and efficiency of the solution are evaluated in comparison to a monolithic state space model and a static fault tree.",1071-9458;10719458,Electronic:978-1-4799-2366-3; POD:978-1-5090-4462-7,10.1109/ISSRE.2013.6698904,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6698904,automated model composition;availability analysis;model decomposition;stochastic model;web application system,Analytical models;Availability;Computational modeling;Servers;Software;State-space methods;Unified modeling language,Internet;Unified Modeling Language;combinatorial mathematics;fault trees;formal specification;software reliability;stochastic processes;virtualisation,SysML-based system specifications;analytic model decomposability;associated stochastic reward nets;automated model composition technique;combinatorial model;comprehensive analytic model;hierarchical decomposition;hierarchical stochastic model;system availability analysis;three-tier Web application system;top-level fault tree;virtualized infrastructure,,0,,26,,no,4-7 Nov. 2013,,IEEE,IEEE Conference Publications
Comprehensive Credit Evaluation Model of Electricity Customer Based on the Changing Trend of Credit,W. Du; C. Zhou; B. Zheng; C. Gao; W. Kong; S. Cao,"SGEPRI, Anhui NARI Jiyuan Software Co., Ltd., Hefei, China",2013 International Conference on Information Science and Cloud Computing Companion,20141204,2013,,,536,542,"In order to solve the credit quantification and scoring problem, this paper did some research on credit evaluation model of electricity customers. The credit was divided into 5 first-grade indicators and 15 second-grade indicators. The paper quantified these indicators and evaluated their weights by utilizing Analytic Hierarchy Process model. The indicators were scored by utilizing exponential scoring. According to the weight and scores of each indicator, we calculated the month credit values of the last 12 months. Through these month credit values and their changing trend, the comprehensive credit was calculated. We used a customer sample data to verify and analyze the model. The experimental result shows that the model can evaluate credit indicators reasonably and effectively. Thus, the credit quantification and scoring problem is solved.",,CD-ROM:978-1-4799-1864-5; Electronic:978-1-4799-5245-8; POD:978-1-4799-5246-5,10.1109/ISCC-C.2013.61,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6973647,Analytic Hierarchy Process;changing trend of credit;credit evaluation;credit scoring;exponential scoring,Analytical models;Economics;Electricity;Electronic mail;Market research;Software,analytic hierarchy process;electricity supply industry;finance,analytic hierarchy process;comprehensive credit evaluation model;credit quantification;electricity customer;exponential scoring;scoring problem,,0,,10,,no,7-8 Dec. 2013,,IEEE,IEEE Conference Publications
Constructing Defect Predictors and Communicating the Outcomes to Practitioners,T. Taipale; M. Qvist; B. Turhan,"EB (Elektrobit Wireless Commun. Ltd.), Finland",2013 ACM / IEEE International Symposium on Empirical Software Engineering and Measurement,20131212,2013,,,357,362,"Background: An alternative to expert-based decisions is to take data-driven decisions and software analytics is the key enabler for this evidence-based management approach. Defect prediction is one popular application area of software analytics, however with serious challenges to deploy into practice. Goal: We aim at developing and deploying a defect prediction model for guiding practitioners to focus their activities on the most problematic parts of the software and improve the efficiency of the testing process. Method: We present a pilot study, where we developed a defect prediction model and different modes of information representation of the data and the model outcomes, namely: commit hotness ranking, error probability mapping to the source and visualization of interactions among teams through errors. We also share the challenges and lessons learned in the process. Result: In terms of standard performance measures, the constructed defect prediction model performs similar to those reported in earlier studies, e.g. 80% of errors can be detected by inspecting 30% of the source. However, the feedback from practitioners indicates that such performance figures are not useful to have an impact in their daily work. Pointing out most problematic source files, even isolating error-prone sections within files are regarded as stating the obvious by the practitioners, though the latter is found to be helpful for activities such as refactoring. On the other hand, visualizing the interactions among teams, based on the errors introduced and fixed, turns out to be the most helpful representation as it helps pinpointing communication related issues within and across teams. Conclusion: The constructed predictor can give accurate information about the most error prone parts. Creating practical representations from this data is possible, but takes effort. The error prediction research done in Elektrobit Wireless Ltd is concluded to be useful and we will further improve the present- tions made from the error prediction data.",1949-3770;19493770,Electronic:978-0-7695-5056-5; POD:978-1-4799-1144-8,10.1109/ESEM.2013.45,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6681379,data-driven decisions;error prediction;machine learning algorithms;prediction algorithms;software testing,Accuracy;Data mining;Data models;Error probability;Predictive models;Software;Testing,data visualisation;error statistics;learning (artificial intelligence);program testing;software metrics,data information representation;data-driven decisions;defect predictor construction;error probability mapping;error-prone section isolation;evidence-based management approach;expert-based decisions;hotness ranking;interaction source;interaction visualization;machine learning algorithms;outcome communication;performance measures;software analytics;source files;testing process,,2,,7,,yes,10-11 Oct. 2013,,IEEE,IEEE Conference Publications
Convergence of evolutionary biology and software engineering: Putting practice in action,W. A. Lawrence-Fowler; L. Grabowski; R. H. Fowler; G. Yedid,"Dept. of Comput. Sci., Univ. of Texas-Pan American, Edinburg, TX, USA",2013 IEEE Frontiers in Education Conference (FIE),20131219,2013,,,356,361,"This paper presents a project in experiential learning where students put knowledge of software engineering processes into action in a multidisciplinary project combining computer science and biology. Visualization serves as a primary element to bind the concepts of the two disciplines. Students seeking to further their experience and strengthen their skills in software engineering may choose to complete their senior capstone course working on an ongoing project to construct a toolkit for visualization of phylogenies generated from Avida experimental data. Avida provides a complex computational environment in which the evolution of digital organisms is tracked and analyzed to help find answers to a wide range of research questions. Student projects involve extensions of existing analytic and visualization techniques, as well as the addition of new, often novel, techniques. Importantly, to be successful a visualization technique must be appropriate for the domain in which it is to be used, requiring students to also understand elements of biology. It is our premise that exposing computer science students to the convergence of these two disciplines will strengthen their ability to work at different levels of abstraction and develop new conceptual frameworks to address current and future challenges in hardware and software.",0190-5848;01905848,Electronic:978-1-4673-5261-1; POD:978-1-4673-5259-8; USB:978-1-4673-5260-4,10.1109/FIE.2013.6684847,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6684847,Avida;experiential learning;software engineering;visualization,Computer science;Data visualization;Phylogeny;Software;Software engineering,biology computing;computer aided instruction;computer science education;educational courses;evolution (biological);program visualisation;project management;software engineering,Avida experimental data;analytic techniques;complex computational environment;computer science;digital organisms evolution;evolutionary biology;experiential learning;multidisciplinary project;phylogenies visualization;senior capstone course;software engineering;student skills;toolkit;visualization techniques,,1,,30,,no,23-26 Oct. 2013,,IEEE,IEEE Conference Publications
Criteria for ERP selection using an AHP approach,J. P. Silva; J. J. GonÌ_alves; J. A. Fernandes; M. M. Cunha,"Sch. of Technol., Polytech. Inst. of Cavado, Barcelos, Portugal",2013 8th Iberian Conference on Information Systems and Technologies (CISTI),20131017,2013,,,1,6,"Information systems are a foundation key element of modern organizations. Quite often, chief executive officers and managers have to decide about the acquisition of new software solution based in an appropriated set of criteria. Analytic Hierarchy Process (AHP) is one technique used to support that kind of decisions. This paper proposes the application of AHP method to the selection of ERP (Enterprise Resource Planning) systems, identifying the set of criteria to be used. A set of criteria was retrieved from the scientific literature and validated through a survey-based approach.",2166-0727;21660727,Electronic:978-989-98434-0-0; POD:978-1-4799-1217-9,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6615818,AHP method;ERP selection criteria;Software packages selection,Analytic hierarchy process;Correlation;Organizations;Software packages,analytic hierarchy process;enterprise resource planning;information systems,AHP approach;ERP selection;analytic hierarchy process;enterprise resource planning systems;information systems;software solution;survey-based approach,,0,,30,,no,19-22 June 2013,,IEEE,IEEE Conference Publications
Cultural heritage omni-stereo panoramas for immersive cultural analytics ‰ÛÓ From the Nile to the Hijaz,N. G. Smith; S. Cutchin; R. Kooima; R. A. Ainsworth; D. J. Sandin; J. Schulze; A. Prudhomme; F. Kuester; T. E. Levy; T. A. DeFanti,,2013 8th International Symposium on Image and Signal Processing and Analysis (ISPA),20140109,2013,,,552,557,"The digital imaging acquisition and visualization techniques described here provides a hyper-realistic stereoscopic spherical capture of cultural heritage sites. An automated dualcamera system is used to capture sufficient stereo digital images to cover a sphere or cylinder. The resulting stereo images are projected undistorted in VR systems providing an immersive virtual environment in which researchers can collaboratively study the important textural details of an excavation or historical site. This imaging technique complements existing technologies such as LiDAR or SfM providing more detailed textural information that can be used in conjunction for analysis and visualization. The advantages of this digital imaging technique for cultural heritage can be seen in its non-invasive and rapid capture of heritage sites for documentation, analysis, and immersive visualization. The technique is applied to several significant heritage sites in Luxor, Egypt and Saudi Arabia.",1845-5921;18455921,Electronic:978-953-184-194-8; POD:978-1-4799-3125-5; USB:978-953-184-187-0,10.1109/ISPA.2013.6703802,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6703802,Luxor;Saudi Arabia;Taif;cultural heritage;gigapixel;spherical panoramas;stereo imagery;virtual reality;visualization,Cameras;Cultural differences;Digital images;Image resolution;Software;Stereo image processing;Visualization,data visualisation;history;image sensors;image texture;stereo image processing;virtual reality,Egypt;Hijaz;LiDAR;Luxor;Nile;Saudi Arabia;SfM;VR systems;automated dual-camera system;cultural heritage omni-stereo panoramas;cultural heritage sites;detailed textural information;digital imaging acquisition;digital imaging technique;hyperrealistic stereoscopic spherical capture;immersive cultural analytics;immersive virtual environment;noninvasive capture;rapid capture;stereo digital images;visualization techniques,,3,,19,,no,4-6 Sept. 2013,,IEEE,IEEE Conference Publications
Cura: A Cost-Optimized Model for MapReduce in a Cloud,B. Palanisamy; A. Singh; L. Liu; B. Langston,"Coll. of Comput., Georgia Tech, Atlanta, GA, USA",2013 IEEE 27th International Symposium on Parallel and Distributed Processing,20130729,2013,,,1275,1286,"We propose a new MapReduce cloud service model, Cura, for data analytics in the cloud. We argue that performing MapReduce analytics in existing cloud service models - either using a generic compute cloud or a dedicated MapReduce cloud - is inadequate and inefficient for production workloads. Existing services require users to select a number of complex cluster and job parameters while simultaneously forcing the cloud provider to use those potentially sub-optimal configurations resulting in poor resource utilization and higher cost. In contrast Cura leverages MapReduce profiling to automatically create the best cluster configuration for the jobs so as to obtain a global resource optimization from the provider perspective. Secondly, to better serve modern MapReduce workloads which constitute a large proportion of interactive real-time jobs, Cura uses a unique instant VM allocation technique that reduces response times by up to 65%. Thirdly, our system introduces deadline-awareness which, by delaying execution of certain jobs, allows the cloud provider to optimize its global resource allocation and reduce costs further. Cura also benefits from a number of additional performance enhancements including cost-aware resource provisioning, VMaware scheduling and online virtual machine reconfiguration. Our experimental results using Facebook-like workload traces show that along with response time improvements, our techniques lead to more than 80% reduction in the compute infrastructure cost of the cloud data center.",1530-2075;15302075,Electronic:978-0-7695-4971-2; POD:978-1-4673-6066-1,10.1109/IPDPS.2013.20,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569903,,Adaptation models;Computational modeling;Optimization;Production;Resource management;Schedules;Time factors,cloud computing;computer centres;resource allocation;software cost estimation;virtual machines,Cura;Facebook-like workload;MapReduce analytics;MapReduce cloud service model;MapReduce profiling;MapReduce workload;VM allocation technique;VMaware scheduling;cloud data center;cloud provider;cluster configuration;complex cluster;cost reduction;cost-aware resource provisioning;cost-optimized model;data analytics;deadline-awareness;dedicated MapReduce cloud;generic compute cloud;global resource allocation;global resource optimization;interactive real-time job;job execution;job parameter;online virtual machine reconfiguration;production workload;resource utilization;suboptimal configuration,,5,,46,,no,20-24 May 2013,,IEEE,IEEE Conference Publications
Data stream mining to address big data problems,E. Ì_lmezo€Ùullar€±; €¡. Ar€±; Ì_. F. Ìàelebi; S. ErgÌ_t,"Bilgisayar M&#x00FC;hendisli&#x011F;i B&#x00F6;l&#x00FC;m&#x00FC;, &#x00D6;zye&#x011F;in &#x00DC;niversitesi, &#x0130;stanbul, T&#x00FC;rkiye",2013 21st Signal Processing and Communications Applications Conference (SIU),20130613,2013,,,1,4,"Today, the IT world is trying to cope with ‰ÛÏbig data‰Ûù problems (data volume, velocity, variety, veracity) on the path to obtaining useful information. In this paper, we present implementation details and performance results of realizing ‰ÛÏonline‰Ûù Association Rule Mining (ARM) over big data streams for the first time in the literature. Specifically, we added Apriori and FP-Growth algorithms for stream mining inside an event processing engine, called Esper. Using the system, these two algorithms were compared over LastFM social music site data and by using tumbling windows. The better-performing FP-Growth was selected and used in creation of a real-time rule-based recommendation engine. Our most important findings show that online association rule mining can generate (1) more rules, (2) much faster and more efficiently, and (3) much sooner than offline rule mining. In addition, we have found many interesting and realistic musical preference rules such as ‰ÛÏGeorge Harrison‰àÕBeatles‰Ûù. We hope that our findings can shed light on the design and implementation of other big data analytics systems in the future.",,Electronic:978-1-4673-5563-6; POD:978-1-4673-5562-9,10.1109/SIU.2013.6531483,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6531483,Apriori;Data stream mining;FP-Growth;association rule mining;complex event processing,Association rules;Big data;Engines;Real-time systems;Software;Software algorithms,data analysis;data mining;information retrieval;music;recommender systems;social networking (online),Apriori algorithm;Beatles;Esper;FP-Growth algorithm;George Harrison;IT world;LastFM social music site data;big data analytics system;big data problem;data stream mining;data variety;data velocity;data veracity;data volume;event processing engine;musical preference rules;online ARM;online association rule mining;real-time rule-based recommendation engine;rule generation;tumbling windows,,1,,16,,no,24-26 April 2013,,IEEE,IEEE Conference Publications
Decision metrix: A new approach to voltage quality monitoring on the Irish distribution grid,D. Mongey; K. Niall; A. O. Kelly,"Premium Power Ltd., Dublin, Ireland","Power Engineering Conference (UPEC), 2013 48th International Universities'",20140120,2013,,,1,5,"Distribution Service Operators (DSO's) have historically ensured voltage quality with a range of strategies. However, DSO's now face tough challenges addressing voltage quality due to a number of emerging drivers. Integration of renewable energy into the distribution grid, increased sensitivity of customers to voltage quality and ongoing regulatory developments are increasing the workload of DSO's. This paper introduces Decision Metrix, a vendor neutral software solution developed to address the needs of ESB Networks and other network operators in regards to grid wide performance analytics. Specifically Decision Metrix continuously evaluates the voltage quality performance of bulk supply points and at the point of connection with generators and demand customers. The system is capable of embedding EN 50160 [2] and internal standards in a flexible rules engine for continuous evaluation. Decision Metrix was deployed in ESB Networks (The Irish distribution system operator) in summer 2012. Significant benefits have been seen in relation to automated compliance monitoring, proactive network management, increased awareness and preventative maintenance through risk identification. Our experience shows that an advanced compliance monitoring tool is an essential component for delivering a secure, sustainable and low carbon electrical network of the future.",,Electronic:978-1-4799-3254-2; POD:978-1-4799-3255-9; USB:978-1-4799-3253-5,10.1109/UPEC.2013.6714869,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6714869,Compliance Monitoring;Decision Support;Grid Codes;Power Quality;Voltage Quality,Generators;Market research;Monitoring;Power quality;Standards;Voltage control;Voltage fluctuations,decision support systems;power distribution planning;power engineering computing;power grids;power supply quality;preventive maintenance;risk management,DSO;EN 50160;ESB networks;Irish distribution grid;Irish distribution system operator;automated compliance monitoring;decision metrix;decision support;distribution service operators;increased awareness;low carbon electrical network;network operations;network planning;network regulation;power quality;preventative maintenance;proactive network management;regulatory developments;risk identification;rules engine;vendor neutral software solution;voltage quality monitoring;voltage quality performance evaluation,,0,,17,,no,2-5 Sept. 2013,,IEEE,IEEE Conference Publications
Decision support system of software architect,A. Harchenko; I. Bodnarchuk; I. Halay,"Nat. Aviation Univ., Kiev, Ukraine",2013 IEEE 7th International Conference on Intelligent Data Acquisition and Advanced Computing Systems (IDAACS),20131114,2013,1,,265,269,"Represented system is for supplying of architect with data, knowledge and methods needed to make decisions in the process of software systems (SS) design. It includes two subsystems: first one is subsystem of requirements specification and communication to SS, and second is architecture design subsystem. For requirements specification and communication formalism of basic protocols method and technic of expert pairwise comparisons are used. The selection of architecture decisions is represented as model of multi criteria hierarchic optimization, where modified Analytic Hierarchic Process (AHP) was applied. Main units of the system is implemented as formalized models or software units.",,Electronic:978-1-4799-1429-6; POD:978-1-4799-1425-8,10.1109/IDAACS.2013.6662686,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6662686,decision support system;optimization;requirements to the software system;software system architecture,Computer architecture;Optimization;Protocols;Software architecture;Software systems;Standards,decision making;formal specification;optimisation;software architecture,AHP;SS design;architecture decisions;architecture design subsystem;basic protocols method;communication formalism;decision support system;expert pairwise comparisons;modified analytic hierarchic process;multicriteria hierarchic optimization;requirements specification;software architecture;software system design,,0,,8,,no,12-14 Sept. 2013,,IEEE,IEEE Conference Publications
Delivering optimal real-time manufacturing intelligence,J. Cooley; J. Petrusich,"Concordia University, MBA Program, Portland, OR - USA",2013 Proceedings of PICMET '13: Technology Management in the IT-Driven Services (PICMET),20131021,2013,,,1658,1668,"IT Services can now deliver optimal Real-Time Manufacturing Intelligence (‰ÛÏMI‰Ûù). Enterprise Resource Planning (‰ÛÏERP‰Ûù) has been implemented for years, providing supply chain and business improvement intelligence. Real-Time MI has the opportunity for rapid ROI with enhanced yields, less downtime, and less waste. Much research has shown that people are poor at determining correlations subjectively. Most industry experts only trust computer calculated correlations using statistical models. MI is a concept in the world of process manufacturing - unique for analytically alarming process trends to prevent out of control product problems. Chemical, packaging, pharmaceutical and energy companies have long known that automating data collection and data analysis can lead to improving processes and yields. These companies have developed massive databases that collect detailed measurements from factory automation tools which are used later for off-line analysis. The key to today's MI improvement is to use new IT capabilities to leverage these existing, disparate database silos; provide real-time analysis and intelligence; and identify and correct problems in-line. This can be done without creating, duplicating or installing yet another database. This paper looks at MI and case studies in chemical, pharmaceutical, and packaging process manufacturing and compares the major companies providing software options.",2159-5100;21595100,Electronic:978-1-890843-27-4; POD:978-1-4799-1149-3,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6641576,,Companies;Databases;Manufacturing;Materials requirements planning;Real-time systems;Software,competitive intelligence;correlation methods;data analysis;enterprise resource planning;factory automation;intelligent manufacturing systems;statistical analysis;supply chain management,ERP;IT capabilities;IT services;analytically alarming process;business improvement intelligence;computer calculated correlations;data analysis automation;data collection automation;database silos;enterprise resource planning;factory automation tools;industry experts;off-line analysis;optimal real-time manufacturing intelligence;process manufacturing;real-time MI;real-time analysis;software options;statistical models;supply chain,,0,,32,,no,July 28 2013-Aug. 1 2013,,IEEE,IEEE Conference Publications
Designing Hybrid Architectures for Massive-Scale Graph Analysis,D. Ediger; D. A. Bader,"Georgia Inst. of Technol., Atlanta, GA, USA","2013 IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum",20131031,2013,,,2262,2265,"Turning large volumes of data into actionable knowledge is a top challenge in high performance computing. Our previous work in this area demonstrated algorithmic techniques for massively parallel graph analysis on multithreaded systems. This work led to the development of GraphCT, the first end-to-end graph analytics platform for the Cray XMT and x86-class systems with OpenMP, and STINGER, a high performance, multithreaded, dynamic graph data structure and algorithms. Both of these packages are freely available as open source software. This dissertation research culminates in experimental and analytical techniques to study the marriage of disk-based systems, such as Hadoop, with shared memory-based systems, such as the Cray XMT, for data-intensive applications. David Ediger is a fifth year PhD candidate in Electrical and Computer Engineering.",,Electronic:978-0-7695-4979-8; POD:978-1-4799-1372-5,10.1109/IPDPSW.2013.172,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6651145,,Algorithm design and analysis;Computational modeling;Data models;Data warehouses;Databases;Memory management,application program interfaces;data structures;graph theory;multi-threading;public domain software;shared memory systems;software architecture,Cray XMT;GraphCT;Hadoop;OpenMP;STINGER;disk-based systems;dynamic graph data structure;end-to-end graph analytics platform;high performance computing;high performance data structure;hybrid architecture design;massive-scale graph analysis;massively parallel graph analysis;multithreaded data structure;multithreaded systems;open source software;shared memory-based systems;x86-class systems,,0,,11,,no,20-24 May 2013,,IEEE,IEEE Conference Publications
Developer Dashboards: The Need for Qualitative Analytics,O. Baysal; R. Holmes; M. W. Godfrey,University of Waterloo,IEEE Software,20130626,2013,30,4,46,52,"Prominent technology companies including IBM, Microsoft, and Google have embraced an analytics-driven culture to help improve their decision making. Analytics aim to help practitioners answer questions critical to their projects, such as ""Are we on track to deliver the next release on schedule?"" and ""Of the recent features added, which are the most prone to defects?"" by providing fact-based views about projects. Analytic results are often quantitative in nature, presenting data as graphical dashboards with reports and charts. Although current dashboards are often geared toward project managers, they aren't well suited to help individual developers. Mozilla developer interviews show that developers face challenges maintaining a global understanding of the tasks they're working on and that they desire improved support for situational awareness, a form of qualitative analytics that's difficult to achieve with current quantitative tools. This article motivates the need for qualitative dashboards designed to improve developers' situational awareness by providing task tracking and prioritizing capabilities, presenting insights on the workloads of others, listing individual actions, and providing custom views to help manage workload while performing day-to-day development tasks.",0740-7459;07407459,,10.1109/MS.2013.66,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6509380,developer dashboards;qualitative analytics;qualitative dashboards;situational awareness,Analytical models;Computer bugs;Market research;Software development;Software measurement;Software metrics;Software quality;Software reliability,business data processing;computer graphics;data analysis;project management,Google;IBM;Microsoft;Mozilla;data presentation;decision making;developer dashboard;developer situational awareness;graphical dashboard;project manager;qualitative analytics;quantitative tool,,1,,5,,yes,July-Aug. 2013,,IEEE,IEEE Journals & Magazines
Development and validation of analytic equations of the electromagnetic fields radiated by the elementary dipoles in time domain,B. Zitouna; J. Ben Hadj Slama,"SAGE advanced system in electrical engineer Tunisia, ENISo, National Engineering School of Sousse, University of Sousse, Tunisia","10th International Multi-Conferences on Systems, Signals & Devices 2013 (SSD13)",20130722,2013,,,1,6,"In this paper, we will present the development of the analytic equations and the corresponding calculating code to models and evaluate the electromagnetic field radiated by the elementary dipoles in time domain. Results obtained with analytic equations are compared to those obtained after simulations made with software based on numerical method in the time domain. Then, they are compared to results obtained with analytic and numerical methods working in frequency domain. In this paper, the development of analytic resolution is carried out in order to implement the electromagnetic inverse method in time domain.",,Electronic:978-1-4673-6457-7; POD:978-1-4673-6459-1; USB:978-1-4673-6458-4,10.1109/SSD.2013.6564022,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6564022,Electric dipole;analytic equation;magnetic dipole;time domain,Equations;Frequency-domain analysis;Inverse problems;Magnetic domains;Mathematical model;Time-domain analysis,electromagnetic field theory;method of moments;time-domain analysis,EMC;electromagnetic fields;elementary dipoles;moment method;numerical method;time domain,,4,,14,,no,18-21 March 2013,,IEEE,IEEE Conference Publications
Does SEO Matter? Increasing Classroom Blog Visibility through Search Engine Optimization,S. Zhang; N. Cabage,,2013 46th Hawaii International Conference on System Sciences,20130318,2013,,,1610,1619,"Educators today motivate learning and foster engagement through the use of Web 2.0 software such as classroom blogs. In this study, we discussed the reasons and benefits of moving classroom blogs to the public, and how Search Engine Optimization (SEO) strategies from industry can help increase classroom blog visibility. We proposed an SEO approach and demonstrated how it can be applied to the design, implementation, and analysis classroom blogs in higher education through an empirical study.",1530-1605;15301605,Electronic:978-1-5090-5646-0; POD:978-1-4673-5933-7; USB:978-0-7695-4892-0,10.1109/HICSS.2013.184,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6480034,search engine optimization;web analytics;web development;web optimization,Blogs;Facebook;Google;Internet;Search engines;Software,Web sites;computer aided instruction;search engines,SEO;Web 2.0 software;classroom blog visibility;educators;higher education;search engine optimization,,0,,22,,no,7-10 Jan. 2013,,IEEE,IEEE Conference Publications
"Droid Analytics: A Signature Based Analytic System to Collect, Extract, Analyze and Associate Android Malware",M. Zheng; M. Sun; J. C. S. Lui,"Comput. Sci. & Eng. Dept., Chinese Univ. of Hong Kong, Hong Kong, China","2013 12th IEEE International Conference on Trust, Security and Privacy in Computing and Communications",20131212,2013,,,163,171,"Smartphones and mobile devices are rapidly becoming indispensable devices for many users. Unfortunately, they also become fertile grounds for hackers to deploy malware. There is an urgent need to have a ""security analytic & forensic system"" which can facilitate analysts to examine, dissect, associate and correlate large number of mobile applications. An effective analytic system needs to address the following questions: How to automatically collect and manage a high volume of mobile malware? How to analyze a zero-day suspicious application, and compare or associate it with existing malware families in the database? How to reveal similar malicious logic in various malware, and to quickly identify the new malicious code segment? In this paper, we present the design and implementation of DroidAnalytics, a signature based analytic system to automatically collect, manage, analyze and extract android malware. The system facilitates analysts to retrieve, associate and reveal malicious logics at the ""opcode level"". We demonstrate the efficacy of DroidAnalytics using 150, 368 Android applications, and successfully determine 2, 475 Android malware from 102 different families, with 327 of them being zero-day malware samples from six different families. To the best of our knowledge, this is the first reported case in showing such a large Android malware analysis/detection. The evaluation shows the DroidAnalytics is a valuable tool and is effective in analyzing malware repackaging and mutations.",2324-898X;2324898X,Electronic:978-0-7695-5022-0; POD:978-1-4799-1444-9,10.1109/TrustCom.2013.25,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6680837,Android Malware;Signature;Zero-day,Androids;Databases;Humanoid robots;Malware;Mobile communication;Payloads;Smart phones,digital forensics;digital signatures;invasive software;mobile computing;smart phones,Android malware analysis;Android malware association;Android malware collection;Android malware extraction;DroidAnalytics;forensic system;malicious code segment;malicious logic;malware families;malware mutation;malware repackaging;mobile devices;mobile malware;opcode level;security analytic;signature based analytic system;smartphones;zero-day suspicious application analysis,,10,1,36,,no,16-18 July 2013,,IEEE,IEEE Conference Publications
Dynamic information-theoretic measures for security informatics,R. Colbaugh; K. Glass; T. Bauer,"Sandia National Laboratories, Albuquerque, NM USA",2013 IEEE International Conference on Intelligence and Security Informatics,20130815,2013,,,45,49,"Many important security informatics problems require consideration of dynamical phenomena for their solution; examples include predicting the behavior of individuals in social networks and distinguishing malicious and innocent computer network activities based on activity traces. While information theory offers powerful tools for analyzing dynamical processes, to date the application of information-theoretic methods in security domains has focused on static analyses (e.g., cryptography, natural language processing). This paper leverages information-theoretic concepts and measures to quantify the similarity of pairs of stochastic dynamical systems, and shows that this capability can be used to solve important problems which arise in security applications. We begin by presenting a concise review of the information theory required for our development, and then address two challenging tasks: 1.) characterizing the way influence propagates through social networks, and 2.) distinguishing malware from legitimate software based on the instruction sequences of the disassembled programs. In each application, case studies involving real-world datasets demonstrate that the proposed techniques outperform standard methods.",,Electronic:978-1-4673-6213-9; POD:978-1-4673-6214-6,10.1109/ISI.2013.6578784,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6578784,cyber security;information theory;predictive analytics;security informatics;social network dynamics,Informatics;Information theory;Malware;Markov processes;Social network services;Vehicle dynamics,information theory;invasive software;social networking (online),activity trace;behavior prediction;cryptography;dynamic information-theoretic measure;dynamical process analysis;innocent computer network activity;instruction sequence;legitimate software;malicious computer network activity;malware;natural language processing;security application;security domain;security informatics;social network;static analysis;stochastic dynamical system,,0,,23,,no,4-7 June 2013,,IEEE,IEEE Conference Publications
E-Learning standards and learning analytics. Can data collection be improved by using standard data models?,ÌÅ. del Blanco; ÌÅ. Serrano; M. Freire; I. MartÌ_nez-Ortiz; B. FernÌÁndez-ManjÌ_n,"Complutense University of Madrid, School of Computer Science, Department of Software Engineering and Artificial Intelligence, C Profesor Jos&#x00E9; Garc&#x00ED;a Santesmases sn, 28040, Spain",2013 IEEE Global Engineering Education Conference (EDUCON),20130613,2013,,,1255,1261,"The Learning Analytics (LA) discipline analyzes educational data obtained from student interaction with online resources. Most of the data is collected from Learning Management Systems deployed at established educational institutions. In addition, other learning platforms, most notably Massive Open Online Courses such as Udacity and Coursera or other educational initiatives such as Khan Academy, generate large amounts of data. However, there is no generally agreedupon data model for student interactions. Thus, analysis tools must be tailored to each system's particular data structure, reducing their interoperability and increasing development costs. Some e-Learning standards designed for content interoperability include data models for gathering student performance information. In this paper, we describe how well-known LA tools collect data, which we link to how two e-Learning standards - IEEE Standard for Learning Technology and Experience API - define their data models. From this analysis, we identify the advantages of using these e-Learning standards from the point of view of Learning Analytics.",2165-9559;21659559,Electronic:978-1-4673-6110-1; POD:978-1-4673-6111-8; USB:978-1-4673-6109-5,10.1109/EduCon.2013.6530268,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6530268,Experience API;Learning Analytics;SCORM;e-Learning Standards;educational data mining,Data collection;Data models;Educational institutions;Electronic learning;Interoperability;Least squares approximations;Standards,application program interfaces;computer aided instruction;data models;open systems,Khan academy;LA tools;content interoperability;data collection;e-learning standards;educational data;educational institutions;learning analytics;learning management systems;massive open online courses;online resources;standard data models;student interaction;student performance information,,3,,18,,no,13-15 March 2013,,IEEE,IEEE Conference Publications
"Educational software engineering: Where software engineering, education, and gaming meet",T. Xie; N. Tillmann; J. de Halleux,"Department of Computer Science, North Carolina State University, Raleigh, USA","2013 3rd International Workshop on Games and Software Engineering: Engineering Computer Games to Enable Positive, Progressive Change (GAS)",20131017,2013,,,36,39,"We define and advocate the subfield of educational software engineering (i.e., software engineering for education), which develops software engineering technologies (e.g., software testing and analysis, software analytics) for general educational tasks, going beyond educational tasks for software engineering. In this subfield, gaming technologies often play an important role together with software engineering technologies. We expect that researchers in educational software engineering would be among key players in the education domain and in the coming age of Massive Open Online Courses (MOOCs). Educational software engineering can and will contribute significant solutions to address various critical challenges in education especially MOOCs such as automatic grading, intelligent tutoring, problem generation, and plagiarism detection. In this position paper, we define educational software engineering and illustrate Pex for Fun (in short as Pex4Fun), one of our recent examples on leveraging software engineering and gaming technologies to address educational tasks on teaching and learning programming and software engineering skills.",,Electronic:978-1-4673-6263-4; POD:978-1-4673-6262-7,10.1109/GAS.2013.6632588,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6632588,,Education;Encoding;Games;Programming profession;Software;Software engineering,computer aided instruction;computer games;computer science education;educational courses;software engineering,MOOC;Massive Open Online Courses;Pex for Fun;Pex4Fun;automatic grading;educational software engineering;educational tasks;gaming technologies;intelligent tutoring;plagiarism detection;problem generation;programming learning;programming teaching;software analysis;software analytics;software engineering skills;software engineering technologies;software testing,,6,,24,,no,18-18 May 2013,,IEEE,IEEE Conference Publications
Efficient techniques for relative motion analysis between eccentric orbits under J2 effect,S. Sgubini; G. B. Palmerini,"DIAEE, University of Rome &#x201C;La Sapienza&#x201D;, Via Salaria 851, 00138 Roma, Italy",2013 IEEE Aerospace Conference,20130513,2013,,,1,9,"Accurate orbital propagation is required in order to correctly estimate (in design phase) and carry out (during operations) the control actions needed to maintain relative geometry among formation's spacecraft. Such an accurate propagation could be easily obtained by numerical methods, but their relevant computation cost would neither allow for general trade-offs during design nor match the onboard capabilities once spacecraft are in space. The interest for analytic, closed form solution is clear, as it would save on computation resources and allows for both speed and portability advantages. This paper proposes a special writing of the equations of motion which does provide a closed form solution for orbits including the oblateness effect. Such a representation is far more realistic than the approximate Keplerian one for LEO and medium altitude formations environment, and has indeed a remarkable appeal. The approach, originated from previous literature, is to express the variables of interest (radius, node, inclination, anomaly) as a series, which can be limited to the desired accuracy level in terms of eccentricity. The authors worked on this approach for several years, including currently available symbolic mathematics to allow for exact computation of the parameters of interest at every desired time. The more important contribution is a correct writing of the formulation in terms of relative dynamics, i.e. in terms of differences in the orbital parameters of the platforms, which is actually what is required in the spacecraft formation case. The paper details this special writing of the equation of motion and provides the analytical solution for eccentricities up to 0.2; i.e., remarkably extending the range of orbits previously considered in literature. These solutions are validated with respect to standard numerical propagators that end up with taking orders of magnitude longer to provide the same accuracy. Their quite high efficiency in terms of computational - esources needed make them a suitable solution for inclusion in the onboard software, or a performing option for trade-off analysis during design phase.",1095-323X;1095323X,Electronic:978-1-4673-1813-6; POD:978-1-4673-1812-9,10.1109/AERO.2013.6497186,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6497186,,Orbits,celestial mechanics;space vehicles,J2 effect;Keplerian one;accurate orbital propagation;closed form solution;design phase;eccentric orbit;eccentricity analytical solution;low earth orbit;medium altitude formation environment;motion equation;oblateness effect;relative motion analysis;symbolic mathematics,,1,,9,,no,2-9 March 2013,,IEEE,IEEE Conference Publications
Embedded Analytics and Statistics for Big Data,P. Louridas; C. Ebert,,IEEE Software,20131028,2013,30,6,33,39,"Embedded analytics and statistics for big data have emerged as an important topic across industries. As the volumes of data have increased, software engineers are called to support data analysis and applying some kind of statistics to them. This article provides an overview of tools and libraries for embedded data analytics and statistics, both stand-alone software packages and programming languages with statistical capabilities.",0740-7459;07407459,,10.1109/MS.2013.125,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6648585,big data;embedded analytics;software technology;statistics,Big Data;Data handling;Embedded systems;Information management;Linux;Programming,data analysis;embedded systems;high level languages;software packages;statistical analysis,big data;data volume;embedded data analytics tools;libraries;programming language;stand-alone software package;statistical capability;statistics,,2,,7,,no,Nov.-Dec. 2013,,IEEE,IEEE Journals & Magazines
Enabling Bring-Your-Own-Device using mobile application instrumentation,P. C. Castro; J. W. Ligman; M. Pistoia; J. Ponzo; G. S. Thomas; S. P. Wood; M. Baluda,"IBM Research Division, Thomas J. Watson Research Center, Yorktown Heights, NY, USA",IBM Journal of Research and Development,20131114,2013,57,6,7:01,7:11,"Many enterprises are investigating Bring-Your-Own-Device (BYOD) policies, which allow employees to use their personal devices in the workplace. This has led to mixed-use scenarios, where consumer and enterprise software are installed on the same device. In this paper, we describe the Secured Application Framework for Enterprise (SAFE), a comprehensive system for enabling BYOD that allows enterprise and consumer applications to coexist side-by-side on the device. Rather than partition the device by profiles, SAFE embeds enterprise functions in each enterprise application; this allows for a seamless user experience and minimal intrusiveness on the part of the enterprise. We describe the SAFE toolset that implements the embedding of the SAFE instrumentation layer, and then provide an overview of several enterprise features that can be configured using SAFE. Specifically, we describe modeling for analytics, testing and replay, anomaly detection, and cloud data services, all enterprise features that can transparently be added to mobile applications.",0018-8646;00188646,,10.1147/JRD.2013.2279640,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6665094,,Analytical models;Business;Mobile communication;Runtime;Smart phones;Software;Visualization,,,,1,,,,no,Nov.-Dec. 2013,,IBM,IBM Journals & Magazines
Enabling comprehensive data-driven system management for large computational facilities,J. C. Browne; R. L. DeLeon; C. D. Lu; M. D. Jones; S. M. Gallo; A. Ghadersohi; A. K. Patra; W. L. Barth; J. Hammond; T. R. Furlani; R. T. McLay,"Center for Comput. Res., SUNY at Buffalo, Buffalo, NY, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,11,"This paper presents a tool chain, based on the open source tool TACC_Stats, for systematic and comprehensive job level resource use measurement for large cluster computers, and its incorporation into XDMoD, a reporting and analytics framework for resource management that targets meeting the information needs of users, application developers, systems administrators, systems management and funding managers. Accounting, scheduler and event logs are integrated with system performance data from TACC_Stats. TACC_Stats periodically records resource use including many hardware counters for each job running on each node. Furthermore, system level metrics are obtained through aggregation of the node (job) level data. Analysis of this data generates many types of standard and custom reports and even a limited predictive capability that has not previously been available for open-source, Linux-based software systems. This paper presents case studies of information that can be applied for effective resource management. We believe this system to be the first fully comprehensive system for supporting the information needs of all stakeholders in open-source software based HPC systems.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503230,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877519,,Abstracts;Bandwidth;Market research;Performance evaluation;Servers;Sockets;Standards,Linux;public domain software;resource allocation;workstation clusters,Linux-based software systems;TACC_Stats;XDMoD;cluster computers;computational facilities;data-driven system management;funding managers;job level resource;open source tool;open-source software based HPC systems;resource management;system administrators;system level metrics;system management;systematic resource;tool chain,,4,,28,,no,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Enabling Distributed Key-Value Stores with Low Latency-Impact Snapshot Support,J. Polo; Y. Becerra; D. Carrera; J. Torres; E. AyguadÌ©; M. Spreitzer; M. Steinder,"Barcelona Supercomput. Center (BSC), Tech. Univ. of Catalonia, Barcelona, Spain",2013 IEEE 12th International Symposium on Network Computing and Applications,20131010,2013,,,65,72,"Current distributed key-value stores generally provide greater scalability at the expense of weaker consistency and isolation. However, additional isolation support is becoming increasingly important in the environments in which these stores are deployed, where different kinds of applications with different needs are executed, from transactional workloads to data analytics. While fully-fledged ACID support may not be feasible, it is still possible to take advantage of the design of these data stores, which often include the notion of multiversion concurrency control, to enable them with additional features at a much lower performance cost and maintaining its scalability and availability. In this paper we explore the effects that additional consistency guarantees and isolation capabilities may have on a state of the art key-value store: Apache Cassandra. We propose and implement a new multiversioned isolation level that provides stronger guarantees without compromising Cassandra's scalability and availability. As shown in our experiments, our version of Cassandra allows Snapshot Isolation-like transactions, preserving the overall performance and scalability of the system.",,Electronic:978-0-7695-5043-5; POD:978-1-4799-0605-5,10.1109/NCA.2013.42,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6623643,Cassandra;Data store;Distributed;Isolation;Key-value;Latency;Snapshot,Compaction;Data models;Distributed databases;Peer-to-peer computing;Scalability;Throughput,concurrency control;data integrity;distributed databases;distributed processing;public domain software,Apache Cassandra;Cassandra's availability;Cassandra's scalability;consistency guarantees;data analytics;data stores;distributed key-value stores;fully-fledged ACID support;isolation capabilities;low latency-impact snapshot support;multiversion concurrency control;multiversioned isolation level;performance cost;snapshot isolation-like transactions;transactional workloads,,0,,16,,no,22-24 Aug. 2013,,IEEE,IEEE Conference Publications
Engineering High-Performance Community Detection Heuristics for Massive Graphs,C. L. Staudt; H. Meyerhenke,"Inst. of Theor. Inf., KarInstitute of Theoretical Informaticslsruhe Inst. of Technol. (KIT), Karlsruhe, Germany",2013 42nd International Conference on Parallel Processing,20131219,2013,,,180,189,"The amount of graph-structured data has recently experienced an enormous growth in many applications. To transform such data into useful information, high-performance analytics algorithms and software tools are necessary. One common graph analytics kernel is community detection (or graph clustering). Despite extensive research on heuristic solvers for this task, only few parallel codes exist, although parallelism is often necessary to scale to the data volume of real-world applications. We address the deficit in computing capability by a flexible and extensible clustering algorithm framework with shared-memory parallelism. Within this framework we implement our parallel variations of known sequential algorithms and combine them by an ensemble approach. In extensive experiments driven by the algorithm engineering paradigm, we identify the most successful parameters and combinations of these algorithms. The processing rate of our fastest algorithm exceeds 10M edges/second for many large graphs, making it suitable for massive data streams. Moreover, the strongest algorithm we developed yields a very good tradeoff between quality and speed.",0190-3918;01903918,Electronic:978-0-7695-5117-3; POD:978-1-4799-1448-7,10.1109/ICPP.2013.27,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6687351,Community detection;graph clustering;high-performance network analysis;parallel algorithm engineering,Algorithm design and analysis;Benchmark testing;Clustering algorithms;Communities;Graphics processing units;Linear programming;Parallel processing,data handling;graph theory;parallel processing;pattern clustering;performance evaluation,data volume;engineering high-performance community detection heuristics;graph analytics kernel;graph clustering;graph structured data;massive graphs;parallel codes;parallel variations;real-world applications;shared-memory parallelism;software tools,,18,,22,,no,1-4 Oct. 2013,,IEEE,IEEE Conference Publications
Entropic Inequalities and Marginal Problems,T. Fritz; R. Chaves,"Inst. de Cienc. Fotoniques, Barcelona, Spain",IEEE Transactions on Information Theory,20130116,2013,59,2,803,817,"A marginal problem asks whether a given family of marginal distributions for some set of random variables arises from some joint distribution of these variables. Here, we point out that the existence of such a joint distribution imposes nontrivial conditions already on the level of Shannon entropies of the given marginals. These entropic inequalities are necessary (but not sufficient) criteria for the existence of a joint distribution. For every marginal problem, a list of such Shannon-type entropic inequalities can be calculated by Fourier-Motzkin elimination, and we offer a software interface to a Fourier-Motzkin solver for doing so. For the case that the hypergraph of given marginals is a cycle graph, we provide a complete analytic solution to the problem of classifying all relevant entropic inequalities, and use this result to bound the decay of correlations in stochastic processes. Furthermore, we show that Shannon-type inequalities for differential entropies are not relevant for continuous-variable marginal problems; non-Shannon-type inequalities are both in the discrete and in the continuous case. In contrast to other approaches, our general framework easily adapts to situations where one has additional (conditional) independence requirements on the joint distribution, as in the case of graphical models. We end with a list of open problems. A complementary article discusses applications to quantum nonlocality and contextuality.",0018-9448;00189448,,10.1109/TIT.2012.2222863,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6336823,Entropic inequalities;marginal problem;polymatroids;quantum contextuality,Context;Context modeling;Databases;Entropy;Joints;Probability distribution;Random variables,correlation theory;entropy;graph theory;stochastic processes,Fourier-Motzkin elimination;Fourier-Motzkin solver;Shannon-type entropic inequalities;continuous-variable marginal problems;correlations decay;cycle graph;hypergraph;joint distribution;marginal distributions;nonShannon-type inequalities;open problems;quantum contextuality;quantum nonlocality;random variables;stochastic process,,22,,64,,no,Feb. 2013,,IEEE,IEEE Journals & Magazines
Equivalent radius analytic formulas of Substrate Integrated Cylindrical Cavity,X. z. Luan; K. j. Tan,"Sch. of Inf. Sci. & Technol., Dalian Maritime Univ., Dalian, China",2013 Proceedings of the International Symposium on Antennas & Propagation,20140123,2013,2,,746,749,"Equivalent radius of a Substrate Integrated Cylindrical Cavity (SICC) is an important parameter for calculating the resonant frequency and designing a SICC device. In this paper, based on two sets of equivalent width analytic formulas of Substrate Integrated Waveguide (SIW), the equivalent radius analytic formulas of SICC were derived by the conformal transformation method for the first time. In order to verify the validity of these formulas, using the formulas given in this paper calculated the equivalent radii and the corresponding resonant frequencies of TM<sub>010</sub> mode SICCs, and the resonant frequencies of TM<sub>010</sub> mode SICCs were also calculated by the method introduced in [12] and simulated by electromagnetic simulation software. The results show that the formulas given in this paper have higher precision, and are convenient, suitable to more application fields, so they will play important role in the analysis and design of SICC devices.",,Electronic:978-7-5641-4279-7; POD:978-1-4799-0921-6,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6717586,conformal transformation method;equivalent radius;resonant frequency;substrate integrated cylindrical cavity,Cavity resonators;Microwave filters;Rectangular waveguides;Resonant frequency;Solids;Substrates,circular waveguides;substrate integrated waveguides,SIW;TM<sub>010</sub> mode SICC;conformal transformation method;electromagnetic simulation software;equivalent radius analytic formula;resonant frequency;substrate integrated cylindrical cavity;substrate integrated waveguide,,0,,12,,no,23-25 Oct. 2013,,IEEE,IEEE Conference Publications
ERP Effort Estimation Based on Expert Judgments,I. P. Erasmus; M. Daneva,"Univ. of Twente & SAP B.V., Enschede, Netherlands",2013 Joint Conference of the 23rd International Workshop on Software Measurement and the 8th International Conference on Software Process and Product Measurement,20140102,2013,,,104,109,"A new technology shift brings to the ERP domain a change in the industry and a new platform build on in-memory optimized databases, introduced and known as SAP HANA [1]. This technology shift in the ERP domain led to SAP's ERP on HANA, the solution where the ERP suite is offered on the same platform as ERP Services such as Business Analytics. The integration of ERP Services and the ERP suite brings to the industry new opportunities to ""fine tune"" customer and industry specific business processes. This radical shift in innovation brings with it new challenges in terms of ERP effort estimation. No longer can we rely on a single method such as functional size measurement methods, due to the wide range of customization possibilities. This shift from a typical predefined solution scope to a highly customizable landscape poses a challenge to project estimation practitioners as the functional size estimation techniques used in the past for ERP solutions address a fixed scope deployable in multiple landscapes, and hence are no longer suitable for dynamically definable scope. Today's highly volatile and customized ERP landscape demands a new approach to estimate effort by leveraging ERP professionals' tacit knowledge and expert judgments. This paper presents the ERP Service estimation method that leverages the strengths of expert-judgment-based estimation techniques while using a more structured approach to reduce the effects of expert bias and avoid common pitfalls associated with judgment-based estimation.",,Electronic:978-0-7695-5078-7; POD:978-1-4799-1478-4,10.1109/IWSM-Mensura.2013.25,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693229,ERP Services;ERP effort estimation;Estimation;Expert judgement;Expert judgement tactics;Experts consciousness;SAP ERP suite on HANA;Stakeholders involvement,Business;Educational institutions;Estimation;Industries;Radiation detectors;Software;Uncertainty,enterprise resource planning,ERP domain;ERP effort estimation;ERP service estimation method;ERP services;ERP suite;SAP HANA platform;business analytics;enterprise resource planning;expert-judgment-based estimation techniques;functional size measurement methods;in-memory optimized databases;innovation shift,,0,,32,,no,23-26 Oct. 2013,,IEEE,IEEE Conference Publications
Evaluating Security of Software Components Using Analytic Network Process,S. Nazir; S. Shahzad; M. Nazir; H. u. Rehman,"Dept. of Comput. Sci., Univ. of Peshawar, Peshawar, Pakistan",2013 11th International Conference on Frontiers of Information Technology,20140123,2013,,,183,188,"Increasing use of Component Based Software Engineering (CBSE) has raised the issues related with the security of software components. Several methodologies are being used to evaluate security of software components and that of the base system with which it is integrated. Security characteristics of a component must be specified effectively and unambiguously. To make possible software development progression, it will be effective to have a method which evaluates the security of software components. The study presented here attempts to propose analytic network process (ANP) for component security evaluation. The method is applied using ISO/IEC 27002 (ISO 27002) standard.",,Electronic:978-1-4799-2503-2; POD:978-1-4799-2700-5,10.1109/FIT.2013.41,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6717250,ANP;security of component,IEC standards;ISO standards;Security;Software engineering;Software systems,IEC standards;ISO standards;object-oriented programming;security of data;software engineering;software standards,ANP;CBSE;ISO/IEC 27002 standard;analytic network process;component based software engineering;component security evaluation;software components;software development progression,,0,,13,,no,16-18 Dec. 2013,,IEEE,IEEE Conference Publications
Evaluating the credibility of the C3I information network simulation based on fuzzy AHP,Zhiyong Lu; Yijun Zhang; Yongqiang Bai; Zhoujie Yan,"LEETC, Luoyang, China",2013 IEEE 4th International Conference on Software Engineering and Service Science,20130930,2013,,,1041,1044,"The evaluation of a simulation model's credibility is a hot issue in present research of simulation. Furthermore, the credibility evaluation on the information network simulation of the command_control equipment is an important part to build and evaluate the information network's simulation model through simulation experiment. also a critical challenge for us to evaluate the credibility of command-control equipment information network simulation. On the basis of the analysis of the command-control equipment information network's system structure, this paper establishes a half-real simulation model of a command-control equipment network, employs the Analytical Hierarchy Process to build up the indicator set that affect the evaluation of the simulation credibility, and further figures out the weight of each indicator, then upbuilds a grey model of evaluation on simulation's credibility on the ground of putting forward positive, negative ideal comparatively standard, to make a synthetic evaluation and analysis of its simulation credibility. Finally, the method's feasibility and validity are verified by citing and analyzing some practical examples.",2327-0586;23270586,Electronic:978-1-4673-5000-6; POD:978-1-4673-4999-4,10.1109/ICSESS.2013.6615485,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6615485,Analytical Hierarchy Process;Assessment;C3I;credibility;information network;simulation,Switches,analytic hierarchy process;information networks;security of data,analytical hierarchy process;credibility of command-control equipment information network simulation;fuzzy AHP-based C<sup>3</sup>I information network simulation;grey model;half-real simulation model;indlcator set;positive-negative ideal comparatively standard;simulation experiment;simulation model's credibility,,0,,9,,no,23-25 May 2013,,IEEE,IEEE Conference Publications
Evaluating the Trust of Android Applications through an Adaptive and Distributed Multi-criteria Approach,G. Dini; F. Martinelli; I. Matteucci; M. Petrocchi; A. Saracino; D. Sgandurra,"Dipt. di Ing. dell'Inf., Univ. di Pisa, Pisa, Italy","2013 12th IEEE International Conference on Trust, Security and Privacy in Computing and Communications",20131212,2013,,,1541,1546,"New generation mobile devices, and their app stores, lack of a methodology to associate a level of trust to applications to faithfully represent their potential security risks. This problem is even more critical with newly published applications, for which either user reviews are missing or the number of downloads is still low. In this scenario, users may not fully estimate the risk associated with downloading apps found on on-line stores. Hence, here we propose a methodology for evaluating the trust level of an application through an adaptive, flexible, and dynamic framework. The evaluation of an application trust is performed using both static and dynamic parameters, which consider the application meta-data, its run-time behavior and the reports of users with respect to the software critical operations. We have validated the proposed approach by testing it on more than 180 real applications found both on official and unofficial markets by showing that it correctly categorizes applications as trusted or untrusted in 94% of the cases and it is resilient to poisoning attacks.",2324-898X;2324898X,Electronic:978-0-7695-5022-0; POD:978-1-4799-1444-9,10.1109/TrustCom.2013.189,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6681013,,Analytic hierarchy process;Androids;Batteries;Computer bugs;Humanoid robots;Security;Servers,distributed processing;mobile computing;trusted computing,android applications;app stores;distributed multicriteria approach;downloading application;mobile devices;software critical operations,,2,,14,,no,16-18 July 2013,,IEEE,IEEE Conference Publications
Evaluation model of TPL provider of agricultural products basing on Analytic Network Process,Y. Huaizhen; X. Dongmei; L. Lei,"Sch. of Bus., Guilin Univ. of Electron. Technol., Guilin, China","2013 6th International Conference on Information Management, Innovation Management and Industrial Engineering",20140109,2013,3,,14,19,"Basing on the present situation and exiting problems, and considering the basic characteristics such as independence, seasonality and regionalism of agricultural logistics in China, this paper set up an evaluation index system, and then construct a multi-attribute comprehensive decision model on Third-Part Logistics (TPL) provider of agricultural products. Finally, with the application of Super Decisions Software, this research offers a case study with ANP method to verify the correctness of the model, and evaluates some different programs. The ultimate results indicate that the model is efficacious on the evaluation of TPL providers based on the practical situation of the TPL enterprises.",2155-1456;21551456,Electronic:978-1-4799-0245-3; POD:978-1-4799-0243-9,10.1109/ICIII.2013.6703534,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6703534,Analytic Network Process;agricultural products logistics;index system;program evaluation;third party logistics,Agricultural products;Companies;Decision making;Indexes;Logistics;Packaging;Software,agricultural products;analytic hierarchy process;decision support systems;logistics;production engineering computing;service industries,ANP method;China;TPL provider;agricultural logistics;agricultural products;analytic network process;evaluation index system;multiattribute comprehensive decision model;super decisions software;third party logistics provider,,0,,16,,no,23-24 Nov. 2013,,IEEE,IEEE Conference Publications
Evaluation of NEMP vulnerability on cots electronic equipments,L. Labarbe,"CEA/DAM, Gramat, F-46500 Gramat, France",2013 International Conference on Electromagnetics in Advanced Applications (ICEAA),20131017,2013,,,102,105,"This study deals with a new evaluation method of NEMP (Nuclear ElectroMagnetic Pulse) vulnerability on cots electronic equipments. The method consists in comparing EMC (ElectroMagnetic Compatibility) test severities to NEMP conducted stresses. The comparison uses five characteristic criteria of the induced stresses, calculated with an analytic method. The process is based on a software named ‰ÛÏSUSIE‰Ûù (in French: SUSceptibiliteíÅ aíÛ l'IEMN ‰ä» Impulsion eíÅlectroMagneíÅtique d'origine NucleíÅaire Haute Altitude ‰äÇ). Today, only conducted stresses are analyzed. An evolution is planned in 2013 to determine the equivalent vulnerability of radiated stresses and to validate the software.",,Electronic:978-1-4673-5707-4; POD:978-1-4673-5706-7,10.1109/ICEAA.2013.6632197,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6632197,,Databases;Electromagnetics;Electronic equipment;Immunity testing;Software;Stress,electromagnetic compatibility;electromagnetic pulse;electronic equipment testing,EMC;NEMP conducted stresses;NEMP vulnerability;SUSIE software;cots electronic equipments;electromagnetic compatibility;nuclear electromagnetic pulse vulnerability;susceptibilite a liemn impulsion electromagnetique d'origine nucleaire haute altitude,,0,,2,,no,9-13 Sept. 2013,,IEEE,IEEE Conference Publications
Exploring big data in small forms: A multi-layered knowledge extraction of social networks,Y. W. Zhao; W. J. van den Heuvel; X. Ye,"Sch. of Software, Tsinghua Univ., Beijing, China",2013 IEEE International Conference on Big Data,20131223,2013,,,60,67,"Big data poses great challenges for social network analysts in both the data volume and the latent dimensions hidden in the unstructured data. In this paper, we propose a comprehensive knowledge extraction approach for social networks to guide latent dimensions analysis. An improved hypergraph model of social behaviors was then proposed for conveniently conducting multi-faceted analytics in relationships inherent to social media. A real life case study based on Twitter's data was also presented to illustrate the multi-dimensional relations between users based on the categories they co-join and the tweets they co-spread with three orthogonal dimensions of affect analyzed simultaneously, i.e. valence, activation, and intention.",,Electronic:978-1-4799-1293-3; POD:978-1-4799-1294-0,10.1109/BigData.2013.6691784,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6691784,behavior informatics;big data;knowledge extraction;social networks,Educational institutions;Entropy;Frequency measurement;Knowledge engineering;Semantics;Twitter,Big Data;data analysis;graph theory;knowledge acquisition;social networking (online),Twitter;activation dimension;big data;comprehensive knowledge extraction approach;data volume;hypergraph model;intention dimension;latent dimensions analysis;multidimensional relations;multifaceted analytics;multilayered knowledge extraction;orthogonal dimensions;social networks;valence dimension,,0,,39,,no,6-9 Oct. 2013,,IEEE,IEEE Conference Publications
Exploring energy and performance behaviors of data-intensive scientific workflows on systems with deep memory hierarchies,M. Gamell; I. Rodero; M. Parashar; S. Poole,"NSF Cloud & Autonomic Comput. Center, Rutgers Univ., Piscataway, NJ, USA",20th Annual International Conference on High Performance Computing,20140417,2013,,,226,235,"The increasing gap between the rate at which large scale scientific simulations generate data and the corresponding storage speeds and capacities is leading to more complex system architectures with deep memory hierarchies. Advances in non-volatile memory (NVRAM) technology have made it an attractive candidate as intermediate storage in this memory hierarchy to address the latency and performance gap between main memory and disk storage. As a result, it is important to understand and model its energy/performance behavior from an application perspective as well as how it can be effectively used for staging data within an application workflow. In this paper, we target a NVRAM-based deep memory hierarchy and explore its potential for supporting in-situ/in-transit data analytics pipelines that are part of application workflows patterns. Specifically, we model the memory hierarchy and experimentally explore energy/performance behaviors of different data management strategies and data exchange patterns, as well as the tradeoffs associated with data placement, data movement and data processing.",1094-7256;10947256,Electronic:978-1-4799-0730-4; POD:978-1-4799-0728-1,10.1109/HiPC.2013.6799122,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6799122,,Analytical models;Bandwidth;Benchmark testing;Data models;Hard disks;Nonvolatile memory;Random access memory,data analysis;random-access storage;storage management;workflow management software,NVRAM technology;data exchange patterns;data management strategies;data movement;data placement;data processing;data-intensive scientific workflows;deep memory hierarchies;disk storage;energy behavior;in-situ data analytics pipelines;in-transit data analytics pipelines;nonvolatile memory;performance behavior;storage capacity;storage speed,,5,,36,,no,18-21 Dec. 2013,,IEEE,IEEE Conference Publications
Failure data analytics to build failure prediction mechanisms,S. Dey; K. K. Jacob; J. A. Lopez; K. Trivedi,,2013 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW),20131219,2013,,,102,103,"With ever-growing complexity of computer systems, proactive failure management is turning out to be an effective and essential approach for enhancing availability. Several techniques have been proposed to develop failure prediction models [3]. In this paper we have concentrated on the process to build up a failure prediction model based on the failure reports (service ticket logs) from hardware storage devices.",,Electronic:978-1-4799-2552-0; POD:978-1-4799-2553-7,10.1109/ISSREW.2013.6688883,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6688883,,Analytical models;Computational modeling;Computers;Hardware;History;Maintenance engineering;Predictive models,data analysis;fault tolerant computing;system recovery,availability enhancement;computer system complexity;failure data analytics;failure prediction mechanisms;failure reports;hardware storage devices;proactive failure management;service ticket logs,,0,,4,,no,4-7 Nov. 2013,,IEEE,IEEE Conference Publications
FlexIO: I/O Middleware for Location-Flexible Scientific Data Analytics,F. Zheng; H. Zou; G. Eisenhauer; K. Schwan; M. Wolf; J. Dayal; T. A. Nguyen; J. Cao; H. Abbasi; S. Klasky; N. Podhorszki; H. Yu,"Georgia Inst. of Technol., Atlanta, GA, USA",2013 IEEE 27th International Symposium on Parallel and Distributed Processing,20130729,2013,,,320,331,"Increasingly severe I/O bottlenecks on High-End Computing machines are prompting scientists to process simulation output data online while simulations are running and before storing data on disk. There are several options to place data analytics along the I/O path: on compute nodes, on separate nodes dedicated to analytics, or after data is stored on persistent storage. Since different placements have different impact on performance and cost, there is a consequent need for flexibility in the location of data analytics. The FlexIO middleware described in this paper makes it easy for scientists to obtain such flexibility, by offering simple abstractions and diverse data movement methods to couple simulation with analytics. Various placement policies can be built on top of FlexIO to exploit the trade-offs in performing analytics at different levels of the I/O hierarchy. Experimental results demonstrate that FlexIO can support a variety of simulation and analytics workloads at large scale through flexible placement options, efficient data movement, and dynamic deployment of data manipulation functionalities.",1530-2075;15302075,Electronic:978-0-7695-4971-2; POD:978-1-4673-6066-1,10.1109/IPDPS.2013.46,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569822,Flexibility;I/O;In Situ Data Analytics;Placemen,Analytical models;Arrays;Computational modeling;Data models;Monitoring;Runtime;Software,data analysis;input-output programs;middleware;parallel machines;storage management,FlexIO middleware;I/O bottleneck;I/O hierarchy;I/O path;data storage;disk;diverse data movement method;dynamic data manipulation functionality deployment;flexible data placement policy;high end computing machine;location flexible scientific data analytics;online simulation data processing;performing analytics;persistent storage,,6,,54,,no,20-24 May 2013,,IEEE,IEEE Conference Publications
Front Cover,,,IEEE Software,20130905,2013,30,5,c1,c1,September/October 2013 IEEE Software: The Many Faces of Software Analytics,0740-7459;07407459,,10.1109/MS.2013.104,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6588532,software analytics,,,,,0,,,,no,Sept.-Oct. 2013,,IEEE,IEEE Journals & Magazines
Front Cover,,,IEEE Software,20130626,2013,30,4,c1,c1,July/August 2013 IEEE Software: Software Analytics: So What?,0740-7459;07407459,,10.1109/MS.2013.75,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6547616,software analytics,,,,,0,,,,no,July-Aug. 2013,,IEEE,IEEE Journals & Magazines
"Game Analytics for Game User Research, Part 1: A Workshop Review and Case Study",M. S. El-Nasr; H. Desurvire; B. Aghabeigi; A. Drachen,Northeastern University,IEEE Computer Graphics and Applications,20130320,2013,33,2,6,11,"The emerging field of game user research (GUR) investigates interaction between players and games and the surrounding context of play. Game user researchers have explored methods from, for example, human-computer interaction, psychology, interaction design, media studies, and the social sciences. They've extended and modified these methods for different types of digital games, such as social games, casual games, and serious games. This article focuses on quantitative analytics of in-game behavioral user data and its emergent use by the GUR community. The article outlines open problems emerging from several GUR workshops. In addition, a case study of a current collaboration between researchers and a game company demonstrates game analytics' use and benefits.",0272-1716;02721716,,10.1109/MCG.2013.26,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6482531,Analytical models;GUR;Game theory;Games;Human computer interaction;Software development;User centered design;User interfaces;computer games;computer graphics;game analytics;game user research;human-computer interaction;software development;visual analytics,Analytical models;Game theory;Games;Human computer interaction;Software development;User centered design;User interfaces,computer games;human computer interaction;interactive systems;psychology;social sciences,GUR community;GUR workshops;casual games;digital games;game analytics;game company;game user research;game-player interaction;human-computer interaction;in-game behavioral user data quantitative analytics;interaction design;media studies;psychology;serious games;social games;social sciences,,2,,9,,no,March-April 2013,,IEEE,IEEE Journals & Magazines
General chairs and keynote speakers,A. P. Karduck; M. W. Condry; J. Cheng; L. A. Zadeh; T. Dillon; E. Damiani; H. Pirahesh; C. Wagner; R. Pieraccini; O. S. Tumer; M. Cayley; D. Patrick,"Furtwangen Univ., Furtwangen, Germany",2013 7th IEEE International Conference on Digital Ecosystems and Technologies (DEST),20130926,2013,,,1,9,"These tutorials/keynote speeches: Z-numbers, a new direction in the analysis of uncertain and complex systems; human space computing and cyber-physical systems; digital ecosystems, the resources for future humanity and society; massive data analytics for smart planet; social media for sustained digital ecosystems; new era of civilization, technology understand human and human; SAP co-innovation, envision the future, crossroots innovation; prediction markets, virtual currencies and social scores applied and technology innovation for networked life.",2150-4938;21504938,Electronic:978-1-4799-0786-1; POD:978-1-4799-0785-4,10.1109/DEST.2013.6611306,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6611306,,,innovation management;social aspects of automation;social sciences computing,Z-numbers;complex systems;crossroots innovation;cyber-physical systems;digital ecosystems;human space computing;massive data analytics;networked life;prediction markets;smart planet;social media;social scores;sustained digital ecosystems;technology innovation;uncertain systems;virtual currencies,,0,,,,no,24-26 July 2013,,IEEE,IEEE Conference Publications
Generating Erroneous Human Behavior From Strategic Knowledge in Task Models and Evaluating Its Impact on System Safety With Model Checking,M. L. Bolton; E. J. Bass,"Department of Mechanical and Industrial Engineering, University of Illinois at Chicago, Chicago, IL, USA","IEEE Transactions on Systems, Man, and Cybernetics: Systems",20131015,2013,43,6,1314,1327,"Human-automation interaction, including erroneous human behavior, is a factor in the failure of complex, safety-critical systems. This paper presents a method for automatically generating formal task analytic models encompassing both erroneous and normative human behavior from normative task models, where the misapplication of strategic knowledge is used to generate erroneous behavior. Resulting models can be automatically incorporated into larger formal system models so that safety properties can be formally verified with a model checker. This allows analysts to prove that a human-automation interactive system (as represented by the formal model) will or will not satisfy safety properties with both normative and generated erroneous human behavior. Benchmarks are reported that illustrate how this method scales. The method is then illustrated with a case study: the programming of a patient-controlled analgesia pump. In this example, a problem resulting from a generated erroneous human behavior is discovered. The method is further employed to evaluate the effectiveness of different solutions to the discovered problem. The results and future research directions are discussed.",2168-2216;21682216,,10.1109/TSMC.2013.2256129,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6519275,Formal methods;human error;human‰ÛÒautomation interaction (HAI);model checking;system safety;task analysis,Human computer interaction;Human factors;Model checking;System testing,formal verification;safety-critical software,erroneous human behavior;formal task analytic models;human-automation interaction;human-automation interactive system;model checking;normative task models;patient-controlled analgesia pump programming;safety properties;safety-critical systems;strategic knowledge,,7,,67,,no,Nov. 2013,,IEEE,IEEE Journals & Magazines
Getting more for less in optimized MapReduce workflows,Z. Zhang; L. Cherkasova; B. T. Loo,University of Pennsylvania,2013 IFIP/IEEE International Symposium on Integrated Network Management (IM 2013),20130801,2013,,,93,100,"Many companies are piloting the use of Hadoop for advanced data analytics over large datasets. Typically, such MapReduce programs represent workflows of MapReduce jobs. Currently, a user must specify the number of reduce tasks for each MapReduce job. The choice of the right number of reduce tasks is non-trivial and depends on the cluster size, input dataset of the job, and the amount of resources available for processing this job. In the workflow of MapReduce jobs, the output of one job becomes the input of the next job, and therefore the number of reduce tasks in the previous job may impact the performance and processing efficiency of the next job. In this work,1 we offer a novel performance evaluation framework for easing the user efforts of tuning the reduce task settings while achieving performance objectives. The proposed framework is based on two performance models: a platform performance model and a workflow performance model. A platform performance model characterizes the execution time of each generic phase in the MapReduce processing pipeline as a function of processed data. The complementary workflow performance model evaluates the completion time of a given workflow as a function of i) input dataset size(s) and ii) the reduce tasks' settings in the jobs that comprise a given workflow. We validate the accuracy, effectiveness, and performance benefits of the proposed framework using a set of realistic MapReduce applications and queries from the TPC-H benchmark.",1573-0077;15730077,Electronic:978-3-901882-50-0; POD:978-1-4799-1708-2; USB:978-1-4673-5229-1,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6572974,,Benchmark testing;Computational modeling;Data models;Phase measurement;Production;Time measurement;Tuning,data analysis;parallel programming;pipeline processing;software performance evaluation;task analysis;workflow management software,Hadoop;MapReduce job workflow;MapReduce processing pipeline;MapReduce programs;MapReduce queries;TPC-H benchmark;advanced data analytics;cluster size;complementary workflow performance model;generic phase;input dataset;input dataset size;job processing;optimized MapReduce workflows;performance evaluation framework;performance impact;platform performance model;processing efficiency;task reduction;workflow completion time evaluation;workflow performance model,,0,,21,,no,27-31 May 2013,,IEEE,IEEE Conference Publications
HFMS: Managing the lifecycle and complexity of hybrid analytic data flows,A. Simitsis; K. Wilkinson; U. Dayal; M. Hsu,"HP Labs Palo Alto, USA",2013 IEEE 29th International Conference on Data Engineering (ICDE),20130624,2013,,,1174,1185,"To remain competitive, enterprises are evolving their business intelligence systems to provide dynamic, near realtime views of business activities. To enable this, they deploy complex workflows of analytic data flows that access multiple storage repositories and execution engines and that span the enterprise and even outside the enterprise. We call these multi-engine flows hybrid flows. Designing and optimizing hybrid flows is a challenging task. Managing a workload of hybrid flows is even more challenging since their execution engines are likely under different administrative domains and there is no single point of control. To address these needs, we present a Hybrid Flow Management System (HFMS). It is an independent software layer over a number of independent execution engines and storage repositories. It simplifies the design of analytic data flows and includes optimization and executor modules to produce optimized executable flows that can run across multiple execution engines. HFMS dispatches flows for execution and monitors their progress. To meet service level objectives for a workload, it may dynamically change a flow's execution plan to avoid processing bottlenecks in the computing infrastructure. We present the architecture of HFMS and describe its components. To demonstrate its potential benefit, we describe performance results for running sample batch workloads with and without HFMS. The ability to monitor multiple execution engines and to dynamically adjust plans enables HFMS to provide better service guarantees and better system utilization.",1063-6382;10636382,Electronic:978-1-4673-4910-9; POD:978-1-4673-4909-3,10.1109/ICDE.2013.6544907,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6544907,,Business;Connectors;Databases;Engines;Fault tolerance;Monitoring;Optimization,competitive intelligence;data analysis;storage management,HFMS;business intelligence system;complexity management;executor module;hybrid analytic data flow;hybrid flow management system;independent execution engine;independent software layer;lifecycle management;multiengine flows hybrid flow;optimization module;storage repository;system utilization;workload management,,2,,27,,no,8-12 April 2013,,IEEE,IEEE Conference Publications
Hiding data in indexed images,H. Wang; R. Fei,"Sch. of Electr. & Inf. Eng., Liaoning Inst. of Sci. & Technol., Benxi, China",2013 IEEE 4th International Conference on Software Engineering and Service Science,20130930,2013,,,833,836,"In this paper, a method for hiding data is proposed based on color table expansion technique for indexed images. At the same time the security and the hiding capacity are analyzed in great detail. The analytic results show that this method has very strong security and very high hiding capacity. Secret data can be hidden in a stego-image which has exactly the same visual effect as the original image. If the correct key is used, these confidential data can be recovered from the stego-image without distortion. In addition, due to the use of random numbers, the statistical properties of the confidential data are completely overshadowed, and the stego-image is independent of those confidential data.",2327-0586;23270586,Electronic:978-1-4673-5000-6; POD:978-1-4673-4999-4,10.1109/ICSESS.2013.6615434,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6615434,Cover Image;Data Hiding;Hiding Capacity;Statistical Property;Stego-image,Irrigation;MATLAB,image colour analysis;security of data;statistical analysis;steganography,color table expansion technique;confidential data;data hiding;data security;hiding capacity;indexed images;random numbers;secret data;statistical properties;stego-image,,0,,12,,no,23-25 May 2013,,IEEE,IEEE Conference Publications
HierarchicalTopics: Visually Exploring Large Text Collections Using Topic Hierarchies,W. Dou; L. Yu; X. Wang; Z. Ma; W. Ribarsky,University of North Carolina at Charlotte,IEEE Transactions on Visualization and Computer Graphics,20131016,2013,19,12,2002,2011,"Analyzing large textual collections has become increasingly challenging given the size of the data available and the rate that more data is being generated. Topic-based text summarization methods coupled with interactive visualizations have presented promising approaches to address the challenge of analyzing large text corpora. As the text corpora and vocabulary grow larger, more topics need to be generated in order to capture the meaningful latent themes and nuances in the corpora. However, it is difficult for most of current topic-based visualizations to represent large number of topics without being cluttered or illegible. To facilitate the representation and navigation of a large number of topics, we propose a visual analytics system - HierarchicalTopic (HT). HT integrates a computational algorithm, Topic Rose Tree, with an interactive visual interface. The Topic Rose Tree constructs a topic hierarchy based on a list of topics. The interactive visual interface is designed to present the topic content as well as temporal evolution of topics in a hierarchical fashion. User interactions are provided for users to make changes to the topic hierarchy based on their mental model of the topic space. To qualitatively evaluate HT, we present a case study that showcases how HierarchicalTopics aid expert users in making sense of a large number of topics and discovering interesting patterns of topic groups. We have also conducted a user study to quantitatively evaluate the effect of hierarchical topic structure. The study results reveal that the HT leads to faster identification of large number of relevant topics. We have also solicited user feedback during the experiments and incorporated some suggestions into the current version of HierarchicalTopics.",1077-2626;10772626,,10.1109/TVCG.2013.162,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634160,Algorithm design and analysis;Analytical models;Computational modeling;Hierarchical topic representation;Text mining;Visual analytics;Vocabulary;rose tree;topic modeling;visual analytics,Algorithm design and analysis;Analytical models;Computational modeling;Text mining;Visual analytics;Vocabulary,computational complexity;data visualisation;text analysis;trees (mathematics);user interfaces;vocabulary,HierarchicalTopic;computational algorithm;hierarchical topic structure;interactive visual interface;interactive visualizations;text collections;text corpora;textual collections;topic groups;topic hierarchy;topic rose tree;topic-based text summarization methods;topic-based visualizations;user feedback;user interactions;visual analytics system;vocabulary,"Algorithms;Artificial Intelligence;Computer Graphics;Documentation;Image Enhancement;Image Interpretation, Computer-Assisted;Information Storage and Retrieval;Natural Language Processing;Pattern Recognition, Automated;Software;User-Computer Interface",19,,35,,no,Dec. 2013,,IEEE,IEEE Journals & Magazines
High-performance imaging subsystems and their integration in mobile devices,M. Lindwer; M. R. Pedersen,"IAG/MCG/VIED, Intel Corporation, Eindhoven, The Netherlands","2013 Design, Automation & Test in Europe Conference & Exhibition (DATE)",20130504,2013,,,170,170,"Within today's SoCs, functionality such as video, audio, graphics, and imaging is increasingly integrated through IP blocks, which are subsystems in their own right. Integration of IP blocks within SoCs always brought software integration aspects with it. However, since these subsystems increasingly consist of programmable processors, many more layers of firmware and software need to be integrated. In the imaging domain, this is particularly true. Imaging subsystems typically are highly heterogeneous, with high levels of parallelism. The construction of their firmware requires target-specific optimization, yet needs to take interoperability with sensor input systems and graphics/display subsystems into account. Hard real-time scheduling within the subsystem needs to cooperate with less stringent image analytics and SoC-level (OS) scheduling. In many of today's systems, the latter often only supports soft scheduling deadlines. At HW level, IP subsystems need to be integrated such that they can efficiently exchange both short-latency control signals and high-bandwidth data-plane blocks. Solutions exist, but need to be properly configured. However, at the SW level, currently no support exists that provides (i) efficient programmability, (ii) SW abstraction of all the different HW features of these blocks, and (iii) interoperability of these blocks. Starting points could be languages such as OpenCL and OpenCV, which do provide some abstractions, but are not yet sufficiently versatile.",1530-1591;15301591,Electronic:978-3-9815370-0-0; POD:978-1-4673-5071-6,10.7873/DATE.2013.048,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6513494,ASIP;IP integration;MPSoC;Software;imaging,Graphics;IP networks;Imaging;Program processors;Scheduling;System-on-chip,,,,0,,3,,no,18-22 March 2013,,IEEE,IEEE Conference Publications
High-Performance RDMA-based Design of Hadoop MapReduce over InfiniBand,M. Wasi-ur-Rahman; N. S. Islam; X. Lu; J. Jose; H. Subramoni; H. Wang; D. K. D. Panda,"Dept. of Comput. Sci. & Eng., Ohio State Univ., Columbus, OH, USA","2013 IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum",20131031,2013,,,1908,1917,"MapReduce is a very popular programming model used to handle large datasets in enterprise data centers and clouds. Although various implementations of MapReduce exist, Hadoop MapReduce is the most widely used in large data centers like Facebook, Yahoo! and Amazon due to its portability and fault tolerance. Network performance plays a key role in determining the performance of data intensive applications using Hadoop MapReduce as data required by the map and reduce processes can be distributed across the cluster. In this context, data center designers have been looking at high performance interconnects such as InfiniBand to enhance the performance of their Hadoop MapReduce based applications. However, achieving better performance through usage of high performance interconnects like InfiniBand is a significant task. It requires a careful redesign of communication framework inside MapReduce. Several assumptions made for current socket based communication in the current framework do not hold true for high performance interconnects. In this paper, we propose the design of an RDMA-based Hadoop MapReduce over InfiniBand and several design elements: data shuffle over InfiniBand, in-memory merge mechanism for the Reducer, and pre-fetch data for the Mapper. We perform our experiments on native InfiniBand using Remote Direct Memory Access (RDMA) and compare our results with that of Hadoop-A [1] and default Hadoop over different interconnects and protocols. For all these experiments, we perform network level parameter tuning and use optimum values for each Hadoop design. Our performance results show that, for a 100GB TeraSort running on an eight node cluster, we achieve a performance improvement of 32% over IP-over InfiniBand (IPoIB) and 21% over Hadoop-A. With multiple disks per node, this benefit rises up to 39% over IPoIB and 31% over Hadoop-A.",,Electronic:978-0-7695-4979-8; POD:978-1-4799-1372-5,10.1109/IPDPSW.2013.238,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6651094,,Benchmark testing;Corporate acquisitions;Distributed databases;IP networks;Information management;Libraries;Software,computer network performance evaluation;distributed databases;fault tolerant computing;file organisation;merging;public domain software,Hadoop MapReduce;Hadoop-A;IP-over InfiniBand;IPoIB;TeraSort;big data analytic applications;communication framework;data intensive applications;data shuffle;default Hadoop;enterprise data centers;fault tolerance;high-performance RDMA-based design;in-memory merge mechanism;mapper;network level parameter tuning;network performance;open-source frameworks;performance interconnects;portability;pre-fetch data;programming model;protocols;reducer;remote direct memory access,,5,,24,,no,20-24 May 2013,,IEEE,IEEE Conference Publications
I/O Containers: Managing the Data Analytics and Visualization Pipelines of High End Codes,J. Dayal; J. Cao; G. Eisenhauer; K. Schwan; M. Wolf; F. Zheng; H. Abbasi; S. Klasky; N. Podhorszki; J. Lofstead,,"2013 IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum",20131031,2013,,,2015,2024,"Lack of I/O scalability is known to cause measurable slowdowns for large-scale scientific applications running on high end machines. This is prompting researchers to devise 'I/O staging' methods in which outputs are processed via online analysis and visualization methods to support desired science outcomes. Organized as online workflows and carried out in I/O pipelines, these analysis components run concurrently with science simulations, often using a smaller set of nodes on the high end machine termed 'staging areas'. This paper presents a new approach to dealing with several challenges arising for such online analytics, including: how to efficiently run multiple analytics components on staging area resources providing them with the levels of end-to-end performance they need and how to manage staging resources when analytics actions change due to user or data-dependent behavior. Our approach designs and implements middleware constructs that delineate and manage I/O pipeline resources called 'I/O Containers'. Experimental evaluations of containers with realistic scientific applications demonstrate the feasibility and utility of the approach.",,Electronic:978-0-7695-4979-8; POD:978-1-4799-1372-5,10.1109/IPDPSW.2013.198,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6651106,Data Analytics;Data Staging;Runtime Management;Scalable I/O;Visualization;in-Situ;resource sharing,Analytical models;Computational modeling;Containers;Data models;Data visualization;Monitoring;Pipelines,data analysis;data visualisation;middleware;pipeline processing;resource allocation;software performance evaluation,I-O pipeline resource management;I-O scalability;I-O staging methods;I/O containers;data analytics;data visualization pipelines;data-dependent behavior;end-to-end performance;high end codes;high end machine;high end machines;large-scale scientific applications;middleware constructs;online analysis methods;online visualization methods;online workflows;staging area resources,,5,,33,,no,20-24 May 2013,,IEEE,IEEE Conference Publications
IEC61850 Gateway SCL Configuration Document Research,H. Sizu; L. Wei; K. Fengying,"Electr. & Electron. Eng. Acad., North China Electr. Power Univ., Baoding, China",2013 Third International Conference on Intelligent System Design and Engineering Applications,20130207,2013,,,824,831,"Smart grid's development is rapid, and the application of IEC61850 standard has become a necessary tendency. It can use nets to realize the transformation of the original statute to IEC61850 standard, because the configuration document description IED model cannot be recognized by MMS. It needed to adopt a specifical function which was analyzed for the document and produced IEC 61850 object corresponding MMS object. Because how the SCL configuration document parses the relationship to code is the effectiveness of the conversion, it has become the key to the whole system. By logging on SCLServer gateway Telnet terminal on the modified configuration information, we can get the magnitude and the bottom of the real-time communication data. The monitoring system of remote sensing and remote data transmission set up the view through the database design and the configuration document. The analytic design and original data such as the gateway's date can be got through locating design software.",,Electronic:978-0-7695-4923-1; POD:978-1-4673-4893-5,10.1109/ISDEA.2012.196,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6455739,SCL;data modeling;gateway,Data models;IEC standards;Logic gates;Monitoring;Protocols;Real-time systems,IEC standards;internetworking;monitoring;network servers;power engineering computing;remote sensing;smart power grids,IEC61850 gateway;IEC61850 standard;MMS object;SCL configuration document research;SCLServer gateway;Telnet terminal;configuration document description IED model;database design;design software;monitoring system;real-time communication data;remote data transmission;remote sensing;smart grid development,,0,,3,,no,16-18 Jan. 2013,,IEEE,IEEE Conference Publications
Improved decision-support making for selecting future traffic signal controllers using expert-knowledge acquisition,M. N. Mladenovi€à; M. M. Abbas,"Virginia Polytech. Inst. & State Univ., Blacksburg, VA, USA",16th International IEEE Conference on Intelligent Transportation Systems (ITSC 2013),20140130,2013,,,732,735,"Transportation agencies are facing the decision-making problem while selecting traffic signal controller that corresponds to the needs of their future signal system. The complexity of this problem originates from the current level of controller standardization, market-driven competition, responsibility for long-term operation, and scale of investment. This paper presents an improvement of methodology and a decision-support system (DSS) for selecting traffic signal controllers. DSS bases upon Analytical Hierarchy Process, and is developed as an application in MS Excel. The main improvement is the component for expert knowledge acquisition for assignment of criteria weights. The graphical user interface and supporting analytical engine based on fuzzy logic are developed to enhance the expert knowledge acquisition. Paper presents application interface and analytical engine with an example. Possibilities for further research should provide potential for greater flexibility of this application to aid in decision-making for other equipment selection.",2153-0009;21530009,Electronic:978-1-4799-2914-6; POD:978-1-4799-2915-3; USB:978-1-4799-2913-9,10.1109/ITSC.2013.6728318,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6728318,,Control systems;Decision making;Decision support systems;Graphical user interfaces;Pragmatics;Software;Transportation,analytic hierarchy process;decision making;decision support systems;fuzzy logic;graphical user interfaces;knowledge acquisition;road traffic control;traffic engineering computing,MS Excel;analytical engine;analytical hierarchy process;application interface;controller standardization;criteria weight assignment;decision support system;equipment selection;expert knowledge acquisition;fuzzy logic;graphical user interface;improved decision support making;market-driven competition;traffic signal controller selection;transportation agencies,,0,,33,,no,6-9 Oct. 2013,,IEEE,IEEE Conference Publications
Improved power control of photovoltaic generation system under unbalanced grid voltage conditions,X. Chen; Y. Zhang; J. Yang; Y. Chen; Q. Wang; N. Zhou,"Key Lab. of Clean Energy Technol., Guangzhou, China",2013 IEEE PES Asia-Pacific Power and Energy Engineering Conference (APPEEC),20140619,2013,,,1,6,"In order to ride through the unbalanced grid fault, the output power of photovoltaic inverter will fluctuate and its output current will rise and distort under unbalanced grid voltage. This paper proposes an improved formula of reference current and its optimal coefficients method for the photovoltaic inverter under unbalanced grid voltage sag, taking into account the requirements of active and reactive power control. Then the analytic formula of its current harmonic distortion, three-phase current peak, active and reactive power fluctuation, DC voltage fluctuation and the control factor of its output current reference were derived. Considering the constraints of inverter phase current and the dc voltage fluctuation of capacitance in dc side, the optimal model of inverter current reference was established with minimum integrated amplitude of the active and reactive power function as a goal. The correctness of the proposed method has been verified by PSCAD/EMTDC simulation software.",2157-4839;21574839,Electronic:978-1-4799-2522-3; POD:978-1-4799-2523-0,10.1109/APPEEC.2013.6837218,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6837218,DC voltage fluctuation;current peak;photovoltaic;power control;unbalanced voltage sag,Fluctuations;Harmonic distortion;Inverters;Power control;Reactive power;Voltage control;Voltage fluctuations,harmonic distortion;invertors;photovoltaic power systems;power grids;power supply quality;reactive power control,DC voltage fluctuation;PSCAD-EMTDC simulation software;current harmonic distortion;inverter current reference;optimal coefficients;photovoltaic generation system;photovoltaic inverter;reactive power control;reactive power fluctuation;reference current;three-phase current peak;unbalanced grid fault;unbalanced grid voltage conditions;voltage sag,,1,,11,,no,8-11 Dec. 2013,,IEEE,IEEE Conference Publications
Improving Multi-job MapReduce Scheduling in an Opportunistic Environment,Y. Ji; L. Tong; T. He; J. Tan; K. w. Lee; L. Zhang,"Sch. of Electr. & Comput. Eng., Cornell Univ., Ithaca, NY, USA",2013 IEEE Sixth International Conference on Cloud Computing,20131202,2013,,,9,16,"As a state-of-the-art programming model for big data analytics, MapReduce is well suited for parallel processing of large data sets in opportunistic environments. Existing research on MapReduce in opportunistic environment has focused on improving single job performance, the issue of fairness that is critical in the more dominant scenario of multiple concurrent jobs remains unexplored. We address this problem by proposing an opportunistic fair scheduling algorithm, which extends the broadly adopted Fair Scheduler to an environment where nodes are intermittently available with possibly different availability patterns. The proposed scheduler maintains statistics specific to the opportunistic environment, e.g., node availability rates and pairwise availability correlations, and utilizes this information in scheduling decisions to improve fairness. Using a Hadoop-based implementation, we compare our scheduler with the current Hadoop Fair Scheduler on representative benchmarks. Our experiments verify that our scheduler can significantly reduce the variability in job completion times.",2159-6182;21596182,Electronic:978-0-7695-5028-2; POD:978-1-4799-0490-7,10.1109/CLOUD.2013.84,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676672,,Availability;Correlation;Hardware;History;Processor scheduling;Schedules;Scheduling,cloud computing;data analysis;parallel programming;public domain software;scheduling,Hadoop fair scheduler;big data analytics;concurrent jobs;job completion times;multijob MapReduce scheduling;opportunistic environment;opportunistic fair scheduling algorithm;parallel processing;programming model;scheduling decisions;single job performance,,0,,15,,no,June 28 2013-July 3 2013,,IEEE,IEEE Conference Publications
Incorporating Uncertainty into In-Cloud Application Deployment Decisions for Availability,Q. Lu; X. Xu; L. Zhu; L. Bass; Z. Li; S. Sakr; P. L. Bannerman; A. Liu,"Software Syst. Res. Group, NICTA, Sydney, NSW, Australia",2013 IEEE Sixth International Conference on Cloud Computing,20131202,2013,,,454,461,"Cloud consumers have a variety of deployment related techniques, such as auto-scaling policies and recovery strategies, for dealing with the uncertainties in the cloud. Uncertainties can be characterized as stochastic (such as failures, disasters, and workload spikes) and subjective (such as choice among various deployment options). Cloud consumers must consider both stochastic and subjective uncertainties. Analytic support for consumers in selecting appropriate techniques and setting the required parameters in the face of different types of uncertainty is currently limited. In this paper, we propose a set of application availability analysis models that capture subjective uncertainties in addition to stochastic uncertainties. We built and validated the models by using industry best practices on deployment, and actual commercial products for disaster recovery and live migration. Our results show that the models permit more informed and quantitative availability analysis than industry best practices under a wide range of scenarios.",2159-6182;21596182,Electronic:978-0-7695-5028-2; POD:978-1-4799-0490-7,10.1109/CLOUD.2013.87,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676727,availability;cloud computing;deployment architecture;stochastic reward nets;uncertainty,Analytical models;Availability;Best practices;Databases;Industries;Stochastic processes;Uncertainty,cloud computing,application availability analysis models;auto-scaling policies;cloud consumers;cloud uncertainty;deployment related techniques;disaster recovery;in-cloud application deployment decisions;live migration;recovery strategies;stochastic uncertainty;subjective uncertainty,,1,,23,,no,June 28 2013-July 3 2013,,IEEE,IEEE Conference Publications
Influence of learning analytics in software engineering education,N. Pratheesh; T. Devi,"Department of Computer Applications, School of Computer Science and Engineering, Bharathiar University, Coimbatore - 641 046, India","2013 IEEE International Conference ON Emerging Trends in Computing, Communication and Nanotechnology (ICECCN)",20130613,2013,,,712,716,"Software is an important aspect in the modern world because it uses by everyone and everywhere. Therefore software engineering education gets more important in the computer science curricula but it has lapses to produce good software engineers to the industries requirements. Learning analytics helps the students to improve their learning activities. This was analyzed among the software engineering students how the learning style influences in gathering knowledge. Forty six questionnaires were distributed to the first year MCA students in the Department of Computer Applications, Bharathiar University, Coimbatore, India for this study.",,Electronic:978-1-4673-5036-5; POD:978-1-4673-5037-2,10.1109/ICE-CCN.2013.6528597,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6528597,Learning Analytics;Learning Style;Social Learning Analytics;Software Engineering;Software Engineering Education,Computer science;Educational institutions;Industries;Knowledge engineering;Software;Software engineering,computer science education;educational courses;educational institutions;software engineering,Bharathiar University;Coimbatore;India;computer science curricula;department of computer applications;first year MCA students;industries requirements;learning activities;learning analytics;software engineering education,,0,,24,,no,25-26 March 2013,,IEEE,IEEE Conference Publications
Informing development decisions: From data to information,O. Baysal,"David R. Cheriton School of Computer Science, University of Waterloo, ON, Canada",2013 35th International Conference on Software Engineering (ICSE),20130926,2013,,,1407,1410,"Software engineers generate vast quantities of development artifacts such as source code, bug reports, test cases, usage logs, etc., as they create and maintain their projects. The information contained in these artifacts could provide valuable insights into the software quality and adoption, as well as development process. However, very little of it is available in the way that is immediately useful to various stakeholders. This research aims to extract and analyze data from software repositories to provide software practitioners with up-to-date and insightful information that can support informed decisions related to the business, management, design, or development of software systems. This data-centric decision-making is known as analytics. In particular, we demonstrate that by employing software development analytics, we can help developers make informed decisions around user adoption of a software project, code review process, as well as improve developers' awareness of their working context.",0270-5257;02705257,Electronic:978-1-4673-3076-3; POD:978-1-4673-3075-6,10.1109/ICSE.2013.6606729,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606729,,Communities;Data mining;Decision making;Market research;Open source software;Software systems,program diagnostics;program testing;project management;software development management;software quality,bug reports;code review process;data-centric decision-making;developer awareness improvement;development artifacts;development process;informed decision;software adoption;software development analytics;software development decision;software engineering;software project adoption;software quality;software repository;software system;source code;test cases;usage logs,,0,,30,,no,18-26 May 2013,,IEEE,IEEE Conference Publications
Innovative practices session 5C: Cloud atlas ‰ÛÓ Unreliability through massive connectivity,H. Naeimi; S. Natarajan; K. Vaid; P. Kudva; M. Natu,Intel Corporation,2013 IEEE 31st VLSI Test Symposium (VTS),20130627,2013,,,1,1,"The rapid pace of integration, emergence of low power, low cost computing elements, and ubiquitous and ever-increasing bandwidth of connectivity have given rise to data center and cloud infrastructures. These infrastructures are beginning to be used on a massive scale across vast geographic boundaries to provide commercial services to businesses such as banking, enterprise computing, online sales, and data mining and processing for targeted marketing to name a few. Such an infrastructure comprises of thousands of compute and storage nodes that are interconnected by massive network fabrics, each of them having their own hardware and firmware stacks, with layers of software stacks for operating systems, network protocols, schedulers and application programs. The scale of such an infrastructure has made possible service that has been unimaginable only a few years ago, but has the downside of severe losses in case of failure. A system of such scale and risk necessitates methods to (a) proactively anticipate and protect against impending failures, (b) efficiently, transparently and quickly detect, diagnose and correct failures in any software or hardware layer, and (c) be able to automatically adapt itself based on prior failures to prevent future occurrences. Addressing the above reliability challenges is inherently different from the traditional reliability techniques. First, there is a great amount of redundant resources available in the cloud from networking to computing and storage nodes, which opens up many reliability approaches by harvesting these available redundancies. Second, due to the large scale of the system, techniques with high overheads, especially in power, are not acceptable. Consequently, cross layer approaches to optimize the availability and power have gained traction recently. This session will address these challenges in maintaining reliable service with solutions across the hardware/software stacks. The currently available commercial data-cente- and cloud infrastructures will be reviewed and the relative occurrences of different causalities of failures, the level to which they are anticipated and diagnosed in practice, and their impact on the quality of service and infrastructure design will be discussed. A study on real-time analytics to proactively address failures in a private, secure cloud engaged in domain-specific computations, with streaming inputs received from embedded computing platforms (such as airborne image sources, data streams, or sensors) will be presented next. The session concludes with a discussion on the increased relevance of resiliency features built inside individual systems and components (private cloud) and how the macro public cloud absorbs innovations from this realm.",1093-0167;10930167,Electronic:978-1-4673-5543-8; POD:978-1-4673-5542-1,10.1109/VTS.2013.6548907,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6548907,,Cloud computing;Hardware;Software reliability;Streaming media;Very large scale integration,,,,0,,,,no,April 29 2013-May 2 2013,,IEEE,IEEE Conference Publications
Integrated wireless sensor network for large scale intelligent systems,P. K. Tiwari; S. Parthasarathy; A. N. Chatterjee; N. Krishna,"Semiconductor R&D Center, IBM, Bangalore, India","2013 International Conference on Advances in Computing, Communications and Informatics (ICACCI)",20131021,2013,,,1849,1854,"There is a critical need to improve the efficiency of the large systems and eco-system of this planet. It is important to successfully integrate a variety of technology blocks ranging from sensors to communication system to analytics software in order to realize our goal of smarter systems and eco-systems. In order to apply a proactive approach to solve this planet's problem, everything must be instrumented. We would like to deploy the sensors widely so that these sensors act as the eyes, ears, and noses for the analytics engine and provide real time data at a desired frequency. In this paper, we demonstrate the first step to realize a convergence of wireless sensor network with analytics software. The wireless sensors transmit data at regular intervals through the 2.4 GHz ISM band to a coordinator. The coordinator forwards the aggregated data to a messaging software. The time stamps, topics, and payloads of the messages can be visualized using a GUI explorer of the messaging software. These messages can be further forwarded through a message broker to database servers and web servers via internet. Accordingly, the analytics software could make use of the live data to provide decision support for engineers, automated control system, and actuators.",,Electronic:978-1-4673-6217-7; POD:978-1-4799-1664-1,10.1109/ICACCI.2013.6637463,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6637463,Intelligent Systems;Internet of Things;MQTT;Wireless sensor network;ZigBee PRO,Databases;Protocols;Sensors;Software;Wireless communication;Wireless sensor networks;Zigbee,Internet;data analysis;data visualisation;graphical user interfaces;telecommunication computing;wireless sensor networks,GUI explorer;analytics software;graphical user interface;integrated wireless sensor network;large scale intelligent systems;message visualization;messaging software;network convergence,,0,,10,,no,22-25 Aug. 2013,,IEEE,IEEE Conference Publications
Integration of DEMATEL and ANP methods for calculate the weight of characteristics software quality based model ISO 9126,Sugiyanto; S. Rochimah,"Dept. of Inf., Inst. Teknol. Adhi Tama Surabaya (ITATS), Surabaya, Indonesia",2013 International Conference on Information Technology and Electrical Engineering (ICITEE),20131202,2013,,,143,148,"One of the difficulties that occur in the model is to decide the weights of quality characteristics. This is due to the interrelations existence among the quality factors based model ISO 9126. Each of these characteristics can influence or even contradict each other. The interrelations existence among the factors affects the weight of characteristics software quality, and will affect the software quality calculation. Therefore, researchers will integrate DEMATEL and ANP methods for calculate the weight of characteristics software quality based model ISO 9126. DEMATEL method used to calculate sum of influences for each characteristics model ISO 9126, while the ANP method used to calculate local weights and global weight for each sub characteristics model ISO 9126. Results from this study is the value of local weights for each of the characteristics of ISO 9126, and global weights for each sub characteristics ISO 9126 which represent the level of importance of the characteristics and sub characteristics ISO 9126.",,Electronic:978-1-4799-0425-9; POD:978-1-4799-0422-8,10.1109/ICITEED.2013.6676228,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676228,ANP;DEMATEL;ISO 9126;software quality,,ISO standards;analytic hierarchy process;software quality,ANP method;DEMATEL method;ISO 9126;characteristics software quality based model;global weight calculation;local weight calculation;software products;software quality calculation,,0,,10,,no,7-8 Oct. 2013,,IEEE,IEEE Conference Publications
IntegrityMR: Integrity assurance framework for big data analytics and management applications,Y. Wang; J. Wei; M. Srivatsa; Y. Duan; W. Du,"Florida Int. Univ., Miami, FL, USA",2013 IEEE International Conference on Big Data,20131223,2013,,,33,40,"Big data analytics and knowledge management is becoming a hot topic with the emerging techniques of cloud computing and big data computing model such as MapReduce. However, large-scale adoption of MapReduce applications on public clouds is hindered by the lack of trust on the participating virtual machines deployed on the public cloud. In this paper, we extend the existing hybrid cloud MapReduce architecture to multiple public clouds. Based on such architecture, we propose IntegrityMR, an integrity assurance framework for big data analytics and management applications. We explore the result integrity check techniques at two alternative software layers: the MapReduce task layer and the applications layer. We design and implement the system at both layers based on Apache Hadoop MapReduce and Pig Latin, and perform a series of experiments with popular big data analytics and management applications such as Apache Mahout and Pig on commercial public clouds (Amazon EC2 and Microsoft Azure) and local cluster environment. The experimental result of the task layer approach shows high integrity (98% with a credit threshold of 5) with non-negligible performance overhead (18% to 82% extra running time compared to original MapReduce). The experimental result of the application layer approach shows better performance compared with the task layer approach (less than 35% of extra running time compared with the original MapReduce).",,Electronic:978-1-4799-1293-3; POD:978-1-4799-1294-0,10.1109/BigData.2013.6691780,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6691780,Big Data;Cloud Computing;Integrity Assurance;MapReduce,Accuracy;Cloud computing;Computer architecture;Data handling;Data storage systems;Error analysis;Information management,Big Data;cloud computing;knowledge management;virtual machines,Amazon EC2;Apache Hadoop MapReduce;Apache Mahout;IntegrityMR;Microsoft Azure;Pig Latin;alternative software layers;big data analytics;big data computing model;cloud computing;commercial public clouds;hybrid cloud MapReduce architecture;integrity assurance framework;integrity check techniques;knowledge management;large-scale adoption;local cluster environment;management applications;nonnegligible performance overhead;virtual machines,,2,,23,,no,6-9 Oct. 2013,,IEEE,IEEE Conference Publications
Intelligent BVAC information capturing system for smart building information modelling,L. C. M. Tang; S. Y. Cho; L. Xia,"Dept. of Archit. & Built Environ. Eng., Univ. of Nottingham Ningbo China, Ningbo, China",2013 5th International Conference on Power Electronics Systems and Applications(PESA),20140612,2013,,,1,4,"Building Information Modelling (BIM) has implications for all processes and activities related to construction supply chain and can, thus, make significant contributions to lean construction process. The existing dimensions of BIM not only attend to most aspects of the construction work and processes, but the technology also has the potential to add further dimensions responding to other existing or future challenges. This paper will look into ways in which Artificial Neural Network (ANN)-COBie can help architects and engineers to perform HVAC analysis with the support of a BIM platform. Other applications e.g. HVAC load analysis and life-cycle cost analysis for any system or component associated with a building may be conducted using the ANN-COBie system that can be stored in the BIM authoring application and exported using IFC or gbXML to any analytic software. These applications' associated future challenges will be briefly discussed.",,CD-ROM:978-1-4799-3490-4; Electronic:978-1-4799-3491-1; POD:978-1-4799-3492-8,10.1109/PESA.2013.6828247,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6828247,,Artificial neural networks;Buildings;Cooling;Data models;Heating;Humidity,HVAC;building management systems;buildings (structures);home automation;lean production;neural nets;power engineering computing;supply chains,ANN-COBie system;BIM authoring application;IFC;artificial neural network;construction supply chain;gbXML;intelligent HVAC information capturing system;lean construction process;smart building information modelling,,0,,13,,no,11-13 Dec. 2013,,IEEE,IEEE Conference Publications
Intelligent system for multivariables reconfiguration of distribution networks,A. P. Mello; D. P. Bernardon; L. L. Pfitscher; M. Sperandio; B. B. Toller; M. Ramos,"Universidade Federal de Santa Maria, Av. Roraima, 1000 Santa Maria/RS, 97105-900, Brasil",2013 IEEE PES Conference on Innovative Smart Grid Technologies (ISGT Latin America),20130715,2013,,,1,6,"This paper proposes a new distribution network reconfiguration approach in normal operating conditions. The multivariables are considered such as the energy losses minimization of the primary network and two reliability indices. The selection strategy of network configuration is based on a heuristic method. The method considers only the automated equipment such as switches and remote controlled reclosers in the new configuration analysis. The AHP (Analytic Hierarchic Process) method is used to define the weights for the optimization criteria and to determine the best switching sequence of the network reconfiguration. The reconfiguration is performed by monitoring the real time network. Changes in demand feeders are considered and analyzed by load rates. Also, the best setting for each operation level is defined. The results of the implemented techniques indicate a satisfactory methodology. Tests were performed using a real power utility system.",,Electronic:978-1-4673-5274-1; POD:978-1-4673-5272-7,10.1109/ISGT-LA.2013.6554380,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6554380,AHP;Automatic Reconfiguration;Distribution Network;Multicriteria Decision Making;Smart Grid,Analytic hierarchy process;Electronic mail;Intelligent systems;Monitoring;Noise measurement;Smart grids;Software,analytic hierarchy process;distribution networks;optimisation,AHP method;analytic hierarchic process method;automated equipment;demand feeders;distribution network reconfiguration approach;energy losses minimization;heuristic method;intelligent system;load rates;multivariables reconfiguration;normal operating conditions;operation level;optimization criteria;primary network;real power utility system;real time network;reliability indices;remote controlled reclosers;selection strategy;switching sequence,,1,,11,,no,15-17 April 2013,,IEEE,IEEE Conference Publications
Interactive Exploration of Collaborative Software-Development Data,,,,,2013,,,,,"Modern collaborative software-development tools generate a rich data record, over the lifecycle of the project, which can be analyzed to provide team members and managers with insights into the performance and contributions of individual members and the overall team dynamic. This data can be analyzed from different perspectives, sliced and diced across different dimensions, and visualized in different ways. Frequently the most useful analysis depends on the actual data, which makes the design of single authoritative visualization a challenge. In this paper we describe an analysis and visualization tool that supports the flexible run-time mapping of such a data record to a number of alternative visualizations. We have used our framework to analyze and gain an understanding of how individuals work within their teams and how teams differ in their work on these term projects.",,,,http://dl.acm.org/citation.cfm?id=2550626&CFID=696538919&CFTOKEN=83912867,,,,,,,,,,yes,,,,
Introducing a Data Sliding Mechanism for Cooperative Caching in Manycore Architectures,S. Dahmani; L. Cudennec; G. Gogniat,"Embedded Real Time Syst. Lab., CEA, Gif-sur-Yvette, France","2013 IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum",20131031,2013,,,335,344,"In future micro-architectures, the increase of the number of cores and wire network complexity is leading to several performance degradation. These platforms are intended to process large amount of data. One of the biggest challenges for systems scalability is actually the memory wall: the memory latency is hardly increasing compared to technology expectations. Recent works explore potential software and hardware solutions mainly based on different caching schemes for addressing off-chip access issues. In this paper, we propose a new cooperative caching method improving the cache miss rate for many core micro-architectures. The work is motivated by some limitations of recent adaptive cooperative caching proposals. Elastic Cooperative caching (ECC), is a dynamic memory partitioning mechanism that allows sharing cache across cooperative nodes according to the application behavior. However, it is mainly limited with cache eviction rate in case of highly stressed neighborhood. Another system, the adaptive Set-Granular Cooperative Caching (ASCC), is based on finer set-based mechanisms for a better adaptability. However, heavy localized cache loads are not efficiently managed. In such a context, we propose a cooperative caching strategy that consists in sliding data through closer neighbors. When a cache receives a storing request of a neighbor's private block, it spills the least recently used private data to a close neighbor. Thus, solicited saturated nodes slide local blocks to their respective neighbors to always provide free cache space. We also propose a new Priority-based Data Replacement policy to decide efficiently which blocks should be spilled, and a new mechanism to choose host destination called Best Neighbor selector. The first analytic performance evaluation shows that the proposed cache management policies reduce by half the average global communication rate. As frequent accesses are focused in the neighboring zones, it efficiently improves on-Chip traff- c. Finally, our evaluation shows that cache miss rate is enhanced: each tile keeps the most frequently accessed data 1-Hop close to it, instead of ejecting them Off-Chip. Proposed techniques notably reduce the cache miss rate in case of high solicitation of the cooperative zone, as it is shown in the performed experiments.",,Electronic:978-0-7695-4979-8; POD:978-1-4799-1372-5,10.1109/IPDPSW.2013.205,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6650905,Best Neighbor Selector;Cache Partitioning;Cooperative Caching;Data Sliding;Many-cores;Memory Hierarchy;Priority-Based Replacement Policy;Tiled Micro-architectures,Context;Cooperative caching;Distributed databases;Large Hadron Collider;Protocols;Radiation detectors;System-on-chip,cache storage;granular computing;memory architecture;microprocessor chips;multiprocessing systems,ASCC;ECC;adaptive set-granular cooperative caching;application behavior;average global communication rate;best neighbor selector;cache eviction rate;cache management policies;cache miss rate improvement;cooperative nodes;data sliding mechanism;dynamic memory partitioning mechanism;elastic cooperative caching;many core microarchitectures;memory latency;off-chip access issues;on-chip traffic improvement;performance degradation;priority-based data replacement policy;set-based mechanisms;solicited saturated nodes;systems scalability;wire network complexity,,1,,35,,no,20-24 May 2013,,IEEE,IEEE Conference Publications
Investigating Graph Algorithms in the BSP Model on the Cray XMT,D. Ediger; D. A. Bader,"Georgia Inst. of Technol., Atlanta, GA, USA","2013 IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum",20131031,2013,,,1638,1645,"Implementing parallel graph algorithms in large, shared memory machines, such as the Cray XMT, can be challenging for programmers. Synchronization, deadlock, hot spotting, and others can be barriers to obtaining linear scalability. Alternative programming models, such as the bulk synchronous parallel programming model used in Google's Pregel, have been proposed for large graph analytics. This model eases programmer complexity by eliminating deadlock and simplifying data sharing. We investigate the algorithmic effects of the BSP model for graph algorithms and compare performance and scalability with hand-tuned, open source software using GraphCT. We analyze the innermost iterations of these algorithms to understand the differences in scalability between BSP and shared memory algorithms. We show that scalable performance can be obtained with graph algorithms in the BSP model on the Cray XMT. These algorithms perform within a factor of 10 of hand-tuned C code.",,Electronic:978-0-7695-4979-8; POD:978-1-4799-1372-5,10.1109/IPDPSW.2013.107,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6651060,,Clustering algorithms;Computational modeling;Parallel programming;Scalability;Software algorithms;System recovery,graph theory;iterative methods;parallel algorithms;parallel programming;shared memory systems,BSP model;Cray XMT;GraphCT;bulk synchronous parallel programming model;data sharing;deadlock elimination;innermost iterations;large graph analytics;open source software;parallel graph algorithms;programmer complexity;shared memory machines,,1,,25,,no,20-24 May 2013,,IEEE,IEEE Conference Publications
IOT-StatisticDB: A General Statistical Database Cluster Mechanism for Big Data Analysis in the Internet of Things,Z. Ding; X. Gao; J. Xu; H. Wu,"Inst. of Software, Beijing, China","2013 IEEE International Conference on Green Computing and Communications and IEEE Internet of Things and IEEE Cyber, Physical and Social Computing",20131212,2013,,,535,543,"In large scale Internet of Things (IoT) systems, statistical analysis is a crucial technique for transforming data into knowledge and for obtaining overall information about the physical world. However, most existing statistical analysis methods for sensor sampling data are implemented outside the database kernel and focus on specialized analytics, making them unsuited for the IoT environment where both the data types and the statistical queries are diverse. To solve this problem, we propose a General Statistical Database Cluster Mechanism for Big Data Analysis in the Internet of Things (IOT-StatisticDB) in this paper. In IOT-StatisticDB, statistical functions are performed through statistical operators inside the DBMS kernel, so that complicated statistical queries can be expressed in the standard SQL format. Besides, statistical analysis is executed in a distributed and parallel manner over multiple servers so that the performance can be greatly improved, which is confirmed by the experiments.",,Electronic:978-0-7695-5046-6; POD:978-1-4799-0631-4,10.1109/GreenCom-iThings-CPSCom.2013.104,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6682118,Big Data;Internet of Things;Sensor Sampling Data;Spatial-Temporal Data;Statistical Database,Databases;Global Positioning System;Internet;Memory;Monitoring;Servers;Statistical analysis,Big Data;Internet of Things;SQL;data analysis;sampling methods,IOT-StatisticDB;Internet of Things;IoT systems;SQL format;Structured Query Languages;big data analysis;general statistical database cluster mechanism;sensor sampling data;statistical analysis methods;statistical queries,,1,,16,,no,20-23 Aug. 2013,,IEEE,IEEE Conference Publications
IP geolocation suspicious email messages,A. Butkovi€à; S. Mrdovi€à; S. Muja€çi€à,"Police Support Agency of Bosnia & Herzegovina, Sarajevo, Bosnia-Herzegovina",2013 21st Telecommunications Forum Telfor (TELFOR),20140120,2013,,,881,884,"As the Internet and electronic mail continue to be utilized by an ever increasing number of users, so does fraudulent and criminal activity via the Internet and email increase. The negative effects of cybercrime activities on the use of the Internet for e-business and secure communications increased interest in studying the factors that motivate these criminals, their tactics and what can be done to mitigate their activities. The research in the area of email analysis usually focuses on two areas, email traffic analysis and email content analysis, but very poor in the area of visual analytics of emails. The paper presents the software for visualizing suspicious email messages based on the information provided in the email header (rather than the content of the email). This IP mapping tool, called MIPA, uses a Google Map to display the geographic position and integrates InfoDB, WhoIS databases, and the Google Maps API. Thus, the proposed work can be helpful for identifying and investigating suspicious email messages and also assist the investigators to get the information in time to take effective actions to reduce the criminal activities.",,Electronic:978-1-4799-1420-3; POD:978-1-4799-1418-0,10.1109/TELFOR.2013.6716371,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6716371,Cybercrime Investigation;Email client;IP geolocation;Maps API,Computer crime;Electronic mail;Geology;IP networks;Internet;Protocols;Servers,IP networks;computer crime;data visualisation;electronic mail,Google Map;IP geolocation;IP mapping tool;InfoDB;MIPA;WhoIS database;criminal activity;e-mail content analysis;e-mail header;e-mail traffic analysis;fraudulent activity;geographic position;suspicious e-mail message visualization;suspicious e-mail messages,,0,,11,,no,26-28 Nov. 2013,,IEEE,IEEE Conference Publications
Keeping requirements on track via visual analytics,N. Niu; S. Reddivari; Z. Chen,"Department of Computer Science and Engineering, Mississippi State University, USA",2013 21st IEEE International Requirements Engineering Conference (RE),20131021,2013,,,205,214,"For many software projects, keeping requirements on track needs an effective and efficient path from data to decision. Visual analytics creates such a path that enables the human to extract insights by interacting with the relevant information. While various requirements visualization techniques exist, few have produced end-to-end values to practitioners. In this paper, we advance the literature on visual requirements analytics by characterizing its key components and relationships. This allows us to not only assess existing approaches, but also create tool enhancements in a principled manner. We evaluate our enhanced tool supports through a case study where massive, heterogeneous, and dynamic requirements are processed, visualized, and analyzed. In particular, our study illuminates how increased interactivity of requirements visualization could lead to actionable decisions.",1090-705X;1090705X,Electronic:978-1-4673-5765-4; POD:978-1-4673-5763-0,10.1109/RE.2013.6636720,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6636720,Requirements management;requirements engineering visualization;visual analytical reasoning,Cognition;Data models;Data visualization;Decision making;Measurement;Software;Visualization,data analysis;data visualisation;formal specification;formal verification;project management,dynamic requirements;heterogeneous requirements;massive requirements;requirements visualization techniques;software projects;tool enhancements;visual requirements analytics,,1,,42,,no,15-19 July 2013,,IEEE,IEEE Conference Publications
Land suitability evaluation method based on GIS technology,L. Qu; Y. Shao; L. Zhang,"School of Geodesy and Geomatics, Jiangsu Normal University, China",2013 Second International Conference on Agro-Geoinformatics (Agro-Geoinformatics),20131007,2013,,,7,12,"Land suitability evaluation plays an important role in the planning of land use. With the help of GIS technology, the efficiency and the accuracy of land suitability evaluation improve greatly. The paper focuses on: 1) the method of land suitability evaluation based on GIS technology, 2) Enriching the styles of the evaluation results and improving its application value. The study of GIS-based evaluation method, proposed in this paper, includes the following aspects: 1) Collect and collate basic data in research area, 2) Build a graphics and properties database through collecting layer information by using ArcGIS software. 3) Process graphics data to generate DEM in research area, and carry out elevation analysis and slope analysis, 4) Taking agriculture, forestry and animal husbandry as land use goals, establish a land suitability evaluation index, get the weight of each index with AHP and Delphi method and divide land suitability level system. 5) Identify the minimum evaluation unit. 6 )Modeling and calculate the score of minimum evaluation unit, and finally get the land suitability rank of each evaluation unit to the specified purposes. In order to improve application value of the evaluation results, it is necessary to mine spatial data based on the evaluation results and other materials. We get some thematic maps such as arable land potential area, returning farmland to forest area, and returning farmland to pastoral areas. GIS plays an important role in this process. These materials can be used directly to serve land use planning. Multivariate analysis method is the basic method and it is proved to be effect to improve the land suitability evaluation. To get a correct result, it is important to establish index systems and select suitable indicators to divide evaluation units according to spatial scales and regional types. Given the socio-economic property of land-use, it's necessary to select socio-economic indicators.",,Electronic:978-1-4799-0868-4; POD:978-1-4799-0867-7,10.1109/Argo-Geoinformatics.2013.6621869,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6621869,Land suitability evaluation;geographical information system (GIS);land-use suitability modeling;thematic Maps,Agriculture;Forestry;Geographic information systems;Graphics;Indexes;Materials;Soil,analytic hierarchy process;data mining;digital elevation models;geographic information systems;land use planning;socio-economic effects;statistical analysis,AHP;ArcGIS software;DEM;Delphi method;GIS technology;agriculture;analytic hierarchy process;animal husbandry;arable land potential area;digital elevation model;elevation analysis;farmland;forest area;forestry;geographical information system;land suitability evaluation index;land suitability evaluation method;land suitability level system;land suitability rank;land use planning;multivariate analysis method;pastoral areas;slope analysis;socio-economic indicators;socio-economic property;spatial data mining;thematic maps,,0,,19,,no,12-16 Aug. 2013,,IEEE,IEEE Conference Publications
Large Payload Streaming Database Sort and Projection on FPGAs,B. Sukhwani; M. Thoennes; H. Min; P. Dube; B. Brezzo; S. Asaad; D. Dillenberger,"IBM T. J. Watson Res. Center, Yorktown Heights, NY, USA",2013 25th International Symposium on Computer Architecture and High Performance Computing,20140109,2013,,,25,32,"In recent years, real-time analytics has seen widespread adoption in the business world. While it provides useful business insights and improved market responsiveness, it also adds a computational burden to traditional online transaction processing (OLTP) systems. Analytics queries involve complex database operations such as sort, aggregation, and join that consume significant computational resources, and, when executed on the same system, may affect the performance of OLTP queries. In this paper, we try to address this issue by accelerating two such database operations, namely, projection and sort, using a field programmable gate array (FPGA). Our prototype is implemented on an Alter a Stratix V FPGA and achieves an order of magnitude speedup in the sort operation compared to baseline software. Furthermore, our prototype implements projection in parallel with other query operations on FPGA, thus completely eliminating the cost of projection without consuming any extra cycles on the FPGA. FPGA accelerated sort and projection have been integrated with our previous work on accelerating other query operations [1], making our analytics acceleration prototype on FPGA applicable to a wider variety of queries.",1550-6533;15506533,Electronic:978-1-4799-2928-3; POD:978-1-4799-2929-0,10.1109/SBAC-PAD.2013.21,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6702576,Analytics;Database;FPGA;Sort,Acceleration;Field programmable gate arrays;Payloads;Query processing;Sorting;Throughput,business data processing;data mining;database management systems;field programmable gate arrays;query processing;real-time systems;transaction processing,Altera Stratix V FPGA;OLTP queries;OLTP systems;business world;field programmable gate array;market responsiveness;online transaction processing;payload streaming database;real-time analytics,,1,,18,,no,23-26 Oct. 2013,,IEEE,IEEE Conference Publications
Leveraging applied materials TechEdge Prizm‰ã¢ for advanced lithography process control,P. Llanos; R. Cornell,"Advanced Software Services Applied Materials Hopewell Junction, NY",ASMC 2013 SEMI Advanced Semiconductor Manufacturing Conference,20130708,2013,,,256,261,"Presently the CDSEM metrology tool continues to be the ‰ÛÏruler of the fab‰Ûù providing metrology at over 100 steps along the way to building a modern semiconductor product. We explore the history of data volume generated with Fab CDSEM fleets. As we quantify the size of the fleet dataset, it becomes clear why historically only a subset of the rich dataset is ever utilized. We review what the treatment of the data using modern ‰ÛÏbig data‰Ûù analytics and enterprise level server hardware does to accelerate development learning, increase engineering turns per day and raise the velocity of accurate manufacturing feedback. This paper further explores ways to leverage TechEdge Prizm to extract more value out of metrology for Lithography process control. The paper will describe some of the challenges facing the changing Lithography metrology landscape and how Prizm delivers tailored analysis for these challenges. Results at a customer site demonstrate that various types of analysis of CDSEM images and data are 10 times more efficient and thorough than traditional methods.",1078-8743;10788743,Electronic:978-1-4673-5007-5; POD:978-1-4673-5006-8,10.1109/ASMC.2013.6552814,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6552814,CDSEM;Data mining;Lithography;Metrology,Big data;Lithography;Metrology;Pattern recognition;Process control;Semiconductor device measurement,lithography;process control;semiconductor device manufacture,CDSEM fleets;CDSEM metrology tool;TechEdge Prizm;applied materials;enterprise level server hardware;fleet dataset;lithography metrology landscape;lithography process control;manufacturing feedback;semiconductor product,,0,,5,,no,14-16 May 2013,,IEEE,IEEE Conference Publications
Leveraging Process-Mining Techniques,G. T. Lakshmanan; R. Khalaf,IBM T.J. Watson Research Center,IT Professional,20131010,2013,15,5,22,30,"Semi-structured processes are data-driven, human-centric, flexible processes whose execution between instances can vary dramatically. Due to their unpredictability and data-driven nature, it's becoming increasingly important to mine traces of events collected from these processes. This enables the extraction of mined process models that could help users handle new process instances. Process-mining techniques can help facilitate this goal, but it can be daunting for users new to process-aware analytics to sift through the literature and available software to determine which process-mining algorithm to use. The authors compare five process-mining algorithms and present a decision tree to help readers determine which mining algorithm to use for a specific problem. Semi-structured processes, however, present challenges that these mining techniques don't address. So, the authors also identify three key characteristics of semi-structured processes and the mining challenges they present, highlighting a selection of emerging mining approaches that can address these challenges.",1520-9202;15209202,,10.1109/MITP.2012.88,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6279446,analytics;business insight;case management;data driven;discovery;information technology;mining;monitoring;process;semi-structured,Algorithm design and analysis;Biological system modeling;Business;Data mining;Monitoring;Noise measurement;Parallel processing;Software algorithms,business data processing;data analysis;data mining;decision trees,data-driven process;decision tree;human-centric process;mined process model extraction;process-aware analytics;process-mining techniques;semi-structured processes;trace mining,,1,3,21,,no,Sept.-Oct. 2013,,IEEE,IEEE Journals & Magazines
"Leveraging the Crowd: How 48,000 Users Helped Improve Lync Performance",R. Musson; J. Richards; D. Fisher; C. Bird; B. Bussone; S. Ganguly,Microsoft,IEEE Software,20130626,2013,30,4,38,45,"Performance is a critical component of customer satisfaction with network-based applications. Unfortunately, accurately evaluating the performance of collaborative software that operates in extremely heterogeneous environments is difficult with traditional techniques such as modeling workloads or testing in controlled environments. To evaluate performance of an application in the wild during development, the authors deployed early versions of the software, collecting performance data from application users for key usage scenarios. The analysis package they used produces visualizations to help development teams identify and prioritize performance issues by focusing on performance early in the development cycle, evaluating progress, identifying defects, and estimating timelines.",0740-7459;07407459,,10.1109/MS.2013.67,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6509371,data analysis;data collection;performance monitoring;software analytics;software data visualization;software performance,Analytical models;Customer satisfaction;Performance evaluation;Software development;Software quality,groupware;software engineering,Lync performance;collaborative software;customer satisfaction;data visualization;software development,,1,,2,,no,July-Aug. 2013,,IEEE,IEEE Journals & Magazines
Library Automation in Cloud,D. G. Chandra; M. Kathing; D. P. Kumar,"DGE&T, MoLE, New Delhi, India",2013 5th International Conference and Computational Intelligence and Communication Networks,20131111,2013,,,474,479,"By combining the most effective practices of Vitalization, Grid computing, utility computing and network technologies cloud computing is the resultant cost effective computing service delivery mechanism. This survey paper discusses prospect of cloud based Library Management System for cost effective library automation. Another important aspect of this survey work is analytic discussion on various open source library automation software. CYBRARIANTM<sup>TM</sup> an internet based integrated Library Automation and Management solution developed by CR2 Technologies Ltd is also included for the benefit of implementer.",,Electronic:978-0-7695-5069-5; POD:978-1-4799-1326-8,10.1109/CICN.2013.104,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6658039,Cloud Computing;Library Management System (LMS);Library Management as a Service (LMaaS);Open Source Software,Automation;Cloud computing;Educational institutions;Libraries,cloud computing;library automation,CR2 Technologies Ltd;CYBRARIANTM;Internet based integrated library automation and management solution;cloud based library management system;cloud computing;open source library automation software,,1,,12,,no,27-29 Sept. 2013,,IEEE,IEEE Conference Publications
LLSuperCloud: Sharing HPC systems for diverse rapid prototyping,A. Reuther; J. Kepner; W. Arcand; D. Bestor; B. Bergeron; C. Byun; M. Hubbell; P. Michaleas; J. Mullen; A. Prout; A. Rosa,"Comput. & Analytics Group, MIT Lincoln Lab., Lexington, MA, USA",2013 IEEE High Performance Extreme Computing Conference (HPEC),20131121,2013,,,1,6,"The supercomputing and enterprise computing arenas come from very different lineages. However, the advent of commodity computing servers has brought the two arenas closer than they have ever been. Within enterprise computing, commodity computing servers have resulted in the development of a wide range of new cloud capabilities: elastic computing, virtualization, and data hosting. Similarly, the supercomputing community has developed new capabilities in heterogeneous, massively parallel hardware and software. Merging the benefits of enterprise clouds and supercomputing has been a challenging goal. Significant effort has been expended in trying to deploy supercomputing capabilities on cloud computing systems. These efforts have resulted in unreliable, low-performance solutions, which requires enormous expertise to maintain. LLSuperCloud provides a novel solution to the problem of merging enterprise cloud and supercomputing technology. More specifically LLSuperCloud reverses the traditional paradigm of attempting to deploy supercomputing capabilities on a cloud and instead deploys cloud capabilities on a supercomputer. The result is a system that can handle heterogeneous, massively parallel workloads while also providing high performance elastic computing, virtualization, and databases. The benefits of LLSuperCloud are highlighted using a mixed workload of C MPI, parallel MATLAB, Java, databases, and virtualized Web services.",,Electronic:978-1-4799-1365-7; POD:978-1-4799-1363-3,10.1109/HPEC.2013.6670329,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6670329,cloud computing;high performance computing;virtual machines,Cloud computing;Databases;Hardware;MATLAB;Servers;Virtual machine monitors;Virtual machining,C language;Java;Web services;application program interfaces;cloud computing;parallel processing;software prototyping;virtual machines;virtualisation,C MPI;HPC system sharing;Java;LLSuperCloud;cloud capabilities;commodity computing servers;data hosting;elastic computing;enterprise computing;heterogeneous massively parallel hardware;heterogeneous massively parallel software;high-performance computing;parallel MATLAB;rapid prototyping;supercomputing;virtualization;virtualized Web services,,6,,24,,no,10-12 Sept. 2013,,IEEE,IEEE Conference Publications
Malware detection by text and data mining,G. G. Sundarkumar; V. Ravi,"Center of Excellence in CRM & Analytics, Inst. for Dev. & Res. in Banking Technol., Hyderabad, India",2013 IEEE International Conference on Computational Intelligence and Computing Research,20140127,2013,,,1,6,"Cyber frauds are a major security threat to the banking industry worldwide. Malware is one of the manifestations of cyber frauds. Malware authors use Application Programming Interface (API) calls to perpetrate these crimes. In this paper, we propose a static analysis method to detect Malware based on API call sequences using text and data mining in tandem. We analyzed the dataset available at CSMINING group. First, we employed text mining to extract features from the dataset consisting a series of API calls. Further, mutual information is invoked for feature selection. Then, we resorted to over-sampling to balance the data set. Finally, we employed various data mining techniques such as Decision Tree (DT), Multi Layer Perceptron (MLP), Support Vector Machine (SVM), Probabilistic Neural Network (PNN) and Group Method for Data Handling (GMDH). We also applied One Class SVM (OCSVM). Throughout the paper, we used 10-fold cross validation technique for testing the techniques. We observed that SVM and OCSVM achieved 100% sensitivity after balancing the dataset.",,Electronic:978-1-4799-1597-2; POD:978-1-4799-1596-5,10.1109/ICCIC.2013.6724229,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6724229,Application Programming Interface calls;Data Mining;Mutual Information;Over Sampling;Text Mining,Accuracy;Feature extraction;Malware;Mutual information;Support vector machines;Text mining,application program interfaces;data mining;decision trees;feature extraction;invasive software;neural nets;support vector machines;text analysis,API call sequences;DT;GMDH;MLP;Malware authors;OCSVM;PNN;SVM;application programming interface;cyber frauds;data mining;decision tree;feature extraction;feature selection;group method for data handling;malware detection;multi layer perceptron;one class SVM;probabilistic neural network;security threat;static analysis method;support vector machine;text mining,,1,,50,,no,26-28 Dec. 2013,,IEEE,IEEE Conference Publications
Massively scalable near duplicate detection in streams of documents using MDSH,P. Logasa Bogen; C. T. Symons; A. McKenzie; R. M. Patton; R. E. Gillen,"Comput. Data Analytics Group, Oak Ridge Nat. Lab., Oak Ridge, TN, USA",2013 IEEE International Conference on Big Data,20131223,2013,,,480,486,"In a world where large-scale text collections are not only becoming ubiquitous but also are growing at increasing rates, near duplicate documents are becoming a growing concern that has the potential to hinder many different information filtering tasks. While others have tried to address this problem, prior techniques have only been used on limited collection sizes and static cases. We will briefly describe the problem in the context of Open Source analysis along with our additional constraints for performance. In this work we propose two variations on Multi-dimensional Spectral Hash (MDSH) tailored for working on extremely large, growing sets of text documents. We analyze the memory and runtime characteristics of our techniques and provide an informal analysis of the quality of the near-duplicate clusters produced by our techniques.",,Electronic:978-1-4799-1293-3; POD:978-1-4799-1294-0,10.1109/BigData.2013.6691610,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6691610,Big Data;MDSH;Near Duplicate Detection;Open Source Intelligence;Streaming Text,Electronic publishing;Encyclopedias;Internet;Memory management;Random access memory;Runtime,file organisation;information filtering;public domain software;text analysis,MDSH;document stream;information filtering task;large-scale text collections;memory characteristics;multidimensional spectral hash;near duplicate detection;near duplicate documents;near-duplicate clusters;open source analysis;quality informal analysis;runtime characteristics;text documents,,0,,32,,no,6-9 Oct. 2013,,IEEE,IEEE Conference Publications
Master Data Management and Data Warehouse: An architectural approach for improved decision-making,E. G. e. R. M. Barros,"Dept. de Comput., Univ. Estadual de Londrina, Londrina, Brazil",2013 8th Iberian Conference on Information Systems and Technologies (CISTI),20131017,2013,,,1,4,"In corporate environments it is common to have multiple data sources that represent the same real world entity (such as customers, suppliers, products, etc.). This situation can also occur for external data (social networks, regulatory institutions). The extraction activity has the need to decide among the available sources, how best to obtain a given data. This paper presents an architectural approach to dealing with the problem of multiple data sources with the objective of improving the data quality of a Data Warehouse in conjunction with the presentation of a set of best practices to identify and implement the process of Master Data Management (MDM).",2166-0727;21660727,Electronic:978-989-98434-0-0; POD:978-1-4799-1217-9,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6615708,Business Intelligence;Data Quality;Data Warehouse;Decision Support Systems;Master Data Management,Analytic hierarchy process;Bismuth;Data mining;Data warehouses;Decision support systems;Software,data handling;data warehouses;decision making,MDM;architectural approach;data quality;data warehouse;decision-making;extraction activity;master data management,,0,,13,,no,19-22 June 2013,,IEEE,IEEE Conference Publications
MC2: Map Concurrency Characterization for MapReduce on the Cloud,M. Hammoud; M. F. Sakr,"Carnegie Mellon Univ. in Qatar, Doha, Qatar",2013 IEEE Sixth International Conference on Cloud Computing,20131202,2013,,,17,26,"MapReduce is now a pervasive analytics engine on the cloud. Hadoop is an open source implementation of MapReduce and is currently enjoying wide popularity. Hadoop offers a high-dimensional space of configuration parameters, which makes it difficult for practitioners to set for efficient and cost-effective execution. In this work we observe that MapReduce application performance is highly influenced by map concurrency. Map concurrency is defined in terms of two configurable parameters, the number of available map slots and the number of map tasks running over the slots. We show that some inherent MapReduce characteristics enable well-informed prediction of map concurrency. We propose Map Concurrency Characterization (MC<sup>2</sup>), a standalone utility program that can predict the best map concurrency for any given MapReduce application. By leveraging the generated predicted information, MC<sup>2</sup> can judiciously guide Map phase configuration and, consequently, improve Hadoop performance. Unlike many of relevant schemes, MC<sup>2</sup> does not employ simulation, dynamic instrumentation, and/or static analysis of unmodified job code to predict map concurrency. In contrast, MC<sup>2</sup> utilizes a simple, yet effective mathematical model, which exploits the MapReduce characteristics that impact map concurrency. We implemented MC<sup>2</sup> and conducted comprehensive experiments on a private cloud and on Amazon MC<sup>2</sup> using Hadoop 0.20.2. Our results show that MC<sup>2</sup> can correctly predict the best map concurrencies for the tested benchmarks and provide up to 2.2X speedup in runtime.",2159-6182;21596182,Electronic:978-0-7695-5028-2; POD:978-1-4799-0490-7,10.1109/CLOUD.2013.93,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676673,Hadoop;Map Concurrency;Map Concurrency Characterization;MapReduce,Benchmark testing;Concurrent computing;Engines;Equations;Mathematical model;Runtime;Time factors,cloud computing;concurrency control;data analysis;public domain software,Amazon EC2;Hadoop 0.20.2;Hadoop performance;MC<sup>2</sup> standalone utility program;Map phase configuration;MapReduce application;configurable parameters;configuration parameters;cost-effective execution;high-dimensional space;map concurrency characterization;map slots;map tasks;open source implementation;pervasive analytics engine;predicted information;private cloud;static analysis;unmodified job code,,0,,38,,no,June 28 2013-July 3 2013,,IEEE,IEEE Conference Publications
Message from the PROMISE 2013 Chairs,B. Turhan; S. Wagner; A. Bener; M. D. Penta; Y. Yang,,2013 ACM / IEEE International Symposium on Empirical Software Engineering and Measurement,20131212,2013,,,394,394,"PROMISE conference is an annual forum for researchers and practitioners to present, discuss and exchange ideas, results, expertise and experiences in construction and/or application of prediction models in software engineering. Such models could be targeted at: planning, design, implementation, testing, maintenance, quality assurance, evaluation, process improvement, management, decision making, and risk assessment in software and systems development. PROMISE is distinguished from similar forums with its public data repository and focus on methodological details, providing a unique interdisciplinary venue for software engineering and machine learning communities, and seeking for verifiable and repeatable prediction models that are useful in practice.",1949-3770;19493770,Electronic:978-0-7695-5056-5; POD:978-1-4799-1144-8,10.1109/ESEM.2013.7,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6681384,predictive models; defect prediction; effort estimation; software analytics; empirical software engineering,,,,,0,,,,no,10-11 Oct. 2013,,IEEE,IEEE Conference Publications
Methods and Metrics for Evaluating Analytic Insider Threat Tools,F. L. Greitzer; T. A. Ferryman,"PsyberAnalytix, Richland, WA, USA",2013 IEEE Security and Privacy Workshops,20130722,2013,,,90,97,"The insider threat is a prime security concern for government and industry organizations. As insider threat programs come into operational practice, there is a continuing need to assess the effectiveness of tools, methods, and data sources, which enables continual process improvement. This is particularly challenging in operational environments, where the actual number of malicious insiders in a study sample is not known. The present paper addresses the design of evaluation strategies and associated measures of effectiveness; several quantitative/statistical significance test approaches are described with examples, and a new measure, the Enrichment Ratio, is proposed and described as a means of assessing the impact of proposed tools on the organization's operations.",,Electronic:978-0-7695-5017-6; POD:978-1-4799-0458-7,10.1109/SPW.2013.34,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6565235,assessment;evaluation;insider threat;metrics;validation,Data models;Measurement;Monitoring;Predictive models;Sociology;Statistics;Testing,security of data;software metrics;software process improvement,analytic insider threat tool evaluation method;analytic insider threat tool evaluation metrics;continual process improvement;data sources;enrichment ratio;government organizations;industry organizations;insider threat programs;malicious insiders;quantitative significance test;statistical significance test,,2,,21,,no,23-24 May 2013,,IEEE,IEEE Conference Publications
Mining student repositories to gain learning analytics. An experience report,G. Robles; J. M. Gonzalez-Barahona,"GSyC/LibreSoft, Universidad Rey Juan Carlos, Madrid, Spain",2013 IEEE Global Engineering Education Conference (EDUCON),20130613,2013,,,1249,1254,"Engineering students often have to deliver small computer programs in many engineering courses. Instructors have to evaluate these assignments according to the learning goals and their quality, but ensure as well that there is no plagiarism. In this paper, we report the experience of using mining software repositories techniques in a multimedia networks course where students have to submit several software programs. We show how we have proceeded, the tools that we have used and provide some useful links and ideas that other lecturers may use.",2165-9559;21659559,Electronic:978-1-4673-6110-1; POD:978-1-4673-6111-8; USB:978-1-4673-6109-5,10.1109/EduCon.2013.6530267,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6530267,automatic assessment;e-learning;mining software repositories;plagiarism;software analytics,Conferences;Engineering education,computer aided instruction;data mining;educational courses;engineering education;multimedia systems,computer programs;engineering courses;engineering students;learning analytics;mining software repositories techniques;multimedia network course;plagiarism;software programs;student repository mining,,0,,14,,no,13-15 March 2013,,IEEE,IEEE Conference Publications
Mobile malware visual analytics and similarities of Attack Toolkits (Malware gene analysis),A. Paturi; M. Cherukuri; J. Donahue; S. Mukkamala,"Institute for Complex Additive Systems Analysis, Computational Analysis and Network Enterprise Solutions (CAaNES), USA",2013 International Conference on Collaboration Technologies and Systems (CTS),20130725,2013,,,149,154,"We use Normalized Compression Distance (NCD) (owing to its capabilities to perform similarity measure of unstructured data) to enumerate code similarity between malicious Android apps and visualize their clusters. Our classification methods and visual analytics can help the antivirus community to ensure that a variant of a known malware can still be detected without the need of creating a signature. We also present when a new malware is released, our methods can be used to understand the similarity/behavior with known malware families.",,Electronic:978-1-4673-6404-1; POD:978-1-4673-6403-4,10.1109/CTS.2013.6567221,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6567221,NCD;attack toolkits;component mobile malware;cosine similarity;similarity measures;web malware,Androids;Feature extraction;Humanoid robots;Mobile communication;Trojan horses;Vectors,data analysis;data visualisation;invasive software;mobile computing;operating systems (computers),Android application;NCD;attack toolkit;code similarity;data similarity measure;malware family;malware gene analysis;mobile malware;normalized compression distance;visual analytics,,1,,14,,no,20-24 May 2013,,IEEE,IEEE Conference Publications
Model-driven software design for smart grid data analytics,H. V. Haghi; M. A. Golkar; H. Haeri,"K.N. Toosi Univ., Tehran, Iran",22nd International Conference and Exhibition on Electricity Distribution (CIRED 2013),20131216,2013,,,1,4,"Practical data analytics for the smart grid requires a software platform which enables the distribution companies to better correlate the projects, improve understating of system requirements and simplify system design by decomposing its complexity and large-scale data. This paper proposes the model-driven software design (MDSD) to managing and optimizing the smart grid data. It defines a language for visualizing, specifying, analyzing, and documenting the distributed object-oriented data. An MDSD experience is reported that employs three software environments interfaced with each other in order to create a chain of the desired methods for an energy efficiency program. The aim is to transform the data into actionable decisions accessible and understandable by distribution companies by taking software development into a higher level of abstraction. As a pilot project, a comprehensive data logging were performed for an MV/LV distribution system. Subsequently, the technical losses were studied using the developed software package. A Java-based GUI, MATLAB/Simulink, and an embedded C-based data management and calculation module are used in the proposed MDSD.",,,10.1049/cp.2013.0677,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6683280,,,Java;data analysis;data loggers;data visualisation;distribution networks;graphical user interfaces;object-oriented methods;power engineering computing;smart power grids;software architecture;software packages,Java-based GUI;MDSD;MV-LV distribution system;Matlab-Simulink;comprehensive data logging;distributed object-oriented data visualization;distribution company;embedded C-based data management;energy efficiency program;model-driven software design;smart grid data analytics;software development;software package;software platform,,0,,,,no,10-13 June 213,,IET,IET Conference Publications
Models for Measuring Access Security of Web Application: Security Reference Model,A. C. Guerra; M. S. d. P. PessÌ«a,"Center for Inf. Technol. Renato Archer (CTI), Campinas, Brazil",2013 International Conference on Social Computing,20140102,2013,,,1030,1033,Measurement units and knowledge of security properties are hardly known. This process causes the complex systems decomposition into simpler and smaller systems thus allowing the estimative of properties that will help the understanding and measurement of software systems security properties. This process provides the security model and the score of security attributes priority is calculated by AHP methodology. A security model example to illustrate this approach is presented.,,Electronic:978-0-7695-5137-1; POD:978-1-4799-1519-4,10.1109/SocialCom.2013.166,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693463,AHP;Security model;security measures;software security properties,Biological system modeling;Biomedical measurement;IEC standards;ISO standards;Security;Software;Software measurement,Internet;analytic hierarchy process;security of data,AHP methodology;Web application;access security measurement;analytical hierarchy processing;measurement units;security properties;security reference model,,0,,16,,no,8-14 Sept. 2013,,IEEE,IEEE Conference Publications
MotionExplorer: Exploratory Search in Human Motion Capture Data Based on Hierarchical Aggregation,J. Bernard; N. Wilhelm; B. KrÌ_ger; T. May; T. Schreck; J. Kohlhammer,Fraunhofer Institute for Computer Graphics Research Darmstadt,IEEE Transactions on Visualization and Computer Graphics,20131016,2013,19,12,2257,2266,"We present MotionExplorer, an exploratory search and analysis system for sequences of human motion in large motion capture data collections. This special type of multivariate time series data is relevant in many research fields including medicine, sports and animation. Key tasks in working with motion data include analysis of motion states and transitions, and synthesis of motion vectors by interpolation and combination. In the practice of research and application of human motion data, challenges exist in providing visual summaries and drill-down functionality for handling large motion data collections. We find that this domain can benefit from appropriate visual retrieval and analysis support to handle these tasks in presence of large motion data. To address this need, we developed MotionExplorer together with domain experts as an exploratory search system based on interactive aggregation and visualization of motion states as a basis for data navigation, exploration, and search. Based on an overview-first type visualization, users are able to search for interesting sub-sequences of motion based on a query-by-example metaphor, and explore search results by details on demand. We developed MotionExplorer in close collaboration with the targeted users who are researchers working on human motion synthesis and analysis, including a summative field study. Additionally, we conducted a laboratory design study to substantially improve MotionExplorer towards an intuitive, usable and robust design. MotionExplorer enables the search in human motion capture data with only a few mouse clicks. The researchers unanimously confirm that the system can efficiently support their work.",1077-2626;10772626,,10.1109/TVCG.2013.178,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634102,Data collection;Data visualization;Databases;Time series analysis;Visual analytics;cluster glyph;data aggregation;exploratory search;motion capture data;multivariate time series,Data collection;Data visualization;Databases;Time series analysis;Visual analytics,data visualisation;image motion analysis;image retrieval;image sequences;interactive systems;time series,MotionExplorer;animation;combination;data exploration;data navigation;data search;data visualization;drill-down functionality;exploratory analysis system;exploratory search system;hierarchical aggregation;human motion capture data;human motion sequences;interactive aggregation;interpolation;medicine;motion capture data collection;motion state analysis;motion state visualization;motion transition analysis;motion vector synthesis;multivariate time series data;query-by-example metaphor;sports;visual analysis;visual retrieval;visual summary,"Actigraphy;Algorithms;Computer Graphics;Data Interpretation, Statistical;Databases, Factual;Humans;Information Storage and Retrieval;Movement;Software;User-Computer Interface",11,,44,,no,Dec. 2013,,IEEE,IEEE Journals & Magazines
Multi-perspective Process Variability: A Case for Smart Green Buildings (Short Paper),A. Murguzur; H. L. Truong; S. Dustdar,"Inf. Technol. Area, IK4-Ikerlan Res. Center, Eibar, Spain",2013 IEEE 6th International Conference on Service-Oriented Computing and Applications,20140123,2013,,,25,29,"The variability scale in large-scale Cyber-Physical Systems (CPSs) is high and complex due to the voluminousness, dynamicity and diversity of available computing resources (people, things and software services), domain-specific processes, domain-specific elements (stakeholders, assets and contracts), and their relationships. This requires us to go beyond current variability modeling and management techniques which neglect the complexity and the diversity of relevant stakeholders, data and assets, and thus cannot cope with intelligent business and analytics requirements in dynamic environments, such as smart city management. In this paper, we present a comprehensive analysis for understanding the multi-perspective variability in processes atop people, data and things in CPSs, particularly, for the sustainability governance of Smart Green Buildings (SGBs). We examine domain-specific processes and domain-specific elements and their relationships to derive a multiple-perspective variability management for SGBs. On the basis of this, we conceptualize a novel model for the multi-perspective process variability representation.",2163-2871;21632871,Electronic:978-1-4799-2702-9; POD:978-1-4799-3172-9,10.1109/SOCA.2013.40,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6717280,process configuration;smart building;smart city;variability,Buildings;Contracts;Data models;Energy consumption;Maintenance engineering;Monitoring;Process control,building management systems;sustainable development,CPS;SGB;analytics requirements;computing resources;cyber-physical systems;domain-specific elements;domain-specific processes;intelligent business requirements;multiperspective process variability representation;smart city management;smart green buildings;sustainability governance;variability modeling,,0,,14,,no,16-18 Dec. 2013,,IEEE,IEEE Conference Publications
Multivariate Network Exploration with JauntyNets,I. Jusufi; A. Kerren; B. Zimmer,"Dept. of Comput. Sci., Linnaeus Univ., Vaxju, Sweden",2013 17th International Conference on Information Visualisation,20131202,2013,,,19,27,"The amount of data produced in the world every day implies a huge challenge in understanding and extracting knowledge from it. Much of this data is of relational nature, such as social networks, metabolic pathways, or links between software components. Traditionally, those networks are represented as node-link diagrams or matrix representations. They help us to understand the structure (topology) of the relational data. However in many real world data sets, additional (often multidimensional) attributes are attached to the network elements. One challenge is to show these attributes in context of the underlying network topology in order to support the user in further analyses. In this paper, we present a novel approach that extends traditional force-based graph layouts to create an attribute-driven layout. In addition, our prototype implementation supports interactive exploration by introducing clustering and multidimensional scaling into the analysis process.",1550-6037;15506037,Electronic:978-0-7695-5049-7; POD:978-1-4799-0834-9,10.1109/IV.2013.3,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676538,attribute-driven layout;force-based layouts;graph drawing;interaction;multivariate networks;network visualization;visual analytics,,data visualisation;graph theory;matrix algebra;social networking (online),JauntyNets;attribute-driven layout;force-based graph layouts;matrix representations;metabolic pathways;multidimensional attributes;multivariate network exploration;node-link diagrams;social networks;software components links,,1,,33,,no,16-18 July 2013,,IEEE,IEEE Conference Publications
MVSE: A Multi-core Video decoder System level analytics Engine,D. Y. Chen; C. C. Ju; C. T. Ho; C. H. Tsai,"Multimedia Development Division, Mediatek Inc., No. 1, Dusing Rd. 1, Hsinchu Science Park, Hsinchu, Taiwan, R.O.C.","2013 International Symposium onVLSI Design, Automation, and Test (VLSI-DAT)",20130617,2013,,,1,4,"Multi-core platform has become a trend in hand-held embedded systems, such as smartphone and tablet. To improve the video decoding performance by using the multiple cores, one of parallel algorithms should be adopted. However, different parallel algorithm should be selected for different video standard on different platform. Therefore, an engine to estimate performance on a target platform from existing single-thread video decoder is very helpful. This paper proposes a Multi-core Video decoder System level analytics Engine (MVSE) to estimate the performance on a target multi-core platform. In the MVSE, a general video decoder runs according to profiling data and macroblock information by three major parallel algorithms. The profiling data and macroblock information are obtained from existing single-thread video decoder so that the MVSE can support different video standard. The MVSE runs on target platform to consider the effect of memory access contention and cache intercommunication, which are traditionally difficult to estimate. Our experimental result shows the MVSE estimation is accuracy enough. The estimation results from MVSE shows the best speedup ratio is 1.7 times in a dual-core platform and 2.9 times in a quad-core platform for H.264 720p decoding. In addition, MVSE is also helpful for hardware and software co-design in heterogeneous computing. The experimental results show the best performance is improved by VLD hardware, and the speedup ratio is 2.3 times in a dual-core platform and 3.9 times in a quad-core platform.",,Electronic:978-1-4673-4436-4; POD:978-1-4673-4435-7,10.1109/VLDI-DAT.2013.6533859,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6533859,,Decoding;Estimation;Hardware;Multicore processing;Parallel algorithms;Software;Streaming media,data compression;hardware-software codesign;microprocessor chips;video coding,H.264 720p decoding;MVSE estimation;cache intercommunication;dual-core platform;hand-held embedded systems;hardware codesign;macroblock information;memory access contention;multicore video decoder system level analytics engine;multiple cores;parallel algorithms;quadcore platform;single-thread video decoder;software codesign,,0,,9,,no,22-24 April 2013,,IEEE,IEEE Conference Publications
Need 4 Speed: Leverage New Metrics to Boost Your Velocity without Compromising on Quality,R. Tabib,"Hewlett Packard, Sunnyvale, CA, USA",2013 Agile Conference,20130930,2013,,,117,120,"As Agile becomes the de-facto SDLC practice, it has become evident that additional practices are required to allow enterprises to realize the premise of it. In this experience report, we will share the practices learnt and exercised over the past 3 years that helped us cope with the common challenges of large scale enterprise projects. The concept of ""Application Lifecycle Intelligence"" discussed in this paper covers the key principles of these best practices. It shares the additional analytics and metrics that allowed HP to enable cross time zones collaboration and alignment through transparency and provides insights into quality and risk. The key principal accomplished by extracting data from various practitioners tools and surface it in an actionable format for the various stakeholders.",,Electronic:978-0-7695-5076-3; POD:978-1-4799-1017-5,10.1109/AGILE.2013.32,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6612886,ALM;Analytics;Enterprise Agile;Scaled Agile,Collaboration;Data mining;Heating;Market research;Measurement;Project management;Testing,data handling;software metrics;software prototyping,data extraction;defacto SDLC practice;leverage new metrics;lifecycle intelligence application;need 4 speed;time zones collaboration,,0,,2,,no,5-9 Aug. 2013,,IEEE,IEEE Conference Publications
On evaluation of a multiscale-based CT image analysis and visualisation algorithm,K. P. Lam; D. J. Collins; J. Sule-Suso; R. Bhana; A. Moloney,"Sch. of Comput. & Math., Univ. of Keele, Keele, UK",2013 6th International Conference on Biomedical Engineering and Informatics,20140224,2013,,,148,153,"Development of computed tomography (CT) protocols that minimise radiation dose for specific clinical treatments continues to be a major research focus. Building on the success of an earlier collaborative case study concerning prostate cancer diagnosis with pelvis CT images, this paper presents our evaluation results of a multi-scale texture analytic procedure developed to aid detection of specific anatomical features (abnormality, lesions, etc) in such images. This is a critical step in realising an intelligent and integrated image visualisation platform which will facilitate the construction of highly customised and personalised treatment plans. The ultimate aim is to provide in a single framework optimal software techniques for treatment planning, CT image-guided positioning and treatment delivery.",1948-2914;19482914,Electronic:978-1-4799-2761-6; POD:978-1-4799-2762-3,10.1109/BMEI.2013.6746924,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6746924,Active contours and Level sets;Mutliscale feature analysis;ground truth;image segmentation;radiotherapy treatment planning,Biomedical imaging;Computed tomography;Educational institutions;Feature extraction;Image segmentation;Planning;Shape,cancer;computerised tomography;dosimetry;feature extraction;image texture;medical image processing;radiation therapy,CT image-guided positioning;abnormality;anatomical feature detection;computed tomography protocols;integrated image visualisation platform;intelligent image visualisation platform;lesions;multiscale texture analytic procedure;multiscale-based CT image analysis;pelvis CT images;personalised treatment plans;prostate cancer diagnosis;radiation dose minimisation;specific clinical treatments;treatment delivery;visualisation algorithm,,0,,23,,no,16-18 Dec. 2013,,IEEE,IEEE Conference Publications
On mixing high-speed updates and in-memory queries: A big-data architecture for real-time analytics,T. Zhong; K. A. Doshi; X. Tang; T. Lou; Z. Lu; H. Li,,2013 IEEE International Conference on Big Data,20131223,2013,,,102,109,"Up-to-date business intelligence has become a critical differentiator for the modern data-driven highly engaged enterprise. It requires rapid integration of new information on a continuous basis for subsequent analyses. ETL-based and traditionally batch-processing oriented methods of absorbing changes into a relational database schema take time, and are therefore incompatible with very low-latency demands of realtime analytics. Instead, in-memory clustered stores that employ tunable consistency mechanisms are becoming attractive since they dispense with the need to transform and transit data between storage layouts and tiers. When data is updated infrequently, in-memory approaches such as RDD transformations in Spark can suffice, but as updates become frequent, such in-memory approaches need to be extended to support dynamic datasets. This paper describes a few key additional requirements that result from having to support in-memory processing of data while updates proceed concurrently. The paper describes Real-time Analytics Foundation (RAF), an architecture to meet the new requirements. Performance of an early implementation of RAF is also described: for an unaudited TPC-H derived workload, RAF shows a node-to-node scaling ratio of 88% at 8 nodes, and for a query equivalent to Q6 in the TPC-H set, RAF is able to show 9x improvement over that of Hive-Hadoop. The paper also describes two RAF based solutions that are being put together by two independent software vendors in China.",,Electronic:978-1-4799-1293-3; POD:978-1-4799-1294-0,10.1109/BigData.2013.6691704,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6691704,Analytics;Big Data;CRUD;Clustering;Low-latency;Real-time;Resilient Distributed Datasets,Data handling;Data storage systems;Distributed databases;Information management;Memory management;Real-time systems;Software,DP industry;competitive intelligence;query processing;real-time systems;relational databases,China;Hive-Hadoop;RAF;TPC-H;big-data architecture;business intelligence;data-driven highly engaged enterprise;high-speed updates;in-memory queries;real-time analytics foundation;relational database;software vendors,,1,,19,,no,6-9 Oct. 2013,,IEEE,IEEE Conference Publications
On the Hardware/Software Design and Implementation of a High Definition Multiview Video Surveillance System,S. C. Chan; S. Zhang; J. F. Wu; H. J. Tan; J. Q. Ni; Y. S. Hung,"Department of Electrical and Electronic Engineering, The University of Hong Kong, Hong Kong",IEEE Journal on Emerging and Selected Topics in Circuits and Systems,20130607,2013,3,2,248,262,"This paper proposes a distributed architecture for high definition multiview video surveillance system. It adopts a modular design where single view/stereo intelligent internet protocol (IP)-based video surveillance cameras are connected to a front-end field programmable gate array (FPGA) board(s) which are connected to a back-end local video server through the IP network. The data intensive video analytics (VA) algorithms such as background modeling, connected component labeling and single view object tracking are implemented in the FPGA using an efficient fix-point based architecture. Each back-end video server is equipped with a storage and graphics processing units for supporting high-level VA and other processing algorithms such as video decompression/display, mean depth estimation and consistent labeling. A real-time prototype system was constructed to illustrate the architecture and VA algorithms involved. Satisfactory results were obtained for both publicly available data set and real surveillance video data.",2156-3357;21563357,,10.1109/JETCAS.2013.2256822,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6515196,Field programmable gate array (FPGA);graphics processing unit (GPU);intelligent video surveillance (IVS);internet protocol (IP) camera;object tracking;video analytics (VA),,IP networks;field programmable gate arrays;graphics processing units;hardware-software codesign;high definition video;object tracking;video cameras;video servers;video surveillance,FPGA;IP network;back-end video server;background modeling;data intensive video analytics;field programmable gate arrays;fix-point based architecture;graphics processing units;hardware-software design;high definition multiview video surveillance system;local video server;mean depth estimation;modular design;single view object tracking;stereo intelligent Internet protocol based video surveillance cameras;video decompression,,3,,32,,no,13-Jun,,IEEE,IEEE Journals & Magazines
OpenCV based road sign recognition on Zynq,M. Russell; S. Fischaber,"Analytics Engines, Belfast, Northern Ireland",2013 11th IEEE International Conference on Industrial Informatics (INDIN),20131010,2013,,,596,601,"Road sign recognition is a key component in autonomous vehicles and also has applications in driver assistance systems and road sign maintenance. Here an algorithm is presented using the Xilinx Zynq-7020 chip on a Zedboard to scan 1920ÌÑ1080 images taken by an ON Semiconductor VITA-2000 sensor attached via the FMC slot. The PL section of the Zynq is used to perform essential image pre-processing functions and color based filtering of the image. Software classifies the shapes in the filtered image, and OpenCV's template matching function is used to identify the signs from a database of UK road signs. The system was designed in six weeks, and can process one frame in approximately 5 seconds. This is a promising start for a real-time System on Chip based approach to the problem of road sign recognition and also for using the Zynq platform for rapid deployment of these types of applications.",1935-4576;19354576,Electronic:978-1-4799-0752-6; POD:978-1-4799-0750-2; USB:978-1-4799-0751-9,10.1109/INDIN.2013.6622951,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6622951,,Field programmable gate arrays;Hardware;Image color analysis;Image segmentation;Roads;Shape;Software,filtering theory;image classification;image colour analysis;public domain software;road vehicles;system-on-chip;traffic engineering computing,FMC slot;OpenCV-based road sign recognition;UK road sign database;Xilinx Zynq-7020 chip;Zedboard;Zynq PL section;autonomous vehicles;color-based image filtering;driver assistance systems;filtered image shape classification;image preprocessing functions;image scanning;on-semiconductor VITA-2000 sensor;real-time system-on-chip-based approach;road sign identification;road sign maintenance;template matching function,,3,,26,,no,29-31 July 2013,,IEEE,IEEE Conference Publications
Optimal Clustering of Time Periods for Electricity Demand-Side Management,D. F. Rogers; G. G. Polak,"Department of Operations, Business Analytics, and Information Systems, Carl H. Lindner College of Business, University of Cincinnati, Cincinnati, OH, USA",IEEE Transactions on Power Systems,20131017,2013,28,4,3842,3851,"Several pure binary integer optimization models are developed for clustering time periods by similarity for electricity utilities seeking assistance with pricing strategies. The models include alternative objectives for characterizing various notions of within-cluster distances, admit as feasible only clusters that are contiguous, and allow for circularity, where time periods at the beginning and end of the planning cycle may be in the same cluster. Restrictions upon cluster size may conveniently be included without the need of additional constraints. The models are populated with a real-world dataset of electricity usage for 93 buildings and solutions and run-times attained by conventional optimization software are compared with those by dynamic programming, or by a greedy algorithm applicable to one of the models, that run in polynomial time. The results provide time-of-use segments that an electricity utility may employ for selective pricing for peak and off-peak time periods to influence demand for the purpose of load leveling.",0885-8950;08858950,,10.1109/TPWRS.2013.2252373,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6522912,Demand-side management;dynamic programming;electricity supply industry;integer linear programming;load management;mathematical programming;minimax techniques;optimization;power system planning,Dynamic programming;Electricity supply industry;Integer linear programming;Load management;Mathematical programming;Minimax techniques;Optimization;Power system planning,demand side management;dynamic programming;greedy algorithms;integer programming;polynomials;pricing,binary integer optimization models;dynamic programming;electricity demand-side management;electricity usage real-world dataset;electricity utility;greedy algorithm;load leveling;off-peak time periods;optimal time periods clustering;optimization software;peak time periods;polynomial time;pricing strategies,,5,,35,,no,Nov. 2013,,IEEE,IEEE Journals & Magazines
Optimizations and Analysis of BSP Graph Processing Models on Public Clouds,M. Redekopp; Y. Simmhan; V. K. Prasanna,"Univ. of Southern California, Los Angeles, CA, USA",2013 IEEE 27th International Symposium on Parallel and Distributed Processing,20130729,2013,,,203,214,"Large-scale graph analytics is a central tool in many fields, and exemplifies the size and complexity of Big Data applications. Recent distributed graph processing frameworks utilize the venerable Bulk Synchronous Parallel (BSP) model and promise scalability for large graph analytics. This has been made popular by Google's Pregel, which provides an architecture design for BSP graph processing. Public clouds offer democratized access to medium-sized compute infrastructure with the promise of rapid provisioning with no capital investment. Evaluating BSP graph frameworks on cloud platforms with their unique constraints is less explored. Here, we present optimizations and analyses for computationally complex graph analysis algorithms such as betweenness-centrality and all-pairs shortest paths on a native BSP framework we have developed for the Microsoft Azure Cloud, modeled on the Pregel graph processing model. We propose novel heuristics for scheduling graph vertex processing in swaths to maximize resource utilization on cloud VMs that lead to a 3.5x performance improvement. We explore the effects of graph partitioning in the context of BSP, and show that even a well partitioned graph may not lead to performance improvements due to BSP's barrier synchronization. We end with a discussion on leveraging cloud elasticity for dynamically scaling the number of BSP workers to achieve a better performance than a static deployment, and at a significantly lower cost.",1530-2075;15302075,Electronic:978-0-7695-4971-2; POD:978-1-4673-6066-1,10.1109/IPDPS.2013.76,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569812,Betweennness centrality;Bulk Synchronous Parallel;Cloud computing;Graph analytics;MapReduce;Pregel,Algorithm design and analysis;Cloud computing;Computational modeling;Optimization;Programming;Scalability;Synchronization,cloud computing;graph theory;optimisation;software architecture;virtual machines,BSP barrier synchronization;BSP graph processing;BSP graph processing models;Google Pregel;Microsoft Azure cloud;Pregel graph processing model;all-pairs shortest paths;architecture design;betweenness-centrality;big data applications;capital investment;cloud VM;cloud platforms;complex graph analysis algorithms;distributed graph processing frameworks;large-scale graph analytics;medium-sized compute infrastructure;optimizations;public clouds;venerable bulk synchronous parallel model,,8,,28,,no,20-24 May 2013,,IEEE,IEEE Conference Publications
Overcoming Limitations of Off-the-Shelf Priority Schedulers in Dynamic Environments,F. Yan; S. Hughes; A. Riska; E. Smirni,"Coll. of William & Mary, Williamsburg, VA, USA","2013 IEEE 21st International Symposium on Modelling, Analysis and Simulation of Computer and Telecommunication Systems",20140203,2013,,,505,514,"It is common nowadays to architect and design scaled-out systems with off-the-shelf computing components operated and managed by off-the-shelf open-source tools. While web services represent the critical set of services offered at scale, big data analytics is emerging as a preferred service to be colocated with cloud web services at a lower priority raising the need for off-the-shelf priority scheduling. In this paper we report on the perils of Linux priority scheduling tools when used to differentiate between such complex services. We demonstrate that simple priority scheduling utilities such as nice and ionice can result in dramatically erratic behavior. We provide a remedy by proposing an autonomic priority scheduling algorithm that adjusts its execution parameters based on on-line measurements of the current resource usage of critical applications. Detailed experimentation with a user-space prototype of the algorithm on a Linux system using popular benchmarks such as SPEC and TPC-W illustrate the robustness and versatility of the proposed technique, as it provides consistency to the expected performance of a high-priority application when running simultaneously with multiple low priority jobs.",1526-7539;15267539,Electronic:978-0-7695-5102-9; POD:978-1-4799-1209-4,10.1109/MASCOTS.2013.72,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6730807,Linux priority utilities;background work;priority scheduling;workload characterization,Benchmark testing;Computers;Linux;Monitoring;Scheduling;Scheduling algorithms;Time factors,Big Data;Linux;Web services;cloud computing;data analysis;fault tolerant computing;public domain software;scheduling,Linux priority scheduling tools;SPEC;TPC-W;autonomic priority scheduling algorithm;big data analytics;cloud Web services;critical application resource usage online measurement;dynamic environments;off-the-shelf computing components;off-the-shelf open-source tools;off-the-shelf priority scheduler;off-the-shelf priority scheduling;scaled-out systems;user-space prototype,,1,,28,,no,14-16 Aug. 2013,,IEEE,IEEE Conference Publications
Pacer: A Progress Management System for Live Virtual Machine Migration in Cloud Computing,J. Zheng; T. S. E. Ng; K. Sripanidkulchai; Z. Liu,"Rice Univ., Houston, TX, USA",IEEE Transactions on Network and Service Management,20131211,2013,10,4,369,382,"Live migration of virtual machines is a key management function in cloud computing. Unfortunately, no live migration progress management system exists in the state-of-theart, leading to (1) guesswork over how long a migration might take and the inability to schedule dependent tasks accordingly; (2) unacceptable application degradation when application components become split over distant cloud datacenters for an arbitrary period during migration; (3) inability to tradeoff application performance and migration time e.g. to finish migration later for less impact on application performance. Pacer is the first migration progress management system that solves these problems. Pacer's techniques are based on robust and lightweight run-time measurements of system and workload characteristics, efficient and accurate analytic models for progress predictions, and online adaptation to maintain user-defined migration objectives for coordinated and timely migrations. Our experiments on a local testbed and on Amazon EC2 show that Pacer is highly effective under a range of application workloads and network conditions.",1932-4537;19324537,,10.1109/TNSM.2013.111013.130522,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6662353,Live migration;cloud computing;datacenter;progress management,Cloud computing;Prediction algorithms;Virtual machine monitors;Virtual machining;Virtualization;Web servers,cloud computing;computer centres;scheduling;software maintenance;software management;virtual machines,Amazon EC2;Pacer;application performance tradeoff;cloud computing;dependent task scheduling;distant cloud datacenters;live virtual machine migration;migration progress management system;migration time tradeoff;progress predictions;unacceptable application degradation;user-defined migration objectives,,7,,56,,no,13-Dec,,IEEE,IEEE Journals & Magazines
Parallel techniques for improving three-dimensional models storing and accessing performance,H. Luan; M. Zhou; Y. Fu,"Beijing Normal Univ., Beijing, China",2013 Ninth International Conference on Natural Computation (ICNC),20140519,2013,,,1177,1182,"Nowadays, the volume of multimedia and unstructured data has grown rapidly. More and more three-dimensional (3D) models are created for ever increasing applications. New storage and processing technologies are needed to keep pace with the continuous growth of big data. Hadoop is an attractive and open-source platform for large-scale data storage and analytics. Our previous research work has applied Hadoop distributed file system to efficiently manage 3D data for a 3D model retrieval system. To take better advantages of Hadoop, in this paper we propose two parallel strategies to improve the storing and accessing performance of 3D models. The MapReduce paradigm is adopted to provide a coarse grained parallelism for data loading, and a lightweight multithreaded algorithm is presented for data accesses. We conduct an extensive performance study on a cluster and the results show that significant performance increase can be gained for the parallel techniques.",2157-9555;21579555,Electronic:978-1-4673-4714-3; POD:978-1-4673-4712-9,10.1109/ICNC.2013.6818156,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6818156,3D models;Hadoop;MapReduce;parallel techniques,Computational modeling;Data models;Indexes;Load modeling;Loading;Solid modeling;Three-dimensional displays,Big Data;data analysis;data models;multi-threading;public domain software;storage management,3D data management;3D model retrieval system;3D models;Big Data;Hadoop distributed file system;MapReduce paradigm;coarse grained parallelism;data analytics;data loading;large-scale data storage;lightweight multithreaded algorithm;multimedia data;open-source platform;parallel techniques;three-dimensional accessing performance;three-dimensional model storage performance;unstructured data,,1,,14,,no,23-25 July 2013,,IEEE,IEEE Conference Publications
Parallel-Resonance-Type Fault Current Limiter,S. B. Naderi; M. Jafari; M. Tarafdar Hagh,"Department of Electrical Engineering, Sarab Branch, Islamic Azad University, Tehran, Iran",IEEE Transactions on Industrial Electronics,20130307,2013,60,7,2538,2546,"This paper proposes a new parallel-<i>LC</i>-resonance-type fault current limiter (FCL) that uses a resistor in series with a capacitor. The proposed FCL is capable of limiting the fault current magnitude near to the prefault magnitude of distribution feeder current by placing the mentioned resistor in the structure of the FCL. In this way, the voltage of the point of common coupling does not experience considerable sag during the fault. In addition, the proposed FCL does not use a superconducting inductor which has high construction cost. Analytical analysis for this structure is presented in detail, and simulation results using power system computer-aided design/electromagnetic transients, including dc software are obtained to validate the effectiveness of this structure. Also, an experimental setup is provided to show the accuracy of the analytic analyses and simulation results.",0278-0046;02780046,,10.1109/TIE.2012.2196899,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6192321,Fault current limiter (FCL);parallel resonance;resistor,Bridge circuits;Circuit faults;Fault currents;Impedance;Inductors;RLC circuits;Semiconductor diodes,fault current limiters;power system CAD,FCL;dc software;distribution feeder current;fault current magnitude;parallel-LC-resonance-type fault current limiter;point of common coupling;power system computer-aided design-electromagnetic transients;prefault magnitude,,15,,33,,no,13-Jul,,IEEE,IEEE Journals & Magazines
Pathways to technology transfer and adoption: Achievements and challenges (mini-tutorial),D. Zhang; T. Xie,"Microsoft Research Asia, Beijing, 100080, China",2013 35th International Conference on Software Engineering (ICSE),20130926,2013,,,951,952,"Producing industrial impact has often been one of the important goals of academic or industrial researchers when conducting research. However, it is generally challenging to transfer research results into industrial practices. There are some common challenges faced when pursuing technology transfer and adoption while particular challenges for some particular research areas. At the same time, various opportunities also exist for technology transfer and adoption. This mini-tutorial presents achievements and challenges of technology transfer and adoption in various areas in software engineering, with examples drawn from research areas such as software analytics along with software testing and analysis. This mini-tutorial highlights success stories in industry, research achievements that are transferred to industrial practice, and challenges and lessons learned in technology transfer and adoption.",0270-5257;02705257,Electronic:978-1-4673-3076-3; POD:978-1-4673-3075-6,10.1109/ICSE.2013.6606644,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606644,,Cloning;Collaboration;Software;Software engineering;Speech;Technology transfer;Testing,program testing;research and development;software engineering;technology transfer,academic researchers;industrial impact;industrial researchers;software analysis;software analytics;software engineering;software testing;technology adoption;technology transfer,,1,,27,,no,18-26 May 2013,,IEEE,IEEE Conference Publications
PatRICIA -- A Novel Programming Model for IoT Applications on Cloud Platforms,S. Nastic; S. Sehic; M. VÌ_gler; H. L. Truong; S. Dustdar,"Distrib. Syst. Group, Vienna Univ. of Technol., Vienna, Austria",2013 IEEE 6th International Conference on Service-Oriented Computing and Applications,20140123,2013,,,53,60,"Cloud computing technologies have recently been intensively exploited for the development and management of large-scale IoT systems, due to their capability to integrate diverse types of IoT devices and to support big IoT data analytics in an elastic manner. However, due to the diversity, complexity and scale of IoT systems, the need to handle large volumes of IoT data in a nontrivial manner, and the plethora of domain-dependent IoT controls, programming IoT applications on cloud platforms still remains a great challenge. To date, existing work neglects high-level programming models and focuses on low-level IoT data and device integration. In this paper, we outline PatRICIA, which aims at providing an end-to-end solution for high-level programming and provisioning of IoT applications on cloud platforms. We present a novel programming model, based on the concept of intent and intent scope. Further, we introduce its runtime for dealing with the complexity, diversity and scale of IoT systems in the cloud. Our programming model defines abstractions to enable easier, efficient and more intuitive development of cloud-scale IoT applications. To illustrate our programming model, we present a case study with real-world applications for controlling and managing electric vehicles.",2163-2871;21632871,Electronic:978-1-4799-2702-9; POD:978-1-4799-3172-9,10.1109/SOCA.2013.48,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6717285,Intent;IntentScope;cloud-scale IoT applications;programming model,Business;Cloud computing;Complexity theory;Monitoring;Programming;Runtime;Vehicles,Big Data;Internet of Things;cloud computing;control engineering computing;data analysis;electric vehicles;programming;software engineering,IoT application programming;IoT applications;IoT data handling;IoT devices;PatRICIA;big IoT data analytics;cloud computing technologies;cloud platforms;concept of intent;device integration;domain-dependent IoT control;electric vehicle control;electric vehicle management;high-level programming model;intent scope;large-scale IoT systems;low-level IoT data,,12,,23,,no,16-18 Dec. 2013,,IEEE,IEEE Conference Publications
Performance analysis for priority based broadcast in vehicular networks,Rinara Woo; Dong Seog Han; Jung-Hoon Song,"Sch. of Electron. Eng., Kyungpook Nat. Univ., Daegu, South Korea",2013 Fifth International Conference on Ubiquitous and Future Networks (ICUFN),20130930,2013,,,51,55,"Transportation safety is one of the most important applications for vehicular ad-hoc networks based on IEEE 802.11p. When a vehicle is in an emergency condition, a safety-related message is transmitted to the neighboring vehicles and infrastructures. Vehicles and infrastructures are exchanging periodic messages on the vehicle position, traffic information to provide various services. When the traffic load is very high, the emergency message cannot be delivered immediately. To overcome this situation, a priority based transmission scheme is considered to guarantee the transmission of the emergency message. We analyze the performance of vehicular communication networks in two perspectives. Firstly, an analytical Markov-chain model for vehicle-to-vehicle (V2V) ad-hoc communication networks is proposed for broadcasting messages with priority based on IEEE 802.11p. Secondly, an analytic Queuing model for vehicular communication networks are proposed to evaluate the network performance for dealing with safety and non-safety messages from the point of the infrastructure.",2165-8528;21658528,Electronic:978-1-4673-5990-0; POD:978-1-4673-5989-4; USB:978-1-4673-5988-7,10.1109/ICUFN.2013.6614776,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6614776,ITS;Markov chain model;Queuing model;WAVE;vehicular ad-hoc networks,Ad hoc networks;Analytical models;Communication networks;Delays;Safety;Throughput;Vehicles,Markov processes;vehicular ad hoc networks;wireless LAN,IEEE 802.11p;Markov chain model;ad hoc communication networks;analytic queuing model;broadcasting messages;emergency condition;emergency message;neighboring vehicles;performance analysis;priority based broadcast;safety related message;traffic information;traffic load;transmission scheme;transportation safety;vehicle position;vehicular ad hoc networks;vehicular communication networks;vehicular networks,,0,,7,,no,2-5 July 2013,,IEEE,IEEE Conference Publications
Performance analysis of a fault-tolerant exact motif mining algorithm on the cloud,N. Nguyen; M. M. H. Khan,"Dept. of Comput. Sci. & Eng., Univ. of Connecticut, Storrs, CT, USA",2013 IEEE 32nd International Performance Computing and Communications Conference (IPCCC),20140220,2013,,,1,9,"In this paper, we present the performance analysis and design challenges of implementing a fault-tolerant parallel exact motif mining algorithm leveraging the services provided by the underlying cloud storage platform (e.g., data replication, node failure detection). More specifically, first, we present the design of the intermediate data structures and data models that are needed for effective parallelization of the motif mining algorithm on the cloud. Second, we present the design and implementation of a fault-tolerant parallel motif mining algorithm that enables the data analytic system to recover from arbitrary node failures in the cloud environment by detecting node failures and redistributing remaining computational tasks in real-time. We also present a data caching scheme to improve the system performance even further. We evaluated the impact of various factors such as the replication factor and random node failures on the performance of our system using two different datasets, namely, an EOG dataset and an image dataset. In both cases, our algorithm exhibits superior performance over the existing algorithms, thus demonstrating the effectiveness of our presented system.",1097-2641;10972641,CD-ROM:978-1-4799-3213-9; Electronic:978-1-4799-3214-6; POD:978-1-4799-3215-3,10.1109/PCCC.2013.6742786,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6742786,cloud;motif;time series,Algorithm design and analysis;Cloud computing;Data mining;Fault tolerance;Fault tolerant systems;Sensors;Time series analysis,cache storage;cloud computing;data analysis;data models;software fault tolerance;time series,EOG dataset;arbitrary node failures detection;cloud environment;cloud storage platform;data analytic system;data caching scheme;data models;design challenges;fault-tolerant parallel exact motif mining algorithm;image dataset;intermediate data structures design;parallelization;random node failures;replication factor;system performance analysis;time series,,0,,30,,no,6-8 Dec. 2013,,IEEE,IEEE Conference Publications
Performance Evaluation of an Audio Tuner System Based on AIR Audio Processing Library,K. Naoe; K. Kaneko,"Dept. of Adv. Inf. Technol., Kyushu Univ., Fukuoka, Japan","2013 4th International Conference on Intelligent Systems, Modelling and Simulation",20130415,2013,,,204,207,"In the development of applications for mobile devices that can be easily portable and lightweight, the language of the byte code, such as Action script is preferred. However, if you need a high speed, it is necessary to cooperate with software library written in C/C++. In its cooperation, buffer management, such as efficiency of binary data conversion, is still an important research topic. In this paper, we focus on the buffer management of spectrum analysis on mobile audio signal processing. And, we report the performance evaluation in cooperation with the Fourier transform C++ language library and Action script running on Adobe AIR.",2166-0662;21660662,Electronic:978-0-7695-4963-7; POD:978-1-4673-5653-4,10.1109/ISMS.2013.89,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6498265,Adobe AIR;Adobe Flash;Audio input/output;Auio Processing;Methodologies and techniques;Multimedia Information System;Music Information Retrieval;Sound and Music Computing;Spectrum Analytic,Fourier transforms;Libraries;Microphones;Mobile handsets;Performance evaluation;Signal processing;Tuners,C++ language;Fourier transforms;audio signal processing;buffer storage;mobile computing;music;software libraries;software performance evaluation;spectral analysis;tuning,AIR audio processing library;Action script;Adobe AIR;C/C++;Fourier transform C++ language library;audio tuner system;binary data conversion;buffer management;byte code;mobile audio signal processing;mobile devices;performance evaluation;software library;spectrum analysis,,0,,5,,no,29-31 Jan. 2013,,IEEE,IEEE Conference Publications
Preface: Massive-scale analytics,A. Soffer,Big Data Analytics IBM Research - Haifa Laboratory,IBM Journal of Research and Development,20130517,2013,57,4-Mar,0:01,0:04,"Big Data refers to large datasets that are beyond the capability of traditional software tools to quickly manage,process, and analyze. The development of techniques for gaining insight from such information provides potential benefits in such arenas as business, science, and public policy. Big Data is generally characterized by its volume, variety, velocity, and veracity. This special issue of the IBM Journal of Research and Development emphasizes applications, analytics, software, and hardware technologies that form the foundational building blocks for massive-scale analytics (MSA) and the processing of Big Data.",0018-8646;00188646,,10.1147/JRD.2013.2251981,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6517308,,Content management;Data handling;Database systems;Information management;Information processing;Large scale systems;Special issues and sections,,,,0,,,,no,May-July 2013,,IBM,IBM Journals & Magazines
Prescriptive Analytics System for Improving Research Power,S. K. Song; D. J. Kim; M. Hwang; J. Kim; D. H. Jeong; S. Lee; H. Jung; W. Sung,"Software Res. Center, Korea Inst. of Sci. & Technol. Inf., Daejeon, South Korea",2013 IEEE 16th International Conference on Computational Science and Engineering,20140306,2013,,,1144,1145,"We introduce a prescriptive analytics system, InSciTe advisory, to provide researchers with advice of their future research direction and strategy. The system analyzes several thousands of heterogeneous types of data sources such as papers, patents, reports, Web news, Web magazines, and collective intelligence data. It consists of two main parts of descriptive analytics and prescriptive analytics. Once given a researcher, the descriptive analytics part provides results from activity history and research power w.r.t the designated researcher. Then, prescriptive analytics part suggests a group of role model researchers to the researcher, as well as how to be like the role model researchers. The prescription for the researcher is provided according to 5W1H questions and their corresponding answers. All of the analytical results and their explanations about the given researcher are automatically generated and saved to a report. This researcher-centric prescriptive analytics has not been introduced before and it is useful tool to understand the designated researcher in the perspective of prescriptive as well as descriptive analytics.",,Electronic:978-0-7695-5096-1; POD:978-1-4799-4897-0,10.1109/CSE.2013.169,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6755350,Activity History;Big Data;Descriptive Analytics;Prescriptive Analytics;Researcher Power,Analytical models;Companies;History;Market research;Semantic Web,data mining;graphical user interfaces;research and development;semantic Web;text analysis,5W1H questions;InSciTe advisory;Web magazines;Web news;activity history;analytical analysis;collective intelligence data;descriptive analytics;heterogeneous data source types;papers;patents;prescriptive analytics;reports;research power;research power improvement;researcher-centric prescriptive analytics;role model researchers;semantic Web technologies;text mining,,0,,8,,no,3-5 Dec. 2013,,IEEE,IEEE Conference Publications
Prioritizing CRC cards as a simple design tool in extreme programming,S. Alshehri; L. Benedicenti,"Software Systems Engineering University of Regina Regina, Canada",2013 26th IEEE Canadian Conference on Electrical and Computer Engineering (CCECE),20130725,2013,,,1,4,"The analytic hierarchy process (AHP) has been applied in many fields and especially to complex engineering problems and applications. The AHP is capable of structuring decision problems and finding mathematically determined judgments built on knowledge and experience. This suggests that AHP should prove useful in agile software development where complex decisions occur routinely. In this paper, the AHP is used to prioritize Class Responsibility Collaboration (CRC) cards in Extreme Programming (XP) design activity. XP encourages the simplicity in design that takes less time to accomplish than more complex approaches. The CRC cards tool has been proved to effectively achieve this purpose. Moreover, prioritizing CRC cards helps the XP team to implement the most valuable class to design.",0840-7789;08407789,Electronic:978-1-4799-0033-6; POD:978-1-4799-0031-2,10.1109/CCECE.2013.6567820,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6567820,Analytic Hierarchy process (AHP);CRC Cards;Extreme Programming (XP),Analytic hierarchy process;Collaboration;Educational institutions;Programming;Software;Stability criteria;Unified modeling language,analytic hierarchy process;software prototyping,AHP;CRC card tool;XP design;agile software development;analytic hierarchy process;class responsibility collaboration cards;complex engineering problems;decision problems;extreme programming design;simple design tool,,0,,10,,no,5-8 May 2013,,IEEE,IEEE Conference Publications
Ranking of financial and electronic debts using analytic hierarchy process (AHP),L. Torki; R. Dalali Isfahani; A. Rezai,"University of Isfahan, Economics Department",7th International Conference on e-Commerce in Developing Countries:with focus on e-Security,20130715,2013,,,1,13,"Financial and electronic debts are one of the important issues in Iran's financial scope that are considered by economists due to their role in creating financial instabilities. Therefore, a model was represented in this survey to evaluate and rank financial and electronic debts in Iran including foreign debts, governmental debts, non-governmental debts and banking debts during the period 1999-2012 using analytic hierarchy process (AHP). To this end, four indexes of volume of debt, ability to repay, willingness to repay and return rate of debts were identified and paired comparison of debts was conducted given to the research literature in this scope and interview with experts. Then total weight of each index was calculated, score of each index was exploited and finally ranking of all kinds of financial and electronic debts was determined by sum of the score of indexes. Expert Choice 11 software was used to calculate the weights.",,Electronic:978-1-4799-0393-1; POD:978-1-4799-0394-8,10.1109/ECDC.2013.6556751,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6556751,Key words: financial and credit debts;analytic hierarchy process (ARP);ranking,Analytic hierarchy process;Banking;Economic indicators;Educational institutions;Indexes,analytic hierarchy process;financial data processing,AHP;Expert Choice 11 software;Iran financial scope;ability-to-repay;analytic hierarchy process;banking debts;debt volume;debts return rate;electronic debt ranking;financial debt ranking;financial instabilities;foreign debts;governmental debts;nongovernmental debts;willingness-to-repay,,0,,23,,no,17-18 April 2013,,IEEE,IEEE Conference Publications
Rapid virtual prototyping of real-time systems using predictable platform characterizations,S. Hosein Attarzadeh Niaki; M. Mikulcak; I. Sander,"KTH Royal Institute of Technology School of Information and Communication Technology, Unit of Electronic Systems Stockholm, Sweden",Proceedings of the 2013 Forum on specification and Design Languages (FDL),20131028,2013,,,1,8,"Virtual prototypes (VPs) provide an early development platform to embedded software designers when the hardware is not ready yet and allows them to explore the design space of a system, both from the software and architecture perspective. However, automatic generation of VPs is not straightforward because several aspects such as the validity of the generated platforms and the timing of the components needs to be considered. To address this problem, based on a framework which characterizes predictable platform templates, we propose a method for automated generation of VPs which is integrated into a combined design flow consisting of analytic and simulation based design-space exploration. Using our approach the valid TLM 2.0-based simulated VP instances with timing annotation can be generated automatically and used for further development of the system in the design flow. We have demonstrated the potential of our method by designing a JPEG encoder system.",1636-9874;16369874,Electronic:978-2-9530504-8-6; POD:978-1-4799-0576-8; USB:978-2-9530504-7-9,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6646652,automation;design-space exploration;predictable platforms;real-time systems;simulation;virtual prototyping,Analytical models;Program processors;Prototypes;Real-time systems;Sockets;Space exploration;Time division multiplexing,,,,0,,17,,no,24-26 Sept. 2013,,IEEE,IEEE Conference Publications
Real-Time Analytics for Legacy Data Streams in Health: Monitoring Health Data Quality,A. Berry; Z. Milosevic,"Deontik Pty Ltd., Brisbane, QLD, Australia",2013 17th IEEE International Enterprise Distributed Object Computing Conference,20131114,2013,,,91,100,"Healthcare organizations are increasingly using information technology to ensure patient safety, increase effectiveness and improve efficiency of healthcare delivery. While the use of health information technology (HIT) has realized many improvements, it has also introduced new failure modes arising from data quality and IT system usability issues. This paper presents an approach towards addressing these failure modes by applying real-time analytics to existing streams of clinical messages exchanged by HIT systems. We use complex event processing provided by the Event Swarm software framework to monitor data quality in such systems through intercepting messages and applying rules reflecting the syndromic surveillance model proposed in [4]. We believe this is the first work reporting on the real-time application of syndromic surveillance rules to legacy clinical data streams. Our design and implementation demonstrates the feasibility of this approach and highlights benefits obtained through improved operational quality of HIT systems, notably better patient safety, reduced risks in healthcare delivery and potentially reduced costs.",1541-7719;15417719,Electronic:978-0-7695-5081-7; POD:978-1-4799-0869-1,10.1109/EDOC.2013.19,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6658267,data quaility;health analytics;real-time;syndromic surveillance,Laboratories;Medical services;Pattern matching;Real-time systems;Safety;Surveillance,data analysis;health care;medical information systems,HIT systems;IT system usability;event swarm software framework;failure modes;health data quality monitoring;healthcare delivery efficiency;healthcare organizations;information technology;legacy clinical data streams;patient safety;real-time analytics;syndromic surveillance model,,3,,22,,no,9-13 Sept. 2013,,IEEE,IEEE Conference Publications
Real-time data analysis in ClowdFlows,J. Kranjc; V. Podpe€çan; N. Lavra€ç,"Jozef Stefan Inst., Ljubljana, Slovenia",2013 IEEE International Conference on Big Data,20131223,2013,,,15,22,"ClowdFlows is an open cloud based platform for composition, execution, and sharing of interactive data mining workflows. In this paper we extend the ClowdFlows platform with the ability to mine real-time data streams. This functionality was implemented by creating a specialized type of workflow component and a stream mining daemon that delegates the execution of workflows in real-time. In this way, we have transformed a batch data processing platform into a real-time stream mining platform with an intuitive user interface. The real-time analytics aspect of the platform is demonstrated in a Twitter sentiment analysis use case where the sentiment of tweets about whistleblower Edward Snowden was monitored for approximately one month.",,Electronic:978-1-4799-1293-3; POD:978-1-4799-1294-0,10.1109/BigData.2013.6691682,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6691682,data mining platform;real-time data analysis;sentiment analysis;stream mining;web application;workflows,Data mining;Engines;Graphical user interfaces;Real-time systems;Servers;Twitter;Visualization,batch processing (computers);cloud computing;data mining;public domain software;user interfaces,ClowdFlows platform;Edward Snowden;Twitter sentiment analysis;batch data processing platform;interactive data mining workflows;intuitive user interface;open cloud based platform;real-time analytics;real-time data analysis;real-time data streams;real-time stream mining platform;stream mining daemon;whistleblower;workflow component,,0,,19,,no,6-9 Oct. 2013,,IEEE,IEEE Conference Publications
Reliability of erasure coded storage systems: A geometric approach,A. Campello; V. A. Vaishampayan,"Inst. of Math., Stat., & Comput. Sci., Univ. of Campinas, Campinas, Brazil",2013 IEEE International Conference on Big Data,20131223,2013,,,12,16,"We consider the probability of data loss in an erasure coded distributed storage system. Data loss in an erasure coded system depends on the repair duration and the failure probability of individual disks. This dependence on the repair duration complicates the data loss probability analysis. In previous work, the data loss probability of such systems has been studied under the assumption of exponentially distributed disk life and disk repair durations, using well-known analytic methods from the theory of Markov processes. Here, we assume that the repair duration is a constant and derive an upper bound on the probability of data loss by calculating the volumes of specific polytopes that are determined by the code. Closed form bounds are exhibited for some example codes.",,Electronic:978-1-4799-1293-3; POD:978-1-4799-1294-0,10.1109/BigData.2013.6691662,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6691662,,Maintenance engineering;Random variables;Reliability;Simulation;Upper bound;Vectors,distributed processing;forward error correction;probability;software reliability;storage management,data loss probability analysis;erasure coded distributed storage system reliability;geometric approach;individual disk failure probability;polytope volumes;repair duration,,2,,10,,no,6-9 Oct. 2013,,IEEE,IEEE Conference Publications
Research and implementation of AHP scheduling algorithms for load balancing,Jian-dong Zhang; Jin Yang; Xuan Ma; Cai-ming Liu; Bin Li,"Department of Computer Science, Laboratory of Intelligent Information Processing and Application, LeShan Normal Univ., 614004, SiChuan, China",IEEE Conference Anthology,20140410,2013,,,1,5,"For great change of service time for request, big difference of hardware and software server and different network performance, this paper proposes a dynamic-feedback algorithm based on AHP in the course of studying the algorithm of load balancing in the cluster-based system. Combined with Weighted scheduling algorithm of the kernel, based on the parameters influencing the performance of cluster system from dynamic feedback, we can adjust the servers' weight, solve the load imbalance problem among the servers effectively and certainly improve the throughput of the whole system.",,Electronic:978-1-4799-1660-3,10.1109/ANTHOLOGY.2013.6784988,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6784988,Load balancing;Scheduling algorithms;Server cluster;The Analytic Hierarchy Proccess,Bandwidth;Databases;Hardware;Processor scheduling;Servers;Software;Throughput,,,,0,,11,,no,1-8 Jan. 2013,,IEEE,IEEE Conference Publications
Research of analytic hierarchy process applying for case based reasoning in automotive welding jig design,H. Yu; G. Li; J. Zhang,"Sch. of Electromech. & Automobile Eng., Yantai Univ., Yantai, China","Proceedings 2013 International Conference on Mechatronic Sciences, Electric Engineering and Computer (MEC)",20140828,2013,,,3222,3226,"On the basis of the structure of automotive welding jigs, the welding jig case database is constructed in this paper. The analytic hierarchy process (AHP) is applied for partitioning the influence factors of automotive welding jig case unit. The weight calculation and weighted coefficient distribution of case unit properties is presented adopting 1 - 9 scaling algorithm. Thus similarity can be calculated using neighbor similarity. The system case based on reasoning for welding jig case database is developed with VC++6.0 and NX8.0 CAD software, which satisfies the design requirement of rapidity, high efficiency, flexibility and portability.",,CD-ROM:978-1-4799-2563-6; Electronic:978-1-4799-2565-0; POD:978-1-4799-2566-7,10.1109/MEC.2013.6885574,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6885574,Analytic Hierarchy Process;Case Based Reasoning;Similarity,Automobiles;Databases;Design automation;Educational institutions;Knowledge based systems;Software;Welding,analytic hierarchy process;automotive engineering;case-based reasoning;design engineering;production engineering computing;welding,1 - 9 scaling algorithm;AHP;NX8.0 CAD software;VC++6.0;analytic hierarchy process;automotive welding jig design;case based reasoning;case unit properties;design requirement;neighbor similarity;weight calculation;weighted coefficient distribution;welding jig case database,,0,,7,,no,20-22 Dec. 2013,,IEEE,IEEE Conference Publications
Research on Fuzzy Comprehensive Evaluation of Human-Machine Interface Layout of Driller Control Room,D. Li; Y. Sui-huai; W. Wen-jun,"Inst. of Ind. Design, Northwestern Polytech. Univ., Xi'an, China",2013 International Conference on Computational and Information Sciences,20131024,2013,,,1114,1117,"In order to evaluate the human-machine interface layout of oil rig driller control room effectively, a kind of evaluation method that uses CATIA software simulation and Fuzzy AHP (Fuzzy-Analytic Hierarchy Process) was put forward. According to the drillers' visual area, human body size, comfortable joint range and so on physiological characteristics, CATIA was used to evaluate the accessibility, visibility, and comfort objectively. Combining with the expert opinions of drillers, such as psychological characteristics, operating habits and experience, the evaluation level and evaluation indexes were determined, Fuzzy-AHP evaluation method was used to realize the conversion of qualitative index to quantitative index, and overcome the subjectivity of the evaluation. This comprehensive method balanced the objective evaluation and subjective evaluation, and realized the scientific evaluation of human-machine interface layout of driller control room. Finally the further research work was pointed out.",,Electronic:978-0-7695-5004-6; POD:978-1-4799-0300-9,10.1109/ICCIS.2013.296,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6643213,CATIA;Fuzzy AHP;driller control room;evaluation;human-machine interface,Ergonomics;Indexes;Layout;Man machine systems;Manipulators;Standards;Visualization,analytic hierarchy process;control engineering computing;fuzzy set theory;human computer interaction;offshore installations;oil drilling,CATIA software simulation;accessibility;comfortable joint range;drillers visual area;evaluation indexes;evaluation level;fuzzy comprehensive evaluation;fuzzy-AHP evaluation method;fuzzy-analytic hierarchy process;human body size;human-machine interface layout;objective evaluation;oil rig driller control room;operating habits;physiological characteristics;psychological characteristics;qualitative index;quantitative index;scientific evaluation;subjective evaluation;visibility,,0,,5,,no,21-23 June 2013,,IEEE,IEEE Conference Publications
Resource optimization for speculative execution in a MapReduce Cluster,Huanle Xu; Wing Cheong Lau,"Dept. of Inf. Eng., Chinese Univ. of Hong Kong, Hong Kong, China",2013 21st IEEE International Conference on Network Protocols (ICNP),20140210,2013,,,1,3,"The MapReduce paradigm is now the de facto standard for large-scale data analytics. In this paper we address the resource management issues in MapReduce Cluster. Speculative execution (task backup) plays an important role in resource management. We propose two different strategies and build two models to formulate the backup issue as an optimization problem when the cluster is lightly loaded. Moreover, we present an Enhanced Speculative Execution (ESE) algorithm when the cluster is heavily loaded and adopt the approximate analysis to get an optimal value for the parameter in the algorithm. The simulation results show that the algorithm can reduce the job completion time by 50% while consuming much less resource compared to the naive method without backup.",1092-1648;10921648,Electronic:978-1-4799-1270-4; POD:978-1-4799-1268-1,10.1109/ICNP.2013.6733646,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6733646,MapReduce;job scheduling;speculative execution;theoretical analysis,Algorithm design and analysis;Approximation algorithms;Clustering algorithms;Google;Load modeling;Optimization;Simulation,data analysis;optimisation;pattern clustering;public domain software,ESE;MapReduce cluster;enhanced speculative execution algorithm;large-scale data analytics;naive method;resource management;resource optimization problem,,0,,7,,no,7-10 Oct. 2013,,IEEE,IEEE Conference Publications
Resource Provisioning for Staging Components,T. A. Nguyen; G. Eisenhauer; K. Schwan; M. Wolf; H. Abbasi; S. Klasky; N. Podhorszki,"Georgia Inst. of Technol., Atlanta, GA, USA","2013 IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum",20131031,2013,,,1947,1953,"To deal with the inordinate output data volumes of current and future high end simulations, researchers are turning to online methods in which multiple software components that implement desired data analytics and visualization are run on 'staging resources' of the petascale machine, concurrently and coupled with the simulations producing these outputs. Efficient online execution of data analytics 'in the output stream', however, requires careful provisioning of staging resources, to obtain delays for analytics processing that prevent applications from blocking on stalled output, while also bounding total required staging resources. This paper addresses the 'staging provisioning' problem, assuming sets of components arranged as potentially multiple analytics/output pipelines that differ in runtime behavior and resource requirements. For such configurations, it then meets the throughput constraint of online analytics while also minimizing end-to-end pipeline latency, all based on runtime observations and predictions of component performance. Experimental evaluations demonstrate the algorithm's utility. Its complexity for minimizing latency without violating throughput constraints is O(M), where M is the number of components in the staging area.",,Electronic:978-0-7695-4979-8; POD:978-1-4799-1372-5,10.1109/IPDPSW.2013.152,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6651098,resource management;resource provisioning;staging,Analytical models;Data models;Pipelines;Principal component analysis;Program processors;Runtime;Throughput,computational complexity;data visualisation;minimisation;object-oriented programming;resource allocation,data analytics;data visualization;end-to-end pipeline latency;inordinate output data volumes;latency minimization;multiple software components;resource provisioning;runtime observations;staging provisioning problem;staging resources,,0,,27,,no,20-24 May 2013,,IEEE,IEEE Conference Publications
Rethinking computer architecture for throughput computing,W. m. W. Hwu,"University of Illinois, Urbana-Champaign and MulticoreWare Inc., United States","2013 International Conference on Embedded Computer Systems: Architectures, Modeling, and Simulation (SAMOS)",20131007,2013,,,i,i,"Summary form only given. The rise of rich media in mobile devices and massive analytics in data centers has created new opportunities and challenges for computer architects. On one hand, commercial computer organizations have been undergoing fast transformation to drastically increase the throughput of processing large amounts of data while keeping the power consumption in check. On the other hand, computer architecture has evolved too slowly to facilitate hardware innovations, software productivity, algorithm advancement and user perceived improvements. In this talk, I will present some major challenges facing the computer architecture research community and some recent advancements in throughput computing. I will conclude by arguing that we must rethink the scope of computer architecture research as we seek to create growth paths for the computer systems industry.",,Electronic:978-1-4799-0103-6; POD:978-1-4799-0104-3; USB:978-1-4799-0102-9,10.1109/SAMOS.2013.6621096,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6621096,,Awards activities;Computer science;Computers;Educational institutions;Multicore processing;Throughput,computer architecture;computer centres;mobile computing;multiprocessing systems,algorithm advancement;computer architecture research community;computer systems industry;data centers;hardware innovations;massive analytics;mobile devices;power consumption;rich media;software productivity;throughput computing;user perceived improvements,,0,,,,no,15-18 July 2013,,IEEE,IEEE Conference Publications
Ripple: Improved Architecture and Programming Model for Bulk Synchronous Parallel Style of Analytics,M. Spreitzer; M. Steinder; I. Whalley,,2013 IEEE 33rd International Conference on Distributed Computing Systems,20131212,2013,,,460,469,"We present Ripple, an architecture and a programming model for a broad set of data analytics. Ripple builds on the ideas of iterated MapReduce and adds two innovations. First it has a richer programming model, including more ideas from the Bulk Synchronous Parallel (BSP) model of computation and others. By doing so, Ripple creates a flexible and higher-level platform that is easier for both application programmers and platform implementors. Second, Ripple is based on a limited interface for key/value storage making it portable among many different key/value store implementations. By building on these two ideas Ripple improves the scope, performance, and openness of the data analytics platform. We evaluate Ripple using three representative, and non-trivial, data analysis scenarios requiring iterative computation. Using these examples, we show how Ripple achieves clear performance advantages over iterated MapReduce.",1063-6927;10636927,Electronic:978-0-7695-5000-8; POD:978-1-4799-0183-8,10.1109/ICDCS.2013.67,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6681615,Distributed databases;Distributed programming,Computational modeling;Computer architecture;Data models;Distributed databases;Programming;Synchronization;Trademarks,data analysis;distributed databases;iterative methods;parallel programming;software architecture,BSP model;MapReduce;Ripple;application programmers;architecture;bulk synchronous parallel model;data analysis;data analytics;distributed database;iterative computation;key/value storage;platform implementors;programming model,,0,,19,,no,8-11 July 2013,,IEEE,IEEE Conference Publications
Robust graph traversal: Resiliency techniques for data intensive supercomputing,S. Hukerikar; P. C. Diniz; R. F. Lucas,"Inf. Sci. Inst., Univ. of Southern California, Marina del Rey, CA, USA",2013 IEEE High Performance Extreme Computing Conference (HPEC),20131121,2013,,,1,6,"Emerging large-scale, data intensive applications that use the graph abstraction to represent problems in a broad spectrum of scientific and analytics applications have radically different features from floating point intensive scientific applications. These complex graph applications, besides having large working datasets, exhibit very low spatial and temporal locality which makes designing algorithmic fault tolerance for these quite challenging. They will run on future exascale-class High Performance Computing (HPC) systems that will contain massive number of components, and will be built from devices far less reliable than those used today. In this paper we propose software based approaches that increase robustness for these data intensive, graph-based applications by managing the resiliency in terms of the data flow progress and validation of pointer computations. Our experimental results show that such a simple approach incurs fairly low execution time overheads while allowing these computations to survive a large number of faults that would otherwise always result in the termination of the computation.",,Electronic:978-1-4799-1365-7; POD:978-1-4799-1363-3,10.1109/HPEC.2013.6670340,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6670340,,Computer crashes;Data structures;Error correction codes;Fault tolerance;Fault tolerant systems;Robustness;Runtime,data flow computing;fault tolerant computing;floating point arithmetic;graph theory;parallel machines,HPC;algorithmic fault tolerance;complex graph applications;data flow progress;data intensive supercomputing;floating point intensive scientific applications;graph abstraction;graph-based applications;high performance computing systems;pointer computations;resiliency techniques;robust graph traversal;software based approaches,,1,,16,,no,10-12 Sept. 2013,,IEEE,IEEE Conference Publications
Roundtable: What's Next in Software Analytics,A. E. Hassan; A. Hindle; P. Runeson; M. Shepperd; P. Devanbu; S. Kim,"Queen's University, Canada",IEEE Software,20130626,2013,30,4,53,56,"For this special issue, the guest editors asked a panel of six established experts in software analytics to highlight what they thought were the most important, or overlooked, aspect of this field. They all pleaded for a much broader view of analytics than seen in current practice: software analytics should go beyond developers (Ahmed Hassan) and numbers (Per Runeson). Analytics should also prove its relevance to practitioners (Abram Hindle, Martin Shepperd). There are now opportunities for ""natural"" software analytics based on statistical natural language processing (Prem Devanbu). Lastly, software analytics needs information analysts and field agents like Chloe O'Brian and Jack Bauer in the TV show 24 (Sung Kim).",0740-7459;07407459,,10.1109/MS.2013.85,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6547637,decision support;information search and retrieval;management;metrics;software/program verification;statistical methods;testing and debugging,Analytical models;Data mining;Natural language processing;Software development;Software engineering,natural language processing;software engineering;statistical analysis,natural software analytics;statistical natural language processing,,4,,7,,no,July-Aug. 2013,,IEEE,IEEE Journals & Magazines
SALMA: Standard Arabic Language Morphological Analysis,M. Sawalha; E. Atwell; M. A. M. Abushariah,"Computer Information Systems Department, King Abdullah II School of Information Technology, University of Jordan, Amman, Jordan","2013 1st International Conference on Communications, Signal Processing, and their Applications (ICCSPA)",20130328,2013,,,1,6,"Morphological analyzers are preprocessors for text analysis. Many Text Analytics applications need them to perform their tasks. This paper reviews the SALMA-Tools (Standard Arabic Language Morphological Analysis) [1]. The SALMA-Tools is a collection of open-source standards, tools and resources that widen the scope of Arabic word structure analysis - particularly morphological analysis, to process Arabic text corpora of different domains, formats and genres, of both vowelized and non-vowelized text. Tag-assignment is significantly more complex for Arabic than for many languages. The morphological analyzer should add the appropriate linguistic information to each part or morpheme of the word (proclitic, prefix, stem, suffix and enclitic); in effect, instead of a tag for a word, we need a subtag for each part. Very fine-grained distinctions may cause problems for automatic morphosyntactic analysis - particularly probabilistic taggers which require training data, if some words can change grammatical tag depending on function and context; on the other hand, fine-grained distinctions may actually help to disambiguate other words in the local context. The SALMA - Tagger is a fine grained morphological analyzer which is mainly depends on linguistic information extracted from traditional Arabic grammar books and prior-knowledge broad-coverage lexical resources; the SALMA - ABCLexicon. More fine-grained tag sets may be more appropriate for some tasks. The SALMA - Tag Set is a standard tag set for encoding, which captures long-established traditional fine-grained morphological features of Arabic, in a notation format intended to be compact yet transparent.",,CD-ROM:978-1-4673-2819-7; Electronic:978-1-4673-2821-0; POD:978-1-4673-2820-3,10.1109/ICCSPA.2013.6487311,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6487311,Fine-grain;Morphological analysis;Tag Set;Traditional Arabic Grammar;Traditional Arabic Lexicons,Accuracy;Algorithm design and analysis;Educational institutions;Gold;Morphology;Standards,information retrieval;linguistics;natural language processing;public domain software;text analysis,ABCLexicon;Arabic text corpora;Arabic word structure analysis;SALMA-TagSet;SALMA-Tagger;SALMA-tools;automatic morphosyntactic analysis;fine grained morphological analyzer;fine-grained tag sets;grammatical tag;linguistic information;linguistic information extraction;nonvowelized text;open source standards;prior-knowledge broad-coverage lexical resources;probabilistic taggers;standard Arabic language morphological analysis;tag-assignment;text analysis;text analytics applications;traditional Arabic grammar books;vowelized text;word morpheme,,2,,38,,no,12-14 Feb. 2013,,IEEE,IEEE Conference Publications
SAMOA -- A Visual Software Analytics Platform for Mobile Applications,R. Minelli; M. Lanza,"Fac. of Inf., Univ. of Lugano, Lugano, Switzerland",2013 IEEE International Conference on Software Maintenance,20131202,2013,,,476,479,"Mobile applications, also known as apps, are dedicated software systems that run on handheld devices, such as smartphones and tablet computers. The apps business has in a few years turned into a multi-billion dollar market. From a software engineering perspective apps represent a new phenomenon, and there is a need for tools and techniques to analyze apps. We present SAMOA, a visual web-based software analytics platform for mobile applications. It mines software repositories of apps and uses a set of visualization techniques to present the mined data. We describe SAMOA, detail the analyses it supports, and describe a methodology to understand apps from a structural and historical perspective. The website of SAMOA, containing the screen cast of the tool demo, is located at http://samoa.inf.usi.ch/about.",1063-6773;10636773,Electronic:978-0-7695-4981-1; POD:978-1-4673-5218-5,10.1109/ICSM.2013.76,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676936,apps;mobile applications;samoa;software evolution;software visualization;tool demo,Androids;Ecosystems;History;Humanoid robots;Mobile communication;Software;Visualization,Internet;data visualisation;mobile computing,SAMOA;dedicated software systems;handheld devices;mobile applications;multibillion dollar market;smartphones;software repositories;tablet computers;visual Web-based software analytics platform;visual software analytics platform;visualization techniques,,3,,10,,no,22-28 Sept. 2013,,IEEE,IEEE Conference Publications
SASH: Enabling continuous incremental analytic workflows on Hadoop,M. Sethi; N. Sachindran; S. Raghavan,"IBM India Research Lab Manyatha Embassy Business Park, Outer Ring Road, Bangalore, India",2013 IEEE 29th International Conference on Data Engineering (ICDE),20130624,2013,,,1219,1230,"There is an emerging class of enterprise applications in areas such as log data analysis, information discovery, and social media marketing that involve analytics over large volumes of unstructured and semi-structured data. These applications are leveraging new analytics platforms based on the MapReduce framework and its open source Hadoop implementation. While this trend has engendered work on high-level data analysis languages, NoSQL data stores, workflow engines etc., there has been very little attention to the challenges of deploying analytic workflows into production for continuous operation. In this paper, we argue that an essential platform component for enabling continuous production analytic workflows is an analytics store. We highlight five key requirements that impact the design of such a store: (i) efficient incremental operations, (ii) flexible storage model for hierarchical data, (iii) snapshot support (iv) object-level incremental updates, and (v) support for handling change sets. We describe the design of SASH, a scalable analytics store that we have developed on top of HBase to address these requirements. Using the workload from a production workflow that powers search within IBM's intranet and extranet, we demonstrate orders of magnitude improvement in IO performance using SASH.",1063-6382;10636382,Electronic:978-1-4673-4910-9; POD:978-1-4673-4909-3,10.1109/ICDE.2013.6544911,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6544911,,Computer architecture;Data models;Indexing;Libraries;Media;Production,SQL;data analysis;data mining;data structures;high level languages;intranets;object-oriented programming;public domain software;social networking (online);storage management;workflow management software,HBase;IBM intranet and extranet;IO performance;MapReduce framework;NoSQL data stores;SASH;continuous incremental analytic workflows;continuous operation;continuous production analytic workflows;enterprise applications;flexible storage model;handling change sets support;hierarchical data;high-level data analysis languages;incremental operations;information discovery;log data analysis;object-level incremental updates;open source Hadoop implementation;platform component;production workflow;scalable analytics store;semistructured data;snapshot support;social media marketing;unstructured data;workflow engines,,2,2,27,,no,8-12 April 2013,,IEEE,IEEE Conference Publications
ScatterBlogs2: Real-Time Monitoring of Microblog Messages through User-Guided Filtering,H. Bosch; D. Thom; F. Heimerl; E. PÌ_ttmann; S. Koch; R. KrÌ_ger; M. WÌ_rner; T. Ertl,"Visualization and Interactive Systems, University of Stuttgart",IEEE Transactions on Visualization and Computer Graphics,20131016,2013,19,12,2022,2031,"The number of microblog posts published daily has reached a level that hampers the effective retrieval of relevant messages, and the amount of information conveyed through services such as Twitter is still increasing. Analysts require new methods for monitoring their topic of interest, dealing with the data volume and its dynamic nature. It is of particular importance to provide situational awareness for decision making in time-critical tasks. Current tools for monitoring microblogs typically filter messages based on user-defined keyword queries and metadata restrictions. Used on their own, such methods can have drawbacks with respect to filter accuracy and adaptability to changes in trends and topic structure. We suggest ScatterBlogs2, a new approach to let analysts build task-tailored message filters in an interactive and visual manner based on recorded messages of well-understood previous events. These message filters include supervised classification and query creation backed by the statistical distribution of terms and their co-occurrences. The created filter methods can be orchestrated and adapted afterwards for interactive, visual real-time monitoring and analysis of microblog feeds. We demonstrate the feasibility of our approach for analyzing the Twitter stream in emergency management scenarios.",1077-2626;10772626,,10.1109/TVCG.2013.186,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634195,Blogs;Information retrieval;Labeling;Microblog analysis;Real-time systems;Social network services;Spatiotemporal phenomena;Twitter;filter construction;information visualization;live monitoring;query construction;social media monitoring;text analytics;text classification;visual analytics,Blogs;Information retrieval;Labeling;Real-time systems;Social network services;Spatiotemporal phenomena;Twitter,decision making;emergency management;information filtering;information filters;meta data;pattern classification;query processing;social networking (online);statistical distributions,ScatterBlogs2;Twitter;co-occurrence statistical distribution;decision making;emergency management scenarios;message retrieval;metadata restrictions;microblog feeds;microblog message real-time monitoring;microblog posts;query creation;situational awareness;supervised classification;task-tailored message filters;term statistical distribution;time-critical tasks;user-defined keyword queries;user-guided filtering,Algorithms;Blogging;Computer Graphics;Computer Systems;Information Storage and Retrieval;Reproducibility of Results;Sensitivity and Specificity;Social Media;Software;User-Computer Interface,19,,35,,no,Dec. 2013,,IEEE,IEEE Journals & Magazines
Searching under the Streetlight for Useful Software Analytics,P. M. Johnson,University of Hawaii at Manoa,IEEE Software,20130626,2013,30,4,57,63,"For more than 15 years, researchers at the Collaborative Software Development Laboratory at the University of Hawaii at Manoa have looked for analytics that help developers understand and improve development processes and products. This article reviews that research and discusses the trade-off between studying easily obtained analytics and studying richer analytics with higher overhead.",0740-7459;07407459,,10.1109/MS.2013.69,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6509376,measurement;software analytics;software engineering;software quality;software quality assurance,Analytical models;Collaboration;Data analysis;Data collection;Software development;Software measurement;Software metrics;Software quality,software engineering,Collaborative Software Development Laboratory;Manoa;University of Hawaii;software analytics,,2,,13,,yes,July-Aug. 2013,,IEEE,IEEE Journals & Magazines
Semi-formal and formal interface specification for system of systems architecture,J. Bryans; R. Payne; J. Holt; S. Perry,"School of Computing Science, Newcastle University, UK",2013 IEEE International Systems Conference (SysCon),20130701,2013,,,612,619,"The independence of the constituent systems of a system of systems presents a key challenge to the discipline of system of systems (SoS) engineering. The fact that constituent systems can and do function independently of the SoS means that engineers of a constituent system cannot rely on the behaviour of other constituent systems. This paper advocates a model-based approach to SoS engineering that requires the interfaces to constituent systems to be specified. We propose an use of an interface design pattern for interface specification that uses the industry standard notation, SysML.We also indicate a translation of these specifications to a formal notation, CML, in order to extend the range of analytic techniques available to the SoS engineer.",,Electronic:978-1-4673-3108-1; POD:978-1-4673-3107-4,10.1109/SysCon.2013.6549946,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6549946,,Computer architecture;Contracts;Object oriented modeling;Protocols;Unified modeling language,formal specification;software architecture;user interfaces,CML;SoS;SysML;constituent systems;formal notation;industry standard notation;interface design pattern;interface specification;semiformal interface specification;system of systems architecture,,8,,37,,no,15-18 April 2013,,IEEE,IEEE Conference Publications
Sensation of learning analytics top prevail the software engineering education,N. Pratheesh; D. Thirupathigrlau,"Department of Computer Applications School of Computer Science & Engineering Bharathiar University, Coimbatore - 641 046, India",2013 International Conference on Advanced Computing and Communication Systems,20141030,2013,,,1,7,"Software pockets an indispensable role in the modern living style to make their work easy. Software engineers are the people who develop the software to comply with the user needs and make them joyful. Software engineering education is the place where the software engineers mold up academically for the society's requirement. As a result, software engineering education grabs the essence in the computer education, albeit it fall shorts to cook up the genius to meet with the industries necessity. To overcome these issues, researchers suggested number of software engineering learning/teaching methods to egg on students to acquire their profundity knowledge in software engineering. Even though the suggestions do not utterly surmount this crucial issue since the suggested approaches did not attract the maj ority of the students and touch their learning style. Learning analytics plays a vital role to improve the students learning activities. Learning style and learning engagement are the key factors for the learning analytics. This paper focuses on learning style and learning engagement of the software engineering students. This work classifies the students in line with their learning style and identifies the needs of the software engineering students using learning engagement finally proposed the model to inspire the students to overcome such issues and motivate them in gathering software engineering knowledge.",,Electronic:978-1-4799-3506-2; POD:978-1-4799-3507-9,10.1109/ICACCS.2013.6938704,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6938704,Learning Analytics;Learning Engagement;Learning Style;Learning/Teaching Methods;Social Learning Analytics;Software Engineering;Software Engineering Education,Educational institutions;Industries;Knowledge engineering;Materials;Software;Software engineering,computer aided instruction;computer science education;software engineering;teaching,computer education;learning analytics;learning engagement;learning style;software engineering education;software engineering learning methods;software engineering teaching methods,,0,,35,,no,19-21 Dec. 2013,,IEEE,IEEE Conference Publications
Senseseer mobile-cloud-based Lifelogging framework,R. Albatal; C. Gurrin; J. Zhou; Y. Yang; D. Carthy; N. Li,CLARITY - Dublin City University,2013 IEEE International Symposium on Technology and Society (ISTAS): Social Implications of Wearable Computing and Augmediated Reality in Everyday Life,20130930,2013,,,144,146,"Smart-phones are becoming our constant companions, they are with us all of the time, being used for calling, web surfing, apps, music listening, TV viewing, social networking, buying, gaming, and a myriad of other uses. Smart-phones are a technology that knows us much better than most of us could imagine. Based on our usage and the fact that we are never far away from our smart phones, they know where we go, who we interact with, what information we consume, and with a little clever software, they can know what we are doing and even why we are doing it. They are beginning to know us better than we know ourselves. In this work we present SenseSeer a generic mobile-cloud-based mobile Lifelogging framework. This framework supports customisable analytic services for sensing the person, understanding the semantics of life activities and the easy deployment of analytic tools and novel interfaces. At present, SenseSeer supports services in many domains, such as personal health monitoring, location tracking, lifestyle analysis and tourism focused applications. This work demonstrate the design principles of SenseSeer and three of its services: My Health, My Location and My Social Activity.",2158-3404;21583404,Electronic:978-1-4799-0929-2; POD:978-1-4799-0930-8,10.1109/ISTAS.2013.6613113,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6613113,,Data collection;Data mining;Mobile communication;Mobile handsets;Semantics;Sensors;Servers,Web services;assisted living;cloud computing;human computer interaction;mobile computing;smart phones,SenseSeer;analytic tool deployment;customisable analytic service;life activity semantics;mobile cloud-based mobile Lifelogging framework;novel interface;smart phone,,2,,10,,no,27-29 June 2013,,IEEE,IEEE Conference Publications
SentiView: Sentiment Analysis and Visualization for Internet Popular Topics,C. Wang; Z. Xiao; Y. Liu; Y. Xu; A. Zhou; K. Zhang,"Software Eng. Inst., East China Normal Univ., Shanghai, China",IEEE Transactions on Human-Machine Systems,20131125,2013,43,6,620,630,"There would be value to several domains in discovering and visualizing sentiments in online posts. This paper presents SentiView, an interactive visualization system that aims to analyze public sentiments for popular topics on the Internet. SentiView combines uncertainty modeling and model-driven adjustment. By searching and correlating frequent words in text data, it mines and models the changes of the sentiment on public topics. In addition, using a time-varying helix together with an attribute astrolabe to represent sentiments, it can visualize the changes of multiple attributes and relationships among demographics of interest and the sentiments of participants on popular topics. The relationships of interest among different participants are presented in a relationship map. Using a new evolution model that is based on cellular automata, it is able to compare the time-varying features for sentiment-driven forums on both simulated and real data. Adaptable for different social networking platforms, such as Twitter, blog and forum, the methods demonstrate the effectiveness of SentiView in analyzing and visualizing public sentiments on the Web.",2168-2291;21682291,,10.1109/THMS.2013.2285047,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6650118,Microblog;sentiment;social networks;visual analytics;web forums,Data mining;Data visualization;Internet;Market research;Sentiment analysis;Social network services,data visualisation;interactive systems;social networking (online),Internet popular topics;SentiView;Twitter;blog;cellular automata;demographics;frequent words;interactive visualization system;model-driven adjustment;online posts sentiments;public sentiments visualization;public topics;relationship map;sentiment analysis;sentiment-driven forums;social networking platforms;text data;time-varying features;time-varying helix;uncertainty modeling,,10,,32,,no,Nov. 2013,,IEEE,IEEE Journals & Magazines
Service-Generated Big Data and Big Data-as-a-Service: An Overview,Z. Zheng; J. Zhu; M. R. Lyu,"Dept. of Comput. Sci. & Eng., Chinese Univ. of Hong Kong, Hong Kong, China",2013 IEEE International Congress on Big Data,20130916,2013,,,403,410,"With the prevalence of service computing and cloud computing, more and more services are emerging on the Internet, generating huge volume of data, such as trace logs, QoS information, service relationship, etc. The overwhelming service-generated data become too large and complex to be effectively processed by traditional approaches. How to store, manage, and create values from the service-oriented big data become an important research problem. On the other hand, with the increasingly large amount of data, a single infrastructure which provides common functionality for managing and analyzing different types of service-generated big data is urgently required. To address this challenge, this paper provides an overview of service-generated big data and Big Data-as-a-Service. First, three types of service-generated big data are exploited to enhance system performance. Then, Big Data-as-a-Service, including Big Data Infrastructure-as-a-Service, Big Data Platform-as-a-Service, and Big Data Analytics Software-as-a-Service, is employed to provide common big data related services (e.g., accessing service-generated big data and data analytics results) to users to enhance efficiency and reduce cost.",2379-7703;23797703,Electronic:978-0-7695-5006-0; POD:978-1-4799-0182-1,10.1109/BigData.Congress.2013.60,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6597164,Big Data-as-a-Service;big data;service computing,Data handling;Data storage systems;Data visualization;Fault tolerance;Fault tolerant systems;Information management;Quality of service,cloud computing;service-oriented architecture,Internet;big data analytics software-as-a-service;big data infrastructure-as-a-service;big data platform-as-a-service;cloud computing;service computing;service-generated big data,,23,,51,,no,June 27 2013-July 2 2013,,IEEE,IEEE Conference Publications
Shared memory aware MPSoC software deployment,T. SchÌ_nwald; A. Viehl; O. Bringmann; W. Rosenstiel,"FZI Forschungszentrum Informatik, Haid-und-Neu-Str. 10-14, 76131 Karlsruhe, Germany","2013 Design, Automation & Test in Europe Conference & Exhibition (DATE)",20130504,2013,,,1771,1776,"In this paper we present a novel approach for mapping interconnected software components onto cores of homogenous MPSoC architectures. The analytic mapping process considers shared memory communication as well as the routing algorithm controlling packet-based communication. The software components are mapped with the constraints of avoiding communication conflicts as well as access conflicts to shared memory resources. The core of the elaborated approach consists of an algorithm for software mapping which is inspired by force-directed scheduling from high-level synthesis. Experimental results show that the presented approach increases the overall system performance by 22% while reducing the average communication latency by 35%. For presenting the major advantages of the developed solution, we optimized an advanced driver assistance system on the Tilera TILEPro64 processor.",1530-1591;15301591,Electronic:978-3-9815370-0-0; POD:978-1-4673-5071-6,10.7873/DATE.2013.356,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6513802,,Computer architecture;Data communication;Force;Matched filters;Routing;Software;Software algorithms,,,,0,,24,,no,18-22 March 2013,,IEEE,IEEE Conference Publications
Simfrastructure: A Flexible and Adaptable Middleware Platform for Modeling and Analysis of Socially Coupled Systems,K. R. Bisset; S. Deodhar; H. Makkapati; M. V. Marathe; P. Stretz; C. L. Barrett,"Network Dynamics & Simulation Sci. Lab., Virginia Tech, Blacksburg, VA, USA","2013 13th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing",20130624,2013,,,506,513,"Socially coupled systems are comprised of interdependent social, organizational, economic, infrastructure and physical networks. Today's urban regions serve as an excellent example of such systems. People and institutions confront the implications of the increasing scale of information becoming available due to a combination of advances in pervasive computing, data acquisition systems as well as high performance computing. Integrated modeling and decision making environments are necessary to support planning, analysis and counter factual experiments to study these complex systems. Here, we describe SIMFRASTRUCTURE - a flexible coordination middleware that supports high performance computing oriented decision and analytics environments to study socially coupled systems. Simfrastructure provides a multiplexing mechanism by which simple and intuitive user-interfaces can be plugged in as front-end systems, and high-end computing resources can be plugged in as back-end systems for execution. This makes the computational complexity of the simulations completely transparent to the users. The decoupling of user interfaces and data repository from simulation execution allows users to access simulation results asynchronously and enables them to add new datasets and simulation models dynamically. Simfrastructure enables implementation of a simple yet powerful modeling environment with built-in analytics-as-a service platform, which provides seamless access to high end computational resources, through an intuitive interface for studying socially coupled systems. We illustrate the applicability of Simfrastructure in the context of an integrated modeling environment to study public health epidemiology.",,Electronic:978-0-7695-4996-5; POD:978-1-4673-6465-2,10.1109/CCGrid.2013.78,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6546132,Socially coupled systems;distributed systems;middleware;public health epidemiology;software architecture,Analytical models;Computational modeling;Computer architecture;Data models;Data transfer;Libraries;Middleware,computational complexity;data acquisition;decision making;digital simulation;health care;middleware;parallel processing;socio-economic effects;ubiquitous computing;user interfaces,Simfrastructure;adaptable middleware platform;back-end systems;built-in analytics-as-a-service platform;computational complexity;data acquisition systems;data repository;decision making;economic networks;flexible coordination middleware;flexible middleware platform;front-end systems;high end computational resource access;high performance computing;high-end computing resources;infrastructure networks;integrated modeling environments;interdependent networks;intuitive user interfaces;multiplexing mechanism;organizational networks;pervasive computing;physical networks;public health epidemiology;simulation execution;social networks;socially coupled system analysis;socially coupled system modeling,,1,,24,,no,13-16 May 2013,,IEEE,IEEE Conference Publications
Simulation of 2.5-dimensional borehole acoustic waves with convolutional perfectly matched layer,L. Liu; W. j. Lin; H. l. Zhang; X. m. Wang,"State Key Lab. of Acoust., Inst. of Acoust., Beijing, China","2013 Symposium on Piezoelectricity, Acoustic Waves, and Device Applications",20140623,2013,,,1,4,"A 2.5-dimensional method using the PDE package of the commercial finite element software COMSOL Multiphysics is developed to simulate wave propagation in a borehole. The computation is conducted in the frequency wave-number domain. A convolutional perfectly matched layer is implemented to eliminate the reflections from artificial truncation boundaries. Waveforms obtained in time domain are in good agreement with analytic solutions in a special model, which proves the validity of the method. A numerical modeling example is presented to illustrate the capabilities of the method. It is shown that this method can be used to solve a variety of non-axisymmetric borehole acoustic wave propagation problems.",,Electronic:978-1-4799-3288-7; POD:978-1-4799-3514-7,10.1109/SPAWDA.2013.6841132,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6841132,2.5-dimensional;Borehole acoustics;Convolutional PML;Finite element method;Non-axisymmetric,Acoustics;Fluids;Frequency-domain analysis;Mathematical model;Propagation;Solids;Steel,acoustic wave propagation;digital simulation;drilling (geotechnical);finite element analysis;mining industry;partial differential equations;well logging,2.5-dimensional borehole acoustic wave simulation;PDE package;artificial truncation boundaries;convolutional perfectly matched layer;finite element software COMSOL Multiphysics;frequency wave-number domain;nonaxisymmetric borehole acoustic wave propagation problems;wave propagation simulation;waveforms,,0,,10,,no,25-27 Oct. 2013,,IEEE,IEEE Conference Publications
Simulation of CSS communication system in VLF atmospheric noise,Y. Wu; H. Wang; L. Sun; G. Xu,"Coll. of Commun. Eng., PLA Univ. of Sci. & Technol., Nanjing, China",2013 IEEE Third International Conference on Information Science and Technology (ICIST),20140227,2013,,,1580,1583,"Chirp spread spectrum (CSS) technique has been extensively studied in the application of Ultra-wide band (UWB). Considering the long distance propagation capacity and low intercepted probability of chirp signal, we propose to apply the CSS technique to the very low frequency (VLF) communication system. First, we discuss the CSS system based on fractional Fourier transform (FRFT), including the definition of FRFT and means of modulation and demodulation. Then we introduce an analytic model of VLF atmospheric noise presented by E C. Field [1] and generate atmospheric noise by MATLAB software. Finally, we perform simulation of the CSS communication system in atmospheric noise. The simulation results validate its feasibility.",2164-4357;21644357,Electronic:978-1-4673-2764-0; POD:978-1-4673-2762-6,10.1109/ICIST.2013.6747838,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6747838,,Atmospheric modeling;Cascading style sheets;Chirp;Communication systems;Demodulation;Fourier transforms;Noise,Fourier transforms;spread spectrum communication;ultra wideband communication,CSS communication system;VLF;atmospheric noise;chirp signal;chirp spread spectrum technique;fractional Fourier transform;long distance propagation capacity;low intercepted probability;ultra-wide band communication;very low frequency communication system,,0,,10,,no,23-25 March 2013,,IEEE,IEEE Conference Publications
SketchPadN-D: WYDIWYG Sculpting and Editing in High-Dimensional Space,B. Wang; P. Ruchikachorn; K. Mueller,"Visual Analytics and Imaging Laboratory, Computer Science Department, Stony Brook University",IEEE Transactions on Visualization and Computer Graphics,20131016,2013,19,12,2060,2069,"High-dimensional data visualization has been attracting much attention. To fully test related software and algorithms, researchers require a diverse pool of data with known and desired features. Test data do not always provide this, or only partially. Here we propose the paradigm WYDIWYGS (What You Draw Is What You Get). Its embodiment, SketchPad<sup>ND</sup>, is a tool that allows users to generate high-dimensional data in the same interface they also use for visualization. This provides for an immersive and direct data generation activity, and furthermore it also enables users to interactively edit and clean existing high-dimensional data from possible artifacts. SketchPad<sup>ND</sup> offers two visualization paradigms, one based on parallel coordinates and the other based on a relatively new framework using an N-D polygon to navigate in high-dimensional space. The first interface allows users to draw arbitrary profiles of probability density functions along each dimension axis and sketch shapes for data density and connections between adjacent dimensions. The second interface embraces the idea of sculpting. Users can carve data at arbitrary orientations and refine them wherever necessary. This guarantees that the data generated is truly high-dimensional. We demonstrate our tool's usefulness in real data visualization scenarios.",1077-2626;10772626,,10.1109/TVCG.2013.190,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634118,Data visualization;Image color analysis;N-D navigation;Shape analysis;Synthetic data generation;data acquisition and management;data editing;high-dimensional data;interaction;multiple views;multivariate data;parallel coordinates;scatterplot;user interface,Data visualization;Image color analysis;Shape analysis,data visualisation;user interfaces,N-D polygon;SketchPad<sup>N-D</sup>;WYDIWYG editing;WYDIWYG sculpting;What You Draw Is What You Get;data cleaning;data density;direct data generation activity;high-dimensional data visualization;immersive data generation activity;interactive data editing;probability density functions,"Algorithms;Computer Graphics;Image Interpretation, Computer-Assisted;Paintings;Reproducibility of Results;Sculpture;Sensitivity and Specificity;Software;User-Computer Interface",2,,25,,no,Dec. 2013,,IEEE,IEEE Journals & Magazines
Small Is Beautiful: Summarizing Scientific Workflows Using Semantic Annotations,P. Alper; K. Belhajjame; C. Goble; P. Karagoz,"Sch. of Comput. Sci., Univ. of Manchester, Manchester, UK",2013 IEEE International Congress on Big Data,20130916,2013,,,318,325,"Scientific workflows have become the workhorse of Big Data analytics for scientists. As well as being repeatable and optimizable pipelines that bring together datasets and analysis tools, workflows make-up an important part of the provenance of data generated from their execution. By faithfully capturing all stages in the analysis, workflows play a critical part in building up the audit-trail (a.k.a. provenance) meta-data for derived datasets and contributes to the veracity of results. Provenance is essential for reporting results, reporting the method followed, and adapting to changes in the datasets or tools. These functions, however, are hampered by the complexity of workflows and consequently the complexity of data-trails generated from their instrumented execution. In this paper we propose the generation of workflow description summaries in order to tackle workflow complexity. We elaborate reduction primitives for summarizing workflows, and show how primitives, as building blocks, can be used in conjunction with semantic workflow annotations to encode different summarization strategies. We report on the effectiveness of the method through experimental evaluation using real-world workflows from the Tavern a system.",2379-7703;23797703,Electronic:978-0-7695-5006-0; POD:978-1-4799-0182-1,10.1109/BigData.Congress.2013.49,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6597153,Annotation;Motif;Rule-Based Summarization;Scientific Workflow,Complexity theory;Libraries;Ontologies;Organizations;Pipelines;Ports (Computers);Semantics,meta data;natural sciences computing;workflow management software,Tavern;audit-trail meta-data;big data analytics;data provenance;data-trail complexity;scientific workflow summarization;semantic workflow annotations;workflow complexity;workflow description summary generation,,2,,10,,no,June 27 2013-July 2 2013,,IEEE,IEEE Conference Publications
Smart data centers for green Clouds,D. Bruneo; M. Fazio; F. Longo; A. Puliafito,"Dipt. di Ing. Civile, Inf., Edile, Ambientale e Mat. Ambientale, Univ. degli Studi di Messina, Messina, Italy",2013 IEEE Symposium on Computers and Communications (ISCC),20140306,2013,,,3,8,"The key idea behind this work is the development of smart data centers able to monitor, control, and manage themselves through advanced analytics and management policies, collecting and analyzing real time data on their behavior. They also make adjustments to interdependent components across the physical infrastructure, in order to address changing business and technology needs. To this aim, we have designed and implemented a new framework for supporting green computing in the management of Cloud data centers. Through heterogeneous sensing devices deployed in the system, the framework is able to know the working state of the data center and to activate specific energy saving policies. We have evaluated the proposed solution through experiments on a real testbed implemented at the University of Messina. Experiments show interesting results thus proving real benefits deriving from the adoption of the proposed framework.",1530-1346;15301346,Electronic:978-1-4799-3755-4; POD:978-1-4799-3756-1; USB:978-1-4799-3754-7,10.1109/ISCC.2013.6754914,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6754914,Cloud;data center;green computing;heterogeneous devices;sensing;smart management,Blades;Green products;Monitoring;Sensors;Servers;Software;Standards,cloud computing;computer centres;energy conservation;green computing,University of Messina;analytics policy;business needs;cloud data centers;data analysis;data center control;data center management;data center monitoring;data collection;energy saving policies;green clouds;green computing;management policy;smart data centers;technology needs,,3,,13,,no,7-10 July 2013,,IEEE,IEEE Conference Publications
Software analytics for incident management of online services: An experience report,J. G. Lou; Q. Lin; R. Ding; Q. Fu; D. Zhang; T. Xie,"Microsoft Res. Asia, Beijing, China",2013 28th IEEE/ACM International Conference on Automated Software Engineering (ASE),20140102,2013,,,475,485,"As online services become more and more popular, incident management has become a critical task that aims to minimize the service downtime and to ensure high quality of the provided services. In practice, incident management is conducted through analyzing a huge amount of monitoring data collected at runtime of a service. Such data-driven incident management faces several significant challenges such as the large data scale, complex problem space, and incomplete knowledge. To address these challenges, we carried out two-year software-analytics research where we designed a set of novel data-driven techniques and developed an industrial system called the Service Analysis Studio (SAS) targeting real scenarios in a large-scale online service of Microsoft. SAS has been deployed to worldwide product datacenters and widely used by on-call engineers for incident management. This paper shares our experience about using software analytics to solve engineers' pain points in incident management, the developed data-analysis techniques, and the lessons learned from the process of research development and technology transfer.",,Electronic:978-1-4799-0215-6; POD:978-1-4799-0216-3,10.1109/ASE.2013.6693105,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693105,Online service;incident management;service incident diagnosis,Measurement;Monitoring;Radiation detectors;Runtime;Servers;Software;Synthetic aperture sonar,Internet;computer centres;data handling;program diagnostics;technology transfer,Microsoft;Service Analysis Studio;data-analysis techniques;data-driven incident management;data-driven techniques;large data scale;large-scale online service;monitoring data;online services;research development;software analytics;software-analytics research;technology transfer;worldwide product datacenters,,7,,27,,yes,11-15 Nov. 2013,,IEEE,IEEE Conference Publications
Software Analytics for Mobile Applications--Insights & Lessons Learned,R. Minelli; M. Lanza,"REVEAL @ Fac. of Inf., Univ. of Lugano, Lugano, Switzerland",2013 17th European Conference on Software Maintenance and Reengineering,20130415,2013,,,144,153,"Mobile applications, known as apps, are software systems running on handheld devices, such as smartphones and tablet PCs. The market of apps has rapidly expanded in the past few years into a multi-billion dollar business. Being a new phenomenon, it is unclear whether approaches to maintain and comprehend traditional software systems can be ported to the context of apps. We present a novel approach to comprehend apps from a structural and historical perspective, leveraging three factors for the analysis: source code, usage of third-party APIs, and historical data. We implemented our approach in a web-based software analytics platform named SAMOA. We detail our approach and the supporting tool, and present a number of findings obtained while investigating a corpus of mobile applications. Our findings reveal that apps differ significantly from traditional software systems in a number of ways, which calls for the development of novel approaches to maintain and comprehend them.",1534-5351;15345351,Electronic:978-0-7695-4948-4; POD:978-1-4673-5833-0,10.1109/CSMR.2013.24,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6498463,mining software repositories;mobile applications;software analytics;software evolution;software maintenance;software visualization,Androids;History;Humanoid robots;Measurement;Mobile communication;Software systems,application program interfaces;mobile computing,SAMOA platform;Web-based software analytics;mobile application;smart phone;software analytics;tablet PC;tablet personal computer;third-party API,,12,,36,,yes,5-8 March 2013,,IEEE,IEEE Conference Publications
Software Analytics in Practice,D. Zhang; S. Han; Y. Dang; J. G. Lou; H. Zhang; T. Xie,Microsoft Research Asia,IEEE Software,20130903,2013,30,5,30,37,"With software analytics, software practitioners explore and analyze data to obtain insightful, actionable information for tasks regarding software development, systems, and users. The StackMine project produced a software analytics system for Microsoft product teams. The project provided lessons on applying software analytics technologies to positively impact software development practice. The lessons include focusing on problems that practitioners care about, using domain knowledge for correct data understanding and problem modeling, building prototypes early to get practitioners' feedback, taking into account scalability and customizability, and evaluating analysis results using criteria related to real tasks.",0740-7459;07407459,,10.1109/MS.2013.94,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6559957,Algorithm design and analysis;Data mining;Debugging;Performance analysis;Software analytics;Software engineering;Software systems;StackMine;actionable information;data exploration;insightful information;mining software repositories;software analytics;software artifacts;software engineering;technology transfer,Algorithm design and analysis;Data mining;Debugging;Performance analysis;Software analytics;Software engineering;Software systems,data analysis;software engineering,Microsoft product team;StackMine project;data analysis;data customizability;data exploration;data scalability;data understanding;domain knowledge;problem modeling;software analytics technology;software development practice,,10,,13,,no,Sept.-Oct. 2013,,IEEE,IEEE Journals & Magazines
Software analytics: Achievements and challenges,D. Zhang; T. Xie,"Microsoft Research Asia, Beijing, 100080, China",2013 35th International Conference on Software Engineering (ICSE),20130926,2013,,,1487,1487,"A huge wealth of various data exist in the practice of software development. Further rich data are produced by modern software and services in operation, many of which tend to be data-driven and/or data-producing in nature. Hidden in the data is information about the quality of software and services or the dynamics of software development. Software analytics is to utilize a data-driven approach to enable software practitioners to perform data exploration and analysis in order to obtain insightful and actionable information; such information is used for completing various tasks around software systems, software users, and software development process. This tutorial presents achievements and challenges of research and practice on principles, techniques, and applications of software analytics, highlighting success stories in industry, research achievements that are transferred to industrial practice, and future research and practice directions in software analytics.",0270-5257;02705257,Electronic:978-1-4673-3076-3; POD:978-1-4673-3075-6,10.1109/ICSE.2013.6606753,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606753,,,,,,0,,8,,no,18-26 May 2013,,IEEE,IEEE Conference Publications
Software Analytics: So What?,T. Menzies; T. Zimmermann,,IEEE Software,20130626,2013,30,4,31,37,"The guest editors of this special issue of IEEE Software invited submissions that reflected the benefits (and drawbacks) of software analytics, an area of explosive growth. They had so many excellent submissions that they had to split this special issue into two volumes--you'll see even more content in the September/October issue. They divided the articles on conceptual grounds, so both volumes will feature equally excellent work. The Web extra at http://youtu.be/nO6X0azR0nw is a video interview in which IEEE Software editor in chief Forrest Shull speaks with Tim Menzies about the growing importance of software analytics.",0740-7459;07407459,,10.1109/MS.2013.86,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6547619,analysis;big data;measurement;metrics;software analytics,Data analysis;Data models;Decision making;Software algorithms;Software development;Software engineering;Special issues and sections,program diagnostics;software engineering,IEEE Software;explosive software growth;software analytics,,15,,10,,no,July-Aug. 2013,,IEEE,IEEE Journals & Magazines
"Software defined networks (SDN) - Enabling virtualised, programmable infrastructure",K. Bloch,"Cisco Australia, Sydney, NSW, Australia",38th Annual IEEE Conference on Local Computer Networks - Workshops,20140310,2013,,,xxxi,xxxi,"Abstract form only given. In its formative stages, SDN was popular in research and venture capital communities. However as organisations tackle issues such as complexity, automation and time to market, the industry has become involved in this transition. SDN potentially addresses some of these challenges at the same time as exposing new options for cloud, predictive analytics and the Internet of Everything. This presentation provides an industry perspective of the drivers, status and potential of SDN and the promise of infrastructure virtualisation and programmability.",,Electronic:978-1-4799-0540-9; POD:978-1-4799-0538-6,10.1109/LCNW.2013.6758469,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6758469,,,Internet of Things;software radio;virtualisation,Internet of Everything;SDN;cloud predictive analytics;infrastructure virtualisation;programmable infrastructure;software defined networks,,0,,,,no,21-24 Oct. 2013,,IEEE,IEEE Conference Publications
"Software defined networks (SDN) - Enabling virtualised, programmable infrastructure",K. Bloch,"Cisco Australia & New Zealand, Sydney, NSW, Australia",38th Annual IEEE Conference on Local Computer Networks,20140310,2013,,,xxxi,xxxi,"In its formative stages, SDN was popular in research and venture capital communities. However as organisations tackle issues such as complexity, automation and time to market, the industry has become involved in this transition. SDN potentially addresses some of these challenges at the same time as exposing new options for cloud, predictive analytics and the Internet of Everything. This presentation provides an industry perspective of the drivers, status and potential of SDN and the promise of infrastructure virtualisation and programmability.",0742-1303;07421303,Electronic:978-1-4799-0537-9; POD:978-1-4799-0535-5,10.1109/LCN.2013.6761213,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6761213,,,cloud computing;organisational aspects;virtualisation,Internet-of-everything;SDN;cloud analytics;infrastructure virtualisation;predictive analytics;programmable infrastructure;research communities;software defined networks;venture capital communities,,0,,,,no,21-24 Oct. 2013,,IEEE,IEEE Conference Publications
Software engineering for the industrial Internet: Situation-aware smart applications,H. A. MÌ_ller,"Department of Computer Science University of Victoria Victoria, Canada",2013 15th IEEE International Symposium on Web Systems Evolution (WSE),20131024,2013,,,1,1,"Summary form only given. With the rise of the Industrial Internet the world entered a new era of innovation. At the heart of this new industrial revolution is the convergence of the global industrial system with computing power, low-cost sensing, big data, predictive analytics, and ubiquitous connectivity. The growing proliferation of smart devices and applications is accelerating the convergence of the physical and the digital worlds. Smart apps allow users, with the help of sensors and networks, to do a great variety of things, from tracking their friends to controlling remote devices and machines. At the core of such smart systems are self-adaptive systems that optimize their own behaviour according to high-level objectives and constraints to address changes in functional and non-functional requirements as well as environmental conditions. Self-adaptive systems are implemented using four key technologies: runtime models, context management, feedback control theory, and run-time verification and validation. The proliferation of highly dynamic and smart applications challenges the software engineering community in re-thinking the boundary between development time and run time and developing techniques for adapting systems at run time. The key challenge is to automate traditional software engineering, maintenance and evolution techniques to adapt and evolve systems at run time with minimal or no human interference. Hitherto, most developers did not instrument their software with sensors and effectors to observe whether requirements are satisfied in an evolving environment at run time. One way to break out of this mold is to make the four key technologies readily accessible at run time.",1550-4441;15504441,Electronic:978-1-4799-1610-8; POD:978-1-4799-1609-2,10.1109/WSE.2013.6642408,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6642408,context management;feedback control;run-time verification and validation;runtime models;self-adaptive systems;smart systems,Computer science;Convergence;Educational institutions;Internet;Sensors;Software;Software engineering,,,,1,,,,no,27-27 Sept. 2013,,IEEE,IEEE Conference Publications
Statistical model of power supply subsystem Satellite,M. Mirshams; E. Zabihian; A. R. Zabihian,"Space Research Laboratory &#x201C;Space RL&#x201D;, K.N.Toosi University of Technology Faculty of Aerospace Engineering, Tehran, Iran",2013 6th International Conference on Recent Advances in Space Technologies (RAST),20130815,2013,,,665,669,"In this paper, based on the fact that in designing the energy providing subsystem for satellites, most approaches and relations are empirical and statistical, and also, considering the aerospace sciences and its relation with other engineering fields such as electrical engineering to be young, these are no analytic or one hundred percent proven empirical relations in many fields. Therefore, we consider the statistical design of this subsystem. The presented approach in this paper is entirely innovative and all parts of the energy providing subsystem for the satellite are specified. In codifying this approach, the data of 602 satellites and some software programs such as SPSS have been used. In this approach, after proposing the design procedure, the total needed power for the satellite, the mass of the energy providing subsystem, the material of solar array, and finally the placement of these arrays on the satellite are designed. All these parts are designed based on the mission of the satellite and its weight class.",,Electronic:978-1-4673-6396-9; POD:978-1-4673-6395-2,10.1109/RAST.2013.6581293,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6581293,Statistical model;database;power supply subsystem;the design procedure,Arrays;Batteries;Chemicals;Satellites;Sensors;Silicon;Space vehicles,aerospace computing;aircraft power systems;artificial satellites;electrical engineering;solar cell arrays,SPSS software programs;aerospace sciences;electrical engineering;energy providing subsystem;power supply subsystem satellite;satellite mission;solar array;statistical model,,2,,8,,no,12-14 June 2013,,IEEE,IEEE Conference Publications
Surveillance of sentiment and affect in open source text,J. Carlson; P. David; T. Hawes,"Decisive Analytics Corp., Arlington, VA, USA",2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2013),20140410,2013,,,1054,1057,"New topics of discussion emerge continuously as political and cultural environments shift. These topics can convey a range of ideas, stories, values, and shared experiences. In open source data, discourse is often infused with emotionally-charged prose expressed by large and diverse pools of authors. Manually identifying and analyzing the emotional content of combined discussions from millions of individuals is an impossible task for any single analyst or decision maker. Technologies that can automatically identify emerging topics of discussion and align them with public opinions can provide early warning to acts of violence or other threats [1]. We present an approach to detecting basic emotions associated with topics by combining work in topic modeling with affect analysis. We apply the correspondence LDA topic model (CorrLDA2) [2] to correlate emotional states with topics. Using this technique, topics have associated emotions and emotion categories are correlated over a corpus of text.",,Electronic:978-1-4799-1498-2; POD:978-1-4799-1497-5,10.1145/2492517.2500323,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6785831,CorrLDA2;affect analysis;sentiment analysis;topic modeling,Adaptation models;Analytical models;Conferences;Data mining;Resource management;Social network services;Standards,emotion recognition;public domain software;text analysis,LDA topic model;affect analysis;affect surveillance;emotion categories;emotion detection;emotional content;emotional states;latent Dirichlet allocation;open source text;public opinions;sentiment surveillance;text corpus;topic modeling,,0,,9,,no,25-28 Aug. 2013,,IEEE,IEEE Conference Publications
Symbolic derivation of nonlinear benchmark bicycle dynamics with holonomic and nonholonomic constraints,E. X. Wang; J. Zou; Y. Liu; Q. Fan; Y. Xiang,"Sch. of Inf. Eng., GDUT, Guangzhou, China",16th International IEEE Conference on Intelligent Transportation Systems (ITSC 2013),20140130,2013,,,316,323,"We present a symbolic method for modeling nonlinear multibody underactuated systems with holonomic and nonholonomic constraints. Using MAPLE software, we are able to solve the quartic holonomic constraint analytically. We then use the constraints and extra Lagrange-Euler equations to systematically eliminate all the auxiliary coordinates and Lagrange multipliers, thereby obtaining a minimum set of unconstrained nonlinear analytic ordinary differential equations corresponding to the degrees of freedom of the system. The method is applied to a benchmark bicycle, in which all the six ground contact constraint equations are eliminated, leaving analytic coupled ordinary differential equations corresponding to the bicycle rear body roll, steer angle, and rear wheel rotation degrees of freedom without any approximation. This reduced analytic model offers insights in understanding complex nonlinear bicycle dynamic behaviors and enables the development of an efficient model suitable for real time control outside of the linear regime.",2153-0009;21530009,Electronic:978-1-4799-2914-6; POD:978-1-4799-2915-3; USB:978-1-4799-2913-9,10.1109/ITSC.2013.6728251,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6728251,,Analytical models;Benchmark testing;Bicycles;Equations;Mathematical model;Wheels,bicycles;differential equations;mechanical contact;vehicle dynamics,Lagrange multipliers;Lagrange-Euler equations;MAPLE software;bicycle rear body roll;ground contact constraint equations;nonholonomic constraints;nonlinear benchmark bicycle dynamics;nonlinear multibody underactuated systems;rear wheel rotation;steer angle;symbolic method;unconstrained nonlinear analytic ordinary differential equations,,1,,26,,no,6-9 Oct. 2013,,IEEE,IEEE Conference Publications
"Synergizing people, process, and technology to motivate knowledge sharing and collaboration Industry case study",C. K. Pickering,"IT Collaboration Engineering, Intel Corporation, Chandler, AZ, USA",2013 International Conference on Collaboration Technologies and Systems (CTS),20130725,2013,,,35,39,"Intel is the world's largest Semiconductor chip manufacturing company with over 95,000 employees and 164 sites in 63 countries across the globe. Not only does Intel's IT group keep Intel's business operations running, it also contributes to Intel's business transformation via user experience research and architecture path finding for leading edge technologies. IT sees social computing as a strategic way to improve collaboration, foster innovation, and facilitate learning. Our research has identified the best opportunities for using social computing and other technologies to boost collaboration and productivity across Intel. To keep achieving maximum benefit from its collaboration efforts, Intel IT continues to invest in social capabilities and also partners with Intel HR to help address cultural and motivational barriers. Beyond improving personal productivity, we are looking to enable efficiency in Intel's business divisions for product design, manufacturing, and sales, through the use of cutting-edge social technologies, including social analytics, immersive video/sketching, federated identity and access management, and cross system activity stream aggregation. Our goal is to continue to transform collaboration across Intel into a seamless and unified experience that brings together relevant information, people, and business intelligence to fully support employee and business workflows.",,Electronic:978-1-4673-6404-1; POD:978-1-4673-6403-4,10.1109/CTS.2013.6567200,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6567200,activity stream aggregation;business intelligence;business workflow;collaboration;expertise finding;future of work;immersive video;innovation;knowledge management;knowledge worker;recommendation engine;social analytics;social computing;unified user experience;workforce transformation,Architecture;Collaboration;Companies;Context;Engines;Social network services,business data processing;competitive intelligence;groupware;human resource management;innovation management;knowledge management;manufacturing data processing;personnel;product design;productivity;semiconductor industry;workflow management software,Intel HR;Intel business division;Intel business operations;Intel business transformation;access management;architecture path finding;business intelligence;business workflow;collaboration;cross system activity stream aggregation;cultural barriers;cutting-edge social technology;employee;immersive video;innovation;knowledge sharing;leading edge technology;learning;motivational barriers;personal productivity;product design;sales;semiconductor chip manufacturing company;sketching;social analytics;social capability;social computing;user experience research,,0,1,11,,no,20-24 May 2013,,IEEE,IEEE Conference Publications
Synthesizable Integrated Circuit and System Design for Solar Chargers,Y. C. Kuo; Y. J. Luo; L. J. Liu,"Department of Electronic Engineering , National Kaohsiung First University of Science and Technology, Kaohsiung, Taiwan",IEEE Transactions on Power Electronics,20130215,2013,28,9,4260,4266,"In this paper, an automatic design tool for a solar energy harvesting IC and system is developed with visual basic software, and the synthesis tool employed in this approach can be used to shorten the design time to market. In addition, a smart meter system is developed to measure the solar energy harvesting system's information with an online system. Users can thus get the proposed system's information at any time and from anywhere. Finally, good agreement has been found between the analytic and experimental results.",0885-8993;08858993,,10.1109/TPEL.2012.2222448,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6320660,Digital-to-analog converter (DAC);energy harvesting;pulsewidth modulation (PWM),Batteries;Computer architecture;Microcontrollers;Pulse width modulation;Solar energy;Transistors,Visual BASIC;digital-analogue conversion;electronic engineering computing;energy harvesting;integrated circuit design;solar power;time to market,Visual Basic software;automatic design tool;digital-to-analog converter;online system;smart meter system;solar charger;solar energy harvesting IC;solar energy harvesting system information;synthesis tool;synthesizable integrated circuit;system design;time to market,,1,,25,,no,Sept. 2013,,IEEE,IEEE Journals & Magazines
Teaching business analytics,L. Yang; X. Liu,"Dept. of Comput. Sci. & Eng., Univ. of Tennessee at Chattanooga, Chattanooga, TN, USA",2013 IEEE Frontiers in Education Conference (FIE),20131219,2013,,,1516,1518,"It is essential to prepare students with knowledge and skills in area of business analytics (BA) which will help business to process data, find patterns and relations, develop insights from past transactions, and make prediction. We develop hands-on labs to teach business analytics to students in Computer Science, Information Technology, and Software Engineering disciplines. Our hands-on labs can be adopted in courses such as database systems, data warehousing, data mining, etc. We use enterprise BA tools including MS SQL Server Business Intelligence and Cognos 10 platforms, which are essential to increase student interests, improve student learning, and enhance student confidence. Our hands-on labs contain three parts with one is built upon another: 1) Data integration; 2) Data Warehouse; and 3) Business analytics.",0190-5848;01905848,Electronic:978-1-4673-5261-1; POD:978-1-4673-5259-8; USB:978-1-4673-5260-4,10.1109/FIE.2013.6685090,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6685090,business analytics;data analytics;hands-on learning,Barium;Data handling;Data mining;Data storage systems;Education;Information management,SQL;business data processing;computer aided instruction;data integration;data warehouses;educational courses;knowledge management;management education;teaching,Cognos 10 platforms;MS SQL server business intelligence;business analytics;business data process;computer science;courses;data integration;data mining;data warehouse;data warehousing;database systems;enterprise BA tools;hands-on labs;information technology;software engineering disciplines;student confidence;student interests;student learning;student skills;students knowledge;teaching,,0,,5,,no,23-26 Oct. 2013,,IEEE,IEEE Conference Publications
The Berkeley Data Analytics Stack: Present and future,M. Franklin,"UC Berkeley, Berkeley, CA, USA",2013 IEEE International Conference on Big Data,20131223,2013,,,2,3,"The Berkeley AMPLab was founded on the idea that the challenges of emerging Big Data applications requires a new approach to analytics systems. Launching in early 2011, the project set out to rethink the traditional analytics stack, breaking down technical and intellectual barriers that had arisen during decades of evolutionary development. The vision of the lab is to seamlessly integrate the three main resources available for making sense of data at scale: Algorithms (such as machine learning and statistical techniques), Machines (in the form of scalable clusters and elastic cloud computing), and People (both individually as analysts and en masse, as with crowdsourced human computation). To pursue this goal, we assembled a research team with diverse interests across computer science, forged relationships with domain experts on campus and elsewhere, and obtained the support of leading industry partners and major government sponsors. The lab is realizing its ideas through the development of a freely-available Open Source software stack called BDAS: the Berkeley Data Analytics Stack. In the nearly three years the lab has been in operation, we've released major components of BDAS. Several of these components have gained significant traction in industry and elsewhere: the Mesos cluster resource manager, the Spark inmemory computation framework, and the Shark query processing system. BDAS shows up prominently in many industry discussions of the future of the Big Data analytics ecosystem - a rare degree of impact for an ongoing academic project. Given this initial success, the lab is continuing on its research path, moving ""up the stack"" to better integrate and support deep machine learning and to make people a full-fledged resource for making sense of Big Data. In this talk, I'll first outline the motivation and insights behind our research approach and describe how we have organized to address the cross-disciplinary nature of Big Data challenges. I will then describe- the current state of BDAS with an emphasis on the key components listed above and will address our current efforts on machine learning scalability and ease of use, and hybrid human/computer processing. Finally I will present our current views of how all the pieces will fit together to form a system that can adaptively bring the right resources to bear on a given data-driven question to meet time, cost and quality requirements throughout the analytics lifecycle.",,Electronic:978-1-4799-1293-3; POD:978-1-4799-1294-0,10.1109/BigData.2013.6691545,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6691545,,,Big Data;cloud computing;learning (artificial intelligence),Berkeley data analytics stack;Mesos cluster resource manager;Shark query processing system;Spark inmemory computation framework;elastic cloud computing;evolutionary development;hybrid human-computer processing;machine learning;ppen source software stack;scalable cluster;statistical technique,,1,,,,no,6-9 Oct. 2013,,IEEE,IEEE Conference Publications
"The BTWorld use case for big data analytics: Description, MapReduce logical workflow, and empirical evaluation",T. Hegeman; B. GhiÅ£; M. Capot€Ä; J. Hidders; D. Epema; A. Iosup,"Parallel & Distrib. Syst. Group, Delft Univ. of Technol., Delft, Netherlands",2013 IEEE International Conference on Big Data,20131223,2013,,,622,630,"The commoditization of big data analytics, that is, the deployment, tuning, and future development of big data processing platforms such as MapReduce, relies on a thorough understanding of relevant use cases and workloads. In this work we propose BTWorld, a use case for time-based big data analytics that is representative for processing data collected periodically from a global-scale distributed system. BTWorld enables a data-driven approach to understanding the evolution of BitTorrent, a global file-sharing network that has over 100 million users and accounts for a third of today's upstream traffic. We describe for this use case the analyst questions and the structure of a multi-terabyte data set. We design a MapReduce-based logical workflow, which includes three levels of data dependency - inter-query, inter-job, and intra-job - and a query diversity that make the BTWorld use case challenging for today's big data processing tools; the workflow can be instantiated in various ways in the MapReduce stack. Last, we instantiate this complex workflow using Pig-Hadoop-HDFS and evaluate the use case empirically. Our MapReduce use case has challenging features: small (kilobytes) to large (250 MB) data sizes per observed item, excellent (10<sup>-6</sup>) and very poor (10<sup>2</sup>) selectivity, and short (seconds) to long (hours) job duration.",,Electronic:978-1-4799-1293-3; POD:978-1-4799-1294-0,10.1109/BigData.2013.6691631,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6691631,,Data handling;Data processing;Data storage systems;Engines;Information management;Open source software;Programming,Big Data;data analysis;peer-to-peer computing;query processing,BTWorld use case;BitTorrent evolution understanding;MapReduce logical workflow;MapReduce stack;MapReduce-based logical workflow;Pig-Hadoop-HDFS;big data processing platform;data dependency;data-driven approach;global file-sharing network;global-scale distributed system;interjob;interquery;intrajob;job duration;multiterabyte data set;query diversity;time-based big data analytics;upstream traffic,,4,,33,,no,6-9 Oct. 2013,,IEEE,IEEE Conference Publications
The effectiveness evaluation of nano-satellites used in military operations,Y. f. Tang; X. h. Yu,"Acad. of Equip., Beijing, China","Proceedings 2013 International Conference on Mechatronic Sciences, Electric Engineering and Computer (MEC)",20140828,2013,,,3012,3016,"The feasibility and effectiveness of nano-satellites in military operations need further study. An overview of nano-satellite programs with explicit military objectives was presented. In order to evaluate the effectiveness of nano-satellite, an optimization method for reconnaissance nano-satellite constellation was proposed based on a simulation environment using STK and Visual C++ software tools. Then, a more detailed effectiveness evaluation process for the optimized constellation was proposed using fuzzy Analytical Hierarchy Process (AHP) method. The result shows that a nano-satellite constellation is more effective than a large satellite for tactical missions.",,CD-ROM:978-1-4799-2563-6; Electronic:978-1-4799-2565-0; POD:978-1-4799-2566-7,10.1109/MEC.2013.6885544,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6885544,constellation;effectiveness evaluation;military operation;nano-satellite,Imaging;Indexes;Military aircraft;Reconnaissance;Satellites;Space vehicles;Vectors,aerospace computing;aerospace simulation;analytic hierarchy process;artificial satellites;fuzzy set theory;optimisation;software tools,AHP method;STK;Visual C++ software tools;fuzzy analytical hierarchy process;military operations;nanosatellite effectiveness evaluation;optimization method;reconnaissance nanosatellite constellation;tactical missions,,0,,8,,no,20-22 Dec. 2013,,IEEE,IEEE Conference Publications
The enterprises' core competencies evaluation based on knowledge capitals: An empirical study in hotel industry,W. Zhang,"Tourism Dept., Tianjin Univ. of Finance & Econ., Tianjin, China","2013 6th International Conference on Information Management, Innovation Management and Industrial Engineering",20140109,2013,2,,522,525,"Facing increasingly drastic competition situation, the enterprises imminently need to build core competencies in order to gain and sustain competitive advantage. The impact of knowledge capitals on the enterprises' core competencies is increasing. Based on differentiating these related concepts of resources, capabilities, competencies, core competencies, traditional capitals and knowledge capitals, from a perspective of knowledge capitals, this paper divided the enterprises' core competencies into five dimensions: human capitals and staff capabilities, information capitals and technology capabilities, organizational capitals and management capabilities, customer capitals and marketing capabilities, social capitals and cooperation capabilities, constructs the evaluation indicators system of the enterprises' core competencies, calculates the evaluation indicators weights in hotel industry by yaahp V6.0 software, and comprehensively evaluates and compares four hotels through AHP (Analytical Hierarchy Process) method.",2155-1456;21551456,Electronic:978-1-4799-0245-3; POD:978-1-4799-0243-9,10.1109/ICIII.2013.6703203,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6703203,Analytical Hierarchy Process;Core Competencies;Hotels;Knowledge Capitals,Companies;Educational institutions;Industries;Production;Software,analytic hierarchy process;customer services;hotel industry;information management;knowledge management;labour resources;marketing;organisational aspects;social sciences;technology management,AHP;analytical hierarchy process method;cooperation capabilities;customer capitals;enterprise core competency evaluation;evaluation indicators system;hotel industry;human capitals;information capitals;knowledge capitals;management capabilities;marketing capabilities;organizational capitals;social capitals;staff capabilities;technology capabilities;traditional capitals;yaahp V6.0 software,,0,,4,,no,23-24 Nov. 2013,,IEEE,IEEE Conference Publications
The extension of GDSS architecture by the subsystem of group decision method synthesis,N. Mironova,"Software Eng. Dept., Zaporizhzhya Nat. Tech. Univ., Zaporizhzhya, Ukraine",2013 IEEE 7th International Conference on Intelligent Data Acquisition and Advanced Computing Systems (IDAACS),20131114,2013,1,,216,219,"In the article the architecture of group decision support system is described. The developed system differs from the existing ones by the presence of the subsystem for synthetic procedure of group decision, which allows to synthesize the required version of group decision method.",,Electronic:978-1-4799-1429-6; POD:978-1-4799-1425-8,10.1109/IDAACS.2013.6662674,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6662674,Analytic Hierarchy Process;group decision problem;group decision support system;synthetic procedure of group decision,Analytic hierarchy process;Computer architecture;Databases;Decision support systems;Information systems;Software,analytic hierarchy process;group decision support systems,GDSS architecture;group decision method synthesis;group decision support systems;group decision synthetic procedure,,0,,10,,no,12-14 Sept. 2013,,IEEE,IEEE Conference Publications
The Handoff Performance of Mobile Wi-Fi Systems in Vehicular Networks,J. Yoo,"Dept. of Software Design & Manage., Gachon Univ., Seongnam, South Korea",2013 International Conference on Information Science and Applications (ICISA),20130815,2013,,,1,2,The Mobile Wi-Fi system using Mobile Access Points (M-APs) is a new and effective method for vehicles to access the Internet service. This paper presents an analytic model to evaluate several handoff algorithms for using M-APs in vehicular networks. The analytic results show that the best handoff strategy depends on factors such as M-AP availability or connection duration.,2162-9048;21629048,Electronic:978-1-4799-0604-8; POD:978-1-4799-0602-4,10.1109/ICISA.2013.6579342,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6579342,,Analytical models;Delays;IEEE 802.11 Standards;Mobile communication;Mobile computing;Throughput;Vehicles,Internet;mobility management (mobile radio);radio access networks;wireless LAN,Internet service;M-AP;analytic model;handoff performance;mobile Wi-Fi system;mobile access point;vehicular network,,0,,4,,no,24-26 June 2013,,IEEE,IEEE Conference Publications
The Lowly API Is Ready to Step Front and Center,L. Garber,,Computer,20130821,2013,46,8,14,17,The API is taking on new roles and is becoming critical to important technologies such as cloud computing and to the use of both Web and mobile applications.,0018-9162;00189162,,10.1109/MC.2013.290,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6583176,API;API frameworks;API service providers;BaaS;M2M;OAuth;OpenID Connect;REST;Representational State Transfer;Web applications;analytics;application programming interface;back end as a service;cloud computing;machine to machine;mobile applications;software,Application programming interfaces;Cloud computing;Computer interfaces;Internet;Software development,application program interfaces;cloud computing;mobile computing,API;Web applications;cloud computing;mobile applications,,0,,,,no,13-Aug,,IEEE,IEEE Journals & Magazines
The Many Faces of Software Analytics,T. Menzies; T. Zimmermann,West Virginia University,IEEE Software,20130903,2013,30,5,28,29,"Articles regarding the many faces of software analytics highlight the power of analytics for different types of organizations: large organizations and open source projects, as well as small- to medium-sized projects.",0740-7459;07407459,,10.1109/MS.2013.114,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6588525,analysis;analytics;software analytics,Software analytics;Software engineering;Special issues and sections,,,,1,,,,no,Sept.-Oct. 2013,,IEEE,IEEE Journals & Magazines
Thermal analysis software design and implementation applied in line focusing solar collector,Zhang Qianqian; Zhang Yingchun,"Sch. of Electr. Eng. & Autom. Qilu, Univ. of Technol., Jinan, China","Proceedings 2013 International Conference on Mechatronic Sciences, Electric Engineering and Computer (MEC)",20140828,2013,,,2510,2513,This paper introduces the implementation process of the software which can analyze the thermal performance of the line focusing solar collector. The paper focused on the mathematical model and the algorithm of the software. Heat-collecting analytic software uses least squares to solve the problem of the multiple high order equations. Finally the software is completed using the Matlab GUI.,,CD-ROM:978-1-4799-2563-6; Electronic:978-1-4799-2565-0; POD:978-1-4799-2566-7,10.1109/MEC.2013.6885459,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6885459,Matlab software;Solar collector;least square method;mathematical model;thermal properties,Equations;Heating;Metals;Optical reflection;Wind speed,least squares approximations;power engineering computing;solar absorber-convertors;thermal analysis,Matlab GUI;heat-collecting analytic software;high order equations;least squares;line focusing solar collector;mathematical model;software algorithm;software process;thermal analysis software design;thermal performance,,0,,4,,no,20-22 Dec. 2013,,IEEE,IEEE Conference Publications
TimeBench: A Data Model and Software Library for Visual Analytics of Time-Oriented Data,A. Rind; T. Lammarsch; W. Aigner; B. Alsallakh; S. Miksch,"Vienna University of Technology, Institute of Software Technology & Interactive Systems",IEEE Transactions on Visualization and Computer Graphics,20131016,2013,19,12,2247,2256,"Time-oriented data play an essential role in many Visual Analytics scenarios such as extracting medical insights from collections of electronic health records or identifying emerging problems and vulnerabilities in network traffic. However, many software libraries for Visual Analytics treat time as a flat numerical data type and insufficiently tackle the complexity of the time domain such as calendar granularities and intervals. Therefore, developers of advanced Visual Analytics designs need to implement temporal foundations in their application code over and over again. We present TimeBench, a software library that provides foundational data structures and algorithms for time-oriented data in Visual Analytics. Its expressiveness and developer accessibility have been evaluated through application examples demonstrating a variety of challenges with time-oriented data and long-term developer studies conducted in the scope of research and student projects.",1077-2626;10772626,,10.1109/TVCG.2013.206,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634096,Data models;Data structures;Data visualization;Time-domain analysis;Visual Analytics;Visual analytics;information visualization;software infrastructure;temporal data;time;toolkits,Data models;Data structures;Data visualization;Time-domain analysis;Visual analytics,data models;data visualisation;software libraries;time-domain analysis,TimeBench;application code;calendar granularity;data model;electronic health records;flat numerical data type;foundational data algorithms;foundational data structures;long-term developer study;medical insights;network traffic;software library;temporal foundations;time domain complexity;time-oriented data;visual analytics designs,"Algorithms;Computer Graphics;Decision Making;Decision Support Techniques;Pattern Recognition, Automated;Software;User-Computer Interface",7,,52,,no,Dec. 2013,,IEEE,IEEE Journals & Magazines
TimeSeer: Scagnostics for High-Dimensional Time Series,T. N. Dang; A. Anand; L. Wilkinson,"Dept. of Comput. Sci., Univ. of Illinois at Chicago, Chicago, IL, USA",IEEE Transactions on Visualization and Computer Graphics,20130109,2013,19,3,470,483,"We introduce a method (Scagnostic time series) and an application (TimeSeer) for organizing multivariate time series and for guiding interactive exploration through high-dimensional data. The method is based on nine characterizations of the 2D distributions of orthogonal pairwise projections on a set of points in multidimensional euclidean space. These characterizations include measures, such as, density, skewness, shape, outliers, and texture. Working directly with these Scagnostic measures, we can locate anomalous or interesting subseries for further analysis. Our application is designed to handle the types of doubly multivariate data series that are often found in security, financial, social, and other sectors.",1077-2626;10772626,,10.1109/TVCG.2012.128,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200267,Scagnostics;high-dimensional visual analytics;multiple time series;scatterplot matrix,Density measurement;Employment;Length measurement;Lenses;Shape;Time series analysis;Visualization,data analysis;time series,2D distributions;TimeSeer;density;doubly multivariate data series;financial sectors;high-dimensional time series scagnostics;interactive exploration;multidimensional euclidean space;multivariate time series;orthogonal pairwise projections;outliers;security sectors;shape;skewness;social sectors;texture,"Algorithms;Computer Graphics;Computer Simulation;Image Interpretation, Computer-Assisted;Imaging, Three-Dimensional;Models, Statistical;Multivariate Analysis;Reproducibility of Results;Sensitivity and Specificity;Software;User-Computer Interface",5,,48,,no,13-Mar,,IEEE,IEEE Journals & Magazines
Toward a Care Process Metamodel: For business intelligence healthcare monitoring solutions,S. A. Behnam; O. Badreddin,"Dept. of Electrical Engineering and Computer Science University of Ottawa Ottawa, Canada",2013 5th International Workshop on Software Engineering in Health Care (SEHC),20130919,2013,,,79,85,"Improving care processes in healthcare institutions relies on effectively monitoring and making timely decisions for improving patient experience. Business Intelligence solutions have proven to be effective for monitoring processes in other industries. However, healthcare organizations face three challenges for implementing Business Intelligence solutions that effectively monitor care processes. First, the great variation of processes in healthcare domain makes it difficult to model them. Second, there is a gap between abstract administrative indicators and fine-grained operation-level measures of healthcare processes. Finally, it is difficult to reuse the underlying healthcare processes used for other successful solutions. In this paper, we present a Care Process Metamodel geared towards modeling healthcare processes. This metamodel (a) provides a platform for creating uniform care processes, (b) enables hierarchical care processes for modeling of composite processes as well as bridging the gap between abstract performance indicators and operation-level measures of healthcare processes, and (c) facilitates reusing the processes and the data structures required for monitoring them. This metamodel thus addresses some of the challenges for implementing successful Business Intelligence care process monitoring solutions for healthcare organizations. We also demonstrate how the Care Process Metamodel-based processes fit into an architecture, where data collected about encounters of patients can be used by stakeholders for improving the process and its execution. We use samples of cardiac-related processes to illustrate our approach.",,Electronic:978-1-4673-6282-5; POD:978-1-4673-6281-8,10.1109/SEHC.2013.6602483,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6602483,Business Analytics;Business Intelligence Solution;Care Process Metamodel;Care Process Model;Healthcare Process Monitoring;Performance Indicators,Bismuth;Discharges (electric);Measurement;Medical services;Monitoring;Organizations,competitive intelligence;data structures;decision making;health care;hospitals,abstract administrative indicators;business intelligence healthcare monitoring solutions;business intelligence solutions;cardiac-related processes;care process metamodel-based processes;data structures;decision making;fine-grained operation-level measures;healthcare domain;healthcare institutions;healthcare organizations;healthcare process modeling;operation-level measures;patient experience,,0,,17,,no,20-21 May 2013,,IEEE,IEEE Conference Publications
Toward a scale-out data-management middleware for low-latency enterprise computing,L. L. Fong; Y. Gao; X. R. Guerin; Y. G. Liu; T. Salo; S. R. Seelam; W. Tan; S. Tata,"IBM Research Division, Thomas J. Watson Research Center, Yorktown Heights, NY, USA",IBM Journal of Research and Development,20130517,2013,57,4-Mar,6:01,6:14,"Emerging transactional workloads from Internet and mobile commerce require low-latency, massive-scale, and integrated data analytics to enhance user experience and to improve up-selling opportunities. These analytics require new application platforms that must be able to absorb large volumes of data, provide low-latency access to the data, and cache data objects to improve access times in distributed environments. This paper reports on recent technologies built at IBM Research to address challenges in data access latency, data ingestion, and caching in the exemplary context of an online product recommendation application. We describe three technologies related to the issues and optimizations of key-value data object store and access. First, we describe the architecture of a global secondary index to greatly improve data access latency of Hadoop‰ã¢ Database (HBase‰ã¢), an open-source key-value distributed data store. Second, we present an in-memory write-ahead log feature on HBase that significantly improves write operations for high-volume data ingestion. Third, we detail an innovative distributed caching system that exploits low-latency interconnects to use hash maps of data keys on each server for local lookup, while data resides and are accessed across clustered systems. The distributed cache can achieve a 100- to 1,000-fold performance gain over many caching methods. These technologies together form some necessary building blocks for a next-generation data-centric middleware for integrated transaction and analytic workloads.",0018-8646;00188646,,10.1147/JRD.2013.2240171,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6517300,,Computational modeling;Data models;Data processing;Distributed databases;Information management;Internet;Real-time systems;Servers,,,,0,3,,,no,May-July 2013,,IBM,IBM Journals & Magazines
Towards a Hybrid Approach of Primitive Cognitive Network Process and K-Means Clustering for Social Network Analysis,C. Guan; K. K. F. Yuen,"Dept. of Comput. Sci. & Software Eng., Xi'an Jiaotong-Liverpool Univ., Suzhou, China","2013 IEEE International Conference on Green Computing and Communications and IEEE Internet of Things and IEEE Cyber, Physical and Social Computing",20131212,2013,,,1267,1271,"Social network sites (SNSs) have been influencing the social activities of many people. Consequently, analysis of social network data may produce meaningful information for decision making. This paper represents the basic hybrid approach of Primitive Cognitive Network Process (PCNP) and classical K-Means Clustering for grouping users in social network sites (SNSs) into appropriate clusters by the similarities among users. This new method has combined the PCNP approach, which is a revised approach of the Analytic Hierarchy Process (AHP), and the K-means method for evaluating the weighted attributes influencing the similarity between users. The proposed approach can act as a friends referring function in various kinds of SNSs.",,Electronic:978-0-7695-5046-6; POD:978-1-4799-0631-4,10.1109/GreenCom-iThings-CPSCom.2013.220,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6682233,Clustering;Data Mining;Decision Making;K-Means;Primitive Cognitive Network Process;Social Network,Analytic hierarchy process;Artificial intelligence;Clustering algorithms;Clustering methods;Data mining;Educational institutions;Social network services,analytic hierarchy process;data mining;pattern clustering;social networking (online),AHP;PCNP;SNSs;analytic hierarchy process;data mining;decision making;hybrid approach;k-means clustering;primitive cognitive network process;social network analysis;social network sites;weighted attributes,,1,,13,,no,20-23 Aug. 2013,,IEEE,IEEE Conference Publications
Towards a knowledge base for a wind electric pumping system design,A. Arbaoui; M. A. Bennini; M. Asbik,"M2I, Ecole Nationale sup&#x00E9;rieure d'Arts et M&#x00E9;tiers, BP 4024, Mekn&#x00E8;s Isma&#x00EF;lia, Morocco",2013 International Renewable and Sustainable Energy Conference (IRSEC),20130613,2013,,,224,227,"A knowledge base tool for wind electric pumping system design has been presented. In this paper, the used approach is illustrated through the rotor component. The performance of the rotor is predicted using the axial momentum theory combined with the blade element theory. The design variables of the blade are its geometry and the airfoil aerodynamic characteristics which are introduced into the tool using the analytic model AERODAS. To validate the developed model, it is formulated as a Mixed Complementarity (Problem and is solved using the GAMS software (Generalized Algebraic Modeling System). The model validation refers to the experimental data of NREL (National Renewable Energy Laboratory) obtained using the NASA Ames wind tunnel. The validated model is then used to design a new blade for a small wind turbine which we attempt to manufacture locally.",,Electronic:978-1-4673-6374-7; POD:978-1-4673-6373-0,10.1109/IRSEC.2013.6529708,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6529708,Design;GAMS software;Knowledge base;Model validation;Pumping;Small wind turbine,Laboratories;Legged locomotion;Monitoring,aerodynamics;blades;rotors;wind tunnels;wind turbines,GAMS software;NASA Ames wind tunnel;NREL;National Renewable Energy Laboratory;airfoil aerodynamic characteristics;analytic model AERODAS;axial momentum theory;blade element theory;generalized algebraic modeling system;rotor component;small wind turbine;wind electric pumping,,1,,13,,no,7-9 March 2013,,IEEE,IEEE Conference Publications
Towards a quality model for the evaluation of DSS based on KDD process,E. Ben Ayed; M. Ben Ayed,"REGIM-Lab: REsearch Group on Intelligent Machines, ENIS, University of Sfax, Sfax, Tunisia",2013 International Conference on Advanced Logistics and Transport,20130725,2013,,,187,192,"A Decision Support System (DSS) based on Knowledge Discovery from Data (KDD) process is used to give confident knowledge to the final users in order to help them making right decisions. Such systems can be underused if the mined knowledge is unconfident, or if the system is hardly usable or unusable. Our target is to supply out a Quality Model (QM) ensuring a global evaluation of DSS based on KDD process (DSS/KDD). In our point of view, a QM should involve three dimensions: the evaluation of the DSS as a Software Product, as a User Interface and as a DSS. We should also take into account ISO recommendations. We intend to build a model which defines a set of criteria and allows measurement of a DSS/KDD quality evaluation using Goal-Question Method (GQM) and Analytic Hierarchy Process (AHP).",,Electronic:978-1-4799-0313-9; POD:978-1-4799-0314-6,10.1109/ICAdLT.2013.6568457,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6568457,Decision Support System;ISO;Knowledge Discovery from Data;Quality model;Usability;Utility,Data mining;Decision support systems;ISO standards;Measurement;Usability,data mining;decision support systems;user interfaces,AHP;DSS evaluation;GQM;KDD process;analytic hierarchy process;decision support system;goal question method;knowledge discovery from data;quality model;software product;user interface,,2,,42,,no,29-31 May 2013,,IEEE,IEEE Conference Publications
Towards Cloud-Based Analytics-as-a-Service (CLAaaS) for Big Data Analytics in the Cloud,F. Zulkernine; P. Martin; Y. Zou; M. Bauer; F. Gwadry-Sridhar; A. Aboulnaga,"Sch. of Comput., Queen's Univ., Kingston, ON, Canada",2013 IEEE International Congress on Big Data,20130916,2013,,,62,69,"Data Analytics has proven its importance in knowledge discovery and decision support in different data and application domains. Big data analytics poses a serious challenge in terms of the necessary hardware and software resources. The cloud technology today offers a promising solution to this challenge by enabling ubiquitous and scalable provisioning of the computing resources. However, there are further challenges that remain to be addressed such as the availability of the required analytic software for various application domains, estimation and subscription of necessary resources for the analytic job or workflow, management of data in the cloud, and design, verification and execution of analytic workflows. We present a taxonomy for analytic workflow systems to highlight the important features in existing systems. Based on the taxonomy and a study of the existing analytic software and systems, we propose the conceptual architecture of CLoud-based Analytics-as-a-Service (CLAaaS), a big data analytics service provisioning platform, in the cloud. We outline the features that are important for CLAaaS as a service provisioning system such as user and domain specific customization and assistance, collaboration, modular architecture for scalable deployment and Service Level Agreement.",2379-7703;23797703,Electronic:978-0-7695-5006-0; POD:978-1-4799-0182-1,10.1109/BigData.Congress.2013.18,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6597120,AaaS;Analytics;CLAaaS;analysis;cloud;scientific workflow management system;service;taxonomy;workflow,Collaboration;Data handling;Data storage systems;Data visualization;Information management;Software;Taxonomy,cloud computing;data analysis;workflow management software,CLAaaS;analytic software;analytic workflow systems;analytics service provisioning platform;cloud based analytics as a service;cloud technology;computing resources;data analytics;knowledge discovery;modular architecture;scalable deployment;scalable provisioning;service level agreement;service provisioning system;software resources,,8,,29,,no,June 27 2013-July 2 2013,,IEEE,IEEE Conference Publications
Towards logistics service provider selection strategy using primitive cognitive network process,K. K. F. Yuen,"Department of Computer science and Software Engineering, Xi'an Jiaotong-Liverpool University, Suzhou Industrial Park, China",2013 10th International Conference on Service Systems and Service Management,20130919,2013,,,416,418,"Outsourcing of logistics activities is the common business practice in the enterprise. Selection of a proper logistics service provider (LSP) meeting the requirements of operation strategy of the outsourcing enterprise is a complex task, due to its significant influence on the business performance. This paper demonstrates an application of the Primitive Cognitive Network Process (P-CNP) to the LSP selection strategy considering multiple criteria and alternatives.",2161-1890;21611890,Electronic:978-1-4673-4843-0; POD:978-1-4673-3080-0,10.1109/ICSSSM.2013.6602547,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6602547,analytic hierarchy process;cognitive network process;logistic outsourcing;multiple criteria decision making;third-party logistics service provider,Analytic hierarchy process;Artificial intelligence;Companies;Logistics;Outsourcing,logistics;outsourcing,LSP selection strategy;P-CNP;business performance;logistics activities outsourcing;logistics service provider selection strategy;operation strategy requirements;outsourcing enterprise;primitive cognitive network process,,0,,10,,no,17-19 July 2013,,IEEE,IEEE Conference Publications
Towards Portable Learning Analytics Dashboards,A. Vozniuk; S. Govaerts; D. Gillet,"EPFL, Lausanne, Switzerland",2013 IEEE 13th International Conference on Advanced Learning Technologies,20130919,2013,,,412,416,"This paper proposes a novel approach to build and deploy learning analytics dashboards in multiple learning environments. Existing learning dashboards are barely portable: once deployed on a learning platform, it requires considerable effort to deploy the dashboard elsewhere. We suggest constructing dashboards from lightweight web applications, namely widgets. Our approach allows to port dashboards with no additional cost between learning environments that implement open specifications (Open Social and Activity Streams) for data access and use widget APIs. We propose to facilitate reuse by sharing the dashboards and widgets via a centralized analytics repository.",2161-3761;21613761,Electronic:978-0-7695-5009-1; POD:978-1-4799-0373-3,10.1109/ICALT.2013.126,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6601968,ActivityStreams;OpenSocial;dashboards;learning analytics;open standards;portability;widget,Blogs;Computer aided manufacturing;Context;Google;Media;Software;Standards,Internet;application program interfaces;computer aided instruction,centralized analytics repository;learning platform;lightweight Web applications;multiple learning environments;open social and activity streams;portable learning analytics dashboards;widget API,,4,,24,,no,15-18 July 2013,,IEEE,IEEE Conference Publications
Towards software defined ICN based edge-cloud services,R. Ravindran; Xuan Liu; A. Chakraborti; Xinwen Zhang; Guoqiang Wang,"Huawei Res. Center, Santa Clara, CA, USA",2013 IEEE 2nd International Conference on Cloud Networking (CloudNet),20140116,2013,,,227,235,"ICN deployment will be based on the grounds of saving CAPEX/OPEX and/or enabling new services. This paper makes a case for the latter leveraging, emerging technologies such as network function virtualization (NFV) and software defined networking (SDN). We propose a framework to enable ICN based service platform as virtualized network functions to enable several edge-cloud services such as enterprise applications, big data analytic, or M2M/IoT services. This platform is generic to support several ICN protocols and corresponding real-time and non-real time services leveraging ICN features such as name based routing, caching, multicasting, and flexible security techniques. As an implementation of this architecture, we discuss how a scalable network based conferencing solution can be realized over the proposed ICN platform and compare it with a peer-to-peer design through a performance analysis.",,Electronic:978-1-4799-0568-3; POD:978-1-4799-0567-6; USB:978-1-4799-0566-9,10.1109/CloudNet.2013.6710583,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6710583,,Bandwidth;Conferences;Context;Protocols;Real-time systems;Software;Virtualization,cache storage;cloud computing;computer network security;computer networks;routing protocols;virtualisation,CAPEX;ICN protocols;IoT services;M2M services;NFV;OPEX;SDN;big data analytic;caching;enterprise applications;flexible security techniques;information-centric networking;multicasting;network function virtualization;routing;software defined ICN based edge-cloud services;software defined networking;virtualized network functions,,8,,10,,no,11-13 Nov. 2013,,IEEE,IEEE Conference Publications
Tracer: A Tool to Measure and Visualize Student Engagement in Writing Activities,M. Liu; R. A. Calvo; A. Pardo,"Sch. of Electr. & Inf. Eng., Univ. of Sydney, Sydney, NSW, Australia",2013 IEEE 13th International Conference on Advanced Learning Technologies,20130919,2013,,,421,425,"Learning analytic techniques are allowing the observation of complex learning activities that were hidden until now. Writing is a task in which behavioral patterns can be observed to measure the level of engagement. Previous studies relied mostly on data collected by observers. In this paper Tracer, a novel learning analytic system to visualize behavioral patterns of students while writing and measuring engagement is described. The tool combines and analyzes the information obtained from document revisions and Website logs while students work in a writing assignment and provides visualizations and measurements for the level of engagement. A user study was conducted in a software engineering course where students wrote and submitted a project proposal using Google Docs. Tracer generated a graphical view of the gauged engagement, and an engagement time for each student. The obtained results show that the engagement time gauged by Tracer was moderately correlated to those reported by the students.",2161-3761;21613761,Electronic:978-0-7695-5009-1; POD:978-1-4799-0373-3,10.1109/ICALT.2013.129,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6601971,Behavioral Analytic;Engagement;Visualization,Computational modeling;Data visualization;Educational institutions;Google;History;Proposals;Writing,Web sites;computer aided instruction;software engineering,Google Docs;Website logs;behavioral patterns;complex learning activities;document revisions;learning analytic techniques;software engineering course;tracer;writing activities,,2,,17,,no,15-18 July 2013,,IEEE,IEEE Conference Publications
Understanding How Companies Interact with Free Software Communities,J. M. Gonzalez-Barahona; D. Izquierdo-Cortazar; S. Maffulli; G. Robles,Universidad Rey Juan Carlos,IEEE Software,20130903,2013,30,5,38,45,"When free, open source software development communities work with companies that use their output, it's especially important for both parties to understand how this collaboration is performing. The use of data analytics techniques on software development repositories can improve factual knowledge about performance metrics.",0740-7459;07407459,,10.1109/MS.2013.95,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6560081,Electronic mail;Open source software;Software analytics;Software measurement;Statistics;computing milieu;measurement;process metrics;software analytics;software engineering;statistics,Electronic mail;Open source software;Software analytics;Software measurement;Statistics,data analysis;public domain software;software metrics,data analytics techniques;free software communities;open source software development communities;software development repositories;software performance metrics,,3,,9,,no,Sept.-Oct. 2013,,IEEE,IEEE Journals & Magazines
Usage Control of Programs and Application Libraries in z/OS Environment in the Big Data Age,E. A. d. Reis; L. S. Brito,"Inf. Sci., Univ. Estadual Paulista-UNESP, Mari&#x0301;lia, Brazil",2013 IEEE 16th International Conference on Computational Science and Engineering,20140306,2013,,,1140,1141,"The great volume of information is currently a reality. Processing these data, mainly in institutions with a great capacity of processing, is carried out by means of mainframes that are capable of performing operations at a great speed and on an extremely large volume of data in the Big Data age. Indirect methods of gauging the usage of software installed in the mainframe environment are used to measure the usage of products, making it difficult to obtain information to renew these products. This paper presents the z/Audit product, which offers information on the actual usage of the programs installed in the mainframe environment. The product features data extration for mining (data mining) and for presentation (analytics) of this information with open-source products.",,Electronic:978-0-7695-5096-1; POD:978-1-4799-4897-0,10.1109/CSE.2013.167,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6755348,Data Mining;Information Analysis;z/OS Environment,Companies;Data handling;Data mining;Data storage systems;Information management;Software;Software algorithms,Big Data;data mining;feature extraction;mainframes;operating systems (computers);public domain software;software libraries,Big Data age;application libraries;data mining;mainframe environment;open-source products;product features data extration;product usage measurement;program usage control;software usage;z/AUDIT product;z/Audit product;z/OS environment,,0,,6,,no,3-5 Dec. 2013,,IEEE,IEEE Conference Publications
Use of simple analytic performance models for streaming data applications deployed on diverse architectures,J. C. Beard; R. D. Chamberlain,"Dept. of Computer Science and Engineering, Washington University in St. Louis, USA",2013 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS),20130715,2013,,,138,139,Modern hardware is often heterogeneous. With heterogeneity comes multiple abstraction layers that hide underlying complex systems. This complexity makes quantitative performance modeling a difficult task. Designers of high-performance streaming applications for heterogeneous systems must contend with unpredictable and often non-generalizable models to predict performance of a particular application and hardware mapping. This paper outlines a computationally simple approach that can be used to model the overall throughput and buffering needs of a streaming application on heterogeneous hardware. The model presented is based upon a hybrid maximum flow and decomposed discrete queueing model. The utility of the model is assessed using a set of real and synthetic benchmarks with model predictions compared to measured application performance.,,Electronic:978-1-4673-5779-1; POD:978-1-4673-5776-0,10.1109/ISPASS.2013.6557162,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6557162,,Computational modeling;Cryptography;Hardware;Kernel;Streaming media;Throughput;Transform coding,data analysis;parallel architectures;performance evaluation;queueing theory,abstraction layers;analytic performance model;buffering needs;decomposed discrete queueing model;diverse architectures;hardware mapping;heterogeneous hardware;heterogeneous systems;high-performance streaming data application;hybrid maximum flow;throughput;unpredictable nongeneralizable model,,2,,5,,no,21-23 April 2013,,IEEE,IEEE Conference Publications
Uses of differential-algebraic equations for trajectory planning and feedforward control of spatially two-dimensional heat transfer processes,A. Rauh; H. Aschemann; N. S. Nedialkov; J. D. Pryce,"Dept. of Mechatron., Univ. of Rostock, Rostock, Germany",2013 18th International Conference on Methods & Models in Automation & Robotics (MMAR),20131125,2013,,,155,160,"Tracking control design is a common task in many engineering applications, where efficient controllers commonly consist of two components. First, a feedback control law is implemented to guarantee asymptotic stability of the closed-loop system and to meet robustness requirements. Second, a feedforward control signal is usually integrated, which improves the tracking of predefined output trajectories in transient operating phases. The analytic computation of this feedforward signal often becomes complicated or even impossible, if the output variables of the system do not coincide with the system's flat outputs. The situation becomes even worse, if the system is not flat at all, or if nonlinearities are included in the system model, which makes analytic solutions for the state trajectories unavailable in the non-flat case. For these reasons, the differential-algebraic equation solver Daets was used in previous work to solve this task numerically. The current paper describes extensions of the use of Daets in the framework of feedforward control design for non-flat outputs of a spatially two-dimensional heat transfer process with significant parameter uncertainty and actuator constraints. Numerical and experimental results are presented for a suitable test rig at the Chair of Mechatronics at the University of Rostock, where the robust feedback control part is designed using linear matrix inequalities in combination with a polytopic uncertainty model.",,Electronic:978-1-4673-5508-7; POD:978-1-4673-5505-6,10.1109/MMAR.2013.6669898,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6669898,,Equations;Feedforward neural networks;Heat transfer;Heating;Mathematical model;Observers;Robustness,asymptotic stability;closed loop systems;control system synthesis;feedforward;heat systems;heat transfer;linear matrix inequalities;path planning;robust control;trajectory control,Daets solver;asymptotic stability;closed-loop system;differential-algebraic equations;feedback control law;feedforward control;feedforward control signal;linear matrix inequalities;polytopic uncertainty model;robust feedback control part design;robustness requirements;spatially two-dimensional heat transfer processes;tracking control design;trajectory planning;transient operating phase,,2,,21,,no,26-29 Aug. 2013,,IEEE,IEEE Conference Publications
Using Interactive Visual Reasoning to Support Sense-Making: Implications for Design,N. Kodagoda; S. Attfield; B. L. W. Wong; C. Rooney; S. Choudhury,"Middlesex Univ., London, UK",IEEE Transactions on Visualization and Computer Graphics,20131104,2013,19,12,2217,2226,"This research aims to develop design guidelines for systems that support investigators and analysts in the exploration and assembly of evidence and inferences. We focus here on the problem of identifying candidate 'influencers' within a community of practice. To better understand this problem and its related cognitive and interaction needs, we conducted a user study using a system called INVISQUE (INteractive Visual Search and QUery Environment) loaded with content from the ACM Digital Library. INVISQUE supports search and manipulation of results over a freeform infinite 'canvas'. The study focuses on the representations user create and their reasoning process. It also draws on some pre-established theories and frameworks related to sense-making and cognitive work in general, which we apply as a 'theoretical lenses' to consider findings and articulate solutions. Analysing the user-study data in the light of these provides some understanding of how the high-level problem of identifying key players within a domain can translate into lower-level questions and interactions. This, in turn, has informed our understanding of representation and functionality needs at a level of description which abstracts away from the specifics of the problem at hand to the class of problems of interest. We consider the study outcomes from the perspective of implications for design.",1077-2626;10772626,,10.1109/TVCG.2013.211,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6651935,Design methodology;Query processing;User interfaces;Visual analytics;analysis;dataframe mode;evaluation;interaction;interface design;reasoning;sense-making,Design methodology;Query processing;User interfaces;Visual analytics,cognition;data analysis;data visualisation;digital libraries;query processing,ACM digital library;INVISQUE;candidate influencers;cognitive needs;cognitive work;community of practice;design implications;freeform infinite canvas;inferences;interaction needs;interactive visual reasoning;interactive visual search and query environment;reasoning process;sense-making,"Algorithms;Cognition;Comprehension;Computer Graphics;Decision Making;Humans;Pattern Recognition, Automated;Reproducibility of Results;Sensitivity and Specificity;Software;Software Design;User-Computer Interface",3,,36,,no,Dec. 2013,,IEEE,IEEE Journals & Magazines
"Using the ADL Experience API for Mobile Learning, Sensing, Informing, Encouraging, Orchestrating",C. Glahn,"Int. Relations & Security Network, Swiss Fed. Inst. of Technol., Zurich, Switzerland","2013 Seventh International Conference on Next Generation Mobile Apps, Services and Technologies",20131111,2013,,,268,273,"The new ADL Experience API is an attempt for better interoperability between different types of educational systems and devices. This new specification is designed to link sensor networks for collecting and analyzing learning experiences in different contexts. This paper analyses how the concepts of the Experiences API were integrated in a mobile learning application and how the app uses learning analytics functions based on the collected data for informing the learners about their learning performance, for encouraging them to actively use the app, and for orchestrating and sequencing the learning resources.",2161-2889;21612889,Electronic:978-1-4799-2010-5; POD:978-1-4799-2012-9,10.1109/NGMAST.2013.55,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6658136,Interoperability;Learning analytics;Mobile learning;Software System design for Mobile services;Standardization,Context;Interoperability;Mobile communication;Mobile computing;Mobile handsets;Monitoring;Sequential analysis,application program interfaces;computer aided instruction;mobile computing,ADL experience API;educational system;learning analytics function;mobile learning;sensor network,,3,,14,,no,25-27 Sept. 2013,,IEEE,IEEE Conference Publications
UTOPIAN: User-Driven Topic Modeling Based on Interactive Nonnegative Matrix Factorization,J. Choo; C. Lee; C. K. Reddy; H. Park,Georgia Institute of Technology,IEEE Transactions on Visualization and Computer Graphics,20131016,2013,19,12,1992,2001,"Topic modeling has been widely used for analyzing text document collections. Recently, there have been significant advancements in various topic modeling techniques, particularly in the form of probabilistic graphical modeling. State-of-the-art techniques such as Latent Dirichlet Allocation (LDA) have been successfully applied in visual text analytics. However, most of the widely-used methods based on probabilistic modeling have drawbacks in terms of consistency from multiple runs and empirical convergence. Furthermore, due to the complicatedness in the formulation and the algorithm, LDA cannot easily incorporate various types of user feedback. To tackle this problem, we propose a reliable and flexible visual analytics system for topic modeling called UTOPIAN (User-driven Topic modeling based on Interactive Nonnegative Matrix Factorization). Centered around its semi-supervised formulation, UTOPIAN enables users to interact with the topic modeling method and steer the result in a user-driven manner. We demonstrate the capability of UTOPIAN via several usage scenarios with real-world document corpuses such as InfoVis/VAST paper data set and product review data sets.",1077-2626;10772626,,10.1109/TVCG.2013.212,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634167,Analytical models;Computational modeling;Context modeling;Interactive states;Latent dirichlet allocation;Visual analytics;interactive clustering;nonnegative matrix factorization;text analytics;topic modeling;visual analytics,Analytical models;Computational modeling;Context modeling;Interactive states;Visual analytics,data analysis;data visualisation;interactive systems;matrix decomposition;text analysis,UTOPIAN;flexible visual analytics system;latent Dirichlet allocation;probabilistic graphical modeling;real-world document corpuses;reliable visual analytics system;semisupervised formulation;text document collection analysis;topic modeling method;topic modeling techniques;user feedback;user-driven manner;user-driven topic modeling based on interactive nonnegative matrix factorization;visual text analytics,"Artificial Intelligence;Computer Graphics;Computer Simulation;Image Enhancement;Image Interpretation, Computer-Assisted;Information Storage and Retrieval;Models, Statistical;Natural Language Processing;Pattern Recognition, Automated;Software",24,,36,,no,Dec. 2013,,IEEE,IEEE Journals & Magazines
VAICo: Visual Analysis for Image Comparison,J. Schmidt; M. E. GrÌ_ller; S. Bruckner,Vienna University of Technology,IEEE Transactions on Visualization and Computer Graphics,20131016,2013,19,12,2090,2099,"Scientists, engineers, and analysts are confronted with ever larger and more complex sets of data, whose analysis poses special challenges. In many situations it is necessary to compare two or more datasets. Hence there is a need for comparative visualization tools to help analyze differences or similarities among datasets. In this paper an approach for comparative visualization for sets of images is presented. Well-established techniques for comparing images frequently place them side-by-side. A major drawback of such approaches is that they do not scale well. Other image comparison methods encode differences in images by abstract parameters like color. In this case information about the underlying image data gets lost. This paper introduces a new method for visualizing differences and similarities in large sets of images which preserves contextual information, but also allows the detailed analysis of subtle variations. Our approach identifies local changes and applies cluster analysis techniques to embed them in a hierarchy. The results of this process are then presented in an interactive web application which allows users to rapidly explore the space of differences and drill-down on particular features. We demonstrate the flexibility of our approach by applying it to multiple distinct domains.",1077-2626;10772626,,10.1109/TVCG.2013.213,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634107,Comparative visualization;Data visualization;Image color analysis;Image segmentation;Shape analysis;Visual analytics;focus+context visualization;image set comparison,Data visualization;Image color analysis;Image segmentation;Shape analysis;Visual analytics,Internet;data visualisation;interactive systems;pattern clustering,VAICo;cluster analysis techniques;comparative visualization tools;contextual information;image comparison;image difference visualization;image similarity visualization;interactive Web application;subtle variation analysis;visual analysis,"Algorithms;Computer Graphics;Computer Simulation;Image Interpretation, Computer-Assisted;Pattern Recognition, Automated;Reproducibility of Results;Sensitivity and Specificity;Software;Subtraction Technique;User-Computer Interface",6,,41,,no,Dec. 2013,,IEEE,IEEE Journals & Magazines
Video analytics for abandoned object detection and its evaluation on atom and ARM processor,L. G. Sole; A. S. Sonawane; S. R. Shinde; V. M. Mane,"Dept. of Electron. Eng., Vishwakarma Inst. of Technol., Pune, India",2013 IEEE International Conference on Computational Intelligence and Computing Research,20140127,2013,,,1,6,"The goal of proposed method is to enhance the safety and security by identifying the abandoned objects in the environment under consideration. This paper mainly exploits some of the properties of image processing and embedded system to implement the Video Analytics based robust and simple Security System for the surveillance of environment for twenty four hours a day. Proposed method supports a human operator by automatically detecting abandoned objects and drawing operator's attention to such events. This method is based on the various methods of the image processing and pattern recognition such as Gaussian Mixture model, Absolute background subtraction, image segmentation, connected component analysis and Histogram of oriented gradient. The algorithm evaluation is done on the hardware platform viz. Friendly Arm and INTEL's IVI board. Here we find results which explain whether the accuracy and reliability can be achieved at the cost of processing power. This is done to reduce the cost of system. Abandoned object can be decided by time basis. Proposed algorithm fits for the PETS2006 dataset and works real time. We have used OpenCV as software platform.",,Electronic:978-1-4799-1597-2; POD:978-1-4799-1596-5,10.1109/ICCIC.2013.6724150,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6724150,Abandoned object detection;OpenCV;Segmentation,Gaussian distribution;Gaussian mixture model;Lighting;Object detection;Real-time systems;Surveillance,Gaussian processes;image segmentation;object detection;video surveillance,ARM processor;Gaussian mixture model;OpenCV;abandoned object detection;absolute background subtraction;connected component analysis;histogram of oriented gradient;image processing;image segmentation;pattern recognition;simple security system;video analytics,,0,,11,,no,26-28 Dec. 2013,,IEEE,IEEE Conference Publications
Viral Marketing and Its Application in Enterprise Drive Operation,L. Wang; L. J. Zhang; N. Li; D. Liu; Y. F. Guo; J. H. Zheng; B. Hu; N. Ke,"Kingdee Res., Kingdee Software (China) Co., Ltd., Shenzhen, China",2013 IEEE Ninth World Congress on Services,20131107,2013,,,420,427,"This paper mainly develops the operation strategy of an enterprise drive based on viral marketing. Firstly, it introduces the concept, the features and successful cases of viral marketing. Next, it introduces what is Kingdee cloud drive and shows the comparative results among Kingdee cloud drive and other drives. With respect to the market positioning, this paper designs the operation strategy. Based on viral marketing, the detailed progress and reward mechanism are put forward in order to attract more users. Finally, the lottery model is developed based on the analytic network process in the reward mechanism.",2378-3818;23783818,Electronic:978-0-7695-5024-4; POD:978-1-4799-0412-9,10.1109/SERVICES.2013.56,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6655730,viral marketing; operation strategy; Kdrive; ANP,Analytic hierarchy process;Cloud computing;Electronic mail;Mobile handsets;Psychology,analytic hierarchy process;cloud computing;marketing,Kingdee cloud drive;analytic network process;enterprise drive operation strategy;lottery model;market positioning;reward mechanism;viral marketing,,0,,16,,no,June 28 2013-July 3 2013,,IEEE,IEEE Conference Publications
Vis4Heritage: Visual Analytics Approach on Grotto Wall Painting Degradations,J. Zhang; K. Kang; D. Liu; Y. Yuan; Y. E.,"School of Computer Software and Information Technology Research Center for Cultural Heritage Conservation and Promotion, Tianjin University",IEEE Transactions on Visualization and Computer Graphics,20131016,2013,19,12,1982,1991,"For preserving the grotto wall paintings and protecting these historic cultural icons from the damage and deterioration in nature environment, a visual analytics framework and a set of tools are proposed for the discovery of degradation patterns. In comparison with the traditional analysis methods that used restricted scales, our method provides users with multi-scale analytic support to study the problems on site, cave, wall and particular degradation area scales, through the application of multidimensional visualization techniques. Several case studies have been carried out using real-world wall painting data collected from a renowned World Heritage site, to verify the usability and effectiveness of the proposed method. User studies and expert reviews were also conducted through by domain experts ranging from scientists such as microenvironment researchers, archivists, geologists, chemists, to practitioners such as conservators, restorers and curators.",1077-2626;10772626,,10.1109/TVCG.2013.219,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634172,Correlation;Cultural Heritage;Cultural differences;Data visualization;Degradation;Painting;Visual Analytics;Visual analytics;Wall Paintings,Correlation;Cultural differences;Data visualization;Painting;Visual analytics,data analysis;data visualisation;history;painting;walls,Vis4Heritage;degradation area scales;degradation pattern discovery;grotto wall painting degradations;grotto wall protection;historic cultural icons;multidimensional visualization techniques;multiscale analytic;nature environment;renowned World Heritage site;visual analytics approach;visual analytics framework,"Algorithms;Computer Graphics;Image Interpretation, Computer-Assisted;Materials Testing;Paint;Paintings;Reproducibility of Results;Sensitivity and Specificity;User-Computer Interface",1,,46,,no,Dec. 2013,,IEEE,IEEE Journals & Magazines
Visual analysis of large-scale network anomalies,Q. Liao; L. Shi; C. Wang,"Department of Computer Science, Central Michigan University, Mount Pleasant, MI, USA",IBM Journal of Research and Development,20130517,2013,57,4-Mar,13:01,13:12,"The amount of information flowing across communication networks has rapidly increased. The highly dynamic and complex networks, represented as large graphs, make the analysis of such networks increasingly challenging. In this paper, we provide a brief overview of several useful visualization techniques for the analysis of spatiotemporal anomalies in large-scale networks. We make use of community-based similarity graphs (CSGs), temporal expansion model graphs (TEMGs), correlation graphs (CGs), high-dimension projection graphs (HDPGs), and topology-preserving compressed graphs (TPCGs). CSG is used to detect anomalies based on community membership changes rather than individual nodes and edges and therefore may be more tolerant to the highly dynamic nature of large networks. TEMG transforms network topologies into directed trees so that efficient search is more likely to be performed for anomalous changes in network behavior and routing topology in large dynamic networks. CG and HDPG are used to examine the complex relationship of data dimensions among graph nodes through transformation in a high-dimensional space. TPCG groups nodes with similar neighbor sets into mega-nodes, thus making graph visualization and analysis more scalable to large networks. All the methods target efficient large-graph anomaly visualization from different perspectives and together provide valuable insights.",0018-8646;00188646,,10.1147/JRD.2013.2249356,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6517342,,Data handling;Data visualization;Information management;Information processing;Network topology;Visual analytics;Visualization,,,,0,,,,no,May-July 2013,,IBM,IBM Journals & Magazines
Visual Analytics for Big Data Using R,A. Nasridinov; Y. H. Park,"Dept. of Multimedia Sci., Sookmyung Women's Univ., Seoul, South Korea",2013 International Conference on Cloud and Green Computing,20131219,2013,,,564,565,"The growth in volumes of data has affected today's large organization, where commonly used software tools to capture, manage, and process the data cannot handle big data effectively. The main challenge is that organizations must analyze a large amount of big data and extract useful information or knowledge for future actions in a short time. This type of demands has produced the markets for various innovative big data control mechanisms, such as visual analytics for big data. In this paper, we propose to visually analyze the big data using R statistical software. The proposed method is composed of three steps. In the first step, we extract the data set from the target Web site. In the second step, we parse the extracted raw data according to the types, and store in a database. In the third, we perform visual analysis from the stored data in database using R statistical software.",,Electronic:978-0-7695-5114-2; POD:978-1-4799-1362-6,10.1109/CGC.2013.96,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6686088,R statistical software;big data;visual analytics,Data handling;Data mining;Data storage systems;Information management;Organizations;Social network services;Software,Big Data;data visualisation;information retrieval;program compilers;statistical analysis,Big Data analysis;Big Data control mechanisms;R statistical software;Web site;data capturing;data management;data processing;data set extraction;extracted raw data parsing;information extraction;knowledge extraction;software tools;visual analytics,,0,,4,,no,Sept. 30 2013-Oct. 2 2013,,IEEE,IEEE Conference Publications
Visual Analytics for Model Selection in Time Series Analysis,M. BÌ_gl; W. Aigner; P. Filzmoser; T. Lammarsch; S. Miksch; A. Rind,Vienna University of Technology,IEEE Transactions on Visualization and Computer Graphics,20131016,2013,19,12,2237,2246,"Model selection in time series analysis is a challenging task for domain experts in many application areas such as epidemiology, economy, or environmental sciences. The methodology used for this task demands a close combination of human judgement and automated computation. However, statistical software tools do not adequately support this combination through interactive visual interfaces. We propose a Visual Analytics process to guide domain experts in this task. For this purpose, we developed the TiMoVA prototype that implements this process based on user stories and iterative expert feedback on user experience. The prototype was evaluated by usage scenarios with an example dataset from epidemiology and interviews with two external domain experts in statistics. The insights from the experts' feedback and the usage scenarios show that TiMoVA is able to support domain experts in model selection tasks through interactive visual interfaces with short feedback cycles.",1077-2626;10772626,,10.1109/TVCG.2013.222,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634112,Analytical models;Autoregressive processes;Data models;Mathematical model;Time series analysis;Visual analytics;coordinated &amp;amp; multiple views;model selection;time series analysis;visual interaction,Analytical models;Autoregressive processes;Data models;Mathematical model;Time series analysis,data analysis;data visualisation;iterative methods;mathematics computing;statistical analysis;time series,TiMoVA prototype;automated computation;domain experts;epidemiology;human judgement;interactive visual interfaces;iterative expert feedback;model selection tasks;statistical software tools;time series analysis;user experience;user stories;visual analytics process,"Algorithms;Computer Graphics;Computer Simulation;Data Interpretation, Statistical;Decision Support Techniques;Models, Statistical;Pattern Recognition, Automated;Reproducibility of Results;Sensitivity and Specificity;User-Computer Interface",8,,42,,no,Dec. 2013,,IEEE,IEEE Journals & Magazines
Visual analytics for software requirements engineering,S. Reddivari,"Department of Computer Science and Engineering Mississippi State University Mississippi State, USA",2013 21st IEEE International Requirements Engineering Conference (RE),20131021,2013,,,389,392,"The research on visual analytics for requirements engineering has noticeably advanced in the past few years. For many software projects, requirements management needs an effective and efficient path from data to decision. Visual analytics (VA) creates such a path that enables the user to extract insights by interacting with the relevant information. While various requirements visualization techniques exist, only few have produced end-to-end values to practitioners. In this research proposal, we advance the literature on visual requirements analytics by characterizing its key components and relationships. Such a characterization allows us to not only assess existing approaches, but also develop tool enhancements in a principled manner. We describe our ongoing work on VA and outline future research plans.",1090-705X;1090705X,Electronic:978-1-4673-5765-4; POD:978-1-4673-5763-0,10.1109/RE.2013.6636762,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6636762,Requirements management;decision making;requirements engineering visualization;visual analytics,Cognition;Data visualization;Decision making;Labeling;Software;Visual analytics,data analysis;data visualisation;formal specification;project management;software management,VA;requirements management;requirements visualization techniques;software projects;software requirements engineering;tool enhancements;visual requirements analytics,,1,,23,,no,15-19 July 2013,,IEEE,IEEE Conference Publications
Visual Analytics Infrastructures: From Data Management to Exploration,J. D. Fekete,INRIA,Computer,20130808,2013,46,7,22,29,"Analysts exploring big data require more from information visualization, data analysis, and data management than these components can now deliver. New infrastructures must address the nature of exploration as well as data scale. The Web extra at http://youtu.be/K9PvskathGI is a video segment that gives an overview of how research in visual analytics can help tackle the challenges of managing and interpreting big data in various domains.",0018-9162;00189162,,10.1109/MC.2013.120,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6488679,hardware infrastructures;software infrastructures;visual analytics;visualization,Data handling;Data visualization;Database systems;Hardware;Software architecture;Visual analytics,data analysis;data visualisation,data analysis;data exploration;data management;data scale;information visualization;visual analytics infrastructures,,2,,15,,no,13-Jul,,IEEE,IEEE Journals & Magazines
Visual Analytics Model for Intrusion Detection in Flood Attack,J. Zhang; M. L. Huang,"Sch. of Software, Univ. of Technol., Sydney, Sydney, NSW, Australia","2013 12th IEEE International Conference on Trust, Security and Privacy in Computing and Communications",20131212,2013,,,277,284,"Flood attacks are common forms of Distributed Denial-of-Service (DDoS) attack threats on internet in nature. This has necessitated the need for visual analysis within an intrusion detection system to identify these attacks. The challenges are how to increase the accuracy of detection and how to visualize and present flood attacks in networks for early detection. In this paper, we introduce three coefficients, which not only classify the behaviors of flood attacks, but also measure the system performance under those flood attacks: a) attack-density that patterns the characters of flood attack, b) system workload which represents the system capability in handling flood attack and c) the scalability to classify the impact level of the flood attack at victim site. A visual clustered method is used to display the DDoS flood attacks. The experimentation results are presented to demonstrate our new model significantly improves the accuracy of the detection of DDoS attacks and provides a better understanding of the nature of flood attacks on networks.",2324-898X;2324898X,Electronic:978-0-7695-5022-0; POD:978-1-4799-1444-9,10.1109/TrustCom.2013.38,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6680852,DDoS flood attack;Network security;attack density;attack scale;information visualization;workload,Analytical models;Computer crime;Floods;IP networks;Ports (Computers);Telecommunication traffic;Visualization,Internet;computer network security;data analysis;data visualisation;pattern classification;pattern clustering,DDoS attack;Internet;attack-density;distributed denial-of-service attack;flood attack behavior classification;intrusion detection;system workload;visual analytics model;visual clustered method,,3,,36,,no,16-18 July 2013,,IEEE,IEEE Conference Publications
Visualizing geolocation of spam email,A. Muallem; S. Shetty; S. K. Hargrove,"College of Engineering, Tennessee State University, Nashville, TN USA","2013 Computing, Communications and IT Applications Conference (ComComAp)",20130617,2013,,,63,68,"With the recent surge in cyber attacks, there is a growing demand for effective security analytics tools. Though, there are advanced data collection techniques in the form of honeypots and malware collectors, the value of data are only as useful as the analysis technique used. One of the primary drawbacks of current security analytic tools is the lack of visualization controls to effectively analyze the data. In this paper, we develop a visualization tool to analyze the geographical locations of spammers based on the integration of MaxMind and WhoIS databases with Google Maps API. The visualization tool provides an insight into spam origins, along with patterns of spammers identified from spam activity. A key component in the development of this tool is its extensible framework allowing for the addition of resources to retrieve more information about a spammer and analyze additional patterns of spammers for spam analysis.",,Electronic:978-1-4673-6044-9; POD:978-1-4673-6043-2,10.1109/ComComAp.2013.6533610,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6533610,IP Geolocation;Network Visualization;Security Visualization;Spam Analysis,Data visualization;Google;IP networks;Unsolicited electronic mail;Visual databases,data analysis;data visualisation;geographic information systems;information retrieval;invasive software;unsolicited e-mail,Google Maps API;MaxMind databases;WhoIS databases;current security analytic tools;cyber attacks;honeypots collectors;information retrieval;malware collectors;spam email geolocation visualization;spammer geographical locations,,1,,9,,no,1-4 April 2013,,IEEE,IEEE Conference Publications
Web analytics in e-learning: Agent-based and neural network approaches,V. Artemenko,"Lviv Acad. of Commerce, Lviv, Ukraine",2013 IEEE 7th International Conference on Intelligent Data Acquisition and Advanced Computing Systems (IDAACS),20131114,2013,2,,774,780,"Current research explores web analytics tools in the field of electronic learning using agent-based and neural network approaches. A hybrid agent-based model with built-in artificial neural networks is proposed. The model aims to support the computer simulation experiment assessment of knowledge production and knowledge dissemination trends among the agents of three types: authors, tutors and students of on-line courses. In the research we study the effectiveness of using the software to implement this model as an example in one of the higher education institutions.",,Electronic:978-1-4799-1429-6; POD:978-1-4799-1425-8,10.1109/IDAACS.2013.6663030,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6663030,artificial neural network;e-learning;hybrid agent-based model;web analytics,Artificial neural networks;Computational modeling;Electronic learning;Internet;Production,Internet;computer aided instruction;educational courses;educational institutions;further education;multi-agent systems;neural nets,Web analytics;agent-based approach;artificial neural networks;computer simulation experiment assessment;e-learning;electronic learning;higher education institutions;knowledge dissemination;knowledge production;neural network approach;online courses,,0,,19,,no,12-14 Sept. 2013,,IEEE,IEEE Conference Publications
Where is the business logic?,,,,,2013,,,,,"One of the challenges in maintaining legacy systems is to be able to locate business logic in the code, and isolate it for different purposes, including implementing requested changes, refactoring, eliminating duplication, unit testing, and extracting business logic into a rule engine. Our new idea is an iterative method to identify the business logic in the code and visualize this information to gain better understanding of the logic distribution in the code, as well as developing a domain-specific business vocabulary. This new method combines and extends several existing technologies, including search, aggregation, and visualization. We evaluated the visualization method on a large-scale application and found that it yields useful results, provided an appropriate vocabulary is available.",,,,http://dl.acm.org/citation.cfm?id=2494588&CFID=696538919&CFTOKEN=83912867,,,,,,,,,,yes,,,,
3D software tool for reliability assessment based on three dimensional wiener process model considering big data on cloud computing,Y. Tamura; S. Yamada,"Yamaguchi University, Tokiwadai 2-16-1, Ube-shi, 755-8611, Japan","Proceedings of 3rd International Conference on Reliability, Infocom Technologies and Optimization",20150122,2014,,,1,6,"At present, many software services by using cloud computing are provided because of the unification management of data, low cost. We focus on big data on cloud computing by using open source software such as OpenStack and Eucalyptus. In order to consider the interesting aspect of the big data on cloud computing, we propose a new approach to software reliability assessment based on an AHP and three dimensional stochastic differential equation models. Moreover, we develop the 3D software tool based on our model.",,Electronic:978-1-4799-6896-1; POD:978-1-4799-6897-8,10.1109/ICRITO.2014.7014658,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7014658,Cloud computing;big data;decision-making;software reliability modelling,,Big Data;analytic hierarchy process;cloud computing;differential equations;software reliability;stochastic processes,3D software tool;AHP;Eucalyptus;OpenStack;big data;cloud computing;differential equation;open source software;software reliability assessment;software services;three dimensional Wiener process model;three dimensional stochastic equation;unification management,,0,,19,,no,8-10 Oct. 2014,,IEEE,IEEE Conference Publications
A Big Data Financial Information Management Architecture for Global Banking,A. Munar; E. Chiner; I. Sales,"GFT Group, Valencia, Spain",2014 International Conference on Future Internet of Things and Cloud,20141215,2014,,,385,388,"Global investment banks and financial institutions are facing growing data processing demands. These originate not only from increasing regulatory requirements and an expanding variety and disparity of data sources, but also from ongoing pressures in cost reduction without compromising system scalability and flexibility. In this context, the ability to apply promising state-of-the-art big data technologies to extract the maximum value from the vast amounts of the data generated is generating a lot of interest in the financial services industry. In this paper we present a Big Data architecture system design, based in open distributed computing paradigms like Hadoop map-reduce, offering horizontal scalability and no-SQL flexibility while at the same time meeting the stringent quality and resilience requirements of the banking software standards. The proposed architecture is able to consolidate, validate, enrich and process with different Big Data analytics techniques the data gathered from the different source systems as encountered in the banking practice, while at the same time supporting the different data integration, transmission and process orchestration requirements traditionally encountered in a global financial institution.",,Electronic:978-1-4799-4357-9; POD:978-1-4799-4356-2,10.1109/FiCloud.2014.68,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6984224,big data analytics;big data architectures;financial information;hadoop;map-reduce,Banking;Big data;Data models;Distributed databases;Reliability,Big Data;banking;information management;open systems;software standards,Big Data;Hadoop map-reduce;banking software standards;data processing demands;financial information management architecture;financial institutions;financial services industry;global financial institution;global investment banks;no-SQL flexibility;open distributed computing paradigms,,1,,8,,no,27-29 Aug. 2014,,IEEE,IEEE Conference Publications
A Community Information System for Ubiquitous Informal Learning Support,P. Nicolaescu; D. Renzel; I. Koren; R. Klamma; J. Purma; M. Bauters,"Adv. Community Inf. Syst. (ACIS), RWTH Aachen Univ., Aachen, Germany",2014 IEEE 14th International Conference on Advanced Learning Technologies,20140922,2014,,,138,140,"There are many mobile apps supporting informal learning tasks like sense-making out of multimedia materials. However, regardless how well these have been implemented, their scope and scalability is limited either by over-specialization and consequently limited transferability or by over-generalization - with a lack of informal learning support for most Web 2.0 apps. Our cloud-based mobile Web information system approach is theoretically founded and designed for scalability in informal learning. As a proof of concept we present a semantic video annotation scenario for supporting informal learning at workplace. As the approach has been designed for community learning analytics, we can present results from a preliminary evaluation. We conducted our research with a strong commitment towards open source software development, so that our solutions are already available and can have impact beyond the usual scope of a funded project.",2161-3761;21613761,Electronic:978-1-4799-4038-7; POD:978-1-4799-4037-0,10.1109/ICALT.2014.48,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6901419,informal learning;semantic video annotation;ubiquitous multimedia information systems;workplace learning,Communities;Employment;Information systems;Mobile communication;Multimedia communication;Semantics;Streaming media,cloud computing;computer aided instruction;information systems;mobile computing;multimedia systems;public domain software,Web 2.0;cloud-based mobile Web information system;community information system;informal learning tasks;mobile apps;multimedia materials;open source software;ubiquitous informal learning support,,0,,12,,no,7-10 July 2014,,IEEE,IEEE Conference Publications
A comparison of alerting strategies for hemorrhage identification during prehospital emergency transport,J. Liu; A. T. Reisner; S. Edla; J. Reifman,"Department of Defense Biotechnology High Performance Computing Software Applications Institute (BHSAI), Telemedicine and Advanced Technology Research Center (TATRC), U.S. Army Medical Research and Materiel Command (USAMRMC), Fort Detrick, MD 21702 USA",2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society,20141106,2014,,,2670,2673,"Early and accurate identification of physiological abnormalities is one feature of intelligent decision support. The ideal analytic strategy for identifying pathological states would be highly sensitive and highly specific, with minimal latency. In the field of manufacturing, there are well-established analytic strategies for statistical process control, whereby aberrancies in a manufacturing process are detected by monitoring and analyzing the process output. These include simple thresholding, the sequential probability ratio test (SPRT), risk-adjusted SPRT, and the cumulative sum method. In this report, we applied these strategies to continuously monitored prehospital vital-sign data from trauma patients during their helicopter transport to level I trauma centers, seeking to determine whether one strategy would be superior. We found that different configurations of each alerting strategy yielded widely different performances in terms of sensitivity, specificity, and average time to alert. Yet, comparing the different investigational analytic strategies, we observed substantial overlap among their different configurations, without any one analytic strategy yielding distinctly superior performance. In conclusion, performance did not depend as much on the specific analytic strategy as much as the configuration of each strategy. This implies that any analytic strategy must be carefully configured to yield the optimal performance (i.e., the optimal balance between sensitivity, specificity, and latency) for a specific use case. Conversely, this also implies that an alerting strategy optimized for one use case (e.g., long prehospital transport times) may not necessarily yield performance data that are optimized for another clinical application (e.g., short prehospital transport times, intensive care units, etc.).",1094-687X;1094687X,Electronic:978-1-4244-7929-0; POD:978-1-4244-7927-6,10.1109/EMBC.2014.6944172,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6944172,,Accuracy;Biomedical monitoring;Hemorrhaging;Medical diagnostic imaging;Monitoring;Process control;Sensitivity,decision support systems;emergency management;injuries;medical computing;medical disorders;statistical process control,alerting strategies;continuously monitored prehospital vital-sign data;cumulative sum method;helicopter transport;hemorrhage identification;intelligent decision support;intensive care units;manufacturing process;minimal latency;pathological states;physiological abnormality;prehospital emergency transport;process output;risk-adjusted SPRT;sequential probability ratio test;short prehospital transport times;statistical process control;trauma centers;trauma patients,,0,,11,,no,26-30 Aug. 2014,,IEEE,IEEE Conference Publications
A Compliance Aware Software Defined Infrastructure,M. A. McCarthy; L. M. Herger; S. M. Khan,"Office of the CIO, IBM Corp., Raleigh, NC, USA",2014 IEEE International Conference on Services Computing,20141020,2014,,,560,567,"With cloud eclipsing the $100B mark, it is clear that the main driver is no longer strictly cost savings. The focus now is to exploit the cloud for innovation, utilizing the agility to expand resources to quickly build out new designs, products, simulations and analysis. As the cloud lowers the unit cost of IT and improves agility, the time to market for applications will improve significantly. Companies will use this agility and speed as competitive advantage. An example of the agility is the adoption by enterprises of the software-defined datacenter (SDDC)[3] model, which allows for the rapid build of environments with composable infrastructures. With adoption of the SDDC model, intelligent and automated management of the SDDC is an immediate priority, required to support the changing workloads and dynamic patterns of the enterprise. Often, security and compliance become an 'after thought', bolted on later when problems arise. In this paper, we will discuss our experience in developing and deploying a centralized management system for public, as well as an Openstack [4] based cloud platform in SoftLayer, with an innovative, analytics-driven 'security compliance as a service' that constantly adjusts to varying compliance requirements based on workload, security and compliance requirements. In this paper we will also focus on techniques we have developed for capturing and replaying the previous state of a failing client virtual machine (VM) image, roll back, and then re-execute to analyze failures related to security or compliance. This technique contributes to agility, since failing VM's with security issues can quickly be analyzed and brought back online, this is often not the case with security problems, where analysis and forensics can take several days/weeks.",,Electronic:978-1-4799-5066-9; POD:978-1-4799-5067-6,10.1109/SCC.2014.79,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6930580,Compliance;Compliance Architecture;Compliance Remediation;Compliance as a Service,Companies;Forensics;Monitoring;Process control;Security;Software,cloud computing;configuration management;security of data,Openstack;SDDC model;SoftLayer;centralized management system;cloud platform;compliance aware software defined infrastructure;security compliance;software-defined datacenter;virtual machine,,1,,20,,no,June 27 2014-July 2 2014,,IEEE,IEEE Conference Publications
A Comprehensive Service-Oriented Innovation Support System for E-Commerce Innovation Process,J. Shen; Y. Li; M. Zhang; W. Zhou,"Sch. of Comput. Sci., Fudan Univ., Shanghai, China",2014 IEEE 11th International Conference on e-Business Engineering,20141211,2014,,,274,279,"E-commerce innovations have to break through a four-phase process before adopted by industry. To improve the comprehensiveness and extensibility for innovation supporting, this paper proposes a service-oriented innovation support system which covers all phases of the innovation process, customizes for most kinds of E-commerce innovations and provides an AHP (Analytic Hierarchy Process) and entropy method combined approach for innovation evaluation. Such service-oriented system enables multi-granularity service deployment, composition and orchestration, and provides uniform interface for service providing, invoking and managing. Compared to other innovation support system, the proposed system enhances the capacity for the support of entire innovation process and extensibility for collaborative innovation. The implemented system is adopted by National Engineering Laboratory for E-commerce Technologies of China to serve as the official public service platform.",,Electronic:978-1-4799-6563-2; POD:978-1-4799-6564-9,10.1109/ICEBE.2014.54,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6982092,E-commerce innovation process;Innovation evaluation;Innovation support system;Service-oriented,Business;Entropy;Industries;Patents;Portals;Service-oriented architecture;Technological innovation,analytic hierarchy process;electronic commerce;entropy;service-oriented architecture,AHP;China;E-commerce Technologies;E-commerce innovation process;National Engineering Laboratory;analytic hierarchy process;collaborative innovation;entropy method;multigranularity service deployment;public service platform;service-oriented innovation support system,,0,,18,,no,5-7 Nov. 2014,,IEEE,IEEE Conference Publications
A data access framework for integration to facilitate efficient building operation,J. McCarthy; D. O'Sullivan,"School of Engineering, University College Cork, Cork, Ireland",Proceedings of the 2014 IEEE Emerging Technology and Factory Automation (ETFA),20150112,2014,,,1,4,"Building Automation and Control Systems (BACs) are used to manage the day-to-day functions, operation and maintenance of a huge diversity of equipment within facilities of varying size and function. These systems are developed by a large number of hardware and software manufacturers who produce proprietary products designed to solve specific problems. As a result a number of different BACS can be operating within a single facility controlling various devices and producing significant quantities of data. This data can prove difficult to access due to the proprietary nature of the individual applications, thereby limiting the potential for a holistic view of a facilities operation and limiting the scope for big data analytics. This paper proposes the implementation of a Data Access Framework to coalesce the disparate information sources that can exist within a facility via OPC into a tagged database with particular focus on Heating Ventilation and Air Conditioning (HVAC) as a test case.",1946-0740;19460740,Electronic:978-1-4799-4845-1; POD:978-1-4799-4844-4,10.1109/ETFA.2014.7005265,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7005265,Data Access;Efficiency;OPC;Tagging,Buildings;Computer architecture;Control systems;Databases;Servers;Tagging;Temperature measurement,Big Data;HVAC;building management systems;control engineering computing;data analysis,BACS;HVAC;OPC;big data analytics;building automation and control systems;data access framework;day-to-day functions;efficient building operation;facilities operation;hardware manufacturers;heating ventilation and air conditioning;software manufacturers;tagged database,,0,,12,,no,16-19 Sept. 2014,,IEEE,IEEE Conference Publications
"A Domain-Driven, Generative Data Model for Big Pet Store",R. J. Nowling; J. Vyas,"Red Hat Inc., Raleigh, NC, USA",2014 IEEE Fourth International Conference on Big Data and Cloud Computing,20150209,2014,,,49,55,"Generating large amounts of semantically-rich data for testing big data workflows is paramount for scalable performance benchmarking and quality assurance in modern machine-learning and analytics workloads. The most obvious use case for such a generative algorithm is in conjunction with a big data application blueprint, which can be used by developers (to test their emerging big data solutions) as well as end users (as a starting point for validating infrastructure installations, building novel applications, and learning analytics methods). We present a new domain-driven, generative data model for Big Pet Store, a big data application blueprint for the Hadoop ecosystem included in the Apache Big Top distribution. We describe the model and demonstrate its ability to generate semantically-rich data at variable scale ranging from a single machine to a large cluster. We validate the model by using the generated data to answer questions about customer locations and purchasing habits for a fictional targeted advertising campaign, a common business use case.",,Electronic:978-1-4799-6719-3; POD:978-1-4799-6720-9,10.1109/BDCloud.2014.38,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7034765,benchmarking;big data;data generation;probabilistic models;synthetic data sets;testing,Benchmark testing;Big data;Data models;Generators;Hidden Markov models;Probability density function,Big Data;data models;learning (artificial intelligence);program testing;public domain software;quality assurance;software quality;workflow management software,Apache big top distribution;Big Data application blueprint;Big data workflow testing;BigPetStore;Hadoop ecosystem;analytics workloads;domain-driven generative data model;generative algorithm;machine-learning;quality assurance;scalable performance benchmarking;semantically-rich data,,0,,26,,no,3-5 Dec. 2014,,IEEE,IEEE Conference Publications
"A Dynamic Software Product Line Architecture for Prepackaged Expert Analytics: Enabling Efficient Capture, Reuse and Adaptation of Operational Knowledge",K. Smiley; S. Mahate; P. Wood,"ABB Corp. Res., Ind. Software Syst., Raleigh, NC, USA",2014 IEEE/IFIP Conference on Software Architecture,20140609,2014,,,205,214,"Advanced asset health management solutions blend business intelligence with analytics that incorporate expert operational knowledge of industrial equipment and systems. Key challenges in developing these solutions include: streamlining the capture and prepackaging of operational experts' knowledge as analytic modules, efficiently evolving the modules as knowledge grows, adapting the analytics in the field for diverse operating circumstances and industries, and executing the analytics with high performance in industrial and enterprise software systems. A Quality Attribute Workshop (QAW) was used to elicit and analyze variability at development time and runtime for creating, integrating, evolving, and tailoring reusable analytic modules for ABB/Ventyx asset health solution offerings. Dynamic software product line (DSPL) architecture approaches were then applied in designing an analytics plug in architecture for asset health solutions. This paper describes our approach and experiences in designing the analytics product line architecture and its SME Workbench toolset, and how we achieved significant improvements in speed and flexibility of deploying industrial analytics.",,Electronic:978-1-4799-3412-6; POD:978-1-4799-3413-3,10.1109/WICSA.2014.11,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6827120,asset health;dynamic software product line;extensibility;industrial analytics;industrial software systems;interoperability;knowledge;performance;reusability,Analytical models;Business;Computer architecture;Data models;Industries;Runtime;Software,knowledge management;manufacturing data processing;small-to-medium enterprises;software architecture;software product lines,ABB/Ventyx asset health solution offerings;DSPL architecture;QAW;SME Workbench toolset;asset health management solutions;business analytics;business intelligence;dynamic software product line architecture;industrial analytics;industrial equipment;industrial systems;operational knowledge adaptation;operational knowledge capture;operational knowledge reuse;prepackaged expert analytics;quality attribute workshop;reusable analytic modules;small-and-medium sized enterprise,,1,,20,,no,7-11 April 2014,,IEEE,IEEE Conference Publications
A Flexible Framework for Asynchronous in Situ and in Transit Analytics for Scientific Simulations,M. Dreher; B. Raffin,"LIG, INRIA, Montbonnot, France","2014 14th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing",20140708,2014,,,277,286,"High performance computing systems are today composed of tens of thousands of processors and deep memory hierarchies. The next generation of machines will further increase the unbalance between I/O capabilities and processing power. To reduce the pressure on I/Os, the in situ analytics paradigm proposes to process the data as closely as possible to where and when the data are produced. Processing can be embedded in the simulation code, executed asynchronously on helper cores on the same nodes, or performed in transit on staging nodes dedicated to analytics. Today, software environnements as well as usage scenarios still need to be investigated before in situ analytics become a standard practice. In this paper we introduce a framework for designing, deploying and executing in situ scenarios. Based on a component model, the scientist designs analytics workflows by first developing processing components that are next assembled in a dataflow graph through a Python script. At runtime the graph is instantiated according to the execution context, the framework taking care of deploying the application on the target architecture and coordinating the analytics workflows with the simulation execution. Component coordination, zero-copy intra-node communications or inter-nodes data transfers rely on per-node distributed daemons. We evaluate various scenarios performing in situ and in transit analytics on large molecular dynamics systems simulated with Gromacs using up to 2048 cores. We show in particular that analytics processing can be performed on the fraction of resources the simulation does not use well, resulting in a limited impact on the simulation performance (less than 9%). Our more advanced scenario combines in situ and in transit processing to compute a molecular surface based on the Quick surf algorithm.",,Electronic:978-1-4799-2784-5; POD:978-1-4799-2785-2,10.1109/CCGrid.2014.92,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6846463,IO;Molecular Dynamics;n Situ Analytics and Visualization,Analytical models;Computational modeling;Data models;Data visualization;Numerical models;Ports (Computers);Standards,data analysis;electronic data interchange;input-output programs;molecular dynamics method;natural sciences computing;parallel processing;workflow management software,Gromacs;I/O capability;Python script;Quick surf algorithm;analytics workflows;asynchronous in situ analytics;asynchronous in transit analytics;component coordination;dataflow graph;deep memory hierarchy;high performance computing system;internodes data transfers;molecular dynamics system;per-node distributed daemon;processing components;scientific simulation;simulation code;simulation execution;simulation performance;software environments;staging nodes;target architecture;zero-copy intranode communication,,1,,37,,no,26-29 May 2014,,IEEE,IEEE Conference Publications
A framework for power saving in IoT networks,M. Taneja,"Cisco Systems, Bangalore, India","2014 International Conference on Advances in Computing, Communications and Informatics (ICACCI)",20141201,2014,,,369,375,"An IoT / M2M system may support large number of battery operated devices in addition to some mains operated devices. It is important to conserve energy of these battery operated constrained devices. An IoT / M2M Gateway used in this system is an intermediate node between IoT / M2M devices and an IoT / M2M Service Platform. It enables distributed analytics and helps to reduce traffic load in the network. This gateway could be stationary or mobile. In an IoT / M2M system, it becomes important to conserve energy of this Gateway as well. This paper proposes a framework to reduce power consumption of M2M / IoT devices as well as Gateway nodes. We buffer data at IoT Application, IoT Gateways and Devices to keep devices and Gateway nodes in sleep mode as long as possible. We allow computation of the duration to buffer this data using factors such as QoS requirements, predicted pattern of future IoT / M2M messages and congestion indicators from different network nodes. This potentially also allows intelligent aggregation of IoT messages at the Gateway node. We also enhance signaling mechanisms and present software building blocks for this framework. Mesh as well as Cellular access technologies are considered here.",,Electronic:978-1-4799-3080-7; POD:978-1-4799-3081-4; USB:978-1-4799-3079-1,10.1109/ICACCI.2014.6968211,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6968211,IEEE802.15.4;Internet of Things (IoT);Machine to Machine (M2M) Communication;Power Saving,Batteries;Delays;Logic gates;Protocols;Schedules;Security;Temperature sensors,Internet of Things;cellular radio;internetworking;mobile computing;telecommunication power management;telecommunication traffic;wireless mesh networks,Internet-of-things;IoT networks;IoT service platform;M2M gateway;M2M messages;M2M service platform;QoS requirements;battery operated constrained devices;cellular access technologies;congestion indicators;distributed analytics;energy conservation;gateway nodes;intelligent IoT message aggregation;intermediate node;machine-to-machine communication;network nodes;power saving;software building blocks;traffic load reduction,,2,,10,,no,24-27 Sept. 2014,,IEEE,IEEE Conference Publications
A Framework for Supporting Creative Thinking in Concept Mapping,G. Thanasis; E. Kehris; H. Samara; S. Mpakavos,"Dept. of Bus. Adm., Technol. & Educ. Inst., Serres, Greece",2014 IEEE 14th International Conference on Advanced Learning Technologies,20140922,2014,,,493,494,"In this paper, we present a framework associated with the development of analytic and synthetic thinking that is also related to improving learner creativity. In this framework, the whole concept map is treated as a structure, the characteristics of which the learner has to learn to improve, similarly every proposition is treated as an entity, the characteristics of which express the creativity of the learner.",2161-3761;21613761,Electronic:978-1-4799-4038-7; POD:978-1-4799-4037-0,10.1109/ICALT.2014.145,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6901520,concept maps;creativity;framework,Business;Conferences;Educational institutions;Joining processes;Ontologies;Software;Standards,educational computing,concept mapping;creative thinking;learner creativity;synthetic thinking,,0,,2,,no,7-10 July 2014,,IEEE,IEEE Conference Publications
A goal-based technique for requirements prioritization,M. A. A. Elsood; H. A. Hefny; E. S. Nasr,"Computer Science and Information Systems Department, Institute of Statistical Studies and Research, Cairo, Egypt",2014 9th International Conference on Informatics and Systems,20150212,2014,,,SW-18,SW-24,"It is a well-known fact that the number of software requirements from customers usually exceeds the number of features that can be implemented within a given time and available resources. Therefore, requirements prioritization (RP) is essential during the requirements elicitation activity of the requirements engineering phase of software development. In addition, not all requirements can be implemented within one release, so RP is also essential for release planning. There are many RP techniques available in the literature, which are mostly attempting to solve a multi-criteria decision making problem. However, most of them work well on a small number of requirements, and many still suffer from different shortcomings such as scalability, uncertainty, a lot of stakeholders' time consumption, and complexity. According to many studies, none of the RP techniques can be considered the best; the best RP technique depends on the situation. In addition, most of the RP techniques don't take into account the effects of the required goals on the final alternatives' ranking. In this paper, we present a brief survey of the most popular decision making RP techniques, before presenting our new goal-based RP technique. Our goal-based RP technique is based on generating a relative weight for the requirements with respect to the identified goals by stakeholders. It is inspired from prioritization decision making techniques in an attempt to enhance reported RP problems of time consumption, scalability and complexity.",,CD-ROM:978-977-403-689-7; Electronic:978-977-403-695-8; POD:978-1-4799-4018-9,10.1109/INFOS.2014.7036697,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7036697,goal-based technique;requirements engineering;requirements prioritization,Analytic hierarchy process;Computers;Informatics;Scalability;Software;Software engineering,decision making;formal specification;formal verification;mathematical programming;systems analysis,RP;goal-based technique;multicriteria decision making;requirements engineering;requirements prioritization;software development;software requirement,,0,,26,,no,15-17 Dec. 2014,,IEEE,IEEE Conference Publications
A Hybrid Fuzzy Framework for Cloud Service Selection,L. Sun; H. Dong; F. K. Hussain; O. K. Hussain; J. Ma; Y. Zhang,"Centre for Appl. Inf., Victoria Univ., Melbourne, VIC, Australia",2014 IEEE International Conference on Web Services,20141204,2014,,,313,320,"QoS-based service rating has made positive contributions to the area of service selection. Especially for Cloud service users, the right decision when choosing suitable Cloud services can help them improve user satisfaction and trading revenues. This work aims to address the issue of uncertainty in service requests, service descriptions, user and expert preferences, as well as evaluation criteria in a MCDM-based service selection procedure. A hybrid fuzzy framework for Cloud service selection is proposed, addressing the challenge using three approaches: a fuzzy-ontology-based approach for function matching and service filtering, a fuzzy AHP technique for informed criterion weighting, and, a fuzzy TOPSIS approach for service ranking.",,Electronic:978-1-4799-5054-6; POD:978-1-4799-5055-3,10.1109/ICWS.2014.53,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6928913,Cloud service selection;Multi-criteria decision making;fuzzy AHP;fuzzy TOPSIS;fuzzy ontology,Cloud computing;Computational modeling;Finite impulse response filters;Ontologies;Quality of service;Time complexity;Vectors,analytic hierarchy process;cloud computing;fuzzy set theory;ontologies (artificial intelligence);user interfaces,MCDM-based service selection procedure;QoS-based service rating;cloud service selection;evaluation criteria;expert preferences;function matching;fuzzy AHP technique;fuzzy TOPSIS approach;fuzzy-ontology-based approach;hybrid fuzzy framework;informed criterion weighting;multicriteria decision making;service descriptions;service filtering;service ranking;service requests;trading revenue improvement;user preferences;user satisfaction improvement,,0,,32,,no,June 27 2014-July 2 2014,,IEEE,IEEE Conference Publications
"A Laboratory-Targeted, Data Management and Processing System for the Early Detection Research Network",R. Verma; A. F. Hart; C. A. Mattmann; D. J. Crichton; H. Kincaid; S. C. Kelly; M. J. Joyce; P. Zimdars; D. L. Tabb; J. D. Holman; M. Chambers; K. Anton; M. Colbert; C. Patriotis; S. Srivastava,"Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA",2014 IEEE 27th International Symposium on Computer-Based Medical Systems,20140825,2014,,,401,405,"The National Institutes of Health (NIH), National Cancer Institute's Early Detection Research Network (EDRN) is a cross-institutional collaborative initiative seeking to accelerate the clinical application of cancer biomarker research. Over the past decade, it has been our role, as EDRN's Informatics Center (IC), to develop a comprehensive information services infrastructure as well as a set of software tools and services to support this overall initiative. We have recently developed a novel application called the Laboratory Catalog and Archive Service (LabCAS) whose focus is to extend EDRN IC data management and processing capabilities to EDRN laboratories. By leveraging the same technologies used to manage and process NASA Earth and Planetary data sets, we offer EDRN researchers an effective way of managing their laboratory data. More specifically, LabCAS enables EDRN researchers to reliably archive their experimental data, to optionally share these data in a controlled manner with other researchers, and to gain insight into these data through highly configurable data analysis pipelines tailored to the broad range of biomarker related experiments. This particular collaboration leverages expertise from NASA's Jet Propulsion Laboratory, Vanderbilt University Medical Center, and Dartmouth Medical School, as well as builds upon existing cross-governmental collaboration between NASA and the NIH.",1063-7125;10637125,Electronic:978-1-4799-4435-4; POD:978-1-4799-4434-7,10.1109/CBMS.2014.139,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6881914,analytics;biomarkers;cancer;government;informatics,Cancer;Informatics;Integrated circuits;Laboratories;Pipeline processing;Pipelines;Software,cancer;data analysis;information services;medical information systems;patient diagnosis,Dartmouth Medical School;EDRN IC data management capabilities;EDRN IC data processing capabilities;EDRN Informatics Center;EDRN laboratories;LabCAS;Laboratory Catalog and Archive Service;NASA Earth data sets;NASA Jet Propulsion Laboratory;NIH;National Cancer Institute Early Detection Research Network;National Institutes of Health;Planetary data sets;Vanderbilt University Medical Center;cancer biomarker research clinical application;cross-institutional collaborative initiative;data analysis pipelines;information service infrastructure;laboratory-targeted data management system;laboratory-targeted data processing system;software tools,,0,,20,,no,27-29 May 2014,,IEEE,IEEE Conference Publications
A Linear Performance-Breakdown Model for GPU Programming Optimization Guidance,M. A. C. M.; S. Hiroyuki,"Dept. of Electr. Eng. & Inf. Sci., Univ. of Tokyo, Tokyo, Japan",2014 IEEE International Parallel & Distributed Processing Symposium Workshops,20141204,2014,,,596,603,"The use Graphic Processing Units (GPU) as computing accelerators has been. Nevertheless, writing efficient GPU programs is a difficult and time consuming task. In this paper we present the Linear Performance Breakdown Model (LBPM), an analytic model that is used to extract the breakdown of GPU kernel programs execution time into the three major components that affect its running time. The model can be used as a tool to provide guidelines to detect the performance bottlenecks. Our approach is the incorporation of three elements, the Global-to-Shared Memory Time slice, Shared-to-Private Time slice and Processing Units Time slice. These three factors are integrated into a performance model formula by applying the Normalized Least Squares Method (NLSM). The resulting coefficients are used to construct a performance breakdown graph that reveals the effects of each element in the total execution time of the kernel program. We demonstrate the results obtained with our proposed model with two common numeric routines: Single-Precision General Matrix Multiplication (SGMM) and Fast Fourier Transform (FFT), and apply the model to the results obtained from two GPU devices: A8-3870 AMD Accelerated Processing Unit (APU) and a GTX 660 Nvidia GPU.",,Electronic:978-1-4799-4116-2; POD:978-1-4799-4115-5,10.1109/IPDPSW.2014.70,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6969440,GPGPU;Modeling;OpenCL,Computational modeling;Computer architecture;Graphics processing units;Kernel;Performance evaluation;Programming;Registers,fast Fourier transforms;graph theory;graphics processing units;least squares approximations;matrix multiplication;shared memory systems;software performance evaluation,A8-3870 AMD accelerated processing unit;APU;FFT;GPU devices;GPU kernel program execution;GPU programming optimization guidance;GTX 660 Nvidia GPU;LBPM;NLSM;SGMM;analytic model;computing accelerators;fast Fourier transform;global-to-shared memory time slice;graphic processing units;kernel program;linear performance-breakdown model;normalized least squares method;performance breakdown graph;processing unit time slice;shared-to-private time slice;single-precision general matrix multiplication;time consuming task,,0,,25,,no,19-23 May 2014,,IEEE,IEEE Conference Publications
A log data analytics based scheduling in open source cloud software,V. Srikrishnan; E. Sivasankar; R. Pitchiah,"Centre for Development of Advanced Computing, Chennai, Tamil Nadu, India","2014 International Conference on Parallel, Distributed and Grid Computing",20150205,2014,,,390,395,"The paper proposes a Log Data Analytics based Scheduling in the Private Cloud environment in order to boost up the probability of launching Virtual Machine successfully. Analytics is applied to the logs maintained by Eucalyptus, a Open Source Cloud Software. The Proposed Methodology focuses on scheduling in the private cloud built with Eucalyptus Cloud Software, so that the proposed Scheduling at Eucalyptus Cloud Software is Analytics based; From the logs certain use cases are determined, which shall be used for capacity planning. Analytics is a conjucture, grouping, investigation, tracking and reporting of data for the cause of perception and optimization of utilization.",,CD-ROM:978-1-4799-7681-2; Electronic:978-1-4799-7683-6; POD:978-1-4799-7684-3,10.1109/PDGC.2014.7030777,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7030777,Capacity Planning;Cloud Computing;Data Analytics;Eucalyptus;HDFS;Hadoop;Scheduling;Xen,Cloud computing;Data analysis;Processor scheduling;Servers;Virtual machining;Virtualization,cloud computing;data analysis;public domain software;scheduling;virtual machines,Eucalyptus cloud software;capacity planning;log data analytics based scheduling;open source cloud software;private cloud environment;virtual machine,,0,,11,,no,11-13 Dec. 2014,,IEEE,IEEE Conference Publications
A method for the selection of software development life cycle models using analytic hierarchy process,M. A. Khan; A. Parveen; M. Sadiq,"Fac. of Eng. & Technol., Electr. Eng. Sect., Jamia Millia Islamia, New Delhi, India",2014 International Conference on Issues and Challenges in Intelligent Computing Techniques (ICICT),20140403,2014,,,534,540,"The success rate of software system depends upon the type of software development life cycle (SDLC) models that we employ at the time of software development process. In literature, we have identified different types of SDLC models like, agile methods, rapid application development method, and traditional methods; and choosing one of them is not an easy task according to need/criteria of the software projects. Therefore, in order to address this issue, we present a method for the selection of SDLC models using analytic hierarchy process (AHP). Finally, the utilization of the proposed approach is demonstrated with the help of an example.",,DVD:978-1-4799-2899-6; Electronic:978-1-4799-2900-9; POD:978-1-4799-2901-6,10.1109/ICICICT.2014.6781338,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6781338,AHP;Decision Making Process;SDLC,Adaptation models;Computational modeling;Software,analytic hierarchy process;software engineering,AHP;SDLC models;agile methods;analytic hierarchy process;rapid application development method;software development life cycle model selection;software projects;traditional methods,,0,,37,,no,7-8 Feb. 2014,,IEEE,IEEE Conference Publications
A Method of Discriminative Information Preservation and In-Dimension Distance Minimization Method for Feature Selection,S. Huang; J. Zhang; X. Liu; L. Wang,"Adv. Analytics Inst., Univ. of Technol. Sydney, Sydney, NSW, Australia",2014 22nd International Conference on Pattern Recognition,20141206,2014,,,1615,1620,"Preserving sample's pair wise similarity is essential for feature selection. In supervised learning, labels can be used as a direct measure to check whether two samples are similar with each other. In unsupervised learning, however, such similarity information is usually unavailable. In this paper, we propose a new feature selection method through spectral clustering based on discriminative information as an underlying data structure. Laplacian matrix is used to obtain more partitioning information than other previously proposed structures such as the Eigen space of original data. The high dimension of sample data is projected into a low dimensional space. The in-dimension distance is also considered to get a better compact clustering result. The proposed method can be solved efficiently by updating the projection matrix and its inverse normalized diagonal matrix. A comprehensive experimental study has demonstrated that the proposed method outperforms many state-of-the-art feature selection algorithms with different criterion including the accuracy of clustering/classification and Jaccard score.",1051-4651;10514651,Electronic:978-1-4799-5209-0; POD:978-1-4799-5210-6,10.1109/ICPR.2014.286,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6976996,,Accuracy;Clustering algorithms;Educational institutions;Face;Feature extraction;Laplace equations;Support vector machines,feature selection;matrix algebra;pattern clustering;unsupervised learning,Laplacian matrix;discriminative information preservation;feature selection method;in-dimension distance minimization method;inverse normalized diagonal matrix;projection matrix;spectral clustering;unsupervised learning,,0,,22,,no,24-28 Aug. 2014,,IEEE,IEEE Conference Publications
A Method to Test the Information Quality of Technical Documentation on Websites,O. Shpak; W. LÌ_we; A. Wingkvist; M. Ericsson,"Dept. of Comput. Sci., Linnaeus Univ., Vaxjo, Sweden",2014 14th International Conference on Quality Software,20141120,2014,,,296,304,"In software engineering, testing is one of the corner-stones of quality assurance. The idea of software testing can be applied to information quality as well. Technical documentation has a set of intended uses that correspond to use cases in a software system. Documentation is, in many cases, presented via software systems, e.g., web servers and browsers, and contains software, e.g., Javascript for user interaction, animation, and customization, etc. This makes it difficult to find a clear-cut distinction between a software system and technical documentation. However, we can assume that each use case of a technical documentation involves retrieval of some sort of information that helps a user answer a specific questions. To assess information testing as a method, we implemented QAnalytics, a tool to assess the information quality of documentation that is provided by a website. The tool is web-based and allows test managers and site owners to define test cases and success criteria, disseminate the test cases to testers, and to analyze the test results. This way, information testing is easily manageable even for non-technical stakeholders. We applied our testing method and tool in a case study. According to common perception, the website of Linnaeus University needs to be re-engineered. Our method and tool helped the stakeholders identify what information is presented well and which parts of the website that need to change. The test results allowed the design and development effort to prioritize actual quality issues and potentially save time and resources.",1550-6002;15506002,Electronic:978-1-4799-7198-5; POD:978-1-4799-7199-2,10.1109/QSIC.2014.48,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6958417,information quality;web analytics;web testing,Documentation;Educational institutions;Gold;Software;Software testing;Standards,Web sites;document handling;program testing;software quality,Linnaeus University;Web sites;information quality;information testing;quality assurance;software engineering;software testing;technical documentation,,2,,11,,no,2-3 Oct. 2014,,IEEE,IEEE Conference Publications
A Methodological Approach to Provide Effective Web-Based Training by Using Collaborative Learning and Social Networks,S. CaballÌ©; D. Britch; L. Barolli; F. Xhafa,"Dept. of Comput. Sci., Multimedia, & Telecommun., Open Univ. of Catalonia, Barcelona, Spain","2014 Eighth International Conference on Complex, Intelligent and Software Intensive Systems",20141002,2014,,,64,71,"This paper presents an approach to rebuild the benefits lost from moving from traditional Instructor-led Training to Web-based Training by using Computer-Supported Collaborative Learning and Social Networks. The innovative approach of using Learning Analytics and Educational Data Mining techniques to track and analyse the interactive behavior of on-line collaborative learning and social networks is explored to enhance and improve corporate distance training. The results of this multidisciplinary proposal addressing pedagogical, technological and business issues to support Web-based Training becomes decisive for enhancing and improving the overall distance training experience and for finding new opportunities for cost-effective ways to deliver training programs. We believe the outcomes of this research will be crucial for greatly enhance and improve corporate distance training.",,CD-ROM:978-1-4799-4326-5; Electronic:978-1-4799-4325-8; POD:978-1-4799-1677-1,10.1109/CISIS.2014.10,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6915498,Computer-Supported Collaborative Learning;Educational data mining;Instructor-led training;Learning analytics;Massive Open Online Course;Social Networks;Web-based training;eLearning,Collaboration;Collaborative work;Computational modeling;Data mining;Social network services;Training,computer based training;data analysis;data mining;distance learning;groupware;social networking (online),Web-based training;computer-supported collaborative learning;corporate distance training;educational data mining technique;instructor-led training;learning analytics technique;methodological approach;social networks;training programs,,0,,37,,no,2-4 July 2014,,IEEE,IEEE Conference Publications
A model and system for applying Lean Six sigma to agile software development using hybrid simulation,K. Ghane,"Anagira, Inc., USA",2014 IEEE International Technology Management Conference,20141009,2014,,,1,4,"Software quality control and quality assurance have close ties with predictability, speed/time, and cost of software development. Process improvement has essential impact on these factors that drive the quality of software project outcomes. While stochastic design and process improvement methodologies based on the Lean Six Sigma can greatly help with process design and improvement, software development processes are substantially different from the processes in the other disciplines such as manufacturing or service operations that produce same/similar product/services. It's not feasible to quantify software processes in a discrete manner that is required by the Six Sigma methodologies. The discrete simulation that is used in operations such as car manufacturing relies on the fact that system activities change state at discrete time points. However this cannot be applied to software development as the activities are not repetitive and they have time estimates at best. The continuous simulation approach lacks the discrete simulation advantage of identifying inefficiencies and improving the processes along the line. Then the discrete simulation has the shortcoming of detecting consequences of improvements late in the process. The model and system introduced in this paper applies Six Sigma methodologies to software processes using hybrid simulation. It uses the relatively detailed empirical data - which the lean software development and agile methodologies produce - to simulate future activities. Such predictions are used as the baseline measurement data to assess the actual results of the continuous improvement activities. The Monte Carlo simulation is used to eliminate dependency on assumption of a specific distribution function for software development activities. The System also includes a framework for collecting process data and creating the empirical knowledge base that optimizes simulation, analytics and data mining. The System collects empirical data on proce- s actors and uses them in simulation to provide estimations that incorporate the human factor that has substantial role in software processes. Over time the System effectively uses machine learning to improve estimation and in some cases to recommend actions to improve performance. The resulted data can be used not only for process improvement but also for evaluating impacts of factors such as outsourcing, geographical base cost, and time zone difference on the process quality.",,Electronic:978-1-4799-3312-9; POD:978-1-4799-3313-6,10.1109/ITMC.2014.6918594,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6918594,,Capability maturity model;Estimation;Monte Carlo methods;Six sigma;Software;Software measurement,Monte Carlo methods;data mining;digital simulation;human factors;learning (artificial intelligence);project management;six sigma (quality);software process improvement;software prototyping;software quality,Monte Carlo simulation;agile software development;continuous simulation approach;data mining;discrete simulation;empirical data;human factor;hybrid simulation;lean six sigma;machine learning;process actors;process improvement methodologies;software project outcome quality;software quality assurance;software quality control;stochastic design,,1,,19,,no,12-15 June 2014,,IEEE,IEEE Conference Publications
A model architecture for Big Data applications using relational databases,E. E. A. Durham; A. Rosen; R. W. Harrison,"Department of Computer Science, Georgia State University, Atlanta, USA",2014 IEEE International Conference on Big Data (Big Data),20150108,2014,,,9,16,"Effective Big Data applications dynamically handle the retrieval of decisioned results based on stored large datasets efficiently. One effective method of requesting decisioned results, or querying, large datasets is the use of SQL and database management systems such as MySQL. But a problem with using relational databases to store huge datasets is the decisioned result retrieval time, which is often slow largely due to poorly written queries / decision requests. This work presents a model to re-architect Big Data applications in order to efficiently present decisioned results: lowering the volume of data being handled by the application itself, and significantly decreasing response wait times while allowing the flexibility and permanence of a standard relational SQL database, supplying optimal user satisfaction in today's Data Analytics world. In this paper we review a Big Data case study in the telecommunications field and use it to experimentally demonstrate the effectiveness of our approach.",,Electronic:978-1-4799-5666-1; POD:978-1-4799-5667-8,10.1109/BigData.2014.7004462,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7004462,Big Data analysis;Business Intelligence;Data Mining;Relational database;SQL;materialized view;query;query optimization,Big data;Companies;Databases;Modems;Software;Standards,Big Data;SQL;query processing;relational databases;software architecture,Big Data applications;MySQL;data analytics;data handling;database management systems;decision requests;decisioned result retrieval time;model architecture;optimal user satisfaction;querying;response wait times;standard relational SQL database;stored large datasets,,1,,49,,no,27-30 Oct. 2014,,IEEE,IEEE Conference Publications
A Multiple Features Distance Preserving (MFDP) Model for Saliency Detection,D. Guo; J. Zhang; M. Xu; X. He; M. Li; C. Zhao,"Sch. of Comput. Sci. & Eng., Nanjing Univ. of Sci. & Technol., Nanjing, China",2014 International Conference on Digital Image Computing: Techniques and Applications (DICTA),20150115,2014,,,1,7,"Playing a vital role, saliency has been widely applied for various image analysis tasks, such as content-aware image retargeting, image retrieval and object detection. It is generally accepted that saliency detection can benefit from the integration of multiple visual features. However, most of the existing literatures fuse multiple features at saliency map level without considering cross-feature information, i.e. generate a saliency map based on several maps computed from an individual feature. In this paper, we propose a Multiple Feature Distance Preserving (MFDP) model to seamlessly integrate multiple visual features through an alternative optimization process. Our method outperforms the state-of-the-arts methods on saliency detection. Saliency detected by our method is further cooperated with seam carving algorithm and significantly improves the performance on image retargeting.",,Electronic:978-1-4799-5409-4; POD:978-1-4799-5410-0,10.1109/DICTA.2014.7008087,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7008087,,Computational modeling;Educational institutions;Equations;Feature extraction;Image color analysis;Mathematical model;Visualization,feature extraction;object detection;optimisation,content-aware image retargeting;image analysis tasks;image retrieval;multiple feature distance preserving model;object detection;optimization process;saliency detection;seam carving algorithm;visual features,,0,,26,,no,25-27 Nov. 2014,,IEEE,IEEE Conference Publications
A network analytics system in the SDN,V. Sokolov; I. Alekseev; D. Mazilov; M. Nikitinskiy,"P. G. Demidov Yaroslavl State University, Russia",2014 International Science and Technology Conference (Modern Networking Technologies) (MoNeTeC),20150115,2014,,,1,3,"The emergence of virtualization and security problems of the network services, their lack of scalability and flexibility force network operators to look for ‰ÛÏsmarter‰Ûù tools for network design and management. With the continuous growth of the number of subscribers, the volume of traffic and competition at the telecommunication market, there is a stable interest in finding new ways to identify weak points of the existing architecture, preventing the collapse of the network as well as evaluating and predicting the risks of problems in the network. To solve the problems of increasing the fail-safety and the efficiency of the network infrastructure, we offer to use the analytical software in the SDN context.",,Electronic:978-1-4799-7595-2; POD:978-1-4799-7596-9,10.1109/MoNeTeC.2014.6995603,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6995603,analysis;analytics;application programming interface;big data;dynamic network model;fail-safety;flow;flow table;heuristic;load balancing;monitoring;network statistics;network topology;openflow;protocol;sdn;smart tool;software system;software-defined networking;weighted graph,Bandwidth;Data models;Monitoring;Network topology;Ports (Computers);Protocols;Software systems,computer network management;computer network security;network analysers;software defined networking;virtualisation,SDN context;analytical software;fail-safety;force network operators;network analytics system;network design;network infrastructure;network management;network services;security problems;smarter tools;software-defined network;telecommunication market;virtualization,,1,,4,,no,28-29 Oct. 2014,,IEEE,IEEE Conference Publications
A new algorithm for numerical comparison of environmental sustainability of heating systems,R. Lamedica; E. Santini; D. Z. Romito,"DIAEE - Dept. of Astronaut., Electr. & Energy Eng., SAPIENZA Univ. of Rome, Rome, Italy",2014 IEEE International Energy Conference (ENERGYCON),20140710,2014,,,1156,1161,"This paper describes a technique for the selection of an heating system, among a choice of technologies, depending only on the environmental sustainability of the system itself. The analytical bases of this technique are introduced and a suitable numerical procedure is developed. The software implementation, built by the Authors in a Matlab environment, is described and numerical results are shown.",,Electronic:978-1-4799-2449-3; POD:978-1-4799-2450-9; USB:978-1-4799-2448-6,10.1109/ENERGYCON.2014.6850569,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6850569,Analytic Hierarchy Process;Heating system;Pollutant;Sustainability,Buildings;Fuels;Gases;Global warming;Heating;Ions;Software,electric heating;power engineering computing;sustainable development,Matlab environment;environmental sustainability;heating systems;numerical procedure;software implementation,,0,,14,,no,13-16 May 2014,,IEEE,IEEE Conference Publications
A New Coupled Metric Learning for Real-time Anomalies Detection with High-Frequency Field Programmable Gate Arrays,F. Jiang; D. Luo,"Fac. of Eng. & IT, Univ. of Technol., Sydney, Sydney, NSW, Australia",2014 IEEE International Conference on Data Mining Workshop,20150129,2014,,,1254,1261,"Billions of internet end-users and device to device connections contribute to the significant data growth in recent years, large scale, unstructured, heterogeneous data and the corresponding complexity present challenges to the conventional real-time online fraud detection system security. With the advent of big data era, it is expected the data analytic techniques to be much faster and more efficient than ever before. Moreover, one of the challenges with many modern algorithms is that they run too slowly in software to have any practical value. This paper proposes a Field Programmable Gate Array (FPGA) -based intrusion detection system (IDS), driven by a new coupled metric learning to discover the inter- and intra-coupling relationships against the growth of data volumes and item relationship to provide a new approach for efficient anomaly detections. This work is experimented on our previously published NetFlow-based IDS dataset, which is further processed into the categorical data for coupled metric learning purpose. The overall performance of the new hardware system has been further compared with the presence of conventional Bayesian classifier and Support Vector Machines classifier. The experimental results show the very promising performance by considering the coupled metric learning scheme in the FPGA implementation. The false alarm rate is successfully reduced down to 5% while the high detection rate (=99.9%) is maintained.",2375-9232;23759232,Electronic:978-1-4799-4274-9; POD:978-1-4799-4273-2,10.1109/ICDMW.2014.203,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7022747,Metric Learning; Field Programmable Gate Arrays; Netflow; Intrusion Detection Systems,Field programmable gate arrays;Intrusion detection;Measurement;Neural networks;Real-time systems;Software;Vectors,Internet;data analysis;field programmable gate arrays;security of data;support vector machines,Bayesian classifier;FPGA-based intrusion detection system;Internet end-users;NetFlow-based IDS dataset;data analytic techniques;device to device connections;false alarm rate;high-frequency field programmable gate arrays;metric learning;real-time anomalies detection;real-time online fraud detection system security;support vector machines classifier,,1,,29,,no,14-14 Dec. 2014,,IEEE,IEEE Conference Publications
A novel domain-specific language for the robot welding automation domain,M. Moser; M. Pfeiffer; J. Pichler,"Software Analytics and Evolution, Software Competence Center Hagenberg GmbH, 4232 Hagenberg, Austria",Proceedings of the 2014 IEEE Emerging Technology and Factory Automation (ETFA),20150112,2014,,,1,6,"Implementation, fault analysis, and maintenance of robot welding automation solutions are traditionally restricted to professional software developers only. Program code is written in a general purpose programming language and, hence, unmanageable by other stakeholders with limited or no programming skills. To tackle this problem we have implemented a domain-specific language (DSL) specifically designed to the domain of robot welding automation and to be intuitively manageable by all stakeholders. The created DSL supports a textual and visual notation and is embedded within a full featured tool chain which let our customer fully replace the creation and maintenance of welding automation solutions by our DSL-based development approach.",1946-0740;19460740,Electronic:978-1-4799-4845-1; POD:978-1-4799-4844-4,10.1109/ETFA.2014.7005348,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7005348,Domain-Specific Language;Industrial Automation,Automation;DSL;Programming;Robot control;Visualization;Welding,programming languages;robotic welding,DSL-based development approach;domain-specific language;fault analysis;program code;programming language;robot welding automation;stakeholders,,0,,21,,no,16-19 Sept. 2014,,IEEE,IEEE Conference Publications
A practical approaches to decrease the consistency index in AHP,S. Han,"Faculty of Management in Nagoya University of Commerce and Business, Nagoya, Japan",2014 Joint 7th International Conference on Soft Computing and Intelligent Systems (SCIS) and 15th International Symposium on Advanced Intelligent Systems (ISIS),20150219,2014,,,867,872,"The Consistency is the most important measurement of the results from pairwise comparison in the AHP. Pairwise comparison is a method to calculate the weights for each element in order to perform a comparison of two advantages. The consistency of the AHP will be determined by the value of C.I. from the pairwise comparison table. If its value is for the case exceeding 0.1 (Saaty's criteria 1980) [12] is not consistent, pairwise comparison has to perform again. A longstanding weakness of AHP is explored by considering combinations of research costs, which seeks to address a difficult problem. This article aims to develop an efficient method to decrease the value of C.I. using an evolutionary algorithm and random search approach. Finally, we propose a software solution and discuss its effectiveness.",,Electronic:978-1-4799-5955-6; POD:978-1-4799-5956-3; USB:978-1-4799-5954-9,10.1109/SCIS-ISIS.2014.7044748,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7044748,Analytic Hierarchy Process (AHP);Consistency Index(CI);Genetic Algorithm(GA);Random Search(RS);Software,Analytic hierarchy process;Filtering;Genetic algorithms;Indexes;Mathematical model;Software,analytic hierarchy process;evolutionary computation,AHP;analytic hierarchy process;consistency index;difficult problem;evolutionary algorithm;pairwise comparison;pairwise comparison table;random search approach;research costs,,0,,17,,no,3-6 Dec. 2014,,IEEE,IEEE Conference Publications
A Quantitative Evaluation of ERP Systems Quality Model,T. A. Alrawashdeh; M. I. Muhairat; S. M. Alqatawneh,"Dept. of Comput. Sci., Al-Zaytoohnah Univ. of Jordan, Amman, Jordan",2014 11th International Conference on Information Technology: New Generations,20140602,2014,,,46,49,"The ERP system is a complex and comprehensive software that integrates various enterprise's functions and resources. Although this system provides the firms many benefits, they still hesitate to adopt it due to high cost and risks. Thus, this study identifies and analysis critical quality characteristics that should be considered to ensure successful development and implementation for the ERP system. The study identifies the new features of ERP systems that differ from other information systems' features. Then in order to develop ERP system quality model, ISO/IEC 9126 standard has been adapted. Finally, analytic Hierarchy Process (AHP) has been applied to evaluate the quality of the proposed model's characteristics. The derived quality characteristics could be used to compare ERP systems which help the organizations to implement better system quality.",,Electronic:978-1-4799-3188-0; POD:978-1-4799-3189-7,10.1109/ITNG.2014.37,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6822174,Analytic Hierarchy Process (AHP);ERP Systems Quality;ERP Systems Quality Model;ERP systems Quality Characteristics,Adaptation models;Analytic hierarchy process;Complexity theory;IEC standards;ISO standards;Software quality,IEC standards;ISO standards;analytic hierarchy process;enterprise resource planning;software quality,AHP;ERP systems quality model;ISO/IEC 9126 standard;analytic hierarchy process;critical quality characteristics analysis;information system feature;quantitative evaluation,,0,,12,,no,7-9 April 2014,,IEEE,IEEE Conference Publications
A Reference Model for the New Product Development in Medium-Sized Technology-Based Electronics Enterprises,E. Gomes Salgado; V. Antonio Pamplona Salomon; C. H. Pereira Mello; C. E. Sanches da Silva,"Univ. Fed. de Alfenas (UNIFAL-MG), Alfenas, Brazil",IEEE Latin America Transactions,20150120,2014,12,8,1341,1348,"New Product development (NPD) is getting even more significant to the entrepreneurship competitiveness due to the increasing market internationalization, to the product diversity and variety and the reduction of the product life cycle. This present work has as its main goal to propose a reference model adapted to NPD of technology-based companies (TBC) that produce electronics. This research follows a combined methodology approach, that is, a qualitative-quantitative approach. First, a qualitative approach is applied with the study of multiple cases in order to identify the NPD features based on electronics manufacturing. Then, a qualitative approach is employed with application on multi-criterion decision-making method as to select macro-phases, phases and activities from the proposed model for NPD. Finally, studies of multiple cases are carried out to verify the proposed model adequacy. The results analyses suggests that the proposed model may be considered convenient.",1548-0992;15480992,,10.1109/TLA.2014.7014499,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7014499,Analytic Hierarchy Process;Medium-sized electronics companies;New product development;Technology-based companies,Adaptation models;Companies;Consumer electronics;Monitoring;Product development;Software;Solid modeling,electronics industry;innovation management;product life cycle management;technology management,NPD;electronics enterprises;electronics manufacturing;entrepreneurship competitiveness;increasing;market internationalization;medium-sized technology;multicriterion decision making;new product development;product diversity;product life cycle reduction;technology-based companies,,0,,,,no,Dec. 2014,,IEEE,IEEE Journals & Magazines
A social analytics platform for smarter commerce solutions,M. Desmond; H. L. Guo; F. F. Heath; S. Bao; E. Khabiri; S. Krasikov; N. Modani; S. Nagar; M. Ohno; H. Srinivasan; H. Takeuchi; R. VaculÌ_n; S. W. Zhao; T. Hamid,,IBM Journal of Research and Development,20141121,2014,58,6-May,10:01,10:14,"When providing customers with a personalized shopping experience, there is tremendous value in understanding and applying social data shared by those consumers. Understanding this data and how best to generate business value from it is the core challenge of many businesses today. Friends, family, and experts alike influence consumers in their shopping preferences and purchase decisions. Yet, the ability of a business to analyze data on such influence, and recommend products and services that best respond to its customers' needs or aspirations, is typically limited by fragmented capabilities; a business relies heavily on the use of spreadsheets, manual market analysis, isolated software, or reactive messaging. This paper offers a solution to this fragmentary approach by introducing a social analytics platform for smarter commerce. This platform provides a holistic understanding of the customer by making use of social and enterprise data to present recommendations and related opinions, and to isolate influencers so as to ultimately provide customers with a personalized shopping experience. The functionality described in this paper is in the context of the retail industry but can be applied to other industries. The paper describes the architecture of the social analytics platform and the various analytics components currently implemented as part of the platform.",0018-8646;00188646,,10.1147/JRD.2014.2346262,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6964897,,Behavioral science;Consumer behavior;Customer purchases;Electronic commerce;Internet;Sales and marketing;Social factors,,,,0,,,,no,Sept.-Nov. 2014,,IBM,IBM Journals & Magazines
A System of Systems Service Design for Social Media Analytics,R. K. Wong; C. H. Chi; Z. Yu; Y. Zhao,"Nat. ICT Australia, Univ. of New South Wales, Sydney, NSW, Australia",2014 IEEE International Conference on Services Computing,20141020,2014,,,789,796,"Most social media analyses such as sentiment analysis for microblogs are often built as standalone, endpoint to endpoint applications. This makes the collaboration among distributed software and data service providers to create composite social analytic solutions difficult. This paper first proposes a system of systems service architecture (SoS-SA) design for social media analytics that support and facilitate efficient collaboration among distributed service providers. Then we propose a novel Twitters sentiment analysis service implemented on top of this design to illustrate its potentials. Current sentiment classification applications based on supervised learning methods relies too heavily on the chosen large training datasets, approaches using automatically generated training datasets also often result in the huge imbalance between the subjective classes and the objective classes in the sentiment of tweets, making it difficult to obtain good recall performance for the subjective ones. To address this issue, our proposed solution is based on a semi-supervised learning method for tweet sentiment classification. Experiments show that the performance of our method is better than those of the previous work.",,Electronic:978-1-4799-5066-9; POD:978-1-4799-5067-6,10.1109/SCC.2014.107,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6930609,semi-supervised learning;service-oriented architecture;social analytics,Computer architecture;Databases;Geospatial analysis;Servers;Software;Training;Twitter,learning (artificial intelligence);pattern classification;social networking (online);software architecture,SoS-SA design;Twitters sentiment analysis service;composite social analytic solutions difficult;data service provider;distributed service provider;distributed software;microblogs;semi-supervised learning method;sentiment classification application;social media analysis;social media analytics;supervised learning methods;system of systems service architecture design;system of systems service design;training dataset;tweet sentiment classification,,1,,22,,no,June 27 2014-July 2 2014,,IEEE,IEEE Conference Publications
A three step process to design visualisations for GeoTemporal analysis (VAST 2014 Mini Challenge 2),A. Chua; R. Sakai; J. Aerts; A. Vande Moere,"Datavis Lab, University of Leuven, Belgium",2014 IEEE Conference on Visual Analytics Science and Technology (VAST),20150216,2014,,,349,350,"Given vehicle tracking data, loyalty and credit card logs of employees from a fictitious company, GAStech, participants of VAST 2014 mini challenge 2 were tasked to extract the common daily routine of employees and identify any suspicious activities that may be present in the data. In this paper, we reflect on our analysis procedure focusing on each step of the process that contributed to problem solving. Accordingly, we describe the features incorporated into our software at each stage of the process and justify the design decisions that were made. Inspired by DiBiase's approach to visual analysis [1], our procedure consists of three stages (Fig. 1). With off-the-shelf software, such as R and QGIS, we conducted exploratory data analysis [2] to generate a diverse range of insights. The insights were evaluated based on their relevance to the given task. They were used to formulate hypotheses and data task abstraction [3] resulting in a set of complementary tools comprising of an origin-destination map, a timeline and a flow diagram that we developed in processing. These tools were not designed to function as an integrated software package but were treated as rapid prototypes that would afford more flexibility in the design and development cycle [4].",,Electronic:978-1-4799-6227-3; POD:978-1-4799-6186-3,10.1109/VAST.2014.7042560,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7042560,,Credit cards;Data visualization;Electronic mail;Employment;Software;Splines (mathematics);Visualization,,,,0,,5,,no,25-31 Oct. 2014,,IEEE,IEEE Conference Publications
A Three-Dimensional Model for Software Security Evaluation,Z. Han; X. Li; R. Feng; J. Hu; G. Xu; Z. Feng,"Dept. of Comput. Sci. & Technol., Tianjin Univ., Tianjin, China",2014 Theoretical Aspects of Software Engineering Conference,20141206,2014,,,34,41,"Software security evaluation is considered as a significant and indispensible activity in all phases of software development lifecycle, and there are also many factors that should be taken into account such as the environment, risks, and development documents. Despite the achievements of the past several decades, there is still a lack of methodology in evaluating software security systematically. In this paper, we propose a comprehensive model for evaluating the software security from three different but complementary points of view: technology, management and engineering. The technological dimension is 7 security levels based on Evaluation Assurance Levels (EALs) from ISO/IEC15408, the management dimension mainly concerns the management of software infrastructures, development documents and risks, and the engineering dimension focuses on 5 stages of software development lifecycle. Experts evaluate software security through the evidence items which are collected from these three dimensions and provide their assessments. Relying on Analytic Hierarchy Process (AHP) and Dempster-Shafer Evidence Theory, assessments obtained from the experts can be combined and merged to get a score which presents the security degree of software. A case study illustrates how the evaluators may use the proposed approach to evaluate security of their system.",,Electronic:978-1-4799-5029-4; POD:978-1-4799-5030-0; USB:978-1-4799-5028-7,10.1109/TASE.2014.31,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6976565,Common Criteria;Evidence;Software Life Cycle;Software Security Evaluation;Three-Dimensional Model,Analytical models;Capability maturity model;Security;Software;Solid modeling;Testing;Uncertainty,analytic hierarchy process;inference mechanisms;security of data;software engineering;uncertainty handling,AHP;Dempster-Shafer evidence theory;analytic hierarchy process;software development lifecycle;software infrastructure management;software security evaluation,,0,,28,,no,1-3 Sept. 2014,,IEEE,IEEE Conference Publications
A web-based visual analytics system for air quality monitoring data,Z. Liao; Y. Peng; Y. Li; X. Liang; Y. Zhao,"School of Software, Central South University, Changsha 410075, China",2014 22nd International Conference on Geoinformatics,20141110,2014,,,1,6,"With the increasingly severe air pollution, how to effectively process and analyze air quality data has become a hot issue. This paper introduces visual analytics into the processing and analysis of air quality data, and presents a web-based visual analytics system, AirVIS, for collaborative and comprehensive analysis on the spatial-temporal and multi-dimensional features of air quality monitoring datasets. This system offers three visual views: a GIS view used for spatial analysis, a scatter plot view used for temporal analysis and a parallel coordinate's view used for multi-dimensional analysis, all of which are inner-tied by the newest air environment standard, unified color-mapping strategies of Air Quality Index (AQI) levels and various interactive means. In case studies, our paper takes the air quality data in Beijing as our examples and illustrates that how this system provides users with deepened insight to discern the spatial-temporal characteristics and analyze the internal links of different air pollutants.",2161-024X;2161024X,Electronic:978-1-4799-5714-9; POD:978-1-4799-5715-6,10.1109/GEOINFORMATICS.2014.6950834,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6950834,Air Quality Index (AQI);Geographic Information System (GIS);air pollution;multi-dimensional analysis;parallel coordinates;scatter diagram;spatio-temporal analysis;visual analytics,Air pollution;Cognitive science;Data visualization;Gases;Monitoring;Visual analytics,air pollution;air quality;geographic information systems,AQI levels;AirVIS;Beijing;GIS view;air environment standard;air pollution;air quality index;air quality monitoring datasets;collaborative analysis;comprehensive analysis;multidimensional analysis;parallel coordinate view;temporal analysis;unified color-mapping strategies;web-based visual analytics system,,0,,16,,no,25-27 June 2014,,IEEE,IEEE Conference Publications
‰ÛÏAdvanced Modeling Grid Research: An overview of DOE's activities‰Ûù,G. Bindewald,"U.S. Dept. of Energy (DOE), Washington, DC, USA",2014 IEEE PES General Meeting | Conference & Exposition,20141030,2014,,,1,1,"Abstract form only given. Shifting operational data analytics from a traditionally off-line environment to real-time situational awareness (e.g., visibility) to measurement-based, fast control will require significant advancements in algorithms and computational approaches. The Advanced Modeling Grid Research Program at the U.S. Department of Energy leverages scientific research in mathematics for application to power system models and software tools. This will result in a new class of decision support tools that will simulate dynamic events and help inform operators on real-time conditions to maintain stability.",1932-5517;19325517,Electronic:978-1-4799-6415-4; POD:978-1-4799-6416-1; USB:978-1-4799-6414-7,10.1109/PESGM.2014.6939841,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6939841,,Abstracts;Computational modeling;Data analysis;Mathematical model;Power system stability;Real-time systems;Velocity measurement,decision support systems;power grids;power system simulation;power system stability;software tools,Advanced Modeling Grid Research Program;United States Department of Energy;decision support tools;operational data analytics;power system models;real time situational awareness;software tools,,0,,,,no,27-31 July 2014,,IEEE,IEEE Conference Publications
Abstract Machine Models and Proxy Architectures for Exascale Computing,J. A. Ang; R. F. Barrett; R. E. Benner; D. Burke; C. Chan; J. Cook; D. Donofrio; S. D. Hammond; K. S. Hemmert; S. M. Kelly; H. Le; V. J. Leung; D. R. Resnick; A. F. Rodrigues; J. Shalf; D. Stark; D. Unat; N. J. Wright,"Sandia Nat. Labs., Albuquerque, NM, USA",2014 Hardware-Software Co-Design for High Performance Computing,20150122,2014,,,25,32,"To achieve exascale computing, fundamental hardware architectures must change. This will significantly impact scientific applications that run on current high performance computing (HPC) systems, many of which codify years of scientific domain knowledge and refinements for contemporary computer systems. To adapt to exascale architectures, developers must be able to reason about new hardware and determine what programming models and algorithms will provide the best blend of performance and energy efficiency in the future. An abstract machine model is designed to expose to the application developers and system software only the aspects of the machine that are important or relevant to performance and code structure. These models are intended as communication aids between application developers and hardware architects during the co-design process. A proxy architecture is a parameterized version of an abstract machine model, with parameters added to elucidate potential speeds and capacities of key hardware components. These more detailed architectural models enable discussion among the developers of analytic models and simulators and computer hardware architects and they allow for application performance analysis, system software development, and hardware optimization opportunities. In this paper, we present a set of abstract machine models and show how they might be used to help software developers prepare for exascale. We then apply parameters to one of these models to demonstrate how a proxy architecture can enable a more concrete exploration of how well application codes map onto future architectures.",,Electronic:978-1-4799-7564-8; POD:978-1-4799-7565-5,10.1109/Co-HPC.2014.4,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7017960,,Abstracts;Computational modeling;Hardware;Instruction sets;Multicore processing;System-on-chip,optimisation;parallel processing;power aware computing,HPC;abstract machine model;application codes;application performance analysis;code structure;codesign process;computer hardware architects;contemporary computer systems;energy efficiency;exascale computing;fundamental hardware architectures;hardware architects;hardware optimization opportunities;high performance computing systems;programming models;proxy architecture;proxy architectures;scientific applications;system software;system software development,,3,,22,,no,17-17 Nov. 2014,,IEEE,IEEE Conference Publications
Accommodating the Variable Timing of Software AES Decryption on Mobile Receivers,K. Kang; J. Ryu; D. K. Noh,"Dept. of Comput. Sci. & Eng., Hanyang Univ., Ansan, South Korea",IEEE Systems Journal,20140821,2014,8,3,726,736,"Broadcast and multicast services in CDMA2000 wireless networks restrict the provision of high-quality multimedia services to their intended recipients by encrypting the content using the advanced encryption standard (AES) block cipher in the security layer of the broadcast protocol suite. We profile the execution time and the energy of each transformation within the AES decryption process and propose a novel analytic model for predicting the time and energy that are required to decrypt the content at a mobile receiver. The model uses the cross-layer information, including the characteristics of error control in the MAC layer and the varying conditions of the fading channel in the physical layer. In particular, we find that the decryption time varies significantly with the condition of the physical channel. Rate control is, therefore, required to smooth out these variations in the decryption time. For this purpose, we propose the introduction of a jitter buffer into the security layer and estimate the size of this jitter buffer to provide seamless multimedia services.",1932-8184;19328184,,10.1109/JSYST.2012.2232552,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6412708,Advanced encryption standard (AES) cipher;mobile systems;multimedia broadcasting;timing analysis,Ciphers;Encryption;Mobile communication;Multimedia communication;Protocols,3G mobile communication;cryptography;protocols,CDMA2000 wireless networks;MAC layer;advanced encryption standard;block cipher;broadcast protocol suite;cross-layer information;fading channel;high-quality multimedia services;mobile receivers;novel analytic model;rate control;software AES decryption;variable timing,,1,,35,,no,Sept. 2014,,IEEE,IEEE Journals & Magazines
Addressing human bottlenecks in big data,J. M. Hellerstein,,2014 IEEE International Conference on Big Data (Big Data),20150108,2014,,,4,4,"We live in an era when compute is cheap, data is plentiful, and system software is being given away for free. Today, the critical bottlenecks in data-driven organizations are human bottlenecks, measured in the costs of software developers, IT professionals, and data analysts. How can computer science remain relevant in this context? The Big Data ecosystem presents two archetypal settings for answering this question: NoSQL distributed databases, and analytics on Hadoop. In the case of NoSQL, developers are being asked to build parallel programs for global-scale systems that cannot even guarantee the consistency of a single register of memory. How can this possibly be made to work? I'll talk about what we have seen in the wild in user deployments, and what we've learned from developers and their design patterns. Then I'll present theoretical results - the CALM Theorem - that shed light on what's possible here, and what requires more expensive tools for coordination on top of the typical NoSQL offerings. Finally, I will highlight some new approaches to writing and testing software - exemplified by the Bloom language - that can help developers of distributed software avoid expensive coordination when possible, and have the coordination logic synthesized for them automatically when necessary. In the Hadoop context, the key bottlenecks lie with data analysts and data engineers, who are routinely asked to work with data that cannot possibly be loaded into tools for statistical analytics or visualization. Instead, they have to engage in time-consuming data ‰ÛÏwrangling‰Ûù - to try and figure out what's in their data, whip it into a rectangular shape for analysis, and figure out how to clean and integrate it for use. I'll discuss what we heard talking with data analysts in both academic interviews and commercial engagements. Then I'll talk about how techniques from human-computer interaction, machine learning, and database systems can be brought together - o address this human bottleneck, as exemplified by our work on various systems including the Data Wrangler project and Trifacta's platform for data transformation.",,Electronic:978-1-4799-5666-1; POD:978-1-4799-5667-8,10.1109/BigData.2014.7004205,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7004205,,,Big Data;data analysis;distributed databases;human computer interaction;learning (artificial intelligence);program testing,Big Data ecosystem;Bloom language;CALM theorem;Data Wrangler project;Hadoop;NoSQL distributed databases;Trifacta platform;data analysts;data engineers;data transformation;data wrangling;database systems;distributed software developers;global-scale systems;human-computer interaction;machine learning;parallel programs;software testing,,0,,,,no,27-30 Oct. 2014,,IEEE,IEEE Conference Publications
Adoption of Free Libre Open Source Software (FLOSS): A Risk Management Perspective,R. S. Kenett; X. Franch; A. Susi; N. Galanis,"KPA Group, Raanana, Israel",2014 IEEE 38th Annual Computer Software and Applications Conference,20140922,2014,,,171,180,"Free Libre Open Source Software (FLOSS) has become a strategic asset in software development, and open source communities behind FLOSS are a key player in the field. The analysis of open source community dynamics is a key capability in risk management practices focused on the integration of FLOSS in all types of organizations. We are conducting research in developing methodologies for managing risks of FLOSS adoption and deployment in various application domains. This paper is about the ability to systematically capture, filter, analyze, reason about, and build theories upon, the behavior of an open source community in combination with the structured elicitation of expert opinions on potential organizational business risk. The novel methodology presented here blends together qualitative and quantitative information as part of a wider analytics platform. The approach combines big data analytics with automatic scripting of scenarios that permits experts to assess risk indicators and business risks in focused tactical and strategic workshops. These workshops generate data that is used to construct Bayesian networks that map data from community risk drivers into statistical distributions that are feeding the platform risk management dashboard. A special feature of this model is that the dynamics of an open source community are tracked using social network metrics that capture the structure of unstructured chat data. The method is illustrated with a running example based on experience gained in implementing our approach in an academic smart environment setting including Mood bile, a Mobile Learning for Moodle (www.moodbile.org). This example is the first in a series of planned experiences in the domain of smart environments with the ultimate goal of deriving a complete risk model in that field.",,Electronic:978-1-4799-3575-8; POD:978-1-4799-3576-5,10.1109/COMPSAC.2014.25,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6899215,Bayesian networks;FLOSS;Free Libre Open Source Software;Moodbile;OSS;mobile technologies;risk management;smart device;smart environment;social network analysis,Bayes methods;Communities;Ecosystems;Mobile communication;Organizations;Risk management,Bayes methods;public domain software;risk management;software development management;statistical distributions,Bayesian networks;FLOSS;free libre open source software;risk management;social network metrics;software development;statistical distributions,,2,,23,,no,21-25 July 2014,,IEEE,IEEE Conference Publications
Advanced planning and control of manufacturing processes in steel industry through big data analytics: Case study and architecture proposal,J. Krumeich; D. Werth; P. Loos; J. Schimmelpfennig; S. Jacobi,"German Research Center for Artificial Intelligence (DFKI GmbH) Saarbr&#x00FC;cken, Germany",2014 IEEE International Conference on Big Data (Big Data),20150108,2014,,,16,24,"Enterprises in today's globalized world are compelled to react on threats and opportunities in a highly flexible manner. Hence, companies that are able to analyze the current state of their business processes, forecast their most optimal progresses and with this proactively control them will have a decisive competitive advantage. Technological progress in sensor technology has boosted real-time situation awareness, especially in manufacturing operations. The paper at hands examines, based on a case study stemming from the steel manufacturing industry, which production-related data is collectable using state of the art sensors forming a basis for a detailed situation awareness and for deriving accurate forecasts. However, analyses of this data point out that dedicated big data analytics approaches are required to utilize the full potential out of it. By proposing an architecture for predictive process planning and control systems, the paper intends to form a working and discussion basis for further research and implementation efforts in big data analytics.",,Electronic:978-1-4799-5666-1; POD:978-1-4799-5667-8,10.1109/BigData.2014.7004408,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7004408,Business activity monitoring;Business process forecast and simulation;Business process intelligence;Complex event processing;Event-driven business process management;Ontology;Predictive analytics,Big data;Industries;Manufacturing processes;Process control;Steel,business data processing;manufacturing data processing;process planning;steel industry,big data analytics;business process;enterprise;predictive process planning;sensor technology;steel manufacturing industry;technological progress,,2,,30,,no,27-30 Oct. 2014,,IEEE,IEEE Conference Publications
Agile Software Architecture in Advanced Data Analytics,K. E. Harper; A. Dagnino,"Ind. Software Syst., ABB Corp. Res., Raleigh, NC, USA",2014 IEEE/IFIP Conference on Software Architecture,20140609,2014,,,243,246,"Requirements evolve over the development lifecycle of a software project. Agile practices are designed specifically to address this challenge while showing early and continuous progress towards project goals. Applying an agile approach allows stakeholders to adapt the scope and capabilities of a development release to changing market needs. More recently, an agile approach has been recommended for developing the architecture of software systems, enabling the design to support current requirements and early releases while evolving to meet future expectations. Our experience defining emergent software systems to build a product line architecture for advanced data analytics demonstrates the benefits that can be gained from prioritizing work activities and delaying architecture decisions. This paper proposes a process and ontology for agile architecture development. Only the necessary aspects for each evolutionary release are designed and prototyped, as determined by expectations of the identified application domain scenarios. Feedback from implementing the scenarios using the architecture extends our understanding of the requirements and provides the backlog for successive design iterations.",,Electronic:978-1-4799-3412-6; POD:978-1-4799-3413-3,10.1109/WICSA.2014.16,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6827125,agile development;industrial experiments;industry best practices;product line architectures;software architecture,Automation;Computer architecture;Ontologies;Software architecture;Software systems;Unified modeling language,ontologies (artificial intelligence);software architecture;software prototyping,advanced data analytics;agile architecture development;agile practices;agile software architecture;application domain scenario;architecture decisions;design iterations;ontology;project goals;software project development lifecycle;software system architecture,,2,,13,,no,7-11 April 2014,,IEEE,IEEE Conference Publications
Air damping model for laterally oscillating MOEMS vibration sensors,A. Kainz; F. Keplinger; W. Hortschitz; M. Stifter,"Institute of Sensor and Actuator Systems, Vienna University of Technology, Austria",IEEE SENSORS 2014 Proceedings,20141215,2014,,,590,593,"This paper presents a comprehensive model for the damping coefficient of a laterally moving micro-opto-electro-mechanical system. While viscous forces acting onto large areas of laterally oscillating microstructures are well understood, there are contributions to the air damping that receive less attention. These include pressure forces, the effects of regularly placed holes perforating the large surfaces as well as forces exerted onto the typically much smaller side faces. The results of our analytic models are backed up with very good agreement by finite volume method simulations with the open source software OpenFOAMå¨ as well as measurements. Since the read-out of our MOEMS sensor is optical, it does not involve highly complicated comb-drive geometries that are hard to model. Thus, our semi-numerical model describes the damping behavior of the MOEMS very accurately.",1930-0395;19300395,Electronic:978-1-4799-0162-3; POD:978-1-4799-0160-9; USB:978-1-4799-0161-6,10.1109/ICSENS.2014.6985067,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6985067,,Atmospheric measurements;Atmospheric modeling;Damping;Energy measurement;Numerical models;Seismic measurements;Thickness measurement,damping;finite volume methods;micro-optomechanical devices;microsensors;vibration measurement,OpenFOAM open source software;air damping model;damping coefficient;finite volume method simulations;laterally oscillating MOEMS vibration sensors;microopto-electromechanical system;pressure forces,,0,,7,,no,2-5 Nov. 2014,,IEEE,IEEE Conference Publications
AllJoyn Lambda: An architecture for the management of smart environments in IoT,M. Villari; A. Celesti; M. Fazio; A. Puliafito,"DICIEAMA, University of Messina, Contrada di Dio, S. Agata, 98166, Italy",2014 International Conference on Smart Computing Workshops,20150223,2014,,,9,14,"The increasing number of everyday embedded devices that are interconnected over the Internet leads to the need of new software solutions for managing them in an efficient, scalable, and smart way. In addition, such devices produce a huge amount of information, causing the well known Big Data problem, that need to be stored and processed. This emerging scenario, known as Internet of Things (IoT), raises two main challenges: large-scale smart environments management and Big Data storage/analytics. In this paper, we address both challenges, proposing AllJoyn Lambda, a software solution integrating AllJoyn in the Lambda architecture used for Big Data storage and analytics. In order, to describe how the architecture works, a software prototype integrating the AllJoyn system with MongoDB and Storm is presented and tested, analyzing a ‰ÛÏsmart home‰Ûù case of study. Finally, we will focus on how can be possible to manage embedded devices in a smart fashion processing both batch and real-time data.",,Electronic:978-1-4799-6447-5; POD:978-1-4799-6448-2,10.1109/SMARTCOMP-W.2014.7046676,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7046676,AllJoyn;Apache Storm;Big Data;Internet of Things;MongoDB;Smart Environment,Androids;Big data;Computer architecture;Real-time systems;Sensors;Software;Storms,Big Data;Internet of Things;data analysis;home computing;software prototyping,AllJoyn Lambda;Big Data analytics;Big Data storage;Internet of Things;IoT;Lambda architecture;MongoDB;Storm;embedded device management;embedded devices;smart environment management;smart home;software prototype;software solutions,,5,,11,,no,5-5 Nov. 2014,,IEEE,IEEE Conference Publications
"An advanced technique to recover from BOC(1,1) false locks during the acquisition stage",L. Siniscalco; A. Emmanuele; A. Ferrarlo; M. Puccitelli; S. Fantinato; P. Crosta; R. Weiler,"Thales Alenia Space Italy, Milan, Italy",2014 7th ESA Workshop on Satellite Navigation Technologies and European Workshop on GNSS Signals and Signal Processing (NAVITEC),20150219,2014,,,1,8,"This paper investigates a novel algorithm based on the Bump and Jump technique, capable of detecting and recovering from possible false lock events during the acquisition stage of BOC(1,1) Signals. Indeed the traditional Bump and Jump technique might lead to a false lock, in particular in harsh scenarios affected by strong multipath. Concerning this undesired event, an analysis of False Lock probability is provided to assess the goodness of the new approach w.r.t. the traditional Bump and Jump algorithm. The new algorithm is compared to the classical one, providing statistics on the False Locks occurrence after the Transition to Tracking stage, given the probability density function of the coarse delays estimated in the Acquisition stage. This analysis has been extensively assessed by means of a semi-analytic MATLAB‰ã¢ simulator, representative of the Acquisition and Transition to Tracking stages of a GNSS Ground Receiver. Two different multipath models have been considered applicable to the Reference Stations context: the two rays ground multipath and diffuse multipath. A preliminary assessment with a GNSS signals simulator and a receiver prototype are also included so as to demonstrate the applicability of the software simulation results to both Bump and Jump methods.",2325-5439;23255439,Electronic:978-1-4799-6529-8; POD:978-1-4799-6530-4,10.1109/NAVITEC.2014.7045153,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7045153,1);Acquisition;Algorithm;BOC(1;False Lock Probability;Multipath,Channel estimation;Correlation;Correlators;Delays;Probability density function;Receivers;Signal processing algorithms,multipath channels;probability;satellite navigation;signal processing,"BOC(1,1) false locks;BOC(1,1) signals;GNSS ground receiver;GNSS signals simulator;acquisition stage;bump technique;coarse delays;diffuse multipath;false lock probability;ground multipath;jump technique;probability density function;reference stations context;semianalytic MATLAB simulator;software simulation;strong multipath",,0,,6,,no,3-5 Dec. 2014,,IEEE,IEEE Conference Publications
An analytics approach to traffic analysis in network virtualization,H. Zhang; J. Rhee; N. Arora; Q. Xu; C. Lumezanu; G. Jiang,"NEC Laboratories America, Princeton, New Jersey 08540, USA",10th International Conference on Network and Service Management (CNSM) and Workshop,20150119,2014,,,316,319,"Network virtualization has been propounded as a diversifying attribute of the future inter-networking paradigm. However, monitoring and troubleshooting operational virtual networks can be a daunting task, due to their size, distributed state, and additional complexity introduced by network virtualization. We propose an analytics approach for the analysis of network traces collected across hypervisors and switches. To re-organize individual trace events into path-wise slices that represent the life-cycle of individual packets, we first present a trace slicing scheme. Then, we develop a path characterization scheme to extract feature matrices from those trace slices. Using those feature metrics, we develop a set of trace analysis algorithms to cluster, rank, query, and verify packet traces. We have developed the analytics approach in a SDN network management tool, and presented evaluation results to show how it can enable visibility and effective problem diagnosis in a SDN network.",2165-9605;21659605,Electronic:978-3-901882-67-8; POD:978-1-4799-6690-5,10.1109/CNSM.2014.7014183,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7014183,Network Management;Network Virtualization;Software-Defined Networks;Traffic Analysis,Feature extraction;IP networks;Ports (Computers);Protocols;Routing;Software;Virtualization,feature extraction;software defined networking;telecommunication traffic,SDN network management tool;feature matrices extraction;inter-networking paradigm;network virtualization;operational virtual networks;path characterization scheme;trace analysis algorithms;traffic analysis,,1,,15,,no,17-21 Nov. 2014,,IEEE,IEEE Conference Publications
An approach to discover the best-fit factors for the optimal performance of Hadoop map reduce in virtualized environment,S. Vellaipandiyan; V. Srikrishnan,"Centre for Development of Advanced Computing, Chennai, India",2014 IEEE International Conference on Computational Intelligence and Computing Research,20150907,2014,,,1,5,"Map Reduce pioneered by Google is mainly employed in Big Data analytics. In Map Reduce environment, most of the algorithms are re-used for mining the data. Prediction of execution time and system overhead of MapReduce job is very vital, from which performance shall be ascertained. Cloud computing is widely used as a computing platform in business and academic communities. Performance plays a major role, when user runs an application in the cloud. User may want to estimate the application execution time (latency) before submitting a Task or a Job. Hadoop clusters are deployed on Cloud environment performing the experiment. System overhead is determined by running Map Reduce job over Hadoop Clusters. While performing the experiment, metrics such as network I/O, CPU, Swap utilization, Time to complete the job and RSS, VSZ were captured and evaluated in order to diagnose, how performance of Hadoop is influenced by reconstructing the block size and split size with respect to block size.",,Electronic:978-1-4799-3975-6; POD:978-1-4799-3976-3,10.1109/ICCIC.2014.7238471,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7238471,Big Data;Distributed framework;Hadoop;MapReduce;Performance;VM,Big data;Cloud computing;Conferences;Measurement;Random access memory;Virtualization,Big Data;cloud computing;data analysis;data mining;parallel processing;software performance evaluation;virtualisation,CPU;Cloud environment;Google;Hadoop clusters;MapReduce job;RSS;Swap utilization;VSZ;best-fit factors;big data analytics;block size reconstruction;data mining;execution time;network I/O;optimal Hadoop MapReduce performance;split size reconstruction;system overhead;virtualized environment,,0,,16,,no,18-20 Dec. 2014,,IEEE,IEEE Conference Publications
An approach to provide security to unstructured Big Data,M. R. Islam; M. E. Islam,"Department of Computer Science, American International University-Bangladesh(AIUB), Dhaka, Bangladesh","The 8th International Conference on Software, Knowledge, Information Management and Applications (SKIMA 2014)",20150409,2014,,,1,5,"Security of Big Data is a big concern. In broad sense Big Data contains two types of data such as structured and unstructured. To provide security to unstructured data is more difficult than that of structured. In this paper we have developed an approach to give adequate security to the unstructured data by considering the types of the data and their sensitivity levels. We have reviewed the different analytics methods of Big Data, which gives us the facility to build a data node of databases of different types of data. Each type of data has been further classified to provide adequate security and enhance the overhead of the security system. To provide security to data node a security suite has been designed by incorporating different security standards and algorithms. The proper security standards or algorithms can be activated using an algorithm, which has been interfaced with the data node. We have shown that data classification with respect to sensitivity levels enhance the performance of the system.",,Electronic:978-1-4799-6399-7; POD:978-1-4799-6400-0,10.1109/SKIMA.2014.7083392,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7083392,Big Data;data analytics;security standards;security suite;unstructured data,Authentication;Big data;Data analysis;Sensitivity;Standards;XML,Big Data;pattern classification;security of data;standards,data classification;security standards;security suite;unstructured Big Data,,1,,19,,no,18-20 Dec. 2014,,IEEE,IEEE Conference Publications
An experimental design to compare software requirements prioritization techniques,B. A. Mustafa; A. Zainuddin,"Faculty of Computer Systems and Software Engineering, University Malaysia Pahang, Malaysia",2014 International Conference on Computational Science and Technology (ICCST),20150219,2014,,,1,5,"Software products are getting increasingly complex described by a large number of requirements that characterize the user needs. In most cases, not all requirements can usually be met with available time and resource constraints; therefore, taking correct decisions about which requirements to implement in a release is crucial as the wrong decision from requirements engineers results in implementation of the false requirements, which lead to failure of project or product in the market. Although much research is done on requirements prioritization techniques, but studies in the area pointed out that when investigating the research made within the area, there seems to be little evidence regarding which approach is better than others and in what situations and environments. This paper presents an initial design of a controlled experiment to compare three prioritization methods. The goal of this experiment is to investigate which technique is better in terms of time consumption, accuracy, and ease of use. The experiment will be conducted in an academic context in Malaysian university.",,Electronic:978-1-4799-3241-2; POD:978-1-4799-3242-9,10.1109/ICCST.2014.7045010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7045010,controlled experiment;decision making;empirical comparison;prioritization methods;requirements prioritization,Accuracy;Analytic hierarchy process;Educational institutions;Planning;Software;Software engineering,decision making;failure analysis;software reliability;systems analysis,controlled experiment;decision making;experimental design;project failure;resource constraint;software products;software requirements prioritization techniques,,0,,17,,no,27-28 Aug. 2014,,IEEE,IEEE Conference Publications
An Image-Based Approach to Extreme Scale in Situ Visualization and Analysis,J. Ahrens; S. Jourdain; P. OLeary; J. Patchett; D. H. Rogers; M. Petersen,"Los Alamos Nat. Lab., Los Alamos, NM, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,424,434,"Extreme scale scientific simulations are leading a charge to exascale computation, and data analytics runs the risk of being a bottleneck to scientific discovery. Due to power and I/O constraints, we expect in situ visualization and analysis will be a critical component of these workflows. Options for extreme scale data analysis are often presented as a stark contrast: write large files to disk for interactive, exploratory analysis, or perform in situ analysis to save detailed data about phenomena that a scientists knows about in advance. We present a novel framework for a third option - a highly interactive, image-based approach that promotes exploration of simulation results, and is easily accessed through extensions to widely used open source tools. This in situ approach supports interactive exploration of a wide range of results, while still significantly reducing data movement and storage.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.40,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013022,,Analytical models;Atmospheric modeling;Cameras;Computational modeling;Data models;Data visualization;Databases,data analysis;data visualisation;image processing;public domain software;software tools,extreme scale data analysis;extreme scale in situ visualization;extreme scale scientific simulations;interactive image-based approach;open source tools,,8,,41,,no,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
An Improved Network Terminal Security Evaluation Index System,Y. Ou; J. Xie; J. Ling,"Fac. of Comput., Guangdong Univ. of Technol., Guangzhou, China",2014 International Conference on Management of e-Commerce and e-Government,20150223,2014,,,65,69,"For Liao Hui and others proposed network terminal security assessment index system exists index weights unreasonable distribution problem, using Delphi method and AHP calculate each index weight and the weights of total ranking of lowest level indexes relative to the highest lever indexes, identified the indicators which have greatest impact to the assessment objectives and apply it to a host of information security evaluation system. Combined with examples to prove the index system after improving its weight distribution can be more scientifically reflect the importance of the indexes in the evaluation system and the result of the host information security evaluation is reasonable and comprehensive.",,Electronic:978-1-4799-6543-4; POD:978-1-4799-6544-1,10.1109/ICMeCG.2014.23,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7046892,AHP;Delphi;Host information security;Index weight distribution;Terminal Security Assessment,Computers;Indexes;Information security;Risk management;Software;Sorting,analytic hierarchy process;forecasting theory;security of data,AHP;Delphi method;host information security evaluation;improved network terminal security evaluation index system;weight distribution,,0,,11,,no,Oct. 31 2014-Nov. 2 2014,,IEEE,IEEE Conference Publications
An Instructional Cloud-Based Testbed for Image and Video Analytics,T. J. Hacker; Y. H. Lu,"Comput. & Inf. Technol., Purdue Univ., West Lafayette, IN, USA",2014 IEEE 6th International Conference on Cloud Computing Technology and Science,20150212,2014,,,859,862,"This paper describes a cloud-based software infrastructure for teaching big data analytics. Using this infrastructure, students can retrieve and analyze real-time visual data retrieved from globally distributed network cameras.",,Electronic:978-1-4799-4093-6; POD:978-1-4799-4092-9; USB:978-1-4799-4094-3,10.1109/CloudCom.2014.61,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7037774,cloud computing;education;video analysis,Big data;Cameras;Distributed databases;Educational institutions;Streaming media;Transform coding,Big Data;cloud computing;computer aided instruction;computer science education;data analysis;teaching;video retrieval;video signal processing,big data analytics teaching;cloud-based software infrastructure;globally distributed network cameras;image analytics;instructional cloud-based testbed;real-time visual data analysis;real-time visual data retrieval;video analytics,,3,,7,,no,15-18 Dec. 2014,,IEEE,IEEE Conference Publications
An Integrated Security Framework for GOSS Power Grid Analytics Platform,T. Gibson; S. Ciraci; P. Sharma; C. Allwardt; M. Rice; B. Akyol,"Pacific Northwest Nat. Lab., Richland, WA, USA",2014 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks,20140922,2014,,,786,791,"In power grid operations, security is an essential component for any middleware platform. Security protects data against unwanted access as well as cyber attacks. GridOpticsTM Software System (GOSS) is an open source power grid analytics platform that facilitates ease of access between applications and data sources and promotes development of advanced analytical applications. GOSS contains an API that abstracts many of the difficulties in connecting to various heterogeneous data sources. A number of applications and data sources have already been implemented to demonstrate functionality and ease of use. A security framework has been implemented which leverages widely accepted, robust Java TM security tools in a way such that they can be interchanged as needed. This framework supports the complex fine-grained, access control rules identified for the diverse data sources already in GOSS. Performance and reliability are also important considerations in any power grid architecture. An evaluation is done to determine the overhead cost caused by security within GOSS and ensure minimal impact to performance.",1530-0889;15300889,Electronic:978-1-4799-2233-8; POD:978-1-4799-2773-9,10.1109/DSN.2014.106,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6903642,jaas;middleware;pmu;power grid;security;smartgrid,Authentication;Authorization;Organizations;Phasor measurement units;Power grids,Java;application program interfaces;authorisation;middleware;power grids;power system analysis computing;public domain software,API;GOSS power grid analytics platform;GridOptics software system;Java security tools;complex fine-grained access control rules;cyber attacks;integrated security framework;middleware platform;open source power grid analytics platform;power grid architecture;power grid operations,,0,,21,,no,23-26 June 2014,,IEEE,IEEE Conference Publications
An Online Performance Prediction Framework for Service-Oriented Systems,Y. Zhang; Z. Zheng; M. R. Lyu,"Shenzhen Key Laboratory of Rich Media Big Data Analytics and Applications, Shenzhen Research Institute, The Chinese University of Hong Kong","IEEE Transactions on Systems, Man, and Cybernetics: Systems",20140814,2014,44,9,1169,1181,"The exponential growth of Web service makes building high-quality service-oriented systems an urgent and crucial research problem. Performance of the service-oriented systems highly depends on the remote Web services as well as the unpredictability of the Internet. Performance prediction of service-oriented systems is critical for automatically selecting the optimal Web service composition. Since the performance of Web services is highly related to the service status and network environments which are variable over time, it is an important task to predict the performance of service-oriented systems at run-time. To address this critical challenge, this paper proposes an online performance prediction framework, called OPred, to provide personalized service-oriented system performance prediction efficiently. Based on the past usage experience from different users, OPred builds feature models and employs time series analysis techniques on feature trends to make performance prediction. The results of large-scale real-world experiments show the effectiveness and efficiency of OPred.",2168-2216;21682216,,10.1109/TSMC.2013.2297401,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6720144,Performance prediction;Web service;time series analysis,Market research;Prediction algorithms;Predictive models;Runtime;Time factors;Vectors;Web services,Web services;service-oriented architecture;software performance evaluation;time series,OPred;Web service;online performance prediction framework;personalized service-oriented system performance prediction;time series analysis techniques,,3,,35,,no,Sept. 2014,,IEEE,IEEE Journals & Magazines
An overview of cloud middleware services for interconnection of healthcare platforms,A. Ochian; G. Suciu; O. Fratu; C. Voicu; V. Suciu,"Res. Dept., Beia Consult Int., Bucharest, Romania",2014 10th International Conference on Communications (COMM),20140726,2014,,,1,4,"Using heterogeneous clouds has been considered to improve performance of big-data analytics for healthcare platforms. However, the problem of the delay when transferring big-data over the network needs to be addressed. The purpose of this paper is to analyze and compare existing cloud computing environments (PaaS, IaaS) in order to implement middleware services. Understanding the differences and similarities between cloud technologies will help in the interconnection of healthcare platforms. The paper provides a general overview of the techniques and interfaces for cloud computing middleware services, and proposes a cloud architecture for healthcare. Cloud middleware enables heterogeneous devices to act as data sources and to integrate data from other healthcare platforms, but specific APIs need to be developed. Furthermore, security and management problems need to be addressed, given the heterogeneous nature of the communication and computing environment. The present paper fills a gap in the electronic healthcare register literature by providing an overview of cloud computing middleware services and standardized interfaces for the integration with medical devices.",,Electronic:978-1-4799-2385-4; POD:978-1-4799-2387-8; USB:978-1-4799-5472-8,10.1109/ICComm.2014.6866753,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6866753,big data;cloud;healthcare;middleware;security;standards,Big data;Biomedical imaging;Cloud computing;Computer architecture;Context;Medical services,Big Data;application program interfaces;cloud computing;data analysis;data integration;electronic health records;health care;medical computing;middleware;security of data;software architecture;software performance evaluation,API;IaaS;PaaS;big-data analytics;cloud architecture;cloud computing middleware services;cloud technologies;data integration;data sources;delay problem;electronic healthcare register literature;healthcare platform interconnection;heterogeneous devices;management problem;medical devices;performance improvement;security problem,,4,,25,,no,29-31 May 2014,,IEEE,IEEE Conference Publications
Analysis and application of solenoid inductor,B. Xiang; H. Meng; Z. Yin; S. Hao,"Nanjing Electronic Equipment Institute, Nanjing 210007, China",Proceedings of 2014 3rd Asia-Pacific Conference on Antennas and Propagation,20141222,2014,,,1300,1302,"An accurate analytic method of the coreless solenoid inductor is presented in this paper. The coreless solenoid inductor is simulated with electromagnetic field simulation software, and its S-parameters are obtained and translated to ABCD matrix. The coreless solenoid inductor is equivalent to pi network. The pi network is expressed with ABCD matrix, and then the coreless solenoid inductor can be inverted to equivalent lumped inductor. According to this method, the coreless solenoid inductor is used in a band pass filter. The simulation results of the band pass filter are close to measurement results.",,CD-ROM:978-1-4799-4355-5; Electronic:978-1-4799-4354-8; POD:978-1-4799-4353-1,10.1109/APCAP.2014.6992759,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6992759,ABCD matrix;band pass filter;solenoid inductor,Band-pass filters;Inductance;Inductors;Integrated circuit modeling;Scattering parameters;Software;Solenoids,S-parameters;band-pass filters;inductors;matrix algebra;solenoids,ABCD matrix;S-parameters;band pass filter;coreless solenoid inductor;electromagnetic field simulation software;equivalent lumped inductor;pi network,,0,,6,,no,26-29 July 2014,,IEEE,IEEE Conference Publications
Analysis of Bayesian classification-based approaches for Android malware detection,S. Y. Yerima; S. Sezer; G. Mcwilliams,"Centre for Secure Inf. Technol. (CSIT), Queen's Univ., Belfast, UK",IET Information Security,20131219,2014,8,1,25,36,"Mobile malware has been growing in scale and complexity spurred by the unabated uptake of smartphones worldwide. Android is fast becoming the most popular mobile platform resulting in sharp increase in malware targeting the platform. Additionally, Android malware is evolving rapidly to evade detection by traditional signature-based scanning. Despite current detection measures in place, timely discovery of new malware is still a critical issue. This calls for novel approaches to mitigate the growing threat of zero-day Android malware. Hence, the authors develop and analyse proactive machine-learning approaches based on Bayesian classification aimed at uncovering unknown Android malware via static analysis. The study, which is based on a large malware sample set of majority of the existing families, demonstrates detection capabilities with high accuracy. Empirical results and comparative analysis are presented offering useful insight towards development of effective static-analytic Bayesian classification-based solutions for detecting unknown Android malware.",1751-8709;17518709,,10.1049/iet-ifs.2013.0095,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6687155,,,invasive software;learning (artificial intelligence);operating system kernels;pattern classification;smart phones,Android malware detection;machine learning;mobile malware;signature based scanning;smartphones;static analysis;static analytic Bayesian classification,,5,1,,,no,Jan. 2014,,IET,IET Journals & Magazines
Analytic model for cross-layer dependencies in VDSL2 access networks,J. A. Andersson; S. HÌ_st; D. Cederholm; M. Kihl,"Department of Electrical and Information Technology, Lund University, Sweden","2014 22nd International Conference on Software, Telecommunications and Computer Networks (SoftCOM)",20150212,2014,,,269,273,"Recent changes in user employment of Internet based services, new deployment technologies for mobile networks as well as an ongoing realisation of fixed and mobile converged networks e.g. the EU FP7 project COMBO, are significant examples of enablers for increasing demands on DSL links. Investigating cross-layer dependencies between all layers in the OSI reference model becomes increasingly important. In this paper we present an analytical model and experimental results for the relation between impulse noise on a VDSL2 link and the effect this have on the network layer packet loss. We show how the packet loss rate is dependent not only on the disturbance signal level and periodicity but also on the link utilisation.",,Electronic:978-9-5329-0052-1; POD:978-1-4799-6497-0,10.1109/SOFTCOM.2014.7039075,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7039075,,DSL;IPTV;Noise;OFDM;Packet loss;Physical layer,digital subscriber lines;mobile communication,Internet based services;OSI reference model;VDSL2 access networks;analytic model;cross-layer dependencies;link utilisation;network layer packet loss;packet loss rate,,0,,14,,no,17-19 Sept. 2014,,IEEE,IEEE Conference Publications
Analytic throughput model for TCP-NC,X. Lan; S. Li; R. Zhang; C. Han; S. Zhang,"School of Software Engineering, Southeast University Nanjing, 211189 China",2014 IEEE Wireless Communications and Networking Conference (WCNC),20141120,2014,,,2659,2664,"Network coding improves TCP's performance in lossy wireless networks. However, the complex congestion window evolution of network coded TCP (TCP-NC) makes the analysis of end-to-end throughput challenging. This paper analyzes the evolutionary process of TCP-NC against lossy links. An analytic model is established by applying a two-dimensional Markov chain. With maximum window size, end-to-end erasure rate and redundancy parameter as input parameters, the analytic model can reflect window evolution and calculate end-to-end throughput of TCP-NC precisely. The key point of our model is the novel definition for the states of Markov chain. It substantially reduces related states and much lower complexity is obtained. Our work helps understand the factors that affect TCP-NC's performance and lay the foundation of optimization. Extensive simulations on NS2 show that the analytic model features fairly high accuracy.",1525-3511;15253511,Electronic:978-1-4799-3083-8; POD:978-1-4799-3084-5; USB:978-1-4799-3082-1,10.1109/WCNC.2014.6952828,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6952828,Markov chain;Wireless mesh networks;network coding;transmission control,Mobile communication;Mobile computing;Wireless networks,computer networks;network coding;telecommunication congestion control;transport protocols;wireless mesh networks,TCP-NC;analytic throughput model;complex congestion window evolution;network coding;two dimensional Markov chain;wireless mesh networks,,0,,13,,no,6-9 April 2014,,IEEE,IEEE Conference Publications
Analytical Network Process based model to estimate the quality of software components,A. K. Pandey; C. P. Agrawal,"Dept. of Inf. Technol., KIET Group of Instn., Ghaziabad, India",2014 International Conference on Issues and Challenges in Intelligent Computing Techniques (ICICT),20140403,2014,,,678,682,"Software components are software units designed to interact with other independently developed software components. These components are assembled by third parties into software applications. The success of final software applications largely depends upon the selection of appropriate and easy to fit components in software application according to the need of customer. It is primary requirement to evaluate the quality of components before using them in the final software application system. All the quality characteristics may not be of same significance for a particular software application of a specific domain. Therefore, it is necessary to identify only those characteristics/ sub-characteristics, which may have higher importance over the others. Analytical Network Process (ANP) is used to solve the decision problem, where attributes of decision parameters form dependency networks. The objective of this paper is to propose ANP based model to prioritize the characteristics /sub-characteristics of quality and to o estimate the numeric value of software quality.",,DVD:978-1-4799-2899-6; Electronic:978-1-4799-2900-9; POD:978-1-4799-2901-6,10.1109/ICICICT.2014.6781361,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6781361,ANP;Software component;prioritization and software application;quality,Interoperability;Measurement;Software reliability;Stability analysis;Usability,analytic hierarchy process;decision theory;object-oriented programming;software quality,ANP based model;analytical network process based model;decision parameter attribution;decision problem;dependency networks;final software application system;software component quality estimation;software quality numeric value estimation;software units,,0,,17,,no,7-8 Feb. 2014,,IEEE,IEEE Conference Publications
Analyzing the Learning Process in Online Educational Game Design: A Case Study,N. Ahmadi; M. Jazayeri,"Fac. of Inf., Univ. of Lugano, Lugano, Switzerland",2014 23rd Australian Software Engineering Conference,20140605,2014,,,84,93,"Educational game design environments are used for teaching computational thinking and software engineering concepts to novices. In software engineering education, there has recently been calls for ""innovative methods for software teaching and training in online courses"" (http://2014.icse-conferences.org/seet). However, to date, learning these concepts is tied to a formal learning environment and the presence of a teacher. In line with the new educational opportunities provided by the Web such as massive open online courses (MOOCs) and e-learning 2.0 platforms, we have created an online educational game design environment with integrated learning resources including video tutorials, showcases, and communication tools. To understand the effect of online educational game design environments with integrated support for learning on novices' use of the system and their learning, we conducted a mixed-method study with nine participants. While the learning goals were achieved to a high degree, the analysis of participants' interaction with the system reveals interesting phenomena about user preferences, such as the fact that during the given computational thinking tasks, the participants preferred the synchronous communication channel to other forms of commonly provided learning resources such as forums.",1530-0803;15300803,Electronic:978-1-4799-3149-1; POD:978-1-4799-3150-7,10.1109/ASWEC.2014.34,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6824112,MOOC;computational thinking;computer science education;cyberlearning;educational game design;end-user programming;learning analytics,Communities;Education;Games;Programming profession;Software;Software engineering,Internet;computer aided instruction;computer games;computer science education;software engineering;teaching,MOOCs;communication tools;computational thinking teaching;e-learning 2.0 platforms;formal learning environment;integrated learning resources;learning process analysis;massive open online courses;online course training;online educational game design environments;showcases;software engineering concepts;software engineering education;software teaching;synchronous communication channel;video tutorials,,0,,29,,no,7-10 April 2014,,IEEE,IEEE Conference Publications
AngeLA: Putting the teacher in control of student privacy in the online classroom,A. Vozniuk; S. Govaerts; L. Bollen; S. Manske; T. Hecking; D. Gillet,"REACT, EPFL, Station 9, CH-1015 Lausanne, Switzerland",2014 Information Technology Based Higher Education and Training (ITHET),20150716,2014,,,1,4,"Learning analytics (LA) is often considered as a means to improve learning and learning environments by measuring student behaviour, analysing the tracked data and acting upon the results. The use of LA tools implies recording and processing of student activities conducted on software platforms. This paper proposes a flexible, contextual and intuitive way to provide the teacher with full control over student activity tracking in online learning environments. We call this approach AngeLA, inspired by an angel guarding over LA privacy. AngeLA mimics in a virtual space the privacy control mechanism that works well in a physical room: if a person is present in a room, she is able to observe all activities happening in the room. AngeLA serves two main purposes: (1) it increases the awareness of teachers about the activity tracking and (2) provides an intuitive way to manage the activity tracking permissions. This approach can be applied to various learning environments and social media platforms. We have implemented AngeLA in Graasp, a social platform that fosters collaborative activities.",,Electronic:978-1-4673-6730-1; POD:978-1-4673-6731-8,10.1109/ITHET.2014.7155683,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7155683,,Aerospace electronics;Context;Data privacy;Europe;Facebook;Privacy;User interfaces,computer aided instruction;data privacy;groupware;student experiments;virtualisation,AngeLA;collaborative activities;learning analytics;learning environments;online classroom;student behaviour;student privacy;virtual space,,0,,16,,no,11-13 Sept. 2014,,IEEE,IEEE Conference Publications
Animated Geo-temporal Clusters for Exploratory Search in Event Data Document Collections,P. Craig; N. R. SeÌøler; A. D. O. Cervantes,"Dept. of Comput. Sci. & Software Eng., Xi'an Jiaotong-Liverpool Univ., Suzhou, China",2014 18th International Conference on Information Visualisation,20140922,2014,,,157,163,"This paper presents a novel visual analytics technique developed to support exploratory search tasks for event data document collections. The technique supports discovery and exploration by clustering results and overlaying cluster summaries onto coordinated timeline and map views. Users can also explore and interact with search results by selecting clusters to filter and re-cluster the data with animation used to smooth the transition between views. The technique demonstrates a number of advantages over alternative methods for displaying and exploring geo-referenced search results and spatio-temporal data. Firstly, cluster summaries can be presented in a manner that makes them easy to read and scan. Listing representative events from each cluster also helps the process of discovery by preserving the diversity of results. Also, clicking on visual representations of geo-temporal clusters provides a quick and intuitive way to navigate across space and time simultaneously. This removes the need to overload users with the display of too many event labels at any one time. The technique was evaluated with a group of nineteen users and compared with an equivalent text based exploratory search engine.",1550-6037;15506037,Electronic:978-1-4799-4103-2; POD:978-1-4799-4102-5,10.1109/IV.2014.69,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6902897,human-computer information retrieval;information visualisation;visual analytics,Data visualization;Electronic publishing;Encyclopedias;History;Internet;Navigation,computer animation;data visualisation;document handling;document image processing;information retrieval;pattern clustering,animated geo-temporal clusters;animation;coordinated timeline;equivalent text based exploratory search engine;event data document collections;geo-referenced search results;map views;spatio-temporal data;visual analytics technique,,2,,56,,no,16-18 July 2014,,IEEE,IEEE Conference Publications
Application Characterization Using Oxbow Toolkit and PADS Infrastructure,S. Sreepathi; M. L. Grodowitz; R. Lim; P. Taffet; P. C. Roth; J. Meredith; S. Lee; D. Li; J. Vetter,"Oak Ridge Nat. Lab., Oak Ridge, TN, USA",2014 Hardware-Software Co-Design for High Performance Computing,20150122,2014,,,55,63,"Characterizing the behavior of a scientific application and its associated proxy application is essential for determining whether the proxy application actually does mimic the full application. To support our ongoing characterization activities, we have developed the Oxbow toolkit and an associated data store infrastructure for collecting, storing, and querying this characterization information. This paper presents recent updates to the Oxbow toolkit and introduces the Oxbow project's Performance Analytics Data Store (PADS). To demonstrate the possible insights when using the toolkit and data store, we compare the characterizations of several full and proxy applications, along with the High Performance Linpack (HPL) and High Performance Conjugate Gradient (HPCG) benchmarks. Using techniques such as cluster visualizations of PADS data across many experiments, we found that the results show unexpected similarities and differences between proxy applications, and a greater similarity of proxy applications to HPCG than to HPL along many dimensions.",,Electronic:978-1-4799-7564-8; POD:978-1-4799-7565-5,10.1109/Co-HPC.2014.11,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7017964,,Benchmark testing;Computer architecture;Data models;Databases;Measurement;Portals;Software,benchmark testing;conjugate gradient methods;parallel processing;query processing;scientific information systems,HPCG benchmark;HPL benchmark;High Performance Conjugate Gradient benchmark;High Performance Linpack benchmark;Oxbow project;Oxbow toolkit;PADS infrastructure;associated data store infrastructure;characterization information collection;characterization information querying;characterization information storage;cluster visualizations;performance analytics data store;proxy application;scientific application behavior characterization,,1,,21,,no,17-17 Nov. 2014,,IEEE,IEEE Conference Publications
Application of Hybrid Assessment Method for Priority Assessment of Functional and Non-Functional Requirements,M. Dabbagh; S. P. Lee; R. M. Parizi,"Fac. of Comput. Sci. & IT, Univ. of Malaya Kuala Lumpur, Kuala Lumpur, Malaysia",2014 International Conference on Information Science & Applications (ICISA),20140708,2014,,,1,4,"Requirements prioritization is recognized as a critical but often neglected activity during software development process. To achieve a high quality software system, both functional and non-functional requirements must be taken into consideration during the prioritization process. Although in recent past years a lot of research has been devoted to requirements prioritization problems, research on proposing approaches to consider both functional and non-functional requirements throughout the prioritization process is still limited. In this article, we propose an approach using Hybrid Assessment Method (HAM) to prioritize both functional and non-functional requirements simultaneously. The effectiveness of the proposed approach has been evaluated through an experiment with the aim of comparing the approach with the other state-of-the-art-based approach, Analytic Hierarchy Process (AHP).",2162-9048;21629048,Electronic:978-1-4799-4441-5; POD:978-1-4799-4440-8,10.1109/ICISA.2014.6847365,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6847365,,Analytic hierarchy process;Educational institutions;Genetic algorithms;Software engineering;Software systems,analytic hierarchy process;formal verification;software quality,AHP;analytic hierarchy process;high quality software system;hybrid assessment method;nonfunctional requirement;priority assessment;requirements prioritization;software development process,,2,,22,,no,6-9 May 2014,,IEEE,IEEE Conference Publications
Applying learning analytics to simplify serious games deployment in the classroom,ÌÅ. Serrano-Laguna; B. FernÌÁndez-ManjÌ_n,"Dept. of Software Eng. & Artificial Intell., Complutense Univ., Madrid, Spain",2014 IEEE Global Engineering Education Conference (EDUCON),20140605,2014,,,872,877,"In this paper we present our approach to introduce educational videogames as class exercises in face-to-face education. The main objective is to simplify teachers' task when using games by providing real-time information of the actual students' use of the games while in the classroom. The approach is based on defining the educational goals for the exercise/game precisely, designing a game that captures these goals, establishing relations between game interactions and educational goals and finally, create data capturing and visualizations of the relevant information to support the teacher. We applied this approach to a real case study, creating an educational videogame about the XML markup language that substituted the usual exercises in a Web Technologies class. This was tested with 34 computer science students with positive and promising results.",2165-9559;21659559,Electronic:978-1-4799-3191-0; POD:978-1-4799-3192-7; USB:978-1-4799-3190-3,10.1109/EDUCON.2014.6826199,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6826199,classroom exercises;game based learning;learning analytics;serious games;visualizations,Concrete;Conferences;Cultural differences;Data visualization;Engineering education;Games;XML,XML;computer aided instruction;computer science education;data visualisation;serious games (computing),Web technology class;XML markup language;class exercises;classroom;computer science students;data capturing;data visualization;educational goals;educational videogames;face-to-face education;game interactions;learning analytics;serious game deployment,,2,,10,,no,3-5 April 2014,,IEEE,IEEE Conference Publications
AR-miner: mining informative reviews for developers from mobile app marketplace,,,,,2014,,,,,"With the popularity of smartphones and mobile devices, mobile application (a.k.a. ÒappÓ) markets have been growing exponentially in terms of number of users and downloads. App developers spend considerable effort on collecting and exploiting user feedback to improve user satisfaction, but suffer from the absence of effective user review analytics tools. To facilitate mobile app developers discover the most ÒinformativeÓ user reviews from a large and rapidly increasing pool of user reviews, we present ÒAR-MinerÓ Ñ a novel computational framework for App Review Mining, which performs comprehensive analytics from raw user reviews by (i) first extracting informative user reviews by filtering noisy and irrelevant ones, (ii) then grouping the informative reviews automatically using topic modeling, (iii) further prioritizing the informative reviews by an effective review ranking scheme, (iv) and finally presenting the groups of most ÒinformativeÓ reviews via an intuitive visualization approach. We conduct extensive experiments and case studies on four popular Android apps to evaluate AR-Miner, from which the encouraging results indicate that AR-Miner is effective, efficient and promising for app developers.",,,,http://dl.acm.org/citation.cfm?id=2568263&CFID=696538919&CFTOKEN=83912867,,,,,,,,,,yes,,,,
Architectural model for next generation content management system,A. Saha; S. K. Setua,"Department of Computer Science & Engineering, University of Calcutta, Kolkata, India",8th International Conference on Electrical and Computer Engineering,20150129,2014,,,437,440,"Content Management System (CMS) is one of the recent upcoming concepts due to its efficient and explicit use in the real world applications. The demand for growth in its diversity has been documented by many researchers. Due to the limitation of traditional database management system (DBMS), which enforces limits on the type of data items need to be stored, CMS becomes the cost-effective alternative in the scope of information management. In this ever increasing data scenario we need a framework for managing and publishing all types of data with appropriate data analytics and natural language processing. Nowadays the information, especially in the Internet, consists of structured, semi-structured and unstructured data. Therefore it is unwise to think only about the use of DBMS, rather its alternative framework must be used, keeping the objectives same. The main reason for proposing a new architectural model for a content management system is that there are several important characteristics that CMSs should have to provide opportunity to specific optimizations [1]. Our ‰ÛÏNext Generation CMS model‰Ûù includes functionalities like multi-language, copyright, multichannel publishing, natural language processing, garbage collection management which are not included in the traditional ones.",,CD-ROM:978-1-4799-4167-4; Electronic:978-1-4799-4166-7; POD:978-1-4799-4165-0,10.1109/ICECE.2014.7026859,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7026859,Copyright Management;Data Analytics(DA);Fragmentation Management;Garbage Collection;Language Management;Multichannel Publishing;Natural Language Processing(NLP),Content management;Data mining;Internet;Natural language processing;Next generation networking;Security,content management;copyright;database management systems;natural language processing;software architecture;storage management,DBMS;architectural model;copyright;data analytics;data item storage;data management;data publishing;data scenario;database management system;garbage collection management;information management;multichannel publishing;multilanguage functionality;natural language processing;next-generation CMS model;next-generation content management system;real world applications;semistructured data;unstructured data,,0,,5,,no,20-22 Dec. 2014,,IEEE,IEEE Conference Publications
Architecture and capabilities of a data warehouse for ATM research,M. M. Eshow; M. Lui; S. Ranjan,"NASA Ames Research Center, Moffett Field, CA",2014 IEEE/AIAA 33rd Digital Avionics Systems Conference (DASC),20141211,2014,,,1E3-1,1E3-14,"This paper describes the design, implementation, and use of a data warehouse that supports air traffic management (ATM) research at NASA's Ames Research Center. The data warehouse, dubbed Sherlock, has been in development since 2009 and is a crucial piece of the ATM research infrastructure used by Ames and its partners. Sherlock comprises several components, including a database, a Web-based user interface, and supplementary services for query and visualization. The information stored includes raw data collected from the National Airspace System (NAS), parsed and processed data, derived data, and reports derived from pre-defined queries. The raw data include a variety of flight information from live streams of FAA operational systems, weather observations and forecasts, and NAS advisories and statistics. The modified data comprise parsed and merged data sources and metadata, enabling parameterized searches for data of interest. The derived data represent the results of research analyses deemed to be of significant interest to a wide cross-section of users. Sherlock is implemented on an Oracle 11g database, with supplemental services built on open-source packages and custom software. It contains over 20 TB of data spanning several years, and more data are added daily. It has supported several research studies, such as finding similar days in the NAS and predicting imposition of traffic flow management restrictions. Planned enhancements include integrated search across data sources and the capability for large-scale analytics.",2155-7195;21557195,Electronic:978-1-4799-5001-0; POD:978-1-4799-5000-3,10.1109/DASC.2014.6979418,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6979418,,Airports;Databases;FAA;NASA;Software;Weather forecasting,air traffic control;application program interfaces;data warehouses;merging;public domain software;query processing;software packages;user interfaces,ATM research infrastructure;FAA operational systems;NAS advisories;NAS statistics;NASA Ames Research Center;National Airspace System;Oracle 11g database;Sherlock;Web-based user interface;air traffic management research;custom software;data deriving;data parsing;data processing;data source merging;data warehouse architecture;data warehouse design;flight information;information storage;integrated search;large-scale analytics capability;live streams;metadata merging;open-source packages;parameterized data searches;raw data collection;research analyses;supplemental services;supplementary query services;supplementary visualization services;traffic flow management restrictions;weather forecasts;weather observations,,1,,38,,no,5-9 Oct. 2014,,IEEE,IEEE Conference Publications
Automating Deployment of Customized Scientific Data Analytic Environments on Clouds,C. Jin; W. Wu; H. Zhang,"State Key Lab. of Software Dev. Environ., Beihang Univ. Beijing, Beijing, China",2014 IEEE Fourth International Conference on Big Data and Cloud Computing,20150209,2014,,,41,48,"Cloud computing has become a widely used solution for efficiently provisioning computational and storage resources. Meanwhile, it is essential to provide customizable scientific data analytic platforms for researchers to conduct their personalized data intensive analysis. The integration of scientific data analytics and Cloud computing has the potential to improve resource utilization and facilitate the development of scientific researches. This paper proposes an automatic deployment framework for deploying computing environments on Clouds for every customized scientific data analytics. To achieve customization and deployment functionalities, this framework has two major components: customization service and workspace deployment service. Users are allowed to customize their personalized scientific data analytics and required Cloud resources under the customization service. A workspace language is defined in the workspace deployment service to describe the requirements of computing resources and software tools of a scientific data analytics. Workspace deployment service then adopts Chef to deploy corresponding computing environments on Clouds based on the workspace descriptions. We also implement a system based on this automatic deployment framework and present a RNA-seq analysis use case to demonstrate how this framework and its system can be used in practice.",,Electronic:978-1-4799-6719-3; POD:978-1-4799-6720-9,10.1109/BDCloud.2014.22,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7034764,Automatic deployment;Cloud computing;Customization;Scientific data analytics,Cloud computing;Communities;Data analysis;Pipelines;Software tools;XML,cloud computing;data analysis,RNA-seq analysis;automatic deployment framework;cloud computing;customization service;customized scientific data analytic environments;personalized data intensive analysis;resource utilization;software tools;workspace deployment service,,2,,22,,no,3-5 Dec. 2014,,IEEE,IEEE Conference Publications
Beginning with big data simplified,P. Bedi; V. Jindal; A. Gautam,"Department of Computer Science, University of Delhi, India",2014 International Conference on Data Mining and Intelligent Computing (ICDMIC),20141113,2014,,,1,7,"Big Data is a collection of datasets containing massive amount of data in the range of zettabytes and yottabytes. Organizations are facing difficulties in manipulating and managing this massive data as existing traditional database and software techniques are unable to process and analyze voluminous data. Dealing with Big Data requires new tools and techniques that can extract valuable information using some analytic process. Volume, Variety, Velocity, Value, Veracity, Variability and Complexity are attributes associated with Big Data in various works in the literature. In this paper, we briefly describe these existing attributes and also propose to add Viability, Cost and Consistency as new attributes to this set. This paper also discusses existing tools and techniques associated with Big Data. Fleet management is an evolving application of GPS data. It is taken as a case study in this work to illustrate various attributes of Big Data. This paper also presents the implementation of a sorting problem by varying Hadoop cluster sizes for the GPS data.",,Electronic:978-1-4799-4674-7; POD:978-1-4799-4673-0,10.1109/ICDMIC.2014.6954229,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6954229,Big Data;C's of Big Data;GPS data;Hadoop;Map Reduce;V's of Big Data,Big data;Business;Databases;Global Positioning System;Real-time systems;Sorting;Vehicles,Big Data;Global Positioning System;distributed processing;geographic information systems,GPS data;Hadoop cluster;analytic process;big data;fleet management;software techniques;traditional database;valuable information,,2,,19,,no,5-6 Sept. 2014,,IEEE,IEEE Conference Publications
Behavioral analytics for inferring large-scale orchestrated probing events,E. Bou-Harb; M. Debbabi; C. Assi,"NCFTA, Concordia Univ., Montreal, QC, Canada",2014 IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS),20140708,2014,,,506,511,"The significant dependence on cyberspace has indeed brought new risks that often compromise, exploit and damage invaluable data and systems. Thus, the capability to proactively infer malicious activities is of paramount importance. In this context, inferring probing events, which are commonly the first stage of any cyber attack, render a promising tactic to achieve that task. We have been receiving for the past three years 12 GB of daily malicious real darknet data (i.e., Internet traffic destined to half a million routable yet unallocated IP addresses) from more than 12 countries. This paper exploits such data to propose a novel approach that aims at capturing the behavior of the probing sources in an attempt to infer their orchestration (i.e., coordination) pattern. The latter defines a recently discovered characteristic of a new phenomenon of probing events that could be ominously leveraged to cause drastic Internet-wide and enterprise impacts as precursors of various cyber attacks. To accomplish its goals, the proposed approach leverages various signal and statistical techniques, information theoretical metrics, fuzzy approaches with real malware traffic and data mining methods. The approach is validated through one use case that arguably proves that a previously analyzed orchestrated probing event from last year is indeed still active, yet operating in a stealthy, very low rate mode. We envision that the proposed approach that is tailored towards darknet data, which is frequently, abundantly and effectively used to generate cyber threat intelligence, could be used by network security analysts, emergency response teams and/or observers of cyber events to infer large-scale orchestrated probing events for early cyber attack warning and notification.",,Electronic:978-1-4799-3088-3; POD:978-1-4799-3089-0,10.1109/INFCOMW.2014.6849283,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6849283,,Conferences;IP networks;Internet;Malware;Probes,IP networks;Internet;computer network security;data mining;fuzzy set theory;information theory;invasive software;statistical analysis;telecommunication traffic,Internet traffic;coordination pattern;cyber attack;cyber threat intelligence;cyberspace;data mining methods;early cyber attack notification;early cyber attack warning;emergency response teams;fuzzy approaches;information theoretical metrics;large-scale orchestrated probing events;malicious activities;malicious real darknet data;malware traffic;network security analysts;orchestration pattern;routable unallocated IP addresses;signal techniques;statistical techniques,,2,,15,,no,April 27 2014-May 2 2014,,IEEE,IEEE Conference Publications
Big data analytics for supply chain management,J. Leveling; M. Edelbrock; B. Otto,"Software Engineering, Fraunhofer-Institute for Material Flow and Logistics IML, Dortmund, Germany",2014 IEEE International Conference on Industrial Engineering and Engineering Management,20150312,2014,,,918,922,"A high number of business cases are characterized by an expanded complexity. This is based on increased collaboration between companies, customers and governmental organizations on one hand and more individual products and services on the other hand. Due to that, companies are planning to address these issues with Big Data solutions. This paper deals with Big Data solutions focusing on Supply Chains, which represents a key discipline for handling the increased collaboration next to vast amounts of exchanged data. Today, the main focus lays on optimizing Supply Chain Visibility to handle complexity and to support decision making for handling risks and interruptions along supply chains. Therefore, Big Data concepts and technologies will play a key role. This paper describes the current skituation, actual solutions and presents exemplary use-cases for illustration. A classification regarding the area of application and potential benefits arising from Big Data Analytics are also given. Furthermore, this paper outlines general technologies to show capabilities of Big Data analytics.",2157-3611;21573611,Electronic:978-1-4799-6410-9; POD:978-1-4799-6411-6; USB:978-1-4799-6409-3,10.1109/IEEM.2014.7058772,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7058772,big data;supply chain management;supply chain risk management;supply chain visibility,Big data;Companies;Data models;Databases;Supply chain management;Supply chains,Big Data;data analysis;decision making;production engineering computing;risk management;supply chain management,Big Data analytics;data handling;decision making;risk handling;supply chain management;supply chain visibility optimization,,0,,25,,no,9-12 Dec. 2014,,IEEE,IEEE Conference Publications
Big Data analytics frameworks,P. Chandarana; M. Vijayalakshmi,"V.E.S.I.T., Mumbai, India","2014 International Conference on Circuits, Systems, Communication and Information Technology Applications (CSCITA)",20140619,2014,,,430,434,"Big Data concerns massive, heterogeneous, autonomous sources with distributed and decentralized control. These characteristics make it an extreme challenge for organizations using traditional data management mechanism to store and process these huge datasets. It is required to define a new paradigm and re-evaluate current system to manage and process Big Data. In this paper, the important characteristics, issues and challenges related to Big Data management has been explored. Various open source Big Data analytics frameworks that deal with Big Data analytics workloads have been discussed. Comparative study between the given frameworks and suitability of the same has been proposed.",,Electronic:978-1-4799-2494-3; POD:978-1-4799-2495-0,10.1109/CSCITA.2014.6839299,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6839299,Apache Drill;Apache Hadoop;Big Data Analytics;Big Data Issues and Challenges;Project Storm,Big data;Computer architecture;Information technology;Organizations;Real-time systems;Storms,Big Data;data analysis;public domain software,Big Data analytics workloads;Big Data management;Big Data processing;data management mechanism;decentralized control;distributed control;open source Big Data analytics frameworks;re-evaluate current system,,8,,18,,no,4-5 April 2014,,IEEE,IEEE Conference Publications
Big Data analytics with case study on financial organization,A. Mandloi,"Early Warning Services, USA","2014 Conference on IT in Business, Industry and Government (CSIBIG)",20150312,2014,,,1,1,"Use of data and Big Data technologies is becoming a common theme to solve the problems that were otherwise seemed to require huge amount of storage and computing power. In the financial industry too, the use of Big Data is enabling solutions that are being used to detect and fight fraud. In this talk, we will focus on a couple of case studies from financial world to demonstrate the real world problems and their solutions to fight fraud.",,CD-ROM:978-1-4799-3062-3; Electronic:978-1-4799-3064-7; POD:978-1-4799-3065-4,10.1109/CSIBIG.2014.7056919,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7056919,,Abstracts;Lead;Software,Big Data;fraud,Big Data analytics;financial industry;financial organization;fraud detection,,0,,,,no,8-9 March 2014,,IEEE,IEEE Conference Publications
Big data and predictive analytics in ERP systems for automating decision making process,M. S. P. Babu; S. H. Sastry,"Department of CS & SE, Andhra University, Visakhapatnam - 530003, India",2014 IEEE 5th International Conference on Software Engineering and Service Science,20141023,2014,,,259,262,"ERP systems, at present, are found to be inflexible to adapt to changing organizational processes. They are required to quickly adjust to changing processes and value-added chains and streamline their internal organizational structure. Data in ERP systems is becoming increasingly voluminous in their transactional programs. In this scenario, ERP systems are increasingly exposed to big data wherein the combined analysis of larger amounts of structured and unstructured data from disparate systems takes place in a short amount of time. Big data analytics requires greater use of predictive analytics to uncover hidden patterns and their relationships to visualize and explore data. The evolution of big data and predictive analytics have given a new way for exploring new frontiers in analytics-driven automation and decision management in highvolume, front-line operational decisions. In this paper the authors have focused on predictive capabilities of ERP systems, to analyze current data and historical facts in order to identify potential risks and opportunities for any organization. Analytical Decision Management & Business Rules are used to deploy decision as a service.",2327-0586;23270586,CD-ROM:978-1-4799-3277-1; Electronic:978-1-4799-3279-5; POD:978-1-4799-3280-1,10.1109/ICSESS.2014.6933558,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6933558,Analytical Decision Management;Clustering;Decision Service;ERP;Forecasting;Predictive Analytics;Regression,Analytical models;Big data;Data mining;Data models;Decision making;Predictive models,Big Data;business data processing;data analysis;data visualisation;decision making;enterprise resource planning,ERP systems;analytical decision management;analytics-driven automation;big data analytics;business rules;data visualization;decision making process automation;front-line operational decisions;internal organizational structure;organizational processes;potential risk identification;predictive analytics;predictive capabilities;transactional programs;value-added chains,,0,,11,,no,27-29 June 2014,,IEEE,IEEE Conference Publications
Big Data Density Analytics Using Parallel Coordinate Visualization,J. Zhang; M. L. Huang; W. B. Wang; L. F. Lu; Z. P. Meng,"Sch. of Software, Univ. of Technol., Sydney, Sydney, NSW, Australia",2014 IEEE 17th International Conference on Computational Science and Engineering,20150129,2014,,,1115,1120,"Parallel coordinate is a popular tool for visualizing high-dimensional data and analyzing multivariate data. With the rapid growth of data size and complexity, data clutter in parallel coordinates is a major issue for Big Data visualization. This has given rise to three problems, (1) how to rearrange the parallel axes without the loss of data patterns, (2) how to shrink data attributes on each axis without the loss of data trends, (3) how to visualize the structured and unstructured data patterns for Big Data analysis. In this paper, we introduce the 5Ws dimensions as the parallel axes and establish the 5Ws sending density and receiving density as additional axes for Big Data visualization. Our model not only demonstrates Big Data attributes and patterns, but also reduces data over-lapping by up to 80 percent without the loss of data patterns. Experiments show that this new model can be efficiently used for Big Data analysis and visualization.",,Electronic:978-1-4799-7981-3; POD:978-1-4799-7982-0,10.1109/CSE.2014.219,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7023729,5Ws dimension;Big Data;parallel coordinates;shrunk attribute,Big data;Clutter;Data visualization;Educational institutions;Receivers;Software;Visualization,Big Data;data visualisation;parallel processing,5W dimensions;5W receiving density;5W sending density;Big Data attributes;Big Data density analytics;Big Data patterns;Big Data visualization;data attributes;data clutter;data complexity;data over-lapping reduction;data size;data trends;high-dimensional data visualization;multivariate data analysis;parallel axis rearrangement;parallel coordinate visualization;structured data pattern visualization;unstructured data pattern visualization,,0,,28,,no,19-21 Dec. 2014,,IEEE,IEEE Conference Publications
"Big data fueled process management of supply risks: Sensing, prediction, evaluation and mitigation",Miao He; Hao Ji; Qinhua Wang; Changrui Ren; R. Lougee,"IBM Research - China, Building 19, Zhongguancun Software Park, 8 Dongbeiwang WestRoad, Haidian District, Beijing, CHINA",Proceedings of the Winter Simulation Conference 2014,20150126,2014,,,1005,1013,"Supplier risks jeopardize on-time or complete delivery of supply in a supply chain. Traditionally, a company can merely do an ex-post evaluation of a supplier's performance, and handles emergencies in a reactive rather than a proactive way. We propose an agile process management framework to monitor and manage supply risks. The innovation is two fold - Firstly, a business process is established to make sure that the right data, the right insights, and the right decision-makers are in place at the right time. Secondly, we install a big data analytics component, a simulation component and an optimization component into the business process. The big data analytics component senses and predicts supply disruptions with internally (operational) and external (environmental) data. The simulation component supports risk evaluation to convert predicted risk severity to key performance indices (KPIs) such as cost and stockout percentage. The optimization component assists the risk-hedging decision-making.",0891-7736;08917736,Electronic:978-1-4799-7486-3; POD:978-1-4799-7487-0,10.1109/WSC.2014.7019960,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7019960,,Big data;Companies;Meteorology;Predictive models;Risk management;Supply chains,Big Data;business data processing;data analysis;decision making;digital simulation;optimisation;risk management;supply chain management;supply chains,KPIs;agile process management framework;big data analytics component;big data fueled process management;business process;environmental data;key performance indices;operational data;optimization component;risk evaluation;risk-hedging decision-making;simulation component;supply chain;supply disruption prediction;supply risk management,,0,,18,,no,7-10 Dec. 2014,,IEEE,IEEE Conference Publications
Big data implementation and visualization,D. Gupta; S. Siddiqui,"Amity Institute of Information Technology, Noida, India",2014 International Conference on Advances in Engineering & Technology Research (ICAETR - 2014),20150119,2014,,,1,10,"Government agencies and large corporations are launching research programs to address big data's challenges. Visualization in today's time is very effective for presenting essential information in vast amounts of data. Big-data discovery tools present new research opportunities to the graphics and visualization community. The size of the collected data about the Web and mobile device users is even greater. To provide the ability to make sense and maximize utilization of such vast amounts of data for knowledge discovery and decision making is crucial to scientific advancement; we need new tools beyond conventional data mining and statistical analysis. Visualization is a tool which is shown to be effective for gleaning insight in big data. Here we also discuss data cube that fits in a tablet or a smart phone memory, actually for billions of entrances; we call this information structure a nanocube. [13]. We present pseudo code to compute and query a nanocube [13], and show how it can be used to generate well-known visual encodings such as heat maps, histograms, and parallel coordinate plots. While Apache* Hadoop* and other technologies are emerging to support back-end concerns such as storage and processing, visualization-based data discovery tools focus on the front end of big data-on helping businesses explore the data more easily and understand it more fully.",2347-9337;23479337,Electronic:978-1-4799-6393-5; POD:978-1-4799-6394-2,10.1109/ICAETR.2014.7012883,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7012883,Analytics;Apache Hadoop;Visualization;data cubes,Big data;Business;Conferences;Data visualization;Real-time systems;Servers;Visualization,Big Data;data mining;data visualisation;public domain software;query processing;statistical analysis,Apache* Hadoop*;Big-Data discovery tools;data cube;data mining;decision making;government agencies;graphics community;information structure;knowledge discovery;nanocube query;pseudocode;smart phone memory;statistical analysis;tablet;visual encodings;visualization community;visualization-based data discovery tools,,0,,36,,no,1-2 Aug. 2014,,IEEE,IEEE Conference Publications
Big data in daily manufacturing operations,T. Wilschut; I. J. B. F. Adan; J. Stokkermans,"Department of Mechanical Engineering, Technical University Eindhoven, PO Box 513, 5600 MB, THE NETHERLANDS",Proceedings of the Winter Simulation Conference 2014,20150126,2014,,,2364,2375,"Big data analytics is at the brink of changing the landscape in NXP Semiconductors Back End manufacturing operations. Numerous IT tools, implemented over the last decade, collect gigabytes of data daily, though the potential value of this data still remains to be explored. In this paper, the software tool called Heads Up is presented. Heads Up intelligently scans, filters, and explores the data with use of simulation. The software provides real-time relevant information, which is of high value in daily, as well as long term, production management. The software tool has been introduced at the NXP high volume manufacturing plant GuangDong China, where it is about to shift the paradigm on manufacturing operations.",0891-7736;08917736,Electronic:978-1-4799-7486-3; POD:978-1-4799-7487-0,10.1109/WSC.2014.7020080,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7020080,,Magnetic heads;Maintenance engineering;Manufacturing;Production facilities;Software;Throughput,Big Data;data analysis;industrial plants;production engineering computing;production management;semiconductor industry;software tools,China;GuangDong;Heads Up;NXP Semiconductors back end manufacturing operations;NXP high volume manufacturing plant;big data analytics;production management;software tool,,0,,22,,no,7-10 Dec. 2014,,IEEE,IEEE Conference Publications
Big Data issues in Computational Chemistry,V. Yeguas; R. Casado,"Grupo de Mater. Av., Tecnol. Nucl. y Nano/Biotecnologia Aplic., Univ. de Burgos, Burgos, Spain",2014 International Conference on Future Internet of Things and Cloud,20141215,2014,,,389,392,"Digital data have become a torrent engulfing every area of business, science and engineering disciplines. In the age of Big Data, deriving values and insights from large amounts of data using rich analytics becomes an important differentiating capability for competitiveness, success and leadership in every field. Scientists and engineers of many different domains are increasingly clamouring for mechanisms to manage and analyse the massive quantities of information now available in order to obtain new answers and extract from it maximum value. Computational modelling and simulation is the central technology to numerous of these domains. Molecular Dynamics (MD) is a computational simulation technique that describes the physical forces and movements of interacting microscopic elements such atoms and molecules. MD has important applications in the fields of chemistry, biotechnology, pharmaceutical industry, energy, climate or materials science, among others. Advanced MD algorithms include not only Molecular Mechanics (MM), but also Quantum Mechanics (QM) approaches, raising important big data challenges still to be sorted out. MD simulations perform an iterative process generating large amounts of data in streaming. Current software technology is far from being able to manage, analyze and visualize the extremely large and complex data sets generated by important molecular processes. This paper analyzes the current big data limits in the Computational Chemistry field, especially in the MD processes. To overcome these challenging situations, this work provide guidance for future research including advances in scalable algorithms for data analysis, dynamic query technology, data models and storage strategies, parallel executions, I/O optimization, and interactive visual exploration and analysis of MD data.",,Electronic:978-1-4799-4357-9; POD:978-1-4799-4356-2,10.1109/FiCloud.2014.69,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6984225,Big Data;Computational Chemistry;Molecular Dynamics;Quantum Mechanics,Analytical models;Big data;Computational modeling;Data models;Data visualization;Real-time systems;Trajectory,Big Data;chemistry computing;data visualisation;interactive systems;molecular dynamics method;quantum theory,Big Data;I/O optimization;MD algorithms;MD simulations;MM;QM approach;computational chemistry;computational modelling;computational simulation technique;data analysis;data generation;data models;digital data;dynamic query technology;information analysis;information management;interacting microscopic elements;interactive visual analysis;interactive visual exploration;iterative process;large complex data set analysis;large complex data set management;large complex data set visualization;molecular dynamics;molecular mechanics;parallel executions;physical forces;physical movements;quantum mechanics approach;scalable algorithms;software technology;storage strategies,,0,,19,,no,27-29 Aug. 2014,,IEEE,IEEE Conference Publications
Big data machine learning and graph analytics: Current state and future challenges,H. H. Huang; H. Liu,"Department of Electrical and Computer Engineering, George Washington University",2014 IEEE International Conference on Big Data (Big Data),20150108,2014,,,16,17,"Big data machine learning and graph analytics have been widely used in industry, academia and government. Continuous advance in this area is critical to business success, scientific discovery, as well as cybersecurity. In this paper, we present some current projects and propose that next-generation computing systems for big data machine learning and graph analytics need innovative designs in both hardware and software that provide a good match between big data algorithms and the underlying computing and storage resources.",,Electronic:978-1-4799-5666-1; POD:978-1-4799-5667-8,10.1109/BigData.2014.7004471,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7004471,Big Data;Graphics Processing Unit;Hardware and Software Co-Design;Lambda Architecture;Non-Volatile Memory;Solid-State Drive,Big data;Computer architecture;Conferences;Graphics processing units;Hardware;Machine learning algorithms;Nonvolatile memory,Big Data;graph theory;learning (artificial intelligence),Big Data algorithms;Big Data machine learning;business success;computing resources;cybersecurity;graph analytics;hardware innovative designs;next-generation computing systems;scientific discovery;software innovative designs;storage resources,,1,,32,,no,27-30 Oct. 2014,,IEEE,IEEE Conference Publications
Big Data Opportunities and Challenges: Discussions from Data Analytics Perspectives [Discussion Forum],Z. H. Zhou; N. V. Chawla; Y. Jin; G. J. Williams,"National Key Laboratory for Novel Software Tech, Nanjing University, Nanjing, 210023, China",IEEE Computational Intelligence Magazine,20141013,2014,9,4,62,74,"""Big Data"" as a term has been among the biggest trends of the last three years, leading to an upsurge of research, as well as industry and government applications. Data is deemed a powerful raw material that can impact multidisciplinary research endeavors as well as government and business performance. The goal of this discussion paper is to share the data analytics opinions and perspectives of the authors relating to the new opportunities and challenges brought forth by the big data movement. The authors bring together diverse perspectives, coming from different geographical locations with different core research expertise and different affiliations and work experiences. The aim of this paper is to evoke discussion rather than to provide a comprehensive survey of big data research.",1556-603X;1556603X,,10.1109/MCI.2014.2350953,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6920114,,Big data;Content management;Database management;Government;Industries;Information analysis;Market research,Big Data;data analysis,big data movement;data analytics,,19,,63,,no,Nov. 2014,,IEEE,IEEE Journals & Magazines
Big data technologies in support of real time capturing and understanding of electric vehicle customers dynamics,R. G. Qiu; K. Wang; S. Li; J. Dong; M. Xie,"Division of Engineering, Pennsylvania State University, USA & NUAA-IBM Logistics and Service Science Lab, NUAA, Nanjing, China",2014 IEEE 5th International Conference on Software Engineering and Service Science,20141023,2014,,,263,267,"Energy overconsumption and greenhouse gas emission have been contributing to air pollutions and the global warming for years. The unceasingly increasing number of fossil fuels based vehicles around the world is considered as one of main factors making to the situation worse year by year. Electric vehicles (EV) are promoted as a viable and promising alternative transportation means for customers. However, there is an array of issues hindering EVs from the fast adoption in the global auto market. As these issues bear different priorities that surely vary with marketplaces, it becomes essential for EV makers and governments to capture and understand the dynamics of EV consumers in real time. This paper explores how the emerging big data technologies can be applied to facilitate the process of deciphering the acceptance and behavior of EV customers from marketplace to marketplace. A data-collecting web system is discussed. IBM BigInsights platform technologies, including Hadoop, Streams, SPSS modeler and text analytics, are utilized for looking into the insights of collected data. Examples are provided to show the promising future of big data technologies in the field of customer analytics in today's globalized economy.",2327-0586;23270586,CD-ROM:978-1-4799-3277-1; Electronic:978-1-4799-3279-5; POD:978-1-4799-3280-1,10.1109/ICSESS.2014.6933559,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6933559,Electric vehicles;InfoSphere BigInsights;big data technologies;customer behavior;customer dyanmics;incentives;policies,Analytical models;Batteries;Big data;Cities and towns;Government;Real-time systems;Vehicles,Big Data;consumer behaviour;data acquisition;electric vehicles;real-time systems,Big Data technologies;EV consumers dynamics;EV customers acceptance;EV customers behavior;EV makers;Hadoop;IBM BigInsights platform technologies;SPSS modeler;Streams;air pollutions;alternative transportation means;customer analytics;data-collecting Web system;electric vehicle customers dynamics;energy overconsumption;fossil fuels based vehicles;global auto market;global warming;globalized economy;governments;greenhouse gas emission;marketplaces;real time capturing;text analytic,,2,,18,,no,27-29 June 2014,,IEEE,IEEE Conference Publications
Big R: Large-Scale Analytics on Hadoop Using R,O. D. L. Yejas; W. Zhuang; A. Pannu,"IBM Silicon Valley Lab., San Jose, CA, USA",2014 IEEE International Congress on Big Data,20140925,2014,,,570,577,"As the volume of available data continues to rapidly grow from a variety of sources, scalable and performant analytics solutions have become an essential tool to enhance business productivity and revenue. Existing data analysis environments, such as R, are constrained by the size of the main memory and cannot scale in many applications. This paper introduces Big R, a new platform which enables accessing, manipulating, analyzing, and visualizing data residing on a Hadoop cluster from the R user interface. Big R is inspired by R semantics and overloads a number of R primitives to support big data. Hence, users will be able to quickly prototype big data analytics routines without the need of learning a new programming paradigm. The current Big R implementation works on two main fronts: (1) data exploration, which enables R as a query language for Hadoop and (2) partitioned execution, allowing the execution of any R function on smaller pieces of a large dataset across the nodes in the cluster.",2379-7703;23797703,Electronic:978-1-4799-5057-7; POD:978-1-4799-5058-4,10.1109/BigData.Congress.2014.88,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6906830,Big data;Machine Learning;MapReduce,Big data;Data mining;Data visualization;Database languages;Delays;Semantics;Vectors,data analysis;data visualisation;pattern clustering;public domain software;query languages;user interfaces,Big R;Hadoop cluster;R primitives;R semantics;R user interface;data analysis;data exploration;data manipulation;data visualization;large-scale analytics;partitioned execution;query language,,0,,11,,no,June 27 2014-July 2 2014,,IEEE,IEEE Conference Publications
BigData visualization: Parallel coordinates using density approach,J. Zhang; Z. Meng; M. L. Huang,"School of software, Faculty of Engineering & IT, University of Technology, Sydney Sydney, Australia",The 2014 2nd International Conference on Systems and Informatics (ICSAI 2014),20150115,2014,,,1056,1063,"Information visualization is a very important tool in BigData analytics. BigData, structured and unstructured data which contains images, videos, texts, audio and other forms of data, collected from multiple datasets, is too big, too complex and moves too fast to analyse using traditional methods. This has given rise to two issues; 1) how to reduce multidimensional data without the loss of any data patterns for multiple datasets, 2) how to visualize BigData patterns for analysis. In this paper, we have classified the BigData attributes into `5Ws' data dimensions, and then established a `5Ws' density approach that represents the characteristics of data flow patterns. We use parallel coordinates to display the `5Ws' sending and receiving densities which provide more analytic features for BigData analysis. The experiment shows that this new model with parallel coordinate visualization can be efficiently used for BigData analysis and visualization.",,CD-ROM:978-1-4799-5457-5; Electronic:978-1-4799-5458-2; POD:978-1-4799-5459-9,10.1109/ICSAI.2014.7009441,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7009441,5Ws data flow pattern;5Ws density;BigData;information visualization;parallel coordinates,Data visualization;Educational institutions;Electronic mail;Mobile communication;Receivers;Software;Videos,Big Data;data visualisation,5W data dimensions;BigData analytics;BigData visualization;data flow pattern;density approach;information visualization;multidimensional data;parallel coordinate visualization,,0,,37,,no,15-17 Nov. 2014,,IEEE,IEEE Conference Publications
Blueprint for Business Middleware as a Managed Cloud Service,P. Dettori; D. Frank; S. R. Seelam; P. Feillet,"IBM T. J. Watson Res. Cetner, Yorktown Heights, NY, USA",2014 IEEE International Conference on Cloud Engineering,20140922,2014,,,261,270,"Cloud offers numerous technical middleware services such as databases, caches, messaging systems, and storage but very few business middleware services as first tier managed services. Business middleware such as business process management, business rules, operational decision management, content management and business analytics, if deployed in a cloud environment, is typically only available in a hosted (black-box) model. This is partly due to where cloud is in its evolution, and mostly due to the relatively higher complexity of business middleware vs. technical middleware in the deployment, provisioning, usage, etc. Business middleware consists of multiple functions for business processes design and modeling, execution, optimization, monitoring, and analysis. These functions and their associated complexity have inhibited the wholesale migration of existing business middleware to the cloud. To better understand the complexity in bringing business middleware to the cloud and to develop a systematic cloud enablement approach, we studied the deployment of IBM's Operational Decision Manager (ODM) business middleware product as a managed service (Cloud Decision Service) in IBM's BlueMix cloud platform. Our study indicates that complex middleware must be componentized along functional boundaries, and provide these functions for different business users and developers with cloud experience. In addition, middleware services must leverage other cloud services and they should provide interfaces so that they can be consumed by Java applications as well as by polyglot applications (JavaScript, Ruby, Python, etc). Applications can bind to and use our Cloud Decision Service in a matter of seconds. In contrast, it takes hours to days to setup such a service in the traditional packaged software model. Based on the lessons learned from this experiment we develop a blueprint for enabling high value business middleware as managed cloud services.",,Electronic:978-1-4799-3766-0; POD:978-1-4799-3768-4,10.1109/IC2E.2014.68,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6903481,business;cloud;middleware;process;service,Business;Cloud computing;Complexity theory;Databases;Foundries,Java;business data processing;cloud computing;decision making;middleware,IBM BlueMix cloud platform;IBM operational decision manager;Java applications;ODM;black-box model;business analytics;business middleware blueprint;business middleware services;business process analysis;business process design;business process execution;business process management;business process modeling;business process monitoring;business process optimization;business rules;cloud decision service;cloud environment;cloud service;content management;functional boundaries;operational decision management;packaged software model;polyglot applications;systematic cloud enablement approach;technical middleware,,1,,32,,no,11-14 March 2014,,IEEE,IEEE Conference Publications
Building a Massive Stream Computing Platform for Flexible Applications,T. Chen; Z. Man; H. Li; X. Sun; R. K. Wong; Z. Yu,"Dept. of Basic Technol., Baidu Inc., Beijing, China",2014 IEEE International Congress on Big Data,20140925,2014,,,414,421,"Driven by the rapid growth of large scale real-time data mining applications for personalized ads and content recommendations, distributed stream processing systems are widely applied in modern big-data architectures. Designs of existing stream computing systems are mostly focusing on the scalability and availability issues. Other important issues which are essential to the actual cost and productivity, such as the fluctuating work load handling, the stream topology alternation efficiency and the computing topology overlapping, are not well studied. To address these issues in a live, production environment, a new stream processing architecture that is based on a scalability enhanced subscription model is proposed in this paper. We also present a system, called Vortex, that has been implemented using this new architecture. Vortex is a distributed stream computing system engineered to support flexible applications at Baidu. The new architecture enables Vortex to scale well for highly fluctuating workloads and perform on-demand stream topology alternations with minimal overheads. Furthermore, the dynamic message routing mechanism of Vortex allows one processing node to serve different stream topologies. This maximizes the computing resource utilization in the scenarios of topology overlapping. With all these features, Vortex is a powerful platform for both realtime data processing and Map-Reduce job acceleration. Finally, in this paper, we also discuss some applications at Baidu to demonstrate how Vortex can be deployed for various stream computing applications ranging from real-time analytics to the efficient large-scale data mining.",2379-7703;23797703,Electronic:978-1-4799-5057-7; POD:978-1-4799-5058-4,10.1109/BigData.Congress.2014.67,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6906810,Architecture;DRPC;Real World Applications;Stream Computing,Computational modeling;Computer architecture;Payloads;Routing;Scalability;Servers;Topology,Big Data;Internet;data mining;parallel programming;resource allocation;software architecture,Baidu;Internet applications;MapReduce job acceleration;Vortex;availability issue;big-data architectures;computing resource utilization maximization;content recommendations;distributed stream processing systems;dynamic message routing mechanism;flexible applications;large scale real-time data mining applications;massive stream computing platform;on-demand stream topology alternations;personalized ads;real-time data processing;scalability enhanced subscription model;topology overlapping,,0,,22,,no,June 27 2014-July 2 2014,,IEEE,IEEE Conference Publications
Building a National E-Service using Sentire experience report on the use of Sentire: A volere-based requirements framework driven by calibrated personas and simulated user feedback,C. Porter; E. Letier; M. A. Sasse,"Dept. of Computer Science, University College London, United Kingdom",2014 IEEE 22nd International Requirements Engineering Conference (RE),20140929,2014,,,374,383,"User experience (UX) is difficult to quantify and thus more challenging to require and guarantee. It is also difficult to gauge the potential impact on users' lived experience, especially at the earlier stages of the development life cycle, particularly before hi fidelity prototypes are developed. We believe that the enrolment process is a major hurdle for e-government service adoption and badly designed processes might result in negative repercussions for both the policy maker and the different user groups involved; non-adoption and resentment are two risks that may result in low return on investment (ROI), lost political goodwill and ultimately a negative lived experience for citizens. Identity assurance requirements need to balance out the real value of the assets being secured (risk) with the user groups' acceptance thresholds (based on a continuous cost-benefit exercise factoring in cognitive and physical workload). Sentire is a persona-centric requirements framework built on and extending the Volere requirements process with UX-analytics, reusable user behavioural models and simulated user feedback through calibrated personas. In this paper we present a story on how Sentire was adopted in the development of a national public-facing e-service. Daily journaling was used throughout the project and a custom built cloud-based CASE tool was used to manage the whole process. This paper outlines our experiences and lessons learnt.",1090-705X;1090705X,Electronic:978-1-4799-3033-3; POD:978-1-4799-3034-0,10.1109/RE.2014.6912288,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6912288,Personas;Requirements engineering for user experience;Sentire;Volere;enrolment process;industry and research collaboration;usability,Calibration;Computer aided software engineering;Educational institutions;Electronic government;Predictive models;Usability,cloud computing;cost-benefit analysis;government data processing,ROI;Sentire experience report;UX-analytics;calibrated personas;cloud-based CASE tool;continuous cost-benefit exercise factoring;development life cycle;e-government service adoption;identity assurance requirements;low return on investment;national public e-service;negative lived experience;persona-centric requirement framework;reusable user behavioural models;simulated user feedback;user group acceptance thresholds;user lived experience;volere-based requirement framework,,0,,21,,no,25-29 Aug. 2014,,IEEE,IEEE Conference Publications
Challenges and perspectives in an undergraduate flipped classroom experience: Looking through the lens of learning analytics,M. N. Giannakos; N. Chrisochoides,"Department of Computer and Information Science, Norwegian University of Science and Technology, Trondheim, Norway",2014 IEEE Frontiers in Education Conference (FIE) Proceedings,20150219,2014,,,1,5,"Recent technical and infrastructural developments posit flipped classroom approaches ripe for exploration. Flipped classroom approaches have students use technology to access the lecture and other instructional resources outside the classroom in order to engage them in active learning during in-class time. Scholars and educators have reported a variety of positive outcomes of a flipped (or inverted) approach to instruction. Although, flipped classroom practices have been used in a number of education studies, the detailed framework and data obtained from students' interaction with the technology materials are typically not described. In this paper, we present a flipped classroom framework and the first captured results of such data. The framework incorporates basic e-learning tools and traditional learning practices, making it accessible to anyone wanting to implement a flipped classroom experience in his/her course. The framework is structured on open-source and easy-to-use tools, allowing for the incorporation of any additional specificities of a course. This work-in-progress can provide insights for other scholars and practitioners to further validate, examine, and extend the proposed approach. This approach can be used for those interested in incorporating flipped classroom in their teaching, since it is a flexible procedure that may be adapted to meet their needs.",0190-5848;01905848,Electronic:978-1-4799-3922-0; POD:978-1-4799-3923-7; USB:978-1-4799-3921-3,10.1109/FIE.2014.7044449,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7044449,Classroom Flip;Inverted Classroom;Learning Analytics,Education;Games;Land mobile radio;Materials;Mobile handsets;Seminars,computer aided instruction;educational courses;further education;public domain software;teaching,active learning;course;e-learning tools;instructional resources;learning analytics;open-source tools;teaching;traditional learning practices;undergraduate flipped classroom experience,,1,,12,,no,22-25 Oct. 2014,,IEEE,IEEE Conference Publications
Characterization of semi-synthetic dataset for big-data semantic analysis,R. Techentin; D. Foti; S. Al-Saffar; P. Li; E. Daniel; B. Gilbert; D. Holmes,"Mayo Clinic College of Medicine, Rochester, MN, USA",2014 IEEE High Performance Extreme Computing Conference (HPEC),20150212,2014,,,1,6,"Over the past decade, the use of semantic databases has served as the basis for storing and analyzing complex, heterogeneous, and irregular data. While there are similarities with traditional relational database systems, semantic data stores provide a rich platform for conducting non-traditional analyses of data. In support of new graph analytic algorithms and specialized graph analytic hardware, we have developed a large semi-synthetic, semantically rich dataset. The construction of this dataset mimics the real-world scenario of using relational databases as the basis for semantic data construction. In order to achieve real-world variable distributions and variable dependencies, data.gov data was used as the basis for developing an approach to build arbitrarily large semi-synthetic datasets. The intent of the semi-synthetic dataset is to serve as a testbed for new semantic graph analyses and computational software/hardware platforms. The construction process and basic data characterization is described. All code related to the data collection, consolidation, and augmentation are available for distribution.",,Electronic:978-1-4799-6233-4; POD:978-1-4799-6234-1,10.1109/HPEC.2014.7040994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7040994,RDF;big data;data.gov;graph computing;semantic representation,Benchmark testing;Complexity theory;Data warehouses;Relational databases;Resource description framework;Semantics,Big Data;data analysis;relational databases;semantic Web,big-data semantic analysis;computational software-hardware platforms;data.gov data;graph analytic algorithms;relational database systems;semantic data construction;semantic databases;semantic graph analyses;semisynthetic dataset characterization;specialized graph analytic hardware,,0,,19,,no,9-11 Sept. 2014,,IEEE,IEEE Conference Publications
ChronoTwigger: A Visual Analytics Tool for Understanding Source and Test Co-evolution,B. Ens; D. Rea; R. Shpaner; H. Hemmati; J. E. Young; P. Irani,"Dept. of Comput. Sci., Univ. of Manitoba Winnipeg, Winnipeg, MB, Canada",2014 Second IEEE Working Conference on Software Visualization,20141211,2014,,,117,126,"Applying visual analytics to large software systems can help users comprehend the wealth of information produced by source repository mining. One concept of interest is the co-evolution of test code with source code, or how source and test files develop together over time. For example, understanding how the testing pace compares to the development pace can help test managers gauge the effectiveness of their testing strategy. A useful concept that has yet to be effectively incorporated into a co-evolution visualization is co-change. Co-change is a quantity that identifies correlations between software artifacts, and we propose using this to organize our visualization in order to enrich the analysis of co-evolution. In this paper, we create, implement, and study an interactive visual analytics tool that displays source and test file changes over time (co-evolution) while grouping files that change together (co-change). Our new technique improves the analyst's ability to infer information about the software development process and its relationship to testing. We discuss the development of our system and the results of a small pilot study with three participants. Our findings show that our visualization can lead to inferences that are not easily made using other techniques alone.",,Electronic:978-1-4799-6150-4; POD:978-1-4799-6151-1,10.1109/VISSOFT.2014.28,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6980223,3D visualization;Co-evolution;co-change;information visualization;mining software repositories;temporal data visualization;visual analytics,Couplings;Data mining;Data visualization;Software;Testing;Three-dimensional displays;Visual analytics,data visualisation;interactive systems;program testing;software engineering;source code (software),ChronoTwigger;analyst information inference ability improvement;co-change;co-evolution visualization;file grouping;interactive visual analytics tool;software artifacts;software development;software systems;software testing;source code co-evolution;source files;source repository mining;test code co-evolution;test files,,0,,22,,no,29-30 Sept. 2014,,IEEE,IEEE Conference Publications
Clarifying the digital content output formats for mobile learning in higher education,N. Spyropoulou; I. Karathanasis; C. Pierrakeas; A. Kameas,"Educational Content, Methodology and Technology Laboratory (e-CoMeT Lab), Hellenic Open University (HOU), Patras, Greece",2014 International Conference on Interactive Mobile Communication Technologies and Learning (IMCL2014),20150119,2014,,,139,146,"Mobile learning (m-learning) is an emerging area of distance education and there is a great interest in incorporating it in higher education, due to the mobile devices capabilities which are rapidly increasing and renders mobile devices more efficient and more attractive to students. On the other hand, the educational content plays a significant role in the process of delivering knowledge, especially in the field of distance education. The delivery of the web content for mobile devices depends on heterogeneous software and hardware environments; therefore, web content adaptation to various mobile environments is a challenge for the educational institutions. The Hellenic Open University (HOU), an educational institution which offers distance learning courses, has started to emphasize on the arising importance of m-learning with the aim to involve it in the educational process. Towards this direction, in this paper we aim to define different file formats of the digital educational content that can be reproduced by mobile devices regardless of the various operating systems. The clarification of the digital content output formats for mobile learning will contribute to the creation of a number of analytic guides with detailed technical specifications for the development of educational content for mobile devices.",,Electronic:978-1-4799-4742-3; POD:978-1-4799-4741-6; USB:978-1-4799-4743-0,10.1109/IMCTL.2014.7011121,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7011121,content adaptation;distance education;higher education;mobile learning,Educational institutions;Mobile communication;Operating systems;Smart phones,content management;educational courses;educational institutions;further education;mobile learning,HOU;Hellenic Open University;digital content output formats;digital educational content;distance education;distance learning courses;educational institutions;higher education;m-learning;mobile learning,,0,,30,,no,13-14 Nov. 2014,,IEEE,IEEE Conference Publications
Cloud Mobile Media: Reflections and Outlook,Y. Wen; X. Zhu; J. J. P. C. Rodrigues; C. W. Chen,"Sch. of Comput. Eng., Nanyang Technol. Univ., Singapore, Singapore",IEEE Transactions on Multimedia,20140513,2014,16,4,885,902,"This paper surveys the emerging paradigm of cloud mobile media. We start with two alternative perspectives for cloud mobile media networks: an end-to-end view and a layered view. Summaries of existing research in this area are organized according to the layered service framework: i) cloud resource management and control in infrastructure-as-a-service (IaaS), ii) cloud-based media services in platform-as-a-service (PaaS), and iii) novel cloud-based systems and applications in software-as-a-service (SaaS). We further substantiate our proposed design principles for cloud-based mobile media using a concrete case study: a cloud-centric media platform (CCMP) developed at Nanyang Technological University. Finally, this paper concludes with an outlook of open research problems for realizing the vision of cloud-based mobile media.",1520-9210;15209210,,10.1109/TMM.2014.2315596,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6782722,Cloud Computing;Cloud Media;Cloud-Centric Media Network and Media Analytics;Content Distribution Network;Mobile Media;Quality of Experience,Cloud computing;Logic gates;Media;Mobile communication;Mobile computing;Mobile handsets;Wireless communication,cloud computing;mobile computing;multimedia computing,CCMP;IaaS;Infrastructure-as-a-Service;Nanyang Technological University;PaaS;Platform-as-a-Service;SaaS;Software-as-a-Service;cloud centric media platform;cloud mobile media networks;cloud resource management;end-to-end view;layered service framework;layered view,,31,1,103,,no,14-Jun,,IEEE,IEEE Journals & Magazines
Cloud service recommendation based on trust measurement using ternary interval numbers,H. Ma; Z. Hu,"School of Software, Central South University, Changsha, China",2014 International Conference on Smart Computing,20150219,2014,,,21,24,"Owing to the deficiency of usage experiences and the information overload of QoE (quality of experience) evaluations from consumers, how to discover the trustworthy cloud services is a challenge for potential users. This paper proposed a cloud service recommendation approach based on trust measurement using ternary interval numbers for potential user. The concept of ternary interval number is introduced. The user feature maybe affecting the QoE evaluations are analyzed and the client-side feature similarity between consumers and potential user is calculated. The transform mechanism from trust evaluations to ternary interval number is presented by employing the K-means clustering algorithm. On the basis of multi-attributes trust aggregation based On FAHP (fuzzy analytic hierarchy process) method, a new possibility degree formula is designed for ranking ternary interval numbers and selecting trustworthy service. Finally, the experiments and results show that this approach is effective to improve the accuracy of the trustworthy service recommendation.",,Electronic:978-1-4799-5711-8; POD:978-1-4799-5712-5,10.1109/SMARTCOMP.2014.7043834,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7043834,cloud computing;potential users;service recommendation;ternary interval numbers;trust measurement,Accuracy;Analytic hierarchy process;Cloud computing;Collaboration;Quality of service;Training,analytic hierarchy process;cloud computing;fuzzy set theory;number theory;pattern clustering;quality of experience;recommender systems;trusted computing,FAHP method;K-means clustering algorithm;QoE evaluations;client-side feature similarity;cloud service recommendation approach;fuzzy analytic hierarchy process method;information overload;multiattribute trust aggregation;quality of experience;ternary interval numbers;transform mechanism;trust evaluations;trust measurement;trustworthy cloud services;trustworthy service recommendation;usage experiences,,0,,18,,no,3-5 Nov. 2014,,IEEE,IEEE Conference Publications
Cloud-Based Data Analytics Framework for Autonomic Smart Grid Management,Y. B. Qin; J. Housell; I. Rodero,"NSF Cloud & Autonomic Comput. Center, Rutgers Univ., Piscataway, NJ, USA",2014 International Conference on Cloud and Autonomic Computing,20150129,2014,,,97,100,"Global energy problems necessitate an urgent transformation of the existing electrical generation grid into a smart grid, rather than a gradual evolution. A smart grid is a real-time bi-directional communication network between end users and their utility companies which monitors power demand and manages the provisioning and transport of electricity from all generation sources. As a crucial part of this transformation, increasing numbers of smart meters generate correspondingly increasing amounts of data every day. Analyzing this data to extract insight into, and to maintain control over energy usage has become a big data problem - one which cannot be handled manually, and which requires autonomic computing solutions. In this paper, we examine electric vehicles (EVs) as a use case to investigate how to use social media, sensing data, and big data analytics to optimize smart grid management. We discuss the requirements to realize such an approach and describe an autonomic system architecture and a possible design. We believe the proposed architecture and strategy will help optimize how provisioning is performed in a smart grid, even when smart meters are not available.",,Electronic:978-1-4799-5841-2; POD:978-1-4799-5842-9,10.1109/ICCAC.2014.39,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7024050,,Autonomic systems;Cloud computing;Computer architecture;Electricity;Media;Smart grids;Smart meters,Big Data;cloud computing;data analysis;electric vehicles;power engineering computing;smart meters;smart power grids;software fault tolerance,Big Data problem;EV;autonomic smart grid management;cloud-based data analytics framework;electric vehicle;electricity generation grid;power demand monitoring;smart meter,,0,,16,,no,8-12 Sept. 2014,,IEEE,IEEE Conference Publications
CloudWave: Where adaptive cloud management meets DevOps,D. Bruneo; T. Fritz; S. Keidar-Barner; P. Leitner; F. Longo; C. Marquezan; A. Metzger; K. Pohl; A. Puliafito; D. Raz; A. Roth; E. Salant; I. Segall; M. Villari; Y. Wolfsthal; C. Woods,"Universit&#x00E0; degli Studi di Messina, Italia",2014 IEEE Symposium on Computers and Communications (ISCC),20140929,2014,Workshops,,1,6,"The transition to cloud computing offers a large number of benefits, such as lower capital costs and a highly agile environment. Yet, the development of software engineering practices has not kept pace with this change. Moreover, the design and runtime behavior of cloud based services and the underlying cloud infrastructure are largely decoupled from one another.This paper describes the innovative concepts being developed by CloudWave to utilize the principles of DevOps to create an execution analytics cloud infrastructure where, through the use of programmable monitoring and online data abstraction, much more relevant information for the optimization of the ecosystem is obtained. Required optimizations are subsequently negotiated between the applications and the cloud infrastructure to obtain coordinated adaption of the ecosystem. Additionally, the project is developing the technology for a Feedback Driven Development Standard Development Kit which will utilize the data gathered through execution analytics to supply developers with a powerful mechanism to shorten application development cycles.",1530-1346;15301346,Electronic:978-1-4799-4277-0; POD:978-1-4799-4276-3,10.1109/ISCC.2014.6912638,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6912638,Cloud Computing;Coordinated Adaptation;DevOps;Feedback Driven Development,Cloud computing;Monitoring;Real-time systems;Software reliability;Technological innovation,cloud computing;data structures;feedback;software prototyping,CloudWave;DevOps;adaptive cloud management;agile environment;application development cycle;cloud based services;cloud computing;coordinated adaption;ecosystem;execution analytics cloud infrastructure;feedback driven development standard development kit;online data abstraction;programmable monitoring;runtime behavior;software engineering practice,,5,,29,,no,23-26 June 2014,,IEEE,IEEE Conference Publications
Combining commercial consensus and community crowd-sourced categorization of web sites for integrity against phishing and other web fraud,F. Leitold; A. Arrott; F. C. C. Osorio,"Veszprog Ltd. and College of Duna&#x00FA;jv&#x00E1;ros, Hungary",2014 9th International Conference on Malicious and Unwanted Software: The Americas (MALWARE),20150115,2014,,,40,49,"Traditionally, the protection provided by 3rd party anti-Malware endpoint security products is measured using a sample set that is representative of the prevalent universe of attacks at that point in time (malicious URLs and/or malicious files in the world). The methodology used for such a selection of the Malware attack samples, the so-called Stimulus Workload (SW), has been a matter of controversy for a number of years. The reason is simple. Given a carefully crafted selection of such files or URLs, then, the results of the measurements can varied drastically favoring one vendor versus the other. In [1], Colon Osorio, et.al. argued that the selection process must be strictly regulated, and further, that such a selection must take into account the fact that amongst the samples selected, some pose a greater threat to users than others, as they are more widespread, and hence are more likely to affect a given user. Further, some Malware attack samples may only be found on specific websites, affect specific countries/regions, or only be relevant to a particular operating system version or interface languages (English, German, Chinese, and so forth). In [1], [2], the idea of a Customizable Stimulus Workloads, (CSW) was first suggested, whereas, the collection of samples selected as the Stimulus Workload is required to take into account all the elements described above. Within this context, CSWs are created by filtering attack samples base on prevalence, geographic regions, customer application environments, and other factors. Within the context of this methodology, in this manuscript we will pay special attention to one such specific application environment, primarily, Social Networks. With such a target environment in mind, a CSW was created and used to evaluate the performance of end-point security products. Basically, we examine the protection provided against Malware that uses internet Social Networks as part of the attack vector. When Social Network CSWs are used,- together with differential metrics of effectiveness, we found that amongst the Social Networks studied (Facebook, Google+, and Twitter) the amount of inherent protection provided ranged from negligible to a level that we will call modest self-protection (0% to 18% prevention rate). Further, results of our evaluation showed that the supplemental protection provided by 3rd party anti-Malware products was erratic, ranging from a low of 0% to a high of 93% depending on the product and/or Social Network combination.",,DVD:978-1-4799-7327-9; Electronic:978-1-4799-7329-3; POD:978-1-4799-7330-9,10.1109/MALWARE.2014.6999407,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6999407,,Electronic mail;Facebook;Internet;Malware;Media;Uniform resource locators,computer crime;fraud;invasive software;social networking (online),Facebook;Google;Twitter;Web fraud;Web sites;antimalware endpoint security product;commercial consensus;community crowd-sourced categorization;customizable stimulus workload;end-point security product;malicious URL;malicious files;phishing;social network,,0,,3,,no,28-30 Oct. 2014,,IEEE,IEEE Conference Publications
Comparative Evaluation of Registration Algorithms in Different Brain Databases With Varying Difficulty: Results and Insights,Y. Ou; H. Akbari; M. Bilello; X. Da; C. Davatzikos,"Center for Biomedical Image Computing and Analytics (CBICA), Department of Radiology, University of Pennsylvania, Philadelphia",IEEE Transactions on Medical Imaging,20140929,2014,33,10,2039,2065,"Evaluating various algorithms for the inter-subject registration of brain magnetic resonance images (MRI) is a necessary topic receiving growing attention. Existing studies evaluated image registration algorithms in specific tasks or using specific databases (e.g., only for skull-stripped images, only for single-site images, etc.). Consequently, the choice of registration algorithms seems task- and usage/parameter-dependent. Nevertheless, recent large-scale, often multi-institutional imaging-related studies create the need and raise the question whether some registration algorithms can 1) generally apply to various tasks/databases posing various challenges; 2) perform consistently well, and while doing so, 3) require minimal or ideally no parameter tuning. In seeking answers to this question, we evaluated 12 general-purpose registration algorithms, for their generality, accuracy and robustness. We fixed their parameters at values suggested by algorithm developers as reported in the literature. We tested them in 7 databases/tasks, which present one or more of 4 commonly-encountered challenges: 1) inter-subject anatomical variability in skull-stripped images; 2) intensity homogeneity, noise and large structural differences in raw images; 3) imaging protocol and field-of-view (FOV) differences in multi-site data; and 4) missing correspondences in pathology-bearing images. Totally 7,562 registrations were performed. Registration accuracies were measured by (multi-)expert-annotated landmarks or regions of interest (ROIs). To ensure reproducibility, we used public software tools, public databases (whenever possible), and we fully disclose the parameter settings. We show evaluation results, and discuss the performances in light of algorithms' similarity metrics, transformation models and optimization strategies. We also discuss future directions for the algorithm development and evaluations.",0278-0062;02780062,,10.1109/TMI.2014.2330355,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6834815,Brain magnetic resonance imaging (MRI);deformable image registration;evaluation;registration accuracy,Accuracy;Algorithm design and analysis;Databases;Magnetic resonance imaging;Protocols;Tuning,biomedical MRI;brain;image registration;medical image processing;optimisation,FOV;MRI;ROI;algorithm development;algorithm evaluations;algorithm similarity metrics;brain databases;brain magnetic resonance images;comparative evaluation;expert-annotated landmarks;field-of-view differences;general-purpose registration algorithms;image registration algorithms;imaging protocol;intensity homogeneity;intersubject anatomical variability;intersubject registration;large structural differences;multiinstitutional imaging-related studies;multisite data;noise;optimization strategies;parameter settings;pathology-bearing images;public databases;public software tools;raw images;regions of interest;registration accuracies;single-site images;skull-stripped images;task-dependent;transformation models;usage/parameter-dependent,0,6,,129,,no,Oct. 2014,,IEEE,IEEE Journals & Magazines
Compiling text analytics queries to FPGAs,R. Polig; K. Atasu; H. Giefers; L. Chiticariu,"IBM Research - Zurich, Rueschlikon, Switzerland",2014 24th International Conference on Field Programmable Logic and Applications (FPL),20141020,2014,,,1,6,"Extracting information from unstructured text data is a compute-intensive task. The performance of general-purpose processors cannot keep up with the rapid growth of textual data. Therefore we discuss the use of FPGAs to perform large scale text analytics. We present a framework consisting of a compiler and an operator library capable of generating a Verilog processing pipeline from a text analytics query specified in the annotation query language AQL. The operator library comprises a set of configurable modules capable of performing relational and extraction tasks which can be assembled by the compiler to represent a full annotation operator graph. Leveraging the nature of text processing we show that most tasks can be performed in an efficient streaming fashion. We evaluate the performance, power consumption and hardware utilization of our approach for a set of different queries compiled to a Stratix IV FPGA. Measurements show an up to 79 times improvement of document-throughput over a 64 threaded software implementation on a POWER7 server. Moreover the accelerated system's energy efficiency is up to 85 times better.",1946-147X;1946147X,Electronic:978-3-00-044645-0; POD:978-1-4799-3362-4; USB:978-3-00-044909-3,10.1109/FPL.2014.6927500,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6927500,,Acceleration;Database languages;Field programmable gate arrays;Hardware;Hardware design languages;Libraries;Software,field programmable gate arrays;program compilers;query languages;query processing;text analysis,AQL annotation query language;FPGA;POWER7 server;Stratix IV FPGA;Verilog processing pipeline;compiler;configurable modules;extraction task;full annotation operator graph representation;general-purpose processors;information extraction;large scale text analytics;operator library;relational task;text analytics query compilation;text processing,,3,,17,,no,2-4 Sept. 2014,,IEEE,IEEE Conference Publications
COnCEPT developing intelligent information systems to support colloborative working across design teams,A. Liapis; J. Kantorovitch; J. Malins; A. Zafeiropoulos; M. Haesen; M. G. Lopez; M. Funk; J. Alcantara; J. P. Moore; F. Maciver,"Intrasoft International, Markopoulou-Peania Avenue, Athens, Greece",2014 9th International Conference on Software Engineering and Applications (ICSOFT-EA),20151008,2014,,,170,175,"Rapid developments in hardware and software are creating opportunities to enhance the user experience. For example, advances in social analytics can provide near instant feedback. State of the art information extraction tools, filtering, categorization and presentation mechanisms all greatly facilitate knowledge exploitation activities. However, these technologies are not yet fully integrated into modern business systems. This paper describes research being undertaken in order to develop a new collaborative creative design platform (COnCEPT) aimed at investigating of new data-mining and collaboration technologies in order to enhance the information systems of future businesses. This paper describes the software architecture and the components, together with the design principles which underpin the design of the new COnCEPT platform, which is being developed to address the needs of professional design teams working collaboratively in a professional context.",,Electronic:978-9-8975-8124-3; POD:978-1-4799-7691-1,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7293855,Architecture;Business Information Systems;Collaboration;Context-Awareness;Design Principles,Collaboration;Context;Information systems;Knowledge engineering;Knowledge management;Team working,,,,,,19,,no,29-31 Aug. 2014,,IEEE,IEEE Conference Publications
CrashLocator: locating crashing faults based on crash stacks,,,,,2014,,,,,"Software crash is common. When a crash occurs, software developers can receive a report upon user permission. A crash report typically includes a call stack at the time of crash. An important step of debugging a crash is to identify faulty functions, which is often a tedious and labor-intensive task. In this paper, we propose CrashLocator, a method to locate faulty functions using the crash stack information in crash reports. It deduces possible crash traces (the failing execution traces that lead to crash) by expanding the crash stack with functions in static call graph. It then calculates the suspiciousness of each function in the approximate crash traces. The functions are then ranked by their suspiciousness scores and are recommended to developers for further investigation. We evaluate our approach using real-world Mozilla crash data. The results show that our approach is effective: we can locate 50.6%, 63.7% and 67.5% of crashing faults by examining top 1, 5 and 10 functions recommended by CrashLocator, respectively. Our approach outperforms the conventional stack-only methods significantly.",,,,http://dl.acm.org/citation.cfm?id=2610386&CFID=696538919&CFTOKEN=83912867,,,,,,,,,,yes,,,,
Customizing Scientific Data Analytic Platforms via SaaS Approach,W. Wu; C. Jin,"State Key Lab. of Software Dev. Environ., Beihang Univ., Beijing, China",2014 IEEE 8th International Symposium on Service Oriented System Engineering,20140612,2014,,,358,364,"At the era of data driven science discovery, it is essential to provide customizable scientific data analytic platforms for researchers to conduct their personalized data intensive analysis. Science Gateway has been a viable solution to enabling scientists to run scientific simulations, data analysis, and visualization through their web browsers. But most science gateway frameworks are designed for integrating commonly used software tools and datasets in a specific science domain, thus requiring significant effort to implement the essential variability in lab-specific data processing workflows. In this paper we introduce a multitenancy architecture (MTA) based customization framework that can greatly accelerate the customization cycle of science gateway systems. Each tenant has his own workspace that assembles the software stack and tools to meet the software requirements of his specific data analytics tasks. Through this framework, developers can import their domain-specific analysis pipeline scripts and mashup relevant templates including GUI templates, tool recipes and workspace templates to generate both workspace and web interface for running these application workflows and visualizing the output from workflow executions without writing extra wrapping code.",,Electronic:978-1-4799-3616-8; POD:978-1-4799-3617-5,10.1109/SOSE.2014.73,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6830930,Cloud;SaaS;Science Gateway;Workflow,Bioinformatics;Communities;DNA;Data analysis;Logic gates;Pipelines;Software,cloud computing;data analysis;data visualisation;software architecture,MTA;SaaS approach;Science Gateway;data intensive analysis;data visualization;domain-specific analysis pipeline scripts;multitenancy architecture;scientific data analytic platform customization;software requirements;workflow executions,,0,,17,,no,7-11 April 2014,,IEEE,IEEE Conference Publications
Data Fusion as an Enterprise Service,P. M. Widener,"Sandia Nat. Labs., Albuquerque, NM, USA",2014 IEEE International Conference on Services Computing,20141020,2014,,,838,839,"We present our work-in-progress on making an analytic data fusion service, originally developed and deployed to support high-performance computing and cloud-based applications, available in a enterprise service-oriented architecture (SOA) environment. We posit that not only can SOA-based integration of research software be useful in enterprise business use cases, but also that providing ways to integrate domain-specific and enterprise data is beneficial. We describe use cases driving our work, our data fusion service and its analytic capabilities, and our integration efforts using an open-source enterprise service bus framework.",,Electronic:978-1-4799-5066-9; POD:978-1-4799-5067-6,10.1109/SCC.2014.114,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6930616,ESB;data fusion,Data integration;Indexes;Laboratories;Production;Service-oriented architecture,business data processing;cloud computing;data analysis;parallel processing;public domain software;sensor fusion;service-oriented architecture;work in progress,SOA-based integration;analytic data fusion service;cloud-based applications;domain-specific data;enterprise business;enterprise data;enterprise service;enterprise service-oriented architecture environment;high-performance computing;open-source enterprise service bus framework;research software,,0,,3,,no,June 27 2014-July 2 2014,,IEEE,IEEE Conference Publications
Data Interlocking: Coupling Analytics to the Data,Y. Kowsar; H. Dashnow; A. Lonie,"Victorian Life Sci. Comput. Initiative (VLSCI), Univ. of Melbourne, Melbourne, VIC, Australia",2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing,20150202,2014,,,696,701,"'Big data' analytics can be defined by the requirement for flexible, high throughput computational analysis methods applied to large, heterogeneous datasets. We propose an architectural approach to 'big data' challenges in which the movement of data is minimized, and analysis methods are implemented on the data as portable services. We term this approach 'data interlocking'. We demonstrate the feasibility of this approach through a domain specific implementation of a data interlocking architecture, in which an on-demand computational workbench provides portable high-throughput analysis methods to large genomic datasets on cloud infrastructure.",,Electronic:978-1-4799-7881-6; POD:978-1-4799-7882-3,10.1109/UCC.2014.113,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7027580,Software-as-a-Service;big data;cloud;data-intensive;genomics;utility computing,Big data;Bioinformatics;Cloud computing;Computer architecture;Data analysis;Genomics,Big Data;cloud computing;data analysis,Big Data analytics;cloud infrastructure;computational analysis methods;data interlocking approach;data movement;genomic dataset;high-throughput analysis methods;on-demand computational workbench,,0,,35,,no,8-11 Dec. 2014,,IEEE,IEEE Conference Publications
"Data, Data Everywhere...",F. Shull,Carnegie Mellon University,IEEE Software,20140915,2014,31,5,4,7,Editor-in-chief Forrest Shull talks about the practical application of software analytics.,0740-7459;07407459,,10.1109/MS.2014.110,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6898721,software analytics;software engineering;software management;software measurement,,,,,0,,4,,no,Sept.-Oct. 2014,,IEEE,IEEE Journals & Magazines
Decision-Making about Software Release Time Using Analytic Hierarchy Process,T. Arao; Y. Machida; K. Toda; R. Yaegashi; T. Takagi,"Grad. Sch. of Eng., Kagawa Univ., Kagawa, Japan",2014 IIAI 3rd International Conference on Advanced Applied Informatics,20141201,2014,,,751,756,"This paper shows a systematic method to decide software release time using AHP (analytic hierarchy process). The method helps a project team to make a correct decision about the software release time by combining multiple evaluations obtained from different viewpoints. In general, it is difficult to obtain quantitative evaluations of all the aspects to be considered in a software development project, due to the lack of methodologies, manpower, and information. Therefore, qualitative evaluation based on the intuition and experience of specialists in software development, software sales activities, management, etc. is incorporated into the decision-making process.",,CD-ROM:978-1-4799-4175-9; Electronic:978-1-4799-4173-5; POD:978-1-4799-1679-5,10.1109/IIAI-AAI.2014.152,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6913396,analytic hierarchy process;decision-making;software release time,Analytic hierarchy process;Computer bugs;Delays;Software;Software reliability;Testing,analytic hierarchy process;decision making;project management;software engineering,AHP;analytic hierarchy process;decision-making process;project team;quantitative evaluations;software development project team;software release time;software sales activities;software sales management,,0,,9,,no,Aug. 31 2014-Sept. 4 2014,,IEEE,IEEE Conference Publications
Decisively: Application of Quantitative Analysis and Decision Science in Agile Requirements Engineering,S. K. Saxena; R. Chakraborty,"GrayPE Systems (P) Limited, NOIDA, India",2014 IEEE 22nd International Requirements Engineering Conference (RE),20140929,2014,,,323,324,"While many mature Requirements Engineering (RE) tools for Agile exist, RE professionals at large have not been able to benefit from Quantitative Analysis and Decision Science (QUADS) techniques in this context. In this paper we present an Agile RE tool, Decisively, which brings a new perspective to automation in the RE process through application of QUADS to address Requirement Discovery, Analysis, Estimation and Prioritization. Techniques explored in Decisively include Analytical Hierarchical Process (AHP) for prioritization and estimation, Lorenz function to shortlist user stories by analyzing the distribution of votes, Box Plot Analysis to predict velocity, and Text Mining to discover implied requirements from documents.",1090-705X;1090705X,Electronic:978-1-4799-3033-3; POD:978-1-4799-3034-0,10.1109/RE.2014.6912278,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6912278,AHP;Agile;Box Plot;Quantitative Analysis & Decision Science;Requirements Prioritization;SPAN;Story Points Estimation;Text Mining;Velocity Prediction,Context;Decision making;Educational institutions;Estimation;Real-time systems;Statistical analysis;Text mining,analytic hierarchy process;formal specification;software prototyping;software tools,Agile RE tool;Agile requirements engineering;Decisively;Lorenz function;analytical hierarchical process;automation;box plot analysis;quantitative analysis and decision science techniques;requirement analysis;requirement discovery;requirement estimation;requirement prioritization;text mining;user story shortlisting;velocity prediction;vote distribution analysis,,0,,6,,no,25-29 Aug. 2014,,IEEE,IEEE Conference Publications
Design and Implementation of Learning Analytics System for Teachers and Learners Based on the Specified LMS,Q. Zhou; X. Han; J. Yang; J. Cheng,"Inst. of Educ., Tsinghua Univ., Beijing, China",2014 International Conference of Educational Innovation through Technology,20141215,2014,,,79,82,"Learning analytics can give powers to teachers and learners to optimize their teaching and learning. This paper presents the design and implementation of learning analytics system for teachers and students based on a special LMS -- THEOL LMS, which is commonly used in blended learning in universities and colleges. The technical architecture of learning analytics system for teachers and students is different from for managers and researchers because of the difference of the requirements. For helping teachers and learners improve their teaching and learning, the primary aim of the THEOL learning analytics system includes: can collect data from THEOL LMS and show analysis results in THEOL LMS, supports all the teachers and learners in the THEOL LMS system, has the flexibility to do individual data analysis for expert users, can give the analysis result online and on time.",,Electronic:978-1-4799-4230-5; POD:978-1-4799-4229-9,10.1109/EITT.2014.21,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6982567,Design and Implementation;Education Data Mining;Learning Anaylitics;Learning Management System,Analytical models;Data mining;Data visualization;Data warehouses;Educational institutions;Least squares approximations,data acquisition;data analysis;educational institutions;learning management systems;software architecture;teaching,THEOL LMS system;THEOL learning analytics system;blended learning;colleges;data analysis;data collection;learning management system;requirements;teaching;technical architecture;universities,,1,,14,,no,27-29 Oct. 2014,,IEEE,IEEE Conference Publications
"Designing a new infectious healthcare-waste management system in sfax governorate, tunisia",D. Baati; M. Mellouli; W. Hachicha,"Unit of Logistic, Ind. & Quality Manage., Higher Inst. of Ind. Manage. Sfax, Sfax, Tunisia",2014 International Conference on Advanced Logistics and Transport (ICALT),20140726,2014,,,350,355,"Infectious Healthcare-Waste (IHW) management is of great importance due to its potential environmental hazards and public health risks. The collection and the disposal of IHW are highly visible and important services that involve large expenditures in the world and particularly in Tunisia. This paper discusses two main problems in the current solid IHW management practices in Sfax governorate (Tunisia). The first problem is about how choose the best waste treatment and disposal scenario, using a multiple criteria decision making analysis. The Analytic hierarchy process (AHP) implemented with Expert choice software, confirms that the best scenario is to apply central steam sterilization equipment, which will be used by all hospitals. Accordingly, the second problem concerns the off-site transport of IHW from the twelve hospitals (public and private) in the governorate of Sfax to this overall center. This problem of transportation is modeled as a capacitated vehicle routing problem (CVRP). Experimental results are reported for the design of the routing system for IHW transportation, using the solver CPLEX 9.0 software. Future legislation concerning IHW management is supposed to take into account the various results of this research.",,Electronic:978-1-4799-4839-0; POD:978-1-4799-4838-3,10.1109/ICAdLT.2014.6866337,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6866337,Analytic hierarchy process;Infectious healthcare waste;Routing system design;Sfax governorate;Tunisia;disposal scenarios;off-site transport;vehicle routing problem,Containers;Hospitals;Routing;Vehicles;Waste management,analytic hierarchy process;environmental science computing;health hazards;hospitals;operations research;sterilisation (microbiological);vehicle routing;waste disposal,AHP;CVRP;Expert choice software;IHW off-site transport;IHW transportation routing system;Sfax governorate;Tunisia;analytic hierarchy process;capacitated vehicle routing problem;central steam sterilization equipment;environmental hazards;hospitals;infectious healthcare-waste management system;multiple criteria decision making analysis;public health risks;solid IHW management practices;solver CPLEX 9.0 software;waste disposal;waste treatment,,0,,20,,no,1-3 May 2014,,IEEE,IEEE Conference Publications
Designing in-wheel switched reluctance motor for electric vehicles,M. Yildirim; M. Polat; E. Ì_ksÌ_ztepe; Z. OmaÌ_; O. Yakut; H. Eren; M. Kaya; H. KÌ_rÌ_m,"Dept. of Electrical and Electronic Eng., University of Firat, Elazi&#x011F;, Turkey",2014 16th International Power Electronics and Motion Control Conference and Exposition,20141211,2014,,,793,798,"Estimation of dimension parameters for an electrical machine has great importance before manufacturing. For this reason, analytical design should be performed in an optimum form. While motor analysis is accomplished by package programs, initial size parameters are intutivily provided and then various trials are examined to get optimum results. In this study, we are trying to find dimensional and electrical parameters generating mathematical equations in analytic approaches for In-Wheel Switched Reluctance Motor (IW-SRM), which will be employed by Electric Vehicle (EV). Therefore, optimum motor parameters for required speed and torque have been estimated by solving generated equations for in-wheel SRM with 18/12 poles via MATLAB. Using the parameters, analysis of in-wheel SRM has been carried out 3D Finite Element Method (FEM) by Ansoft Maxwell 15.0 Package Software. Consequently, the accuracy of the estimated parameters has been validated by the results of Maxwell 3D FEM.",,Electronic:978-1-4799-2060-0; POD:978-1-4799-2061-7; USB:978-1-4799-2062-4,10.1109/EPEPEMC.2014.6980594,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6980594,Designing In-wheel SRM;Electric Vehicle;Optimum Parameters Estimation,Copper;Inductance;Induction motors;Reluctance motors;Rotors;Stators;Torque,electric machine analysis computing;electric vehicles;finite element analysis;reluctance motors,3D finite element method;Ansoft Maxwell 15.0 package software;EV;IW-SRM;Maxwell 3D FEM;electric vehicle;electric vehicles;inwheel switched reluctance motor,,0,,13,,no,21-24 Sept. 2014,,IEEE,IEEE Conference Publications
Detecting Industrial Control Malware Using Automated PLC Code Analytics,S. Zonouz; J. Rrushi; S. McLaughlin,Rutgers University,IEEE Security & Privacy,20150112,2014,12,6,40,47,"The authors discuss their research on programmable logic controller (PLC) code analytics, which leverages safety engineering to detect and characterize PLC infections that target physical destruction of power plants. Their approach also draws on control theory, namely the field of engineering and mathematics that deals with the behavior of dynamical systems, to reverse-engineer safety-critical code to identify complex and highly dynamic safety properties for use in the hybrid code analytics approach.",1540-7993;15407993,,10.1109/MSP.2014.113,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7006408,PLC code analytics;formal models;industrial control malware;model checking;process control systems;reverse engineering;safety-critical code;security,Computer security;Control systems;Energy management;Industrial control;Malware;Model checking;Process control;Reverse engineering;Safety;Safety devices,control engineering computing;industrial control;invasive software;production engineering computing;program diagnostics;programmable controllers;safety-critical software,automated PLC code analytics;control theory;hybrid code analytics approach;industrial control malware detection;programmable logic controllers;reverse-engineer safety-critical code;safety engineering,,4,,11,,no,Nov.-Dec. 2014,,IEEE,IEEE Journals & Magazines
Development of Tools for Data Analysis of Earthquakes,E. Luksys; E. Asimakopoulou; N. Bessis,"Sch. of Comput. & Math., Univ. of Derby, Derby, UK",2014 International Conference on Intelligent Networking and Collaborative Systems,20150312,2014,,,406,410,"In this paper, we focus on proposing tools for enabling scientists analyze and interpret large-scale datasets about earthquakes in a way which will complement current analytical tools and thinking thus, complement current effort on understanding the event itself. Apart from briefing current trends on analytical techniques we focus on illustrating the architectures of our proposed tools in order to familiarize readers of how things are structured and the way they work. We also describe the implementation of the produced tools as well as we test and thus, demonstrate their functionality and operational order through the development of YouTube help videos and use of relevant use case scenarios.",,Electronic:978-1-4799-6387-4; POD:978-1-4799-6388-1,10.1109/INCoS.2014.24,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7057123,Big data;Data analytics;Disaster scenarios;Earthquake data analysis,Big data;Data analysis;Data visualization;Disaster management;Earthquakes;Software;Videos,data analysis;earthquakes;geophysics computing;social networking (online);software architecture,YouTube;data analysis;earthquakes;software architecture;use case scenarios,,1,,15,,no,10-12 Sept. 2014,,IEEE,IEEE Conference Publications
Discovering cross-organizational business rules from the cloud,M. L. Bernardi; M. Cimitile; F. M. Maggi,"University of Sannio, Benevento, Italy",2014 IEEE Symposium on Computational Intelligence and Data Mining (CIDM),20150115,2014,,,389,396,"Cloud computing is rapidly emerging as a new information technology that aims at providing improved efficiency in the private and public sectors, as well as promoting growth, competition, and business dynamism. Cloud computing represents, today, an opportunity also from the perspective of business process analytics since data recorded by process-centered cloud systems can be used to extract information about the underlying processes. Cloud computing architectures can be used in cross-organizational environments in which different organizations execute the same process in different variants and share information about how each variant is executed. If the process is characterized by low predictability and high variability, business rules become the best way to represent the process variants. The contribution of this paper consists in providing: (i) a cloud computing multi-tenancy architecture to support cross-organizational process executions; (ii) an approach for the systematic extraction/composition of distributed data into coherent event logs carrying process-related information of each variant; (iii) the integration of online process mining techniques for the runtime extraction of business rules from event logs representing the process variants running on the infrastructure. The proposed architecture has been implemented and applied for the execution of a real-life process for acknowledging an unborn child performed in four different Dutch municipalities.",,Electronic:978-1-4799-4518-4; POD:978-1-4799-4517-7,10.1109/CIDM.2014.7008694,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7008694,,Cloud computing;Computer architecture;Monitoring;Organizations;Software as a service,business data processing;cloud computing;data mining,business rules;cloud computing multitenancy architecture;cross-organizational business rules;cross-organizational process executions;event logs;online process mining techniques;process-centered cloud systems;runtime extraction;systematic extraction,,0,,22,,no,9-12 Dec. 2014,,IEEE,IEEE Conference Publications
DIVE: A Graph-Based Visual-Analytics Framework for Big Data,S. J. Rysavy; D. Bromley; V. Daggett,University of Washington,IEEE Computer Graphics and Applications,20140324,2014,34,2,26,37,"The need for data-centric scientific tools is growing; domains such as biology, chemistry, and physics are increasingly adopting computational approaches. So, scientists must deal with the challenges of big data. To address these challenges, researchers built a visual-analytics platform named DIVE (Data Intensive Visualization Engine). DIVE is a data-agnostic, ontologically expressive software framework that can stream large datasets at interactive speeds. In particular, DIVE makes novel contributions to structured-data-model manipulation and high-throughput streaming of large, structured datasets.",0272-1716;02721716,,10.1109/MCG.2014.27,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6777449,DIVE;Data Intensive Visualization Engine;Dynameomics;big data;bioinformatics;computer graphics;molecular dynamics;ontology;visual analytics,Big data;Data visualization;Interoperability;Ontologies;Visual analytics,Big Data;data analysis;data structures;data visualisation;graph theory;ontologies (artificial intelligence),Big Data;DIVE;computational approach;data intensive visualization engine;data-agnostic ontologically expressive software framework;data-centric scientific tools;graph-based visual-analytics framework;high-throughput streaming;structured datasets;structured-data-model manipulation;visual-analytics platform,0,7,,12,,no,Mar.-Apr. 2014,,IEEE,IEEE Journals & Magazines
Do Rapid Releases Affect Bug Reopening? A Case Study of Firefox,R. Souza; C. Chavez; R. A. Bittencourt,"Office of Inf. Technol., Fed. Univ. of Bahia, Salvador, Brazil",2014 Brazilian Symposium on Software Engineering,20141103,2014,,,31,40,"Large software organizations have been adopting rapid release cycles to deliver features and bug fixes earlier to their users. Because this approach reduces time for testing, it raises concerns about the effectiveness of quality assurance in this setting. In this paper, we study how the adoption of rapid release cycles impacts bug reopening rate, an indicator for the quality of the bug fixing process. To this end, we analyze thousands of bug reports from Mozilla Firefox, both before and after their adoption of rapid releases. Results suggest that the bug reopening rate of versions developed in rapid cycles was about 7% higher. Also, as a warning to the software analytics community, we report contradictory results from three attempts to answer our research question, performed with varying degrees of knowledge about the Firefox release process.",,Electronic:978-1-4799-4223-7; POD:978-1-4799-4221-3,10.1109/SBES.2014.10,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6943480,,Computer bugs;Databases;Educational institutions;Electronic mail;Internet;Software;Testing,online front-ends;program debugging;program testing;software quality,Mozilla Firefox;bug fixing process;bug reopening rate;bug reports;quality assurance;rapid release cycles;software analytics community,,1,,17,,no,Sept. 28 2014-Oct. 3 2014,,IEEE,IEEE Conference Publications
DocBot: A novel clinical decision support algorithm,A. Q. Ninh,"Arizona State University, Phoenix, AZ 85004 USA",2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society,20141106,2014,,,6290,6293,"DocBot is a web-based clinical decision support system (CDSS) that uses patient interaction and electronic health record analytics to assist medical practitioners with decision making. It consists of two distinct HTML interfaces: a preclinical form wherein a patient inputs symptomatic and demographic information, and an interface wherein a medical practitioner views patient information and analysis. DocBot comprises an improved software architecture that uses patient information, electronic health records, and etiologically relevant binary decision questions (stored in a knowledgebase) to provide medical practitioners with information including, but not limited to medical assessments, treatment plans, and specialist referrals.",1094-687X;1094687X,Electronic:978-1-4244-7929-0; POD:978-1-4244-7927-6,10.1109/EMBC.2014.6945067,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6945067,,Arrays;Databases;Decision support systems;Diseases;Medical diagnostic imaging,Internet;decision making;decision support systems;electronic health records;human computer interaction;hypermedia markup languages;medical computing;software architecture;user interfaces,CDSS;DocBot;HTML interfaces;Web-based clinical decision support system;decision making;demographic information;electronic health record analytics;etiologically relevant binary decision questions;medical assessments;patient analysis;patient information;patient interaction;preclinical form;software architecture;specialist referrals;symptomatic information;treatment plans,,0,,9,,no,26-30 Aug. 2014,,IEEE,IEEE Conference Publications
Does waveform analysis hold the key to a predictive grid?,G. Nulty,"Tech. Planning & Corp. Dev, Tollgrade Commun. Inc., Reston, VA, USA",2014 Clemson University Power Systems Conference,20140501,2014,,,1,5,"Waveforms can reveal what is a normal operation within your distribution and what is not. When fault current is detected, waveforms of the event and background information (e.g. temperature, GPS location, time) can reveal a lot not only about the real-time conditions of your electric grid, but these signatures could also hold the keys to predicting how to prevent future problems before they arise. But, capturing fault current is not enough to give utilities 100 percent visibility into the potential trouble spots on the distribution network. By capturing, classifying and analyzing fault currents, momentaries and line disturbances that do not cause immediate outages with a software analytics tool, we can build intelligence about what might cause a power failure in the future. For example, incidents like momentaries or line disturbances that spike seasonally across a circuit could prioritize needs around vegetation management. Or, by counting waveform signatures that are normal like recloser operations, it is possible to know when maintenance or replacement is needed - valuable information for a conditioned based asset management strategy.",,Electronic:978-1-4799-3960-2; POD:978-1-4799-3961-9,10.1109/PSC.2014.6808104,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6808104,SCADA systems;algorithms;asset management;circuit breakers;distributed power generation;fault currents;fuses;power generation;power grid;power system faults;power system harmonics;reactive power;relays;substation;switches;transmission lines,Reliability;Switches,asset management;distribution networks;fault currents;power grids;waveform analysis,conditioned based asset management strategy;distribution network;electric grid;fault current;line disturbances;power failure;predictive grid;real-time conditions;recloser operations;trouble spots;vegetation management;waveform analysis;waveform signatures,,1,,6,,no,11-14 March 2014,,IEEE,IEEE Conference Publications
Domain-driven competence assessment in virtual learning environments. Application to planning and time management skills,A. Balderas; ÌÅ. GalÌÁn-PiÌ±ero; J. A. C. HernÌÁndez; G. R. GÌ_mez; J. Dodero; M. Palomo-Duarte,"Department of Computer Science, University of Cadiz, C/ Chile 1, 11002 (Spain)",2014 International Symposium on Computers in Education (SIIE),20150122,2014,,,121,126,"The Learning Management Systems provide a set of facilities for the lecturer to manage his courses. Unfortunately, they have limitations when it comes to assessing generic skills. In most of them, every activity is assessable but just with a simple grade and there is not a direct link between activities and generic skills. In this work we present two alternatives to solve this issue: an assisted method based on a Model-driven architecture approach and a Rest Web service that facilitates the assessment of generic skills. We apply both approaches to a case study consisting in a Moodle-based course where we assess the ability of plan and manage time of each student. Results show that the approaches are complementary, the Web service provides more detailed formative feedback, but the Model-driven approach seems more scalable for courses with a high number of students, where it is more difficult to assess their generic skills.",,Electronic:978-1-4799-4428-6; POD:978-1-4799-4427-9,10.1109/SIIE.2014.7017716,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7017716,Rest Web service;generic skills assessment;learning analytics;learning management system;model-driven architecture;online learning;technological support in online education,Computer architecture;Context;Educational institutions;Least squares approximations;Planning;Service-oriented architecture,Web services;educational courses;learning management systems;planning (artificial intelligence);software architecture;virtual reality,Moodle-based course;Web service;domain-driven competence assessment;learning management systems;model-driven architecture;planning;time management skills;virtual learning environments,,0,,19,,no,12-14 Nov. 2014,,IEEE,IEEE Conference Publications
Drawing Large Weighted Graphs Using Clustered Force-Directed Algorithm,J. Hua; M. L. Huang; Q. V. Nguyen,"Fac. of Eng. & IT, Univ. of Technol., Sydney, NSW, Australia",2014 18th International Conference on Information Visualisation,20140922,2014,,,13,17,"Clustered graph drawing is widely considered as a good method to overcome the scalability problem when visualizing large (or huge) graphs. Force-directed algorithm is a popular approach for laying graphs yet small to medium size datasets due to its slow convergence time. This paper proposes a new method which combines clustering and a force-directed algorithm, to reduce the computational complexity and time. It works by dividing a Long Convergence: LC into two Short Convergences: SC1, SC2, where SC1+SC2 <; LC. We also apply our work on weighted graphs. Our experiments show that the new method improves the aesthetics in graph visualization by providing clearer views for connectivity and edge weights.",1550-6037;15506037,Electronic:978-1-4799-4103-2; POD:978-1-4799-4102-5,10.1109/IV.2014.24,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6902873,clustered graph drawing;data analytics;force-directed graph drawing;graph drawing;graph visualization;information visualization;weighted graph,Clustering algorithms;Clustering methods;Convergence;Data visualization;Educational institutions;Electronic mail;Layout,computational complexity;data visualisation;graph theory;pattern clustering,LC;SC1;SC2;clustered force-directed algorithm;clustered graph drawing;computational complexity;convergence time;graph visualization aesthetics;large graph visualization;large weighted graphs;long convergence;scalability problem;short convergences,,1,,20,,no,16-18 July 2014,,IEEE,IEEE Conference Publications
e-shop user preferences via user behavior,P. OjtÌÁÅÁ; L. PeÅÁka,"Dpt. Software Engineering, Faculty of Mathematics and Physics, Charles University, Malostranske nam. 25, 118 00 Prague, Czech Republic",2014 11th International Conference on e-Business (ICE-B),20160714,2014,,,68,75,We deal with the problem of using user behavior for business relevant analytic task processing. We describe our acquaintance with preference learning from behavior data from an e-shop. Based on our experience and problems we propose a model for collecting (java script tracking) and processing user behavior data. We present several results of offline experiments on real production data. We show that mere data on users (implicit) behavior are sufficient for improvement of prediction of user preference. As a future work we present richer data on time dependent user behavior.,,Electronic:978-9-8985-6598-3; POD:978-1-4673-9242-6,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7509048,Offline Experiments;Performance Metrics;Production Data;Small to Medium Company e-Shop;User Behavior;User Models;User Preference Learning;Web Market without Dominant Seller,Browsers;Business;Data models;Entertainment industry;Portals;Production;Transforms,Java;consumer behaviour;learning (artificial intelligence);retail data processing,Java script tracking;business relevant analytic task processing;e-shop user preferences;preference learning,,,,,,no,28-30 Aug. 2014,,IEEE,IEEE Conference Publications
Early Experience with Model-Driven Development of MapReduce Based Big Data Application,A. Rajbhoj; V. Kulkarni; N. Bellarykar,"Tata Consultancy Services, Pune, India",2014 21st Asia-Pacific Software Engineering Conference,20150423,2014,1,,94,97,"With internet becoming increasingly pervasive, data analytics is playing increasingly critical role in the business. Data to be analyzed exists in large quantity and in multiple formats. Many technologies exist to support Big Data analytics. However, they remain somewhat of a challenge for average developer to use. It's been seen that model-driven development (MDD) approach can eliminate accidental complexity to a large extent. We discuss MDD approach for development of MapReduce based Big Data applications, its efficacy and lessons learnt.",1530-1362;15301362,Electronic:978-1-4799-7426-9; POD:978-1-4799-7427-6,10.1109/APSEC.2014.23,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7091296,Big Data;MapReduce;Meta-model;Model-driven engineering,Analytical models;Big data;Business;Complexity theory;Data analysis;Data models;IPTV,Big Data;Internet;parallel processing;software engineering,Big Data analytics;Internet;MDD approach;MapReduce based Big Data application;model-driven development,,0,,13,,no,1-4 Dec. 2014,,IEEE,IEEE Conference Publications
Economic Governance of Software Delivery,M. Cantor; W. Royce,,IEEE Software,20140228,2014,31,1,54,61,"Agility without objective governance cannot scale, and governance without agility cannot compete. Agile methods are mainstream, and software enterprises are adopting these practices in diverse delivery contexts and at enterprise scale. IBM's broad industry experience with agile transformations and deep internal know-how point to two key principles to deliver sustained improvements in software business outcomes with higher confidence: measure and streamline change costs, and steer with economic governance and Bayesian analytics. Applying these two principles in context is the crux of measured improvement in continuous delivery of smarter software-intensive systems. This article describes more meaningful measurement and prediction foundations for economic governance. The Web extra at http://youtu.be/ghAM8ifyeVI is a video in which Walker Royce, author, IEEE Software editorial board member, and IBM Chief Software Economist, describes how to reason about software delivery governance with lean principles.",0740-7459;07407459,,10.1109/MS.2013.102,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6581776,Bayesian analytics;economic governance;measuring agility;steering leadership,Bayes methods;Cognition;Economics;Measurement uncertainty;Software quality;Uncertainty,DP industry;software prototyping;software quality,Bayesian analytics;IBM broad industry;agile methods;economic governance;smarter software-intensive systems;software business;software delivery governance;software enterprises,,3,,15,,no,Jan.-Feb. 2014,,IEEE,IEEE Journals & Magazines
"Effect of temporal collaboration network, maintenance activity, and experience on defect exposure",,,,,2014,,,,,"Context: Number of defects fixed in a given month is used as an input for several project management decisions such as release time, maintenance effort estimation and software quality assessment. Past activity of developers and testers may help us understand the future number of reported defects. Goal: To find a simple and easy to implement solution, predicting defect exposure. Method: We propose a temporal collaboration network model that uses the history of collaboration among developers, testers, and other issue originators to estimate the defect exposure for the next month. Results: Our empirical results show that temporal collaboration model could be used to predict the number of exposed defects in the next month with R2 values of 0.73. We also show that temporality gives a more realistic picture of collaboration network compared to a static one. Conclusions: We believe that our novel approach may be used to better plan for the upcoming releases, helping managers to make evidence based decisions.",,,,http://dl.acm.org/citation.cfm?id=2652586&CFID=696538919&CFTOKEN=83912867,,,,,,,,,,yes,,,,
Efficacy of an online writing program on URM students in engineering,P. R. Backer,"Department of Aviation and Technology, San Jos&#x00E9; State University, San Jos&#x00E9;, CA 95192",2014 IEEE Frontiers in Education Conference (FIE) Proceedings,20150219,2014,,,1,7,"Recently, universities have explored using online technologies to increase instructor efficiency and improve student performance. One such technology is the Educational Testing Service (ETS) web-based service, Criterion. Criterion is an online writing service whose purpose is to help students improve their grasp of the English language through offering critical, detailed feedback in the areas of: grammar, spelling, mechanics, usage and organization, and development. With Criterion, students get the benefit of additional writing practice without adding to the instructor's workload, allowing instructors to focus on the content and style of students' work. To assess the effectiveness of the Criterion writing program, our researchers used the program's analytics to gauge student proficiency, use, and the effectiveness of the program. Criterion provides the instructor with each student's progress from the moment of assignment submission. This allowed researchers to track the number of submissions each student needs to submit an acceptable writing assignment. In this study we compiled all of the data from Criterion and tracked the number of submissions and types of errors and sorted them by major and ethnicity. A high percentage of San JoseíÅ State University (SJSU)'s incoming freshmen are remedial in English or mathematics. Furthermore, at SJSU underrepresented minority (URM) students have notably higher remediation rates; 70% of African Americans, 60% of Hispanic students, and 50% of Asian Americans require English remediation during their first year, compared with 31% of Caucasian students. Many of these students are U.S. educated English learners with lower English skills than Native English students. They often struggle to complete their English and writing requirements, which negatively impacts their retention and graduation rates. As an online program that offers none of the bias present in traditional university classes, URM students using Criterion d- not feel intimidated or overwhelmed. Additionally, the program is set in such a way that a student may use it as many times as needed to learn at their own pace. It effectively assesses student work and provides opportunities for them to learn from their mistakes, while its structure allows for specific, constructive feedback from faculty.",0190-5848;01905848,Electronic:978-1-4799-3922-0; POD:978-1-4799-3923-7; USB:978-1-4799-3921-3,10.1109/FIE.2014.7044294,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7044294,Online learning;improvement of writing;writing skills of engineering students,Educational institutions;Engineering students;Grammar;Multimedia communication;Software;Springs,Internet;computer aided instruction;educational institutions;engineering education;natural language processing,Criterion writing program;ETS Web-based service;English language;English remediation;English skills;SJSU underrepresented minority;San JoseíÅ State University;URM students;US educated English learners;assignment submission;constructive feedback;development;educational testing service Web-based service;engineering;grammar;instructor efficiency;mechanics;native English students;online technologies;online writing program efficacy;online writing service;organization;program analytics;spelling;student performance;usage;writing assignment;writing practice,,0,,12,,no,22-25 Oct. 2014,,IEEE,IEEE Conference Publications
Efficient People Counting with Limited Manual Interferences,J. Xu; Q. Wu; J. Zhang; B. Silk; G. T. Ngo; Z. Tang,"Sch. of Comput. Sci. & Eng., Nanjing Univ. of Sci. & Technol., Nanjing, China",2014 International Conference on Digital Image Computing: Techniques and Applications (DICTA),20150115,2014,,,1,6,"People counting is a topic with various practical applications. Over the last decade, two general approaches have been proposed to tackle this problem: (a) counting based on individual human detection; (b)counting by measuring regression relation between the crowd density and number of people. Because the regression based method can avoid explicit people detection which faces several well-known challenges, it has been considered as a robust method particularly on a complicated environments. An efficient regression based method is proposed in this paper, which can be well adopted into any existing video surveillance system. It adopts color based segmentation to extract foreground regions in images. Regression is established based on the foreground density and the number of people. This method is fast and can deal with lighting condition changes. Experiments on public datasets and one captured dataset have shown the effectiveness and robustness of the method.",,Electronic:978-1-4799-5409-4; POD:978-1-4799-5410-0,10.1109/DICTA.2014.7008106,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7008106,,Feature extraction;Image color analysis;Image segmentation;Lighting;Measurement;Training;Vectors,feature extraction;image colour analysis;image segmentation;pedestrians;regression analysis,color based segmentation;crowd density;foreground density;foreground region extraction;human detection;lighting condition change;manual interferences;pedestrian counting;people counting;public datasets;regression based method;regression relation measurement;video surveillance system,,1,,26,,no,25-27 Nov. 2014,,IEEE,IEEE Conference Publications
Electrical modeling of split ring resonators operating in the UHF band,S. Naoui; L. Latrach; A. Gharsallah,"Unit of Research Circuits and Electronics Systems High Frequency, Faculty of Science, University ElManar Tunis, Tunisia",2014 Global Summit on Computer & Information Technology (GSCIT),20141204,2014,,,1,4,"The article in hand deals present a new approach focuses on the electric modelling of split ring resonators based on an equivalent circuit. The elements of circuit are determined from geometric parameters forming the three dimensional model of this type of resonators. Firstly an analytic study is proposed about the determination of the components of the circuit in function the physical parameters with a review about comparison between obtained results when the simulation of two different models is presented. Secondly, a demonstrative study on the effect of varying the value of the relative permittivity and determination the error rate at level the resonant frequency in the two models. The objective is to put the conditions and constraints necessary to make our equivalent circuit most responded and most efficient with zero error. The results of simulations are confirmed with using the two commercials software HFSS and ADS.",,Electronic:978-1-4799-5627-2; POD:978-1-4799-5628-9,10.1109/GSCIT.2014.6970101,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6970101,Advanced Design System (ADS);Equivalent Circuit;High Frequency Structure Simulator (HFSS);Metamaterials left hand;Split Ring Resonator,Analytical models;Integrated circuit modeling;Permittivity;Resonant frequency,UHF resonators;equivalent circuits;permittivity,ADS;HFSS;UHF band;equivalent circuit;error rate;geometric parameters;physical parameters;relative permittivity;resonant frequency;split ring resonators;three dimensional model,,0,,18,,no,14-16 June 2014,,IEEE,IEEE Conference Publications
Emerging trends in Cloud Computing and Big Data,D. Janakiram,"Dept. of Comput. Sci. & Eng., Indian Inst. of Technol. Madras, Chennai, India","2014 Conference on IT in Business, Industry and Government (CSIBIG)",20150312,2014,,,1,1,Summary form only given. This talk looks at the processing requirements of Big Data and challenges in using Map-Reduce paradigm. It looks at some of the bench mark big data analytics problems using Map-Reduce on HDFS. We motivate the use of new paradigm called Generate Map Reduce (GMR) for doing big data analytics in the cloud. GMR supports shared data structure abstraction across map jobs and supports iterative and recursive map-reduce jobs. GMR performs quite well for Big Data Analytics on the cloud and the talk presents the results of using GMR for Big Data Analytics on the cloud.,,CD-ROM:978-1-4799-3062-3; Electronic:978-1-4799-3064-7; POD:978-1-4799-3065-4,10.1109/CSIBIG.2014.7056923,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7056923,,Awards activities,Big Data;cloud computing;data analysis;data structures;distributed programming;public domain software,GMR;HDFS;benchmark Big Data analytics problems;cloud computing;generate map reduce;map jobs;recursive MapReduce jobs;shared data structure abstraction,,0,,,,no,8-9 March 2014,,IEEE,IEEE Conference Publications
Enhancing higher education experience: The eMadrid initiative at UNED university,M. RodrÌ_guez-Artacho; E. J. Lorenzo; L. S. Robles; J. CigarrÌÁn; R. Centeno; J. I. Mayorga; J. VÌ©lez; M. Castro; E. SancristÌ_bal; G. DÌ_az; S. MartÌ_n; R. Gil; F. Garcia; J. Cubillo; S. Ros,"Departamento de Lenguajes y Sistemas Inform&#x00E1;ticos, Universidad Nacional de Educaci&#x00F3;n a Distancia, Spain",2014 IEEE Frontiers in Education Conference (FIE) Proceedings,20150219,2014,,,1,4,"In this paper we focus on the achievements of eMadrid initiative in some fields of technology-enhanced learning, mainly involving the improvement of the mechanisms for open educational content retrieval from Internet, considering Internet resources as potential learning objects. Also we facilitate the integration of remote laboratories and external tools in virtual campuses architectures supporting enriched capabilities and describe a way to cluster and identify learner weaknesses using a learning analytics approach in combination with the item response theory.",0190-5848;01905848,Electronic:978-1-4799-3922-0; POD:978-1-4799-3923-7; USB:978-1-4799-3921-3,10.1109/FIE.2014.7044051,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7044051,Information Retrieval;Learning Objects;Remote Laboratories;Virtual Campus,Computer architecture;Educational institutions;Internet;Remote laboratories;Software,Internet;computer aided instruction;educational institutions;further education;information retrieval;laboratories,Internet resources;UNED university;eMadrid initiative;external tools;higher education experience enhancement;item response theory;learning analytics approach;learning objects;open educational content retrieval;remote laboratories;technology-enhanced learning;virtual campus architectures,,0,,13,,no,22-25 Oct. 2014,,IEEE,IEEE Conference Publications
Estimating development effort in free/open source software projects by mining software repositories: a case study of OpenStack,,,,,2014,,,,,"Because of the distributed and collaborative nature of free / open source software (FOSS) projects, the development effort invested in a project is usually unknown, even after the software has been released. However, this information is becoming of major interest, especially ---but not only--- because of the growth in the number of companies for which FOSS has become relevant for their business strategy. In this paper we present a novel approach to estimate effort by considering data from source code management repositories. We apply our model to the OpenStack project, a FOSS project with more than 1,000 authors, in which several tens of companies cooperate. Based on data from its repositories and together with the input from a survey answered by more than 100 developers, we show that the model offers a simple, but sound way of obtaining software development estimations with bounded margins of error.",,,,http://dl.acm.org/citation.cfm?id=2597107&CFID=696538919&CFTOKEN=83912867,,,,,,,,,,yes,,,,
Evacuation and Emergency Management Using a Federated Cloud,S. Sarkar; S. Chatterjee; S. Misra,"Indian Institute of Technology, Kharagpur",IEEE Cloud Computing,20150310,2014,1,4,68,76,"Contemporary disaster relief techniques fall significantly short in terms of efficiency and timeliness. In a postdisaster scenario, the generation and transmission of voluminous data at a high velocity impacts the communication framework. This work exploits the benefits of opportunistic communication and efficient big data management policies, focusing on the collaboration of multiple private and/or public clouds of diverse nature to perform damage assessment and determine the spatial distribution of the live victims and their physical and mental status. Based on the analytics, real-time decision making of the rescue operation is achieved.",2325-6095;23256095,,10.1109/MCC.2014.72,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7057595,big data analytics;cloud;emergency management;federated cloud;wireless sensors,Cloud computing;Computer architecture;Disasters;Emergency services;Media;Mobile communication;Servers;Tsunami,Big Data;cloud computing;emergency management;groupware;software architecture,big data management policies;communication framework;contemporary disaster relief techniques;damage assessment;emergency management;evacuation management;federated cloud computing architecture;live victims spatial distribution determination;mental status;multiple private cloud collaboration;multiple public cloud collaboration;physical status;real-time decision making;rescue operation;voluminous data generation;voluminous data transmission,,0,,14,,no,Nov. 2014,,IEEE,IEEE Journals & Magazines
Evaluating retrofit strategies for greening existing buildings by energy modelling & data analytics,A. Karkare; A. Dhariwal; S. Puradbhat; M. Jain,"Dept. of Energy Sci. Eng., I.I.T. Bombay, Mumbai, India",2014 International Conference on Intelligent Green Building and Smart Grid (IGBSG),20140619,2014,,,1,4,"Design strategies that account for local micro-climatic conditions can reduce the overall energy consumption of buildings over its life. This paper evaluates non-structural retrofit strategies for reducing energy consumption for existing buildings. Based on real-life data on usage statistics of buildings and accounting for local climatic conditions, we validate energy consumption predictions by Sefaira<sup>å¨</sup> building simulation software. A comparative analysis is then performed for various energy conservation scenarios by estimating energy reduction and payback periods. Our analysis shows that various retrofit strategies can lead to reduction in energy consumption upto 60% with a pay back period of about 9 years.",,Electronic:978-1-4673-6123-1; POD:978-1-4673-6121-7,10.1109/IGBSG.2014.6835192,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6835192,Energy efficiency;building design;energy modelling;retrofit;solar,Buildings;Cooling;Data models;Electricity;Energy consumption;Lighting;Solar heating,buildings (structures);energy conservation;energy consumption,buildings;data analytics;energy consumption;energy modelling;energy reduction,,0,,6,,no,23-25 April 2014,,IEEE,IEEE Conference Publications
Evaluating vendor's performance in outsource software development risks using analytic hierarchy process technique,J. H. Yahaya; N. F. Hamzah; A. Deraman,"Faculty of Information Science and Technology, Universiti Kebangsaan Malaysia, Bangi, Selangor, Malaysia",2014 8th. Malaysian Software Engineering Conference (MySEC),20141218,2014,,,61,66,"Outsource software development approach has been recognized as an important practice in government agencies as well as in private sectors. It includes third parties role and involvement in IT implementation and thus this approach has certain level of risks that need to be identified, measured and managed. The benefits and reasons associated with outsourcing implementation are operation and development cost reduction, operational efficiency and improves service quality. Although there are advantages in this approach, but there are also risks that associated with it. In this study, a survey was conducted to recognize key risks involve in outsource software development and to access and classify risk's level and severity in the software project development and implementation. Then, the identified risks were analyzed using AHP technique to evaluate the performance of the vendors in outsourcing data collected in the organization. The evaluation was carried out with officers in The Rubber Industry Smallholders Development Authority or RISDA in Malaysia because this organization has experience in outsourcing for more than 15 years. The results categorize key risks associated with system software development and the method developed helps the organizations to manage risks based on the importance and harshness of the risk towards organisations. The proposed method is developed and established on risk analysis to measure performance of the vendors using multi-criteria selection method.",,Electronic:978-1-4799-5439-1; POD:978-1-4799-5440-7; USB:978-1-4799-5438-4,10.1109/MySec.2014.6985989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6985989,analytic hierarchy process;outsourcing software development;risks;vendor's performance,Analytic hierarchy process;Companies;Contracts;Outsourcing;Risk management;Software,analytic hierarchy process;cost reduction;outsourcing;risk management;software development management,AHP technique;Malaysia;RISDA;analytic hierarchy process technique;development cost reduction;government agencies;multicriteria selection method;operational efficiency;outsource software development risks;private sectors;rubber industry smallholders development authority;service quality;vendor performance evaluation,,0,,20,,no,23-24 Sept. 2014,,IEEE,IEEE Conference Publications
Evaluation of a state-based real-time scheduling analysis technique,T. Gezgin; S. Henkler; I. Stierand; A. Rettberg,"Institute for Information Technology (OFFIS) Oldenburg, Germany",2014 12th IEEE International Conference on Industrial Informatics (INDIN),20141106,2014,,,158,163,"The analysis of real-time properties is crucial in safety critical areas. Systems have to work in a timely manner to offer correct services. The analysis of timing properties is particularly difficult for distributed systems when complex interferences between individual tasks can occur. Considering only critical instances, as analytic approaches do, may deliver pessimistic results leading to higher production costs. In previous works we introduced a state-based approach to validate task-and end-to-end deadlines for distributed systems. To improve scalability and reduce the analysis time, the approach computes the state spaces of the individual resources in a compositional fashion. For this, abstraction and composition operations were defined to remove those parts of the inputs of resources which have no influence on the response times of the allocated tasks. In this work, a new abstraction technique is introduced for scenarios where event bursts occur. Further, we extend our approach for systems with cyclic dependencies among the resources. We evaluate our approach on a set of example scenarios and compare the results with the state-of-the-art tool Uppaal.",1935-4576;19354576,Electronic:978-1-4799-4905-2; POD:978-1-4799-4904-5; USB:978-1-4799-4906-9,10.1109/INDIN.2014.6945501,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6945501,Abstraction Techniques;Model Checking;Real-Time Systems;Scheduling Analysis,Abstracts;Automata;Clocks;Computer architecture;Feedback loop;Indexes;Timing,distributed processing;real-time systems;safety-critical software;scheduling,abstraction operation;abstraction technique;analysis time reduction;composition operation;cyclic dependencies;distributed systems;production costs;real-time property analysis;safety critical areas;scalability improvement;state-based real-time scheduling analysis technique;timing property analysis,,1,,19,,no,27-30 July 2014,,IEEE,IEEE Conference Publications
Event-based text visual analytics,J. Wang; L. Bradel; C. North,Virginia Tech,2014 IEEE Conference on Visual Analytics Science and Technology (VAST),20150216,2014,,,333,334,"We present an event-based approach for solving a directed sensemaking task in which we combine powerful information foraging tools with intuitive synthesis spaces to solve the VAST Challenge 2014 Mini-Challenge 1. A combination of student-created and commericially available software are used to solve various aspects of the scenario. In addition to applying entitiy extraction and topic modelling, we enable the user to explore a large dataset using multi-model semantic interaction, which infers analytical reasoning from user actions to augment the data spatialization and determine what information should be presented and suggested to the user. Additionally, we visualize extracted topics using Tableau to construct a timeline of events surrounding the questions posed by the challenge.",,Electronic:978-1-4799-6227-3; POD:978-1-4799-6186-3,10.1109/VAST.2014.7042552,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7042552,Sensemaking;event extraction;semantic interaction;topic modelling,Data mining;Data models;Organizations;Resource management;Semantics;Visual analytics,,,,0,,5,,no,25-31 Oct. 2014,,IEEE,IEEE Conference Publications
Evolutionary data reorganization for efficient workload processing,A. Razumovskiy; A. Spivak; D. Nasonov; A. Boukhanovsky,"ITMO University, Saint-Petersburg, Russia",2014 IEEE 8th International Conference on Application of Information and Communication Technologies (AICT),20150209,2014,,,1,6,"Digital data universe size is exponentially growing up from year to year and currently is estimated to be more than 4.4 Zb. It compels scientific community to found out more efficient approaches in collecting, organizing and processing of information. A lot of enterprise solutions offer extended software tools based on MapReduce principles for big data analytics. One of the required parts of MapReduce solutions is data replication organization which permanently helps to increase safety and to provide increased performance. In this paper we investigate the possibility of applying queries workload optimization using metaheuristic algorithm for data dynamic reorganization according to executed tasks influence in MapReduce-based storages.",,Electronic:978-1-4799-4119-3; POD:978-1-4799-4118-6,10.1109/ICAICT.2014.7035952,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7035952,MapReduce;genetic algorithm;metaheuristic;optimization;reorganization,Bandwidth;Biological cells;Educational institutions;Genetic algorithms;Gravity;Greedy algorithms;Optimization,Big Data;data analysis;file organisation;genetic algorithms,Big Data analytics;MapReduce;data replication organization;evolutionary data reorganization;metaheuristic algorithm;query workload optimization;workload processing,,0,,11,,no,15-17 Oct. 2014,,IEEE,IEEE Conference Publications
Extracting Dependencies from Software Changes: An Industry Experience Report,T. Wetzlmaier; C. Klammer; R. Ramler,"Software Analytics & Evolution, Software Competence Center Hagenberg GmbH, Hagenberg, Austria",2014 Joint Conference of the International Workshop on Software Measurement and the International Conference on Software Process and Product Measurement,20150105,2014,,,163,168,"Retrieving and analyzing information from software repositories and detecting dependencies are important tasks supporting software evolution. Dependency information is used for change impact analysis, defect prediction as well as cohesion and coupling measurement. In this paper we report our experience from extracting dependency information from the change history of a commercial software system. We analyzed the software system's evolution of about six years, from the start of development to the transition to product releases and maintenance. Analyzing the co-evolution of software artifacts allows detecting logical dependencies between system parts implemented with heterogeneous technologies as well as between different types of development artifacts such as source code, data models or documentation. However, the quality of the extracted dependencies relies on established development practices and conformance to a defined change process. In this paper we indicate resulting limitations and recommend further processing and filtering steps to prepare the dependency data for subsequent analysis and measurement activities.",,Electronic:978-1-4799-4174-2; POD:978-1-4799-7874-8,10.1109/IWSM.Mensura.2014.12,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7000096,change history;dependency analysis;logical coupling;mining software repositories,Couplings;Data mining;History;Java;Servers;Software systems,information analysis;information retrieval;software maintenance,change impact analysis;cohesion measurement;commercial software system;coupling measurement;defect prediction;dependency detection;dependency extraction;filtering step;information analysis;information retrieval;processing step;software artifact coevolution;software change;software repositories,,0,,19,,no,6-8 Oct. 2014,,IEEE,IEEE Conference Publications
"Extraction, Identification, and Ranking of Network Structures from Data Sets",M. Trovati; N. Bessis; A. Huber; A. Zelenkauskaite; E. Asimakopoulou,"Sch. of Comput. & Math., Univ. of Derby, Derby, UK","2014 Eighth International Conference on Complex, Intelligent and Software Intensive Systems",20141002,2014,,,331,337,"Networks are widely used to model a variety of complex, often multi-disciplinary, systems in which the relationships between their sub-parts play a significant role. In particular, there is extensive research on the topological properties associated with their structure as this allows the analysis of the overall behaviour of such networks. However, extracting networks from structured and unstructured data sets raises several challenges, including addressing any inconsistency present in the data, as well as the difficulty in investigating their properties especially when the topological structure is not fully determined or not explicitly defined. In this paper, we propose a novel method to address the automated identification, assessment and ranking of the most likely structure associated with networks extracted from a variety of data sets. More specifically, our approach allows to mine data to assess whether their associated networks exhibit properties comparable to well-known structures, namely scale-free, small world and random networks. The main motivation is to provide a toolbox to classify and analyse real-world networks otherwise difficult to fully assess due to their potential lack of structure. This can be used to investigate their dynamical and statistical behaviour which would potentially lead to a better understanding and prediction of the properties of the system (s) they model. Our initial validation shows the potential of our method providing relevant and accurate results.",,CD-ROM:978-1-4799-4326-5; Electronic:978-1-4799-4325-8; POD:978-1-4799-1677-1,10.1109/CISIS.2014.46,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6915536,Data analytics;Information extraction;Knowledge discovery;Networks;Social graphs,Approximation methods;Data mining;Data models;Feature extraction;Mathematical model;Sentiment analysis;Standards,data analysis;data mining;information retrieval;small-world networks;statistical analysis;topology,assessment;automated identification;data analytics;data mining;dynamical behaviour;information extraction;network behaviour analysis;network structures;random networks;ranking;real-world networks;scale-free networks;small world networks;statistical behaviour;system properties;topological properties;topological structure;unstructured data sets,,6,,11,,no,2-4 July 2014,,IEEE,IEEE Conference Publications
"Facilitating Twitter data analytics: Platform, language and functionality",K. Tao; C. Hauff; G. J. Houben; F. Abel; G. Wachsmuth,"TU Delft, Web Information Systems, PO Box 5031, 2600 GA, Delft, the Netherlands",2014 IEEE International Conference on Big Data (Big Data),20150108,2014,,,421,430,"Conducting analytics over data generated by Social Web portals such as Twitter is challenging, due to the volume, variety and velocity of the data. Commonly, adhoc pipelines are used that solve a particular use case. In this paper, we generalize across a range of typical Twitter-data use cases and determine a set of common characteristics. Based on this investigation, we present our Twitter Analytical Platform (TAP), a generic platform for conducting analytical tasks with Twitter data. The platform provides a domain-specific Twitter Analysis Language (TAL) as the interface to its functionality stack. TAL includes a set of analysis tools ranging from data collection and semantic enrichment, to machine learning. With these tools, it becomes possible to create and customize analytical workflows in TAL and build applications that make use of the analytics results. We showcase the applicability of our platform by building Twinder-a search engine for Twitter streams.",,Electronic:978-1-4799-5666-1; POD:978-1-4799-5667-8,10.1109/BigData.2014.7004259,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7004259,,Data analysis;Data mining;Data models;Monitoring;Pipelines;Semantics;Twitter,data analysis;learning (artificial intelligence);portals;search engines;social networking (online),TAL;TAP;Twinder;Twitter analytical platform;Twitter data analytics;Twitter streams;Twitter-data use cases;data collection;domain-specific Twitter analysis language;machine learning;search engine;semantic enrichment;social Web portals,,0,,33,,no,27-30 Oct. 2014,,IEEE,IEEE Conference Publications
FDT 2.0: Improving scalability of the fuzzy decision tree induction tool - integrating database storage,E. E. A. Durham; X. Yu; R. W. Harrison,"Department of Computer Science, Georgia State University, Atlanta, USA",2014 IEEE Symposium on Computational Intelligence in Healthcare and e-health (CICARE),20150115,2014,,,187,190,"Effective machine-learning handles large datasets efficiently. One key feature of handling large data is the use of databases such as MySQL. The freeware fuzzy decision tree induction tool, FDT, is a scalable supervised-classification software tool implementing fuzzy decision trees. It is based on an optimized fuzzy ID3 (FID3) algorithm. FDT 2.0 improves upon FDT 1.0 by bridging the gap between data science and data engineering: it combines a robust decisioning tool with data retention for future decisions, so that the tool does not need to be recalibrated from scratch every time a new decision is required. In this paper we briefly review the analytical capabilities of the freeware FDT tool and its major features and functionalities; examples of large biological datasets from HIV, microRNAs and sRNAs are included. This work shows how to integrate fuzzy decision algorithms with modern database technology. In addition, we show that integrating the fuzzy decision tree induction tool with database storage allows for optimal user satisfaction in today's Data Analytics world.",,Electronic:978-1-4799-4527-6; POD:978-1-4799-4526-9,10.1109/CICARE.2014.7007853,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7007853,Big Data;HIV protease;drug resistance prediction;fuzzy ID3;fuzzy logic,Accuracy;Algorithm design and analysis;Databases;Decision trees;Educational institutions;Training;Training data,database management systems;decision trees;fuzzy set theory;software tools;storage management,FDT 2.0;FID3 algorithm;HIV;MySQL;data analytics world;data engineering;data retention;data science;database storage;database technology;freeware FDT tool;freeware fuzzy decision tree induction tool;fuzzy decision algorithms;large biological datasets;large data handling;machine learning;microRNA;optimal user satisfaction;optimized fuzzy ID3;robust decisioning tool;sRNA;scalable supervised-classification software tool,,0,,6,,no,9-12 Dec. 2014,,IEEE,IEEE Conference Publications
Feature-based comparison and selection of Software Defined Networking (SDN) controllers,R. Khondoker; A. Zaalouk; R. Marx; K. Bayarou,"Fraunhofer Institute for Secure Information Technology, Rheinstr. 75, Darmstadt, Germany",2014 World Congress on Computer Applications and Information Systems (WCCAIS),20141007,2014,,,1,7,"Software Defined Networking (SDN) is seen as one way to solve some problems of the Internet including security, managing complexity, multi-casting, load balancing, and energy efficiency. SDN is an architectural paradigm that separates the control plane of a networking device (e.g., a switch / router) from its data plane, making it feasible to control, monitor, and manage a network from a centralized node (the SDN controller). However, today there exists many SDN controllers including POX, FloodLight, and OpenDaylight. The question is, which of the controllers is to be selected and used? To find out the answer to this question, a decision making template is proposed in this paper to help researchers choose the SDN controller that best fits their needs. The method works as follows; first, several existing open-source controllers are analyzed to collect their properties. For selecting the suitable controller based on the derived requirements (for example, a ‰ÛÏJava‰Ûù interface must be provided by the controller), a matching mechanism is used to compare the properties of the controllers with the requirements. Additionally, for selecting the best controller based on optional requirements (for example, GUI will be extremely preferred over the age of the controller), a Multi-Criteria Decision Making (MCDM) method named Analytic Hierarchy Process (AHP) has been adapted by a monotonic interpolation / extrapolation mechanism which maps the values of the properties to a value in a pre-defined scale. By using the adapted AHP, the topmost five controllers have been compared and ‰ÛÏRyu‰Ûù is selected to be the best controller based on our requirements.",,Electronic:978-1-4799-3351-8; POD:978-1-4799-7527-3,10.1109/WCCAIS.2014.6916572,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6916572,,Analytic hierarchy process;Control systems;Documentation;Graphical user interfaces;Java;Productivity;Vectors,Internet;analytic hierarchy process;computer network management;computer network security;extrapolation;interpolation;resource allocation;telecommunication computing,AHP;FloodLight;GUI;Internet;Java interface;MCDM method;OpenDaylight;POX;Ryu;SDN controllers;analytic hierarchy process;centralized node;complexity management;data plane;decision making template;energy efficiency;feature-based comparison;feature-based selection;load balancing;matching mechanism;monotonic extrapolation mechanism;monotonic interpolation mechanism;multicasting;multicriteria decision making method;networking device;open-source controllers;software defined networking controllers,,9,,37,,no,17-19 Jan. 2014,,IEEE,IEEE Conference Publications
Fighting Botnets with Cyber-Security Analytics: Dealing with Heterogeneous Cyber-Security Information in New Generation SIEMs,B. G. N. Crespo; A. Garwood,"Atos Res. & Innvoation, Madrid, Spain","2014 Ninth International Conference on Availability, Reliability and Security",20141211,2014,,,192,198,"One of the cyber-threats with the highest impact nowadays, in terms of number of compromised systems and the impact they can have on the Internet at large, is commonly known as the botnet. In the ACDC (Advanced Cyber Defence Centre) project, partners from 14 European countries, including public administrations, private sector organizations and academia, are trying to achieve a sustainable victory over botnets. This paper presents how a new generation SIEM is being used in the ACDC project to leverage its scalability and enhanced analytic capabilities and produce advance cyber-intelligence from the heterogeneous and massive streams of data continuously produced in the cyber-security context, in combination with traditional security events and system logs. The paper describes a case study where this approach is being tested. In the case study, the SIEM has been adapted to cope, not only with traditional security events and system logs, but also with pre-analyzed information about cyber-threats and incidents reported by the tools of some of the ACDC partner organizations. The case study also tests the adoption of the standard XML-based format called STIX, developed by the Mitre Corporation in the USA, and its suitability as a common specification for exchanging cybersecurity information between a subset of ACDC tools, the Atos SL SIEM and the ACDC's centralized data clearing house (CCH).",,Electronic:978-1-4799-4223-7; POD:978-1-4799-7876-2,10.1109/ARES.2014.33,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6980282,STIX;correlation;cyber-analytics;cyber-security;cyber-threats;cyberdefense,Computer security;Correlation;Monitoring;Organizations;Standards organizations,DP management;invasive software,ACDC project;ACDC tools;Advanced Cyber Defence Centre project;Atos SL SIEM;CCH;Internet;SIEMs;STIX;XML-based format;botnets;centralized data clearing house;cyber-intelligence;cyber-security analytics;cyber-threats;heterogeneous cyber-security information;private sector organizations;public administrations;security events;security information and event management;system logs,,0,,15,,no,8-12 Sept. 2014,,IEEE,IEEE Conference Publications
Finding the capacity of next-generation networks by linear programming,C. W. Tan; S. W. Ho; S. Lint; R. W. Yeung,"College of Science and Engi-neering, City University of Hong Kong",2014 IEEE International Conference on Communication Systems,20150129,2014,,,192,196,"Proving or disproving an information inequality is a crucial step in establishing the converse results in the coding theorems of communication networks. However, next-generation networks are very large-scale, typically involving multiple users and many transceivers and relays. This means that an information inequality involving many random variables can be difficult to be proved or disproved manually. In [1], Yeung developed a framework that uses linear programming for verifying linear information inequalities, and it was recently shown in [2] that this framework can be used to explicitly construct an analytic proof of an information inequality or an analytic counterexample to disprove it if the inequality is not true in general. In this paper, we consider the construction of the smallest counterexample, and also give sufficient conditions for that the inequality can be manipulated to become true. We also describe the software development of automating this analytical framework enabled by cloud computing to analytically verify information inequalities in large-scale problem setting.",,Electronic:978-1-4799-5832-0; POD:978-1-4799-5833-7; USB:978-1-4799-5831-3,10.1109/ICCS.2014.7024792,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7024792,,,encoding;linear programming;next generation networks,cloud computing;coding theorems;communication networks;linear information inequalities;linear programming;next-generation networks;software development,,0,,18,,no,19-21 Nov. 2014,,IEEE,IEEE Conference Publications
Flexible traffic management in broadband access networks using Software Defined Networking,J. RÌ_ckert; R. Bifulco; M. Rizwan-Ul-Haq; H. J. Kolbe; D. Hausheer,"Peer-to-Peer Syst. Eng., Tech. Univ. Darmstadt, Darmstadt, Germany",2014 IEEE Network Operations and Management Symposium (NOMS),20140619,2014,,,1,8,"Over the years, the demand for high bandwidth services, such as live and on-demand video streaming, steadily increased. The adequate provisioning of such services is challenging and requires complex network management mechanisms to be implemented by Internet service providers (ISPs). In current broadband network architectures, the traffic of subscribers is tunneled through a single aggregation point, independent of the different service types it belongs to. While having a single aggregation point eases the management of subscribers for the ISP, it implies huge bandwidth requirements for the aggregation point and potentially high end-to-end latency for subscribers. An alternative would be a distributed subscriber management, adding more complexity to the management itself. In this paper, a new traffic management architecture is proposed that uses the concept of Software Defined Networking (SDN) to extend the existing Ethernet-based broadband network architecture, enabling a more efficient traffic management for an ISP. By using SDN-enabled home gateways, the ISP can configure traffic flows more dynamically, optimizing throughput in the network, especially for bandwidth-intensive services. Furthermore, a proof-of-concept implementation of the approach is presented to show the general feasibility and study configuration tradeoffs. Analytic considerations and testbed measurements show that the approach scales well with an increasing number of subscriber sessions.",1542-1201;15421201,Electronic:978-1-4799-0913-1; POD:978-1-4799-0911-7; USB:978-1-4799-0912-4,10.1109/NOMS.2014.6838322,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6838322,,Broadband communication;Computer architecture;Logic gates;Monitoring;Protocols;Servers;Service-oriented architecture,Internet;broadband networks;internetworking;local area networks;software radio;subscriber loops;telecommunication network management;telecommunication traffic,Ethernet-based broadband network architecture;ISP;Internet service providers;SDN-enabled home gateways;aggregation point;bandwidth-intensive services;broadband access networks;broadband network architectures;complex network management mechanisms;end-to-end latency;flexible traffic management;live streaming;on-demand video streaming;software defined networking;subscriber sessions;traffic flows,,4,,32,,no,5-9 May 2014,,IEEE,IEEE Conference Publications
Flow identification and characteristics mining from internet traffic with hadoop,Y. Cai; B. Wu; X. Zhang; M. Luo; J. Su,"Sch. of Comput. Sci., Beijing Univ. of Posts & Commun., Beijing, China","2014 International Conference on Computer, Information and Telecommunication Systems (CITS)",20140818,2014,,,1,5,"Characteristics of flow describe the pattern and trend of network traffic, it helps network operator understanding network usage and user behavior, especially useful for those who concerns more about network capacity planning, traffic engineering and fault handling. Due to the large scale of datacenter network and explosive growth of traffic volume, it's hard to collect, store and analyze Internet traffic on a single machine. Hadoop has become a popular infrastructure for massive data analytics because it facilitates scalable data processing and storage services on a distributed computing system consisting of commodity hardware. In this paper, we present a Hadoop-based traffic analysis system, which accepts input from multiple data traces, performs flow identification, characteristics mining and flow clustering, output of the system provides guidance in resource allocation, flow scheduling and some other tasks. Experiment on a dataset about 8G size from university datacenter network shows that the system is able to finish flow characteristics mining on a four node cluster within 23 minutes.",,CD-ROM:978-1-4799-4384-5; Electronic:978-1-4799-4383-8; POD:978-1-4799-4382-1,10.1109/CITS.2014.6878955,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6878955,Flow Characteristic Mining;Flow Clustering;Hadoop;Software Defined Network;Traffic Analysis,Algorithm design and analysis;Clustering algorithms;Data mining;Educational institutions;IP networks;Internet;Payloads,Internet;data analysis;data mining;pattern clustering;resource allocation;telecommunication traffic,Hadoop;Internet traffic;characteristics mining;commodity hardware;data analytics;distributed computing system;fault handling;flow clustering;flow identification;flow scheduling;multiple data traces;network capacity planning;network operator;network usage;resource allocation;scalable data processing;storage services;traffic engineering;traffic volume;university datacenter network;user behavior,,2,,20,,no,7-9 July 2014,,IEEE,IEEE Conference Publications
Framework for evaluating Capture the Flag (CTF) security competitions,R. Raman; S. Sunny; V. Pavithran; K. Achuthan,"Center for Research in Advanced Technologies for Education, Amrita University, India",International Conference for Convergence for Technology-2014,20150423,2014,,,1,5,"A large number of ethical hacking competitions are organized worldwide as Capture The Flag (CTF) events. But there does not exist a framework to evaluate and rank CTFs that will guide participants as to which CTF's to participate. In a CTF event, the participants are required to either solve a set of challenges to gain points or they are required to defend their system by eliminating the vulnerabilities while attacking other's system vulnerabilities. We are proposing a framework that would evaluate and rank CTFs according to factors like similarity of the tasks to the common critical vulnerabilities, solvability of tasks, periodicity, training given prior to CTF, geographical reach, problem solving skills etc. In the next step these factors are systematically assigned weights using Analytic Hierarchy Process. As part of frame work creation and validation, ten CTFs have been analysed. Our analysis indicates that: All CTFs fall in to one of the three categories (jeopardy, attack-defence and mixed); CTFs often adopt popular software vulnerabilities and threats as tasks to be solved; Only few CTFs give formal training prior to the event; Complexity of the tasks to be solved varies from CTF to CTF. Five CTFs were ranked using the newly developed framework.",,Electronic:978-1-4799-3759-2; POD:978-1-4799-3760-8,10.1109/I2CT.2014.7092098,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7092098,Analytic Hierarchy Process;CTF;Framework;Hacking;Vulnerability,Analytic hierarchy process;Computer crime;Training;Uniform resource locators,analytic hierarchy process;computer crime;software engineering,CTF;analytic hierarchy process;capture the flag security evaluation;ethical hacking competitions;software vulnerabilities,,1,,21,,no,6-8 April 2014,,IEEE,IEEE Conference Publications
Frequency Veering Analysis of Cable-Girder System for Cable-Stayed Bridges,C. Weizhen; Z. Yang; L. Xue,"Tongji Univ., Shanghai, China",2014 7th International Conference on Intelligent Computation Technology and Automation,20150108,2014,,,951,956,Analytic characteristic values of the simplified model were solved for cable-girder system. The characteristic vectors were derived. The system frequencies and modes were obtained which were simplified with non-dimensional method. The frequency veering of cable-girder system was analyzed with Maple numerical analysis software. The results show that there exist frequency veering and mode conversion among cable-girder modes under special parameter combination. Some slight changes of mechanical parameters cause fast conversion among cable-girder modes. Hybrid modes appear in the center of frequency veering area. Mode localization factor can be used to predict whether hybrid modes occur in cable-girder system effectively.,,Electronic:978-1-4799-6636-3; POD:978-1-4799-6637-0,10.1109/ICICTA.2014.229,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7003692,Cable-Girder System;Cable-Stayed Bridge;Frequency Veering;Localization Factor,Bridges;Differential equations;Educational institutions;Eigenvalues and eigenfunctions;Numerical models;Structural beams;Vibrations,beams (structures);bridges (structures);cables (mechanical);numerical analysis;structural engineering;supports;vibrations,Maple numerical analysis software;cable-girder modes;cable-girder system;cable-stayed bridges;characteristic vectors;coupled vibration;frequency veering analysis;hybrid modes;mechanical parameters;mode conversion;mode localization factor,,0,,12,,no,25-26 Oct. 2014,,IEEE,IEEE Conference Publications
From Multiple Linked Views to Multiple Linked Analyses: The Meme Media Digital Dashboard,J. SjÌ_bergh; Y. Tanaka,"Meme Media Lab., Hokkaido Univ., Sapporo, Japan",2014 18th International Conference on Information Visualisation,20140922,2014,,,170,175,"We describe a system called the Digital Dashboard that uses multiple linked views of data. All views allow interaction with the visualization results and interaction is done through direct manipulation. The system has been extended to allow new complex data to be generated in analysis components at runtime, e.g. By statistical analysis or data mining of parts of the data. The resulting data can be used in other linked views or analysis components, so when e.g. A data mining parameter is changed, all linked views (or analysis components) are automatically updated as soon as the new calculations are finished, and when something changes in linked components (e.g. A different subset of the data is selected), the calculations are automatically redone (if necessary).",1550-6037;15506037,Electronic:978-1-4799-4103-2; POD:978-1-4799-4102-5,10.1109/IV.2014.33,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6902899,direct manipulation;explorative data visualization;multiple linked views;visual analytics;visualization,Data mining;Data visualization;Databases;Runtime;Snow;Software;Visual analytics,data mining;data visualisation;social networking (online);statistical analysis,analysis components;complex data;data mining;direct manipulation;meme media digital dashboard;multiple linked analyses;multiple linked views;statistical analysis;visualization results,,0,,16,,no,16-18 July 2014,,IEEE,IEEE Conference Publications
FSBD: A Framework for Scheduling of Big Data Mining in Cloud Computing,L. Ismail; M. M. Masud; L. Khan,"Coll. of Inf. Technol., UAEU, Al-Ain, United Arab Emirates",2014 IEEE International Congress on Big Data,20140925,2014,,,514,521,"Cloud computing is seen as an emerging technology for big data mining and analytics. Cloud computing can provide data mining results in the form of a Software As a Service (SAS). Both performance and quality of mining are fundamentals criteria for the use of a data mining application provided by a Cloud computing environment. In this paper, we propose a Cloud computing framework, which is responsible to distribute and schedule a Cluster-Based data mining application and its data set. The main goal of our proposed framework for scheduling of Big Data Mining (FSBD) is to decrease the overall execution time of the application with minimum loss in mining quality. We consider the Cluster-based data mining technique as a pilot application for our framework. The results show an important speedup with a minimum loss in quality of mining. We obtained a ratio of 2 of the normalized actual makespan vis-a-vis the ideal makespan. The quality of mining scales well with the number of clusters and the increasing size of the dataset. The results are promising, encouraging the adoption of the framework by Cloud providers.",2379-7703;23797703,Electronic:978-1-4799-5057-7; POD:978-1-4799-5058-4,10.1109/BigData.Congress.2014.81,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6906823,Autonomous Computing;Cloud Computing;Data Mining;Distributed Systems;Divisible Load Application;High Performance Computing;Scheduling,Big data;Cloud computing;Clustering algorithms;Computational modeling;Data mining;Scheduling algorithms,Big Data;cloud computing;data mining;pattern clustering;scheduling,FSBD;cloud computing;cluster-based data mining application;framework for scheduling of big data mining;mining quality;normalized actual makespan,,2,,22,,no,June 27 2014-July 2 2014,,IEEE,IEEE Conference Publications
Fuzzy AHP based design decision for Product Line architecture,S. A. Halim; S. Deris; M. Z. M. Zaki,"Software Engineering Dept., Faculty of Computing, UniversitiTeknologi Malaysia, Johor, Malaysia",2014 8th. Malaysian Software Engineering Conference (MySEC),20141218,2014,,,119,124,"Product line architecture (PLA)is one of the important assets in Software Product Line (SPL). In PLA there are numerous variation points and variants in which software engineers can decide and select as the best or suitable enough design decision for the PLA. Therefore it's crucial for SPL to have an explicit architecture design decision representation to enable the important knowledge to be reused in several similar applications in the product line.Though there are many proposals towards incorporating design decision in SPL, however it is still lacking in dealing with multiple criteria and fuzziness of the PLA design decision. In this paper, we propose to overcome this problem by incorporating Fuzzy Analytical Hierarchical Process (FAHP) in the design decision. Our approach considers the use of architecturally significant requirements, which is the quality attributes in the design decision. We demonstrate our approach using the case study of Autonomous Mobile Robot Product Line (AMRPL). The findingstend to show a potential use of FAHP in the design decision.",,Electronic:978-1-4799-5439-1; POD:978-1-4799-5440-7; USB:978-1-4799-5438-4,10.1109/MySec.2014.6986000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6986000,Design Decision;Fuzzy AHP;domain architecture;domain requirements,Computer architecture;Decision making;Equations;Pragmatics;Programmable logic arrays;Software;Vectors,analytic hierarchy process;fuzzy set theory;software architecture;software product lines,AMRPL;FAHP;PLA;SPL;analytic hierarchy processing;autonomous mobile robot product line;fuzzy AHP based design decision;product line architecture;software product line,,0,,17,,no,23-24 Sept. 2014,,IEEE,IEEE Conference Publications
Fuzzy Cognitive Network Process: Comparisons With Fuzzy Analytic Hierarchy Process in New Product Development Strategy,K. K. F. Yuen,"Dept. of Comput. Sci. & Software Eng., Xi'an Jiaotong-Liverpool Univ., Suzhou, China",IEEE Transactions on Fuzzy Systems,20140529,2014,22,3,597,610,"Fuzzy analytic hierarchy process (F-AHP) has increasingly been applied in many areas. However, as the perception and cognition toward the semantic representation for the linguistic rating scale used by the fuzzy pairwise comparison in F-AHP are still open to discuss, F-AHP very likely produces misapplications. This research proposes the fuzzy cognitive network process (F-CNP) as an ideal alternative to F-AHP. A new product development application using F-AHP is revised using F-CNP. This study shows that the proposed F-CNP yields better results due to the appropriate mathematical definition of the fuzzy paired interval scale for the human perception of paired difference.",1063-6706;10636706,,10.1109/TFUZZ.2013.2269150,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6542700,Fuzzy analytic hierarchy process (F-AHP);fuzzy cognitive network process (F-CNP);fuzzy decision analysis;fuzzy pairwise comparison;fuzzy rating scale,,analytic hierarchy process;cognition;fuzzy set theory;product development,F-AHP;F-CNP;fuzzy analytic hierarchy process;fuzzy cognitive network process;fuzzy paired interval scale;fuzzy pairwise comparison;human perception;linguistic rating scale;product development strategy;semantic representation,,11,,49,,no,14-Jun,,IEEE,IEEE Journals & Magazines
Fuzzy qualitative evaluation of reliability of object oriented software system,S. K. Dubey; A. Mishra,"Dept. of Computer Science & Engineering, ASET, Amity University, Sec-125, Noida (U.P.), India",2014 International Conference on Advances in Engineering & Technology Research (ICAETR - 2014),20150119,2014,,,1,6,"Reliability is the key characteristics of any efficient software system. Most of the software system fails due to no-reliable in nature. Evaluating reliability is one of the main functions of the software system. Since last decades, the popularity of object oriented software system is increased in exponential ways. Many methods or models have been developed for reliability evaluation but few of them took considerations of object oriented features. In this paper, fuzzy approach is used to evaluate reliability. Due to reliable in nature, CK metrics are used in reliability measurement. Based on the case studies performed on object oriented software, it is concluded that we can choose reliable software very easily among various available software system.",2347-9337;23479337,Electronic:978-1-4799-6393-5; POD:978-1-4799-6394-2,10.1109/ICAETR.2014.7012813,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7012813,AHP;fuzzy;metrics;model;object-oriented;reliability;system,Measurement;Object oriented modeling;Software reliability;Software systems;Unified modeling language,analytic hierarchy process;object-oriented programming;software performance evaluation;software reliability,fuzzy qualitative evaluation;object oriented features;object oriented software system reliability;software system failure,,0,,28,,no,1-2 Aug. 2014,,IEEE,IEEE Conference Publications
Fuzzy-Set Based Sentiment Analysis of Big Social Data,R. R. Mukkamala; A. Hussain; R. Vatrapu,"IT Univ. of Copenhagen, Copenhagen, Denmark",2014 IEEE 18th International Enterprise Distributed Object Computing Conference,20141204,2014,,,71,80,"Computational approaches to social media analytics are largely limited to graph theoretical approaches such as social network analysis (SNA) informed by the social philosophical approach of relational sociology. There are no other unified modelling approaches to social data that integrate the conceptual, formal, software, analytical and empirical realms. In this paper, we first present and discuss a theory and conceptual model of social data. Second, we outline a formal model based on fuzzy set theory and describe the operational semantics of the formal model with a real-world social data example from Facebook. Third, we briefly present and discuss the Social Data Analytics Tool (SODATO) that realizes the conceptual model in software and provisions social data analysis based on the conceptual and formal models. Fourth, we use SODATO to fetch social data from the Facebook wall of a global brand, H&M and conduct a sentiment classification of the posts and comments. Fifth, we analyse the sentiment classifications by constructing crisp as well as the fuzzy sets of the artefacts (posts, comments, likes, and shares). We document and discuss the longitudinal sentiment profiles of artefacts and actors on the facebook page. Sixth and last, we discuss the analytical method and conclude with a discussion of the benefits of set theoretical approaches based on the social philosophical approach of associational sociology.",1541-7719;15417719,Electronic:978-1-4799-5470-4; POD:978-1-4799-5471-1,10.1109/EDOC.2014.19,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6972052,Big Social Data;Computational Social Science;Data Science;Formal Methods;Social Data Analytics,Analytical models;Data models;Facebook;Fuzzy sets;Media;Sentiment analysis,data analysis;fuzzy set theory;pattern classification;social networking (online);social sciences computing,Facebook page;H&M;SNA;SODATO;associational sociology;big social data;fuzzy set theory;fuzzy-set based sentiment analysis;global brand;real-world social data example;relational sociology;sentiment classification;social data;social data analytics tool;social media analytics;social network analysis;social philosophical approach,,7,,48,,no,1-5 Sept. 2014,,IEEE,IEEE Conference Publications
Gateways to high-perfomance and distributed computing resources for global health challenges,S. Gesing; J. Nabrzyski; S. Jha,"Center for Research Computing, University of Notre Dame, Indiana, USA",2014 IEEE Canada International Humanitarian Technology Conference - (IHTC),20150706,2014,,,1,5,"Computational simulations for disease modeling and efficient analysis tools using large data collections have become invaluable tools for Global Health programs fighting infectious diseases. Simulations are used in many ways e.g., from predicting the effectiveness of interventions for certain diseases through Bayesian-based data-model assimilation to genomic analysis of diverse vector species in their different growth states. Even though the approaches and technologies vary, they have several common requirements on the underlying infrastructure. Simulations for infectious diseases, for example, rely on environmental data like weather, geospatial data, biodiversity and transmission complexity. Data-intensive applications need efficient distributed data management capabilities facilitating replication services or Software-as-a-Service solutions. Such solutions might follow the paradigm to transfer applications to the data instead of transferring data to where the applications are deployed. In this paper we present our work towards providing a common extensible platform to build the computational investigation environment. This platform will provide an API for developers of science gateways, which can be adapted for specific simulations, various distributed data management technologies and diverse data structures. Furthermore, it will include metadata to increase the quality and the information about the data, its provenance and its context. Such an API will ease the development of new science gateways and the core technologies for both modeling and running the models. Developers can focus on the targeted domain and are relieved from re-developing core features for the underlying infrastructure. These gateways also enable the stakeholders (scientists, policy makers, etc.) in using the sophisticated tools and/or offer a single point of entry to large data collections and data analytics tools.",,CD-ROM:978-1-4799-3995-4; Electronic:978-1-4799-3996-1; POD:978-1-4799-3997-8,10.1109/IHTC.2014.7147530,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7147530,,Bioinformatics;Computational modeling;Data models;Diseases;Distributed databases;Logic gates;User interfaces,Bayes methods;application program interfaces;cloud computing;data structures;diseases;health care,API;Bayesian-based data-model assimilation;computational simulations;data analytics tools;data collections;data structures;disease modeling;distributed computing;environmental data;genomic analysis;global health programs;science gateways;software-as-a-service,,0,,27,,no,1-4 June 2014,,IEEE,IEEE Conference Publications
GE Brilliant wind farms,R. Burra; A. Ambekar; H. Narang; E. Liu; C. Mehendale; L. Thirer; K. Longtin; M. Shah; N. Miller,"GE Company, USA",2014 IEEE Symposium on Power Electronics and Machines for Wind and Water Applications,20140929,2014,,,1,10,"Since the Brilliant wind platform was launched in early 2013, GE has developed and commercialized a host of hardware and software features to increase wind farm Annual Energy Production, improve services productivity, and open up new customer revenue streams. By utilizing advanced control algorithms and analytics coupled with deep domain expertise in power electronics and grid integration, GE is creating more with less - getting more power and efficiency out of existing hardware, taking an inherently variable wind resource and integrating it more smoothly onto the grid, managing the complexity of multiple turbines within a farm and multiple farms within a grid system, and making wind predictable and reliable even in areas with weak infrastructure. By harnessing the power of the industrial internet GE will continue to develop innovative features within the Brilliant wind platform, transforming the wind industry as we tackle the next generation of challenges and opportunities.",,Electronic:978-1-4799-5138-3; POD:978-1-4799-1675-7,10.1109/PEMWA.2014.6912227,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6912227,,Automatic voltage control;Inductors;Random access memory;Wind farms,Internet;power electronics;power grids;wind power plants;wind turbines,GE brilliant wind farm;annual energy production;brilliant wind platform;control algorithm;customer revenue stream;grid integration;industrial internet GE;power electronics;wind industry;wind prediction;wind reliability;wind resource;wind turbine,,1,,,,no,24-26 July 2014,,IEEE,IEEE Conference Publications
Giving Text Analytics a Boost,R. Polig; K. Atasu; L. Chiticariu; C. Hagleitner; H. P. Hofstee; F. R. Reiss; H. Zhu; E. Sitaridi,,IEEE Micro,20140806,2014,34,4,6,14,"The amount of textual data has reached a new scale and continues to grow at an unprecedented rate. IBM's SystemT software is a powerful text-analytics system that offers a query-based interface to reveal the valuable information that lies within these mounds of data. However, traditional server architectures are not capable of analyzing so-called big data efficiently, despite the high memory bandwidth that is available. The authors show that by using a streaming hardware accelerator implemented in reconfigurable logic, the throughput rates of the SystemT's information extraction queries can be improved by an order of magnitude. They also show how such a system can be deployed by extending SystemT's existing compilation flow and by using a multithreaded communication interface that can efficiently use the accelerator's bandwidth.",0272-1732;02721732,,10.1109/MM.2014.69,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6871704,big data;field-programmable gate array;hardware;hardware accelerator;heterogeneous system;text analytics,Big data;Computer architecture;Field programmable gate arrays;Instruction sets;Text mining;Text processing,Big Data;multi-threading;query processing;reconfigurable architectures;text analysis;user interfaces,Big Data;IBM SystemT software;compilation flow;information extraction query-based interface;multithreaded communication interface;reconfigurable logic;streaming hardware accelerator bandwidth;text-analytics system;textual data;throughput rates,,2,,16,,no,July-Aug. 2014,,IEEE,IEEE Journals & Magazines
GPFS-based implementation of a hyperconverged system for software defined infrastructure,A. C. Azagury; R. Haas; D. Hildebrand; S. W. Hunter; T. Neville; S. Oehme; A. Shaikh,,IBM Journal of Research and Development,20140415,2014,58,3-Feb,6:01,6:12,"The need for an increasingly dynamic and more cost-efficient data-center infrastructure has led to the adoption of a software defined model that is characterized by: the creation of a federated control plane to judiciously allocate and control appropriate heterogeneous infrastructure resources in an automated fashion, the ability for applications to specify criteria, such as performance, capacity, and service levels, without detailed knowledge of the underlying infrastructure; and the migration of data-plane capabilities previously embodied as purpose-built devices or firmware into software running on a standard operating systems in commercial off-the-shelf servers. This last trend of hardware-based capabilities migrating to software is enabling yet another shift to hyperconvergence, which refers to merger of traditionally separate networking, compute, and storage capabilities in integrated system software. This paper examines the convergence of the software defined infrastructure stack, and introduces a hyperconverged compute and storage architecture, in which the IBM General Parallel File System (GPFSå¨) implements the software defined data plane that dynamically supports workloads ranging from high-I/O virtual desktop infrastructure applications to more compute-oriented analytics applications. The performance and scalability characteristics of this architecture are evaluated with a prototype implementation.",0018-8646;00188646,,10.1147/JRD.2014.2303321,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6798737,,Computer architecture;Costs;Data centers;Market research;Optimization;Servers;Software defined networks;Virtual machine monitors,,,,0,,,,no,March-May 2014,,IBM,IBM Journals & Magazines
Handling Big Data in medical imaging: Iterative reconstruction with large-scale automated parallel computation,J. H. Lee; Y. Yao; U. Shrestha; G. T. Gullberg; Y. Seo,"University of North Carolina, Chapel Hill, 27599 USA",2014 IEEE Nuclear Science Symposium and Medical Imaging Conference (NSS/MIC),20160314,2014,,,1,4,"The primary goal of this project is to implement the iterative statistical image reconstruction algorithm, in this case maximum likelihood expectation maximum (MLEM) used for dynamic cardiac single photon emission computed tomography, on Spark/GraphX. This involves porting the algorithm to run on large-scale parallel computing systems. Spark is an easy-toprogram software platform that can handle large amounts of data in parallel. GraphX is a graph analytic system running on top of Spark to handle graph and sparse linear algebra operations in parallel. The main advantage of implementing MLEM algorithm in Spark/GraphX is that it allows users to parallelize such computation without any expertise in parallel computing or prior knowledge in computer science. In this paper we demonstrate a successful implementation of MLEM in Spark/GraphX and present the performance gains with the goal to eventually make it useable in clinical setting.",,Electronic:978-1-4799-6097-2; POD:978-1-4799-6098-9,10.1109/NSSMIC.2014.7430758,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7430758,,,Big Data;computerised tomography;expectation-maximisation algorithm;image reconstruction;linear algebra;medical image processing;parallel processing,Big Data;GraphX;MLEM algorithm;Spark;dynamic cardiac single photon emission computed tomography;graph analytic system;iterative statistical image reconstruction algorithm;large-scale automated parallel computation;maximum likelihood expectation maximum;medical imaging;parallel computing;parallel computing systems;sparse linear algebra operations,,,,7,,no,8-15 Nov. 2014,,IEEE,IEEE Conference Publications
Hardware Partitioning for Big Data Analytics,L. Wu; R. J. Barker; M. A. Kim; K. A. Ross,"Comput. Sci. Dept., Columbia Univ., Santa Clara, CA, USA",IEEE Micro,20140610,2014,34,3,109,119,"Targeted deployment of hardware accelerators can improve the throughput and energy efficiency of large-scale data processing. Data partitioning is a critical operation for manipulating large datasets and is often the limiting factor in database performance. A hardware-software streaming framework offers a seamless execution environment for streaming accelerators such as the Hardware-Accelerated Range Partitioner (HARP). Together, the streaming framework and HARP provide an order of magnitude improvement in partitioning and energy performance.",0272-1732;02721732,,10.1109/MM.2014.11,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6762799,Accelerators;Bandwidth;Databases;Energy efficiency;Hardware;Large-scale systems;Servers;Throughput;accelerator;big data;data partitioning;hardware;microarchitecture;specialized functional unit;streaming data,Accelerators;Bandwidth;Databases;Energy efficiency;Hardware;Large-scale systems;Servers;Throughput,Big Data;data analysis;energy conservation;power aware computing,Big data analytics;data partitioning;database performance;energy efficiency improvement;hardware accelerators deployment;hardware partitioning;hardware-software streaming;large-scale data processing;range partitioning;throughput improvement,,2,,20,,no,May-June 2014,,IEEE,IEEE Journals & Magazines
Hardware-accelerated text analytics,R. Polig; K. Atasu; C. Hagleitner; L. Chiticariu; F. Reiss; H. Zhu; P. Hofstee,"IBM Research Zurich, Switzerland",2014 IEEE Hot Chips 26 Symposium (HCS),20160704,2014,,,1,24,Presents a collection of slides covering the following topics: SystemT text analytics software; hardware-accelerated SystemT; Big text Data; and field programmable gate array.,,Electronic:978-1-4673-8883-2; POD:978-1-4673-8884-9,10.1109/HOTCHIPS.2014.7478822,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7478822,,Information retrieval;Logic gates;Text analysis;Text mining;Text processing,Big Data;field programmable gate arrays;text analysis,Big text Data;FPGA;SystemT text analytics software;field programmable gate array;hardware-accelerated SystemT;hardware-accelerated text analytics,,,,,,no,10-12 Aug. 2014,,IEEE,IEEE Conference Publications
Heterogeneous Metric Learning with Content-Based Regularization for Software Artifact Retrieval,L. Wu; L. Du; B. Liu; G. Xu; Y. Ge; Y. Fu; J. Li; Y. Zhou; H. Xiong,"Comput. Network Inf. Center, Beijing, China",2014 IEEE International Conference on Data Mining,20150129,2014,,,610,619,"The problem of software artifact retrieval has the goal to effectively locate software artifacts, such as a piece of source code, in a large code repository. This problem has been traditionally addressed through the textual query. In other words, information retrieval techniques will be exploited based on the textual similarity between queries and textual representation of software artifacts, which is generated by collecting words from comments, identifiers, and descriptions of programs. However, in addition to these semantic information, there are rich information embedded in source codes themselves. These source codes, if analyzed properly, can be a rich source for enhancing the efforts of software artifact retrieval. To this end, in this paper, we develop a feature extraction method on source codes. Specifically, this method can capture both the inherent information in the source codes and the semantic information hidden in the comments, descriptions, and identifiers of the source codes. Moreover, we design a heterogeneous metric learning approach, which allows to integrate code features and text features into the same latent semantic space. This, in turn, can help to measure the artifact similarity by exploiting the joint power of both code and text features. Finally, extensive experiments on real-world data show that the proposed method can help to improve the performances of software artifact retrieval with a significant margin.",1550-4786;15504786,Electronic:978-1-4799-4302-9; POD:978-1-4799-4301-2,10.1109/ICDM.2014.147,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7023378,,Electronic mail;Feature extraction;Information retrieval;Measurement;Optimization;Semantics;Software,feature extraction;learning (artificial intelligence);query processing;source code (software);text analysis,artifact similarity;code feature;code repository;content-based regularization;feature extraction method;heterogeneous metric learning approach;information retrieval technique;latent semantic space;semantic information;software artifact retrieval;source code;text feature;textual query;textual representation;textual similarity,,0,,33,,no,14-17 Dec. 2014,,IEEE,IEEE Conference Publications
Hierarchical management of large-scale malware data,L. Kellogg; B. Ruttenberg; A. O'Connor; M. Howard; A. Pfeffer,"Charles River Analytics 625 Mt. Auburn St, Cambridge, MA, 02138",2014 IEEE International Conference on Big Data (Big Data),20150108,2014,,,666,674,"As the pace of generation of new malware accelerates, clustering and classifying newly discovered malware requires new approaches to data management. We describe our Big Data approach to managing malware to support effective and efficient malware analysis on large and rapidly evolving sets of malware. The key element of our approach is a hierarchical organization of the malware, which organizes malware into families, maintains a rich description of the relationships between malware, and facilitates efficient online analysis of new malware as they are discovered. Using clustering evaluation metrics, we show that our system discovers malware families comparable to those produced by traditional hierarchical clustering algorithms, while scaling much better with the size of the data set. We also show the flexibility of our system as it relates to substituting various data representations, methods of comparing malware binaries, clustering algorithms, and other factors. Our approach will enable malware analysts and investigators to quickly understand and quantify changes in the global malware ecosystem.",,Electronic:978-1-4799-5666-1; POD:978-1-4799-5667-8,10.1109/BigData.2014.7004290,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7004290,,Algorithm design and analysis;Big data;Clustering algorithms;Databases;Malware;Organizations,Big Data;data analysis;invasive software;pattern classification;pattern clustering,Big Data approach;clustering evaluation metrics;data representation;global malware ecosystem;hierarchical clustering algorithm;hierarchical data management;hierarchical malware organization;large-scale malware data;malware analysis;malware binaries;malware classification;malware clustering;malware geneation,,0,,23,,no,27-30 Oct. 2014,,IEEE,IEEE Conference Publications
High Performance and Fault Tolerant Distributed File System for Big Data Storage and Processing Using Hadoop,E. Sivaraman; R. Manickachezian,"Res. Dept. of Comput. Sci., NGM Coll., Pollachi, India",2014 International Conference on Intelligent Computing Applications,20141124,2014,,,32,36,Hadoop is a quickly budding ecosystem of components based on Google's MapReduce algorithm and file system work for implementing MapReduce algorithms in a scalable fashion and distributed on commodity hardware. Hadoop enables users to store and process large volumes of data and analyse it in ways not previously possible with SQL-based approaches or less scalable solutions. Remarkable improvements in conventional compute and storage resources help make Hadoop clusters feasible for most organizations. This paper begins with the discussion of Big Data evolution and the future of Big Data based on Gartner's Hype Cycle. We have explained how Hadoop Distributed File System (HDFS) works and its architecture with suitable illustration. Hadoop's MapReduce paradigm for distributing a task across multiple nodes in Hadoop is discussed with sample data sets. The working of MapReduce and HDFS when they are put all together is discussed. Finally the paper ends with a discussion on Big Data Hadoop sample use cases which shows how enterprises can gain a competitive benefit by being early adopters of big data analytics.,,Electronic:978-1-4799-3966-4; POD:978-1-4799-3967-1,10.1109/ICICA.2014.16,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6965006,Analytics;Big data;Fault tolerance;Hadoop;Hadoop Distributed File System (HDFS);Hype cycle;MapReduce;Replication;Unstructured data,Big data;Computer architecture;Fault tolerance;Fault tolerant systems;File systems;Mobile communication;Servers,Big Data;SQL;data analysis;fault tolerant computing;parallel processing;pattern clustering;software performance evaluation,Gartner's Hype Cycle;Google MapReduce algorithm;HDFS;Hadoop MapReduce paradigm;Hadoop clusters;Hadoop distributed file system;SQL-based approaches;big data analytics;big data evolution;big data processing;big data storage;high performance fault tolerant distributed file system;storage resources,,0,,13,,no,6-7 March 2014,,IEEE,IEEE Conference Publications
High Precision Screening for Android Malware with Dimensionality Reduction,B. Wolfe; K. Elish; D. Yao,"Inf. Analytics & Visualization Center, Indiana Univ.-Purdue Univ. Fort Wayne (IPFW), Fort Wayne, IN, USA",2014 13th International Conference on Machine Learning and Applications,20150209,2014,,,21,28,"We present a new method of classifying previously unseen Android applications as malware or benign. The algorithm starts with a large set of features: the frequencies of all possible n-byte sequences in the application's byte code. Principal components analysis is applied to that frequency matrix in order to reduce it to a low-dimensional representation, which is then fed into any of several classification algorithms. We utilize the implicitly restarted Lanczos bidiagonalization algorithm and exploit the sparsity of the n-gram frequency matrix in order to efficiently compute the low-dimensional representation. When trained upon that low-dimensional representation, several classification algorithms achieve higher accuracy than previous work.",,Electronic:978-1-4799-7415-3; POD:978-1-4799-7416-0; USB:978-1-4799-7414-6,10.1109/ICMLA.2014.10,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7033086,android;dimensionality reduction;mobile security;principal components analysis,Accuracy;Androids;Feature extraction;Humanoid robots;Malware;Principal component analysis;Sparse matrices,Android (operating system);invasive software;matrix algebra;pattern classification;principal component analysis,Android applications;Android malware;Lanczos bidiagonalization algorithm;application byte code;classification algorithm;dimensionality reduction;high precision screening;low-dimensional representation;n-byte sequences;n-gram frequency matrix;principal component analysis,,2,,24,,no,3-6 Dec. 2014,,IEEE,IEEE Conference Publications
High-frequency financial statistics with parallel R and Intel Xeon Phi coprocessor,J. Zou; H. Zhang,"Department of Mathematical Sciences, Worcester Polytechnic Institute",2014 IEEE International Conference on Big Data (Big Data),20150108,2014,,,61,69,"Financial statistics covers a wide array of applications in the financial world, such as (high frequency) trading, risk management, pricing and valuation of securities and derivatives, and various business and economic analytics. Portfolio allocation is one of the most important problems in financial risk management. One most challenging part in portfolio allocation is the tremendous amount of data and the optimization procedures that require computing power beyond the currently available desktop systems. In this article, we focus on the portfolio allocation problem using high-frequency financial data, and propose a hybrid parallelization solution to carry out efficient asset allocations in a large portfolio via intra-day high-frequency data. We exploit a variety of HPC techniques, including parallel R, Intel Math Kernel Library, and automatic offloading to Intel Xeon Phi coprocessor in particular to speed up the simulation and optimization procedures in our statistical investigations. Our numerical studies are based on high-frequency price data on stocks traded in New York Stock Exchange in 2011. The analysis results show that portfolios constructed using high-frequency approach generally perform well by pooling together the strengths of regularization and estimation from a risk management perspective. We also investigate the computation aspects of large-scale multiple hypothesis testing for time series data. Using a combination of software and hardware parallelism, we demonstrate a high level of performance on high-frequency financial statistics.",,Electronic:978-1-4799-5666-1; POD:978-1-4799-5667-8,10.1109/BigData.2014.7004414,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7004414,Intel Xeon Phi Coprocessor;high-frequency financial analysis;massive parallelism;parallel R,Libraries;Optimization;Parallel processing;Portfolios;Resource management;Risk management;Vectors,asset management;coprocessors;financial data processing;investment;optimisation;parallel processing;pricing;risk management;stock markets;time series,HPC techniques;Intel Math Kernel Library;Intel Xeon Phi coprocessor;New York Stock Exchange;asset allocations;automatic offloading;business analytics;economic analytics;financial risk management;financial world;hardware parallelism;high frequency trading;high-frequency financial data;high-frequency financial statistics;high-frequency price data;hybrid parallelization solution;intra-day high-frequency data;large-scale multiple hypothesis testing;optimization procedures;parallel R;portfolio allocation problem;pricing;securities valuation;software parallelism;stocks trading;time series data,,0,,44,,no,27-30 Oct. 2014,,IEEE,IEEE Conference Publications
iCARE: A framework for big data-based banking customer analytics,N. Sun; J. G. Morris; J. Xu; X. Zhu; M. Xie,,IBM Journal of Research and Development,20141121,2014,58,6-May,4:01,4:09,"The amount of data stored by banks is rapidly increasing and provides the opportunity for banks to conduct predictive analytics and enhance its businesses. However, data scientists are facing large challenges, handling the massive amount of data efficiently and generating insights with real business value. In this paper, the Intelligent Customer Analytics for Recognition and Exploration (iCARE) framework is presented to analyze banking customer behaviors from banking big data, through analytical modeling methodologies and techniques designed for a key business scenario. Combining IBM software platforms and big data processing power with customized data analytical models, the iCARE solution provides deeper customer insights to satisfy a bank's specific business need and data environment. The advantages of the iCARE framework have been confirmed in a real case study of a bank in southeast China. In this case, iCARE helps generate insights for active customers based on their transaction behavior, using close to 20 terabytes of data.",0018-8646;00188646,,10.1147/JRD.2014.2337118,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6964895,,Analytical models;Banking industry;Big data;Data models;Electronic commerce;Finance,,,,1,,,,no,Sept.-Nov. 2014,,IBM,IBM Journals & Magazines
Implementation of a cognitive radar perception/action cycle,D. M. Zasada; J. J. Santapietro; L. D. Tromp,"Nat. Security Eng. Center, MITRE Corp., Rome, NY, USA",2014 IEEE Radar Conference,20140814,2014,,,544,547,"Herein we demonstrate that a cognitive capability can be readily added to modern digital radars. Such radars already consist of adaptive electronically scanned array front ends (often with built-in on-array processing) driven by back ends composed of modern flexible signal/data processor clusters plus receiver-exciters with arbitrary waveform generation capability. Using these as building blocks, a cognitive cycle can be readily constructed. The cognitive paradigm uses these existing hardware capabilities common to modern radars but implemented with new drivers; specifically a cognitive software overlay cycle linking advanced near-real-time analytic techniques. The authors believe that this investigation, which focused on practical existing implementations of the component algorithms to produce a straightforward cognitive cycle, will help convince the radar community that the Cognitive Radar paradigm merits further investigation.",1097-5659;10975659,CD-ROM:978-1-4799-2034-1; Electronic:978-1-4799-2035-8; POD:978-1-4799-2036-5,10.1109/RADAR.2014.6875651,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875651,,Arrays;Cognition;Radar antennas;Radar signal processing;Radar tracking;Receivers,cognitive radio;radar receivers,adaptive electronically scanned array front ends;arbitrary waveform generation;array processing;cognitive capability;cognitive radar paradigm;cognitive radar perception-action cycle;cognitive software overlay cycle link;digital radars;flexible signal-data processor clusters;near-real-time analytic techniques;radar community;receiver exciters,,1,,7,,no,19-23 May 2014,,IEEE,IEEE Conference Publications
Implementation of continuous integration and automated testing in software development of smart grid scheduling support system,J. Lu; Z. Yang; J. Qian,"R&D Center, NARI Technology Co., Ltd. Nanjing 211106, China",2014 International Conference on Power System Technology,20141222,2014,,,2441,2446,"When smart grid scheduling support system (D5000 system) was developed, the development team ran across tough issue that the system is difficult for integration and becomes unstable after integration due to the complexity. To resolve the problem, the author made research and introduced continuous integration and automated testing approach on D5000 system development. This paper provides the concept and advantages of continuous integration, and analyzes the necessity for continuous integration. It also describes automated testing for quality improvement with code static analytics, automated unit testing, and automated function testing; This paper gives a case study to deploy continuous integration and automated testing on D5000 system development which resolves quality and integration issues effectively and efficiently.",,Electronic:978-1-4799-5032-4; POD:978-1-4799-5033-1,10.1109/POWERCON.2014.6993503,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6993503,automated testing;continuous integration;smart grid,Graphical user interfaces;Java;Market research;Servers;Smart grids;Software;Testing,power engineering computing;scheduling;smart power grids;software engineering,D5000 system;automated function testing;automated testing;automated testing approach;automated unit testing;code static analytics;smart grid scheduling support system;software development,,0,,16,,no,20-22 Oct. 2014,,IEEE,IEEE Conference Publications
Implications of Learning Analytics for Serious Game Design,J. B. Hauge; R. Berta; G. Fiucci; B. F. ManjÌ_n; C. PadrÌ_n-NÌÁpoles; W. Westra; R. Nadolski,"IKAP, Bremer Inst. fur Produktion und Logistik, Bremen, Germany",2014 IEEE 14th International Conference on Advanced Learning Technologies,20140922,2014,,,230,232,"This paper addresses the implications of combining learning analytics and serious games for improving game quality, monitoring and assessment of player behavior, gaming performance, game progression, learning goals achievement, and user's appreciation. We introduce two modes of serious games analytics: in-game (real time) analytics, and post-game (off-line) analytics. We also explain the GLEANER framework for in-game analytics and describe a practical example for off-line analytics. We conclude with a brief outlook on future work, highlighting opportunities and challenges towards a solid uptake of SGs in authentic educational and training settings.",2161-3761;21613761,Electronic:978-1-4799-4038-7; POD:978-1-4799-4037-0,10.1109/ICALT.2014.73,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6901445,Assessment;Game design;Learning analytics;Performance;Serious games,Data models;Educational institutions;Engines;Games;Monitoring;Training,computer based training;computer games;monitoring;quality control;user interfaces,GLEANER framework;educational settings;game progression;game quality;gaming performance;learning analytics;learning goals achievement;monitoring;player behavior;serious game design;training settings;user appreciation,,2,,13,,no,7-10 July 2014,,IEEE,IEEE Conference Publications
Improved analytical multiphysical modeling of a surface PMSM,A. L. RodrÌ_guez; D. J. GÌ_mez; I. Villar; A. LÌ_pez-de-Heredia; I. Etxeberria-Otadui,"IK4-IKERLAN P&#x00BA;. J.M. Arizmendiarrieta 2, 20500 Arrasate-Mondrag&#x00F3;n, Spain",2014 International Conference on Electrical Machines (ICEM),20141120,2014,,,1224,1230,"Permanent Magnet Synchronous Machines (PMSMs) are complex systems where a great amount of physical phenomena are produced simultaneously. Most of the existing PMSMs models are based on empirical formulations and standard design rules which are not suitable for high-performance applications or optimized design processes. The aim of this paper is to present an improved PMSM model which offers a holistic, multiphysic, modular and very fast approach capable of supporting a subsequent optimized design methodology. A complete multiphysic analysis which takes into account a coupled and analytic modeling of the magnetic, electrical, thermal and vibro-acoustics domains will be fully explained and applied to model a 10-poles 12-slots (Q12p5) PMSM. The achieved results are compared with those obtained in commercial software (FLUX2Då¨, ANSYSå¨ and Motor-CADå¨) getting high accuracy.",,Electronic:978-1-4799-4389-0; POD:978-1-4799-4388-3; USB:978-1-4799-4775-1,10.1109/ICELMACH.2014.6960338,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6960338,Analytical multiphysic modeling;permanent magnet machines;thermal analysis;vibrations,Analytical models;Atmospheric modeling;Iron;Magnetic domains;Magnetic flux;Rotors;Stators,permanent magnet machines;synchronous machines,ANSYS;FLUX2D;Motor-CAD;analytical multiphysical modeling;commercial software;coupled modeling;electrical domains;high-performance applications;magnetic domains;permanent magnet synchronous machines;surface PMSM;thermal domains;vibro-acoustics domains,,0,,25,,no,2-5 Sept. 2014,,IEEE,IEEE Conference Publications
Improved low cost induction motor control for stand alone solar pumping,M. Miladi; A. Ben Abdelghani-Bennani; I. Slama-Belkhodja; H. M'Saad,"Universit&#x00E9; de Tunis El Manar, Ecole Nationale d'Ing&#x00E9;nieurs de Tunis, LR 11 ES 15, Laboratoire des Syst&#x00E8;mes Electriques, BP 37. 1002, Tunis le Belv&#x00E9;d&#x00E8;re, Tunisie",2014 International Conference on Electrical Sciences and Technologies in Maghreb (CISTEM),20150402,2014,,,1,8,"Control strategy for standalone solar pumping system based on induction motor and without DC/DC converter has been widely studied and discussed in the literature. This topology is of great concern due its economic issues, especially when a standard frequency converter (SFCs) with scalar control is used instead of a dedicated PV inverter. This paper proposes an external control module to generate SFCs frequency reference in order to ensure both maximum power point tracking (MPPT) and optimize IM power. It is a low cost solution since it requires no additional power equipment. Modeling and design of each system parts are performed to determine the analytic expression of frequency reference. The effectiveness of the proposed approach is illustrated by simulations carried out under PSIM Software, and validated through experimental investigations on a 1.5kW laboratory set-up.",,Electronic:978-1-4799-7300-2; POD:978-1-4799-7301-9,10.1109/CISTEM.2014.7076955,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7076955,MPPT;PV pumping;induction machine;meterological data;statoric frequency f<inf>s</inf>,Arrays;Induction motors;Mathematical model;Maximum power point trackers;Pumps;Stators,frequency convertors;induction motors;invertors;machine control;maximum power point trackers,DC-DC converter;MPPT;PSIM software;SFC;induction motor control;maximum power point tracking;potovoltaic inverter;power 1.5 kW;stand alone solar pumping;standard frequency converter,,0,,18,,no,3-6 Nov. 2014,,IEEE,IEEE Conference Publications
Improved Priority Based Job Scheduling Algorithm in Cloud Computing Using Iterative Method,S. J. Patel; U. R. Bhoi,"Comput. Sci. & Eng. Dept., Parul Inst. of Technol., Vadodara, India",2014 Fourth International Conference on Advances in Computing and Communications,20140929,2014,,,199,202,"Cloud Computing is a platform for computing resources (Hardware and Software) that are delivered as a service over an internet network to the customers. Its intention is to share large scale equipments and resources for computation, storage, information and knowledge for scientific researches. There are many jobs that are required to be executed by the available resources to achieve best performance, minimal total time for completion, shortest response time, utilization of resource usage and etc. Because of these different objectives and high performance of computing environment, we need to design, develop, and propose a scheduling algorithm that outperforms appropriate allocation map of jobs due to different factors. Job scheduling is one of the major issue in cloud computing environment. In job scheduling priority is the biggest issue because some jobs need to be scheduled first then all other remaining jobs which can wait for a long time. In this paper, we have proposed an improvement in priority based job scheduling algorithm in cloud computing which is based on multiple criteria and multiple attribute decision making model.",,Electronic:978-1-4799-4363-0; POD:978-1-4799-4362-3,10.1109/ICACC.2014.55,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6906024,Analytic Hierarchy Process (AHP);Cloud Computing;Job Scheduling;Priority,Algorithm design and analysis;Analytic hierarchy process;Cloud computing;Scheduling;Scheduling algorithms;Vectors,cloud computing;decision making;iterative methods;resource allocation;scheduling,Internet network;cloud computing;iterative method;job allocation map;multiple attribute decision making model;multiple criteria decision making model;priority based job scheduling algorithm,,1,,13,,no,27-29 Aug. 2014,,IEEE,IEEE Conference Publications
Improving Performance of Forensics Investigation with Parallel Coordinates Visual Analytics,W. B. Wang; M. L. Huang; L. Lu; J. Zhang,"Fac. of Eng. & IT, Univ. of Technol., Sydney, NSW, Australia",2014 IEEE 17th International Conference on Computational Science and Engineering,20150129,2014,,,1838,1843,"Computer forensics investigators aim to analyse and present facts through the examination of digital evidences in short times. As the volume of suspicious data is becoming large, the difficulties of catching the digital evidence in a legally acceptable time are high. This paper proposes an effective method for reducing investigation time redundancy to achieve the normalization of data on hard disk drives (HDD) for computer forensics. We use visualization techniques, parallel coordinates, to analyse data instead of using data analysis algorithms only, and also choose a Red-Black tree structure to de-duplicate data. It reduces the time complexity, including the time spent of searching data, adding data as well as deleting data. We show the advantages of our approach, moreover, we demonstrate how this method can enhance the efficiency and quality of computer forensics task.",,Electronic:978-1-4799-7981-3; POD:978-1-4799-7982-0,10.1109/CSE.2014.337,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7023848,Computer Forensics;Digital Evidence;Red-Black Tree;Visuaization Techniques;parallel coordinates,Analytical models;Computers;Data models;Data visualization;Forensics;Hard disks;Image color analysis,computational complexity;data analysis;data visualisation;digital forensics;tree data structures;trees (mathematics),HDD;Red-Black tree structure;computer forensics investigation;data addition;data analysis;data deduplication;data deletion;data normalization;data searching;digital evidence examination;hard disk drives;investigation time redundancy;parallel coordinates visual analytics;performance improvement;suspicious data;time complexity reduction;visualization techniques,,0,,33,,no,19-21 Dec. 2014,,IEEE,IEEE Conference Publications
In unity there is strength: Showcasing a unified big data platform with MapReduce Over both object and file storage,R. Zhang; D. Hildebrand; R. Tewari,IBM Research - Almaden,2014 IEEE International Conference on Big Data (Big Data),20150108,2014,,,960,966,"Big Data platforms often need to support emerging data sources and applications while accommodating existing ones. Since different data and applications have varying requirements, multiple types of data stores (e.g. file-based and object-based) frequently co-exist in the same solution today without proper integration. Hence cross-store data access, key to effective data analytics, can not be achieved without laborious application re-programming, prohibitively expensive data migration, and/or costly maintenance of multiple data copies. We address this vital issue by introducing a first unified big data platform over heterogeneous storage. In particular, we present a prototype joining Apache Hadoop MapReduce with OpenStack's open-source object store Swift and IBM's cluster file system GPFS<sup>TM</sup>. A sentiment analysis application using 3 months of real Twitter data is employed to test and showcase our prototype. We have found that our prototype achieves 50% data capacity savings, eliminates data migration overhead, offers stronger reliability and enterprise support. Through our case study, we have learned important theoretical lessons concerning performance and reliability, as well as practical ones related to platform configuration. We have also identified several potentially high-impact research directions.",,Electronic:978-1-4799-5666-1; POD:978-1-4799-5667-8,10.1109/BigData.2014.7004328,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7004328,,Big data;Portfolios;Protocols;Prototypes;Reliability;Sentiment analysis;Twitter,Big Data;parallel programming;public domain software;social networking (online),Apache Hadoop MapReduce;GPFS IBM cluster file system;OpenStack open-source object store;Swift;Twitter data;cross-store data access;data analytics;data applications;data capacity savings;data migration overhead elimination;data sources;enterprise support;file-based data stores;heterogeneous storage;object-based data stores;platform configuration;reliability analysis;sentiment analysis application;unified Big Data platform,,0,,22,,no,27-30 Oct. 2014,,IEEE,IEEE Conference Publications
In‰öÑbug: Visual analytics of bug repositories,T. Dal Sasso; M. Lanza,"REVEAL @ Fac. of Inf., Univ. of Lugano, Lugano, Switzerland","2014 Software Evolution Week - IEEE Conference on Software Maintenance, Reengineering, and Reverse Engineering (CSMR-WCRE)",20140227,2014,,,415,419,"Bug tracking systems are used to track and store the defects reported during the life of software projects. The underlying repositories represent a valuable source of information used for example for defect prediction and program comprehension. However, bug tracking systems present the actual bugs essentially in textual form, which is not only cumbersome to navigate, but also hinders the understanding of the intricate pieces of information that revolve around software bugs. We present in*Bug, a web-based visual analytics platform to navigate and inspect bug repositories. in*Bug provides several interactive views to understand detailed information about the bugs and the people that report them. The tool can be downloaded at http://inbug.inf.usi.ch",,Electronic:978-1-4799-3752-3; POD:978-1-4799-3753-0,10.1109/CSMR-WCRE.2014.6747208,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6747208,,Complexity theory;Computer bugs;Data visualization;Navigation;Software;Visual analytics,data visualisation;information storage;program debugging;software tools,Web-based visual analytics platform;bug repositories;bug tracking systems;defect prediction;in*Bug;information repositories;program comprehension;software bugs;software projects,,0,,10,,no,3-6 Feb. 2014,,IEEE,IEEE Conference Publications
Individual decision model for urban regional land planning,A. Fahrul; Sumaryono; S. Lambang; R. Afif,"Comput. Sci. Dept., Mulawarman Univ., Samarinda, Indonesia","2014 The 1st International Conference on Information Technology, Computer, and Electrical Engineering",20150326,2014,,,260,265,"One example of the problems that affect many people's lives is urban regional land planning. Required a collaboration mechanism which based on the ability of decision analysis in determining the spatial policy of the use of the land. This paper contains an individual decision model by using Analytical Hierarchical Process-AHP technique. The model is applied to the case of the selection of the best location Green Open Spaces Samarinda City. Among the eight criteria, namely C1 (land area) is the criterion with the highest weight (0.208), followed by C3 (population=0.152) and then C2 (land prices=143) and so on. Location of Citra Niaga is the most suitable to serve as a green open space, followed by the Ex Kaltim, Mahakam edge, the port area and the around warehouses.",,Electronic:978-1-4799-6432-1; POD:978-1-4799-6433-8,10.1109/ICITACEE.2014.7065753,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7065753,AHP;Green Open Space;Individual Decision Model;MCDA;Samarinda;Urban Regional Land Planning,Analytical models;Biological system modeling;Cities and towns;Communities;Companies;Green products;Software,analytic hierarchy process;decision theory;town and country planning,AHP technique;Ex Kaltim;Green Open Spaces Samarinda City;Mahakam edge;analytical hierarchical process;collaboration mechanism;decision analysis;individual decision model;land area;land prices;population;port area;spatial policy;urban regional land planning;warehouses,,0,,9,,no,8-8 Nov. 2014,,IEEE,IEEE Conference Publications
Integration and Virtualization of Relational SQL and NoSQL Systems Including MySQL and MongoDB,R. Lawrence,"Dept. of Comput. Sci., Univ. of British Columbia, Kelowna, BC, Canada",2014 International Conference on Computational Science and Computational Intelligence,20140529,2014,1,,285,290,"NoSQL databases are growing in popularity for Big Data applications in web analytics and supporting large web sites due to their high availability and scalability. Since each NoSQL system has its own API and does not typically support standards such as SQL and JDBC, integrating these systems with other enterprise and reporting software requires extra effort. In this work, we present a generic standards-based architecture that allows NoSQL systems, with specific focus on MongoDB, to be queried using SQL and seamlessly interact with any software supporting JDBC. A virtualization system is built on top of the NoSQL sources that translates SQL queries into the source-specific APIs. The virtualization architecture allows users to query and join data from both NoSQL and relational SQL systems in a single SQL query. Experimental results demonstrate that the virtualization layer adds minimal overhead in translating SQL to NoSQL APIs, and the virtualization system can efficiently perform joins across sources.",,Electronic:978-1-4799-3010-4; POD:978-1-4799-3011-1; USB:978-1-4799-3009-8,10.1109/CSCI.2014.56,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6822123,Big Data;MongoDB;NoSQL;integration;virtualization,Computer architecture;Databases;Engines;Software;Standards;Virtualization,Big Data;SQL;Web sites;application program interfaces;relational databases;virtualisation,API;JDBC;MongoDB;NoSQL system;Web analytics;big data applications;generic standards-based architecture;large Web sites;relational SQL system;single SQL query;virtualization system,,4,3,21,,no,10-13 March 2014,,IEEE,IEEE Conference Publications
Intersection of the Cloud and Big Data,E. Collins,,IEEE Cloud Computing,20140710,2014,1,1,84,85,"The two biggest trends in the data center today are cloud computing and big data. This column will examine the intersection of the two. Industry hype has resulted in nebulous definitions for each, so I'll start by defining terms.",2325-6095;23256095,,10.1109/MCC.2014.12,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6848687,AaaS;Big Data;PaaS;SaaS;analytics as a service;cloud;cloud computing;converged analytics;platform as a service;software as a service,Big data;Cloud computing;Context awareness;Market research;Pricing;Service computing;Software as a service,Big Data;cloud computing,big data;cloud computing;industry hype,,3,,,,no,14-May,,IEEE,IEEE Journals & Magazines
Introduction to Social Media and E-Business Transformation Minitrack,C. M. K. Cheung; M. K. O. Lee; C. Wagner,,2014 47th Hawaii International Conference on System Sciences,20140310,2014,,,550,550,"Social media are online platforms that facilitate global collaboration and sharing amongst users. New social media applications in e-business and e-commerce appear on a daily basis and result in enormous shocks to the ecosystem of individuals and businesses. This minitrack provides a forum for the exchange of research ideas and best practices related to social media in e-business environments. It also aims to raise awareness in terms of the latest developments in social media, and address the challenges of using social media. This year, eight papers were selected for inclusion in the proceedings. The first paper, ""Social Media at Socio Systems Inc.: A Socio-technical Systems Analysis of Strategic Action"" by Don Heath, Rahul Singh and Jai Ganesh proposes an analytic framework to explain organizational strategies for directed action in social media. The next paper by Eric T.K. Lim, Dianne Cyr, and Chee-Wee Tan, ""Understanding Members' Attachment to Social Networking Sites: An Empirical Investigation of Three Theories"", constructs a theoretical model of members' communal attachments within SNSs. The model is then empirically validated via an online survey of 787 active members of SNSs. Drawing from the push-pull-mooring model and uses and gratification theory, Fei Liu and Bo Xiao proposed and empirically tested a theoretical model explaining SNS users' switching behavior in their paper, ""Do I Switch? Understanding Users' Intention to Switch between Social Network Sites"". The fourth paper by Alexander Richte, David Wagner and Andrea Back, ""Leadership 2.0: Engaging and Supporting Leaders in the Transition Towards a Networked Organization"", illustrates the concept of Leadership 2.0 through a series of interviews with the persons who are responsible for the implementation of social software at publicly listed, multinational organizations in Germany. The next paper, ""Understanding Information Adopt- on in Online Review Communities: The Role of Herd Factors"" by Xiao-Liang Shen, Kem Z.K. Zhang, and Sesia J. Zhao, extends prior research on information adoption by incorporating the perspective of herd behavior to explain the influence of massive online reviews in online communities. The research model was empirically tested with 376 users of a Chinese online review community. ""Impact of Online Firm Generated Content (FGC) on Supply Chain Performance: An Exploratory Empirical Analysis"", by Ajaya Swain and Qing Cao uses an advanced sentiment analysis approach to examine the impact of FGC effect on supply chain performance. Information sharing and collaboration are identified as two key FGC elements affecting supply chain performance. Based on an experimental investigation of the judgment ability of 478 subjects, Christian Wagner and Ayoung Suh found that collective size and expertise transfer effects are moderated by task difficulty and are strongest for tasks in a medium difficulty range in their paper, ""The Wisdom of Crowds: Impact of Collective Size and Expertise Transfer on Collective Performance"". The final paper, ""Assessing the Effects of Navigation Support and Group Structure on Collaborative Online Consumers' Consensus and Mutual Understanding"" by Yanzhen Yue and Zhenhui (Jack) Jiang, explores an emerging phenomenon of collaborative online shopping by investigating the effects of navigation support and group structure on collaborative online consumers' consensus and mutual understanding. We thank the authors for submitting their work to make this another engaging minitrack. We hope you enjoy the papers and their presentation at the conference.",1530-1605;15301605,Electronic:978-1-4799-2504-9; POD:978-1-4799-2505-6,10.1109/HICSS.2014.75,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6758671,,Cities and towns;Collaboration;Communities;Educational institutions;Media;Supply chains;Switches,,,,0,,,,no,6-9 Jan. 2014,,IEEE,IEEE Conference Publications
Janus -- Analytics-Driven Transition Planner,M. Belhe; K. Shrivastava; M. Natu; V. Sadaphal,"Tata Res. Dev. & Design Centre, Pune, India",2014 IEEE International Conference on Data Mining,20150129,2014,,,719,724,"In this paper, we address the problem of transition of IT operations from one service provider to another. We present analytics-driven solutions to generate a transition plan while addressing various aspects such as coverage, risk, time, and cost. We model the IT operations through graphs and use the well defined problems in graph theory to build solutions for transition planner. We demonstrate the proof-of-concept of proposed ideas using a real-world case-study.",1550-4786;15504786,Electronic:978-1-4799-4302-9; POD:978-1-4799-4301-2,10.1109/ICDM.2014.79,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7023390,Data center management;IT Operations;Transition,Business;Communities;Databases;Graph theory;Hardware;IEEE Potentials;Software,business data processing;graph theory,IT operations;Janus;analytics-driven transition planner;graph theory;service provider,,0,,12,,no,14-17 Dec. 2014,,IEEE,IEEE Conference Publications
Keynote 1: Visualization for Software Analytics by Margaret-Anne (Peggy) Storey,M. A. Storey,"Dept. of Comput. Sci., Univ. of Victoria, Victoria, BC, Canada",2014 Second IEEE Working Conference on Software Visualization,20141211,2014,,,xiv,xiv,"The popularity of software visualization research over the past 30 years has led to innovative techniques that are now seeing widespread adoption by professional software practitioners. But this research has barely kept pace with some of the radical changes occurring in software engineering today. In this talk, I explore current trends in software engineering, including the prevalence of software ecosystems and software delivery as a service, and the emergence of the social coder within a participatory development culture. I will also discuss how the field of software analytics has matured and seeks to support practitioners in improving software quality, user experience and developer productivity through data-driven tasks. Finally, I suggest that software visualization should be playing a bigger role in these recent trends, emphasizing that interactive visualizations are poised to play a critical role in the field of software analytics.",,Electronic:978-1-4799-6150-4; POD:978-1-4799-6151-1,10.1109/VISSOFT.2014.10,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6980204,,,data visualisation;interactive systems;software quality,data-driven tasks;developer productivity improvement;interactive visualizations;participatory development culture;social coder;software analytics visualization;software delivery as a service;software ecosystems;software engineering;software quality improvement;user experience improvement,,0,,,,no,29-30 Sept. 2014,,IEEE,IEEE Conference Publications
KWIVER: An open source cross-platform video exploitation framework,K. Fieldhouse; M. J. Leotta; A. Basharat; R. Blue; D. Stoup; C. Atkins; L. Sherrill; B. Boeckel; P. Tunison; J. Becker; M. Dawkins; M. Woehlke; R. Collins; M. Turek; A. Hoogs,"Kitware, Inc., 28 Corporate Drive, Clifton Park, NY 12065",2014 IEEE Applied Imagery Pattern Recognition Workshop (AIPR),20150216,2014,,,1,4,"We introduce KWIVER, a cross-platform video exploitation framework that Kitware has begun releasing as open source. Kitware is utilizing a multi-tiered open-source approach to reach as wide an audience as possible. Kitware's government-funded efforts to develop critical defense technology will be released back to the defense community via Forge.mil, a government open source repository. Infrastructure, algorithms, and systems without release restrictions will be provided to the larger video analytics community via kwiver.org and GitHub. Our goal is to provide a video analytics technology baseline for repeatable and reproducible experiments and to serve as a framework for the development of computer vision and machine learning systems. We hope that KWIVER will provide a focal point for collaboration and contributions from groups across the community.",1550-5219;15505219,Electronic:978-1-4799-5921-1; POD:978-1-4799-5922-8,10.1109/AIPR.2014.7041910,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7041910,,Communities;Computer vision;Government;Learning systems;Libraries;Software,computer vision;learning (artificial intelligence);video signal processing,GitHub;KWIVER;Kitware;computer vision;government open source repository;machine learning system;multi-tiered open-source approach;open source cross-platform video exploitation framework;video analytics community,,0,,17,,no,14-16 Oct. 2014,,IEEE,IEEE Conference Publications
Learning Analytics on federated remote laboratories: Tips and techniques,P. OrduÌ±a; A. Almeida; D. LÌ_pez-de-IpiÌ±a; J. Garcia-Zubia,"DeustoTech - Deusto Inst. of Technol., Univ. of Deusto, Bilbao, Spain",2014 IEEE Global Engineering Education Conference (EDUCON),20140605,2014,,,299,305,"A remote laboratory is a software and hardware tool which enables students to use real equipment -located in an educational institution- through the Internet. This way, students can experiment as if they were using the laboratories with their own hands. And, depending on the design, instructors can later see the results of these students. During the last decade, federation protocols to share remote laboratories have emerged. The focus of these protocols is to be make remote laboratories of one institution available in other in an automated manner, through institutional contracts. And these federation protocols usually rely on existing Remote Laboratory Management Systems (RLMS), which usually provide APIs for tracking student usage. At the same time, the interest on Learning Analytics is increasing. Learning Analytics focuses on the measurement and analysis of data about learners in their context. In the particular context of federated remote laboratories, new challenges arise: on the one hand, remote laboratories must be prepared to track insightful information from the student session so as to extract patterns, and on the other hand, the usage of a federated environment requires different degrees of anonymity. This contribution describes the new Learning Analytics dashboard of WebLab-Deusto, detailing what information can be extracted and how the usage of a RLMS simplifies the development of such tools in a federated environment.",2165-9559;21659559,Electronic:978-1-4799-3191-0; POD:978-1-4799-3192-7; USB:978-1-4799-3190-3,10.1109/EDUCON.2014.6826107,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6826107,,Context;Educational institutions;Engineering education;Internet;Protocols;Remote laboratories,Internet;computer aided instruction;data analysis;distance learning;educational administrative data processing;laboratory techniques;student experiments;virtual instrumentation,Internet;RLMS;Remote Laboratory Management Systems;WebLab-Deusto;educational institution;federated remote laboratories;hardware tool;learning analytics dashboard;software tool,,4,,60,,no,3-5 April 2014,,IEEE,IEEE Conference Publications
Learning Methods for Rating the Difficulty of Reading Comprehension Questions,D. Hutzler; E. David; M. Avigal; R. Azoulay,"Dept. of Math. & Comput. Sci., Open Univ. of Israel, Raanana, Israel","2014 IEEE International Conference on Software Science, Technology and Engineering",20140901,2014,,,54,62,"This work deals with an Intelligent Tutoring System (ITS) for reading comprehension. Such a system could promote reading comprehension skills. An important step towards building a full ITS for reading comprehension is to build an automated ranking system that will assign a hardness level to questions used by the ITS. This is the main concern of this work. For this purpose we, first, had to define the set of criteria that determines the rate of difficulty of a question. Second, we prepared a bank of questions that were rated by a panel of experts using the set of criteria defined above. Third, we developed an automated rating software based on the criteria defined above. In particular, we considered and compared different machine learning techniques for the ranking system of the third part of the process: Artificial Neural Network (ANN), Support Vector Machine (SVM), decision tree and naiíöve Bayesian network. The definition of the criteria set for rating a question's difficulty, and the development of an automated software for rating a questions' difficulty, contribute to a tremendous advancement in the ITS domain for reading comprehension by providing a uniform, objective and automated system for determining a question's difficulty.",,Electronic:978-1-4799-4432-3; POD:978-1-4799-4431-6,10.1109/SWSTE.2014.16,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6887542,Evaluation methodologies;Intelligent Tutoring Systems;Machine Learning and Analytics,Artificial neural networks;Data mining;Educational institutions;Learning systems;Software;Support vector machines;Taxonomy,Bayes methods;decision trees;intelligent tutoring systems;learning (artificial intelligence);neural nets;support vector machines,ANN;ITS;SVM;artificial neural network;automated ranking system;automated rating software;decision tree;intelligent tutoring system;learning methods;machine learning techniques;naiíöve Bayesian network;reading comprehension questions;support vector machine,,0,,22,,no,11-12 June 2014,,IEEE,IEEE Conference Publications
Linking Social Media with Open Innovation: An Intelligent Model,S. Li; J. Z. Li,"Westminster Bus. Sch., Univ. of Westminster, London, UK",2014 7th International Conference on Intelligent Computation Technology and Automation,20150108,2014,,,331,335,"A hybrid intelligent model for linking social media with open innovation strategies, processes and diffusion is proposed and discussed in this paper. In order to deal with the various facets or properties of the open innovation problem, we recommend and present a new paradigm and framework for integrating the strengths or advantages of intelligent software agents, fuzzy logic, expert systems, complex adaptive system theory, the analytic hierarchy process, simulation technique, and hybrid intelligent system method. The theoretical underpinning and rationale for the hybrid framework are provided. In addition, the effectiveness and efficiency of using social media, artificial intelligence, groupware and group decision support systems, and other decision support techniques and technologies in open innovation management are explored. Furthermore, research hypotheses on this topic are formulated.",,Electronic:978-1-4799-6636-3; POD:978-1-4799-6637-0,10.1109/ICICTA.2014.87,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7003550,Social media;analytic hierarchy process;complex adaptive system;expert system;fuzzy logic;hybrid intelligent model;intelligent decision support system;intelligent software agent;open innovation management;simulation,Adaptation models;Decision support systems;Fuzzy logic;Innovation management;Media;Technological innovation,adaptive systems;analytic hierarchy process;artificial intelligence;expert systems;fuzzy logic;group decision support systems;innovation management;large-scale systems;social networking (online);software agents;system theory,analytic hierarchy process;artificial intelligence;complex adaptive system theory;expert systems;fuzzy logic;group decision support systems;groupware;hybrid intelligent model;hybrid intelligent system method;intelligent software agents;open innovation problem;simulation technique;social media,,2,,27,,no,25-26 Oct. 2014,,IEEE,IEEE Conference Publications
Load balancing solution based on AHP for Hadoop,Huixiang Zhou; Qiaoyan Wen,"State Key Lab. of Networking & Switching Technol., BUPT, Beijing, China","2014 IEEE Workshop on Electronics, Computer and Applications",20140630,2014,,,633,636,"Apache Hadoop is an open-source software framework for cloud computing, the server load balancing algorithm named as Balancer which Hadoop provided can load balance for each DataNode, but Balancer algorithm only consider the storage space factor, in this case it will easily lead to load imbalance while operation of the system for a long time. To solve this problem, we consider the file number of visited, frequency of concurrent access, bandwidth, CPU power, memory utilization and other factors, we propose a load balancing solution combined APH, our solution can more accurately calculate the load capacity for each DataNode for Hadoop, and it can automatically adjust the load capacity of Hadoop distributed system while user requests a file again.",,DVD:978-1-4799-4566-5; Electronic:978-1-4799-4565-8; POD:978-1-4799-4564-1,10.1109/IWECA.2014.6845699,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6845699,AHP;Hadoop;cloud computing;load balance,Yarn,analytic hierarchy process;cloud computing;distributed processing;public domain software;resource allocation;storage management,AHP;Apache Hadoop software;Balancer algorithm;CPU power;DataNode;Hadoop;Hadoop distributed system;analytic hierarchy processing;bandwidth;cloud computing;concurrent access;file number;load balancing solution;memory utilization;open-source software framework;storage space factor,,0,,7,,no,8-9 May 2014,,IEEE,IEEE Conference Publications
Location semantics prediction for living analytics by mining smartphone data,C. M. Huang; J. J. C. Ying; V. S. Tseng; Z. H. Zhou,"Institute of Computer Science and Information Engineering, National Cheng Kung University, No.1, University Road, Tainan City 701, Taiwan, R.O.C.",2014 International Conference on Data Science and Advanced Analytics (DSAA),20150312,2014,,,527,533,"Automatic location semantics prediction for living analytics based on smartphone data has attracted extensive attention in just recent years. Basically, this task can be formulated as a multi-class classification problem, where different location/places are regarded as different labels. Previous studies were mostly based on common classification techniques directly, neglecting the critical challenging issue of class imbalance in such a problem (e.g., people go to offices much more often than they go to cinemas). It is also noteworthy that in contrast to common multi-class problems where the classes can be treated independently and interchangeably, the places for labeling usually have important correlations, which should be taken account in the classification/labeling process. Moreover, several activities may occur in the same place and thus the same place label might convey different semantics. In this paper, we address the above issues for location semantics prediction by proposing the FS-Mining (Frame-based Semantics Mining) approach. We treat the raw sensor data in the smartphone as a sequence of short and non-overlapping frames, based on which the user behavior at each place can be characterized and the place semantics can be modeled. To deal with the issues of label relation and class imbalance, a multi-level classification model with class-split and class-merge mechanisms was also developed. An ensemble strategy was also employed to further improve the performance. Experiments on the dataset of Nokia Mobile Data Challenge [1] demonstrate promising performances for the FS-Mining approach.",,Electronic:978-1-4799-6991-3; POD:978-1-4799-6982-1,10.1109/DSAA.2014.7058122,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7058122,Automatic place labeling;Class imbalance;Location semantics prediction;Multi-level classification;User behavior analysis,Buildings;Data mining;Data models;Educational institutions;Feature extraction;Labeling;Semantics,data mining;mobile computing;pattern classification,FS-mining approach;Nokia Mobile Data Challenge;automatic location semantics prediction;class-merge mechanisms;class-split mechanisms;ensemble strategy;frame-based semantics mining approach;multiclass classification problem;place labeling process;smartphone data mining,,1,,18,,no,Oct. 30 2014-Nov. 1 2014,,IEEE,IEEE Conference Publications
M2C: Energy efficient mobile cloud system for deep learning,K. Sun; Z. Chen; J. Ren; S. Yang; J. Li,"Sch. of Software Technol., Dalian Univ. of Technol., Dalian, China",2014 IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS),20140708,2014,,,167,168,"With the number increasing of applications and services that are available on mobile devices, mobile cloud computing has drawn a substantial amount of attention by academia and industry in the past several years. When facing the most exciting machine learning applications such as deep learning, the computing requirement is intensive. For the purpose of improving energy efficiency of mobile device and enhancing the performance of applications through reducing execution time, M2C offloads computation of its machine learning application to the cloud side. We propose the prototype of M2C with the mobile side on Android, iPad and with the cloud side on the open source cloud: Spark, a part of the Berkeley Data Analytics Stack with NVIDA GPU. M2C's distinct set of varying computational tools and mobile nodes allows for thorough implementing distributed machine learning algorithm and innovative wireless protocols with energy efficiency, verifying the theoretical research and bringing the user extremely fast experience.",,Electronic:978-1-4799-3088-3; POD:978-1-4799-3089-0,10.1109/INFCOMW.2014.6849208,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6849208,,Graphics processing units;Mobile handsets;Mobile nodes;Protocols;Sparks,cloud computing;learning (artificial intelligence);mobile computing;protocols;smart phones,Android;Berkeley data analytic stack;M2C offload computation;M2C prototype;NVIDA GPU;Spark;deep learning;distributed machine learning algorithm;energy efficient mobile cloud system;iPad;innovative wireless protocols;machine learning applications;mobile cloud computing;mobile devices;open source cloud,,0,,6,,no,April 27 2014-May 2 2014,,IEEE,IEEE Conference Publications
Making sense of daily life data: From commonalities to anomalies: VAST 2014 Mini Challenge #2,J. Wang; P. Mi; C. North,Virginia Tech,2014 IEEE Conference on Visual Analytics Science and Technology (VAST),20150216,2014,,,365,366,"We report the approach and results on the VAST 2014 Mini-Challenge 2: Analysis Movement and Tracking data of GAStech Employees' daily lives. Based on the commercial interactive visualization software Tableau[l], we follow the sense-making loop for analysis of the massive multi-dimensional, multi-source and time-varying data sets. The findings show that we can effectively identify the patterns and discovery the anomaly from these complex data sets.",,Electronic:978-1-4799-6227-3; POD:978-1-4799-6186-3,10.1109/VAST.2014.7042568,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7042568,Geo-visualization;human information interaction;intelligence analysis;sense-making loop;visual analysis,Data visualization;Global Positioning System;Heating;Security;Software;Visual analytics,,,,0,,2,,no,25-31 Oct. 2014,,IEEE,IEEE Conference Publications
Management of complex data objects in ship designing process,R. Bao; H. Cai,"School of Software, Shanghai Jiaotong University, China",2014 International Conference on Data Science and Advanced Analytics (DSAA),20150312,2014,,,534,540,"Management of ship designing is difficult because of the complexity in modeling of the process and large amounts of data throughout the process. Though existing technologies of BPMN can solve the first problem, BPMN does not provide sufficient supports on dealing with complex dependencies in the process, e.g., storage and search problems of correlated data with ‰ÛÏmany to many‰Ûù relations. In this paper, we introduce a series of annotations of data objects to solve this problem. First, we extend the annotations of data objects based on BPMN and utilize foreign key in relational database to manage the relations between data objects. Second, we use SQL queries to execute the common operations to data objects in the process. Our approach is the extension of mature BPMN and database technologies, so it is standard and reusable. We implemented our approach on the Produce Management System to verify the availability and efficiency.",,Electronic:978-1-4799-6991-3; POD:978-1-4799-6982-1,10.1109/DSAA.2014.7058123,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7058123,BPMN;SQL;Ship designing;annotations;data object;process,Business;Data models;Databases;Educational institutions;Engines;Manufacturing;Marine vehicles,SQL;data handling;information systems;query processing;relational databases;ships,BPMN;SQL query;complex data object management;data object annotations;database technology;enterprise information systems;produce management system;relational database;ship designing management;ship designing process,,0,,14,,no,Oct. 30 2014-Nov. 1 2014,,IEEE,IEEE Conference Publications
MatchVis: A generalized visual multi-scale analysis framework for competitive sports,W. Wang; J. Zhang; T. Liu,"School of Computer Software, Tianjin University, China",2014 IEEE Conference on Visual Analytics Science and Technology (VAST),20150216,2014,,,287,288,"Sports are highly competitive, fast-paced, and teamwork-based. In this article, we introduce a novel approach in analyzing competitive sports based on music metaphor. Our proposed framework MatchVis extracts match information from raw webcast dataset about NBA and incorporates historical models into the investigation, providing a more compact and understandable visual representation of the details and patterns of match, which can consequently aid analysts in performing specific tasks and decision-making.",,Electronic:978-1-4799-6227-3; POD:978-1-4799-6186-3,10.1109/VAST.2014.7042533,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7042533,competitive team sports;multi-scale analysis;multidimensional scaling;visualization framework,Educational institutions;Heating;Pattern matching;Ports (Computers);Software;Teamwork;Visualization,data visualisation;decision making;music;pattern matching;sport,MatchVis;NBA;competitive sport;decision-making;generalized visual multiscale analysis framework;historical model;match information;music metaphor;pattern matching;raw webcast dataset;visual representation,,0,,2,,no,25-31 Oct. 2014,,IEEE,IEEE Conference Publications
Methods selection and overall process of distribution network planning comprehensive evaluation,L. Ma; W. Liu; T. m. Zhu,"China Electric Power Research Institute, Haidian District, Beijing, China",2014 China International Conference on Electricity Distribution (CICED),20141222,2014,,,633,636,"Distribution network planning is an important constituent of overall urban planning. It is advantageous to the improvement of decision-making level and benefit of investment through performing comprehensive assessment on planning schemes. Comprehensive evaluation of distribution network planning is a multiple attribute decision making problem. The typical methods to solve this problem include analytic hierarchy process (AHP), data envelopment analysis (DEA), and technology for order preference by similarity to ideal solution (TOPSIS), etc. It is of significance to select appropriate methods according to specific distribution network planning. For the first time, this paper proposes that evaluation methods could be selected according to three attributes of the evaluation object, including preference information, scheme number and the comparison between scheme number and index number. In the distribution network planning assessment, indices are not equally important. So it is a problem how to determine the preference information. The object' s scheme number could be either single or multiple, and the scheme number is normally less than the index number. Based on these characteristics, the suitable methods for distribution network planning are analytic hierarchy process (AHP), fuzzy comprehensive evaluation (FCE) and the delphi method (DM). The overall process of distribution network planning assessment using the selected methods is also proposed, including construction of index system, setting of index weight, setting of index score standard, analysis of the evaluation results and so on. The effectiveness of the proposed method is verified by analysis on actual distribution network planning schemes for a certain region in 2015. The results, which can be used for similar evaluation work, will be of great value in real applications.",2161-7481;21617481,Electronic:978-1-4799-4126-1; POD:978-1-4799-4125-4,10.1109/CICED.2014.6991789,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6991789,comprehensive evaluation;distribution network planning;methods selection;overall process,Abstracts;Computer aided software engineering,analytic hierarchy process;forecasting theory;fuzzy reasoning;power distribution planning;power engineering computing,AHP;DEA;FCE;TOPSIS;analytic hierarchy process;data envelopment analysis;decision-making level;delphi method;distribution network planning;evaluation object;fuzzy comprehensive evaluation;index number;index score standard;index system;index weight;multiple attribute decision making problem;overall urban planning;preference information;scheme number;technology for order preference by similarity to ideal solution,,0,,6,,no,23-26 Sept. 2014,,IEEE,IEEE Conference Publications
Metrics for effectiveness of e-learning objects in software engineering education,A. E. Escobar; P. Reyes; M. Van Hilst,"Division of Math, Science and Technology, Nova Southeastern University, Fort Lauderdale, USA",IEEE SOUTHEASTCON 2014,20141110,2014,,,1,5,"In this paper we present the rationale and beginning of work on improving e-learning objects through the use of analytics modeled after Google Analytics. Prior work on the use of metrics in e-learning has focused on user satisfaction, and the ranking and selection of learning objects from a set of available choices. This work differs in its focus on the kinds of metrics needed to improve an existing object, or even more specifically to make improvements to specific pages within an object. This work is based on the now well established track record of using Google Analytics for Web site optimization in e-commerce. We discuss adaptations needed to apply similar metrics in the context of e-learning and more specifically e-learning objects.",1091-0050;10910050,Electronic:978-1-4799-6585-4; POD:978-1-4799-6586-1,10.1109/SECON.2014.6950671,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6950671,Google Analytics;learning management systems;learning objects,Business;Calibration;Current measurement;Electronic learning;Training;World Wide Web,learning management systems,Google Analytics;Web site optimization;analytics modelling;e-Iearning object improvement;e-commerce;e-learning object effectiveness;software engineering education,,0,,20,,no,13-16 March 2014,,IEEE,IEEE Conference Publications
Minimally-invasive body-fluids analyzer for continuous glucose monitoring,M. A. Basha; R. H. Zaghloul; M. Alkordi,"Center for Nanotechnology, Zewail City of Science and Technology, Giza12588, Egypt",2014 USNC-URSI Radio Science Meeting (Joint with AP-S Symposium),20141113,2014,,,247,247,"Summary form only given. Efficient management of blood glucose (BG) levels in diabetic patients relies heavily on accurate and frequent analysis of BG levels. Therefore, detection techniques (direct or indirect) that can provide reliable measurements of BG levels are of utmost interest. In a typical integrated analytic device, sampling (most utilized are the lancing devices) is a painful experience for patients that require frequent finger-prick to draw a minimal volume of blood for analysis. Besides the unpleasant experience, drawing blood is a source of contamination and/or transfer of blood-borne infections. Microneedles is one of the great candidates for minimally invasive BG level monitoring. One type of microneedles is the hollow one which is used to extract small samples of blood or body interstitial fluids to measure the BG level. A second type of microneedles is used to probe the impedance of the blood to indirectly measure the BG level. We designed a new miniature attenuated total reflectance-Fourier transform infrared spectrophotomer for body-fluids analysis. The key component of the system is the microneedle used to probe and detect the BG level either directly or indirectly (interstitial fluid sampling). Our approach will exploit established infrared spectrophotometry technology in a new design of the probe element for spectral analysis. We designed and simulated the microneedle shown in the figure that analyzes blood or interstitial fluid at the site of stinging and directly on the surface of the microneedle. Full 2-D optical analysis using Comsol software is performed. The microneedle is excited from its top tapered side by a normal incident of a Gaussian beam. The beam will propagate inside the microneedle, which will encounter multiple reflections from the microneedle surface in contact with the interstitial fluids and will return back to the same tip of the microneedle. The other side is plated with metallic reflective layer to fully reflect the i- cident Gaussian beam back to exit from the same tip of the microneedle. The reflected Gaussian beam with a reference beam will constitute an interferogram from which the signature of the glucose is mathematically extracted. The fabrication process of the microneedle is a simple three masks process composed of wet and deep reactive ion etching of the device layer of an SOI wafer.",,Electronic:978-1-4799-3746-2; POD:978-1-4799-3747-9; USB:978-1-4799-3745-5,10.1109/USNC-URSI.2014.6955630,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6955630,,Blood;Cities and towns;Fluids;Monitoring;Pollution measurement;Probes;Sugar,Fourier transform spectra;biomedical measurement;blood;diseases;infrared spectra;level measurement;needles;patient monitoring;spectrophotometry,BG level measurements;Comsol software;SOI wafer;blood glucose level management;blood-borne infections;body interstitial fluids;contamination;continuous glucose monitoring;deep reactive ion etching;diabetic patients;drawing blood;finger-prick;full 2D optical analysis;incident Gaussian beam;integrated analytic device;interferogram;mask process;metallic reflective layer;microneedle fabrication process;microneedle surface;miniature attenuated total reflectance-Fourier transform infrared spectrophotomer;minimally invasive BG level monitoring;minimally-invasive body-fluids analyzer;probe element design;spectral analysis;wet etching,,0,,,,no,6-11 July 2014,,IEEE,IEEE Conference Publications
Mobile Commerce: A Broader Perspective,E. Seth,,IT Professional,20140617,2014,16,3,61,65,"Many organizations are trying to understand how best to leverage the unique combination of content, data, and functionality provided by mobile devices. This is especially true in the retail sector, where significant mobile commerce growth is predicted. According to Internet Retailer, of the 49.6 billion visits to the top 500 e-retailers in 2014, 26.4 billion (53.2 percent) will stem from smartphones. Similarly, the percentage of online retail sales placed via mobile devices will grow from 11 percent in 2012 to 25 percent by 2017. However, ecommerce and transaction statistics are only one part of the picture when it comes to leveraging mobile commerce.",1520-9202;15209202,,10.1109/MITP.2014.37,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6824518,data analytics;information technology;mobile;mobile commerce;networking;software engineering,Electronic commerce;Media;Mobile communication;Online banking;Smart phones,electronic commerce;mobile computing;retail data processing,Internet retailer;ecommerce;mobile commerce;mobile devices;online retail sales;organizations;retail sector;smartphones;transaction statistics,,1,,8,,no,May-June 2014,,IEEE,IEEE Journals & Magazines
Mobile Traffic Analysis Exploiting a Cloud Infrastructure and Hardware Accelerators,M. Barbareschi; A. De Benedictis; A. Mazzeo; A. Vespoli,"Dept. of Electr. Eng. & Inf. Technol., Univ. of Naples Federico II, Naples, Italy","2014 Ninth International Conference on P2P, Parallel, Grid, Cloud and Internet Computing",20150129,2014,,,414,419,"Recently, traffic analysis and measurements have been used to characterize, from a security point of view, applications' and network behavior to avoid intrusion attempts, malware injections and data theft. Since most of the generated data traffic is from the embedded mobile devices, the analysis techniques have to cope on the one hand with the scarce computing capabilities and battery limitation of the devices, and on the other hand with tight performance constraints due to the huge generated traffic. In recent years, several machine learning approaches have been proposed in the literature, providing different levels of accuracy and requiring high computation resources to extract the analytic model from available training set. In this paper, we discuss a traffic analysis architecture that exploits FPGA technology to efficiently implement a hardware traffic analyzer on mobile devices, and a cloud infrastructure for the dynamic generation and updating of the data model based on ongoing mis-classification events. Finally, we provide a case study based on the implementation of the proposed traffic analyzer on a Xilinx Zynq 7000 architecture and Android OS, and show an overview of the proposed cloud infrastructure.",,Electronic:978-1-4799-4171-1; POD:978-1-4799-7872-4,10.1109/3PGCIC.2014.86,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7024620,Android;Cloud Infrastructure;Decision Tree;FPGA;Hardware Accelerator;Mobile Traffic Analysis,Analytical models;Androids;Computer architecture;Hardware;Humanoid robots;Mobile communication;Software,cloud computing;data models;field programmable gate arrays;invasive software;mobile computing,Android OS;FPGA technology;Xilinx Zynq 7000 architecture;cloud infrastructure;data model;data theft;data traffic;embedded mobile devices;hardware accelerators;hardware traffic analyzer;intrusion attempts;machine learning approaches;malware injections;mobile devices;mobile traffic analysis architecture,,2,,18,,no,8-10 Nov. 2014,,IEEE,IEEE Conference Publications
Model of capacity estimation of roundabout in Beijing,J. Shi; W. Wang; C. X. Yang,"Beijing University of Technology, China",17th International IEEE Conference on Intelligent Transportation Systems (ITSC),20141120,2014,,,1808,1813,"Weaving sections have been shown to have a great effect on capacity of a roundabout, because at weaving sections the traffic conflict and interference are predominant, resulting in a reduction of speed as well as increase of gaps. It is known that weaving section of roundabout is the bottleneck of the traffic movement. Therefore, it should be given the highest priority in the capacity analysis. The current methods of estimating the approach capacity of a roundabout mainly utilize information of vehicles entering and exiting the roundabout. From literature review it has been found that few studies are conducted based on the traffic characteristics of weaving section in capacity estimation. The purpose of this research is to estimate the capacity of roundabout by modeling radius of roundabout, width of circulating roadway, and gap acceptance at the weaving sections. For this reason, a total of 21 roundabouts that are located at the metropolitan Beijing are selected to conduct data collection. Significance analysis of speed at weaving sections has been conducted by means of the variance analytic method. By taking geometric conditions and traffic conditions into account, new concepts of ‰ÛÏQuasi-saturation state of roundabout‰Ûù and ‰ÛÏThe Capacity of Roundabout‰Ûù are introduced in this study. The regression models of the headway and the radius on both the outer lanes and the inner lanes are established respectively using Statistical Program for Social Sciences (SPSS 18.0) for windows. Moreover, based on the theory of saturation flow rate, a new method is proposed to estimate the capacity of single-lane and multi-lane roundabouts using MATLAB software. And then the real data are taken to validate this model. The findings from this research demonstrate that capacity estimates with weaving section vehicles result in improved prediction of the actual capacity of a roundabout.",2153-0009;21530009,Electronic:978-1-4799-6078-1; POD:978-1-4799-6079-8; USB:978-1-4799-6077-4,10.1109/ITSC.2014.6957955,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6957955,capacity;gap acceptance theory;roundabouts;weaving section,Analytical models;Correlation;Educational institutions;Equations;Mathematical model;Vehicles;Weaving,road traffic;statistical analysis,Beijing;MATLAB software;SPSS 18.0;capacity estimation;circulating roadway;data collection;gap acceptance;multilane roundabouts;roundabout capacity;roundabout modeling radius;roundabout quasisaturation state;saturation flow;single-lane roundabouts;speed reduction;statistical program for social sciences;traffic conflict;traffic movement bottleneck;variance analytic method;weaving section,,0,,13,,no,8-11 Oct. 2014,,IEEE,IEEE Conference Publications
Monitoring students performances in French Institutes of Technology using the ScoDoc software,E. Viennet; L. Petrucci,"L2TI, Institut Galil&#x00E9;e, Universit&#x00E9; Paris 13, Sorbonne Paris Cit&#x00E9;, F-93430, Villetaneuse, France",2014 Information Technology Based Higher Education and Training (ITHET),20150716,2014,,,1,6,"French University-based Institutes of Technology (IUT1) are a major player in France's superior educational system. IUTs provide technical university education, preparing students to careers in the industry and services. The main diploma is called DUT, Diplome Universitaire de Technologie [2]. Designed to train mid-level technical staff in 2 years, IUT programmes also allow graduated students to pursue their studies with a more advanced degree, such as a licence professionnelle [4]. IUT students have very different profiles: most of them are coming from high school (with scientific or technical majors), but a significant proportion come from foreign countries (mainly Africa), or from other university tracks. Assessment of IUT students relies on continuous evaluation: there are in principle no terminal exams, but series of tests, monitoring the acquisition of competences. At the IUT de Villetaneuse (Universite Paris 13 - Sorbonne-Paris-CiteíÅ), we started more than ten years ago to design a dedicated information system to gather all available information concerning our students: personal data, results at all tests and exams, assiduity, orientation decision. This system, implemented in the open-source free software ScoDoc (https://trac.lipn.univ-paris13.fr/projects/ scodoc) monitors the progression of all students and raises alarms when some special pedagogical action should be taken. It is widely distributed and used in other French universities, and is wellsuited to handle the complex national regulations for semesters validation in IUTs. Prevention of student difficulties requires action from the pedagogical team: it could be as simple as talking to an individual student to discuss his personal situation, or as complex as detecting an homogeneous group of students sharing similar problems and proposing them an ad-hoc remediation plan (such as tutoring, personalised learning plan, or ad-hoc learning module). ScoDoc software system is intended for being used- by the pedagogical team as well as by the students. Indeed, students can access their marks summary at all times, and be informed of new marks as soon as they are entered in the system. They thus can monitor their performances and ask advice from the teachers for a better progress. Teachers can be assigned different roles according to their responsibilities in the organisation of studies. All of them can enter notes on a student in order for the pedagogical team to easily track past difficulties. Those involved in a specific course can enter their marks concerning this course, while the head of studies for a particular year or semester can also prepare summaries for the class of students they are in charge of. This is not only necessary for the validation of semesters, but also at any time to have a synthetic view of the class performance, pointing out difficulties in some subjects or of a group within the class. This paper is organised in three parts: first, we present the French IUT system and describe its peculiarities, insisting on the evaluation of the students. In section 2, the main features of the ScoDoc software are briefly presented. In section 3, we discuss how ScoDoc can be used to prevent student difficulties. Finally, in section 4, we discuss some future perspectives, such as the integration of more sophisticated predictive analytics technologies to enhance the relevance and accuracy of student real-time characterisation.",,Electronic:978-1-4673-6730-1; POD:978-1-4673-6731-8,10.1109/ITHET.2014.7155707,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7155707,,Education;Industries;Information systems;Monitoring;Real-time systems;Software systems,educational administrative data processing;educational courses;educational institutions;information systems;public domain software,DUT;Diplome Universitaire de Technologie;French IUT system;French University-based Institutes of Technology;ScoDoc software;continuous student evaluation;course;information system;open-source free software;student performance monitoring;technical university education,,0,,5,,no,11-13 Sept. 2014,,IEEE,IEEE Conference Publications
"Multi-platform strategies, approaches and challenges for developing mobile applications",P. Gokhale; S. Singh,"Business Analytics Division, IBM Software Lab, Pune, India","2014 International Conference on Circuits, Systems, Communication and Information Technology Applications (CSCITA)",20140619,2014,,,289,293,"Developing applications for mobile platforms is challenging because of multiple proprietary environments. Abundant material has been published discussing three kinds of mobile app development - Native, Web, and Hybrid, where Hybrid apps are preferred due to their usability. In this paper we discuss a strategy and approach for developing and delivering existing Web and Desktop applications as mobile apps. This proposal is a variant of Hybrid development model that utilizes code translators to translate existing Web or Desktop applications for the target mobile platforms. Our goal is to validate if investments made by an enterprise in developing Web or Desktop applications are still relevant when the same are to be re-deployed as mobile apps.",,Electronic:978-1-4799-2494-3; POD:978-1-4799-2495-0,10.1109/CSCITA.2014.6839274,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6839274,Android;Code Translator;Java 2 CSharp;Mobile;Multi-platform;Tangible Software;User Interface(UI);iOS,Business;Games;Java;Mobile communication;Servers;Software;Visualization,Android (operating system);Internet;mobile computing;program interpreters,Native mobile application development;Web mobile application development;code translators;desktop applications;hybrid mobile application development;multiplatform strategy;multiple proprietary environments;target mobile platforms,,0,,13,,no,4-5 April 2014,,IEEE,IEEE Conference Publications
Multicriteria decision making with fuzziness and criteria interdependence in cloud service selection,S. Le; H. Dong; F. K. Hussain; O. K. Hussain; J. Ma; Y. Zhang,"Centre for Applied Informatics, Victoria University, Melbourne, Australia",2014 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),20140908,2014,,,1929,1936,"With the advent of Cloud computing and subsequent big data, online decision makers usually find it difficult to make informed decisions because of the great amount of irrelevant, uncertain, or inaccurate information. In this paper, we explore the application of multicriteria decision-making (MCDM) techniques in the area of Cloud computing and big data, to find an efficient way of dealing with criteria relations and fuzzy knowledge based on a great deal of information. We propose a MCDM framework, which combines the ISM-based and ANP-based techniques, to model the interactive relations between evaluation criteria, and to handle data uncertainties. We present an application of Cloud service selection to prove the efficiency of the proposed framework, in which a user-oriented sigmoid utility function is designed to evaluate the performance of each criterion.",1098-7584;10987584,CD-ROM:978-1-4799-2073-0; Electronic:978-1-4799-2072-3; POD:978-1-4799-2074-7,10.1109/FUZZ-IEEE.2014.6891892,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6891892,Cloud service selection;MCDM;criteria interdependence;fuzzy integral;quality of service,Additives;Cloud computing;Decision making;Pragmatics;Symmetric matrices;Uncertainty;Vectors,Big Data;cloud computing;decision making;fuzzy set theory,ANP-based techniques;Big Data;ISM-based techniques;MCDM techniques;analytic network process;cloud computing;cloud service selection;fuzziness;interpretive structure modeling;multicriteria decision making;user-oriented sigmoid utility function,,1,,23,,no,6-11 July 2014,,IEEE,IEEE Conference Publications
Multifunctional Gateway Sensor Node for agriculture and forest actvities,H. H. Bhavsar,"Information Technology, Vadodara Institute of Engineering, India","2014 International Conference on Computation of Power, Energy, Information and Communication (ICCPEIC)",20141002,2014,,,506,510,"The Multifunctional Gateway Sensor Node is an extension of the Gateway sensor node of Wireless Sensor Network having multiple functionalities. Addendum of functionalities comprise different types of technologies which are LCD Color display, keypad to operate Gateway Node, AVR camera and solar panel for additional power support. The node itself also acts as a weather station based on Fuzzy AHP for precise decision support. So the main idea behind this research is to operate WSN without computer. It will be most usable in agriculture, forest monitoring as well as marine securities when pc is unavailable. The location of particular place for weather station feature will be selected by the help of Koppen Classification, instead of GPS. This sensor node is also operable by the specially designed java based software having great intelligence as per the locality. Hence the paper presents the idea about the proposed research area.",,CD-ROM:978-1-4799-3826-1; Electronic:978-1-4799-3827-8; POD:978-1-4799-3828-5,10.1109/ICCPEIC.2014.6915416,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6915416,GPS;Koppen;LCD;fuzzy AHP;gateway;java;keypad;solar;weather station;wireless sensor network,Atmospheric measurements;GSM;Internet;Logic gates;Particle measurements;Pollution;Pollution measurement,agriculture;atmospheric measuring apparatus;fuzzy systems;liquid crystal displays;vegetation;weather forecasting;wireless sensor networks,AVR camera;Java based software;Koppen classification;LCD color display;agriculture activities;forest activities;fuzzy analytic hierarchy process;multifunctional gateway sensor node;weather forecasting;wireless sensor network,,0,,17,,no,16-17 April 2014,,IEEE,IEEE Conference Publications
Multiple Kernel Learning Based Multi-view Spectral Clustering,D. Guo; J. Zhang; X. Liu; Y. Cui; C. Zhao,"Sch. of Comput. Sci. & Eng., Nanjing Univ. of Sci. & Technol., Nanjing, China",2014 22nd International Conference on Pattern Recognition,20141206,2014,,,3774,3779,"For a given data set, exploring their multi-view instances under a clustering framework is a practical way to boost the clustering performance. This is because that each view might reflect partial information for the existing data. Furthermore, due to the noise and other impact factors, exploring these instances from different views will enhance the mining of the real structure and feature information within the data set. In this paper, we propose a multiple kernel spectral clustering algorithm through the multi-view instances on the given data set. By combining the kernel matrix learning and the spectral clustering optimization into one process framework, the algorithm can determine the kernel weights and cluster the multi-view data simultaneously. We compare the proposed algorithm with some recent published methods on real-world datasets to show the efficiency of the proposed algorithm.",1051-4651;10514651,Electronic:978-1-4799-5209-0; POD:978-1-4799-5210-6,10.1109/ICPR.2014.648,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6977360,,Clustering algorithms;Clustering methods;Educational institutions;Kernel;Linear programming;Optimization;Proteins,data handling;learning (artificial intelligence);matrix algebra;pattern clustering,data set;feature information;kernel matrix learning;kernel spectral clustering algorithm;kernel weights;multiple Kernel learning;multiview spectral clustering;partial information;process framework,,2,,27,,no,24-28 Aug. 2014,,IEEE,IEEE Conference Publications
Nakshatra: Towards Running Batch Analytics on an Archive,A. Kathpal; G. A. N. Yasa,,"2014 IEEE 22nd International Symposium on Modelling, Analysis & Simulation of Computer and Telecommunication Systems",20150209,2014,,,479,482,"Long term retention of data has become a norm for reasons like compliance and data preservation for future needs. With storage media continuing to become cheaper, this trend has further strengthened and is testified with introduction of archival solutions like Amazon Glacier and Spectra Logic Black Pearl. On the other hand, analytics and big data have become key enablers for business and research. However, analytics and archiving happens on separate storage silos. This generates additional costs and inefficiencies when part of archived data needs to be analyzed using batch analytics platforms like Hadoop because a) We need additional storage for data transferred from archive to analytics tier and b) Transfer time costs are incurred due to data migration to analytics tier. Moreover, accessing archived data has high times to first byte, as much of the data is stored in offline media like tapes or spun down disks. We introduce Nakshatra, a data processing framework to run analytics directly on an archive based on offline media. To the best of our knowledge, this is the first work of its kind available in literature. We leverage batched pre-fetching and scheduling techniques for improved retrieval of data and scalable analytics on archives. Our preliminary evaluation shows Nakshatra to be upto 81% faster than the traditional ingest-then-compute workflow for archived data.",1526-7539;15267539,Electronic:978-1-4799-5610-4; POD:978-1-4799-5611-1; USB:978-1-4799-5609-8,10.1109/MASCOTS.2014.67,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7033688,Archival storage;Data analytics;Hadoop,Computer architecture;Data analysis;Delays;Media;Processor scheduling;Schedules;Servers,Big Data;data analysis;distributed processing;information retrieval systems;public domain software;scheduling;storage management,Amazon Glacier;Hadoop;Nakshatra;Spectra Logic Black Pearl;analytics tier;archival solutions;archive;batch analytics platforms;batched prefetching technique;batched scheduling technique;data migration;data preservation;data retrieval;long term data retention;offline media;spun down disks;storage silos;tapes;transfer time costs,,0,,13,,no,9-11 Sept. 2014,,IEEE,IEEE Conference Publications
NanoStreams: Advancing the hardware and software stack for real-time analytics on fast data streams,C. J. Gillan; D. S. Nikolopoulos; A. Bilas; C. Bekas,"The School of Electronics, Electrical Engineering and Computer Science Queen's, University of Belfast, Queen's University, BT7 1NN, United Kingdom",eChallenges e-2014 Conference Proceedings,20150319,2014,,,1,8,"NanoStreams is a consortium project funded by the European Commission under its FP7 programme and is a major effort to address the challenges of processing vast amounts of data in real-time, with a markedly lower carbon footprint than the state of the art. The project addresses both the energy challenge and the high-performance required by emerging applications in real-time streaming data analytics. NanoStreams achieves this goal by designing and building disruptive micro-server solutions incorporating real-silicon prototype micro-servers based on System-on-Chip and reconfigurable hardware technologies.",2166-1650;21661650,Electronic:978-1-9058-2446-5; POD:978-1-4799-1810-2,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7058143,,Contracts;Hardware;Libraries;Program processors;Servers;System-on-chip,data analysis;file servers;green computing;system-on-chip,European Commission;FP7 programme;NanoStreams;carbon footprint;consortium project;disruptive microserver solutions;energy challenge;fast data streams;hardware stack;real-silicon prototype microservers;real-time streaming data analytics;reconfigurable hardware technologies;software stack;system-on-chip,,0,,12,,no,29-30 Oct. 2014,,IEEE,IEEE Conference Publications
Needle in a haystack: Cost-Effective data analytics for real-time cloud sharing,Y. Hua; D. Feng,"Wuhan National Lab for Optoelectronics, School of Computer, Huazhong University of Science and Technology, China",2014 IEEE 22nd International Symposium of Quality of Service (IWQoS),20141002,2014,,,159,167,"Real-time file sharing is important to improve the quality of cloud services. Current cloud systems however fail to efficiently offer cost-effective data analytics due to slow response and energy inefficiency. In order to support real-time processing and improve energy efficiency in the cloud, this paper proposes a novel cost-effective data analytics scheme, called Needle. The idea behind Needle is to use flat-addressing and content-aware naming approaches to deliver high performance of network transmission. Needle leverages a suitable division-of-labor model between core and edge network nodes to efficiently support content-aware queries in the cloud data center networks. Experimental results demonstrate that Needle is able to support cloud sharing service in a real-time and efficient manner.",1548-615X;1548615X,Electronic:978-1-4799-4852-9; POD:978-1-4799-5005-8; USB:978-1-4799-4853-6,10.1109/IWQoS.2014.6914316,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6914316,,Cloud computing;Complexity theory;IP networks;Needles;Quality of service;Routing;Scalability,cloud computing;computer centres;data analysis;power aware computing;software quality,Needle;cloud data center networks;cloud services;content-aware naming approach;content-aware queries;core nodes;cost-effective data analytics;division-of-labor model;edge network nodes;energy efficiency;energy inefficiency;flat-addressing approach;network transmission;quality improvement;real-time cloud sharing;real-time file sharing;real-time processing,,0,,35,,no,26-27 May 2014,,IEEE,IEEE Conference Publications
Network-wide traffic visibility in OF@TEIN SDN testbed using sFlow,S. U. Rehman; W. C. Song; M. Kang,"Dept. of Computer Sci. and Eng., Air University, Islamabad, Pakistan",The 16th Asia-Pacific Network Operations and Management Symposium,20141229,2014,,,1,6,"This paper provides insights into the traffic flow monitoring system of OF@TEIN (OpenFlow@Trans Eurasia Information Network) testbed. OF@TEIN is software defined networking (SDN) testbed adapted by KOREN (KOrea advanced REsearch Network) and integrated with the research and education networks of several Asian nations including Japan, Malaysia, Thailand, Vietnam, Philippine, etc. Traditional traffic monitoring solutions such as NetFlow, RSPAN ports, Network Packet Brokers (NPBs) can't provide network-wide visibility in OF@TEIN because OF@TEIN is a large multi-tenant testbed deployed over high speed research networks across several countries. Therefore, we have implemented a new sFlow-based flow monitoring system that is tailored to OF@TEIN requirements and can provide real-time L2 to L7 network wide visibility. OF@TEIN uses SDN-based network virtualization to slice the network among multiple concurrent experimenters. Machines in a network slice (or VLAN) communicate with each other using GRE tunnels. Our traffic monitoring system enables monitoring flow spaces of VLANs as well as physical provider network. It utilizes northbound interfaces (NBIs) exposed by sFlow-RT analytics engine and FloodLight SDN Controller. It periodically fetches flows statistics from sFlow-RT and stores them in time-series format in Whisper RRD database. Graphite real-time charting tool is used to plot the statistics stored in Whisper.",,Electronic:978-4-88552-288-8; POD:978-1-4799-5741-5,10.1109/APNOMS.2014.6996541,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6996541,OF@TEIN;OpenFlow;SDN;Visibility;sFlow;sFlow-RT,Graphite;Measurement;Monitoring;Ports (Computers);Real-time systems;Switches,local area networks;software defined networking;telecommunication traffic;time series;virtualisation,Asian nations;FloodLight SDN controller;GRE tunnels;Japan;KOREN;Korea advanced Research Network;Malaysia;NBI;OF@TEIN SDN testbed;OpenFlow@Trans Eurasia information network testbed;Philippines;SDN-based network virtualization;Thailand;VLAN;Vietnam;Whisper RRD database;education networks;flows statistics;graphite real-time charting tool;multitenant testbed;network slice;network-wide traffic visibility;northbound interfaces;physical provider network;sFlow-RT analytics engine;sFlow-based traffic flow monitoring system;software defined networking testbed;time-series format,,3,,8,,no,17-19 Sept. 2014,,IEEE,IEEE Conference Publications
OCBPSR: Orthogonal Complex BandPass Sampling Receiver,M. AL-Aboodi; I. A. Lami,"Applied Computing Department, The University of Buckingham, UK",2014 World Congress on Computer Applications and Information Systems (WCCAIS),20141007,2014,,,1,6,"Second-order/Complex BandPass Sampling Receivers (CBPSR) are attractive for acquiring multi-signals for SDR/CR applications. One of the issues caused by the implementation of such receivers is the signals IQ mismatch. This paper proposes a CBPSR implementation that eliminates this IQ mismatch by reformatting the received signals in an orthogonal analytic form (thus named OCBPSR). Our implementation will also reduce the required sampling frequency to below the Nyquist rate, and so reducing the processing time to recover the signals. This is achieved by folding the upper-side of the received signals to the same fold-frequency in the baseband domain without overlapping, by making the signals orthogonal. MATLAB simulation is used to evaluate the performance of our OCBPSR using various scenarios of harsh signal environment, including Doppler and multipath effects.",,Electronic:978-1-4799-3351-8; POD:978-1-4799-7527-3,10.1109/WCCAIS.2014.6916633,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6916633,BPSR;Fading channel;Orthogonal signals;RLS and LMS;multi-signal receiver Introduction;multipath,Bandwidth;Baseband;Bit error rate;Doppler effect;Fading;Receivers;Time-frequency analysis,Doppler effect;cognitive radio;multipath channels;radio receivers;software radio,Doppler effect;MATLAB simulation;Nyquist rate;OCBPSR;SDR-CR applications;baseband domain;cognitive radio;fold-frequency;harsh signal environment;multipath effect;multisignals;orthogonal analytic form;orthogonal complex bandpass sampling receiver;received signals;required sampling frequency;second-order-complex bandpass sampling receivers;signals IQ mismatch;software defined radio,,0,,15,,no,17-19 Jan. 2014,,IEEE,IEEE Conference Publications
On computing the L<inf>2</inf> norm of a generalized discrete-time system,C. Dinicu; C. Oar€Ä,"Department of Automatic Control and Systems Engineering, Faculty of Automatic Control and Computers, Politehnica University of Bucharest, Romania","2014 18th International Conference on System Theory, Control and Computing (ICSTCC)",20141215,2014,,,412,417,"We give explicit analytic formulas for computing the L<sub>2</sub> norm of a discrete-time generalised system whose rational transfer matrix function may be improper or polynomial. The norm is expressed in terms of solutions of generalized Lyapunov equations, written down with coefficients from a special type of realisation of the underlying transfer function matrix, much in the same spirit of the standard (proper) case. The main result hints to a numerically-sound prototype algorithm that relies on standard reliable software for computing solutions of generalised Lyapunov equations.",,Electronic:978-1-4799-4601-3; POD:978-1-4799-4600-6,10.1109/ICSTCC.2014.6982451,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6982451,,Eigenvalues and eigenfunctions;Matrix converters;Poles and zeros;Polynomials;Standards;Transforms,Lyapunov matrix equations;discrete time systems;polynomials;transfer function matrices,explicit analytic formulas;generalized Lyapunov equations;generalized discrete-time system;polynomial;rational transfer matrix function;transfer function matrix,,0,,12,,no,17-19 Oct. 2014,,IEEE,IEEE Conference Publications
OOPN-SRAM: A Novel Method for Software Risk Assessment,X. Wu; X. Li; R. Feng; G. Xu; J. Hu; Z. Feng,"Tianjin Key Lab. of Cognitive Comput. & Applic., Tianjin Univ., Tianjin, China",2014 19th International Conference on Engineering of Complex Computer Systems,20141016,2014,,,150,153,"This paper proposes a Software Risk Assessment Method based on Object-Oriented Petri Net (OOPN-SRAM), in which risk assessment procedure is divided into four steps, expressed as four corresponding objects, including asset recognition, weakness analysis, consequence property confirmation and risk calculation. Each object is modeled with Petri net. Specialists recognize software assets by the 1-9 scales method of Analytic Hierarchy Process (AHP). The weaknesses in a system are found by the vulnerability scanner. The damage degree and the exploitation likelihood of a weakness are evaluated by such authorities as Common Weakness Enumeration (CWE). The consequence properties are confirmed by specialists according to the software requirements. Finally, in the risk calculation, risk degree and overall risk value are calculated by using exponential method and weighted average method respectively. Furthermore, we illustrate the application of our OOPN-SRAM method with realistic examples including web-banking and forum, and make a comparison with traditional methods. The results show that OOPN-SRAM not only increases the efficiency of the evaluation process, but also makes the evaluation result more objective and accurate.",,Electronic:978-1-4799-5482-7; POD:978-1-4799-5483-4,10.1109/ICECCS.2014.28,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6923130,CWE;OOPN;risk assessment;software;vulnerability scanner,Availability;Computational modeling;Educational institutions;Object oriented modeling;Risk management;Security;Software,Petri nets;object-oriented methods;risk management;software development management,AHP method;CWE;OOPN-SRAM method;analytic hierarchy process;asset recognition;common weakness enumeration;consequence property confirmation;exponential method;object-oriented Petri net;risk assessment procedure;risk calculation;software requirements;software risk assessment;vulnerability scanner;weakness analysis;weighted average method,,0,,6,,no,4-7 Aug. 2014,,IEEE,IEEE Conference Publications
Opening the Black Box: Strategies for Increased User Involvement in Existing Algorithm Implementations,T. M?hlbacher; H. Piringer; S. Gratzl; M. Sedlmair; M. Streit,"VRVis Research Center, Vienna, Austria",IEEE Transactions on Visualization and Computer Graphics,20141106,2014,20,12,1643,1652,"An increasing number of interactive visualization tools stress the integration with computational software like MATLAB and R to access a variety of proven algorithms. In many cases, however, the algorithms are used as black boxes that run to completion in isolation which contradicts the needs of interactive data exploration. This paper structures, formalizes, and discusses possibilities to enable user involvement in ongoing computations. Based on a structured characterization of needs regarding intermediate feedback and control, the main contribution is a formalization and comparison of strategies for achieving user involvement for algorithms with different characteristics. In the context of integration, we describe considerations for implementing these strategies either as part of the visualization tool or as part of the algorithm, and we identify requirements and guidelines for the design of algorithmic APIs. To assess the practical applicability, we provide a survey of frequently used algorithm implementations within R regarding the fulfillment of these guidelines. While echoing previous calls for analysis modules which support data exploration more directly, we conclude that a range of pragmatic options for enabling user involvement in ongoing computations exists on both the visualization and algorithm side and should be used.",1077-2626;10772626,,10.1109/TVCG.2014.2346578,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875995,Visual analytics infrastructures;integration;interactive algorithms;problem subdivision;user involvement,Algorithm design and analysis;Approximation algorithms;Complexity theory;Context;Data visualization;Software algorithms;Visualization,application program interfaces;data visualisation;human computer interaction;interactive systems,algorithmic API;black boxes;data exploration;interactive visualization tools;user involvement,1,4,,52,,no,Dec. 31 2014,,IEEE,IEEE Journals & Magazines
Ophidia: A full software stack for scientific data analytics,S. Fiore; A. D'Anca; D. Elia; C. Palazzo; D. Williams; I. Foster; G. Aloisio,"Centro Euro-Mediterraneo sui Cambiamenti Climatici, Lecce, Italy",2014 International Conference on High Performance Computing & Simulation (HPCS),20140922,2014,,,343,350,"The Ophidia project aims to provide a big data analytics platform solution that addresses scientific use cases related to large volumes of multidimensional data. In this work, the Ophidia software infrastructure is discussed in detail, presenting the entire software stack from level-0 (the Ophidia data store) to level-3 (the Ophidia web service front end). In particular, this paper presents the big data cube primitives provided by the Ophidia framework, discussing in detail the most relevant and available data cube manipulation operators. These primitives represent the proper foundations to build more complex data cube operators like the apex one presented in this paper. A massive data reduction experiment on a 1TB climate dataset is also presented to demonstrate the apex workflow in the context of the proposed framework.",,CD-ROM:978-1-4799-5311-0; Electronic:978-1-4799-5313-4; POD:978-1-4799-5160-4,10.1109/HPCSim.2014.6903706,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6903706,big data;data analytics;multidimensional data;scientific workflow;software infrastructure,Arrays;Big data;Data models;Meteorology;Servers;Web services,Big Data;Web services;data analysis;data reduction,Ophidia Web service front end;Ophidia data store;Ophidia software infrastructure;big data analytics platform;big data cube primitives;data reduction;scientific data analytics;software stack,,2,,21,,no,21-25 July 2014,,IEEE,IEEE Conference Publications
Opportunities and challenges of the Internet of Things for healthcare: Systems engineering perspective,F. Fernandez; G. C. Pallis,"Department TFB-ATSI FI-ETSIINF, Polytechical University of Madrid (UPM), Campus de Montegancedo, 28660 Madrid, Spain",2014 4th International Conference on Wireless Mobile Communication and Healthcare - Transforming Healthcare Through Innovations in Mobile and Wireless Technologies (MOBIHEALTH),20150122,2014,,,263,266,"In the incoming world of Internet of Things (IoT) for healthcare, different distributed devices will gather, analyze and communicate real time medical information to open, private or hybrid clouds, making it possible to collect, store and analyze big data streams in several new forms, and activate context dependent alarms. This innovative data acquisition paradigm allows continuous and ubiquitous medical data access from any connected device over the Internet, and a novel health application ecosystem emerges. In these complex ecosystems could be insufficient to discuss only classical requirements regarding hardware issues and software support of individual elements. In the involved multidisciplinary development area, with intricate vertical and horizontal markets, it is essential a close collaboration between the corresponding stakeholders: endusers, application domain experts, hardware designers, software developers, market specialists, road mapping strategists and even the collaboration of visionaries to implement successful healthcare ecosystems. In this paper we describe some crucial systems engineering viewpoints to analyse the corresponding complex decision space. We complement the general examination of the IoT space by commenting some particular examples and specific details, which correspond to remarkable options of the involved dimensions.",,Electronic:978-1-63190-014-3; POD:978-1-4799-5024-9; USB:978-1-63190-013-6,10.1109/MOBIHEALTH.2014.7015961,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7015961,Internet of Things (IoT);big data analytics;cloud computing;eHealth;healthcare;mHealth;medical body area networks;systems engineering,Biomedical monitoring;Ecosystems;Medical services;Monitoring;Sensors;Systems engineering and theory;Wireless communication,Big Data;Internet of Things;biomedical equipment;cloud computing;data acquisition;data analysis;data privacy;electronic data interchange;health care;information retrieval;information storage;medical information systems;real-time systems;systems engineering;telemedicine,application domain expert;big data stream analysis;big data stream collection;big data stream storage;complex decision space;context dependent alarm activation;continuous medical data access;enduser;general IoT space examination;hardware designer;hardware requirement;health application ecosystem;healthcare Internet of Things;healthcare IoT challenge;healthcare IoT opportunities;healthcare ecosystem;horizontal market;hybrid cloud;innovative data acquisition paradigm;market specialist;multidisciplinary development area;open cloud;private cloud;real time medical information acquisition;real time medical information analysis;real time medical information communication;road mapping strategist;software developer;software support requirement;stakeholder collaboration;systems engineering;ubiquitous medical data access;vertical market;visionary collaboration,,0,,8,,no,3-5 Nov. 2014,,IEEE,IEEE Conference Publications
Optimization of relational database usage involving Big Data a model architecture for Big Data applications,E. E. A. Durham; A. Rosen; R. W. Harrison,"Department of Computer Science, Georgia State University, Atlanta, USA",2014 IEEE Symposium on Computational Intelligence and Data Mining (CIDM),20150115,2014,,,454,462,"Effective Big Data applications dynamically handle the retrieval of decisioned results based on stored large datasets efficiently. One effective method of requesting decisioned results, or querying, large datasets is the use of SQL and database management systems such as MySQL. But a problem with using relational databases to store huge datasets is the decisioned result retrieval time, which is often slow largely due to poorly written queries/decision requests. This work presents a model to re-architect Big Data applications in order to efficiently present decisioned results: lowering the volume of data being handled by the application itself, and significantly decreasing response wait times while allowing the flexibility and permanence of a standard relational SQL database, supplying optimal user satisfaction in today's Data Analytics world. We experimentally demonstrate the effectiveness of our approach.",,Electronic:978-1-4799-4518-4; POD:978-1-4799-4517-7,10.1109/CIDM.2014.7008703,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7008703,Big Data analysis;Business Intelligence;Data Mining;Relational database;SQL;materialized view;query;query optimization,Big data;Companies;Databases;Modems;Software;Standards,Big Data;SQL;data analysis;relational databases,Big Data applications;MySQL;data analytics;database management systems;model architecture;optimal user satisfaction;relational SQL database;relational database usage optimization,,0,,47,,no,9-12 Dec. 2014,,IEEE,IEEE Conference Publications
Optimized incremental state replication for automation controllers,S. Stattelmann; S. Sehestedt; T. Gamer,"ABB Corporate Research, Ladenburg, Germany",Proceedings of the 2014 IEEE Emerging Technology and Factory Automation (ETFA),20150112,2014,,,1,8,"This paper presents optimization techniques for implementing software-based redundancy in industrial control devices. Initially, a brief survey of software-based state replication techniques with a special focus on their applicability in industrial control devices is conducted. The scalability, predictability and low latency of the technique are of particular interest in this case. Based on this survey, an analytic evaluation of different implementation alternatives is performed. As part of this analysis, a novel state replication algorithm is introduced. The approach uses support from the compiler or runtime environment to detect changes in the application state with very low overhead. This information is used to replicate the state of an automation controller in a redundant setup. Lastly, experimental results using a prototype implementation of the presented technique demonstrate that the proposed novel approach is able to perform state replication with constant overhead.",1946-0740;19460740,Electronic:978-1-4799-4845-1; POD:978-1-4799-4844-4,10.1109/ETFA.2014.7005112,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7005112,,Automation;Checkpointing;Fault tolerant systems;Hardware;Memory management;Redundancy,control engineering computing;optimisation;software fault tolerance,automation controllers;industrial control devices;optimized incremental state replication;software-based redundancy;software-based state replication technique,,0,,11,,no,16-19 Sept. 2014,,IEEE,IEEE Conference Publications
Overcoming Limited Collaboration Channels in Distributed Intelligence Analysis: Visualization Tools and Design Seeds,B. Prue; M. Jenkins; L. D. Stern; J. Pfautz,"Charles River Analytics, Cambridge, MA, USA",2014 IEEE Joint Intelligence and Security Informatics Conference,20141206,2014,,,17,24,"Military intelligence analysis (IA) support tools are often developed using generalized models of IA that fail to take into consideration the real-world constraints put on analysts by factors such as organizational structures and cultures. IA in domains where distributed collaboration is required because direct communication and coordination is infeasible represents a challenge for generalized models of IA. This paper provides our analysis of distributed IA, which we conducted to support the design of software. We present a resulting set of capabilities that have been developed and deployed in an operational community. Our analysis approach and design focuses on extracting requirements and translating them into ""design seeds"" or guidelines for implementation, which are later used to verify that the resulting system meets the expressed requirements.",,CD-ROM:978-1-4799-6363-8; Electronic:978-1-4799-6364-5; POD:978-1-4799-6365-2,10.1109/JISIC.2014.13,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6975549,Collaboration;Distributed Analysis;Intelligence Analysis;Network Visualization;Visual Analytics,Analytical models;Cognition;Collaboration;Communities;Context;Training;Visualization,data analysis;data visualisation;military computing,design seeds;direct communication and coordination;distributed collaboration;distributed intelligence analysis;limited collaboration channels;military intelligence analysis support tools;operational community;organizational cultures;organizational structures;real-world constraints;visualization tools,,1,,33,,no,24-26 Sept. 2014,,IEEE,IEEE Conference Publications
Panel: The future of research in modeling & simulation,L. Yilmaz; S. J. E. Taylor; R. Fujimoto; F. Darema,"Computer Science and Software Engineering, Auburn University, AL 36849, USA",Proceedings of the Winter Simulation Conference 2014,20150126,2014,,,2797,2811,"Due to the increasing availability of data and wider use of analytics, the ingredients for increased reliance on modeling and simulation are now present. Tremendous progress has been made in the field of modeling and simulation over the last six decades. Software and methodologies have advanced greatly. In the area of weather, future-casts based on model predictions have become highly accurate and heavily relied upon. This is happening in other domains, as well. In a similar vein, drivers may come to rely upon future-casts of traffic that are based on predictions from models fed by sensor data. The need for and the capabilities of simulation have never been greater. This panel will examine the future of research in modeling and simulation by (1) examining prior progress, (2) pointing out current weaknesses and limitations, (3) highlighting directions for future research, and (4) discussing support for research including funding opportunities.",0891-7736;08917736,Electronic:978-1-4799-7486-3; POD:978-1-4799-7487-0,10.1109/WSC.2014.7020122,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7020122,,Analytical models;Communities;Computational modeling;Context;Data models;Predictive models;Synchronization,digital simulation,model predictions;modeling;sensor data;simulation;software,,2,,61,,no,7-10 Dec. 2014,,IEEE,IEEE Conference Publications
Performance analysis of virtualized environments using HPC Challenge benchmark suite and Analytic Hierarchy Process,R. Bakhshayeshi; M. K. Akbari; M. S. Javan,"Comput. Eng. & IT Dept., Islamic Azad Univ. of Garmsar, Garmsar, Iran",2014 Iranian Conference on Intelligent Systems (ICIS),20140421,2014,,,1,6,"Steep improvement of cloud computing in recent years, persuaded experts admit it as a suitable and appropriate substitution for traditional computing methods. Nowadays, more and more organizations are getting used to create private clouds, on the other hand, public clouds must be robust enough to handle scientific-driven computing requests of users in an efficient and cost effective manner. Apart from all these necessities, it is intellectual to improve the cloud infrastructure performance by appropriate choices. In this paper we are going to evaluate the performance of some virtualized environments, including VMware ESXi, KVM, Xen, Oracle VirtualBox, and VMware Workstation using HPC Challenge (HPCC) benchmark suite and Open MPI in order to represent solutions for virtualization layer of cloud computing architecture and then designate the best approach in general using Analytic Hierarchy Process.",,Electronic:978-1-4799-3351-8; POD:978-1-4799-3352-5,10.1109/IranianCIS.2014.6802585,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6802585,cloud computing;hypervisor;performance evaluation;virtualization,Bandwidth;Benchmark testing;Cloud computing;Operating systems;Virtual machine monitors;Virtualization;Workstations,analytic hierarchy process;cloud computing;software architecture;virtualisation,HPC challenge benchmark suite;KVM;Open MPI;Oracle VirtualBox;VMware ESXi;VMware Workstation;Xen;analytic hierarchy process;cloud computing architecture;cloud infrastructure;performance analysis;public clouds;scientific driven computing;steep improvement;virtualization layer;virtualized environments,,1,,26,,no,4-6 Feb. 2014,,IEEE,IEEE Conference Publications
"Performance Study of Spindle, A Web Analytics Query Engine Implemented in Spark",B. Amos; D. Tompkins,"Adobe Res. San Jose, San Jose, CA, USA",2014 IEEE 6th International Conference on Cloud Computing Technology and Science,20150212,2014,,,505,510,"This paper shares our experiences building and benchmarking Spindle as an open source Spark-based web analytics platform. Spindle's design has been motivated by real-world queries and data requiring concurrent, low latency query execution. We identify a search space of Spark tuning options and study their impact on Spark's performance. Results from a self-hosted six node cluster with one week of analytics data (13.1GB) indicate tuning options such as proper partitioning can cause a 5x performance improvement.",,Electronic:978-1-4799-4093-6; POD:978-1-4799-4092-9; USB:978-1-4799-4094-3,10.1109/CloudCom.2014.111,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7037709,data processing;distributed systems;performance study;web analytics,Context;Instruction sets;Libraries;Loading;Production;Sparks;Tuning,public domain software;query processing;software performance evaluation,Spark tuning options;Spindle performance study;Web analytics query engine;low latency query execution;open source Spark-based Web analytics platform;real-world queries;self-hosted six node cluster,,0,,17,,no,15-18 Dec. 2014,,IEEE,IEEE Conference Publications
Portable indecisive options selected assistance software based on Fuzzy AHP,Yang Li; HaoMing Zhou,"School of Information Engineering, Guangdong University of Technology, Guangzhou, China",2014 IEEE International Conference on Consumer Electronics - China,20150205,2014,,,1,4,"This paper presents a kind of indecisiveness selecting solution software installed on android mobile device which can help user to choose the ‰ÛÏbest‰Ûù one thing among the options in most cases. When used, the user can customize any related criterions about the things and program will dynamically generated a questionnaire based on it; User may answer the questionnaire and also can upload the problem to the cloud server invite friends or experts online to participate this survey; Program will analysis the survey results by Fuzzy Analytic Hierarchy Process (FAHP) method, ultimately selected the psychological optimal option from the candidate options.",,Electronic:978-1-4799-4756-0; POD:978-1-4799-4754-6,10.1109/ICCE-China.2014.7029865,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7029865,fuzzy AHP;multi-criteria decision;portable software;remote assistance;wide-range use,Analytic hierarchy process;Mobile communication;Servers;Smart phones;Software;Vectors,analytic hierarchy process;mobile computing;smart phones,FAHP method;android mobile device;assistance software;cloud server;fuzzy AHP;fuzzy analytic hierarchy process method;portable software;remote assistance,,0,,11,,no,9-13 April 2014,,IEEE,IEEE Conference Publications
Predicting player churn in the wild,F. Hadiji; R. Sifa; A. Drachen; C. Thurau; K. Kersting; C. Bauckhage,"Technical University Dortmund, Germany",2014 IEEE Conference on Computational Intelligence and Games,20141023,2014,,,1,8,"Free-to-Play or ‰ÛÏfreemium‰Ûù games represent a fundamental shift in the business models of the game industry, facilitated by the increasing use of online distribution platforms and the introduction of increasingly powerful mobile platforms. The ability of a game development company to analyze and derive insights from behavioral telemetry is crucial to the success of these games which rely on in-game purchases and in-game advertising to generate revenue, and for the company to remain competitive in a global marketplace. The ability to model, understand and predict future player behavior has a crucial value, allowing developers to obtain data-driven insights to inform design, development and marketing strategies. One of the key challenges is modeling and predicting player churn. This paper presents the first cross-game study of churn prediction in Free-to-Play games. Churn in games is discussed and thoroughly defined as a formal problem, aligning with industry standards. Furthermore, a range of features which are generic to games are defined and evaluated for their usefulness in predicting player churn, e.g. playtime, session length and session intervals. Using these behavioral features, combined with the individual retention model for each game in the dataset used, we develop a broadly applicable churn prediction model, which does not rely on game-design specific features. The presented classifiers are applied on a dataset covering five free-to-play games resulting in high accuracy churn prediction.",2325-4270;23254270,Electronic:978-1-4799-3547-5; POD:978-1-4799-3548-2,10.1109/CIG.2014.6932876,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6932876,behavior;behavior modeling;churn;churn prediction;free-to-play;freemium;game analytics;game data mining;games,Data mining;Games;Mobile communication,advertising;computer games;mobile computing;public domain software,behavioral telemetry;business models;free-to-play games;freemium games;future player behavior prediction;game development company;game industry;game-design specific features;global marketplace;in-game advertising;in-game purchases;industry standards;marketing strategies;mobile platforms;online distribution platforms;player chum prediction;session intervals;session length,,4,,25,,no,26-29 Aug. 2014,,IEEE,IEEE Conference Publications
Principles of Software-Defined Elastic Systems for Big Data Analytics,H. L. Truong; S. Dustdar,,2014 IEEE International Conference on Cloud Engineering,20140922,2014,,,562,567,"Techniques for big data analytics should support principles of elasticity that are inherent in types of data and data resources being analyzed, computational models and computing units used for analyzing data, and the quality of results expected from the consumer. In this paper, we analyze and present these principles and their consequences for software-defined environments to support data analytics. We will conceptualize software-defined elastic systems for data analytics and present a case study in smart city management, urban mobility and energy systems with our elasticity supports.",,Electronic:978-1-4799-3766-0; POD:978-1-4799-3768-4,10.1109/IC2E.2014.67,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6903529,,Analytical models;Big data;Cities and towns;Computational modeling;Data models;Elasticity;Software,Big Data;data analysis,big data analytics techniques;computational models;computing units;data resources;data types;elasticity supports;energy systems;smart city management;software-defined elastic systems;software-defined environments;urban mobility,,3,,18,,no,11-14 March 2014,,IEEE,IEEE Conference Publications
Process mining software repositories from student projects in an undergraduate software engineering course,,,,,2014,,,,,"An undergraduate level Software Engineering courses generally consists of a team-based semester long project and emphasizes on both technical and managerial skills. Software Engineering is a practice-oriented and applied discipline and hence there is an emphasis on hands-on development, process, usage of tools in addition to theory and basic concepts. We present an approach for mining the process data (process mining) from software repositories archiving data generated as a result of constructing software by student teams in an educational setting. We present an application of mining three software repositories: team wiki (used during requirement engineering), version control system (development and maintenance) and issue tracking system (corrective and adaptive maintenance) in the context of an undergraduate Software Engineering course. We propose visualizations, metrics and algorithms to provide an insight into practices and procedures followed during various phases of a software development life-cycle. The proposed visualizations and metrics (learning analytics) provide a multi-faceted view to the instructor serving as a feedback tool on development process and quality by students. We mine the event logs produced by software repositories and derive insights such as degree of individual contributions in a team, quality of commit messages, intensity and consistency of commit activities, bug fixing process trend and quality, component and developer entropy, process compliance and verification. We present our empirical analysis on a software repository dataset consisting of 19 teams of 5 members each and discuss challenges, limitations and recommendations.",,,,http://dl.acm.org/citation.cfm?id=2591152&CFID=696538919&CFTOKEN=83912867,,,,,,,,,,yes,,,,
Progressive Testbed Application for Performance Analysis in Real Time Ad Hoc Networks Using SAP HANA,R. Khanam; C. Gaurav; D. Chandramouleeswaran,"Tata Consultancy Services Bangalore, Bangalore, India",2014 Fourth International Conference on Advances in Computing and Communications,20140929,2014,,,171,174,"This paper proposes and subsequently delineates quantification of network security metrics using software defined networking approach in real time using a progressive testbed. This comprehensive testbed implements computation of trust values which lend sentient decision making qualities to the participant nodes in a network and fortify it against threats like blackhole and flooding attacks. AODV and OLSR protocols were tested in real time under ideal and malicious environment using the testbed as the controlling point. With emphasis on reliability, interpreting voluminous data, monitoring attacks immediately with negligible time lag, the paper concludes by justifying the use of SAP HANA and UI5 for the testbed.",,Electronic:978-1-4799-4363-0; POD:978-1-4799-4362-3,10.1109/ICACC.2014.48,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6906017,Ad-Hoc Network;HANA- High Performance Analytic Appliance;Performance Analysis;Security Metrics;Trust Model;UI5 SAP User Interface Technology,Ad hoc networks;Equations;Mathematical model;Measurement;Protocols;Routing;Security,ad hoc networks;routing protocols;telecommunication security,AODV protocol;OLSR protocol;SAP HANA;network security metrics;progressive testbed;real time ad hoc networks;sentient decision making;software defined networking;trust values,,0,,7,,no,27-29 Aug. 2014,,IEEE,IEEE Conference Publications
Protection against remote code execution exploits of popular applications in Windows,J. Wu; A. Arrott; F. C. C. Osorio,"PC Security Labs, China",2014 9th International Conference on Malicious and Unwanted Software: The Americas (MALWARE),20150115,2014,,,26,31,"The objective of Malicious Remote Code Execution Exploits is to remotely execute code transparently to the user, and without relying on user interaction, in order to infect targeted machines. This comparative study examines the effectiveness of different proactive exploit mitigation technologies included in popular endpoint security products and specialized anti-exploit tools. The study focuses on exploits of popular applications running on Windows XP SP3 with Internet Explorer (IE8). As such, the Microsoft Enhanced Mitigation Experience Toolkit (MS-EMET) is used as a reference standard for all exploit mitigation solutions. The study compares the effectiveness of endpoint security products and anti-exploit tools by separating measurements of protections in common with MS-EMET from measures of protections supplemental to MS-EMET. This is done in order to understand not just the relative competitive effectiveness of the individual products and tools but also to understand the overall capabilities of the Windows endpoint security solutions to combat the remote code execution exploit capabilities of the overall Windows malware ecosystem.",,DVD:978-1-4799-7327-9; Electronic:978-1-4799-7329-3; POD:978-1-4799-7330-9,10.1109/MALWARE.2014.6999416,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6999416,,Computer crime;Internet;Java;Malware;Servers;Vectors,data protection;invasive software;operating systems (computers),Internet Explorer;MS-EMET;Microsoft Enhanced Mitigation Experience Toolkit;Windows XP SP3;Windows malware ecosystem;data protection;endpoint security products;remote code execution,,2,,5,,no,28-30 Oct. 2014,,IEEE,IEEE Conference Publications
Pythia: Faster Big Data in Motion through Predictive Software-Defined Network Optimization at Runtime,M. V. Neves; C. A. F. D. Rose; K. Katrinis; H. Franke,"Pontifical Catholic Univ. of Rio Grande do Sul, Porto Alegre, Brazil",2014 IEEE 28th International Parallel and Distributed Processing Symposium,20140814,2014,,,82,90,"The rise of Internet of Things sensors, social networking and mobile devices has led to an explosion of available data. Gaining insights into this data has led to the area of Big Data analytics. The MapReduce framework, as implemented in Hadoop, is one of the most popular frameworks for Big Data analysis. To handle the ever-increasing data size, Hadoop is a scalable framework that allows dedicated, seemingly unbound numbers of servers to participate in the analytics process. Response time of an analytics request is an important factor for time to value/insights. While the compute and disk I/O requirements can be scaled with the number of servers, scaling the system leads to increased network traffic. Arguably, the communication-heavy phase of MapReduce contributes significantly to the overall response time, the problem is further aggravated, if communication patterns are heavily skewed, as is not uncommon in many MapReduce workloads. In this paper we present a system that reduces the skew impact by transparently predicting data communication volume at runtime and mapping the many end-to-end flows among the various processes to the underlying network, using emerging software-defined networking technologies to avoid hotspots in the network. Dependent on the network oversubscription ratio, we demonstrate reduction in job completion time between 3% and 46% for popular MapReduce benchmarks like Sort and Nutch.",1530-2075;15302075,Electronic:978-1-4799-3800-1; POD:978-1-4799-3801-8,10.1109/IPDPS.2014.20,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877244,Data communication;Data processing;Distributed computing,Big data;Instruments;Job shop scheduling;Resource management;Routing;Runtime;Servers,Big Data;computer networks;parallel programming;telecommunication traffic,Big Data analytics;Hadoop;MapReduce workloads;Nutch MapReduce benchmark;Pythia;Sort MapReduce benchmark;communication patterns;communication-heavy phase;compute requirements;data communication volume prediction;data size;disk I/O requirements;end-to-end flow mapping;job completion time reduction;network oversubscription ratio;network traffic;predictive software-defined network optimization;response time;runtime analysis;scalable framework;system scaling;unbound server numbers,,3,,23,,no,19-23 May 2014,,IEEE,IEEE Conference Publications
Quality assessment model for wheat storage warehouse using Analytic Hierarchy Process and BP Neural Network,D. Priyanka; S. Manmohan; H. Chowdhary,"Lovely Professional University, India","Proceedings of 3rd International Conference on Reliability, Infocom Technologies and Optimization",20150122,2014,,,1,6,In India the quality of the wheat storage warehouse is assessed manually by officials and there is no scientific model present for the same. In this paper we have developed a model for the quality assessment using the Analytical Hierarchy Process and the Back Propagation Neural Network. The simulations are carried out in MATLAB software and the results are deduced thereafter. The results and the correlation between actual results and the deduced results show the validity of the developed model. It provides an effective way to assess the quality in short time and with a prescribed scientific model.,,Electronic:978-1-4799-6896-1; POD:978-1-4799-6897-8,10.1109/ICRITO.2014.7014734,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7014734,analytic hierarchy process (AHP);back propagation neural network (BPNN);consistency ratio;quality,Analytic hierarchy process;Analytical models;Biological neural networks;Mathematical model;Quality assessment,analytic hierarchy process;backpropagation;neural nets;warehousing,BP neural network;MATLAB software;analytic hierarchy process;back propagation neural network;quality assessment model;wheat storage warehouse,,0,,5,,no,8-10 Oct. 2014,,IEEE,IEEE Conference Publications
Quantitative analysis of graduate-level engineering management programs,I. Bozkurt,"Engineering Management Program, University of Houston - Clear Lake, TX, USA",2014 IEEE International Technology Management Conference,20141009,2014,,,1,8,"The purpose of this study is to analyze the curricula of Engineering Management programs awarding a graduate-level EM degree (Master's and Doctoral). The analysis is conducted using text analysis software, which brings the necessary rigor and objectivity required when dealing with qualitative data. Preliminary research using multiple sources (American Society for Engineering Management and American Society of Engineering Education) result in a list of over one-hundred universities that offer a graduate-level degree in EM. To conduct the analysis, a curriculum database will be formed that includes course names and descriptions. This data will provide the input to the conceptual and semantic analysis using a computer tool. The results of this analysis will provide an insight into different threads, commonalities and differences between EM programs.",,Electronic:978-1-4799-3312-9; POD:978-1-4799-3313-6,10.1109/ITMC.2014.6918590,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6918590,Curriculum;Engineering Management;Graduate Degree;Quantitative;Text Analytics,Data analysis;Databases;Educational institutions;Medical services;Organizations;Research and development management;Software,educational administrative data processing;educational courses;educational institutions;engineering education;further education;management education;text analysis,course descriptions;course names;curriculum database;graduate-level EM degree;graduate-level engineering management programs;quantitative analysis;text analysis software,,0,,7,,no,12-15 June 2014,,IEEE,IEEE Conference Publications
R-Store: A scalable distributed system for supporting real-time analytics,F. Li; M. T. Ì_zsu; G. Chen; B. C. Ooi,"Sch. of Comput., Nat. Univ. of Singapore, Singapore, Singapore",2014 IEEE 30th International Conference on Data Engineering,20140519,2014,,,40,51,"It is widely recognized that OLTP and OLAP queries have different data access patterns, processing needs and requirements. Hence, the OLTP queries and OLAP queries are typically handled by two different systems, and the data are periodically extracted from the OLTP system, transformed and loaded into the OLAP system for data analysis. With the awareness of the ability of big data in providing enterprises useful insights from vast amounts of data, effective and timely decisions derived from real-time analytics are important. It is therefore desirable to provide real-time OLAP querying support, where OLAP queries read the latest data while OLTP queries create the new versions. In this paper, we propose R-Store, a scalable distributed system for supporting real-time OLAP by extending the MapReduce framework. We extend an open source distributed key/value system, HBase, as the underlying storage system that stores data cube and real-time data. When real-time data are updated, they are streamed to a streaming MapReduce, namely Hstreaming, for updating the cube on incremental basis. Based on the metadata stored in the storage system, either the data cube or OLTP database or both are used by the MapReduce jobs for OLAP queries. We propose techniques to efficiently scan the real-time data in the storage system, and design an adaptive algorithm to process the real-time query based on our proposed cost model. The main objectives are to ensure the freshness of answers and low processing latency. The experiments conducted on the TPC-H data set demonstrate the effectiveness and efficiency of our approach.",1063-6382;10636382,Electronic:978-1-4799-2555-1; POD:978-1-4799-2556-8; USB:978-1-4799-2554-4,10.1109/ICDE.2014.6816638,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6816638,,Compaction;Computer architecture;Data models;Distributed databases;Educational institutions;Maintenance engineering;Real-time systems,Big Data;data analysis;data mining;distributed processing;meta data;public domain software;query processing;storage management,HBase;Hstreaming;MapReduce framework;OLAP queries;OLTP database;OLTP queries;R-Store;TPC-H data set;adaptive algorithm;big data;data access patterns;data analysis;data cube storage;metadata;open source distributed key-value system;real-time analytics;real-time data storage;scalable distributed system;storage system,,2,,28,,no,March 31 2014-April 4 2014,,IEEE,IEEE Conference Publications
Recent advances in investigation of propagation of GNSS signals through the ionosphere with local random time-dependent inhomogeneities,N. N. Zernov; V. E. Gherm,"Department of Radio Physics, the University of St. Petersburg, Ulyanovskaya, str. 1, Petrodvorets, 198504, St. Petersburg, Russia",2014 XXXIth URSI General Assembly and Scientific Symposium (URSI GASS),20141020,2014,,,1,4,Analytic theory is developed for solving the set of Markov's momenta parabolic equations for the case of the inhomogeneous ionospheric background layer with the time varying local random inhomogeneities of the electron density in order to describe the GNSS signal propagation in the regime of strong scintillation formed already at the heights of the inhomogenous ionosphere. It is then employed for constructing the software simulator capable of generating the field amplitude and phase random time series at any given height above the Earth's surface and arbitrary geometry of propagation.,,Electronic:978-1-4673-5225-3; POD:978-1-4673-5224-6,10.1109/URSIGASS.2014.6929721,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6929721,,Coherence;Equations;Global Positioning System;Ionosphere;Markov processes;Mathematical model;Nonhomogeneous media,Markov processes;ionospheric electromagnetic wave propagation;ionospheric techniques;satellite navigation;time series,Earth surface;GNSS signal propagation;Markov momenta parabolic equations;electron density;field amplitude;inhomogeneous ionospheric background layer;inhomogenous ionosphere;local random time-dependent inhomogeneities;phase random time series;time varying local random inhomogeneities,,0,,9,,no,16-23 Aug. 2014,,IEEE,IEEE Conference Publications
Recognizing Gaits Across Views Through Correlated Motion Co-Clustering,W. Kusakunniran; Q. Wu; J. Zhang; H. Li; L. Wang,"Faculty of Information and Communication Technology, Mahidol University, Nakhonpathom, Thailand",IEEE Transactions on Image Processing,20140102,2014,23,2,696,709,"Human gait is an important biometric feature, which can be used to identify a person remotely. However, view change can cause significant difficulties for gait recognition because it will alter available visual features for matching substantially. Moreover, it is observed that different parts of gait will be affected differently by view change. By exploring relations between two gaits from two different views, it is also observed that a part of gait in one view is more related to a typical part than any other parts of gait in another view. A new method proposed in this paper considers such variance of correlations between gaits across views that is not explicitly analyzed in the other existing methods. In our method, a novel motion co-clustering is carried out to partition the most related parts of gaits from different views into the same group. In this way, relationships between gaits from different views will be more precisely described based on multiple groups of the motion co-clustering instead of a single correlation descriptor. Inside each group, a linear correlation between gait information across views is further maximized through canonical correlation analysis (CCA). Consequently, gait information in one view can be projected onto another view through a linear approximation under the trained CCA subspaces. In the end, a similarity between gaits originally recorded from different views can be measured under the approximately same view. Comprehensive experiments based on widely adopted gait databases have shown that our method outperforms the state-of-the-art.",1057-7149;10577149,,10.1109/TIP.2013.2294552,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6680737,Gait recognition;bipartite graph multipartitioning;canonical correlation analysis;co-clustering;human identification;view change,Australia;Bipartite graph;Correlation;Gait recognition;Legged locomotion;Training;Vectors,approximation theory;correlation methods;gait analysis;image recognition,CCA;canonical correlation analysis;correlated motion coclustering;correlation descriptor;correlation variance;gait information;gait recognition;linear approximation,0,5,,47,,no,Feb. 2014,,IEEE,IEEE Journals & Magazines
Recomputation.org: Experiences of Its First Year and Lessons Learned,I. P. Gent; L. Kotthoff,"Sch. of Comput. Sci., Univ. of St. Andrews, St. Andrews, UK",2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing,20150202,2014,,,968,973,"We founded recomputation.org about 18 months ago as we write. The site is intended to serve as a repository for computational experiments, embodied in virtual machines so that they can be recomputed at will by other researchers. We reflect in this paper on those aspects of recomputation.org that have worked well, those that have worked less well, and to what extent our views have changed on reproducibility in computational science.",,Electronic:978-1-4799-7881-6; POD:978-1-4799-7882-3,10.1109/UCC.2014.158,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7027625,,Computer science;Educational institutions;Games;Runtime;Software;Tutorials;Virtual machining,Internet;Web sites;virtual machines,Web site;computational experiment repository;recomputation.org;reproducibility;virtual machine,,0,,14,,no,8-11 Dec. 2014,,IEEE,IEEE Conference Publications
Research on dependability of cloud computing systems,Y. Pan; N. Hu,"Reliability Data Center, China Electronic Product Reliability and Environmental, Testing Research Institute, Guangzhou, China","2014 10th International Conference on Reliability, Maintainability and Safety (ICRMS)",20150514,2014,,,435,439,"With the continuous growth of application requirements and a significant advance in the research of cloud computing systems, a large number of cloud computing systems nowadays based on different structures and virtualization technologies are still being developed. However, dependability of cloud computing system is always a critical issue for all the cloud service providers, brokers, carriers and consumers around the world. How to ensure the dependability of cloud computing systems is still not well solved by former researchers. This paper first introduces the infrastructure of a cloud computing system and its major actors. Based on the research of the running mode of a cloud computing system, main influencing factors of cloud computing dependability and its normal failure modes are given. It provides a generic definition of cloud computing system dependability, which including availability, performability, security, recoverability and so on. We also discuss some methods to establish cloud computing dependability models, such as analytic method, state space method and simulating method. In order to evaluate the validity of the cloud dependability measurement method, a novel simulating approach is proposed with well-designed framework and procedure. Through reasonable simulating and analysis, proper solutions can be promoted to find and solve the problem related to cloud computing systems dependability. Our future work will focus on an integrated software platform for cloud computing system dependability simulating and verification.",,Electronic:978-1-4799-6632-5; POD:978-1-4799-6633-2,10.1109/ICRMS.2014.7107234,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7107234,cloud computing;dependability;failure;simulating,Cloud computing;Computational modeling;Hardware;Reliability;Security;Servers,cloud computing;integrated software;security of data;state-space methods;virtualisation,application requirements;availability;cloud computing dependability models;cloud computing systems;cloud dependability measurement method;cloud service brokers;cloud service providers;integrated software platform;performability;recoverability;security;simulating method;state space method;virtualization technologies,,0,,6,,no,6-8 Aug. 2014,,IEEE,IEEE Conference Publications
Research on reliability evaluation and sensitivity analysis of domain software based on AHP method,Chen Qu; Bao Tie; Zheng Wanbo; Lian Wei,"College of Computer Science and Technology, Jilin University, Changchun, China",2014 IEEE Workshop on Advanced Research and Technology in Industry Applications (WARTIA),20141206,2014,,,502,505,"This paper aims at the key problem of software engineering - assessment and sensitivity analysis of the domain software reliability to study. Through the existing research, establish the evaluation attributes of comprehensive categorical, and analysis assessment evidence the assessment model computational logic by AHP, establish corresponding assessment method. Finally, aim at the power plant information management system analysis the domain software reliability by this method, and carry on the attribute sensitivity analysis based on relative data. Through practical experiments the paper prove this method can analysis and evaluate the domain software reliability clearly, and propose sensitivity analysis.",,DVD:978-1-4799-6988-3; Electronic:978-1-4799-6989-0; POD:978-1-4799-6990-6,10.1109/WARTIA.2014.6976306,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6976306,reliability assessment;sensitivity analysis;software engineering;software reliability,Information management;Power generation;Sensitivity analysis;Software reliability;Usability,analytic hierarchy process;information management;sensitivity analysis;software reliability,AHP method;analysis assessment evidence;assessment model computational logic;comprehensive categorical evidence;domain software reliability;evaluation attributes;power plant information management system analysis;relative data;sensitivity analysis,,0,,14,,no,29-30 Sept. 2014,,IEEE,IEEE Conference Publications
Resource-efficient regular expression matching architecture for text analytics,K. Atasu,"IBM Res. - Zurich, Zurich, Switzerland","2014 IEEE 25th International Conference on Application-Specific Systems, Architectures and Processors",20140731,2014,,,1,8,"Text analytics systems, such as IBM's SystemT software, rely on regular expressions (regexs) and dictionaries for transforming unstructured data into a structured format. Unlike network intrusion detection systems, text analytics systems compute and report precisely where the specific and sensitive information starts and ends in a text document. Therefore, advanced regex matching functions, such as start-offset reporting, capturing groups, and leftmost match computation are heavily used in text analytics systems. We present a novel regex matching architecture that supports such functions in a resource-efficient way. The resource efficiency is achieved by 1) eliminating state replication, 2) avoiding expensive offset comparison operations in leftmost match computation, and 3) minimizing the number of offset registers. Experiments on regex sets from text analytics and network intrusion detection domains, using an Altera Stratix IV FPGA, show that the proposed architecture achieves a more than threefold reduction of the logic resources used and a more than 1.25-fold increase of the clock frequency with respect to a recently proposed architecture that supports identical features.",1063-6862;10636862,Electronic:978-1-4799-3609-0; POD:978-1-4799-3610-6; USB:978-1-4799-3608-3,10.1109/ASAP.2014.6868623,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6868623,,Clocks;Computer architecture;Hardware;Information retrieval;Redundancy;Registers;Vectors,data structures;dictionaries;field programmable gate arrays;text analysis,Altera Stratix IV FPGA;IBM SystemT software;advanced regex matching functions;capturing groups;clock frequency;dictionaries;leftmost match computation;logic resources;network intrusion detection domains;offset registers;resource efficiency;resource-efficient regular expression matching architecture;sensitive information;start-offset reporting;structured format;text analytics;text document;unstructured data,,1,,25,,no,18-20 June 2014,,IEEE,IEEE Conference Publications
Reverse Engineering PL/SQL Legacy Code: An Experience Report,M. Habringer; M. Moser; J. Pichler,"voestalpine Stahl GmbH, Linz, Austria",2014 IEEE International Conference on Software Maintenance and Evolution,20141206,2014,,,553,556,"The reengineering of legacy code is a tedious endeavor. Automatic transformation of legacy code from an old technology to a new one preserves potential problems in legacy code with respect to obsolete, changed, and new business cases. On the other hand, manual analysis of legacy code without assistance of original developers is time consuming and error-prone. For the purpose of reengineering PL/SQL legacy code in the steel making domain, we developed tool support for the reverse engineering of PL/SQL code into a more abstract and comprehensive representation. This representation then serves as input for stakeholders to manually analyze legacy code, to identify obsolete and missing business cases, and, finally, to support the re-implementation of a new system. In this paper we briefly introduce the tool and present results of reverse engineering PL/SQL legacy code in the steel making domain. We show how stakeholders are supported in analyzing legacy code by means of general-purpose analysis techniques combined with domain-specific representations and conclude with some of the lessons learned.",1063-6773;10636773,Electronic:978-1-4799-6146-7; POD:978-1-4799-6147-4,10.1109/ICSME.2014.93,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6976137,program comprehension;reverse engineering;source code analysis,Business;Databases;Flow graphs;Production;Reverse engineering;Software;Steel,SQL;reverse engineering;software maintenance,automatic transformation;business cases;domain specific representations;general-purpose analysis techniques;legacy code analysis;reverse engineering PL/SQL legacy code;steel making,,1,,6,,no,Sept. 29 2014-Oct. 3 2014,,IEEE,IEEE Conference Publications
Rock Stars of Big Data Analytics [Advertisement],,,IEEE Software,20140915,2014,31,5,c4,c4,Advertisement: IEEE Computer Society.,0740-7459;07407459,,10.1109/MS.2014.121,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6898689,,,,,,0,,,,no,Sept.-Oct. 2014,,IEEE,IEEE Journals & Magazines
Scalar: Systematic Scalability Analysis with the Universal Scalability Law,T. Heyman; D. Preuveneers; W. Joosen,"iMinds-DistriNet, KU Leuven, Leuven, Belgium",2014 International Conference on Future Internet of Things and Cloud,20141215,2014,,,497,504,"Analyzing the scalability and quality of service of large scale distributed systems requires a highly scalable benchmarking framework with built-in communication and synchronisation functionality, which are features that are lacking in current load generation tools. This paper documents Scalar, our distributed, extensible scalability analysis tool that can generate high request volumes using multiple communicating, coordinated nodes. We show how Scalar offers analytics capabilities that support the Universal Scalability Law. We illustrate Scalar on an electronic payment case study, and find that the framework supports complex work flows and is able to characterize and give predictive insights into the quality of service and relative capacity of the system under test in function of the user load.",,Electronic:978-1-4799-4357-9; POD:978-1-4799-4356-2,10.1109/FiCloud.2014.88,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6984244,benchmarking;distributed systems;load testing;scalability,Benchmark testing;Browsers;Business;Distributed databases;Scalability;Synchronization,distributed processing;software quality;ubiquitous computing,Scalar;electronic payment case;large scale distributed system;load generation tool;quality of service;systematic scalability analysis;universal scalability law,,0,,9,,no,27-29 Aug. 2014,,IEEE,IEEE Conference Publications
Scaling Irregular Applications through Data Aggregation and Software Multithreading,A. Morari; A. Tumeo; D. ChavarrÌ_a-Miranda; O. Villa; M. Valero,"Pacific Northwest Nat. Lab., Richland, WA, USA",2014 IEEE 28th International Parallel and Distributed Processing Symposium,20140814,2014,,,1126,1135,"Emerging applications in areas such as bioinformatics, data analytics, semantic databases and knowledge discovery employ datasets from tens to hundreds of terabytes. Currently, only distributed memory clusters have enough aggregate space to enable in-memory processing of datasets of this size. However, in addition to large sizes, the data structures used by these new application classes are usually characterized by unpredictable and fine-grained accesses: i.e., they present an irregular behavior. Traditional commodity clusters, instead, exploit cache-based processor and high-bandwidth networks optimized for locality, regular computation and bulk communication. For these reasons, irregular applications are inefficient on these systems, and require custom, hand-coded optimizations to provide scaling in both performance and size. Lightweight software multithreading, which enables tolerating data access latencies by overlapping network communication with computation, and aggregation, which allows reducing overheads and increasing bandwidth utilization by coalescing fine-grained network messages, are key techniques that can speed up the performance of large scale irregular applications on commodity clusters. In this paper we describe GMT (Global Memory and Threading), a runtime system library that couples software multithreading and message aggregation together with a Partitioned Global Address Space (PGAS) data model to enable higher performance and scaling of irregular applications on multi-node systems. We present the architecture of the runtime, explaining how it is designed around these two critical techniques. We show that irregular applications written using our runtime can outperform, even by orders of magnitude, the corresponding applications written using other programming models that do not exploit these techniques.",1530-2075;15302075,Electronic:978-1-4799-3800-1; POD:978-1-4799-3801-8,10.1109/IPDPS.2014.117,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877341,Multithreading;PGAS;aggregation;semantic graph databases,Arrays;Bandwidth;Electronics packaging;Message systems;Multithreading;Runtime;Software,data models;data structures;multi-threading;software architecture;software libraries;software performance evaluation,GMT;PGAS data model;bandwidth utilization;commodity clusters;custom hand-coded optimization;data access latency;data aggregation;data structures;distributed memory clusters;fine-grained access;fine-grained network messages;global memory and threading;in-memory dataset processing;large scale irregular application performance;lightweight software multithreading;message aggregation;multinode systems;overhead reduction;partitioned global address space data model;performance scaling;runtime architecture;runtime system library,,5,,29,,no,19-23 May 2014,,IEEE,IEEE Conference Publications
SCOOP - The Social Collaboratory for Outcome Oriented Primary Care,M. Price; J. H. Weber; G. McCallum,"Dept. of Family Practice, Univ. of British Columbia, Vancouver, BC, Canada",2014 IEEE International Conference on Healthcare Informatics,20150305,2014,,,210,215,"Many primary care clinics have transitioned from paper-based record keeping to computer-based Electronic Medical Record (EMR) systems. This transition provides opportunities for computer-based data analytics in support of practice improvement and more evidence-based clinical research. Unfortunately, the data in primary care EMRs is often not readily accessible to researchers, who often have to overcome significant political, organizational and technical hurdles before gaining access to such data. As a consequence, knowledge discovery and translation has been slow and burdensome in this area. Primary care research networks (PCRN) have been proposed as a way to addressing these limitations. This paper reports on the development of a PCRN in British Columbia, referred to as SCOOP (The Social Collaboratory for Outcome Oriented Primary Care). We describe its technical architecture and draw comparisons to related and previous initiatives.",,Electronic:978-1-4799-5701-9; POD:978-1-4799-5702-6; USB:978-1-4799-5700-2,10.1109/ICHI.2014.36,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7052491,distributed data query network;electronic medical records;primary care;research,Computer architecture;Data models;Data privacy;Distributed databases;Educational institutions;Software;Standards,data analysis;data mining;electronic health records;health care;medical computing,British Columbia;PCRN;SCOOP;Social Collaboratory for Outcome Oriented Primary care;computer-based EMR systems;computer-based data analytics;computer-based electronic medical record systems;knowledge discovery;primary care clinics;primary care research networks,,0,,19,,no,15-17 Sept. 2014,,IEEE,IEEE Conference Publications
SCOOP -- The Social Collaboratory for Outcome Oriented Primary Care,M. Price; J. H. Weber; G. McCallum,"Dept. of Family Practice, Univ. of British Columbia, Vancouver, BC, Canada",2014 IEEE 27th International Symposium on Computer-Based Medical Systems,20140825,2014,,,519,520,"Many primary care clinics have transitioned from paper-based record keeping to computer-based Electronic Medical Record (EMR) systems. This transition provides opportunities for computer-based data analytics in support of practice improvement and more evidence-based clinical research. Unfortunately, the data in primary care EMRs is often not readily accessible to researchers, who often have to overcome significant political, organizational and technical hurdles before gaining access to such data. As a consequence, knowledge discovery and translation has been slow and burdensome in this area. Primary care research networks (PCRN) have been proposed as a way to addressing these limitations. This paper reports on the development of a PCRN in British Columbia, referred to as SCOOP (The Social Collaboratory for Outcome Oriented Primary Care). We describe its technical architecture and draw comparisons to related and previous initiatives.",1063-7125;10637125,Electronic:978-1-4799-4435-4; POD:978-1-4799-4434-7,10.1109/CBMS.2014.118,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6881955,EMR;health research network;primary care,Aggregates;Computer architecture;Educational institutions;Ethics;Sociology;Software;Statistics,data mining;electronic health records;health care,British Columbia;PCRN;SCOOP;Social Collaboratory for Outcome Oriented Primary Care;computer-based EMR systems;computer-based data analytics;computer-based electronic medical record systems;evidence-based clinical research;knowledge discovery;knowledge translation;paper-based record keeping;primary care clinics;primary care research networks,,0,,8,,no,27-29 May 2014,,IEEE,IEEE Conference Publications
Security configuration analytics using video games,M. N. Alsaleh; E. A. Al-Shaer,"University of North Carolina at Charlotte, USA",2014 IEEE Conference on Communications and Network Security,20141229,2014,,,256,264,"Computing systems today have a large number of security configuration settings that enforce security properties. However, vulnerabilities and incorrect configuration increase the potential for attacks. Provable verification and simulation tools have been introduced to eliminate configuration conflicts and weaknesses, which can increase system robustness against attacks. Most of these tools require special knowledge in formal methods and precise specification for requirements in special languages, in addition to their excessive need for computing resources. Video games have been utilized by researchers to make educational software more attractive and engaging. Publishing these games for crowdsourcing can also stimulate competition between players and increase the game educational value. In this paper we introduce a game interface, called NetMaze, that represents the network configuration verification problem as a video game and allows for attack analysis. We aim to make the security analysis and hardening usable and accurately achievable, using the power of video games and the wisdom of crowdsourcing. Players can easily discover weaknesses in network configuration and investigate new attack scenarios. In addition, the gameplay scenarios can also be used to analyze and learn attack attribution considering human factors. In this paper, we present a provable mapping from the network configuration to 3D game objects.",,Electronic:978-1-4799-5890-0; POD:978-1-4799-5891-7,10.1109/CNS.2014.6997493,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6997493,,Communication networks;Computational modeling;Conferences;Games;Network topology;Security;Topology,computer games;courseware;formal verification;human factors;security of data;specification languages;user interfaces,3D game object;NetMaze;attack analysis;attack attribution;computing systems;configuration conflict;crowdsourcing;educational software;formal methods;game educational value;game interface;gameplay scenario;human factor;network configuration verification problem;provable mapping;provable verification;security analysis;security configuration analytics;security configuration settings;security property;simulation tool;special languages;system robustness;video games;vulnerability,,0,,23,,no,29-31 Oct. 2014,,IEEE,IEEE Conference Publications
Security Threat Analytics and Countermeasure Synthesis for Power System State Estimation,M. A. Rahman; E. A. Shaer; R. G. Kavasseri,"Dept. of Software & Inf. Syst., Univ. of North Carolina at Charlotte, Charlotte, NC, USA",2014 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks,20140922,2014,,,156,167,"State estimation plays a critically important role in ensuring the secure and reliable operation of the power grid. However, recent works have shown that the widely used weighted least squares (WLS) estimator, which uses several system wide measurements, is vulnerable to cyber attacks wherein an adversary can alter certain measurements to corrupt the estimator's solution, but evade the estimator's existing bad data detection algorithms and thus remain invisible to the system operator. Realistically, such a stealthy attack in its most general form has several constraints, particularly in terms of an adversary's knowledge and resources for achieving a desired attack outcome. In this light, we present a formal framework to systematically investigate the feasibility of stealthy attacks considering constraints of the adversary. In addition, unlike prior works, our approach allows the modeling of attacks on topology mappings, where an adversary can drastically strengthen stealthy attacks by intentionally introducing topology errors. Moreover, we show that this framework allows an operator to synthesize cost-effective countermeasures based on given resource constraints and security requirements in order to resist stealthy attacks. The proposed approach is illustrated on standard IEEE test cases.",1530-0889;15300889,Electronic:978-1-4799-2233-8; POD:978-1-4799-2773-9,10.1109/DSN.2014.29,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6903576,False Data Injection Attack;Formal Method;Power Grid;State Estimation,Equations;Mathematical model;Power measurement;Security;State estimation;Topology;Transmission line measurements,energy management systems;least squares approximations;power grids;power system state estimation;security of data;topology,IEEE test cases;WLS estimator;countermeasure synthesis;data detection algorithms;power grid;power system state estimation;security threat analytics;stealthy cyber attacks;topology errors;topology mappings;weighted least square estimator,,0,,23,,no,23-26 June 2014,,IEEE,IEEE Conference Publications
Security with Privacy -- Opportunities and Challenges: Panel Position Paper,E. Bertino,"CS Dept., Purdue Univ., West Lafayette, IN, USA",2014 IEEE 38th Annual Computer Software and Applications Conference,20140922,2014,,,436,437,This paper summarizes opportunities and challenges concerning how we can achieve security while still ensuring privacy. It identifies research directions and includes a number of questions that have been debated by the panel.,,Electronic:978-1-4799-3575-8; POD:978-1-4799-3576-5,10.1109/COMPSAC.2014.98,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6899246,cyber security;information privacy;privacy-preserving data analytics,Authentication;Biometrics (access control);Cryptography;Data privacy;Privacy,data privacy;security of data,data privacy;data security,,1,,10,,no,21-25 July 2014,,IEEE,IEEE Conference Publications
Semiotics in visualisation,K. Liu,"Informatics Research Centre (IRC), Henley Business School, University of Reading, Reading, RG6 6UD, U.K.",2014 9th International Conference on Evaluation of Novel Approaches to Software Engineering (ENASE),20150402,2014,,,1,3,"Digital visualisation is a way of representing data and information with the aid of digital means. It ranges from a simple form such as a graph or chart to a complex form like animated visualisations that allows user to interact with the underlying data through direct manipulation (Chen et al., 2008). The notion of digital visualisation engages human interpretation on information in order to gain insights in a particular context (Robert, 2007, Ware, 2012, Czernicki, 2010). Hence, it is a complex process involving multiple disciplines, including the socio-technical element. The social element relates to human perception in interpreting information. The technical element on the other hand, refers to the technology used to enable visualisation, for example the SAS suite (SAS, 2014) that offers visual analytics to support interactive dashboard and reporting.",,Electronic:978-989-758-030-7; POD:978-1-4799-7919-6,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7077099,,,,,,0,,12,,no,28-30 April 2014,,IEEE,IEEE Conference Publications
SEPP: Semantics-Based Management of Fast Data Streams,D. Riemer; L. Stojanovic; N. Stojanovic,"FZI Res. Center for Inf. Technol., Karlsruhe, Germany",2014 IEEE 7th International Conference on Service-Oriented Computing and Applications,20141206,2014,,,113,118,"In the era of big data processing there is an emerging need for methodologies supporting the management of data-intensive application scenarios. Complex Event Processing is an integral part of many fast data application as an underlying technology for event correlation and pattern detection. Increased volume of event streams as well as the demand for more complex real-time analytics require for execution of processing pipelines among heterogeneous event processing engines. In this paper, we propose a semantic model for the management of fast data streams using the concept of Semantic Event Processing Pipelines (SEPP). We provide methodology, architecture and language for semantic discovery and binding of real-time processing services from arbitrary stream processing engines. Our approach aims to improve reusability of real-time processing services by providing high-level interfaces to stream processing implementations. By these means this work paves the way for an easier development and management of real-time big data applications.",2163-2871;21632871,Electronic:978-1-4799-6833-6; POD:978-1-4799-6834-3,10.1109/SOCA.2014.52,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6978598,Complex Event Processing;Event Pattern Management;Semantic Web,Grounding;Ontologies;Pipelines;Program processors;Real-time systems;Semantics;Sensors,Big Data;data analysis;pipeline processing;software reusability,SEPP;arbitrary stream processing engines;big data processing;complex event processing;complex real-time analytics;data application;data-intensive application scenario management;event correlation;event streams;fast data streams;high-level interfaces;pattern detection;real-time big data applications;real-time processing services;semantic discovery architecture;semantic discovery language;semantic event processing pipeline;semantic-based management,,0,,14,,no,17-19 Nov. 2014,,IEEE,IEEE Conference Publications
SiLK: A Tool Suite for Unsampled Network Flow Analysis at Scale,M. Thomas; L. Metcalf; J. Spring; P. Krystosek; K. Prevost,"Software Eng. Inst., Carnegie Mellon Univ., Pittsburgh, PA, USA",2014 IEEE International Congress on Big Data,20140925,2014,,,184,191,"A large organization can generate over ten billion network flow records per day, a high-velocity data source. Finding useful, security-related anomalies in this volume of data is challenging. Most large network flow tools sample the data to make the problem manageable, but sampling unacceptably reduces the fidelity of analytic conclusions. In this paper we discuss SiLK, a tool suite created to analyze this high-volume data source without sampling. SiLK implementation and architectural design are optimized to manage this Big Data problem. SiLK provides not just network flow capture and analysis, but also includes tools to analyze large sets and dictionaries that frequently relate to network flow data, incorporating higher-variety data sources. These tools integrate disparate data sources with SiLK analysis.",2379-7703;23797703,Electronic:978-1-4799-5057-7; POD:978-1-4799-5058-4,10.1109/BigData.Congress.2014.34,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6906777,Network Flow;Network Security;Network traffic analysis;Open-source tools;Security,IP networks;Indexes;Open source software;Ports (Computers);Protocols;Routing;Security,Big Data;Internet;dictionaries,Big Data problem;SiLK;System for Internet-Level Knowledge;architectural design;dictionaries;high-velocity data source;high-volume data source;security-related anomalies;tool suite;unsampled network flow analysis,,2,,21,,no,June 27 2014-July 2 2014,,IEEE,IEEE Conference Publications
Simulation based analytics for efficient planning and management in multimodal freight transportation industry,P. Dube; J. P. M. GonÌ_alves; S. Mahatma; F. Barahona; M. Naphade; M. Bedeman,"IBM T. J. Watson Research Center, 1101 Kitchawan Road, Yorktown Heights, NY 10598, USA",Proceedings of the Winter Simulation Conference 2014,20150126,2014,,,1943,1954,"The multimodal freight transportation planning is a complex problem with several factors affecting decisions, including network coverage, carriers and their schedules, existing contractual agreements with carriers and clients, carrier capacity constraints, and market conditions. Day-to-day operations like booking and bidding are mostly done manually and there is a lack of decision support tools to aid the operators. These operations are governed by a complex set of business rules involving service agreements with the clients, contractual agreements with the carriers and forwarder's own business objectives. The multimodal freight transportation industry lacks a comprehensive solution for end-to-end route optimization and planning. We developed analytics for trade lane managers to identify and exploit opportunities to improve procurement, carrier selection, capacity planning, and business rules management. Our simulation based analytics tool is useful for managing business rules and for doing what-if analysis which can lead to better resource planning, cost management, and rate negotiations.",0891-7736;08917736,Electronic:978-1-4799-7486-3; POD:978-1-4799-7487-0,10.1109/WSC.2014.7020041,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7020041,,Business;Engines;Legged locomotion;Oceans;Planning;Ports (Computers);Transportation,capacity planning (manufacturing);digital simulation;freight handling;procurement;software tools,business rule management;capacity planning;carrier selection;cost management;end-to-end route optimization;end-to-end route planning;multimodal freight transportation industry;multimodal freight transportation planning;procurement;rate negotiations;resource planning;simulation based analytics tool;trade lane managers;what-if analysis,,0,,9,,no,7-10 Dec. 2014,,IEEE,IEEE Conference Publications
SoDA: Dynamic visual analytics of big social data,S. Hassan; J. SÌ_nger; G. Pernul,"Dept. of Inf. Syst., Univ. of Regensburg, Regensburg, Germany",2014 International Conference on Big Data and Smart Computing (BIGCOMP),20140217,2014,,,183,188,"In this work we apply dynamic visual analytics on big social data by the example of microblogs from Twitter. Thereby, we address current challenges like real-time analytics as well as analyses of unstructured data. To this end, we propose SoDA - a concept enabling the integrated analysis of the dimensions: message, location and time. Furthermore, we introduce a novel design for tag cloud visualizations, the weighted tag network, offering enhanced semantic insights. All concepts are fully implemented and evaluated by a comprehensive software prototype in different application scenarios.",2375-933X;2375933X,Electronic:978-1-4799-3919-0; POD:978-1-4799-3920-6,10.1109/BIGCOMP.2014.6741433,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6741433,,Data handling;Data visualization;Information management;Real-time systems;Semantics;Twitter;Visual analytics,Big Data;data analysis;data visualisation;social networking (online),SoDA;Twitter;big social data;dynamic visual analytics;location dimension;message dimension;microblogs;real-time analytics;tag cloud visualizations;time dimension;unstructured data analysis;weighted tag network,,1,,26,,no,15-17 Jan. 2014,,IEEE,IEEE Conference Publications
SoftLearn: A Process Mining Platform for the Discovery of Learning Paths,B. V. Barreiros; M. Lama; M. Mucientes; J. C. Vidal,"Center for Res. in Inf. Technol. (CiTIUS), Univ. of Santiago de Compostela, Santiago de Compostela, Spain",2014 IEEE 14th International Conference on Advanced Learning Technologies,20140922,2014,,,373,375,"One of the most challenging issues in learning analytics is the development of techniques and tools that facilitate the evaluation of the learning activities carried out by learners. In this paper, we faced this issue through a process mining-based platform, called Soft Learn, that is able to discover complete, precise and simple learning paths from event logs. This platform has a graphical interface that allows teachers to better understand the real learning paths undertaken by learners.",2161-3761;21613761,Electronic:978-1-4799-4038-7; POD:978-1-4799-4037-0,10.1109/ICALT.2014.111,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6901485,Learning Analytics;Learning Path Discovery;Process Mining,Context;Data mining;Data visualization;Genetic algorithms;Registers;Software architecture;Visualization,computer aided instruction;data mining;graphical user interfaces,SoftLearn;event logs;graphical interface;learning analytics;learning path discovery;process mining platform,,4,,9,,no,7-10 July 2014,,IEEE,IEEE Conference Publications
Software defined environments: An introduction,C. S. Li; B. L. Brech; S. Crowder; D. M. Dias; H. Franke; M. Hogstrom; D. Lindquist; G. Pacifici; S. Pappe; B. Rajaraman; J. Rao; R. P. Ratnaparkhi; R. A. Smith; M. D. Williams,,IBM Journal of Research and Development,20140415,2014,58,3-Feb,1:01,1:11,"During the past few years, enterprises have been increasingly aggressive in moving mission-critical and performance-sensitive applications to the cloud, while at the same time many new mobile, social, and analytics applications are directly developed and operated on cloud computing platforms. These two movements are encouraging the shift of the value proposition of cloud computing from cost reduction to simultaneous agility and optimization. These requirements (agility and optimization) are driving the recent disruptive trend of software defined computing, for which the entire computing infrastructure‰ÛÓcompute, storage and network‰ÛÓis becoming software defined and dynamically programmable. The key elements within software defined environments include capability-based resource abstraction, goal-based and policy-based workload definition, and outcome-based continuous mapping of the workload to the available resources. Furthermore, software defined environments provide the tooling and capabilities to compose workloads from existing components that are then continuously and autonomously mapped onto the underlying programmable infrastructure. These elements enable software defined environments to achieve agility, efficiency, and continuous outcome-optimized provisioning and management, plus continuous assurance for resiliency and security. This paper provides an overview and introduction to the key elements and challenges of software defined environments.",0018-8646;00188646,,10.1147/JRD.2014.2298134,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6798712,,Cloud computing;Computer architecture;Computer security;Costs;Instruction sets;Market research;Mission critical systems;Mobile communication;Optimization;Performance evaluation;Servers;Software defined networks;Unified modeling language,,,,10,,,,no,March-May 2014,,IBM,IBM Journals & Magazines
Star Ratings versus Sentiment Analysis -- A Comparison of Explicit and Implicit Measures of Opinions,P. Lak; O. Turetken,,2014 47th Hawaii International Conference on System Sciences,20140310,2014,,,796,805,"A typical trade-off in decision making is between the cost of acquiring information and the decline in decision quality caused by insufficient information. Consumers regularly face this trade-off in purchase decisions. Online product/service reviews serve as sources of product/service related information. Meanwhile, modern technology has led to an abundance of such content, which makes it prohibitively costly (if possible at all) to exhaust all available information. Consumers need to decide what subset of available information to use. Star ratings are excellent cues for this decision as they provide a quick indication of the tone of a review. However there are cases where such ratings are not available or detailed enough. Sentiment analysis -text analytic techniques that automatically detect the polarity of text- can help in these situations with more refined analysis. In this study, we compare sentiment analysis results with star ratings in three different domains to explore the promise of this technique.",1530-1605;15301605,Electronic:978-1-4799-2504-9; POD:978-1-4799-2505-6,10.1109/HICSS.2014.106,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6758702,comparative analysis;opinion mining;sentiment analysis;star ratings,Accuracy;Blogs;Correlation;Medical services;Software;Tagging;Uncertainty,Internet;consumer behaviour;decision making;text analysis,decision making;decision quality;explicit opinion measures;implicit opinion measures;online product-service reviews;product-service related information;purchase decisions;sentiment analysis;star ratings;text analytic techniques,,3,,26,,no,6-9 Jan. 2014,,IEEE,IEEE Conference Publications
Statistical learning and multiple linear regression model for network selection using MIH,A. Rahil; N. Mbarek; O. Togni; M. Atieh; A. Fouladkar,"LE2I Laboratory - UMR CNRS 6306, University of Burgundy Dijon, France",The Third International Conference on e-Technologies and Networks for Development (ICeND2014),20141218,2014,,,189,194,"A key requirement to provide seamless mobility and guaranteeing Quality of Service in heterogeneous environment is to select the best destination network during handover. In this paper, we propose a new schema for network selection based on Multiple Linear Regression Model (MLRM). A thorough investigation, on a huge live data collected from GPRS/UMTS networks led to identify the Key Performance Indicators (KPIs) that play the most important role in the handover process. These KPIs are: Received Signal Code Power (RSCP), received energy per chip (Ec/No)and Available Bandwidth (ABW) of the destination network. To extract a handover model from collected data, we study the correlation among values of identified KPIs parameters, before, during and after handover, thanks to a statistical learning approach, using the predictive analytics software SPSS. For model assessment, Pearson Correlation Coefficient and determination coefficient R-squared (R<sup>2</sup>) are used. Media Independent Handover (MIH) IEEE 802.21 standard is used in this work to retrieve the lower layer information of available networks and announce the handover needs (handover initiation). The proposed model will help to select the most appropriate network between many existing ones in the vicinity of the mobile node.",,CD-ROM:978-1-4799-3165-1; Electronic:978-1-4799-3166-8; POD:978-1-4799-3167-5,10.1109/ICeND.2014.6991378,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6991378,IEEE 802.21;multiple linear regression;seamless handover;statistical learning,Handover;Linear regression;Mobile communication;Mobile computing;Protocols;Quality of service,3G mobile communication;cellular radio;learning (artificial intelligence);mobility management (mobile radio);multimedia communication;packet radio networks;quality of service;regression analysis;telecommunication computing,ABW;Ec/No;GPRS-UMTS networks;KPIs;MIH;MLRM;Pearson correlation coefficient;RSCP;SPSS predictive analytics software;available bandwidth;data collection;destination network;determination coefficient R-squared;handover process;heterogeneous environment;key performance indicators;media independent handover IEEE 802.21 standard;mobile node;multiple linear regression model;network selection;quality of service;received energy per chip;received signal code power;statistical learning approach,,0,,38,,no,April 29 2014-May 1 2014,,IEEE,IEEE Conference Publications
"Stream computing for large-scale, multi-channel cyber threat analytics",D. L. Schales; M. Christodorescu; X. Hu; J. Jang; J. R. Rao; R. Sailer; M. P. Stoecklin; W. Venema; T. Wang,IBM Research,Proceedings of the 2014 IEEE 15th International Conference on Information Reuse and Integration (IEEE IRI 2014),20150302,2014,,,8,15,"The cyber threat landscape, controlled by organized crime and nation states, is evolving rapidly towards evasive, multi-channel attacks, as impressively shown by malicious operations such as GhostNet, Aurora, Stuxnet, Night Dragon, or APT1. As threats blend across diverse data channels, their detection requires scalable distributed monitoring and cross-correlation with a substantial amount of contextual information. With threats evolving more rapidly, the classical defense life cycle of post-mortem detection, analysis, and signature creation becomes less effective. In this paper, we present a highly-scalable, dynamic cybersecurity analytics platform extensible at runtime. It is specifically designed and implemented to deliver generic capabilities as a basis for future cybersecurity analytics that effectively detect threats across multiple data channels while recording relevant context information, and that support automated learning and mining for new and evolving malware behaviors. Our implementation is based on stream computing middleware that has proven high scalability, and that enables cross-correlation and analysis of millions of events per second with millisecond latency. We report the lessons we have learned from applying stream computing to monitoring malicious activity across multiple data channels (e.g., DNS, NetFlow, ARP, DHCP, HTTP) in a production network of about fifteen thousand nodes.",,Electronic:978-1-4799-5880-1; POD:978-1-4799-5881-8; USB:978-1-4799-5879-5,10.1109/IRI.2014.7051865,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7051865,,Analytical models;Computational modeling;Computer architecture;Computer security;IP networks;Monitoring;Real-time systems,data mining;invasive software;learning (artificial intelligence),automated learning;context information recording;cyber threat landscape;data mining;highly-scalable dynamic cybersecurity analytics;large-scale multichannel cyber threat analytics;malicious activity monitoring;multichannel attacks;multiple data channels;nation states;organized crime;stream computing middleware,,1,,35,,no,13-15 Aug. 2014,,IEEE,IEEE Conference Publications
Stress assessment in power systems and its visualization using synchrophasor based metrics,A. Pal; I. Singh; B. Bhargava,"Bradley Department of Electrical & Computer Engineering, Virginia Tech, Blacksburg, Virginia-24061, U.S.A.",2014 North American Power Symposium (NAPS),20141124,2014,,,1,6,"This paper proposes two metrics for assessing static and dynamic stresses present in a large interconnected power grid. The base loading of the system constitutes the static stress. It refers to the normal/pre-contingency state of the system. The dynamic stress refers to the event/contingency that the system is subjected to and is primarily caused by loss of transmission system or drop in generation. The angle difference between buses located across the network, and the voltage sensitivity of buses lying in the middle are two synchrophasor-based metrics that are found to accurately reflect the system's static loading and its ability to withstand the dynamic stress. The simulations performed using the full WECC system show that by monitoring these metrics in real-time, the ability of the system to withstand a variety of contingencies can be predicted with great accuracy. These metrics can be monitored through analytic and visualization platforms such as RTDMS<sup>å¨1</sup>, which is a synchrophasor based software application. The methodology to be followed for integrating with such a platform is also provided. The analysis shows that the proposed metrics can be very effective in aiding system operators for real-time static and dynamic stress monitoring.",,Electronic:978-1-4799-5904-4; POD:978-1-4799-5905-1,10.1109/NAPS.2014.6965460,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6965460,Angle difference;Phasor measurement unit (PMU);Stress assessment;Voltage sensitivity;Wide area measurement system (WAMS),Loading;Monitoring;Power system dynamics;Real-time systems;Sensitivity;Stress,phasor measurement;power grids;power system interconnection;power transmission planning,RTDMS;WECC system;large interconnected power grid;power system stress assessment;real-time dynamic stress monitoring;real-time static stress monitoring;static loading;synchrophasor based metrics;synchrophasor based software application;transmission system loss;visualization platforms;voltage sensitivity;wide area measurement system,,2,,10,,no,7-9 Sept. 2014,,IEEE,IEEE Conference Publications
Supporting Decision-Making for Biometric System Deployment through Visual Analysis,C. Turkay; S. Mason; I. Gashi; B. Cukic,"Dept. of Comput. Sci., City Univ. London, London, UK",2014 IEEE International Symposium on Software Reliability Engineering Workshops,20141215,2014,,,347,352,"Deployment of biometric systems in the specific environment is not straightforward. Based on pre-deployment performance test results, a decision maker needs to consider the selection of sensors and matching algorithms in terms of the cost, expected false-match and false-non-match failure rates and the underlying quality factors. Which depend on operational scenarios, personnel training, demographics, etc. In this paper, we investigate information aggregation through visualization of fingerprint authentication experiments obtained from a large scale data collection with 494 participants. The data was collected using four biometric image capture devices. Each fingerprint image was analysed with two image quality algorithms, and the matching scores were generated using three different matchers. Additionally we collected and analyzed the impact of demographic characteristics, such as gender, age, ethnicity, height and weight, on system performance.",,Electronic:978-1-4799-7377-4; POD:978-1-4799-7378-1,10.1109/ISSREW.2014.78,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6983865,Biometrics;Decision support;Security data analysis;Visual analytics,Algorithm design and analysis;Authentication;Biometrics (access control);Data visualization;Performance evaluation;Probes;Visualization,data analysis;data visualisation;decision making;fingerprint identification;image matching,algorithm matching;biometric image capture devices;biometric system deployment;decision maker;decision making;demographic characteristics;demographics;ethnicity;false nonmatch failure rates;fingerprint authentication visualization;fingerprint image;image quality algorithm;information aggregation;large scale data collection;matching scores;personnel training;quality factors;visual analysis,,0,,17,,no,3-6 Nov. 2014,,IEEE,IEEE Conference Publications
Supporting Regression Test Scoping with Visual Analytics,E. EngstrÌ_m; M. MantylÌ_; P. Runeson; M. Borg,"Dept. of Comput. Sci., Lund Univ., Lund, Sweden","2014 IEEE Seventh International Conference on Software Testing, Verification and Validation",20140602,2014,,,283,292,"Background: Test managers have to repeatedly select test cases for test activities during evolution of large software systems. Researchers have widely studied automated test scoping, but have not fully investigated decision support with human interaction. We previously proposed the introduction of visual analytics for this purpose. Aim: In this empirical study we investigate how to design such decision support. Method: We explored the use of visual analytics using heat maps of historical test data for test scoping support by letting test managers evaluate prototype visualizations in three focus groups with in total nine industrial test experts. Results: All test managers in the study found the visual analytics useful for supporting test planning. However, our results show that different tasks and contexts require different types of visualizations. Conclusion: Important properties for test planning support are: ability to overview testing from different perspectives, ability to filter and zoom to compare subsets of the testing with respect to various attributes and the ability to manipulate the subset under analysis by selecting and deselecting test cases. Our results may be used to support the introduction of visual test analytics in practice.",2159-4848;21594848,Electronic:978-1-4799-2255-0; POD:978-1-4799-1665-8,10.1109/ICST.2014.41,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6823890,Decision support;Regression test;Visual analytics,Context;Data visualization;Organizations;Prototypes;Software;Testing;Visualization,data visualisation;program testing;software management,decision support;empirical analysis;heat maps;historical test data;prototype visualization evaluation;regression test scoping;software systems;test activities;test case deselection;test case selection;test planning;visual test analytics,,0,,28,,no,March 31 2014-April 4 2014,,IEEE,IEEE Conference Publications
System requirements prioritization based on AHP,F. Fellir; K. Nafil; R. Touahni,"Universit&#x00E9; Ibn Tofail - Facult&#x00E9; des Sciences, Laboratoire LASTID K&#x00E9;nitra, Maroc",2014 Third IEEE International Colloquium in Information Science and Technology (CIST),20150122,2014,,,163,167,"When analyzing software requirements specification, the question of determining which tasks need to be implemented first arises often. In fact, in any software project, it's important to begin by identifying and defining priority tasks. This process is called software requirements prioritization. However in this process, the effect of NFRs on FRs to be prioritized has always been neglected, and all the requirements are prioritized independently. In this paper, we propose a new technique for software requirements prioritization taking account of the relationship of dependency between FR and NFR.",2327-185X;2327185X,Electronic:978-1-4799-5979-2; POD:978-1-4799-5980-8,10.1109/CIST.2014.7016612,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7016612,FR (functional requirement);NFR (non-functional requirement);requirements prioritization,Decision support systems;Software,analytic hierarchy process;formal specification,AHP;nonfunctional requirement;software project;software requirements specification;system requirements prioritization,,0,,13,,no,20-22 Oct. 2014,,IEEE,IEEE Conference Publications
System simulation as decision data in heathcare it,C. S. Brust; R. Clark,"Mayo Clinic, 200 1st St SW, Rochester, MN 55905, USA",Proceedings of the Winter Simulation Conference 2014,20150126,2014,,,1317,1328,"Information Technology in healthcare is an ever-growing enterprise, with medical providers becoming more and more reliant on data to make care decisions. With the increased reliance on these applications for care, questions arise around the availability and manageability of those systems. This paper examines a model which has been developed for the selection of computing infrastructure architectures in healthcare organizations. This model utilizes the Analytics Hierarchy Process (AHP) to weigh the various criteria that come into play for decisions of this nature. Further, to vet the recommendations of the AHP model, and to lend quantitative data to the decision making process, simulations of the various architectural options were built for various application scenarios. The results of these simulations thus serve as additional validation of the model's efficacy. This paper focuses on the use of discrete event simulation using ExtendSim<sup>å¨</sup> to assist in the architectural selection process for computing architectures.",0891-7736;08917736,Electronic:978-1-4799-7486-3; POD:978-1-4799-7487-0,10.1109/WSC.2014.7019987,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7019987,,Bioinformatics;Cloud computing;Computational modeling;Computer architecture;Data models;Medical services;Servers,analytic hierarchy process;discrete event simulation;health care;medical information systems;software architecture,AHP model;ExtendSim;analytic hierarchy process;application scenarios;computing infrastructure architecture selection;decision data;decision making process;discrete event simulation;health care IT;healthcare organizations;information technology;medical providers;model efficacy;quantitative data;system simulation,,0,,20,,no,7-10 Dec. 2014,,IEEE,IEEE Conference Publications
Tablet-based interaction panels for immersive environments,D. M. Krum; T. Phan; L. C. Dukes; P. Wang; M. Bolas,"Inst. for Creative Technol., USC, Los Angeles, CA, USA",2014 IEEE Virtual Reality (VR),20140424,2014,,,91,92,"With the current widespread interest in head mounted displays, we perceived a need for devices that support expressive and adaptive interaction in a low-cost, eyes-free manner. Leveraging rapid prototyping techniques for fabrication, we have designed and manufactured a variety of panels that can be overlaid on multi-touch tablets and smartphones. The panels are coupled with an app running on the multi-touch device that exchanges commands and state information over a wireless network with the virtual reality application. Sculpted features of the panels provide tactile disambiguation of control widgets and an onscreen heads-up display provides interaction state information. A variety of interaction mappings can be provided through software to support several classes of interaction techniques in virtual environments. We foresee additional uses for applications where eyes-free use and adaptable interaction interfaces can be beneficial.",1087-8270;10878270,Electronic:978-1-4799-2871-2; POD:978-1-4799-2872-9; USB:978-1-4799-4279-4,10.1109/VR.2014.6802066,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6802066,"H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems ‰ÛÓ Artificial, augmented and virtual realities;H.5.2 [Information Interfaces and Presentation]: User Interfaces ‰ÛÓ Input devices and strategies",Electronic mail;Smart phones;Software;Tactile sensors;Three-dimensional displays;Virtual environments,helmet mounted displays;interactive devices;smart phones;virtual reality,adaptive interaction;head mounted displays;immersive environments;multitouch tablets;rapid prototyping techniques;smartphones;tablet-based interaction panels;virtual reality;wireless network,,0,,9,,no,March 29 2014-April 2 2014,,IEEE,IEEE Conference Publications
"TDA2X, a SoC optimized for advanced driver assistance systems",J. Sankaran; N. Zoran,"Texas Instrum. Inc., Dallas, TX, USA","2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",20140714,2014,,,2204,2208,"TDA2X is an optimized scalable system on chip (SoC) solution from Texas Instruments that spans various application areas of ADAS such as front-camera, surround-view and the emerging area of sensor fusion. It accomplishes this through a focused set of heterogeneous processors, brought together in a scalable architecture with a rich set of integrated peripherals, providing an optimal mix of performance in a low power footprint for Advanced Driver Assistance Systems (ADAS) vision analytics. Computer vision algorithms across the various ADAS application systems have a rich variation and diversity in processing requirements along with the need to run them concurrently within challenging thermal budgets. A heterogeneous architecture with various programmable elements allows system developers to map various portions of the algorithms to the architectures that are best suited for the underlying task allowing maximizing system performance and reducing development time and effort in developing these complex systems. Scalability of the architecture by varying the number of cores and clock speeds of these heterogeneous architectures, allows for scalability in performance and power across low, mid and high end products with one software investment. A critical focus on functional safety across the cores and various memories is particularly essential given the mission critical nature of ADAS applications.",1520-6149;15206149,Electronic:978-1-4799-2893-4; POD:978-1-4799-2894-1; USB:978-1-4799-2892-7,10.1109/ICASSP.2014.6853990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6853990,ADAS SoC;Front Camera;Sensor Fusion and Programmable Vision Accelerators;Surround View,Acceleration;Computer architecture;Digital signal processing;Engines;Program processors;System-on-chip;Vectors,clocks;computer architecture;computer vision;driver information systems;system-on-chip,ADAS application systems;ADAS vision analytics;SoC;TDA2X;Texas Instruments;advanced driver assistance systems;clock speeds;complex systems development;computer vision algorithms;functional safety;heterogeneous architecture;heterogeneous processors;memories;optimized scalable system on chip;programmable elements;scalable architecture;software investment;system performance,,2,,3,,no,4-9 May 2014,,IEEE,IEEE Conference Publications
Testability of object-oriented systems: An AHP-based approach for prioritization of metrics,P. Khanna,"Siemens Technology and Services, Private Limited, CT DC AA E P-IE BDG3, IFFCO Tower, Plot No. 3, Sector 29, Gurgaon 122001, India",2014 International Conference on Contemporary Computing and Informatics (IC3I),20150126,2014,,,273,281,This paper investigates testability from the perspective of metrics used in an object-oriented system. The idea is to give an overview of object oriented design metrics with the prioritization of same keeping testability as the overall goal. We have used Analytic Hierarchy Process (AHP) method to attain which metric is mostly used and is best for testability.,,Electronic:978-1-4799-6629-5; POD:978-1-4799-6630-1; USB:978-1-4799-6628-8,10.1109/IC3I.2014.7019595,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7019595,AHP;CK metrics suite;MOOD Metrics Model;testability,Analytic hierarchy process;Complexity theory;Couplings;Encapsulation;Measurement;Software;Testing,analytic hierarchy process;object-oriented programming;program testing;software metrics,AHP-based approach;analytic hierarchy process;metrics prioritization;object-oriented systems;testability,,0,,17,,no,27-29 Nov. 2014,,IEEE,IEEE Conference Publications
The Berkeley Data Analytics Stack (BDAS),Jayati,"Impetus Infotech Pvt. Ltd., Indore, India","2014 Conference on IT in Business, Industry and Government (CSIBIG)",20150312,2014,,,1,1,"Summary form only given. The session on ‰ÛÏThe Berkeley Data Analytics Stack‰Ûù shall elucidate its current components which include Spark, Shark and Mesos with emphasis on Spark and it's real-time extension called Spark-Streaming which adds stream processing capabilities to Spark. One-liners describing each of these technologies are as follows: 1) BDAS is an open source, next-generation data analytics stack under development at the UC Berkeley AMPLab. 2) Spark, a high-speed cluster computing system compatible with Hadoop that can outperform it by up to 100x thanks to its ability to perform computations in memory. 3) Shark, a port of Apache Hive onto Spark that is compatible with existing Hive warehouses and queries. Shark can answer HiveQL queries up to 100x faster than Hive without modification to the data and queries, and is also open source as part of BDAS. 4) Mesos is a cluster manager that provides efficient resource isolation and sharing across distributed applications or frameworks. It can run Hadoop, MPI, Hypertable, Spark, and other applications on a dynamically shared pool of nodes. 5) Apart, from an elaborate explanation of various facets of Spark, the session would also aim to walk through machine learning algorithm benchmarking and examples that would substantiate the concepts covered.",,CD-ROM:978-1-4799-3062-3; Electronic:978-1-4799-3064-7; POD:978-1-4799-3065-4,10.1109/CSIBIG.2014.7056925,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7056925,,Abstracts;Sparks,application program interfaces;data analysis;data warehouses;learning (artificial intelligence);message passing;pattern clustering;public domain software;query processing,Apache Hive;BDAS;Berkeley data analytics stack;Hadoop;Hive queries;Hive warehouses;HiveQL queries;Hypertable;MPI;Mesos cluster manager;Shark;Spark-streaming;high-speed cluster computing system;machine learning algorithm benchmarking;open source next-generation data analytics stack,,0,,,,no,8-9 March 2014,,IEEE,IEEE Conference Publications
The CACTOS Vision of Context-Aware Cloud Topology Optimization and Simulation,P. O. Ì_stberg; H. Groenda; S. Wesner; J. Byrne; D. S. Nikolopoulos; C. Sheridan; J. Krzywda; A. Ali-Eldin; J. Tordsson; E. Elmroth; C. Stier; K. Krogmann; J. Domaschka; C. B. Hauser; P. J. Byrne; S. Svorobej; B. Mccollum; Z. Papazachos; D. Whigham; S. RÌ_th; D. Paurevic,"Dept. of Comput. Sci., Umea Univ., Umea, Sweden",2014 IEEE 6th International Conference on Cloud Computing Technology and Science,20150212,2014,,,26,31,"Recent advances in hardware development coupled with the rapid adoption and broad applicability of cloud computing have introduced widespread heterogeneity in data centers, significantly complicating the management of cloud applications and data center resources. This paper presents the CACTOS approach to cloud infrastructure automation and optimization, which addresses heterogeneity through a combination of in-depth analysis of application behavior with insights from commercial cloud providers. The aim of the approach is threefold: to model applications and data center resources, to simulate applications and resources for planning and operation, and to optimize application deployment and resource use in an autonomic manner. The approach is based on case studies from the areas of business analytics, enterprise applications, and scientific computing.",,Electronic:978-1-4799-4093-6; POD:978-1-4799-4092-9; USB:978-1-4799-4094-3,10.1109/CloudCom.2014.62,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7037644,Cloud infrastructure optimization;autonomic computing;cloud modelling;cloud monitoring;cloud simulation,Computational modeling;Data models;Hardware;Monitoring;Optimization;Topology;Virtual machining,cloud computing;computer centres;optimisation;resource allocation;ubiquitous computing,CACTOS vision;cloud computing;context-aware cloud topology optimization;data center resource,,1,,26,,no,15-18 Dec. 2014,,IEEE,IEEE Conference Publications
The Complexity of University Curricula According to Course Cruciality,A. Slim; J. Kozlick; G. L. Heileman; C. T. Abdallah,"Dept. of Electr. & Comput. Eng., Univ. of New Mexico, Albuquerque, NM, USA","2014 Eighth International Conference on Complex, Intelligent and Software Intensive Systems",20141002,2014,,,242,248,"Many universities have recently focused significant efforts on enhancing their graduation rates. Numerous factors may impact a student's ability to succeed and ultimately graduate, including pre-university preparation, as well as the student support services provided by a university. However, even the best efforts to improve in these areas may fail if other institutional factors overwhelm their ability to facilitate student progress. Specifically, in this paper we consider degree to which the underlying curriculum that a student must traverse in order to earn a degree impacts progress. Using complex network analysis and graph theory, this paper proposes a framework for analyzing university course networks at the university, college and departmental levels. The analyses we provide are based on quantifying the importance of a course based on its delay and blocking factors, as well as the number of curricula that incorporate the course, leading to a metric we refer to as the course cruciality. Experimental results, using data from the University of New Mexico, show that the distribution of course cruciality follows a power law distribution. Applications of the proposed framework are extended to study the complexity of curricula within colleges as well as the tendency of a university's disciplines to associate with others that are unlike them. This work may be useful to both students and decision makers at universities as it presents a robust framework for analyzing the ease of flow of students through curricula, which may lead to improvements that facilitate improved student success.",,CD-ROM:978-1-4799-4326-5; Electronic:978-1-4799-4325-8; POD:978-1-4799-1677-1,10.1109/CISIS.2014.34,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6915523,complex networks;institutional analytics;university curricula,Algebra;Complexity theory;Computers;Delays;Educational institutions,educational courses;educational institutions;further education;graph theory;statistical distributions,University of New Mexico;blocking factors;college level;complex network analysis;course cruciality distribution;delay factors;departmental level;graduation rates;graph theory;institutional factors;power law distribution;preuniversity preparation;student support services;university course network analysis;university curricula complexity;university level,,4,,12,,no,2-4 July 2014,,IEEE,IEEE Conference Publications
The Digital Transformation: Staying Competitive,S. Earley,,IT Professional,20140415,2014,16,2,58,60,"Today's information technologies accelerate the speed at which enterprises make decisions, process information, and collaborate to solve problems, but do they provide a competitive advantage? Only if the organization transforms how it does business--using the same old approaches with new software aren't sufficient. The pace of collaboration, problem solving, innovation, and value creation has been increasing as new enabling tools emerge, resulting in a cycle in which innovation leads to new inventions. For example, the development of knowledge bases presented an innovative way to collaborate, which in turn lead to inventions in many fields. Some might assume that this process creates an advantage, but in reality, almost everyone else is developing these tools at the same time. New capabilities are evolving in the context of an ecosystem of competitors, all of whom are trying to do the same thing. The key is to leverage new tools and approaches faster than others in your industry, creating differentiated value for the customer.",1520-9202;15209202,,10.1109/MITP.2014.24,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6798637,big data;data analytics;information technology;productivity paradox,Context awareness;Decision making;Enterprise resource planning;Information technology;Standards;Technological innovation,Big Data;commerce;innovation management;organisational aspects,business;competitive advantage;digital transformation;enterprise decisions;information processing;information technologies;innovation;inventions;knowledge bases development;organization;problem solving;tools development;value creation,,0,,4,,no,Mar.-Apr. 2014,,IEEE,IEEE Journals & Magazines
The EMBERS architecture for streaming predictive analytics,A. Doyle; G. Katz; K. Summers; C. Ackermann; I. Zavorin; Z. Lim; S. Muthiah; L. Zhao; C. T. Lu; P. Butler; R. P. Khandpur; Y. Fayed; N. Ramakrishnan,"CACI Inc., Lanham, MD 20706",2014 IEEE International Conference on Big Data (Big Data),20150108,2014,,,11,13,"Developed under the IARPA Open Source Initiative program, EMBERS (Early Model Based Event Recognition using Surrogates) is a large-scale Big-Data analytics system for forecasting significant societal events, such as civil unrest incidents and disease outbreaks on the basis of continuous, automated analysis of large volumes of publicly available data. It has been operational since November of 2012, delivering approximately 50 predictions each day. EMBERS is built on a streaming, scalable, share-nothing architecture and is deployed on Amazon Web Services (AWS).",,Electronic:978-1-4799-5666-1; POD:978-1-4799-5667-8,10.1109/BigData.2014.7004477,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7004477,,Big data;Computer architecture;Data models;Data visualization;Diseases;Feeds;Predictive models,Big Data;Web services;cloud computing;public domain software;software architecture,AWS;Amazon Web Services;EMBERS architecture;IARPA open source initiative program;civil unrest incidents;continuous automated analysis;disease outbreaks;early model-based event recognition-using-surrogates;large-scale Big-Data analytics system;predictive analytics streaming;publicly available data;societal event forecasting;streaming-scalable-share-nothing architecture,,1,,12,,no,27-30 Oct. 2014,,IEEE,IEEE Conference Publications
The grading scheme based on fuzzy comprehensive evaluation and analytic hierarchy process for classified protection of information system,Y. Yang; Y. Shen; G. Zhang; G. Yu,"School of Information Science & Engineering, Lanzhou University, Lanzhou, Gansu Province, China",2014 IEEE 5th International Conference on Software Engineering and Service Science,20141023,2014,,,331,334,"This paper starts with the grading standard of classified protection of information system, comes up with the grading scheme based on fuzzy comprehensive evaluation and analytic hierarchy process for classified protection of information system. It makes a quantitative and qualitative analysis, argument for the two classificatory factors: the object and the extent of damage to the object when the target of classified security is damaged. The influence which results from judge's subjective differences is decreased. At last, an example is given to prove it that this method can be used efficiently in the grading of classified protection.",2327-0586;23270586,CD-ROM:978-1-4799-3277-1; Electronic:978-1-4799-3279-5; POD:978-1-4799-3280-1,10.1109/ICSESS.2014.6933575,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6933575,analytic hierarchy process;classified protection grading;fuzzy comprehensive evaluation,Analytic hierarchy process;Economics;Information security;National security;Standards,analytic hierarchy process;data protection;fuzzy set theory;information systems;pattern classification,analytic hierarchy process;classificatory factors;classified information system protection;fuzzy comprehensive evaluation;grading scheme;grading standard;qualitative analysis;quantitative analysis,,1,,6,,no,27-29 June 2014,,IEEE,IEEE Conference Publications
The impact analysis of distribution grid based on Battery energy storage system,D. Hui; L. Tian; X. Yang; X. Niu; K. Zhao,"Chinese Electric Power Research Institute, China",2014 China International Conference on Electricity Distribution (CICED),20141222,2014,,,51,56,"The voltage variation caused by increasing renewable energy generation can be attenuated effectively by using Battery energy storage system (BESS). Firstly, the widely used PNGV method is adopted to build the battery cells model in this paper. Then, the identification methods of electrochemistry battery's electrical equivalent circuit are introduced according to `continuous function analytic expression' and `difference equation' respectively. The experimental results show that the identified parameters are agreed with simulation data very well. Meanwhile, the low-frequency equipment model of converter is proposed by neglecting high frequency components since the main performance is determined by the fundamental frequency. The classical distribution grid is established using EMTP_RV software, including the battery storage system, converter, various power load, etc. The study results reported in this paper indicate that the battery storage system can effectively attenuate over-voltage and support the power grid. Furthermore, the proposed modelling method can be used to further analyse the optimization of BESS location and capacity.",2161-7481;21617481,Electronic:978-1-4799-4126-1; POD:978-1-4799-4125-4,10.1109/CICED.2014.6991662,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6991662,battery cell model;battery energy storage system;distributed PV;distribution grid;low-frequency equivalent model,Batteries;Data models;Equations;Mathematical model;Resistance;System-on-chip;Voltage measurement,battery storage plants;electrochemistry;load management;power convertors;power distribution control;power generation control;power grids;voltage control,BESS;EMTP_RV software;PNGV method;battery cells model;battery energy storage system;continuous function analytic expression';difference equation';distribution grid impact analysis;electrochemistry battery electrical equivalent circuit identification method;over-voltage attenuation;power load;renewable energy generation;voltage variation,,0,,6,,no,23-26 Sept. 2014,,IEEE,IEEE Conference Publications
"The Innovation Network as a Complex Adaptive System: Flexible Multi-agent Based Modeling, Simulation, and Evolutionary Decision Making",Q. Long; S. Li,"Sch. of Inf., Zhejiang Univ. of Finance & Econ., Hangzhou, China",2014 Fifth International Conference on Intelligent Systems Design and Engineering Applications,20141206,2014,,,1060,1064,"The literature rarely considers an innovation network as a complex adaptive system. In this paper, theories of complex adaptive systems research are employed to model and analyze intra-organization networks, inter-organization networks as well as their interaction mechanisms in the whole innovation context, with a conceptual framework proposed and presented. Flexible multi-agent based modeling, smart simulation, self-survival and adaptive intelligent software agents, expert systems, analytic hierarchy process, hybrid decision support approach, and statistical methods are integrated to deal with the innovation network problem and support evolutionary decision making in the open and dynamic environments.",,CD-ROM:978-1-4799-4262-6; Electronic:978-1-4799-4261-9; POD:978-1-4799-7889-2,10.1109/ISDEA.2014.234,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6977779,Innovation network;complex adaptive system;evolutionary innovation decision making;hybrid decision support system;multi-agent based modeling;simulation,Adaptation models;Adaptive systems;Analytical models;Data models;Decision making;Educational institutions;Technological innovation,adaptive systems;analytic hierarchy process;decision support systems;expert systems;innovation management;large-scale systems;multi-agent systems;software agents,adaptive intelligent software agents;analytic hierarchy process;complex adaptive system;evolutionary decision making;expert systems;flexible multiagent;hybrid decision support approach;innovation network problem;inter-organization networks;intra-organization networks;modeling;self-survival software agents;smart simulation;statistical methods,,2,,20,,no,15-16 June 2014,,IEEE,IEEE Conference Publications
The Overtime Waiting Model for Web Server Performance Evaluation,L. Zhang; Q. Zhu,"Shanghai Adv. Res. Inst., Shanghai, China",2014 International Conference on Cyber-Enabled Distributed Computing and Knowledge Discovery,20141215,2014,,,229,232,"Though the queuing model with impatient customer has been studied in the past decades, little is definitively known about the overtime waiting characteristics. In this paper, we present an overtime-waiting model for Web servers. This model is taken as a special case of the impatient customer, which quit when the waiting time is longer than a tolerable value. The pdf of the client response time is deduced for analysis of the model. By imposing the timeout threshold on the Web server, the system performance is derived. Finally, numerical results corroborate this analytic model.",,Electronic:978-1-4799-6236-5; POD:978-1-4799-6237-2,10.1109/CyberC.2014.48,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6984311,Client-server systems;Queuing theory;Web design;Web server modeling;Web services,Analytical models;Computational modeling;Mathematical model;Probability density function;Queueing analysis;Time factors;Web servers,Internet;client-server systems;file servers;queueing theory;software performance evaluation,Web server performance evaluation;Web servers;client response time;overtime waiting characteristics;overtime waiting model;queuing model;system performance;timeout threshold;waiting time,,0,,18,,no,13-15 Oct. 2014,,IEEE,IEEE Conference Publications
The role of IC technology in development and application of experimental methods and multivariate analysis,M. Orli€à; M. Marinovic,"Dept. of Civil Eng., Polytech. of Zagreb, Zagreb, Croatia","2014 37th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)",20140724,2014,,,873,878,"Information and communications (IC) technology influences all human activities but also plays an important role in development of various scientific disciplines and other types of technologies thus enabling their large scale implementation. This paper presents the relations among IC technology, experimental methods and multivariate analysis. As the development of experimental techniques and multivariate analysis paved way to revolutionary discoveries referring to IC technology, this, as a feed back, enabled further development and implementation of the mentioned methods in the most diversified fields of human activities. The necessity of the integral approach, based on implementation of multivariate analysis and adequate experimental methods and respective IC technologies is illustrated in the example of solving the problem of preservation, protection and evaluation of cultural heritage objects. In the process, various experimental analytic techniques based on principles of physics and/or chemistry are used along with IC technologies. In order to obtain the best possible analysis and interpretation of series consisting of enormous number of data, aiming to filter only the most important information relevant for particular object, statistical approach is necessary where a prominent position belongs to multivariate analysis - a task that can only be accomplished by using adequate statistics software.",,CD-ROM:978-953-233-081-6; Electronic:978-953-233-077-9; POD:978-1-4799-5657-9,10.1109/MIPRO.2014.6859690,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6859690,,Computers;Cultural differences;Integrated circuits;Internet;Measurement by laser beam;Principal component analysis;Spectroscopy,history;information technology;statistical analysis,cultural heritage objects;information and communications technology;multivariate analysis;statistics software,,0,,23,,no,26-30 May 2014,,IEEE,IEEE Conference Publications
The Solid* toolset for software visual analytics of program structure and metrics comprehension: From research prototype to product,,,,,2014,,,,,"Software visual analytics (SVA) tools combine static program analysis and fact extraction with information visualization to support program comprehension. However, building efficient and effective SVA tools is highly challenging, as it involves extensive software development in program analysis, graphics, information visualization, and interaction. We present a SVA toolset for software maintenance, and detail two of its components which target software structure, metrics and code duplication. We illustrate the toolset's usage for constructing software visualizations with examples in education, research, and industrial contexts. We discuss the design evolution from research prototypes to integrated, scalable, and easy-to-use products, and present several guidelines for the development of efficient and effective SVA solutions.",,,,http://dl.acm.org/citation.cfm?id=2537325&CFID=696538919&CFTOKEN=83912867,,,,,,,,,,yes,,,,
"The state-of-the-art of Social, Mobility, Analytics and Cloud Computing an empirical analysis",B. Dewan; S. R. Jena,"Somaiya Vidyavihar, Mumbai, India",2014 International Conference on High Performance Computing and Applications (ICHPCA),20150219,2014,,,1,6,"Recent years have seen explosive emergence of the SMAC era, which is a combination of Social, Mobility, Analytics and Cloud Computing. The uses of Social networking are growing rapidly to collaborate at all levels of the extended enterprise. Employees are using Mobility to enhance the productivity. Apart from this people started to use Analytics based on big data and Cloud Computing which provides different services like Infrastructure-as-a-Service (IaaS), Software-as-a-Service (SaaS) and Platform-as-a-Service (PaaS). This abundance of services help businesses to develop approximate solutions that ultimately leads to leverage public IT infrastructure, minimizing cost of ownership and minimizing time. The deployment of SMAC in IT sector not only enhances the decision making capability but also allows them to rollout new unchecked business models and increase their reach to customers. Enterprises need to fully understand the capability and utilization of this emerging trend in order to make sense of their intended dominance in the next few years ahead.",,CD-ROM:978-1-4799-5957-0; Electronic:978-1-4799-5958-7; POD:978-1-4799-5959-4,10.1109/ICHPCA.2014.7045356,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7045356,analytics;big data;cloud computing;mobility;social,Africa;Analytical models;Biological system modeling;Cloud computing;Computational modeling;Europe;Productivity,Big Data;cloud computing;mobile computing;social networking (online),Big Data;IT sector;IaaS;PaaS;SMAC;SaaS;decision making capability enhancement;enterprise employees;infrastructure-as-a-service;ownership cost minimization;platform-as-a-service;productivity enhancement;public IT infrastructure leveraging;social networking;social-mobility-analytics-and-cloud computing;software-as-a-service;time minimization;unchecked business models,,1,,21,,no,22-24 Dec. 2014,,IEEE,IEEE Conference Publications
Tools for design of knowledge management systems based on business intelligence,L. J. Sandoval,"Escuela de Ing. en Comput. ITCA-FEPADE, La Libertad, El Salvador",2014 IEEE Central America and Panama Convention (CONCAPAN XXXIV),20150105,2014,,,1,5,The present article is concerned about the knowledge of the different tools of Business Intelligence used to generate bases for Knowledge Management Systems that allow doing a better decisions making with less risk at any level of the organization. Tests will be made using a relational database management system and performing results in a common business application.,,Electronic:978-1-4799-7584-6; POD:978-1-4799-7585-3; USB:978-1-4799-7583-9,10.1109/CONCAPAN.2014.7000410,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7000410,Analytic Intelligence;Business Intelligence;Competitive Intelligence;Corporate Memory;Data Mining;Data Warehouse;Information Hubs;Integration Services;Intellectual Assets;Knowledge Management;Measures of the Intellectual Capital;On Line Analytic Process;Prediction Query Builder,Analytical models;Artificial intelligence;Data warehouses;Knowledge management;Servers;Software,competitive intelligence;knowledge management;relational databases,business application;business intelligence;knowledge management system;relational database management system,,0,,5,,no,12-14 Nov. 2014,,IEEE,IEEE Conference Publications
Tools for Gamification Analytics: A Survey,B. Heilbrunn; P. Herzig; A. Schill,"SAP SE, Dresden, Germany",2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing,20150202,2014,,,603,608,"Application and gamification data contains valuable information about users and their behavior. This data can be used to measure the success of gamification projects, to analyze user behavior, and to continuously improve gamification designs. Existing software solutions promise support for analyzing game- and gamification-related data. However, research shows that most gamification projects do not monitor and analyze this data in an automated way, even though gamification experts are aware of its potential. In this survey paper we identify relevant software solutions and assesses them with regards to their fulfillment of user requirements in the gamification analytics domain. The survey results can be used by practitioners to make tool decisions. Furthermore, we identify gaps of current solutions that should be addressed by future work in the field of gamification analytics.",,Electronic:978-1-4799-7881-6; POD:978-1-4799-7882-3,10.1109/UCC.2014.93,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7027560,Analytics;Data Analysis;Gamification,Bismuth;Customer satisfaction;Data models;Games;Measurement;Monitoring;Testing,behavioural sciences computing;computer games;software engineering;user interfaces,gamification analytics;gamification data;software solutions;user behavior,,1,,23,,no,8-11 Dec. 2014,,IEEE,IEEE Conference Publications
Toward Implementation of a Software Defined Cloud on a Supercomputer,P. Dreher; G. Kallumkal,"Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC, USA",2014 IEEE International Conference on Cloud Engineering,20140922,2014,,,580,585,"Conventional cloud computing architectures may seriously constrain computational throughput for high performance computing (HPC) and high-performance data (HPD) applications. The traditional approach to circumvent such problems has been to map these applications and problems onto other specialized hardware and coprocessor architectures. This is both time and resource expensive, and poses a challenge for rapidly rising demands for computation and data analytics. In this paper we report on progress to develop an alternative experimental software defined cloud implementation that virtualizes the topology of a standard HPC computational architecture. This software defined system re-arranges access to the nodes and dynamically customizes the features of the HPC hardware architecture so that they map to the specifics of the computation and data analysis application. This allows a cloud computing implementation to utilize the specialized infrastructure capabilities of an HPC system. We have created this type of user reconfigurable architecture on an IBM Blue Gene/P supercomputing environment at the Department of Energy's Argonne Leadership Computing Facility (ALCF). This pilot configuration was implemented using both an open source cloud technology called VCL (Virtual Computing Laboratory) in combination with a provisioning module called Kittyhawk. Cloud security is addressed by configuring and running a root-less version of the VCL cloud system on the ALCF's Blue Gene/P login node.",,Electronic:978-1-4799-3766-0; POD:978-1-4799-3768-4,10.1109/IC2E.2014.57,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6903532,cloud computing;federated elastic cloud;software defined system;supercomputers;workflows,Cloud computing;Computer architecture;Databases;Hardware;Servers;Supercomputers,cloud computing;multiprocessing systems;parallel processing;public domain software;security of data,HPC application;HPC computational architecture;HPC hardware architecture;HPD application;IBM Blue Gene/P supercomputing environment;Kittyhawk provisioning module;VCL technology;cloud computing architecture;cloud security;coprocessor architecture;hardware architecture;high performance computing application;high-performance data application;open source cloud technology;software defined cloud;virtual computing laboratory,,1,,14,,no,11-14 March 2014,,IEEE,IEEE Conference Publications
Toward Scalable Systems for Big Data Analytics: A Technology Tutorial,H. Hu; Y. Wen; T. S. Chua; X. Li,"School of Computing, National University of Singapore, Singapore",IEEE Access,20140710,2014,2,,652,687,"Recent technological advancements have led to a deluge of data from distinctive domains (e.g., health care and scientific sensors, user-generated data, Internet and financial companies, and supply chain systems) over the past two decades. The term big data was coined to capture the meaning of this emerging trend. In addition to its sheer volume, big data also exhibits other unique characteristics as compared with traditional data. For instance, big data is commonly unstructured and require more real-time analysis. This development calls for new system architectures for data acquisition, transmission, storage, and large-scale data processing mechanisms. In this paper, we present a literature survey and system tutorial for big data analytics platforms, aiming to provide an overall picture for nonexpert readers and instill a do-it-yourself spirit for advanced audiences to customize their own big-data solutions. First, we present the definition of big data and discuss big data challenges. Next, we present a systematic framework to decompose big data systems into four sequential modules, namely data generation, data acquisition, data storage, and data analytics. These four modules form a big data value chain. Following that, we present a detailed survey of numerous approaches and mechanisms from research and industry communities. In addition, we present the prevalent Hadoop framework for addressing big data challenges. Finally, we outline several evaluation benchmarks and potential research directions for big data systems.",2169-3536;21693536,,10.1109/ACCESS.2014.2332453,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6842585,Big data analytics;Hadoop;cloud computing;data acquisition;data analytics;data storage,Big data;Data acquisition;Information analysis;Medical services;Real-time systems;Scalability;Sensor phenomena and characterization;Sensor systems;Supply chain management,Big Data;data acquisition;data analysis;data communication;public domain software;storage management,Big Data analytics;Big Data value chain;Hadoop;data acquisition;data generation;data storage;data transmission;industry community;large-scale data processing mechanism;scalable system;sequential modules;system architecture;technology tutorial,,81,,290,,no,2014,,IEEE,IEEE Journals & Magazines
Towards a Framework for Enterprise Architecture Analytics,R. Schmidt; M. WiÌÙotzki; D. Jugel; M. MÌ_hring; K. Sandkuhl; A. Zimmermann,"Munich Univ., Munich, Germany",2014 IEEE 18th International Enterprise Distributed Object Computing Conference Workshops and Demonstrations,20141206,2014,,,266,275,"Current approaches for enterprise architecture lack analytical instruments for cyclic evaluations of business and system architectures in real business enterprise system environments. This impedes the broad use of enterprise architecture methodologies. Furthermore, the permanent evolution of systems desynchronizes quickly model representation and reality. Therefore we are introducing an approach for complementing the existing top-down approach for the creation of enterprise architecture with a bottom approach. Enterprise Architecture Analytics uses the architectural information contained in many infrastructures to provide architectural information. By applying Big Data technologies it is possible to exploit this information and to create architectural information. That means, Enterprise Architectures may be discovered, analyzed and optimized using analytics. The increased availability of architectural data also improves the possibilities to verify the compliance of Enterprise Architectures. Architectural decisions are linked to clustered architecture artifacts and categories according to a holistic EAM Reference Architecture with specific architecture metamodels. A special suited EAM Maturity Framework provides the base for systematic and analytics supported assessments of architecture capabilities.",2325-6583;23256583,Electronic:978-1-4799-5467-4; POD:978-1-4799-5468-1,10.1109/EDOCW.2014.47,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6975371,Enterprise Analytics;Enterprise Architecture,Big data;Cloud computing;Computational modeling;Computer architecture;Measurement;Organizations,Big Data;business data processing;software architecture,EAM maturity framework;architectural data;architectural decisions;architecture metamodels;big data technologies;business enterprise system environments;cyclic evaluations;enterprise architecture analytics;enterprise architectures;holistic EAM reference architecture;model representation;permanent systems evolution;system architectures;top-down approach,,1,,91,,no,1-2 Sept. 2014,,IEEE,IEEE Conference Publications
Towards a High Speed Video Cloud Based on Batch Processing Integrated with Fast Processing,W. Zhang; L. Xu; P. Duan; W. Gong; X. Liu; Q. Lu,"Dept. of Software Eng., China Univ. of Pet., Qingdao, China","2014 International Conference on Identification, Information and Knowledge in the Internet of Things",20150323,2014,,,28,33,"With the rise of video surveillance applications for analyzing real-time and batching video data in large scales, traditional video processing systems are being challenged due to real-time, intelligence and fault-tolerance demand for networked high resolution and large-scale video processing. These challenges can be further exacerbated by the existing predicaments of multi-platform, multi-format, multi-codec on video itself. The emergence of cloud computing and big data techniques makes it possible to manage complicated intelligent processing for large-scale video data. This paper proposes a general cloud-based video platform that can provide a robust solution to intelligent analytic and storage for video data, which is called ViCiBaF architecture (Video Cloud integrated with Batch processing and Fast processing) architecture. This ViCiBaF architecture is elaborated through an implementation that can effectively handle massive surveillance video data, where real-time analysis, batch processing, distributed storage and cloud services are seamlessly integrated to meet the requirements of intelligent analysis, real time, fault tolerance and massive storage for massive video data. The evaluations show that the proposed approach is efficient.",,Electronic:978-1-4799-8003-1; POD:978-1-4799-8004-8,10.1109/IIKI.2014.13,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7063992,genetic algorithm;pervasive cloud;task migration,Cameras;Cloud computing;Computer architecture;Data mining;Real-time systems;Storms;Streaming media,cloud computing;fault tolerant computing;image resolution;video surveillance,Big Data techniques;ViCiBaF architecture;cloud computing;cloud services;distributed storage;fault tolerance;high-speed video cloud;intelligent processing;intelligent video data analysis;intelligent video data storage;large-scale video data;multiplatform-multiformat-multicodec;networked high-resolution large-scale video processing;real-time analysis;real-time video data analysis;video cloud integrated-with-batch processing-and-fast-processing architecture;video data batching;video surveillance applications,,1,,25,,no,17-18 Oct. 2014,,IEEE,IEEE Conference Publications
Towards a hybrid approach of primitive cognitive network process and Bayesian network for evaluation,T. Bian; K. K. F. Yuen,"Department of Computer Science and Software Engineering, Xi'an Jiaotong-Liverpool University, Suzhou, China",2014 IEEE International Conference on Progress in Informatics and Computing,20141204,2014,,,114,118,"Evaluation is an essential step for decision making. This paper proposes a hybrid approach, named PCNP-BN, which combines the Primitive Cognitive Network Process (PCNP) and the Bayesian Network (BN) for evaluation activities. PCNP, which is an ideal alternative of the Analytic Hierarchy Process (AHP), helps to quantify the influence of factors, whilst Bayesian network is utilized to combine all the useful feedbacks. The proposed approach can support evaluation activities through quantifying complex factors into measurable values.",,CD-ROM:978-1-4799-2031-0; Electronic:978-1-4799-2030-3; POD:978-1-4799-2032-7,10.1109/PIC.2014.6972307,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6972307,bayesian network;decision making;evaluation;primitive cognitive network process,Analytic hierarchy process;Artificial intelligence;Bayes methods;Risk management;Uncertainty;Weight measurement,analytic hierarchy process;belief networks;cognitive systems,AHP;Bayesian network;PCNP-BN;analytic hierarchy process;complex factors;decision making;evaluation activities;primitive cognitive network process,,0,,15,,no,16-18 May 2014,,IEEE,IEEE Conference Publications
Towards a hybrid approach of primitive cognitive network process and fuzzy cognitive map for box office analysis,N. Y. Zhou; K. K. F. Yuen,"Department of Computer Science and Software Engineering, Xi'an Jiaotong-Liverpool University, Suzhou, China",2014 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),20140908,2014,,,1049,1053,"Box office analysis is critical to make profitable movies. Various factors have different influences on box office sales. This paper combines Primitive Cognitive Network Process (PCNP) and Fuzzy Cognitive Map (FCM) to measure and analyze the factors of box office. PCNP is a revised approach of Analytic Hierarchy Process (AHP) to quantify the weights of factors to construct a concept in FCM. FCM is used to simulate the influences of the concepts in the network. The proposed hybrid approach can enhance the evaluation and. To show the applicability of PCNP-FCM, an example of box office analysis is illustrated.",1098-7584;10987584,CD-ROM:978-1-4799-2073-0; Electronic:978-1-4799-2072-3; POD:978-1-4799-2074-7,10.1109/FUZZ-IEEE.2014.6891629,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6891629,box office analysis;fuzzy cognitive map;model analysis;pairwise comparisons,Analytic hierarchy process;Analytical models;Artificial intelligence;Fuzzy cognitive maps;Motion pictures;Production;Vectors,analytic hierarchy process;fuzzy set theory;profitability,AHP;FCM;PCNP;analytic hierarchy process;box office analysis;fuzzy cognitive map;primitive cognitive network process,,0,,22,,no,6-11 July 2014,,IEEE,IEEE Conference Publications
Towards a hybrid approach of primitive cognitive network process and weighted iterative dichotomiser 3 for customer e-payment adoption analysis,J. W. Su; K. K. F. Yuen,"Department of Computer Science and Software Engineering, Xi'an Jiaotong-Liverpool University, Suzhou, China",2014 IEEE 7th Joint International Information Technology and Artificial Intelligence Conference,20150323,2014,,,218,222,"Attracting customers to use e-payment is the critical success factor for the e-business transaction. Various factors influence the customers to adopt e-Payment. A hybrid approach of Primitive Cognitive Network Process (PCNP) and Weighted Iterative Dichotomiser Three (WID3) is proposed to classify the factors which influence the customer e-payment adoptions. Whilst PCNP is a revised approach of Analytic Hierarchy Process (AHP) to quantify the weights of factors, WED3 is the classification approach combining the weighted factors to the classical ID3 to classify data into distinct groups. An application shows the proposed approach could identify the patterns of customer e-Payment adoption, and predict the potential customer adoption behaviour.",,CD-ROM:978-1-4799-4421-7; Electronic:978-1-4799-4419-4; POD:978-1-4799-4418-7,10.1109/ITAIC.2014.7065038,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7065038,Classification;Cognitive Network Process;E-Payment;ID3;customer adoption,Analytic hierarchy process;Artificial intelligence;Classification algorithms;Decision trees;Equations;Industries;Mathematical model,analytic hierarchy process;cognitive systems;consumer behaviour;customer profiles;electronic money,AHP;analytic hierarchy process;customer adoption behaviour;customer e-payment adoption analysis;customer e-payment adoptions;data classification;e-business transaction;primitive cognitive network process;weighted iterative dichotomiser three,,0,,18,,no,20-21 Dec. 2014,,IEEE,IEEE Conference Publications
Towards a Set Theoretical Approach to Big Data Analytics,R. R. Mukkamala; A. Hussain; R. Vatrapu,"IT Univ. of Copenhagen, Copenhagen, Denmark",2014 IEEE International Congress on Big Data,20140925,2014,,,629,636,"Formal methods, models and tools for social big data analytics are largely limited to graph theoretical approaches such as social network analysis (SNA) informed by relational sociology. There are no other unified modeling approaches to social big data that integrate the conceptual, formal and software realms. In this paper, we first present and discuss a theory and conceptual model of social data. Second, we outline a formal model based on set theory and discuss the semantics of the formal model with a real-world social data example from Facebook. Third, we briefly present and discuss the Social Data Analytics Tool (SODATO) that realizes the conceptual model in software and provisions social data analysis based on the conceptual and formal models. Fourth and last, based on the formal model and sentiment analysis of text, we present a method for profiling of artifacts and actors and apply this technique to the data analysis of big social data collected from Facebook page of the fast fashion company, H&M.",2379-7703;23797703,Electronic:978-1-4799-5057-7; POD:978-1-4799-5058-4,10.1109/BigData.Congress.2014.96,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6906838,Big Social Data;Computational Social Science;Data Science;Formal Methods;Social Data Analytics,Analytical models;Data models;Facebook;Mathematical model;Media;Tagging,Big Data;data analysis;set theory;social networking (online);text analysis,Facebook;Facebook page;H&M;SODATO;conceptual model;fast fashion company;formal model;graph theoretical approach;relational sociology;set theoretical approach;social big data analytics;social data analytic tool;social network analysis;text sentiment analysis,,9,,31,,no,June 27 2014-July 2 2014,,IEEE,IEEE Conference Publications
Towards a Visual Analytics Framework for Handling Complex Business Processes,W. Ribarsky; D. X. Wang; W. Dou; W. J. Tolone,"Dept. of Comput. Sci., Univ. of North Carolina at Charlotte, Charlotte, NC, USA",2014 47th Hawaii International Conference on System Sciences,20140310,2014,,,1374,1383,"Organizing data that can come from anywhere in the complex business process in a variety of types is a challenging task. To tackle the challenge, we introduce the concepts of virtual sensors and process events. In addition, a visual interface is presented in this paper to aid deploying the virtual sensors and analyzing process events information. The virtual sensors permit collection from the streams of data at any point in the process and transmission of the data in a form ready to be analyzed by the central analytics engine. Process events provide a uniform expression of data of different types in a form that can be automatically prioritized and that is readily meaningful to the users. Through the visual interface, the user can place the virtual sensors, interact with and group the process events, and delve into the details of the process at any point. The visual interface provides a multiview investigative environment for sense making and decisive action by the user.",1530-1605;15301605,Electronic:978-1-4799-2504-9; POD:978-1-4799-2505-6,10.1109/HICSS.2014.177,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6758774,business process;dynamic manufacturing;sense making;virtual sensor;visual analytics,Business;Educational institutions;Manufacturing;Production;Sensors;Visual analytics,business data processing;data analysis;data visualisation;organisational aspects,central analytics engine;data organisation;data streaming;handling complex business processes;virtual sensors;visual analytics framework;visual interface,,0,,20,,no,6-9 Jan. 2014,,IEEE,IEEE Conference Publications
Towards Model-Driven Engineering for Big Data Analytics -- An Exploratory Analysis of Domain-Specific Languages for Machine Learning,D. Breuker,"ERCIS, Univ. of Muenster, Muenster, Germany",2014 47th Hawaii International Conference on System Sciences,20140310,2014,,,758,767,"Graphical models and general purpose inference algorithms are powerful tools for moving from imperative towards declarative specification of machine learning problems. Although graphical models define the principle information necessary to adapt inference algorithms to specific probabilistic models, entirely model-driven development is not yet possible. However, generating executable code from graphical models could have several advantages. It could reduce the skills necessary to implement probabilistic models and may speed up development processes. Both advantages address pressing industry needs. They come along with increased supply of data scientist labor, the demand of which cannot be fulfilled at the moment. To explore the opportunities of model-driven big data analytics, I review the main modeling languages used in machine learning as well as inference algorithms and corresponding software implementations. Gaps hampering direct code generation from graphical models are identified and closed by proposing an initial conceptualization of a domain-specific modeling language.",1530-1605;15301605,Electronic:978-1-4799-2504-9; POD:978-1-4799-2505-6,10.1109/HICSS.2014.101,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6758697,Graphical Models;Machine Learning;Model-driven Engineering,Adaptation models;Computational modeling;Data models;Graphical models;Inference algorithms;Random variables;Unified modeling language,Big Data;computer graphics;data analysis;inference mechanisms;learning (artificial intelligence);program compilers;specification languages,big data analytics;direct code generation;domain-specific languages;domain-specific modeling language;general purpose inference algorithms;graphical models;machine learning problems;model-driven development;model-driven engineering;modeling languages;probabilistic models,,0,,37,,no,6-9 Jan. 2014,,IEEE,IEEE Conference Publications
Towards OpenFlow based software defined networks,P. Chhikara; G. S. Matharu; V. Deep,"Department of Information Technology, Amity University Uttar Pradesh, Noida, India",2014 IEEE International Conference on Computational Intelligence and Computing Research,20150907,2014,,,1,6,"Software defined networks (SDN) are an emerging technology that is being increasingly adopted by various network operators. These technologies provide new services and powerful analytics that help to transform the network and unfasten its intelligence to serve today's business demands. This paper briefs about the need for change in the current networking technology and explores the role of Open Flow protocol that is used by researchers to experiment with more realistic settings to provide for a new network architecture. Further, this paper discusses the advantages offered by SDN and the huge potential of OpenFlow based SDN. As SDN can simplify management of virtualized networks, enable cloud computing and reduce costs, the vendors would be encouraged to adopt SDN and OpenFlow. The objective of this paper is to provide an insight into the latest technology to the vendors to assist them in future enhancement of their switch products in the network.",,Electronic:978-1-4799-3975-6; POD:978-1-4799-3976-3,10.1109/ICCIC.2014.7238395,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7238395,OpenFlow;OpenFlow Switch;OpenFlow Working;SDN Architecture;Software defined networks (SDN),Bandwidth;Computer architecture;Protocols;Security;Software;Switches,protocols;software defined networking,OpenFlow protocol;SDN;software defined networks,,0,,16,,no,18-20 Dec. 2014,,IEEE,IEEE Conference Publications
Towards robustness and self-organization of ESB-based solutions using service life-cycle management,P. LeitÌ£o; J. Barbosa; A. Pereira,"Polytechnic Institute of Bragan&#x00E7;a, Campus Sta Apol&#x00F3;nia, Apartado 1134, 5301-857 Bragan&#x00E7;a, Portugal",IECON 2014 - 40th Annual Conference of the IEEE Industrial Electronics Society,20150226,2014,,,4916,4921,"Enterprise Service Bus (ESB) is a middleware infrastructure that provides a way to integrate loosely-coupled heterogeneous software applications based on the services principles. The life-cycle management of services in such environments is a critical issue for the component's reuse, maintenance and operation. This paper introduces a service life-cycle management module that extends the traditional functionalities with advanced monitoring and data analytics to contribute for the robustness, reliability and self-organization of networks of clusters based on ESB platforms. The realization of this module was embedded in the JBoss ESB, considering a sniffer mechanism to collect the service messages crossing the bus and a Liferay portal to display relevant information related to the services' health.",1553-572X;1553572X,Electronic:978-1-4799-4032-5; POD:978-1-4799-4031-8; USB:978-1-4799-4033-2,10.1109/IECON.2014.7049246,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7049246,,Data analysis;Degradation;Monitoring;Ontologies;Robustness;Software,data analysis;middleware;product life cycle management;software management;software reliability;system monitoring,ESB-based solutions;JBoss ESB;Liferay portal;advanced monitoring;cluster network reliability;cluster network self-organization;data analytics;enterprise service bus;loosely-coupled heterogeneous software applications;middleware infrastructure;service life-cycle management module;sniffer mechanism,,0,,18,,no,Oct. 29 2014-Nov. 1 2014,,IEEE,IEEE Conference Publications
Towards the optimization of a parallel streaming engine for telco applications,B. Theeten; I. Bedini; P. Cogan; A. Sala; T. Cucinotta,,Bell Labs Technical Journal,20140429,2014,18,4,181,197,"Parallel and distributed computing is becoming essential to process in real time the increasingly massive volume of data collected by telecommunications companies. Existing computational paradigms such as MapReduce (and its popular open-source implementation Hadoop) provide a scalable, fault tolerant mechanism for large scale batch computations. However, many applications in the telco ecosystem require a real time, incremental streaming approach to process data in real time and enable proactive care. Storm is a scalable, fault tolerant framework for the analysis of real time streaming data. In this paper we provide a motivation for the use of real time streaming analytics in the telco ecosystem. We perform an experimental investigation into the performance of Storm, focusing in particular on the impact of parameter configuration. This investigation reveals that optimal parameter choice is highly non-trivial and we use this as motivation to create a parameter configuration engine. As first steps towards the creation of this engine we provide a deep analysis of the inner workings of Storm and provide a set of models describing data flow cost, central processing unit (CPU) cost, and system management cost.",1089-7089;10897089,,10.1002/bltj.21652,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6770354,,Central Processing Unit;Costs;Data analysis;Distributed databases;Distributed processing;Fault tolerant systems;Flow management;Media streaming;Parallel computing;Real-time systems;Scalability,data analysis;parallel programming;public domain software;software fault tolerance;telecommunication industry,Hadoop;MapReduce;Storm;central processing unit cost;data flow cost;distributed computing;fault tolerant mechanism;incremental streaming approach;large scale batch computations;open-source implementation;parallel computing;parallel streaming engine optimization;parameter configuration engine;real time streaming data analysis;system management cost;telco applications;telco ecosystem,,1,,,,no,14-Mar,,Alcatel-Lucent,Alcatel-Lucent Journal
Towards tool support for analyzing legacy systems in technical domains,C. Klammer; J. Pichler,"Software Analytics & Evolution, Software Competence Center Hagenberg GmbH, Hagenberg, Austria","2014 Software Evolution Week - IEEE Conference on Software Maintenance, Reengineering, and Reverse Engineering (CSMR-WCRE)",20140227,2014,,,371,374,"Software in technical domains contains extensive and complex computations in a highly-optimized and unstructured way. Such software systems developed and maintained over years are prone to become legacy code based on old technology and without accurate documentation. We have conducted several industrial projects to reengineer and re-document legacy systems in electrical engineering and steel making domains by means of self-provided techniques and tools. Based on this experience, we derived requirements for a toolkit to analyze legacy code in technical domains and developed a corresponding toolkit including feature location and static analysis on a multi-language level. We have applied our approach and toolkit for software systems implemented in the C++, Fortran, and PL/SQL programming languages and illustrate main benefits of our approach from these experiences.",,Electronic:978-1-4799-3752-3; POD:978-1-4799-3753-0,10.1109/CSMR-WCRE.2014.6747197,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6747197,Software analytics;feature location;legacy code;program comprehension;reverse engineering;source code analysis,Aging;Algorithms;Computer languages;Reverse engineering;Software systems;Syntactics,C++ language;FORTRAN;SQL;program diagnostics;programming languages;reverse engineering;software maintenance;source code (software);system documentation,C++;Fortran;PL/SQL programming languages;documentation;electrical engineering;feature location;industrial projects;legacy code;multilanguage level;redocument legacy systems;self-provided techniques;software systems;static analysis;steel making domains;technical domains;tool support,,5,,11,,yes,3-6 Feb. 2014,,IEEE,IEEE Conference Publications
Transforming Scagnostics to Reveal Hidden Features,T. N. Dang; L. Wilkinson,"Department of Computer Science, University of Illinois at Chicago",IEEE Transactions on Visualization and Computer Graphics,20141106,2014,20,12,1624,1632,"Scagnostics (Scatterplot Diagnostics) were developed by Wilkinson et al. based on an idea of Paul and John Tukey, in order to discern meaningful patterns in large collections of scatterplots. The Tukeys' original idea was intended to overcome the impediments involved in examining large scatterplot matrices (multiplicity of plots and lack of detail). Wilkinson's implementation enabled for the first time scagnostics computations on many points as well as many plots. Unfortunately, scagnostics are sensitive to scale transformations. We illustrate the extent of this sensitivity and show how it is possible to pair statistical transformations with scagnostics to enable discovery of hidden structures in data that are not discernible in untransformed visualizations.",1077-2626;10772626,,10.1109/TVCG.2014.2346572,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875999,High-Dimensional Visual Analytics;Scagnostics;Scatterplot matrix;Transformation,Data visualization;Feature extraction;Shape analysis;Visual analytics,data mining;data visualisation;statistical analysis,data visualization;hidden structure discovery;scagnostics;scale transformation;scatterplot collection;scatterplot diagnostics;scatterplot matrices;statistical transformations,,0,,44,,no,Dec. 31 2014,,IEEE,IEEE Journals & Magazines
Traversing Trillions of Edges in Real Time: Graph Exploration on Large-Scale Parallel Machines,F. Checconi; F. Petrini,"High Performance Analytics Dept., IBM TJ Watson, Yorktown Heights, NY, USA",2014 IEEE 28th International Parallel and Distributed Processing Symposium,20140814,2014,,,425,434,"The world of Big Data is changing dramatically right before our eyes-from the amount of data being produced to the way in which it is structured and used. The trend of ""big data growth"" presents enormous challenges, but it also presents incredible scientific and business opportunities. Together with the data explosion, we are also witnessing a dramatic increase in data processing capabilities, thanks to new powerful parallel computer architectures and more sophisticated algorithms. In this paper we describe the algorithmic design and the optimization techniques that led to the unprecedented processing rate of 15.3 trillion edges per second on 64 thousand Blue Gene/Q nodes, that allowed the in-memory exploration of a petabyte-scale graph in just a few seconds. This paper provides insight into our parallelization and optimization techniques. We believe that these techniques can be successfully applied to a broader class of graph algorithms.",1530-2075;15302075,Electronic:978-1-4799-3800-1; POD:978-1-4799-3801-8,10.1109/IPDPS.2014.52,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877276,BFS;BlueGene/Q;Graph Algorithms;Graph500,Algorithm design and analysis;Big data;Computer architecture;Matrix decomposition;Optimization;Software algorithms;Supercomputers,graph theory;optimisation;parallel architectures;parallel machines,BlueGene-Q nodes;big data growth;business opportunities;data explosion;data processing capabilities;graph exploration;in-memory exploration;large-scale parallel machines;optimization techniques;parallel computer architectures;petabyte-scale graph;scientific opportunities,,10,,28,,no,19-23 May 2014,,IEEE,IEEE Conference Publications
UCD-Griffin-MC2 submission summary,K. S. Griffin,University of California at Davis,2014 IEEE Conference on Visual Analytics Science and Technology (VAST),20150216,2014,,,351,352,"This paper describes the approach and the visual analytics (VA) tool developed to solve the VAST 2014 Challenge: Mini-Challenge 2. For this challenge, a methodical approach was taken by analyzing each GAStech employees daily activities. Open source software, commercial application programming interfaces (APIs), and custom software were combined to create a very robust VA tool used in this challenge. The tool provides visualization and animation of the tracking data, an overview of credit card and loyalty card transactions, and on-demand details of the aforementioned data. Finally, a novel feature was implemented in the VA tool to insert missing location data for credit card and loyalty card transactions in realtime.",,Electronic:978-1-4799-6227-3; POD:978-1-4799-6186-3,10.1109/VAST.2014.7042561,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7042561,,Animation;Business;Data visualization;Geospatial analysis;Google;Shape;Vehicles,,,,0,,4,,no,25-31 Oct. 2014,,IEEE,IEEE Conference Publications
User Environment Tracking and Problem Detection with XALT,K. Agrawal; M. R. Fahey; R. McLay; D. James,"Nat. Inst. for Comput. Sci. (NICS), Univ. of Tennessee, Knoxville, Knoxville, TN, USA",2014 First International Workshop on HPC User Support Tools,20150409,2014,,,32,40,"This work enhances our understanding of individual users' software needs, then leverages that understanding to help stakeholders conduct business in a more efficient, effective, and systematic way. The product, XALT, builds on work that is already improving the user experience and enhancing support programs for thousands of users on twelve supercomputers across the United States and Europe. XALT will instrument individual jobs on high-end computers to generate a picture of the compilers, libraries, and other software that users need to run their jobs successfully. It will highlight the products our researchers need and do not need, and alert users and support staff to the root causes of software configuration issues as soon as the problems occur. A key objective of this work is generating the information needed to improve efficiency and effectiveness for an extensive community of stakeholders including users, sponsoring institutions, support organizations, and development teams. Efficiency, effectiveness, and responsible stewardship each require a clear picture of users' needs. XALT is an important step in the quest to achieve that clarity.",,Electronic:978-1-4673-6755-4; POD:978-1-4799-7150-3,10.1109/HUST.2014.6,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7081224,XALT; user environment; library tracking; problem detection; reproducibility; job analytics,Assembly;Data mining;Databases;Documentation;Libraries;Software;Supercomputers,software engineering;user interfaces,Europe;United States;XALT;problem detection;software configuration;user environment tracking;user software needs,,4,,27,,no,21-21 Nov. 2014,,IEEE,IEEE Conference Publications
Using a learning analytics tool for evaluation in self-regulated learning,A. R. Groba; B. V. Barreiros; M. Lama; A. Gewerc; M. Mucientes,"Department of Didactic and School Organization, University of Santiago de Compostela, Spain",2014 IEEE Frontiers in Education Conference (FIE) Proceedings,20150219,2014,,,1,8,"In self-regulated learning, evaluation is a complex task of the teaching process, but even more if students have social media that allow them to build their personal learning environment in different ways. In these kind of virtual environments a large amount of data that needs to be assessed by teachers is generated, and therefore they require tools that facilitate the assessment task. In this paper, we present an experiment with a process mining-based learning analytics tool, called SoftLearn, that helps teachers to assess the student's activity in self-regulated learning. The subject of this experiment is taught in blended learning mode with weekly classroom sessions, and the students use a social network software, called ELGG, as an e-portfolio in which they reflect their individual knowledge process construction. The results show that the use of this tool reduces significantly the assessment time and helps teachers to understand the learning process of the students.",0190-5848;01905848,Electronic:978-1-4799-3922-0; POD:978-1-4799-3923-7; USB:978-1-4799-3921-3,10.1109/FIE.2014.7044400,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7044400,,Blogs;Clustering algorithms;Context;Educational institutions;Process control;Social network services,computer aided instruction;data mining;social networking (online),ELGG;SoftLearn;blended learning mode;e-portfolio;individual knowledge process construction;learning analytics tool;personal learning environments;process mining-based learning analytics tool;self-regulated learning;social media;social network software;teaching process;weekly classroom sessions,,2,,30,,no,22-25 Oct. 2014,,IEEE,IEEE Conference Publications
Using commercial web services to build Automated Test Equipment cloud based applications,D. D. Reitze,"Northrop Grumman Corporation, Electronic Systems - Defensive Systems Division, Rolling Meadows, Il 60008, USA",2014 IEEE AUTOTEST,20141027,2014,,,246,250,"The purpose of this paper is to present a framework from which Automated Test Equipment (ATE) manufacturers can use to help them develop, integrate, and deliver ATE cloud-based applications to the consumers of their products. In order to create these applications, the developer can utilize Commercial Web Services (CWS) as a means to help access compute power, storage devices, and other services that provide the flexibility to choose a development platform or programming model that makes the most sense in trying to resolve the problem at hand. CWS provides a flexible environment from which to choose various programming models, operating systems, databases, and architectures to serve the consumers needs. CWS is highly cost-effective in that the developers and consumers pay only for what they use. Using CWS makes it extremely easy to create scalable and elastic systems as the developers can quickly add and subtract resources to their applications in order to meet current or future consumer needs. Furthermore, security is always a concern so CWS builds services in accordance with security best practices by providing the appropriate security features in those services. Using a CWS provides a level of scale, security, reliability, and privacy that are often cost prohibitive for most organizations to meet. This paper will examine available CWS cloud service platforms that organizations can potentially use to help deploy applications and services in a cost effective manner. The CWS platform consists of the following six main services, which will be discussed in more detail in this paper - (1) Computational/Networking, (2) Storage/Content Delivery, (3) Databases, (4) Analytics, (5) Applications, and (6) Deployment and Management services. One or more of these services may be utilized to help develop, integrate, and deliver ATE cloud-based applications to the consumer. The goals of ATE are to (1) quickly and accurately detect and isolate each fault, (2) provide software tool- for analyzing historical data, (3) gather, manage, and distribute accurate and reliable maintenance information for the failed Unit Under Test (UUT). The CWS cloud platform will aid in the development of cloud based tools and applications that are cost effective, flexible, scalable, and secure that can be used by multiple end users to aid in the development of ATE system software tool sets. One example of a cloud-based application is a diagnostic reasoner that could be used to aid in diagnosing UUT repair actions. This paper will show how developers can use CWS to develop ATE cloud-based applications and tools that will help improve the overall ATE testing throughput, thus resulting in bottom line improvements to ATE life cycle costs.",1088-7725;10887725,CD-ROM:978-1-4799-3389-1; Electronic:978-1-4799-3005-0; POD:978-1-4799-3006-7,10.1109/AUTEST.2014.6935153,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6935153,ATE;cloud based applications;commercial web services;maintenance;testing,Cloud computing;Databases;Organizations;Security,Web services;automatic test equipment;cloud computing;data privacy;life cycle costing;operating systems (computers);reliability,ATE;CWS;UUT;automated test equipment;cloud-based application;commercial Web service;computational-networking;elastic system;fault isolation;life cycle costing;operating system;privacy;programming model;reliability;security;storage device;storage-content delivery;unit under test,,0,,2,,no,15-18 Sept. 2014,,IEEE,IEEE Conference Publications
Using data visualization to facilitate secure engineering hardening your code through improved next generation softSecVis,J. Rodgers; D. Jaramillo; J. Wang,"IBM CIO Lab - Security Innovations, University of Central Florida, USA",IEEE SOUTHEASTCON 2014,20141110,2014,,,1,7,"Cyber security threats and vulnerabilities are as prevalent today as ever. Security should be designed and built into programs from day one. There are many code analysis and scanners available, but few offer a data visualization view, and little has been written about leveraging data analytics and visualization for the improvement of security posture across a code base. Analyzing large enterprise projects results in both massive data consumed and reported. The need to visualize this data for security, softSecVis, is very real. We examine prior findings, prototypes, and current tools used in software security visualization at the code level. We go on to propose several new methods of visualizing security vulnerabilities.",1091-0050;10910050,Electronic:978-1-4799-6585-4; POD:978-1-4799-6586-1,10.1109/SECON.2014.6950668,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6950668,data security;data visualization;developer tools;vulnerability analysis,Data visualization;Heating;Proposals;Prototypes;Security;Software;Visualization,data visualisation;security of data,cyber security threats;cyber security vulnerabilities;data analytics;data security;data visualization;enterprise projects;next generation softSecVis;secure engineering;software security visualization,,0,,10,,no,13-16 March 2014,,IEEE,IEEE Conference Publications
Using infographies as a tool for introductory data analytics education in 9‰ÛÒ12,J. Kennedy; P. Abichandani; A. Fontecchio,"ECE Department, Drexel University, Philadelphia, PA 19104",2014 IEEE Frontiers in Education Conference (FIE) Proceedings,20150219,2014,,,1,4,"Data Analytics is an all pervasive concept and involves procuring and analyzing data, visualizing the results of the analysis, and drawing conclusions and insights from the results. This paper describes the results of using infographies to teach data analytics to 262 high school students in grades 9 through 12 across four schools in Philadelphia. For the purpose of this paper, an infographie is defined as a graphical representation of information gleaned from data-driven analyses. A collaboration between university professors, Ph.D. students, and high-school teachers enabled covering core data analytics topics in multiple science classes. Results from the pre-class and post-class survey results indicate that students were able to gain skills to create infographies using basic descriptive statistics. Students were able to articulate data-driven insights and explain their findings using the infographies in the classroom.",0190-5848;01905848,Electronic:978-1-4799-3922-0; POD:978-1-4799-3923-7; USB:978-1-4799-3921-3,10.1109/FIE.2014.7044488,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7044488,Infographic;K-12 STEM Education;data analytics,Biology;Data analysis;Data visualization;Educational institutions;Electronic mail;Software,computer aided instruction;data analysis;data visualisation;teaching,PhD students;Philadelphia;data-driven analyses;data-driven insights;descriptive statistics;graphical representation;high-school teachers;infographics;introductory data analytics education;pervasive concept;post-class survey;preclass survey;university professors,,0,,11,,no,22-25 Oct. 2014,,IEEE,IEEE Conference Publications
Using open source modeling tools to enhance engineering analysis,J. C. Fuller; S. E. McHann; W. Sunderman,"Adv. Power & Energy Syst., Pacific Northwest Nat. Lab., Richland, WA, USA",2014 IEEE Rural Electric Power Conference (REPC),20140626,2014,,,C4-1,C4-5,"There are several very capable open source engineering analysis tools that a utility can use to assist in solving engineering analysis problems. Applying these open source applications, alongside a commercial engineering analysis application, can be a very cost effective way for utility engineers to solve problems presented by newer technologies, such as Solar and Wind DG and PEV charging, using the data they already have in their commercial applications. In this paper we will use GridLAB-D, OpenDSS, and Milsoft's WindMil, to show examples of today's engineering analysis problems that can be solved using open source and commercial applications in concert with each other. The purpose of the paper is to show what is possible, and what is needed to use multiple analysis tools to solve more complex analysis. The purpose of the paper is not, for example, to complete a complex time series study on the affect of PV generation on a distribution network, which could be a paper on its own merit.",0734-7464;07347464,Electronic:978-1-4799-3324-2; POD:978-1-4799-3325-9; USB:978-1-4799-3323-5,10.1109/REPCon.2014.6842209,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6842209,Active Grid Management;Advanced Metering Infrastructure (AMI);Big Data;Data Analytics;Distribution Analysis;Distribution Automation;Distribution State Estimation;Load Flow Analysis;Real Time Distribution Feeder Analysis;S CAD A (System Control and Data Acquisition);Smart Grid;Smart Meters,Analytical models;Computer aided software engineering;Control systems;Data models;Documentation;Harmonic analysis;Voltage measurement,distribution networks;photovoltaic power systems;power system analysis computing;public domain software,GridLAB-D;Milsoft WindMil;OpenDSS;PV generation;distribution network;engineering analysis enhancement;open source engineering analysis tools;open source modeling tools,,1,,11,,no,18-21 May 2014,,IEEE,IEEE Conference Publications
Using software architectures to retrieve interaction information in eLearning environments,J. C. Benito; F. J. GarcÌ_a-PeÌ±alvo; R. TherÌ_n; C. Maderuelo; J. S. PÌ©rez-Blanco; H. Zazo; A. MartÌ_n-SuÌÁrez,"GRIAL Research Group, Department of Computers and Automatics, University of Salamanca, Spain",2014 International Symposium on Computers in Education (SIIE),20150122,2014,,,117,120,"This research paper presents a software architecture based on services and deployed in a cloud environment that retrieves, analyzes and presents information collected from a closed eLearning environment like the Virtual Worlds. This software architecture is able to gather the user interaction in the digital platform, organize the interaction data and to perform measurements, estimates and basic analysis of the data, in order to give information to the managers and teachers about usage indicators and the resolution degree of the goals that students need to achieve in these scenarios and learning systems. To test this idea, the paper describe the application of the analytics layer of this software architecture on a real case, so it is possible to understand how can help this kind of architectures in the detection of the achievement of the learning goals or in the knowledge discover about users usage within the learning environment.",,Electronic:978-1-4799-4428-6; POD:978-1-4799-4427-9,10.1109/SIIE.2014.7017715,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7017715,Second Life;Software architecture;Virtual Worlds;cloud;eLearning;usage analysis;usage indicators,Computer architecture;Educational institutions;Electronic learning;Laboratories;Second Life;Software architecture;Software measurement,cloud computing;computer aided instruction;data analysis;human computer interaction;information analysis;information retrieval;software architecture,Second Life;Virtual Worlds;cloud environment;data analysis;data estimation;data measurement;digital platform;eLearning environments;interaction information retrieval;learning systems;resolution degree;software architectures;usage analysis;usage indicators;user interaction,,0,,12,,no,12-14 Nov. 2014,,IEEE,IEEE Conference Publications
Value Evaluation of Enterprise Management Informatization Based on Comprehensive Method,L. Jia; L. Minbo,"Software Sch., Fudan Univ., Shanghai, China",2014 IEEE 11th International Conference on e-Business Engineering,20141211,2014,,,52,60,"It is very difficult for Chinese enterprises to estimate whether the management informatization has been implemented successfully because of lacking a proper evaluation method. This paper involves 3 first-level indicators, 13 second-level indicators and 100 third-level indicators which are classified respectively according to informatization basis, application and effect. The third-level index can be chosen flexibly based on given restrictions and actual situation. We propose a comprehensive evaluation method which quantifies the qualitative problems through the questionnaires filled in by the measured object of target enterprises rather than onsite judgment by the experts. To make sure the evaluation method is valid, we evaluated five enterprises and used analytic hierarchy process to calculate the weight of the indicator and the expert. The results of evaluation indicate that the proposed evaluation system is reasonable, simple and practical to reflect the performance of enterprise management informatization.",,Electronic:978-1-4799-6563-2; POD:978-1-4799-6564-9,10.1109/ICEBE.2014.21,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6982059,Evaluation index;Evaluation method;Management informatization;Rank level,Collaboration;Companies;Economics;Indexes;Technological innovation,analytic hierarchy process;management information systems;value engineering,Chinese enterprises;analytic hierarchy process;comprehensive method;enterprise management informatization;first-level indicators;second-level indicators;third-level indicators;value evaluation,,0,,24,,no,5-7 Nov. 2014,,IEEE,IEEE Conference Publications
VAST 2014: Summary on grant challenge work,Y. Liu,ASTRI,2014 IEEE Conference on Visual Analytics Science and Technology (VAST),20150216,2014,,,305,306,"VAST 2014 grant challenge required a big picture on a fictional company employee missing event. This paper introduces the software we developed to tackle the challenge. In addition, how we integrate the visual analysis results from related information identified is included. And, the integrated analysis results (motivations, suspects, and related locations) are illustrated to help further investigations.",,Electronic:978-1-4799-6227-3; POD:978-1-4799-6186-3,10.1109/VAST.2014.7042539,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7042539,VAST 2014;grant challenge;visual analysis,Companies;Credit cards;Electronic mail;Fires;Global Positioning System;Software;Visualization,,,,0,,1,,no,25-31 Oct. 2014,,IEEE,IEEE Conference Publications
VAST 2014: Summary on Mini Challenge two,C. K. F. Tse; Y. Liu,"ASTRI, Hong Kong",2014 IEEE Conference on Visual Analytics Science and Technology (VAST),20150216,2014,,,363,364,"VAST 2014 mini challenge two required us to analyze movements and tracking data in a fictional company employee missing event. This paper includes our design and software to tackle the challenge. In addition, the procedure to find the abnormal patterns in the given data is illustrated.",,Electronic:978-1-4799-6227-3; POD:978-1-4799-6186-3,10.1109/VAST.2014.7042567,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7042567,VAST 2014;mini challenge two;visual analysis,Calculators;Companies;Credit cards;Global Positioning System;Information systems;Software;Traffic control,,,,0,,3,,no,25-31 Oct. 2014,,IEEE,IEEE Conference Publications
VAST Challenge MC1: An off the shelf approach to messy data,F. McGee; B. Broeksema; B. Otjacques,Centre de Recherche Public Gabriel Lippmann,2014 IEEE Conference on Visual Analytics Science and Technology (VAST),20150216,2014,,,379,380,"We describe our approach to the VAST challenge using off the shelf software, for analysis and visualization. This allowed us to gain insight into the data within a short time-frame. We discuss the merits of our approach and set out directions for future research based on our experience.",,Electronic:978-1-4799-6227-3; POD:978-1-4799-6186-3,10.1109/VAST.2014.7042575,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7042575,,Cleaning;Companies;Data visualization;Histograms;Software;Visual analytics,,,,0,,4,,no,25-31 Oct. 2014,,IEEE,IEEE Conference Publications
Venture Capital: Fueling the Innovation Economy,S. Srinivasan; I. Barchas; M. Gorenberg; E. Simoudis,,Computer,20140819,2014,47,8,40,47,"Venture capital experts explore current opportunities to fuel and benefit from the innovation economy. Venture capital remains a vibrant force in the US, funding a broad array of disruptive technologies and playing a major role in fueling a startup-based economy. A survey of Austin Technology Incubator's recent startup portfolio suggests promising trends for venture investment in the innovation economy. Next-generation data analytics, offering strategic business applications that integrate data and technology into innovative and affordable tools, provide significant opportunities for venture investment. Corporations worldwide increasingly look to Silicon Valley's culture of entrepreneurship and venture investment to create new models for successful disruptive innovation.",0018-9162;00189162,,10.1109/MC.2014.230,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6879742,Austin Technology Incubator;Internet of Things;Silicon Valley;big data;cloud;data analysis;innovation economy;software-defined networks;venture capital,Companies;Innovation management;Investment;Mobile communication;Technological innovation;Venture capital,Big Data;business data processing;data analysis;innovation management;venture capital,Austin Technology Incubator;Silicon Valley;US;big data analytics;disruptive innovation;disruptive technologies;entrepreneurship;innovation economy;startup portfolio;startup-based economy;strategic business applications;venture capital;venture investment,,0,,,,no,Aug. 2014,,IEEE,IEEE Journals & Magazines
Video security with human identification and tracking,T. Zhang,"Hewlett-Packard Laboratories, India",2014 IEEE International Conference on Multimedia and Expo Workshops (ICMEW),20140908,2014,,,1,6,"With the pervasiveness of monitoring cameras installed in public places, schools, hospitals and homes, video analytics technologies for interpreting the generated video content are becoming more and more relevant to people's lives. Along this context, we develop a human-centric video surveillance system that identifies and tracks people in a given scene. In this paper, a parallel processing pipeline is proposed that integrates image processing modules in the system, such as face detection, person recognition and tracking, efficiently and smoothly, so that multiple people can be simultaneously tracked in real time. Furthermore, significant innovations are involved in this work in making each of the major image analysis modules both fast and robust to variations in pose, illumination, occlusions and so on. A demonstration software has been implemented that supports finding, tagging, identifying and tracking people in live or recorded videos with uncontrolled capturing conditions.",1945-7871;19457871,Electronic:978-1-4799-4717-1; POD:978-1-4799-4716-4,10.1109/ICMEW.2014.6890591,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6890591,face detection;face/head tracking;human identification;video analysis;video parallel computing,Computational modeling;Face;Face detection;Face recognition;Streaming media;Target tracking,object detection;object tracking;parallel programming;security of data;video cameras;video surveillance,generated video content interpretion;human centric video surveillance system;human identification;human tracking;image analysis module;image processing module integration;monitoring camera;parallel processing pipeline;uncontrolled capturing conditions;video analytics technology;video security,,0,,17,,no,14-18 July 2014,,IEEE,IEEE Conference Publications
Vismate: Interactive visual analysis of station-based observation data on climate changes,J. Li; K. Zhang; Z. P. Meng,"School of Computer Science and Technology, Tianjin University, and National Ocean Technology Center, Tianjin, China",2014 IEEE Conference on Visual Analytics Science and Technology (VAST),20150216,2014,,,133,142,"We present a new approach to visualizing the climate data of multi-dimensional, time-series, and geo-related characteristics. Our approach integrates three new highly interrelated visualization techniques, and uses the same input data types as in the traditional model-based analysis methods. As the main visualization view, Global Radial Map is used to identify the overall state of climate changes and provide users with a compact and intuitive view for analyzing spatial and temporal patterns at the same time. Other two visualization techniques, providing complementary views, are specialized in analysing time trend and detecting abnormal cases, which are two important analysis tasks in any climate change study. Case studies and expert reviews have been conducted, through which the effectiveness and scalability of the proposed approach has been confirmed.",,Electronic:978-1-4799-6227-3; POD:978-1-4799-6186-3,10.1109/VAST.2014.7042489,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7042489,Climate changes;radial layout;spatiotemporal visualization;station-based observation data;visual analytics,Data visualization;Image color analysis;Layout;Meteorology;Spatiotemporal phenomena;Time series analysis;Visualization,climatology;data analysis;data visualisation;environmental science computing;interactive systems;time series,Global Radial Map;Vismate;abnormal case detection;climate change;climate data visualization;geo-related characteristics;highly interrelated visualization techniques;input data type;interactive visual analysis;model-based analysis method;multidimensional characteristics;spatial pattern analysis;station-based observation data;temporal pattern analysis;time trend analysis;time-series characteristics;visualization view,,2,,54,,no,25-31 Oct. 2014,,IEEE,IEEE Conference Publications
"Visrad, 3-D target design and radiation simulation code",I. E. Golovkin; J. J. MacFarlane; S. Kulkarni,"Prism Computational Sciences, Inc., Madison, WI 53711, USA",2014 IEEE 41st International Conference on Plasma Sciences (ICOPS) held with 2014 IEEE International Conference on High-Power Particle Beams (BEAMS),20150119,2014,,,1,1,"Summary form only given. The 3-D view factor code VISRAD is widely used in designing HEDP experiments at major laser and pulsed-power facilities, including NIF, OMEGA, OMEGA-EP, ORION, LMJ, Z, and PLX. It simulates target designs by generating a 3-D grid of surface elements, utilizing a variety of 3-D primitives and surface removal algorithms, and can be used to compute the radiation flux throughout the surface element grid by computing element-to-element view factors and solving power balance equations. Target set-up and beam pointing are facilitated by allowing users to specify positions and angular orientations using a variety of coordinates systems (e.g., that of any laser beam, target component, or diagnostic port). Analytic modeling for laser beam spatial profiles for OMEGA DPPs and NIF CPPs is used to compute laser intensity profiles throughout the grid of surface elements. We will discuss recent improvements to the software package and plans for future developments.",0730-9244;07309244,Electronic:978-1-4799-2713-5; POD:978-1-4799-2714-2,10.1109/PLASMA.2014.7012249,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7012249,,Computational modeling;Laser beams;Laser modes;Laser theory;Mathematical model;Power lasers;Solid modeling,plasma diagnostics;plasma production by laser;plasma simulation,3-D grid;3-D primitives;3-D target design;3-D view factor code;HEDP experiments;LMJ;NIF CPP;OMEGA DPP;OMEGA-EP;ORION;PLX;VISRAD;Z;angular orientations;beam pointing;coordinate systems;diagnostic port;element-to-element view factors;laser beam spatial profiles;laser facilities;laser intensity profiles;position orientations;power balance equations;pulsed-power facilities;radiation flux;radiation simulation code;software package;surface element grid;surface removal algorithms;target component;target set-up,,0,,,,no,25-29 May 2014,,IEEE,IEEE Conference Publications
Visual analytical model for educational data,D. A. G. Aguilar; F. J. GarcÌ_a-PeÌ±alvo; R. TherÌ_n,"Dept. de Inf. y Autom., Univ. de Salamanca, Salamanca, Spain",2014 9th Iberian Conference on Information Systems and Technologies (CISTI),20140814,2014,,,1,6,"Current technologies used in learning processes imply the logging of all the performed activities. These data can be exploited to gain insight into the learning process and can be used for the assessment of students, professors and the processes themselves. However, although this wealth of data exists, it is still difficult for the teachers (and interested stakeholders) to verify hypothesis, extract conclusions, or make decisions based on discovered facts or situations. This paper introduces an educational data analysis model based on visual analytics, learning analytics and academic analytics, by means of a software tool that allows performing confirmatory and exploratory data analysis through the interacting with the gathered information from a typical Learning Management System. The main goal thus is to define a model which enable the discovery of knowledge on the specific learning process that, in turn, will permit to improve it.",2166-0727;21660727,Electronic:978-9-8998-4343-1; POD:978-1-4799-6111-5,10.1109/CISTI.2014.6877098,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877098,Educational Data Mining;Feedback;Inferring Social Network;Learning Analytics;Visual Analytics;Visualization;eLearning,Data mining;Electronic learning;Least squares approximations;Social network services;Visual analytics,data analysis;data mining;learning management systems,academic analytics;activity logging;confirmatory data analysis;educational data analysis model;exploratory data analysis;information gathering;knowledge discovery;learning analytics;learning management system;learning process;process assessment;professor assessment;software tool;student assessment;visual analytical model,,0,,48,,no,18-21 June 2014,,IEEE,IEEE Conference Publications
Visual Analytics of Malignant Blood Flow for Medical Professionals,T. Yagi; T. Hoshi; Y. Park,"Center for Adv. Biomed. Sci., Waseda Univ., Tokyo, Japan",2014 IEEE Pacific Visualization Symposium,20140414,2014,,,342,343,"Arterial diseases of sclerosis and aneurysm are known to be related with a local blood flow. There are indeed decades of related studies, recently being with an aid of a computational approach on a patient-specific basis, but those have yet to be translated into clinical medicine. ""Simply why?"", and ""how to achieve it?"" Computational blood flow originates from theoretical and computational fluid dynamics. Understanding the underlying knowledge may be beyond the scope of medical professionals. Engineers have to be fully aware of a barrier on an interface with clinical medicine, and manage to translate the evolving technology with a special focus on balancing the efficacy and safety. Visual analytics is to facilitate to overcome an academic gap between engineering and clinical medicine. This paper deals with the recent development of visual analytics of malignant blood flows in cerebral aneurysms for medical professionals.",2165-8765;21658765,Electronic:978-1-4799-2874-3; POD:978-1-4799-2875-0,10.1109/PacificVis.2014.66,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6787195,aneurysm;blood flow;life science,Aneurysm;Biomedical imaging;Blood flow;Cancer;Software;Vectors;Visual analytics,computational fluid dynamics;data visualisation;diseases;haemodynamics;haemorheology;medical computing,arterial diseases;cerebral aneurysms;clinical medicine;computational fluid dynamics;malignant blood flow;medical professionals;sclerosis;visual analytics,,0,,1,,no,4-7 March 2014,,IEEE,IEEE Conference Publications
Visual learning analytics techniques applied in software engineering subjects,M. ÌÅ. Conde; F. J. GarcÌ_a-PeÌ±alvo; D. A. GÌ_mez-Aguilar; R. Theron,"Deptartment. of Mechanics, Computer Science and Aerospace Engineering., University of Leon, Le&#x00F3;n, Spain",2014 IEEE Frontiers in Education Conference (FIE) Proceedings,20150219,2014,,,1,9,"The technology applied to educational contexts, and specially the learning platforms, provides students and teachers with a set of tools and spaces to carry out the learning processes. Information related to the participation and interaction of these stakeholders with their peers and with the platform is recorded. It would be useful to exploit this information in order to make decisions. However this is a complex activity mainly because of the huge quantity of information stored. This work presents a visual learning analytics system that makes possible the exploitation of that information. The system includes several tools that help to analyze users' interaction attending to different dimensions, such as: when interaction is carried out, which are more important contents for users, how they interact with others, etc. This system has been tested with the subject information recorded during five academic years. From this analysis it is possible to show that visual learning analytics may help to improve educational practices.",0190-5848;01905848,Electronic:978-1-4799-3922-0; POD:978-1-4799-3923-7; USB:978-1-4799-3921-3,10.1109/FIE.2014.7044486,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7044486,Data Mining;LMS;Learning Analytics;Visual Analytics,Analytical models;Context;Data visualization;Least squares approximations;Semantics;Visual analytics,computer science education;data visualisation;learning management systems;software engineering,educational context;learning management systems;software engineering subject;visual learning analytics,,1,,51,,no,22-25 Oct. 2014,,IEEE,IEEE Conference Publications
Visualizing the effects of scale and geography in multivariate comparison,S. Goodwin; J. Dykes; A. Slingsby,"giCentre, City University London",2014 IEEE Conference on Visual Analytics Science and Technology (VAST),20150216,2014,,,251,252,"Our research investigates the sensitivities and complexities of visualizing multivariate data over multiple scales with the consideration of local geography. We investigate this in the context of creating geodemographic classifications, where multivariate comparison for the variable selection process is an important, yet time-consuming and intensive process. We propose a visual interactive approach which allows skewed variables and those with strong correlations to be quickly identified and investigated and the geography of multi-scale correlation to be explored. Our objective is to present comprehensive documentation of the parameter space prior to the development of the visualization tools to help explore it.",,Electronic:978-1-4799-6227-3; POD:978-1-4799-6186-3,10.1109/VAST.2014.7042515,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7042515,D.2.2 [Software Engineering]: Design Tools and Techniques;I.5.2 [Pattern Recognition]: Design Methodology ‰ÛÓ Feature Evaluation and Selection,Correlation;Data visualization;Encoding;Geography;Image color analysis;Input variables;Visualization,data visualisation;demography;feature selection;geography,geodemographic classifications;local geography;multivariate data visualization;variable selection process;visual interactive approach,,1,,13,,no,25-31 Oct. 2014,,IEEE,IEEE Conference Publications
"WAN Optimization Tools, Techniques and Research Issues for Cloud-Based Big Data Analytics",M. B. Nirmala,"Dept. of Comput. Sci., Holy Cross Coll., Trichirappali, India",2014 World Congress on Computing and Communication Technologies,20140403,2014,,,280,285,"Increasing data volumes, data replication at offsite, and the greater than ever use of content-rich and Big Data, applications are mandating IT organizations to optimize their network resources. Trends such as Virtualization and Cloud computing further emphasize this requirement of this current era of Big data. To help with this process, companies are increasingly relying on a new generation of WAN optimization Techniques, Appliances, Controllers, Platforms and Products that are displacing standalone physical appliances by offering more scalability, flexibility, and manageability by additional inclusion of software to handle this Big data and bring valuable insights through big data analytics. An optimized WAN environment can increase network reliability, accessibility and availability and improve cost profiles. It also improves the performance and consistency of data backup, replication, and recovery processes. This paper covers the introduction to WAN optimization, prominent WAN optimization techniques, WAN optimization products used for Big data analytics and finally future trends and research Issues of WAN optimization in the ensuing era of Big data.",,Electronic:978-1-4799-2877-4; POD:978-1-4799-5085-0,10.1109/WCCCT.2014.72,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6755159,Controllers;Devices;Platforms;Solutions for Big data and Big data Analytics;WAN Optimization;WAN Optimization Appliance;WAN Optimization Techniques,Acceleration;Data handling;Data storage systems;Home appliances;Information management;Optimization;Wide area networks,Big Data;computer network reliability;optimisation;wide area networks,IT organizations;WAN optimization products;WAN optimization techniques;WAN optimization tools;cloud computing;cloud-based Big Data analytics;cost profiles;data backup;data recovery process;data replication;data volumes;network accessibility;network availability;network reliability;network resource;standalone physical appliances;virtualization,,0,,24,,no,Feb. 27 2014-March 1 2014,,IEEE,IEEE Conference Publications
Watching the Aliens,S. Mitra-Thakur,,Engineering & Technology,20140904,2014,9,7,28,32,"Big Data analytics has become a powerful tool in monitoring invasive species, with ecologists saying that with the problem of invasive species being on such a large scale, it is essential to analyse lots of data through studying several ecosystems. Supercomputers can help conduct analysis of the terabytes of data available, allowing data to be processed much more quickly. The previouslymentioned GARP software programme, developed at the San Diego Supercomputer Centre, can perform modelling analysis using a genetic algorithm to predict the potential distribution of an invasive species, and these models can be visualised as distribution maps using GIS. The model has been deployed successfully in several studies, for example in 2007 when it predicted that large estuaries including Chesapeake Bay in the US were at risk of invasion of Chinese mitten crab (Eriocheir sinensis). Just one month later crabs were discovered in the estuary's waters.",1750-9637;17509637,,10.1049/et.2014.0704,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6870302,,,Big Data;data analysis;data visualisation;ecology;genetic algorithms;geographic information systems;parallel machines,Big data analytics;GIS;data processed;ecosystems;genetic algorithm;invasive species monitoring;modelling analysis;potential distribution prediction;supercomputers,,0,,,,no,14-Aug,,IET,IET Journals & Magazines
Web resource management and evaluation system based on multi-attribute fusion,Z. Zhu; X. Gao; M. Wang; W. Xu,"The 28 Research Institution of China Electronics Technology Group Corporation, Nanjing 210017, China",Proceedings of the 33rd Chinese Control Conference,20140915,2014,,,6489,6493,"In order to evaluate the services occupying server resources, a comprehensive evaluation of Web system performance model frame has been built and researched in the general. The GAHP and MADM method are separately proposed to evaluate to services health, which have their advantages as same as the dis advantages on multi-attribute resource evaluation. To improve the evaluation quality, we fuse the result of GAHP and MADM by fuzzy method. Resource evaluation of services can be finally sorted. Finally, A simulation result shows feasible of the paper's idea.",,Electronic:978-9-8815-6387-3; POD:978-1-4799-4724-9; USB:978-9-8815-6384-2,10.1109/ChiCC.2014.6896061,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6896061,Fuzzy fusion;GAHP;MDAM;Web system;resource manage and evaluation,Analytic hierarchy process;Bandwidth;Monitoring;Resource management;Servers;Vectors,Web services;analytic hierarchy process;fuzzy set theory;software performance evaluation,GAHP method;MADM method;Web resource management and evaluation system;Web system performance model frame;comprehensive evaluation;evaluation quality;fuzzy method;multiattribute fusion;multiattribute resource evaluation;server resources,,0,,10,,no,28-30 July 2014,,IEEE,IEEE Conference Publications
Web Service Recommendation via Exploiting Location and QoS Information,X. Chen; Z. Zheng; Q. Yu; M. R. Lyu,"Shenzhen Key Laboratory of Rich Media Big Data Analytics and Applications, Shenzhen Research Institute, The Chinese University of Hong Kong",IEEE Transactions on Parallel and Distributed Systems,20140610,2014,25,7,1913,1924,"Web services are integrated software components for the support of interoperable machine-to-machine interaction over a network. Web services have been widely employed for building service-oriented applications in both industry and academia in recent years. The number of publicly available Web services is steadily increasing on the Internet. However, this proliferation makes it hard for a user to select a proper Web service among a large amount of service candidates. An inappropriate service selection may cause many problems (e.g., ill-suited performance) to the resulting applications. In this paper, we propose a novel collaborative filtering-based Web service recommender system to help users select services with optimal Quality-of-Service (QoS) performance. Our recommender system employs the location information and QoS values to cluster users and services, and makes personalized service recommendation for users based on the clustering results. Compared with existing service recommendation methods, our approach achieves considerable improvement on the recommendation accuracy. Comprehensive experiments are conducted involving more than 1.5 million QoS records of real-world Web services to demonstrate the effectiveness of our approach.",1045-9219;10459219,,10.1109/TPDS.2013.308,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6684151,Web service;collaborative filtering;quality of service (QoS);recommendation,Algorithm design and analysis;Collaboration;Prediction algorithms;Quality of service;Recommender systems;Time factors;Web services,Web services;collaborative filtering;quality of service,Internet;QoS information;Web service recommendation;collaborative filtering;integrated software component;interoperable machine-to-machine interaction;location information;quality-of-service;service-oriented application,,16,,38,,no,14-Jul,,IEEE,IEEE Journals & Magazines
Web-based learning object selection software using analytical hierarchy process,T. Yigit; A. H. Isik; M. Ince,"Dept. of Comput. Eng., Univ. of Suleyman Demirel, Isparta, Turkey",IET Software,20140811,2014,8,4,174,183,"The concepts of sustainability and reusability have great importance in engineering education. In this context, metadata provides reusability and the effective use of Learning Objects (LOs). In addition, searching the huge LO Repository with metadata requires too much time. If the selection criteria do not exactly match the metadata values, it is not possible to find the most appropriate LO. When this situation arises, the multi-criteria decision making (MCDM) method can meet the requirements. In this study, the SDUNESA software was developed and this software allows for the selection of a suitable LO from the repository by using an analytical hierarchy process MCDM method. This web-based SDUNESA software is also used to store, share and select a suitable LO in the repository. To meet these features, the SDUNESA software contains Web 2.0 technologies such as AJAX, XML and SOA Web Services. The SDUNESA software was especially developed for computer engineering education. Instructors can use this software to select LOs with defined criteria. The parameters of the web-based SDUNESA learning object selection software that use the AHP method are defined under the computer education priorities. The obtained results show that the AHP method selects the most reliable learning object that meets the criteria.",1751-8806;17518806,,10.1049/iet-sen.2013.0116,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6871464,,,Internet;analytic hierarchy process;computer aided instruction;engineering education;meta data;service-oriented architecture,AHP method;AJAX;LO;SOA Web services;Web 2.0 technologies;Web-based SDUNESA learning object selection software;XML;analytical hierarchy process;computer engineering education;metadata values;multicriteria decision making method,,0,,,,no,14-Aug,,IET,IET Journals & Magazines
"Wikis, semantics, and collaboration: Symposium on collaboration analysis and reasoning systems, at the 2014 conference on collaboration technologies and systems",M. Greaves,"Nat. Security Directorate, Pacific Northwest Nat. Lab., Seattle, WA, USA",2014 International Conference on Collaboration Technologies and Systems (CTS),20140731,2014,,,469,471,"Designing software for collaborative sensemaking environments begins with a set of very challenging requirements. At a high level, the software needs to be flexible enough to support multiple lines of inquiry, contradictory hypotheses, and collaborative tasking by multiple analysts. It should also include support for managing evolving human/machine workflows and analytic products at various levels of strictness and formality, processing partial and ambiguous evidence arriving in streams, and developing explanatory scenarios based on both serendipitous and structured discovery. Eventually, it should support the analytic team as they evaluate multiple alternatives and converge on one or more consensus responses, while preserving the history and underlying reasoning. Finally, it should be delightful and simple to use, not require an inordinate degree of precision and exactness, and be quickly and inexpensively deployable in a variety of rapid-response analytic situations. It has not been possible thus far to create a single software architecture that adequately balances all these goals. However, we can shed useful light on this problem by looking at the experience of semantic wiki architectures: an emerging class of software that blends wikis, databases, social tagging systems, and Semantic Web representations.",,CD-ROM:978-1-4799-5156-7; Electronic:978-1-4799-5158-1; POD:978-1-4799-5159-8,10.1109/CTS.2014.6867607,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6867607,,Collaboration;Electronic publishing;Encyclopedias;Semantic Web;Semantics,Web sites;inference mechanisms;semantic Web;social networking (online);software architecture,collaboration analysis;collaborative sensemaking environments;databases;rapid-response analytic situations;reasoning systems;semantic Web representations;semantic Wiki architectures;social tagging systems;software architecture;software design,,1,,7,,no,19-23 May 2014,,IEEE,IEEE Conference Publications
XDB - A Novel Database Architecture for Data Analytics as a Service,C. Binnig; A. Salama; E. Zamanian; H. Kornmayer; S. Listing; A. C. Mueller,"Baden-Wuerttemberg Cooperative State Univ. Mannheim, Mannheim, Germany",2014 IEEE International Congress on Big Data,20140925,2014,,,96,103,"Parallel shared-nothing database systems are major platforms for efficiently analyzing large amounts of structured data. However, in order to offer SQL-like services for data analytics in the cloud, providers such as Amazon and Google do not use these systems as a basis. A major reason for this trend is that existing parallel shared-nothing database systems are expensive and that they do not fulfill many of the requirements such as elasticity and fault-tolerance needed for providing a service for data analytics in the cloud. In this paper, we present an overview of an elastic and fault-tolerant database system called XDB, which supports complex analytics. XDB builds on the following novel concepts: (1) a partitioning scheme that supports elasticity with regard to data and queries, (2) a cost-based fault-tolerance scheme that allows to recover from mid-query faults, and (3) adaptive parallelization techniques to better support complex analytical queries. XDB is implemented using a middleware approach on top of multiple nodes each hosting an instance of a single node database system (MySQL in our prototype). Initial experiments show that our novel concepts effectively support elasticity, fault-tolerance and complex analytics when compared to the traditional behavior of existing databases.",2379-7703;23797703,Electronic:978-1-4799-5057-7; POD:978-1-4799-5058-4,10.1109/BigData.Congress.2014.23,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6906766,,Elasticity;Fault tolerance;Fault tolerant systems;Middleware;Query processing,SQL;cloud computing;middleware;parallel databases;query processing;software architecture,MySQL;SQL-like services;XDB;adaptive parallelization techniques;cloud computing;complex analytical queries;complex data analytics;cost-based fault-tolerance scheme;database architecture;elastic fault-tolerant database;mid-query fault recovery;middleware approach;multiple nodes;parallel shared-nothing database systems;partitioning scheme;query processing;single-node database system;structured data analysis,,0,,18,,no,June 27 2014-July 2 2014,,IEEE,IEEE Conference Publications