Authors,Document Title,Year,Source title,Volume,Issue,Art. No.,Page start,Page end,label,Other predictin,Page count,Cited by,DOI,PDF Link,Affiliations,Authors with affiliations,Abstract,Author Keywords,Document Type,Source,EID
"Zimmermann S., MÌ_ller M., Heinrich B.",Exposing and selling the use of web services‰ÛÓan option to be considered in make-or-buy decision-making,2016,Decision Support Systems,89,,,28,40,no,,,,10.1016/j.dss.2016.06.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978906907&doi=10.1016%2fj.dss.2016.06.006&partnerID=40&md5=7061da2bb1078f1981731d4e1a7b5c4a,"Department of Information Systems, Production and Logistics Management, University of Innsbruck, UniversitÌ_tsstraÌÙe 15, Innsbruck, Austria; Department of Management Information Systems, University of Regensburg, UniversitÌ_tsstraÌÙe 31, Regensburg, Germany","Zimmermann, S., Department of Information Systems, Production and Logistics Management, University of Innsbruck, UniversitÌ_tsstraÌÙe 15, Innsbruck, Austria; MÌ_ller, M., Department of Information Systems, Production and Logistics Management, University of Innsbruck, UniversitÌ_tsstraÌÙe 15, Innsbruck, Austria; Heinrich, B., Department of Management Information Systems, University of Regensburg, UniversitÌ_tsstraÌÙe 31, Regensburg, Germany","The emergence of the web service market has enabled firms to choose between developing web services internally and purchasing them externally from web service providers. In general, these so-called make-or-buy decisions have been the object of intense debate in the IT outsourcing literature. However, characteristics of web services such as loose coupling and the current trend of digitalizing application interfaces enable new opportunities especially to non-software firms: when a firm decides to develop a web service internally (make-decision), it has the option of exposing and selling the use of this web service after its internal development (sell option). In this paper, we propose a normative approach for the valuation of such a sell option based on real option theory, taking into account the characteristics of web services. This approach enhances traditional make-or-buy approaches by additionally considering this sell option in decision-making. Our results are twofold: (I) the sell option has considerable impact on traditional make-or-buy decisions and makes the internal development of web services more attractive; (II) it is preferable to execute the sell option as soon as possible after completion of the internal development of the web service. å© 2016 Elsevier B.V.",Make-or-buy decisions; Real option theory; Sell option; Web service market; Web services,Article,Scopus,2-s2.0-84978906907
"Zikos D., Ostwal D.",A Platform based on Multiple Regression to Estimate the Effect of in-Hospital Events on Total Charges,2016,"Proceedings - 2016 IEEE International Conference on Healthcare Informatics, ICHI 2016",,,7776391,403,408,no,,,,10.1109/ICHI.2016.72,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010304282&doi=10.1109%2fICHI.2016.72&partnerID=40&md5=73f5edabadef221db83ba4cc893359d1,"Department of Health Administration, Central Michigan University, Mt. Pleasant, MI, United States; Computer Science and Engineering Department, University of Texas at Arlington, Arlington, TX, United States","Zikos, D., Department of Health Administration, Central Michigan University, Mt. Pleasant, MI, United States; Ostwal, D., Computer Science and Engineering Department, University of Texas at Arlington, Arlington, TX, United States","Recently hospitals struggle to control the cost of care while maintaining optimal outcomes. To respond to this challenge, we developed an interactive web platform which utilizes a multiple linear regression model. The user can create and furthermore alter a clinical scenario, during a patient hospitalization to see the dynamic prediction of total charges, via interactive sessions. The R2 value of our model is 0.655 and the standard error of the estimate is 38,732. Predictors with high coefficient scores include the cardioverter implantation, mechanical ventilation, implant of pulsation balloon and hospital-acquired conditions such as staphylococcus aureus septicemia. Our findings indicate that (a) integration of predictive models into clinical decision support systems is feasible and use of regression methods provide direct feedback on the effect of any clinical practice to the in-hospital charges (b) medical claims data can provide a useful estimation of the in-hospital charges (c) hospital acquired conditions have significant impact on the in-hospital charges. å© 2016 IEEE.",decision making; multiple linear regression; prediction; total charges,Conference Paper,Scopus,2-s2.0-85010304282
"Zhu B., Yu L.-A., Geng Z.-Q.",Cost estimation method based on parallel Monte Carlo simulation and market investigation for engineering construction project,2016,Cluster Computing,19,3,,1293,1308,no,,,,10.1007/s10586-016-0585-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976407611&doi=10.1007%2fs10586-016-0585-6&partnerID=40&md5=aa70248dbc6720a8f3137e2ad8e31f92,"College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China","Zhu, B., College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; Yu, L.-A., College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; Geng, Z.-Q., College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China","In this paper, a new cost estimation method based on parallel Monte Carlo simulation and market investigation for the chemical engineering construction project is proposed to consider both the uncertainties of cost estimation and market drivers. The critical items of exerting large impacts on the cost estimation are selected by the market investigation. Then important critical items are chosen by the sensitivity analysis based on the parallel Monte-Carlo simulation combining with the Likert scale method from critical items. The Relative Important Indices and Normalized Important Indices are obtained according to the discipline and procurement experts‰Ûª experience in the relative construction market. Then re-rankings of market drivers will be acted as guidelines for carrying out project cost simulations based on the parallel Monte-Carlo method, with inquired information of important critical items with more efficiency. An illustrative example in a petrochemical Engineering Procurement Construction contracting project in Saudi verified the validity and practicability of the proposed method. å© 2016, Springer Science+Business Media New York.",Likert scale; Market drivers; Parallel Monte-Carlo simulation; Relative importance indices,Article,Scopus,2-s2.0-84976407611
"Zhou Z., Goh Y.M., Shen L.",Overview and Analysis of Ontology Studies Supporting Development of the Construction Industry,2016,Journal of Computing in Civil Engineering,30,6,4016026,,,no,,,1,10.1061/(ASCE)CP.1943-5487.0000594,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992217086&doi=10.1061%2f%28ASCE%29CP.1943-5487.0000594&partnerID=40&md5=5ec5f002e31865699146067289d621d8,"Dept. of Management Science and Engineering, College of Economics and Management, Nanjing Univ. of Aeronautics and Astronautics, Nanjing, China; Dept. of Building, School of Design and Environment, National Univ. of Singapore, Singapore, Singapore","Zhou, Z., Dept. of Management Science and Engineering, College of Economics and Management, Nanjing Univ. of Aeronautics and Astronautics, Nanjing, China; Goh, Y.M., Dept. of Building, School of Design and Environment, National Univ. of Singapore, Singapore, Singapore; Shen, L., Dept. of Building, School of Design and Environment, National Univ. of Singapore, Singapore, Singapore","Being information-intensive, the construction industry has the feature of multiagents, including multiparticipants from different disciplines, multiprocesses with a long-span timeline, and multidocuments generated by various systems. The multistakeholder context of the construction industry creates problems such as poor information interoperability and low productivity arising from difficulties in information reuse. Many researchers have explored the use of ontology to address these issues. This study aims to review ontology research to explore its trends, gaps, and opportunities in the construction industry. A systematic process employing three-phase search method, objective analysis and subjective analysis, helps to provide enough potential articles related to construction ontology research, and to reduce arbitrariness and subjectivity involved in research topic analysis. As a result, three main research topics aligned with the ontology development lifecycle were derived as follows: information integration based on ontology, ontology building, and ontology application. In addition, research gaps and the corresponding research agenda were identified. This detailed review provides the basis for further studies on the use of ontology in the construction industry. The research trends and gaps can serve as motivation for researchers and practitioners to work on the next generation of ontology studies to support the development of the construction industry. å© 2016 American Society of Civil Engineers.",Cluster analysis; Construction industry; Information interoperability; Knowledge representation; Ontology; Semantic web; Word cloud,Article,Scopus,2-s2.0-84992217086
"Zhou X., Xue X., Fan Y., Ren J.",I/Q imbalance estimation in OFDM systems,2015,"Proceedings - 2015 IEEE 11th International Conference on ASIC, ASICON 2015",,,7517192,,,no,,,,10.1109/ASICON.2015.7517192,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982244598&doi=10.1109%2fASICON.2015.7517192&partnerID=40&md5=7562924d94c46cf808f08f8d5d7af6a7,"State-Key Laboratory of ASIC and System, Fudan University, Shanghai, China","Zhou, X., State-Key Laboratory of ASIC and System, Fudan University, Shanghai, China; Xue, X., State-Key Laboratory of ASIC and System, Fudan University, Shanghai, China; Fan, Y., State-Key Laboratory of ASIC and System, Fudan University, Shanghai, China; Ren, J., State-Key Laboratory of ASIC and System, Fudan University, Shanghai, China","The paper proposes a new algorithm for I/Q imbalance estimation of multi-band orthogonal frequency division multiplexing (MB-OFDM) Ultra-wideband (UWB) systems in the presence of Carrier Frequency Offset (CFO) and Sample Frequency Offset (SFO). The proposed algorithm has a low complexity and is robust to both Frequency Dependent I/Q Mismatch (F-D I/Q-M) and Frequency Independent I/Q Mismatch (F-I I/Q-M). Through numerical simulation, the algorithem achieves a 9% lower PER compared to non-compensation condition in 10dB SNR condition and is shown to outperform conventional appoaches. å© 2015 IEEE.",,Conference Paper,Scopus,2-s2.0-84982244598
"Zhou J., Fan J., Jia J.",A cost-efficient resource provisioning algorithm for DHT-based cloud storage systems,2016,Concurrency Computation ,28,18,,4485,4506,no,,,,10.1002/cpe.3795,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959185906&doi=10.1002%2fcpe.3795&partnerID=40&md5=e8acf5c26898a164551c905fffc06545,"School of Computer Science and Technology, Soochow University, Suzhou, China","Zhou, J., School of Computer Science and Technology, Soochow University, Suzhou, China; Fan, J., School of Computer Science and Technology, Soochow University, Suzhou, China; Jia, J., School of Computer Science and Technology, Soochow University, Suzhou, China","Personal cloud storage provides users with convenient data access services. Service providers build distributed storage systems by utilizing cloud resources with distributed hash table (DHT), so as to enhance system scalability. Efficient resource provisioning could not only guarantee service performance, but help providers to save cost. However, the interactions among servers in a DHT-based cloud storage system depend on the routing process, which makes its execution logic more complicated than traditional multi-tier applications. In addition, production data centers often comprise heterogeneous machines with different capacities. Few studies have fully considered the heterogeneity of cloud resources, which brings new challenges to resource provisioning. To address these challenges, this paper presents a novel resource provisioning model for service providers. The model utilizes queuing network for analysis of both service performance and cost estimation. Then, the problem is defined as a cost optimization with performance constraints. We propose a cost-efficient algorithm to decompose the original problem into a sub-optimization one. Furthermore, we implement a prototype system on top of an infrastructure platform built with OpenStack. It has been deployed in our campus network. Based on real-world traces collected from our system and Dropbox, we validate the efficiency of our proposed algorithms by extensive experiments. Copyright å© 2016 John Wiley & Sons, Ltd. Copyright å© 2016 John Wiley & Sons, Ltd.",cloud storage; data access services; queuing network; resource provisioning,Article,Scopus,2-s2.0-84959185906
"Zhou G., Bastani F., Zhu W., Yen I.-L.",A self-stabilizing algorithm for the foraging problem in swarm robotic systems,2016,IEEE International Conference on Intelligent Robots and Systems,2016-November,,7759450,2907,2912,no,,,,10.1109/IROS.2016.7759450,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006455451&doi=10.1109%2fIROS.2016.7759450&partnerID=40&md5=ae038ebacd32277e2c8507ae5b33c679,"Department of Computer Science, University of Texas at Dallas, Richardson, TX, United States","Zhou, G., Department of Computer Science, University of Texas at Dallas, Richardson, TX, United States; Bastani, F., Department of Computer Science, University of Texas at Dallas, Richardson, TX, United States; Zhu, W., Department of Computer Science, University of Texas at Dallas, Richardson, TX, United States; Yen, I.-L., Department of Computer Science, University of Texas at Dallas, Richardson, TX, United States","The foraging problem, which evolved from ants finding food and delivering them to the nest, is for a swarm of robots to transport objects from a source to a destination. To avoid collision of robots or congestions, object transportation is generally conducted in a pipelining manner. Existing solutions towards the foraging problem either have performance problems or cannot tolerate failures. We present a self-stabilizing solution for the swarm robotic system to form a pipeline structure to transport objects and achieve the foraging task. In this paper, we first define the stable state for the swarm, which includes two requirements: (I) the robots operate in non-overlapping regions, i.e., transport objects in a pipeline structure and (2) the transportation rate of the system is optimal. Then, we introduce our self-stabilizing algorithm for the foraging problem and prove its convergence and the correctness of its convergence properties. Due to the self-stabilization nature, our solution is decentralized and fault tolerant. The swarm can achieve the foraging task as long as there is at least one working robot. Due to the definition of the stable state, our swarm system, when converged, can achieve optimal performance. We conduct simulations to evaluate the effectiveness of our algorithm, and the experimental results show that from any state, our algorithm can converge very quickly to reach the stable state and provide optimal performance for object transportation. å© 2016 IEEE.",Foraging problem; Object transportation; Self-stabilization; Swarm robotic systems; Task partitioning,Conference Paper,Scopus,2-s2.0-85006455451
"Zheng Y.-J., Zhang B., Xue J.-Y.",Selection of key software components for formal development using water wave optimization,2016,Ruan Jian Xue Bao/Journal of Software,27,4,,933,942,no,,,,10.13328/j.cnki.jos.004964,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84965172531&doi=10.13328%2fj.cnki.jos.004964&partnerID=40&md5=52c611989710f32aaf30a6dbf84f62a4,"College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; Jiangxi Provincial Key Laboratory of High Performance Computing, Jiangxi Normal University, Nanchang, China","Zheng, Y.-J., College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China, Jiangxi Provincial Key Laboratory of High Performance Computing, Jiangxi Normal University, Nanchang, China; Zhang, B., College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; Xue, J.-Y., Jiangxi Provincial Key Laboratory of High Performance Computing, Jiangxi Normal University, Nanchang, China","Formal methods contribute to the fundamental improvement of software quality and reliability, but this methodology is often very expensive. A compromise is to select and apply formal methods to only a subset of key components of the software system. However, currently there are few effective approaches for such selection process. This paper proposes a 0-1 constrained programming model for selecting key components for formal development, which enables the use of metaheuristic search methods to effectively solve the selection problem. The paper also designs a discrete water wave optimization (WWO) algorithm for the problem. The application to a large-scale software system validates the effectiveness of the proposed problem model, and demonstrates that the WWO algorithm outperforms some other typical metaheuristic search methods. å© Copyright 2016, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Formal method; Metaheuristic search method; Reliability; Water wave optimization (WWO),Article,Scopus,2-s2.0-84965172531
Zheng Y.,Estimating quality of the construction engineering project using fuzzy AHP approach,2016,"Proceedings - 2016 International Conference on Smart Grid and Electrical Automation, ICSGEA 2016",,,7733807,98,101,no,,,,10.1109/ICSGEA.2016.65,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007001309&doi=10.1109%2fICSGEA.2016.65&partnerID=40&md5=cb0272d757cceb996165b7546424a51e,"Tibet Vocational Technical College, Lhasa, China","Zheng, Y., Tibet Vocational Technical College, Lhasa, China","As improving the quality of the construction engineering project is very important for construction industry, in this paper, we propose a fuzzy AHP based approach to evaluate the construction engineering project quality. To cover all influencing factors of the proposed problem, three parts are contained, including 1) Building safety performance, 2) Building utilization function and 3) Construction environment quality. Afterwards, we utilize fuzzy numbers in fuzzy AHP to replace the exact numbers in AHP, and then present a fuzzy AHP based construction engineering project quality evaluation approach. In the end, we design an experiment to test the performance of the proposed approach. Particularly, for each project, we construct a vector with 16 dimensions, in which each dimension is corresponding to an index in the index system. å© 2016 IEEE.",Construction engineering project; Fuzzy analytic hierarchy process; Index system; Triangular fuzzy number,Conference Paper,Scopus,2-s2.0-85007001309
"Zhao Q., Li Y., Hei X., Wang X.",Toward automatic calculation of construction quantity based on building information modeling,2015,"Proceedings - 2015 11th International Conference on Computational Intelligence and Security, CIS 2015",,,7397136,482,485,no,,,,10.1109/CIS.2015.122,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964426456&doi=10.1109%2fCIS.2015.122&partnerID=40&md5=f6954fcfa16132261f1f995566c31b03,"School of Civil Engineering and Architecture, Xi'an University of Technology, Xi'an, China; School of Computer Science and Engineering, Xi'an University of Technology, Xi'an, China","Zhao, Q., School of Civil Engineering and Architecture, Xi'an University of Technology, Xi'an, China; Li, Y., School of Civil Engineering and Architecture, Xi'an University of Technology, Xi'an, China; Hei, X., School of Computer Science and Engineering, Xi'an University of Technology, Xi'an, China; Wang, X., School of Civil Engineering and Architecture, Xi'an University of Technology, Xi'an, China","The application of BIM (Building Information Modeling) technology in practical engineering becomes more and more frequent. The current research on quantity calculation based on BIM discusses the cost estimation of the building projects. There are few methods that can query construction quantity during the design process which facilitates the designer to modify the relevant data at any time. In view of the present situation, this paper describes a calculation construction method on Revit platform, which is one of currently the most popular BIM modeling platform. The effectiveness and disadvantages of the approach are analyzed through a practical engineering example. With this method, designers can get real-Time quantity in the design process in any time, and the design results can be checked and updated automatically based on statistics result of engineering quantity when necessary. å© 2015 IEEE.",BIM; Engineering quantity calculation; Revit,Conference Paper,Scopus,2-s2.0-84964426456
"Zhao H., Huang W., Wang S., Liu T.",Intelligent Cost Estimation Method for Large-Scale Projects Based on PCA Optimized RBF Neural Networks,2016,"Proceedings - 2015 8th International Symposium on Computational Intelligence and Design, ISCID 2015",1,,7469008,525,528,no,,,,10.1109/ISCID.2015.146,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978175172&doi=10.1109%2fISCID.2015.146&partnerID=40&md5=4485e4c56959c5b404f4eb35e4a9cb50,"School of Management, Qingdao University of Technology, Qingdao, China; School of Civil Engineering, Yancheng Institute of Technology, Yancheng, China","Zhao, H., School of Management, Qingdao University of Technology, Qingdao, China; Huang, W., School of Management, Qingdao University of Technology, Qingdao, China; Wang, S., School of Management, Qingdao University of Technology, Qingdao, China; Liu, T., School of Civil Engineering, Yancheng Institute of Technology, Yancheng, China","In this paper, we break down the large-scale project into small units so that we can extract the parameters that relates most to the total cost more easily. However, the amount of these parameters are huge, causing much trouble for RBF neural fitting, so we need to reduce the number of these parameters by using PCA. Then we put the newly generated parameters as the inputs of RBF neural networks. The results of the empirical analysis showed a sound verification that the proposed cost estimation method is worthy of expanding. å© 2015 IEEE.",Cost estimation; Engineering breakdown; Estimation accuracy; Large-scale project; Neural network; PCA,Conference Paper,Scopus,2-s2.0-84978175172
"Zhao C., Dong C., Zhang X.",EM3B2 ‰ÛÒ a semantic integration engine for materials science,2016,Program,50,1,,58,82,no,,,,10.1108/PROG-01-2015-0004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84953314944&doi=10.1108%2fPROG-01-2015-0004&partnerID=40&md5=3c5939babb4113ce7a3defc8ae6b68de,"School of Information Science and Engineering, University of Science and Technology, Beijing, China; UNIVERSITY OF SCIENCE & TECHNOLOGY BEIJING, Beijing, China; Hebei University of Science and Technology, Shijiazhuang, China","Zhao, C., School of Information Science and Engineering, University of Science and Technology, Beijing, China; Dong, C., UNIVERSITY OF SCIENCE & TECHNOLOGY BEIJING, Beijing, China; Zhang, X., Hebei University of Science and Technology, Shijiazhuang, China","Purpose ‰ÛÒ The integration and retrieval of the vast data have attracted sufficient attention, thus the W3C workgroup releases R2RML to standardize the transformation from relational data to semantic-aware data. However, it only provides a data transform mechanism to resource description framework (RDF). The generation of mapping alignments still needs manual work or other algorithms. Therefore, the purpose of this paper is to propose a domain-oriented automatic mapping method and an application of the R2RML standard. Design/methodology/approach ‰ÛÒ In this paper, materials science is focussed to show an example of domain-oriented mapping. source field concept and M3B2 (Metal Materials Mapping Background Base) knowledge bases are established to support the auto-recommending algorithm. As for the generation of RDF files, the idea is to generate the triples and the links, respectively. The links of the triples follow the object-subject relationship, and the links of the object properties can be achieved by the range individuals and the trail path. Findings ‰ÛÒ Consequently based on the previous work, the authors proposed Engine for Metal Materials Mapping Background Base (EM3B2), a semantic integration engine for materials science. EM3B2 not only offers friendly graphical interfaces, but also provides auto-recommending mapping based on materials knowledge to enable users to avoid vast manually work. The experimental result indicates that EM3B2 supplies accurate mapping. Moreover, the running time of E3MB2 is also competitive as classical methods. Originality/value ‰ÛÒ This paper proposed EM3B2 semantic integration engine, which contributes to the relational database-to-RDF mapping by the application of W3C R2RML standard and the domain-oriented mapping. å© 2016, Emerald Group Publishing Limited.",Auto mapping; Knowledge base; Metal materials; R2RML; Semantic Web,Article,Scopus,2-s2.0-84953314944
"Zhang Y., Gupta R.K., Bernard A.",Two-dimensional placement optimization for multi-parts production in additive manufacturing,2016,Robotics and Computer-Integrated Manufacturing,38,,,102,117,no,,,,10.1016/j.rcim.2015.11.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947283087&doi=10.1016%2fj.rcim.2015.11.003&partnerID=40&md5=1aeea2623e4e107580b74d3056dc0c00,"IRCCyN, Ecole Centrale de Nantes, Nantes, France","Zhang, Y., IRCCyN, Ecole Centrale de Nantes, Nantes, France; Gupta, R.K., IRCCyN, Ecole Centrale de Nantes, Nantes, France; Bernard, A., IRCCyN, Ecole Centrale de Nantes, Nantes, France","Additive Manufacturing (AM) processes build parts in a layer by layer manner. This unique characteristic enables AM machines to fabricate different parts simultaneously without using tools or fixtures. However, how to optimally place multi-parts (with same or different geometries) into a specified build space or onto the build platform with respect to user-defined objectives is a complex NP-hard problem. This problem is a special variant of classical nesting or packing problems. Moreover it owns specific constraints from AM. In this paper, the multi-parts placement problem in AM is analyzed and an integrated strategy is proposed to solve one category of the problem, two-dimensional placement of multi-parts. The proposed strategy is composed of two main steps, Applying an ""AM feature-based orientation optimization method"" to optimize each part's build orientation to guarantee the production quality and Applying a designed ""parallel nesting"" algorithm for increasing the compactness of placement by using the parts' projection profiles so as to decrease the total build time and cost. Computational examples are presented in the end for demonstration. å© 2015 Elsevier Ltd. All rights reserved.",Additive manufacturing; Multi-parts production; Nesting/Packing; Orientation optimization; Placement optimization,Article,Scopus,2-s2.0-84947283087
"Zhang W., Duan P., Xie X., Xia F., Lu Q., Liu X., Zhou J.",QoS4IVSaaS: a QoS management framework for intelligent video surveillance as a service,2016,Personal and Ubiquitous Computing,20,5,,795,808,no,,,1,10.1007/s00779-016-0945-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982255180&doi=10.1007%2fs00779-016-0945-5&partnerID=40&md5=305526c5cb503190c3a6a6e1cbba9b8e,"School of Computer and Communication Engineering, China University of Petroleum, Qingdao, China; Science and Technology on Optical Radiation Laboratory, No. 52 Yongding Road, Beijing, China; Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province, School of Software, Dalian University of Technology, Dalian, China; Faculty of Information Technology and Electrical Engineering, University of Oulu, P.O. BOX 8000, Oulu, Finland","Zhang, W., School of Computer and Communication Engineering, China University of Petroleum, Qingdao, China; Duan, P., School of Computer and Communication Engineering, China University of Petroleum, Qingdao, China; Xie, X., Science and Technology on Optical Radiation Laboratory, No. 52 Yongding Road, Beijing, China; Xia, F., Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province, School of Software, Dalian University of Technology, Dalian, China; Lu, Q., School of Computer and Communication Engineering, China University of Petroleum, Qingdao, China; Liu, X., School of Computer and Communication Engineering, China University of Petroleum, Qingdao, China; Zhou, J., Faculty of Information Technology and Electrical Engineering, University of Oulu, P.O. BOX 8000, Oulu, Finland","Quality of service (QoS) is critical for real-time intelligent video surveillance as a service (IVSaaS) platform, which is both computation intensive and data intensive by nature. However, there is scarce work on a QoS framework for IVSaaS platform. In this paper, we propose QoS for intelligent video surveillance as a service, a QoS framework to make computing resources highly available. In the framework, multiple metrics such as throughput, loads of CPU/GPU, memory and IO are taken into account with different time series models to enhance the adaptivity of different video services. A model selection algorithm is proposed to choose the model that achieves the best performance under various error indicators. At the same time, a resource abnormality detection algorithm is designed to detect anomalies when a service is underperformed. Evaluation results show that the proposed QoS framework can successfully ensure QoS by dynamically scheduling computing resources. Ìâå© 2016, Springer-Verlag London.",Anomaly detection; CPU-GPU collaboration; QoS; Stream processing,Article,Scopus,2-s2.0-84982255180
"Zhang P., Yang K., Dou Y., Jiang J.",Scenario-based approach for project portfolio selection in army engineering and manufacturing development,2016,Journal of Systems Engineering and Electronics,27,1,7424932,166,176,no,,,,10.1109/JSEE.2016.00016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960960043&doi=10.1109%2fJSEE.2016.00016&partnerID=40&md5=1eaaf556ae19c45b2aa0efaf92ba2773,"College of Information System and Management, National University of Defense Technology, Changsha, China","Zhang, P., College of Information System and Management, National University of Defense Technology, Changsha, China; Yang, K., College of Information System and Management, National University of Defense Technology, Changsha, China; Dou, Y., College of Information System and Management, National University of Defense Technology, Changsha, China; Jiang, J., College of Information System and Management, National University of Defense Technology, Changsha, China","The decisions concerning portfolio selection for army engineering and manufacturing development projects determine the benefit of those projects to the country concerned. Projects are typically selected based on ex ante estimates of future return values, which are usually difficult to specify or only generated after project launch. A scenario-based approach is presented here to address the problem of selecting a project portfolio under incomplete scenario information and interdependency constraints. In the first stage, the relevant dominance concepts of scenario analysis are studied to handle the incomplete information. Then, a scenario-based programming approach is proposed to handle the interdependencies to obtain the projects, whose return values are multi-criteria with interval data. Finally, an illustrative example of army engineering and manufacturing development shows the feasibility and advantages of the scenario-based multi-objective programming approach. å© 2016 Beijing Institute of Aerospace Information.",interdependence group decision making; portfolio decision analysis; project portfolio selection; scenario-based,Article,Scopus,2-s2.0-84960960043
"Zhang J., Lei H.",Research status and prospect of Internetware reliability,2015,"Proceedings of 2015 4th International Conference on Computer Science and Network Technology, ICCSNT 2015",,,7490779,406,411,no,,,,10.1109/ICCSNT.2015.7490779,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979243061&doi=10.1109%2fICCSNT.2015.7490779&partnerID=40&md5=cf20f2637f334c75fe22e7038af81e26,"School of Computer Science and Technology, University of Electronic and Science Technology of China, Chengdu, China; Panzhihua University, Panzhihua, China; School of Information and Software Engineering, University of Electronic and Science Technology of China, Chengdu, China","Zhang, J., School of Computer Science and Technology, University of Electronic and Science Technology of China, Chengdu, China, Panzhihua University, Panzhihua, China; Lei, H., School of Information and Software Engineering, University of Electronic and Science Technology of China, Chengdu, China","There are some limitations when analyzing and studying Internetware by using traditional software reliability theory and method which is not adaptive to the open, dynamic and changeable network environment. Based on the basic principle, the characteristics and the analysis of Internetware, the theory and the technology of Internetware reliability are innovated and developed. The research status, the theoretical basis, the research emphasis and the development of Internetware are analyzed and studied. The key researches are Internetware reliability model and reliability computation method. The new research direction and research thinking of scientific analysis of Internet influencing factors, automatic extraction method, specialized computing and parallel computing for reliability are proposed. Internetware reliability analysis and research content are discussed from theory, technology and application, which has reference value for continuous research and further application in Internetware. å© 2015 IEEE.",Internetware; prospect; reliability; status; technology; theory,Conference Paper,Scopus,2-s2.0-84979243061
"Zhang J.-L., Yuan J.-F., Wan J., Mao J., Zhu L.-T., Zhou L., Jiang C.-F., Di P., Wang J.",Efficient parallel implementation of incompressible pipe flow algorithm based on SIMPLE,2016,Concurrency Computation ,28,6,,1751,1766,no,,,1,10.1002/cpe.3000,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875019216&doi=10.1002%2fcpe.3000&partnerID=40&md5=4020e63c8e665c2c884674e62c4df515,"School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, China; School of Mechanical Engineering, Hangzhou Dianzi University, Hangzhou, China; School of Mechanical Engineering, University of New South Wales, Sydney, NSW, Australia; Supercomputing Center of Computer Network Information Center, Chinese Academy of Sciences, Beijing, China","Zhang, J.-L., School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, China; Yuan, J.-F., School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, China; Wan, J., School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, China; Mao, J., School of Mechanical Engineering, Hangzhou Dianzi University, Hangzhou, China; Zhu, L.-T., School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, China; Zhou, L., School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, China; Jiang, C.-F., School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, China; Di, P., School of Mechanical Engineering, University of New South Wales, Sydney, NSW, Australia; Wang, J., Supercomputing Center of Computer Network Information Center, Chinese Academy of Sciences, Beijing, China","Parallel semi-implicit method for pressure-linked equations(SIMPLE) algorithm is used to solve the 3-D incompressible pipe flow problem. In this paper, we proposed a novel parallel SIMPLE algorithm that uses the alternate tiling technique. Firstly, a parallel SIMPLE algorithm based on domain decomposition method was established, and the implementation of domain partition and data exchange was presented. Then, we presented serial finite difference stencil algorithm based on alternate tiling. Furthermore, an iteration space parallel two-way finite difference stencil algorithm based on alternate tiling was proposed, introducing the sequence of iterative space tiles as the sequence of execution and using time skewing technique to partition the iteration space, thus to improve the data locality of algorithm. The cache misses and the cost of communication and synchronization are reduced by reordering the tiles of iteration space. Finally, the effectiveness of the two parallel SIMPLE algorithms were compared. The results showed that the parallel SIMPLE algorithm that uses the two-way finite difference stencil algorithm based on alternate tiling has good data locality, performance, and scalability in the Deepcomp7000 cluster computing environment. Copyright å© 2013 John Wiley & Sons, Ltd.",alternate tiling; domain decomposition; finite difference stencil; incompressible pipe flow; SIMPLE algorithm; TDMA,Article,Scopus,2-s2.0-84875019216
"Zhang G., Li M., Yi T.",Cost estimation method of power engineering overhead line based on ARIMA‰ÛÓRBF neural network model,2016,"International Journal of Simulation: Systems, Science and Technology",17,44,,3.1,3.4,no,,,,10.5013/IJSSST.a.17.44.03,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006041897&doi=10.5013%2fIJSSST.a.17.44.03&partnerID=40&md5=3f64a9862b5fdf5ce2f0e1b6369dce0d,"Department of Technology Economics, North China Electric Power University, Beijing, China; Department of Management science and Engineering, North China Electric Power University, Beijing, China","Zhang, G., Department of Technology Economics, North China Electric Power University, Beijing, China; Li, M., Department of Management science and Engineering, North China Electric Power University, Beijing, China; Yi, T., Department of Technology Economics, North China Electric Power University, Beijing, China","Factors in the unit investment forecast of overhead line engineering are various and complex, it is very difficult to get the satisfied forecasting effect using traditional econometric models. In view of this characteristic, this thesis puts forward a kind of combination forecast model, using the ARIMA model and RBF neural network model to seek for linear and nonlinear change rule of historical data of overhead line engineering unit investment, and combine all the values. This thesis elaborates on the principles of combination prediction, and it demonstrates the superiority of combination prediction to single prediction model. å© 2016, UK Simulation Society. All rights reserved.",ARIMA; Overhead lines; RBF neural network; Unit investment forecast,Article,Scopus,2-s2.0-85006041897
"Zhang F., Mockus A., Keivanloo I., Zou Y.",Towards building a universal defect prediction model with rank transformed predictors,2016,Empirical Software Engineering,21,5,,2107,2145,no,Defect,,,10.1007/s10664-015-9396-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941350779&doi=10.1007%2fs10664-015-9396-2&partnerID=40&md5=cfc68c31b20ae580a4981d9fa18d20ff,"School of Computing, Queen‰Ûªs University, Kingston, ON, Canada; Department of Electrical Engineering and Computer Science, University of Tennessee, Knoxville, TN, United States; Department of Electrical and Computer Engineering, Queen‰Ûªs University, Kingston, ON, Canada","Zhang, F., School of Computing, Queen‰Ûªs University, Kingston, ON, Canada; Mockus, A., Department of Electrical Engineering and Computer Science, University of Tennessee, Knoxville, TN, United States; Keivanloo, I., Department of Electrical and Computer Engineering, Queen‰Ûªs University, Kingston, ON, Canada; Zou, Y., Department of Electrical and Computer Engineering, Queen‰Ûªs University, Kingston, ON, Canada","Software defects can lead to undesired results. Correcting defects costs 50 % to 75 % of the total software development budgets. To predict defective files, a prediction model must be built with predictors (e.g., software metrics) obtained from either a project itself (within-project) or from other projects (cross-project). A universal defect prediction model that is built from a large set of diverse projects would relieve the need to build and tailor prediction models for an individual project. A formidable obstacle to build a universal model is the variations in the distribution of predictors among projects of diverse contexts (e.g., size and programming language). Hence, we propose to cluster projects based on the similarity of the distribution of predictors, and derive the rank transformations using quantiles of predictors for a cluster. We fit the universal model on the transformed data of 1,385 open source projects hosted on SourceForge and GoogleCode. The universal model obtains prediction performance comparable to the within-project models, yields similar results when applied on five external projects (one Apache and four Eclipse projects), and performs similarly among projects with different context factors. At last, we investigate what predictors should be included in the universal model. We expect that this work could form a basis for future work on building a universal model and would lead to software support tools that incorporate it into a regular development workflow. å© 2015, Springer Science+Business Media New York.",Context factors; Defect prediction; Large-scale; Rank transformation; Software quality; Universal defect prediction model,Article,Scopus,2-s2.0-84941350779
"Zeng R., Xiao H.",Transportation route selection of construction logistics based on complex network,2016,"Proceedings of the 2016 IEEE 20th International Conference on Computer Supported Cooperative Work in Design, CSCWD 2016",,,7566054,581,583,no,,,,10.1109/CSCWD.2016.7566054,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991710842&doi=10.1109%2fCSCWD.2016.7566054&partnerID=40&md5=e454e8bdd99533152c4231b62c012177,"School of Logistics Engineering, Wuhan University of Technology, Wuhan, China; Huaxia School of Wuhan University of Technology, Department of Automotive Engineering, Wuhan, China","Zeng, R., School of Logistics Engineering, Wuhan University of Technology, Wuhan, China, Huaxia School of Wuhan University of Technology, Department of Automotive Engineering, Wuhan, China; Xiao, H., School of Logistics Engineering, Wuhan University of Technology, Wuhan, China","For the high-risks of construction logistics, a reasonable transportation route is able to reduce the risk effectively and ensure that the materials can reach the construction site successfully. Based on the features of construction logistics and the complex network theory, influence factors in the process of transportation have been converted into the weights of the edge. Then, the average shortest path has been calculated to direct the choice of transportation routes, which can provide the basis for the transport route of construction logistics selection. According to the computer simulation and empirical analysis, the paper finally has analyzed the transport network topology and found the important nodes to reduce the risk of transportation. Finally, the paper put forward the optimal path and verify this method's effectiveness by empirical analysis. å© 2016 IEEE.",complex network; construction logistics; risk; transportation route optimization,Conference Paper,Scopus,2-s2.0-84991710842
Zagonari F.,Choosing among weight-estimation methods for multi-criterion analysis: A case study for the design of multi-purpose offshore platforms,2016,Applied Soft Computing Journal,39,,,1,10,no,,,2,10.1016/j.asoc.2015.11.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947727019&doi=10.1016%2fj.asoc.2015.11.003&partnerID=40&md5=4a6896a16b31d19a014ac7837fdec091,"Dipartimento di Scienze Economiche, UniversitÌÊ di Bologna, via AngherÌÊ 22, Rimini, Italy","Zagonari, F., Dipartimento di Scienze Economiche, UniversitÌÊ di Bologna, via AngherÌÊ 22, Rimini, Italy","Application of the sustainability concept to environmental projects implies that at least three feature categories (i.e., economic, social, and environmental) must be taken into account by applying a participative multi-criterion analysis (MCA). However, MCA results depend crucially on the methodology applied to estimate the relative criterion weights. By using a logically consistent set of data and methods (i.e., linear regression [LR], factor analysis [FA], the revised Simos procedure [RSP], and the analytical hierarchy process [AHP]), the present study revealed that mistakes from using one weight-estimation method rather than an alternative are non-significant in terms of satisfaction of specified acceptable standards (i.e., a risk of up to 1% of erroneously rejecting an option), but significant for comparisons between options (i.e., a risk of up to 11% of choosing a worse option by rejecting a better option). In particular, the risks of these mistakes are larger if both differences in statistical or computational algorithms and in data sets are involved (e.g., LR vs. AHP). In addition, the present study revealed that the choice of weight-estimation methods should depend on the estimated and normalised score differences for the economic, social, and environmental features. However, on average, some pairs of weight-estimation methods are more similar (e.g., AHP vs. RSP and LR vs. AHP are the most and the least similar, respectively), and some single weight-estimation methods are more reliable (i.e., FA > RSP > AHP > LR). å© 2015 Elsevier B.V. All rights reserved.",Analytical hierarchy process; Factor analysis; Linear regression; Multi-criterion analysis; Revised Simos procedure; Weighting,Article,Scopus,2-s2.0-84947727019
"Zabawi A.Y.M., Ahmad R., Abdul-Latip S.F.",An analysis technique for cost estimation in information security,2016,"Proceedings of the 5th International Cryptology and Information Security Conference 2016, CRYPTOLOGY 2016",,,,207,217,no,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984636524&partnerID=40&md5=8df4e6be968bc295ff736fb63458968c,"INFORSNET Faculty of Information and Communication Technology, Universiti Teknikal Malaysia Melaka, Durian Tunggal, Melaka, Malaysia","Zabawi, A.Y.M., INFORSNET Faculty of Information and Communication Technology, Universiti Teknikal Malaysia Melaka, Durian Tunggal, Melaka, Malaysia; Ahmad, R., INFORSNET Faculty of Information and Communication Technology, Universiti Teknikal Malaysia Melaka, Durian Tunggal, Melaka, Malaysia; Abdul-Latip, S.F., INFORSNET Faculty of Information and Communication Technology, Universiti Teknikal Malaysia Melaka, Durian Tunggal, Melaka, Malaysia","In this paper we propose a technique for cost estimation in information security risk analysis based on known threats. The motivation of this study emerges based on the fact that, to the best of our knowledge, there is no existing analysis tools that can provide cost estimation for each risk that had been identified. Most of the current risk analysis tools only use conventional techniques which can be divided into two categories, namely qualitative and quantitative techniques. Therefore, in our work a risk analysis tool comprises of cost estimation and risk assessment known as CARA (cost and risk analysis) was developed for leader. The development of this tool is a part of the preliminary work in data collection for threat analysis and cost estimation in which right now there is no particular tool that can furnish with those features. However the result from our preliminary work shows that this tool can be used as one of threat analysis tool. In the future we aim at analysing the data and improving the tools that may lack of perfection in any aspects. å© 2016, Institute for Mathematical Research (INSPEM). All rights reserved.",CARA; Cost estimation; Qualitative; Quantitative,Conference Paper,Scopus,2-s2.0-84984636524
"Yuan J., Na C., Hu Z., Li P.",Energy conservation and emissions reduction in China's power sector: Alternative scenarios up to 2020,2016,Energies,9,4,266,,,no,,,1,10.3390/en9040266,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964411179&doi=10.3390%2fen9040266&partnerID=40&md5=b1d1b011d165aa5f8f96be6efb78375e,"School of Economics and Management, North China Electric Power University, 2# Beinong Road, Changping District, Beijing, China; School of Physics and Electrical Information Engineering, Ningxia University, 217# Wencui Road, Xixia District, Yinchuan, Ningxia, China; School of Engineering and Applied Sciences, Harvard University, 29 Oxford St, Cambridge, MA, United States","Yuan, J., School of Economics and Management, North China Electric Power University, 2# Beinong Road, Changping District, Beijing, China; Na, C., School of Economics and Management, North China Electric Power University, 2# Beinong Road, Changping District, Beijing, China, School of Physics and Electrical Information Engineering, Ningxia University, 217# Wencui Road, Xixia District, Yinchuan, Ningxia, China; Hu, Z., School of Engineering and Applied Sciences, Harvard University, 29 Oxford St, Cambridge, MA, United States; Li, P., School of Physics and Electrical Information Engineering, Ningxia University, 217# Wencui Road, Xixia District, Yinchuan, Ningxia, China","This paper discusses energy conservation and emissions reduction (ECER) in China's power sector. To better understand China's successes and failures on energy conservation in the electricity industry, first it is important to know the status of China's power sector, and the key energy conservation actions, as well as the achievements in the past years. Second, two ECER scenarios are constructed to probe the 2020 energy conservation potential. Results show that the potential is estimated to be more than 240 million tons of coal equivalent (Mtce). Third, the improvement of coal power operations, structures and technologies, and ambitious deployment of energy conservation measures are proposed to fully explore the potential of China's power industry. Fourth, great challenges for China's ECER and some suggested policies are summed up. The lessons learnt from China will provide a valuable reference and useful inputs for other emerging economies. å© 2016 by the authors.",China; Emissions reduction; Energy conservation; Power sector,Article,Scopus,2-s2.0-84964411179
"Yousuf M., Bokhari M.U., Zeyauddin M.",An analysis of software requirements prioritization techniques: A detailed survey,2016,"Proceedings of the 10th INDIACom; 2016 3rd International Conference on Computing for Sustainable Global Development, INDIACom 2016",,,7725002,3966,3970,no,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997456779&partnerID=40&md5=98ba690d93f80f9234c83d50ce63d065,"Department of Computer Science, Baba Ghulam Shah Badshah University, Rajouri, JandK, India; Department of Computer Science, Aligarh Muslim University, Aligarh, India","Yousuf, M., Department of Computer Science, Baba Ghulam Shah Badshah University, Rajouri, JandK, India; Bokhari, M.U., Department of Computer Science, Aligarh Muslim University, Aligarh, India; Zeyauddin, M., Department of Computer Science, Aligarh Muslim University, Aligarh, India","Requirements prioritization is a crucial part of software engineering which helps to make good decisions regarding project planning and implementation with preferred requirements for single and multiple releases. This phase prioritizes requirements for high quality software. Ranks to requirements are assigned according to their importance and are placed in priority list for implementing them in successive releases of the project. Requirements prioritization decisions are made by the stakeholders and the parameters to be considered while prioritizing requirements are risk, cost, importance, etc. Right requirements are considered a part of good quality software. Due to budgetary constraints and time to market deadlines it is essential to prioritize requirements. There are various techniques available to support this prioritization process. The efficiency of these techniques depends upon many parameters that need to be addressed during prioritization process. The goal of this paper is to present systematic literature review of ten requirements prioritization techniques. A comparison of these techniques is also presented on the basis of following parameters: time, ease of use, number of comparisons, scalability and accuracy. å© 2016 IEEE.",Analytic Hierarchy Process; Bubble Sort; Cumulative Voting; Hierarchy AHP; Minimal Spanning Tree; Numerical Assignment; Planning game; Priority Groups; Software Requirements Prioritization Techniques,Conference Paper,Scopus,2-s2.0-84997456779
Ying H.,The Application of Multiple Linear Regression Method in the Investment Estimation of Urban Bridge Engineering,2015,"Proceedings - 2015 6th International Conference on Intelligent Systems Design and Engineering Applications, ISDEA 2015",,,7462744,821,824,no,,,,10.1109/ISDEA.2015.207,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969630973&doi=10.1109%2fISDEA.2015.207&partnerID=40&md5=7e7cba294671064e7aa981d671ee441e,"School of Economics and Management, Jiangxi Science and Technology Normal University, Nanchang, Jiangxi, China","Ying, H., School of Economics and Management, Jiangxi Science and Technology Normal University, Nanchang, Jiangxi, China","The construction of urban bridge engineering can not only improve the traffic situation of our country, but also can produce considerable economic and social benefits. It has the characteristics of large investment, long construction period, etc. Therefore, the accuracy of the investment estimation of the urban bridge engineering has a direct impact on its construction. In order to ensure the successful completion of the urban bridge project, we should pay more attention to the pre investment decision-making and improve the accuracy of the investment estimation. This paper analyzes the current situation of the investment estimation of the urban bridge engineering, including the establishment and the estimation method of the urban bridge engineering investment estimation. This paper expounds the basic theory of the investment estimation of the urban bridge engineering. The basic theory for the investment estimation of the urban bridge engineering includes the content and formation for the establishment of the investment estimate. Based on the multiple linear regression method, the corresponding index system is determined, and the model of the multiple linear regression analysis is established. The result of the research can provide reference for the government department to carry out the investment decision of the urban bridge engineering, so as to carry out the whole process management of the project cost. å© 2015 IEEE.",Multiple Linear Regression;Urban Bridge;Investment Estimation; Basic Theory,Conference Paper,Scopus,2-s2.0-84969630973
"Yi W., Wang S.",Multi-Objective Mathematical Programming Approach to Construction Laborer Assignment with Equity Consideration,2016,Computer-Aided Civil and Infrastructure Engineering,31,12,,954,965,no,,,,10.1111/mice.12239,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995545698&doi=10.1111%2fmice.12239&partnerID=40&md5=3bbbb6fe10532d3ad0dbb47168c32fc0,"Department of Building and Real Estate, The Hong Kong Polytechnic University, Kowloon, Hong Kong; Department of Logistics & Maritime Studies, The Hong Kong Polytechnic University, Kowloon, Hong Kong","Yi, W., Department of Building and Real Estate, The Hong Kong Polytechnic University, Kowloon, Hong Kong; Wang, S., Department of Logistics & Maritime Studies, The Hong Kong Polytechnic University, Kowloon, Hong Kong","Construction laborer assignment is the assignment of laborers in a team to the tasks for a daily work in a construction project. This study proposes a mathematical programming approach to examine the optimal construction laborer assignment problem with the objectives of productivity and occupational health and safety while considering equity between laborers. Several linearization techniques are presented to transform the mathematical programming model into a mixed-integer linear program, which can be solved by off-the-shelf mixed-integer linear solvers. The proposed model is applied to extensive numerical experiments and the results show that the mathematical programming approach outperforms a conventional heuristic by over 10% in terms of job completion time. This implies considerable overhead, equipment, and manpower savings for the construction industry. å© 2016 Computer-Aided Civil and Infrastructure Engineering",,Article,Scopus,2-s2.0-84995545698
"Yazawa K., Shakouri A.",Heat transfer modeling for bio-heat recovery,2016,"Proceedings of the 15th InterSociety Conference on Thermal and Thermomechanical Phenomena in Electronic Systems, ITherm 2016",,,7517723,1482,1488,no,,,,10.1109/ITHERM.2016.7517723,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983251054&doi=10.1109%2fITHERM.2016.7517723&partnerID=40&md5=15eb65332239765e397e6c21af214642,"Birck Nanotechnology Center, Purdue University, 1205 W. State St., West Lafayette, IN, United States","Yazawa, K., Birck Nanotechnology Center, Purdue University, 1205 W. State St., West Lafayette, IN, United States; Shakouri, A., Birck Nanotechnology Center, Purdue University, 1205 W. State St., West Lafayette, IN, United States","Harvesting heat from the skin surface of humans or homoeothermic animals will provide a great opportunity for implementing wireless communication sensors and/or monitoring devices without the use of batteries. One of the bio-energy harvesting approaches, the thermoelectric generator is a promising candidate for converting the temperature difference between the body skin and the ambient air into electricity. Due to the surface curvature of the human body, a mechanically flexible and thin generator module is desired. Thus, we propose and investigate a new concept which is a woven fabric thermoelectric module made from weaving together thermoelectric strings which have a repeating [p-type-metal-n-type-metal] (p=m=n=m) pattern with Bismuth-Telluride (Bi2Te3) and insulation yarn. This paper reports the performance and cost estimation of the module compared with a conventional (ìÛ-structure) module that consists of a polymer based thermoelectric material, e.g., PEDOT-PSS. Heat Transfer through the thermoelectric element is not only the key to optimizing the module design for maximum power output but is also the series that spreads thermal resistances in laminate films, the parallel thermal path through the gap fill media, and the external heat sink thermal resistances. According to a previously developed generic model, the performance and cost per power production of the woven thermoelectric module is calculated and compared with that of a conventional polymer based module. The woven generator has been found to be able to produce power output that is nearly double that of the conventional polymer one. As a power generator system, the cold side heat sink can be designed to enhance the power output by extending the surface area. As one of the surface extensions, a pin-fin heat transfer is the most promising for the flexible thermoelectric generator module, but it has been found to produce a rather less encouraging enhancement except for a relatively longer pinfin of more than 1 cm. å© 2016 IEEE.",bio heat; electro-thermal optimum; low heat flux; Thermoelectric strings; woven thermoelectric,Conference Paper,Scopus,2-s2.0-84983251054
"Yao X., Moon S.K., Bi G.",A Cost-Driven Design Methodology for Additive Manufactured Variable Platforms in Product Families,2016,"Journal of Mechanical Design, Transactions of the ASME",138,4,41701,,,no,,,1,10.1115/1.4032504,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957878337&doi=10.1115%2f1.4032504&partnerID=40&md5=9918018a16c91743eda2a3c0ca1b1e48,"Singapore Centre for 3D Printing, School of Mechanical and Aerospace Engineering, Nanyang Technological University, 50 Nanyang Avenue, Singapore, Singapore; Singapore Institute of Manufacturing Technology, 71 Nanyang Drive, Singapore, Singapore","Yao, X., Singapore Centre for 3D Printing, School of Mechanical and Aerospace Engineering, Nanyang Technological University, 50 Nanyang Avenue, Singapore, Singapore; Moon, S.K., Singapore Centre for 3D Printing, School of Mechanical and Aerospace Engineering, Nanyang Technological University, 50 Nanyang Avenue, Singapore, Singapore; Bi, G., Singapore Institute of Manufacturing Technology, 71 Nanyang Drive, Singapore, Singapore","Additive manufacturing (AM) has evolved from prototyping to functional part fabrication for a wide range of applications. Challenges exist in developing new product design methodologies to utilize AM-enabled design freedoms while limiting costs at the same time. When major design changes are made to a part, undesired high cost increments may be incurred due to significant adjustments of AM process settings. In this research, we introduce the concept of an additive manufactured variable product platform and its associated process setting platform. Design and process setting adjustments based on a reference part are constrained within a bounded feasible space (FS) in order to limit cost increments. In this paper, we develop a cost-driven design methodology for product families implemented with additive manufactured variable platforms. A fuzzy time-driven activity-based costing (FTDABC) approach is introduced to estimate AM production costs based on process settings. Time equations in the FTDABC are computed in a trained adaptive neuro-fuzzy inference system (ANFIS). The process setting adjustment's FS boundary is identified by solving a multi-objective optimization problem. Variable platform design parameter limitations are computed in a Mamdani-type expert system, and then used as constraints in the design optimization to maximize customer perceived utility. Case studies on designing an R/C racing car family illustrate the proposed methodology and demonstrate that the optimized additive manufactured variable platforms can improve product performances at lower costs than conventional consistent platform-based design. Copyright å© 2016 by ASME.",additive manufacturing process; cost estimation; platform optimization; product family design,Article,Scopus,2-s2.0-84957878337
"Yang Y., Zhou Y., Liu J., Zhao Y., Lu H., Xu L., Xu B., Leung H.",Effort-Aware just-in-Time defect prediction: Simple unsupervised models could be better than supervised models,2016,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,13-18-November-2016,,,157,168,no,Defect,,,10.1145/2950290.295035,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997270955&doi=10.1145%2f2950290.295035&partnerID=40&md5=0ee449b7e78ec60454cf5c438b78f6a5,"Department of Computer Science and Technology, Nanjing University, China; Department of Computing, Hong Kong Polytechnic University, Hong Kong, Hong Kong","Yang, Y., Department of Computer Science and Technology, Nanjing University, China; Zhou, Y., Department of Computer Science and Technology, Nanjing University, China; Liu, J., Department of Computer Science and Technology, Nanjing University, China; Zhao, Y., Department of Computer Science and Technology, Nanjing University, China; Lu, H., Department of Computer Science and Technology, Nanjing University, China; Xu, L., Department of Computer Science and Technology, Nanjing University, China; Xu, B., Department of Computer Science and Technology, Nanjing University, China; Leung, H., Department of Computing, Hong Kong Polytechnic University, Hong Kong, Hong Kong","Unsupervised models do not require the defect data to build the prediction models and hence incur a low building cost and gain a wide application range. Consequently, it would be more desirable for practitioners to apply unsupervised models in effort-Aware just-in-Time (JIT) defect prediction if they can predict defect-inducing changes well. However, little is currently known on their prediction effectiveness in this context. We aim to investigate the predictive power of simple unsupervised models in effort-Aware JIT defect prediction, especially compared with the state-of-The-Art su-pervised models in the recent literature. We first use the most commonly used change metrics to build simple unsupervised models. Then, we compare these unsupervised models with the state-of-The-Art supervised models under cross-validation, time-wise-cross-validation, and across-project prediction set-tings to determine whether they are of practical value. The experimental results, from open-source software systems, show that many simple unsupervised models perform better than the state-of-The-Art supervised models in effort-Aware JIT defect prediction. å© 2016 ACM.",Changes; Defect; EffOrt-Aware; Just-In-Time; Prediction,Conference Paper,Scopus,2-s2.0-84997270955
"Yang Y., Laird L.",Teaching software estimation through LEGOs,2016,"Proceedings - 2016 IEEE 29th Conference on Software Engineering Education and Training, CSEEandT 2016",,,7474465,56,65,no,,,,10.1109/CSEET.2016.22,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974601890&doi=10.1109%2fCSEET.2016.22&partnerID=40&md5=4dd33c014026c4558682243728e384b4,"School of Systems and Enterprises, Stevens Institute of Technology, Hoboken, NJ, United States","Yang, Y., School of Systems and Enterprises, Stevens Institute of Technology, Hoboken, NJ, United States; Laird, L., School of Systems and Enterprises, Stevens Institute of Technology, Hoboken, NJ, United States","Software Estimation is an important part of every Software Engineers' skill set. At Stevens Institute of Technology, estimation has been taught as a core course on ""Software Estimation and Metrics"" in our Software Engineering Masters Program since 2001. Over the past few years, we have evolved a more experiential and engaging teaching style. This paper presents an experimental framework of a new pedagogical method using LEGOs for teaching software estimation and measurement concepts and methods. The framework is evaluated through two case study sessions: one on experienced industry part-time students, one on inexperienced on-campus students. Results from both sessions indicate a positive impact on student learning. This teaching technique, since it appears to reinforce software engineering estimation heuristics and models, has the potential to be extended for other software estimation experiential education and experimentation, such as demonstrating the impact of anchoring on estimations. Students enjoy these exercises and become very engaged. å© 2016 IEEE.",Delphi; Engaged learning; Experiential learning; Expert estimation; Software estimation,Conference Paper,Scopus,2-s2.0-84974601890
"Yan Q., Dokic T., Kezunovic M.",GIS-based risk assessment for electric power consumers under severe weather conditions,2016,"Proceedings of the 18th Mediterranean Electrotechnical Conference: Intelligent and Efficient Technologies and Services for the Citizen, MELECON 2016",,,7495473,,,no,,,,10.1109/MELCON.2016.7495473,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979263305&doi=10.1109%2fMELCON.2016.7495473&partnerID=40&md5=6ae31ccb0739d8d892b8f59ebf607492,"Department of Electrical and Computer Engineering, Texas A and M University, College Station, TX, United States","Yan, Q., Department of Electrical and Computer Engineering, Texas A and M University, College Station, TX, United States; Dokic, T., Department of Electrical and Computer Engineering, Texas A and M University, College Station, TX, United States; Kezunovic, M., Department of Electrical and Computer Engineering, Texas A and M University, College Station, TX, United States",This study incorporates the real-time weather changes into the models for evaluation of customer interruption cost (CIC) and implements the risk assessment of the impact on customers from the possible weather-caused power outage. The methods uses historical weather and weather forecast data to evaluate the risk for customers in case of different weather conditions that are predicted to affect a given area in near future. å© 2016 IEEE.,customer impact; data analysis; geographical information system; meteorology; risk assessment; smart grid,Conference Paper,Scopus,2-s2.0-84979263305
"Yadav S., Vishwakarma V.P.",Interval type-2 fuzzy based pixel wise information extraction: An improved approach to face recognition,2016,"2016 International Conference on Computational Techniques in Information and Communication Technologies, ICCTICT 2016 - Proceedings",,,7514616,409,414,no,,,,10.1109/ICCTICT.2016.7514616,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84980383686&doi=10.1109%2fICCTICT.2016.7514616&partnerID=40&md5=71b052bb4abce713f6458dcea3652f8a,"University School of Information and Communication Technology, Guru Gobind Singh Indraprastha University, Sector 16C, Dwarka, New Delhi, India","Yadav, S., University School of Information and Communication Technology, Guru Gobind Singh Indraprastha University, Sector 16C, Dwarka, New Delhi, India; Vishwakarma, V.P., University School of Information and Communication Technology, Guru Gobind Singh Indraprastha University, Sector 16C, Dwarka, New Delhi, India","Over the last few decades, face recognition has become a popular area of research in computer vision. It is the most successful application of pattern classification in the field of image analysis and understanding. Despite of all success in face recognition systems, it faces the problem of handling inaccurate information. In this paper, we propose a fuzzy logic based information extraction approach for face recognition systems for diminishing the effects of uncertainty formed by various reasons like variations in light direction, facial expressions, face poses etc. To address the issue of uncertainty, interval type-2 membership function on each random generation of train and test image vector is applied. Further, we perform dimension reduction of high dimensional image space using principal component analysis. Next, we apply one of the variant of distance metrics i.e. k-nearest neighbor classification, to obtain the classification error rate between and within class scatter matrix. Type-2 fuzzy set membership function employment makes the system able to reduce average error rate more and, also it handles uncertainty more effectively as compare to type-1 fuzzy set membership function. Experiments performed on the AT&T database validate our proposed approach and the proposed approach does not require any pre-conditioning. å© 2016 IEEE.",Classification Approaches; Face Recognition; Gaussian Membership function; Interval Type-2 Fuzzy Logics,Conference Paper,Scopus,2-s2.0-84980383686
"Xu S., Liu K., Tang L.C.M., Li W.","A framework for integrating syntax, semantics and pragmatics for computer-aided professional practice: With application of costing in construction industry",2016,Computers in Industry,83,,,28,45,no,,,,10.1016/j.compind.2016.08.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988836260&doi=10.1016%2fj.compind.2016.08.004&partnerID=40&md5=11749eb957c55637a8f36396f25a3eab,"Primary Care & Public Health Sciences, King's College London, Guy's Campus, United Kingdom; Business Informatics, Systems & Accounting, Henley Business School, University of Reading, Whiteknights, Reading, United Kingdom; Department of Architecture and Built Environment, University of Nottingham Ningbo, 199 Taikang East Road, Ningbo, China; School of Information Management and Engineering, Shanghai University of Finance and Economics, China","Xu, S., Primary Care & Public Health Sciences, King's College London, Guy's Campus, United Kingdom; Liu, K., Business Informatics, Systems & Accounting, Henley Business School, University of Reading, Whiteknights, Reading, United Kingdom, School of Information Management and Engineering, Shanghai University of Finance and Economics, China; Tang, L.C.M., Department of Architecture and Built Environment, University of Nottingham Ningbo, 199 Taikang East Road, Ningbo, China; Li, W., Business Informatics, Systems & Accounting, Henley Business School, University of Reading, Whiteknights, Reading, United Kingdom","Producing a bill of quantity is a knowledge-based, dynamic and collaborative process, and evolves with variances and current evidence. However, within the context of information system practice in BIM, knowledge of cost estimation has not been represented, nor has it been integrated into the processes based on BIM. This paper intends to establish an innovative means of taking data from the BIM linked to a project, and using it to create the necessary items for a bill of quantity that will enable cost estimation to be undertaken for the project. Our framework is founded upon the belief that three components are necessary to gain a full awareness of the domain which is being computerised; the information type which is to be assessed for compatibility (syntax), the definition for the pricing domain (semantics), and the precise implementation environment for the standards being taken into account (pragmatics). In order to achieve this, a prototype is created that allows a cost item for the bill of quantity to be spontaneously generated, by means of the semantic web ontology and a forward chain algorithm. Within this paper, ‰Û÷cost items‰Ûª signify the elements included in a bill of quantity, including details of their description, quantity and price. As a means of authenticating the process being developed, the authors of this work effectively implemented it in the production of cost items. In addition, the items created were contrasted with those produced by specialists. For this reason, this innovative framework introduces the possibility of a new means of applying semantic web ontology and forward chain algorithm to construction professional practice resulting in automatic cost estimation. These key outcomes demonstrate that, decoupling the professional practice into three key components of syntax, semantics and pragmatics can provide tangible benefits to domain user. å© 2016",BIM; Computer-aided professional practice; Cost estimation; Expert system; IFC; Ontology; Rule-based; Semantic,Article,Scopus,2-s2.0-84988836260
"Xu H., Zhang X., Yu T.",Robust collaborative consensus algorithm for economic dispatch of non-ideal communication network,2016,Dianli Xitong Zidonghua/Automation of Electric Power Systems,40,14,,15,24 and 57,no,,,,10.7500/AEPS20160112003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978776442&doi=10.7500%2fAEPS20160112003&partnerID=40&md5=407a53cab13112c6368dbea6d88d0b3e,"School of Electric Power, South China University of Technology, Guangzhou, China","Xu, H., School of Electric Power, South China University of Technology, Guangzhou, China; Zhang, X., School of Electric Power, South China University of Technology, Guangzhou, China; Yu, T., School of Electric Power, South China University of Technology, Guangzhou, China","Considering the influence of transmission delay, noises and dynamically switching topologies in actual communication networks, an robust collaborative consensus algorithm for economic dispatch of non-ideal communication environment is presented. By introducing a virtual consensus variable,communications among smart units become more flexible as to enhance plug and play adaptability and consensus collaboration under dynamically switching topologies. With the assistance of a consensus gain function, the weakness of consensus-based algorithm to delays and noises in communication networks is eliminated. Finally, simulation results obtained from IEEE 39 and 118 bus test networks show that the proposed algorithm is suitable for the consensus economic dispatch of non-ideal communication network and satisfied optimization results can be obtained while the astringency is ensured. å© 2016 Automation of Electric Power Systems Press.",Consensus gain function; Delays and noises; Dynamically switching topologies; Robust collaborative consensus algorithm; Virtual consensus variable,Article,Scopus,2-s2.0-84978776442
"Xie X., Mei B., Chen J., Du X., Jensen C.S.",Elite: an elastic infrastructure for big spatiotemporal trajectories,2016,VLDB Journal,25,4,,473,493,no,,,1,10.1007/s00778-016-0425-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958765160&doi=10.1007%2fs00778-016-0425-6&partnerID=40&md5=c172b9784857009422f2d67958210f31,"Department of Computer Science, Aalborg University, Aalborg, Denmark; School of Information, Renmin University of China, Beijing, China; Key Lab of Data Engineering and Knowledge Engineering, Renmin University of China, Beijing, China","Xie, X., Department of Computer Science, Aalborg University, Aalborg, Denmark; Mei, B., School of Information, Renmin University of China, Beijing, China; Chen, J., Key Lab of Data Engineering and Knowledge Engineering, Renmin University of China, Beijing, China; Du, X., Key Lab of Data Engineering and Knowledge Engineering, Renmin University of China, Beijing, China; Jensen, C.S., Department of Computer Science, Aalborg University, Aalborg, Denmark","As the volumes of spatiotemporal trajectory data continue to grow at a rapid pace; a new generation of data management techniques is needed in order to be able to utilize these data to provide a range of data-driven services, including geographic-type services. Key challenges posed by spatiotemporal data include the massive data volumes, the high velocity with which the data are captured, the need for interactive response times, and the inherent inaccuracy of the data. We propose an infrastructure, Elite, that leverages peer-to-peer and parallel computing techniques to address these challenges. The infrastructure offers efficient, parallel update and query processing by organizing the data into a layered index structure that is logically centralized, but physically distributed among computing nodes. The infrastructure is elastic with respect to storage, meaning that it adapts to fluctuations in the storage volume, and with respect to computation, meaning that the degree of parallelism can be adapted to best match the computational requirements. Further, the infrastructure offers advanced functionality, including probabilistic simulations, for contending with the inaccuracy of the underlying data in query processing. Extensive empirical studies offer insight into properties of the infrastructure and indicate that it meets its design goals, thus enabling the effective management of big spatiotemporal data. Ìâå© 2016, Springer-Verlag Berlin Heidelberg.",Elasticity; Spatiotemporal data; Trajectories,Article,Scopus,2-s2.0-84958765160
"Wulamu A., Li H., Guo X., Xie Y., Fu Y.",Processing skyline groups on data streams,2015,"Proceedings - 2015 IEEE 12th International Conference on Ubiquitous Intelligence and Computing, 2015 IEEE 12th International Conference on Advanced and Trusted Computing, 2015 IEEE 15th International Conference on Scalable Computing and Communications, 2015 IEEE International Conference on Cloud and Big Data Computing, 2015 IEEE International Conference on Internet of People and Associated Symposia/Workshops, UIC-ATC-ScalCom-CBDCom-IoP 2015",,,7518357,935,942,no,,,,10.1109/UIC-ATC-ScalCom-CBDCom-IoP.2015.178,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983395849&doi=10.1109%2fUIC-ATC-ScalCom-CBDCom-IoP.2015.178&partnerID=40&md5=3eec9c51ac6148a15f628a4d2362c98f,"University of Science and Technology Beijing, Beijing Key Laboratory of Knowledge Engineering for Materials Science, Beijing, China","Wulamu, A., University of Science and Technology Beijing, Beijing Key Laboratory of Knowledge Engineering for Materials Science, Beijing, China; Li, H., University of Science and Technology Beijing, Beijing Key Laboratory of Knowledge Engineering for Materials Science, Beijing, China; Guo, X., University of Science and Technology Beijing, Beijing Key Laboratory of Knowledge Engineering for Materials Science, Beijing, China; Xie, Y., University of Science and Technology Beijing, Beijing Key Laboratory of Knowledge Engineering for Materials Science, Beijing, China; Fu, Y., University of Science and Technology Beijing, Beijing Key Laboratory of Knowledge Engineering for Materials Science, Beijing, China","Skyline is defined as a set of objects in a multidimensional dataset. It returns objects which are not dominated by any other objects in the set. An object p dominates object p' if and only if it is not worse than p' on all of the attributes (dimensions) and is better than p' on at least one attribute. Given the same kind of dataset, the skyline group query returns groups which are not dominated by any other groups in the set and each group has the same number of objects. Although the skyline group query has been investigated in recent years, most techniques are designed for static datasets. However, data are changing with time in many practical applications nowadays and query processing techniques for dynamic datasets are not available. Therefore, finding skyline groups on data streams are highly required. In this paper, we propose a new algorithm to find the skyline groups on data streams. We use two data structures CL and GSM to store the immediate results. CL stores the candidate objects that may become members of the skyline groups. GSM stores the immediate results of the dynamic programming. We do experiments on synthetic datasets. The experimental results show that the algorithms proposed can find skyline groups efficiently. å© 2015 IEEE.",Data streams; Query processing; Skyline; Skyline group,Conference Paper,Scopus,2-s2.0-84983395849
"Wu W., Naughton J.F., Singh H.",Sampling-based query re-optimization,2016,Proceedings of the ACM SIGMOD International Conference on Management of Data,26-Jun-16,,,1721,1736,no,,,,10.1145/2882903.2882914,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979710705&doi=10.1145%2f2882903.2882914&partnerID=40&md5=6542d4e92d1309745036577eff50e5f1,"Department of Computer Sciences, University of Wisconsin-Madison, United States","Wu, W., Department of Computer Sciences, University of Wisconsin-Madison, United States; Naughton, J.F., Department of Computer Sciences, University of Wisconsin-Madison, United States; Singh, H., Department of Computer Sciences, University of Wisconsin-Madison, United States","Despite of decades of work, query optimizers still make mistakes on ""difficult"" queries because of bad cardinality estimates, often due to the interaction of multiple predicates and correlations in the data. In this paper, we propose a low-cost post-processing step that can take a plan produced by the optimizer, detect when it is likely to have made such a mistake, and take steps to fix it. Specifically, our solution is a sampling-based iterative procedure that requires almost no changes to the original query optimizer or query evaluation mechanism of the system. We show that this indeed imposes low overhead and catches cases where three widely used optimizers (PostgreSQL and two commercial systems) make large errors. å© 2016 ACM.",,Conference Paper,Scopus,2-s2.0-84979710705
"Wu H., Sun X., Yang J., Zeng W., Wu F.",Lossless Compression of JPEG Coded Photo Collections,2016,IEEE Transactions on Image Processing,25,6,7448441,2684,2696,no,,,,10.1109/TIP.2016.2551366,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968765100&doi=10.1109%2fTIP.2016.2551366&partnerID=40&md5=36a81169ec49d89a9c0270936df297c3,"Tianjin University, Tianjin, China; Microsoft Research Asia, Beijing, China; University of Science and Technology of China, Hefei, China","Wu, H., Tianjin University, Tianjin, China; Sun, X., Microsoft Research Asia, Beijing, China; Yang, J., Tianjin University, Tianjin, China; Zeng, W., Microsoft Research Asia, Beijing, China; Wu, F., University of Science and Technology of China, Hefei, China","The explosion of digital photos has posed a significant challenge to photo storage and transmission for both personal devices and cloud platforms. In this paper, we propose a novel lossless compression method to further reduce the size of a set of JPEG coded correlated images without any loss of information. The proposed method jointly removes inter/intra image redundancy in the feature, spatial, and frequency domains. For each collection, we first organize the images into a pseudo video by minimizing the global prediction cost in the feature domain. We then present a hybrid disparity compensation method to better exploit both the global and local correlations among the images in the spatial domain. Furthermore, the redundancy between each compensated signal and the corresponding target image is adaptively reduced in the frequency domain. Experimental results demonstrate the effectiveness of the proposed lossless compression method. Compared with the JPEG coded image collections, our method achieves average bit savings of more than 31%. å© 2016 IEEE.",image coding; image collection; Image compression; image set; JPEG; lossless; recompression,Article,Scopus,2-s2.0-84968765100
Wohlin C.,Second-generation systematic literature studies using snowballing,2016,ACM International Conference Proceeding Series,01-03-June-2016,, a15,,,no,,,,10.1145/2915970.2916006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978516310&doi=10.1145%2f2915970.2916006&partnerID=40&md5=44060e0d78bb1f4f4a9f3ba4e575a219,"Blekinge Institute of Technology, Karlskrona, Sweden","Wohlin, C., Blekinge Institute of Technology, Karlskrona, Sweden","Systematic literature studies have become standard practice in software engineering to synthesize evidence in different areas of the discipline. As more such studies are published, there is also a need to extend previously published systematic literature studies to cover new research papers. These first extensions become second-generation systematic literature studies. It has been asserted that snowballing would be a suitable search strategy for these types of second-generation studies, since newer studies ought to refer to previous research on a topic, and in particular to systematic literature studies published in an area. This paper compares using a snowballing search strategy with a published second-generation study using a database search strategy in the area of cross-company vs. within-company effort estimation. It is concluded that the approaches are comparable when it comes to which papers they find, although the snowballing approach is more efficient in this particular case. Copyright is held by the owner/author(s).",Empirical research methods; Snowballing; Systematic literature reviews,Conference Paper,Scopus,2-s2.0-84978516310
"Wen X., Zhou X.",Servitization of manufacturing industries based on cloud-based business model and the down-to-earth implementary path,2016,International Journal of Advanced Manufacturing Technology,87,5-Aug,,1491,1508,no,,,,10.1007/s00170-014-6348-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992602732&doi=10.1007%2fs00170-014-6348-8&partnerID=40&md5=ea160d09195dcc0730c6585c4b87ab33,"Institute of Advanced Manufacturing Engineering, Department of Mechanical Engineering, Zhejiang University, Hangzhou, China","Wen, X., Institute of Advanced Manufacturing Engineering, Department of Mechanical Engineering, Zhejiang University, Hangzhou, China; Zhou, X., Institute of Advanced Manufacturing Engineering, Department of Mechanical Engineering, Zhejiang University, Hangzhou, China","Servitization of manufacturing industries (SMI) is emerging as a new trend all around the world, especially in major manufacturing nations, e.g., USA, Germany, and China, which is believed to be able to upgrade the industry chains and improve the economy. Almost simultaneously, cloud manufacturing (CM) is being generally debated within the academic field as a new manufacturing paradigm. However, little attention was paid to implementing SMI based on cloud-based business model. This paper aims to discuss the ways to make SMI possible through cloud-based manufacturing business model (CMBM) and relevant enabling technologies including CM. Specifically, this paper (1) presents the key elements that motivate SMI and the full definition of SMI; (2) proposes a description of CMBM matching up with the execution of SMI; (3) provides some constructive advices, namely the down-to-earth path, with technology aspects included for the government and enterprises to carry out SMI based on CMBM; and (4) offers a case study related to successful SMI based on CMBM. å© 2014, Springer-Verlag London.",Cloud manufacturing; Cloud-based manufacturing business model; Down-to-earth path; Enabling technologies; Service-oriented manufacturing; Servitization of manufacturing industries,Article,Scopus,2-s2.0-84992602732
"Weinreich R., Groher I.",Software architecture knowledge management approaches and their support for knowledge management activities: A systematic literature review,2016,Information and Software Technology,80,,,265,286,no,,,,10.1016/j.infsof.2016.09.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991377633&doi=10.1016%2fj.infsof.2016.09.007&partnerID=40&md5=783b0b60f3b6cb247b805d78aee96ee8,"Johannes Kepler University Linz, Austria","Weinreich, R., Johannes Kepler University Linz, Austria; Groher, I., Johannes Kepler University Linz, Austria","Context: Numerous approaches for Software Architecture Knowledge Management (SAKM) have been developed by the research community over the last decade. Still, these approaches have not yet found widespread use in practice. Objective: This work identifies existing approaches to SAKM and analyzes them in terms of their support for central architecture knowledge management activities, i.e., capturing, using, maintaining, sharing, and reuse of architectural knowledge, along with presenting the evidence provided for this support. Method: A systematic literature review has been conducted for identifying and analyzing SAKM approaches, covering work published between January 2004 and August 2015. We identified 56 different approaches to SAKM based on 115 studies. We analyzed each approach in terms of its focus and support for important architecture knowledge management activities and in terms of the provided level of evidence for each supported activity. Results: Most of the developed approaches focus on using already-captured knowledge. Using is also the best-validated activity. The problem of efficient capturing is still not sufficiently addressed, and only a few approaches specifically address reuse, sharing, and, especially, maintaining. Conclusions: Without adequate support for other core architecture knowledge management activities besides using, the adoption of SAKM in practice will remain an elusive target. The problem of efficient capturing is still unsolved, as is the problem of maintaining captured knowledge over the long term. We also need more case studies and replication studies providing evidence for the usefulness of developed support for SAKM activities, as well as better reporting on these case studies. å© 2016 Elsevier B.V.",Software architecture; Software architecture knowledge management; Software architecture knowledge management activities; Software architecture knowledge management approaches; Systematic literature review,Review,Scopus,2-s2.0-84991377633
"Wazid M., Das A.K., Kumari S., Li X., Wu F.",Design of an efficient and provably secure anonymity preserving three-factor user authentication and key agreement scheme for TMIS,2016,Security and Communication Networks,9,13,,1983,2001,no,,,4,10.1002/sec.1452,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958612449&doi=10.1002%2fsec.1452&partnerID=40&md5=9ee046e755790c314f0550f025026ec3,"Center for Security, Theory and Algorithmic Research, International Institute of Information Technology, Hyderabad, India; Department of Mathematics, Chaudhary Charan Singh University, Meerut, Uttar Prades, India; School of Computer Science and Engineering, Hunan University of Science and Technology, Xiangtan, China; Department of Computer Science and Engineering, Xiamen Institute of Technology, Huaqiao University, Xiamen, China","Wazid, M., Center for Security, Theory and Algorithmic Research, International Institute of Information Technology, Hyderabad, India; Das, A.K., Center for Security, Theory and Algorithmic Research, International Institute of Information Technology, Hyderabad, India; Kumari, S., Department of Mathematics, Chaudhary Charan Singh University, Meerut, Uttar Prades, India; Li, X., School of Computer Science and Engineering, Hunan University of Science and Technology, Xiangtan, China; Wu, F., Department of Computer Science and Engineering, Xiamen Institute of Technology, Huaqiao University, Xiamen, China","Several remote user authentication techniques for telecare medicine information system (TMIS) have been proposed in the literature. But most existing techniques have limitations such as vulnerable to various attacks, lack of functionalities, and inefficiency. Recently, Amin and Biswas proposed a three-factor authentication and key agreement technique for TMIS. But their scheme is inefficient and has several security drawbacks. The attacks such as privileged-insider, user impersonation, and strong reply attacks are possible on their scheme. It also has flaw in password update phase. In order to overcome drawbacks of their scheme, a new provably secure and efficient three-factor remote user authentication scheme for TMIS is proposed in this paper. The proposed scheme overcomes all drawbacks of their scheme and also provides additional features such as user unlinkability, user anonymity, efficient password, and biometric update. The rigorous informal and formal security analysis using random oracle models and the mostly acceptable Automated Validation of Internet Security Protocols and Applications tool is also performed. During the experimentation, it has been observed that the proposed scheme is secure against various known attacks that include replay and man-in-the-middle attacks. Furthermore, the analysis of computation and communication cost estimation of the proposed scheme depicts that our scheme is efficient as compared with other related exiting schemes. Copyright Ìâå© 2016 John Wiley & Sons, Ltd. Copyright Ìâå© 2016 John Wiley & Sons, Ltd.",AVISPA; provable security; smart card; telecare medicine information systems; user anonymity; user authentication,Article,Scopus,2-s2.0-84958612449
"Wassilew S., Urbas L., Ladiges J., Fay A., Holm T.",Transformation of the NAMUR MTP to OPC UA to allow plug and produce for modular process automation,2016,"IEEE International Conference on Emerging Technologies and Factory Automation, ETFA",2016-November,,7733749,,,no,,,,10.1109/ETFA.2016.7733749,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996567187&doi=10.1109%2fETFA.2016.7733749&partnerID=40&md5=d9740e4e16367f6aa61ee50c3cc97b8d,"Technische UniversitÌ_t Dresden, Institute of Automation, Germany; Helmut-Schmidt-University, Institute of Automation Technology, Hamburg, Germany; Wago Kontakttechnik, Minden, Germany","Wassilew, S., Technische UniversitÌ_t Dresden, Institute of Automation, Germany; Urbas, L., Technische UniversitÌ_t Dresden, Institute of Automation, Germany; Ladiges, J., Helmut-Schmidt-University, Institute of Automation Technology, Hamburg, Germany; Fay, A., Helmut-Schmidt-University, Institute of Automation Technology, Hamburg, Germany; Holm, T., Wago Kontakttechnik, Minden, Germany","Modularization is considered as one enabler for flexible and highly reconfigurable process plants. These characteristics are needed to overcome deficiencies regarding market volatility and shorter product innovation cycles. Current standardization activities aim at the specification of Module Type Package (MTP) files as a semantic description of modules for fast and efficient integration into process control systems. To increase the plug and produce character of the approach, online discovery for modules has to be enabled. OPC UA is considered as a suitable industrial communication technology for the communication between modules and process control systems. Therefore, this paper presents an approach for the representation of MTPs in OPC UA in order to enable the online discovery of process modules. å© 2016 IEEE.",decentralized intelligence; modular process automation; Module Type Package; NAMUR MTP; OPC UA,Conference Paper,Scopus,2-s2.0-84996567187
"Wang J., Wang S., Cui Q., Wang Q.",Local-based active classification of test report to assist crowdsourced testing,2016,ASE 2016 - Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering,,,,190,201,no,,,,10.1145/2970276.2970300,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989154013&doi=10.1145%2f2970276.2970300&partnerID=40&md5=8749b68a84d52bce8cb3e56ccbc7d5b0,"Laboratory for Internet Software Technologies, Institute of Software Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Science, Institute of Software Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; Electrical and Computer Engineering, University of Waterloo, Canada","Wang, J., Laboratory for Internet Software Technologies, Institute of Software Chinese Academy of Sciences, Beijing, China, University of Chinese Academy of Sciences, Beijing, China; Wang, S., Electrical and Computer Engineering, University of Waterloo, Canada; Cui, Q., Laboratory for Internet Software Technologies, Institute of Software Chinese Academy of Sciences, Beijing, China, University of Chinese Academy of Sciences, Beijing, China; Wang, Q., Laboratory for Internet Software Technologies, Institute of Software Chinese Academy of Sciences, Beijing, China, State Key Laboratory of Computer Science, Institute of Software Chinese Academy of Sciences, Beijing, China, University of Chinese Academy of Sciences, Beijing, China","In crowdsourced testing, an important task is to identify the test reports that actually reveal fault true fault, from the large number of test reports submitted by crowd workers. Most existing approaches towards this problem utilized supervised machine learning techniques, which often require users to manually label a large amount of training data. Such process is time-consuming and labor-intensive. Thus, reducing the onerous burden of manual labeling while still being able to achieve good performance is crucial. Active learning is one potential technique to address this challenge, which aims at training a good classifier with as few labeled data as possible. Nevertheless, our observation on real industrial data reveals that existing active learning approaches generate poor and unstable performances on crowdsourced testing data. We analyze the deep reason and find that the dataset has significant local biases. To address the above problems, we propose LOcal-based Active ClassiFication (LOAF) to classify true fault from crowdsourced test reports. LOAF recommends a small portion of instances which are most informative within local neighborhood, and asks user their labels, then learns classifiers based on local neighborhood. Our evaluation on 14,609 test reports of 34 commercial projects from one of the Chinese largest crowdsourced testing platforms shows that our proposed LOAF can generate promising results. In addition, its performance is even better than existing supervised learning approaches which built on large amounts of labelled historical data. Moreover, we also implement our approach and evaluate its usefulness using real-world case studies. The feedbacks from testers demonstrate its practical value. å© 2016 ACM.",Active Learning; Crowdsourced Testing; Test Report Classification,Conference Paper,Scopus,2-s2.0-84989154013
"Wang J., Qiao F., Zhao F., Sutherland J.W.",A Data-Driven Model for Energy Consumption in the Sintering Process,2016,"Journal of Manufacturing Science and Engineering, Transactions of the ASME",138,10,101001,,,no,,,,10.1115/1.4033661,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976309718&doi=10.1115%2f1.4033661&partnerID=40&md5=9a23b418f89254ae60a2dd67b58f9a96,"School of Electronics and Information Engineering, Tongji University, 4800 Cao'an Road, Jiading District, Shanghai, China; School of Mechanical Engineering, Purdue University, West Lafayette, IN, United States; Environmental and Ecological Engineering, Purdue University, West Lafayette, IN, United States","Wang, J., School of Electronics and Information Engineering, Tongji University, 4800 Cao'an Road, Jiading District, Shanghai, China; Qiao, F., School of Electronics and Information Engineering, Tongji University, 4800 Cao'an Road, Jiading District, Shanghai, China; Zhao, F., School of Mechanical Engineering, Purdue University, West Lafayette, IN, United States, Environmental and Ecological Engineering, Purdue University, West Lafayette, IN, United States; Sutherland, J.W., Environmental and Ecological Engineering, Purdue University, West Lafayette, IN, United States","As environmental performance becomes increasingly important, the sintering process is receiving more attention since it consumes large amounts of energy. This paper proposes a data-driven model for sintering energy consumption, which considers both model accuracy and time efficiency. The proposed model begins with removing data anomalies using a local outlier factor (LOF) algorithm and an attribute selection module using the RReliefF method. Then, to accurately predict sintering energy consumption, an integrated predictive model is employed that uses bagging-enhanced extreme learning machine (ELM) and support vector regression (SVR) machine, combined with an entropy weight method. A case study is used to demonstrate the effectiveness of the proposed model using actual production data for a year. Results show that the proposed model outperforms other models and is computationally efficient. Optimal parameters of the LOF (1.3) and number of attributes (30) were identified. It was found that coke powder has the most significant impact on the solid energy consumption (SEC), while cooling water flow rate provides the most significant impact on the gas energy consumption (GEC) within each recorded attribute variation. Parametric analysis further revealed the relationships between energy consumption and the significant attributes mentioned above. It is suggested that the proposed model could effectively reduce the energy consumption by attaining more efficient attribute settings. å© Copyright 2016 by ASME.",energy consumption; integrated prediction; sintering; sustainable manufacturing,Article,Scopus,2-s2.0-84976309718
Wang J.,Estimating system of project cost based on neural network,2015,"Proceedings - 2015 International Conference on Intelligent Transportation, Big Data and Smart City, ICITBS 2015",,,7384049,392,395,no,,,,10.1109/ICITBS.2015.103,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963831411&doi=10.1109%2fICITBS.2015.103&partnerID=40&md5=f61e5ae0d520630c50d669bcbd5edb87,"Xinjiang Corps Xingxin Polytechnic, Urumqi, China","Wang, J., Xinjiang Corps Xingxin Polytechnic, Urumqi, China","Today, the subway has become an important means of transport. However, the huge construction project of subway has not yet formed a complete system in the budget. Taking Guangzhou Metro Line as an example, by BP neural network model, two evaluation model about station cost fees and range expenses were established. At the station costs, take the cost the station and extending into account. In the range of costs, take the range cost and the extending fees into account. In addition, the cost of subway basic data was normalized to the benefits of distributed data. The model will help to quickly conduct the subway cost budget evaluation. å© 2016 IEEE.",BP; Construction; Cost budget; Extending fee; Neural network model; Subway,Conference Paper,Scopus,2-s2.0-84963831411
"Wang D.-D., Wang Q.",Improving the performance of defect prediction based on evolution data,2016,Ruan Jian Xue Bao/Journal of Software,27,12,,3014,3029,no,Defect prediction,,,10.13328/j.cnki.jos.004869,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006304301&doi=10.13328%2fj.cnki.jos.004869&partnerID=40&md5=b8e449ce0ca38dd177a69c318a3ea8e7,"Laboratory for Internet Software Technologies, Institute of Software, The Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Science, Institute of Software, The Chinese Academy of Sciences, Beijing, China","Wang, D.-D., Laboratory for Internet Software Technologies, Institute of Software, The Chinese Academy of Sciences, Beijing, China; Wang, Q., Laboratory for Internet Software Technologies, Institute of Software, The Chinese Academy of Sciences, Beijing, China, State Key Laboratory of Computer Science, Institute of Software, The Chinese Academy of Sciences, Beijing, China","It is an undisputed fact that software continues to evolve. Software evolution is caused by requirement changes which often result in injection of defects. Existing defect prediction techniques mainly focus on utilizing the attributes of software work products, such as documents, source codes and test cases, to predict defects. Consider an evolving software as a species and its development process as a natural species' evolutionary process, the injection of defects may have the characters of a species and will be impacted by its evolution. A great many of researchers have studied the process of software evolution and proposed some evolution related metrics. In this study, a set of new metrics is first proposed based on evolutionary history to characterize software evolution process, and then a case study on building defect prediction models is presented. Experiments on six well-known open source projects achieved good performance, demonstrating the effectiveness of the proposed metrics. å© Copyright 2016, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Defect prediction; Evolution metrics; Software evolution,Article,Scopus,2-s2.0-85006304301
"Wang C.-C., Yang C.-H., Wang C.-S., Chang T.-R., Yang K.-J.",Feature recognition and shape design in sneakers,2016,Computers and Industrial Engineering,102,,,408,422,no,,,,10.1016/j.cie.2016.05.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973513694&doi=10.1016%2fj.cie.2016.05.003&partnerID=40&md5=b87f0b2d411c4fba8bf82c3561eede65,"Department of Multimedia and Game Science, Chung-Chou University of Science and Technology, Chung-Hwa, Taiwan; Department of Industrial Design, Tung-Hai University, Taichung, Taiwan; Department of Industrial Management, Nan-Kai University of TechnologyNan-Tou, Taiwan","Wang, C.-C., Department of Multimedia and Game Science, Chung-Chou University of Science and Technology, Chung-Hwa, Taiwan; Yang, C.-H., Department of Industrial Design, Tung-Hai University, Taichung, Taiwan; Wang, C.-S., Department of Industrial Design, Tung-Hai University, Taichung, Taiwan; Chang, T.-R., Department of Industrial Management, Nan-Kai University of TechnologyNan-Tou, Taiwan; Yang, K.-J., Department of Industrial Design, Tung-Hai University, Taichung, Taiwan","Due to the improvements associated with our modern lifestyle, both personal preferences and commercial market value should be considered when undertaking the challenge of new product development. Thus, a crucial research topic is how to design a customized product for consumers. In this study, a feature recognition and shape-design process for basketball sneakers was established that integrated Kansei engineering and artificial neural networks (ANNs), in combination with famous point guards in the National Basketball Association (NBA), as the basis for constructing a topological feature map for sneakers. We used questionnaires to get 20 fans in NBA games to assign a rating value in Kansei adjectives to evaluate 50 basketball shoes and to identify the top 10 NBA point guards. The fans‰Ûª perspectives combined the psychological and physical scales for the top 10 point guards to establish a gross relationship between sneakers and the point guards. Based on competitive learning in the self-organizing map (SOM), the similarities of the inputs from the Kansei perspective concerning NBA point guards and sneakers can be reduced in their dimensions and grouped into six clusters. A back propagation network (BPN) is proposed to verify the mapping of sneakers onto point guards. The results were validated by comparing them with the target neurons based on SOM feature map. A MATLAB program was developed using SOM and BPN algorithms to verify the groups of sneakers accompanying the point guards. Then, the system can categorize the sneakers that match the players. A feature-based, shape-morphing process for the design of a new style of sneaker was implemented. A model to blend the features was constructed in SolidWorks CAD by choosing any two different shapes from the SOM map. This research was used to add the vast variety of shapes and design of sneakers from the selected SOM feature map to help designers create many different styles in a short period of time. å© 2016 Elsevier Ltd",Back propagation network; Feature recognition; Kansei engineering; Self-organizing map; Shape design; Sneakers,Article,Scopus,2-s2.0-84973513694
"WÌ¦lfl A., Siegmund N., Apel S., Kosch H., Krautlager J., Weber-Urbina G.",Generating qualifiable avionics software: An experience report,2015,"Proceedings - 2015 30th IEEE/ACM International Conference on Automated Software Engineering, ASE 2015",,,7372061,726,736,no,,,1,10.1109/ASE.2015.35,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963811411&doi=10.1109%2fASE.2015.35&partnerID=40&md5=f188481f19640bf1408db99bfad6fa97,"University of Passau, Germany; Airbus Helicopters S.A.S., Germany","WÌ¦lfl, A., University of Passau, Germany; Siegmund, N., University of Passau, Germany; Apel, S., University of Passau, Germany; Kosch, H., University of Passau, Germany; Krautlager, J., Airbus Helicopters S.A.S., Germany; Weber-Urbina, G., Airbus Helicopters S.A.S., Germany","We report on our experience with enhancing the data-management component in the avionics software of the NH90 helicopter at Airbus Helicopters. We describe challenges regarding the evolution of avionics software by means of real-world evolution scenarios that arise in industrial practice. A key role plays a legally-binding certification process, called qualification, which is responsible for most of the development effort and cost. To reduce effort and cost, we propose a novel generative approach to develop qualifiable avionics software by combining model-based and product-line technology. Using this approach, we have already generated code that is running on the NH90 helicopter and that is in the process of replacing the current system code. Based on an interview with two professional developers at Airbus and an analysis of the software repository of the NH90, we systematically compare our approach with established development approaches in the avionics domain, in terms of implementation and qualification effort. å© 2015 IEEE.",,Conference Paper,Scopus,2-s2.0-84963811411
"Vogel-Heuser B., Feldmann S., Folmer J., Rosch S., Heinrich R., Rostami K., Reussner R.",Architecture-Based Assessment and Planning of Software Changes in Information and Automated Production Systems State of the Art and Open Issues,2015,"Proceedings - 2015 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2015",,,7379262,687,694,no,,,,10.1109/SMC.2015.130,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964433268&doi=10.1109%2fSMC.2015.130&partnerID=40&md5=e4f3c06ab84c9f397d897f1cc00d4aad,"Institute of Automation and Information Systems, Technische UniversitÌ_t MÌ¦nchen, Garching near Munich, Germany; Institute for Program Structures and Data Organization, Karlsruhe Institute of Technology, Karlsruhe, Germany","Vogel-Heuser, B., Institute of Automation and Information Systems, Technische UniversitÌ_t MÌ¦nchen, Garching near Munich, Germany; Feldmann, S., Institute of Automation and Information Systems, Technische UniversitÌ_t MÌ¦nchen, Garching near Munich, Germany; Folmer, J., Institute of Automation and Information Systems, Technische UniversitÌ_t MÌ¦nchen, Garching near Munich, Germany; Rosch, S., Institute of Automation and Information Systems, Technische UniversitÌ_t MÌ¦nchen, Garching near Munich, Germany; Heinrich, R., Institute for Program Structures and Data Organization, Karlsruhe Institute of Technology, Karlsruhe, Germany; Rostami, K., Institute for Program Structures and Data Organization, Karlsruhe Institute of Technology, Karlsruhe, Germany; Reussner, R., Institute for Program Structures and Data Organization, Karlsruhe Institute of Technology, Karlsruhe, Germany","Information and automated production systems are long-living, evolvable systems. Consequently, modifications are performed to correct, improve or adapt the respective system. We introduce different approaches to analyze maintainability of software-intensive systems and propose two different case studies from the information and from the automated production systems domain as a basis to validate approaches on system evolution. å© 2015 IEEE.",Architecture; Evolution; Modeling; Modularity,Conference Paper,Scopus,2-s2.0-84964433268
Vitharana P.,Defect propagation at the project-level: results and a post-hoc analysis on inspection efficiency,2017,Empirical Software Engineering,22,1,,57,79,no,,,,10.1007/s10664-015-9415-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947744892&doi=10.1007%2fs10664-015-9415-3&partnerID=40&md5=cfcadb821029344ba841735a3707f069,"Whitman School of Management, Syracuse University, Syracuse, NY, United States","Vitharana, P., Whitman School of Management, Syracuse University, Syracuse, NY, United States","Inspections are increasingly utilized to enhance software quality. While the effectiveness of inspections in uncovering defects is widely accepted, there is a lack of research that takes a more holistic approach by considering defect counts from initial phases of the development process (requirements, design, and coding) and examining defect propagation where defect counts are aggregated to the project-level (i.e., application-level). Using inspection data collected from a large software development firm, this paper investigates the extent of defect propagation at the project-level during early lifecycle phases. I argue that defect propagation can be observed from the relationship between defects in the prior phase and the defects in the subsequent phase. Both Ordinary Least Squares and 3-Stage Least Squares analyses support the hypotheses on defect propagation. Moreover, results show that the inspection efficiency (defects per unit inspection time) decreases as the software product progresses from requirements to design to coding. A post-hoc analysis revealed further insights into inspection efficiency. In each phase, as the inspection time increased, efficiency reached an optimal point and then dropped off. In addition, a project‰Ûªs inspection efficiency generally tends to remain stable from one phase to another. These insights offer managers means to assess inspections, their efficiency, and make adjustments to the time allotted to inspect project‰Ûªs artifacts in both the current and the subsequent phase. Implications for managers and future research directions are discussed. å© 2015, Springer Science+Business Media New York.",Coding; Defect propagation; Design; Inspections; Requirements; Software quality,Article,Scopus,2-s2.0-84947744892
"Vishnoi U., Meixner M., Noll T.G.",A Family of Modular QRD-Accelerator Architectures and Circuits Cross-Layer Optimized for High Area- and Energy-Efficiency,2016,Journal of Signal Processing Systems,83,3,,329,356,no,,,,10.1007/s11265-015-0976-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923059518&doi=10.1007%2fs11265-015-0976-6&partnerID=40&md5=50a98f0c8af83b9e25dc59c5229c1808,"Chair of Electrical Engineering and Computer Systems, RWTH Aachen University, Schinkelstrasse 2, Aachen, Germany","Vishnoi, U., Chair of Electrical Engineering and Computer Systems, RWTH Aachen University, Schinkelstrasse 2, Aachen, Germany; Meixner, M., Chair of Electrical Engineering and Computer Systems, RWTH Aachen University, Schinkelstrasse 2, Aachen, Germany; Noll, T.G., Chair of Electrical Engineering and Computer Systems, RWTH Aachen University, Schinkelstrasse 2, Aachen, Germany","QR-decomposition accelerators are attractive SoC components for many applications with a wide range of specifications. A new family of highly area- and energy-efficient, modular two-way linear-array QRD architectures based on the Givens algorithm and CORDIC rotations is proposed. The template architecture allows for implementations of real-/complex-valued and integer/floating-point QRDs. An accurate algebraic cost model enables cross-layer optimization over architecture, micro-architecture and circuit level using a rich set of parameters. Quantitative results for exemplary applications are presented for implementations in 40-nm CMOS, proving the significant improvement of efficiency. å© 2015, Springer Science+Business Media New York.",Cross-layer optimization; Design space exploration; Early cost estimation; Matrix decomposition; QR-decomposition; QRD accelerators,Article,Scopus,2-s2.0-84923059518
Valdes-Souto F.,Creating a historical database for estimation using the EPCU approximation approach for COSMIC (ISO 19761),2016,"Proceedings - 2016 4th International Conference in Software Engineering Research and Innovation, CONISOFT 2016",,,7477926,159,166,no,,,,10.1109/CONISOFT.2016.32,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978253145&doi=10.1109%2fCONISOFT.2016.32&partnerID=40&md5=9c587b32f19a52e7ccd7408017779db0,"Science Faculty, National Autonomous University of Mexico (UNAM), CDMX Mexico City, Mexico","Valdes-Souto, F., Science Faculty, National Autonomous University of Mexico (UNAM), CDMX Mexico City, Mexico","In the actual competitive software industry, software development organizations need a better and formal estimation approaches in order to increase the success rate of software projects. However, currently the estimation approach typically employed in industry is the expert judgment ('experience-based'). Using measures and estimations that continue to be based on researchers' intuition does not contribute to obtaining successful projects nor to mature software engineering. Any organization that aims to have or develop estimation approaches needs historical data, and the majority of the organizations do not have this information. Additionally acquisition of this information has expensive cost and time/effort consuming. This paper proposed the use of the EPCU approximation approach-that has demonstrated a several benefits-aiming to create a formal database with low cost and effort, that can be employed as a starting point to improve estimations in organizations without historical databases. å© 2016 IEEE.",Approximate Sizing; COSMIC ISO 19761; EPCU Model; Estimation database; FSM; Functional Size,Conference Paper,Scopus,2-s2.0-84978253145
"Vahdani B., Razavi F., Mousavi S.M.",A high performing meta-heuristic for training support vector regression in performance forecasting of supply chain,2016,Neural Computing and Applications,27,8,,2441,2451,no,,,,10.1007/s00521-015-2015-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939624334&doi=10.1007%2fs00521-015-2015-8&partnerID=40&md5=35f63c95073f0997ed1f83a6b11fc794,"Faculty of Industrial and Mechanical Engineering, Qazvin Branch, Islamic Azad University, Qazvin, Iran; Department of Mechanical and Industrial Engineering, University of Toronto, Toronto, Canada; Industrial Engineering Department, Faculty of Engineering, Shahed University, Tehran, Iran","Vahdani, B., Faculty of Industrial and Mechanical Engineering, Qazvin Branch, Islamic Azad University, Qazvin, Iran; Razavi, F., Department of Mechanical and Industrial Engineering, University of Toronto, Toronto, Canada; Mousavi, S.M., Industrial Engineering Department, Faculty of Engineering, Shahed University, Tehran, Iran","The prevalence of the use of third-party logistics (3PL) providers is noticeable. The complexity of the relationships pertinent to 3PL is greater than that of any traditional logistics supplier relationships. Moreover, they can be considered as truly strategic alliances. The use of the mentioned relationships to increase the flexibility of the organization to address the rapid changes occurring in market conditions has become popular while these relationships concentrate on the core competencies as well as the development of long-term growth strategies. A good number of studies have examined the selection of service providers. With respect to the selection of the service providers, the most recent studies approved the better performance of neural networks in comparison with the conventional methods to provide a solution for the real-world engineering problems, one of the sociopolitically inspired optimization strategies named imperialist competitive algorithm (ICA) is used. In order to select the 3PL, integration of the support vector regression (SVR) and self-adaptive ICA (SAICA) has offered a novel model, in which SAICA is utilized to adjust the parameters of the SVR. The suggested model is applied for cosmetics production. Moreover, the comparison of the suggested model and back-propagation neural networks, pure SVR, and ICA‰ÛÒSVR is presented. Higher estimation accuracy is achieved as the results of the proposed model reveal, which leads to the effective prediction. å© 2015, The Natural Computing Applications Forum.",Cosmetics production; Imperialist competitive algorithm (ICA); Supply chain; Support vector regression,Article,Scopus,2-s2.0-84939624334
"Ur Rahman U., Hakeem O., Raheem M., Bilal K., Khan S.U., Yang L.T.",Nutshell: Cloud simulation and current trends,2015,"Proceedings - 2015 IEEE International Conference on Smart City, SmartCity 2015, Held Jointly with 8th IEEE International Conference on Social Computing and Networking, SocialCom 2015, 5th IEEE International Conference on Sustainable Computing and Communications, SustainCom 2015, 2015 International Conference on Big Data Intelligence and Computing, DataCom 2015, 5th International Symposium on Cloud and Service Computing, SC2 2015",,,7463705,77,86,no,,,,10.1109/SmartCity.2015.51,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973865951&doi=10.1109%2fSmartCity.2015.51&partnerID=40&md5=ebc817dc7661a85b686cf38e4efb177c,"Department of Computer Science, COMSATS Institute of Information Technology, Abbottabad, Pakistan; Department of Electrical and Computer Engineering, North Dakota State University, Fargo, ND, United States; Department of Computer Science, St. Francis Xavier University, Antigonish, Canada","Ur Rahman, U., Department of Computer Science, COMSATS Institute of Information Technology, Abbottabad, Pakistan; Hakeem, O., Department of Computer Science, COMSATS Institute of Information Technology, Abbottabad, Pakistan; Raheem, M., Department of Computer Science, COMSATS Institute of Information Technology, Abbottabad, Pakistan; Bilal, K., Department of Computer Science, COMSATS Institute of Information Technology, Abbottabad, Pakistan; Khan, S.U., Department of Electrical and Computer Engineering, North Dakota State University, Fargo, ND, United States; Yang, L.T., Department of Computer Science, St. Francis Xavier University, Antigonish, Canada","Cloud computing has experienced enormous popularity and adoption in many areas, such as research, medical, web, and e-commerce. Providers, like Amazon, Google, Microsoft, and Yahoo have deployed their cloud services for use. Cloud computing pay-as-you-go model, on demand scaling, and low maintenance cost has attracted many users. The widespread adoption of cloud paradigm upshots various challenges. The legacy data center and cloud architectures are unable to handle the escalating user demands. Therefore, new data center network architectures, policies, protocols and topologies are required. However, new solutions must be tested thoroughly, before deployment within a real production environment. As the experimentation and testing is infeasible in the production environment and real cloud setup, therefore, there is an indispensable need for simulation tools that provide ways to model and test applications, and estimate cost, performance, and energy consumption of services and application within cloud environment. Simulation tools providing cloud simulation environments currently are limited in terms of features and realistic cloud setups, focus on a particular problem domain, and require tool-specific modeling, which can be frustrating and time consuming. This paper aims to provide a detailed comparison of various cloud simulators, discuss various offered features, and highlight their strengths and limitations. Moreover, we also demonstrate our work on a new cloud simulator ""Nutshell"", which offers realistic cloud environments and protocols. The Nutshell is designed to diminish flaws and limitations of available cloud simulators, by offering: (a) multiple datacenter network architectures, like three-tier, fat-tree, and dcell, (b) fine grained network details, (c) realistic cloud traffic patterns, (d) congestion control strategies and analysis, (e) energy consumption, (f) cost estimation, and (g) data center monitoring and analysis. Flexibility to stretch the architectures to simulate smart city IT infrastructure. å© 2015 IEEE.",Cloud computing; Cloud simulator; Datacenter architectures; Datacenter congestion control; Energy models; Simulation; Smart city; Traffic patterns,Conference Paper,Scopus,2-s2.0-84973865951
"Unterkalmsteiner M., Gorschek T., Feldt R., Lavesson N.",Large-scale information retrieval in software engineering - an experience report from industrial application,2016,Empirical Software Engineering,21,6,,2324,2365,no,,,,10.1007/s10664-015-9410-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946763269&doi=10.1007%2fs10664-015-9410-8&partnerID=40&md5=69007167c2520dffd075053ca5ed261c,"Department of Software Engineering, Blekinge Institute of Technology, ValhallavÌ_gen, Karlskrona, Sweden; Department of Computer Science and Engineering, Blekinge Institute of Technology, ValhallavÌ_gen 1, Karlskrona, Sweden","Unterkalmsteiner, M., Department of Software Engineering, Blekinge Institute of Technology, ValhallavÌ_gen, Karlskrona, Sweden; Gorschek, T., Department of Software Engineering, Blekinge Institute of Technology, ValhallavÌ_gen, Karlskrona, Sweden; Feldt, R., Department of Software Engineering, Blekinge Institute of Technology, ValhallavÌ_gen, Karlskrona, Sweden; Lavesson, N., Department of Computer Science and Engineering, Blekinge Institute of Technology, ValhallavÌ_gen 1, Karlskrona, Sweden","Software Engineering activities are information intensive. Research proposes Information Retrieval (IR) techniques to support engineers in their daily tasks, such as establishing and maintaining traceability links, fault identification, and software maintenance. We describe an engineering task, test case selection, and illustrate our problem analysis and solution discovery process. The objective of the study is to gain an understanding of to what extent IR techniques (one potential solution) can be applied to test case selection and provide decision support in a large-scale, industrial setting. We analyze, in the context of the studied company, how test case selection is performed and design a series of experiments evaluating the performance of different IR techniques. Each experiment provides lessons learned from implementation, execution, and results, feeding to its successor. The three experiments led to the following observations: 1) there is a lack of research on scalable parameter optimization of IR techniques for software engineering problems; 2) scaling IR techniques to industry data is challenging, in particular for latent semantic analysis; 3) the IR context poses constraints on the empirical evaluation of IR techniques, requiring more research on developing valid statistical approaches. We believe that our experiences in conducting a series of IR experiments with industry grade data are valuable for peer researchers so that they can avoid the pitfalls that we have encountered. Furthermore, we identified challenges that need to be addressed in order to bridge the gap between laboratory IR experiments and real applications of IR in the industry. å© 2015, Springer Science+Business Media New York.",Data mining; Experiment; Information retrieval; Test case selection,Article,Scopus,2-s2.0-84946763269
"Ulonska S., Welo T.",On the use of product portfolio and variant maps as visualization tools to support platform-based development strategies,2016,Concurrent Engineering Research and Applications,24,3,,211,226,no,,,,10.1177/1063293X16654531,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983055625&doi=10.1177%2f1063293X16654531&partnerID=40&md5=1e0295bcb114650afc81dd1044850d6e,"Department of Engineering Design and Materials, Norwegian University of Science and Technology, Trondheim, Norway","Ulonska, S., Department of Engineering Design and Materials, Norwegian University of Science and Technology, Trondheim, Norway; Welo, T., Department of Engineering Design and Materials, Norwegian University of Science and Technology, Trondheim, Norway","Current implementation methodologies for platform-based development strategies presume that product variants are made clear and explicit prior to application. However, this might not be the case in many engineer-to-order companies where product variants typically emerge from individual developments over time, commonly without a predefined product strategy. Also, information related to different product variants is usually spread across multiple sources. This makes it difficult to identify the information on which a future configure-to-order strategy can be built. In this article, a systematic, visual approach to identify, analyze, and structure product variant information has been developed. A real-world, proof-of-concept demonstrator case has been defined to verify the proposed modeling approach applied to the unstructured product portfolio of a small- and medium-sized enterprise company that manufactures road signpost masts and gantries. The outcome is a product portfolio map that visualizes the product variants in the product family side by side, and a variant map that provides a closer overview of functional structure and related physical variants. Hence, these two maps combined provide a systemized overview of the product variants at different abstraction levels as a knowledge source. It is concluded that this study has demonstrated the potential of a step-by-step methodology for preparing a dispersed product portfolio as a basis for the implementation of a configure-to-order strategy. å© The Author(s) 2016.",configure-to-order strategy; product platform; product portfolio mapping; product variant structuring; variant identification,Article,Scopus,2-s2.0-84983055625
"Ul-Haq A., Cecati C., Al-Ammar E.A.",Modeling of a photovoltaic-powered electric vehicle charging station with vehicle-to-grid implementation,2017,Energies,10,1,4,,,no,,,,10.3390/en10010004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009289279&doi=10.3390%2fen10010004&partnerID=40&md5=ed661e3a66205d052eabec8ef3876c88,"DSIM, University of L'Aquila, L'Aquila, Italy; College of E and ME, National University of Science and Technology (NUST), H-12, Islamabad, Pakistan; Department of Electrical Engineering, King Saud University, Riyadh, Saudi Arabia","Ul-Haq, A., DSIM, University of L'Aquila, L'Aquila, Italy, College of E and ME, National University of Science and Technology (NUST), H-12, Islamabad, Pakistan; Cecati, C., DSIM, University of L'Aquila, L'Aquila, Italy; Al-Ammar, E.A., Department of Electrical Engineering, King Saud University, Riyadh, Saudi Arabia","This paper is aimed at modelling of a distinct smart charging station for electric vehicles (EVs) that is suitable for DC quick EV charging while ensuring minimum stress on the power grid. Operation of the charging station is managed in such a way that it is either supplied by photovoltaic (PV) power or the power grid, and the vehicle-to-grid (V2G) is also implemented for improving the stability of the grid during peak load hours. The PV interfaced DC/DC converter and grid interfaced DC/AC bidirectional converter share a DC bus. A smooth transition of one operating mode to another demonstrates the effectiveness of the employed control strategy. Modelling and control of the different components are explained and are implemented in Simulink. Simulations illustrate the feasible behaviour of the charging station under all operating modes in terms of the four-way interaction among PV, EVs and the grid along with V2G operation. Additionally, a business model is discussed with comprehensive analysis of cost estimation for the deployment of charging facilities in a residential area. It has been recognized that EVs bring new opportunities in terms of providing regulation services and consumption flexibility by varying the recharging power at a certain time instant. The paper also discusses the potential financial incentives required to inspire EV owners for active participation in the demand response mechanism.",Electric vehicles; Power management; PV power; Vehicle-to-grid,Article,Scopus,2-s2.0-85009289279
"Tyagi S., Agrawal S., Yang K., Ying H.",An extended Fuzzy-AHP approach to rank the influences of socialization-externalization-combination-internalization modes on the development phase,2017,Applied Soft Computing Journal,52,,,505,518,no,,,,10.1016/j.asoc.2016.10.017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007011596&doi=10.1016%2fj.asoc.2016.10.017&partnerID=40&md5=5eb5f7922fdb5998f20fc2f4b9c6a2c2,"Department of Industrial and Systems Engineering, Wayne State University, Detroit, MI, United States; VISA Inc., Austin, TX, United States; Department of Electrical and Computer Engineering, Wayne State University, Detroit, MI, United States","Tyagi, S., Department of Industrial and Systems Engineering, Wayne State University, Detroit, MI, United States; Agrawal, S., VISA Inc., Austin, TX, United States; Yang, K., Department of Industrial and Systems Engineering, Wayne State University, Detroit, MI, United States; Ying, H., Department of Electrical and Computer Engineering, Wayne State University, Detroit, MI, United States","Continuous innovation intended to deliver products with new attributes is an imperative driver for organizations to remain competitive in today's fast changing market. A successful innovation is often associated with adoption and execution of all SECI (socialization-externalization-combination-internalization) modes of knowledge creation within any Product Development (PD) phase. This article is an attempt to argue with the general notion and to distinguish different PD phases‰Ûª affinity corresponding to distinct SECI modes. In this regard, the paper proposes an extended Fuzzy Analytic Hierarchy Process (EFAHP) approach to determine the ranking in which any PD phase is influenced from SECI modes. In the EFAHP approach, the complex problem of knowledge creation is first itemized into a simple hierarchical structure for pairwise comparisons. Next, a triangular fuzzy number (TFN) concept is applied to capture the inherent vagueness in linguistic terms of a decision-maker. This paper recommends mapping TFNs with normal distributions about X-axis. This allows us to develop a mathematical formulation to estimate the degree of possibility (importance value) when two TFNs do not intersect with each other (current techniques in literature calculate this value as zero). In order to demonstrate the applicability and usefulness of the proposed EFAHP in ranking SECI modes, an empirical study of development phase is considered. Five criteria and their 19 sub-criteria for measuring the phase's performance are identified based on both an extensive discussion with subject matter experts and rigorous literature survey. After stringent analysis, we found that the mode that highly influenced the development phase is ‰Û÷combination mode‰Ûª. The article discusses the application of lean tools that can be employed to improve the knowledge creation process. å© 2016 Elsevier B.V.",Fuzzy-AHP; Knowledge creation; Lean thinking; Product development,Article,Scopus,2-s2.0-85007011596
"Turek M., Werewka J., Paâka D.",The scrum pricing model for developing a common software framework in a multi-project environment,2017,Advances in Intelligent Systems and Computing,521,,,117,132,no,,,,10.1007/978-3-319-46583-8_10,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990050129&doi=10.1007%2f978-3-319-46583-8_10&partnerID=40&md5=6b8f36cf2777ab23578b71b35a299aed,"AGH University of Science and Technology, Krakow, Poland","Turek, M., AGH University of Science and Technology, Krakow, Poland; Werewka, J., AGH University of Science and Technology, Krakow, Poland; Paâka, D., AGH University of Science and Technology, Krakow, Poland","The article presents a new pricing model applied in Scrum contracting. The model will introduce an innovative approach to the promotion of software products with reference to their pricing‰ÛÓbounding non-profit features with the classical cost estimation. It will assume a principle of single charge for all components reused in products created for different customers. The model will be applicable mainly in multi-project Scrum environments, but will also support scaled multi-team Scrum development environments (where software development work is allocated to different Scrum teams). It will introduce a complete mechanism for cost estimation, with component pricing procedures (compatible with Scrum estimation methodologies), component reuse tracking and so on. The price offered for software to be developed will depend on its popularity among customers. So, the introduction of a new feature triggered by one customer can be beneficial for all parties taking part in the framework development, with justified share of the price. In such situations a common base for software development pricing techniques must be established. It will depend on different factors, such as: team effort, team velocities, resources and technologies used. To express these values a reference currency will be defined. The model, along with pricing mechanisms presented in the article, can be effectively used in many software products in which software evolution is a core. The pricing efficiency, commonly recognized as a feature of the model, will encourage potential customers to choose a software company that uses it. å© Springer International Publishing AG 2017.",Agile software development; Outsourcing; Scrum; Scrum contracting; Software development,Conference Paper,Scopus,2-s2.0-84990050129
"TriadÌ_-Aymerich J., Ferrer-MartÌ_ L., GarcÌ_a-Villoria A., Pastor R.",MILP-based heuristics for the design of rural community electrification projects,2016,Computers and Operations Research,71,,,90,99,no,,,,10.1016/j.cor.2016.01.010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957565889&doi=10.1016%2fj.cor.2016.01.010&partnerID=40&md5=5a21107d5f5f981122671532efb1027d,"Escola Superior PolitÌ¬cnica de MatarÌ_, Pompeu Fabra University, C/ Ernest Lluch 32, MatarÌ_, Spain; Inst. of Industrial and Control Engineering, Universitat Politecnica de Catalunya- Barcelona Tech, Av. Diagonal 647, Barcelona, Spain","TriadÌ_-Aymerich, J., Escola Superior PolitÌ¬cnica de MatarÌ_, Pompeu Fabra University, C/ Ernest Lluch 32, MatarÌ_, Spain; Ferrer-MartÌ_, L., Inst. of Industrial and Control Engineering, Universitat Politecnica de Catalunya- Barcelona Tech, Av. Diagonal 647, Barcelona, Spain; GarcÌ_a-Villoria, A., Inst. of Industrial and Control Engineering, Universitat Politecnica de Catalunya- Barcelona Tech, Av. Diagonal 647, Barcelona, Spain; Pastor, R., Inst. of Industrial and Control Engineering, Universitat Politecnica de Catalunya- Barcelona Tech, Av. Diagonal 647, Barcelona, Spain","Wind-photovoltaic systems are a suitable option to provide electricity to isolated communities autonomously. To design these systems, there are recent mathematical models that solve the location and type of each of the electrification components and the design of the possible distribution microgrids. When the amount of demand points to electrify increases, solving the mathematical model requires a computational time that becomes infeasible in practice. To speed up the solving process, three heuristic methods based on mixed integer linear programming (MILP) are presented in this paper: Relax and Fix heuristics, heuristics based on a Corridor Method and Increasing Radius heuristics. In all algorithms first a relaxed MILP is solved to obtain a base solution and then it is used as a starting point to find a feasible solution by searching in a reduced search space. For each type of heuristic several options to relax and to reduce the solution space are developed and tested. Extensive computational experiments based on real projects are carried out and results show that the best heuristic vary according to the size of instances. å© 2016 Elsevier Ltd. All rights reserved.",Heuristics; MILP; Rural electrification; Wind-photovoltaic systems,Article,Scopus,2-s2.0-84957565889
"Trautsch F., Herbold S., Makedonski P., Grabowski J.",Adressing problems with external validity of repository mining studies through a smart data platform,2016,"Proceedings - 13th Working Conference on Mining Software Repositories, MSR 2016",,,,97,108,no,,,,10.1145/2901739.2901753,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974527552&doi=10.1145%2f2901739.2901753&partnerID=40&md5=ddc74744bb6b7e64a5929835d0a41236,"Institute for Computer Science, Georg-August-UniversitÌ_t, GÌ¦ttingen, Germany","Trautsch, F., Institute for Computer Science, Georg-August-UniversitÌ_t, GÌ¦ttingen, Germany; Herbold, S., Institute for Computer Science, Georg-August-UniversitÌ_t, GÌ¦ttingen, Germany; Makedonski, P., Institute for Computer Science, Georg-August-UniversitÌ_t, GÌ¦ttingen, Germany; Grabowski, J., Institute for Computer Science, Georg-August-UniversitÌ_t, GÌ¦ttingen, Germany","Research in software repository mining has grown considerably the last decade. Due to the data-driven nature of this venue of investigation, we identified several problems within the current state-of-the-art that pose a threat to the external validity of results. The heavy re-use of data sets in many studies may invalidate the results in case problems with the data itself are identified. Moreover, for many studies data and/or the implementations are not available, which hinders a replication of the results and, thereby, decreases the comparability between studies. Even if all information about the studies is available, the diversity of the used tooling can make their replication even then very hard. Within this paper, we discuss a potential solution to these problems through a cloud-based platform that integrates data collection and analytics. We created the prototype SmartSHARK that implements our approach. Using SmartSHARK, we collected data from several projects and created different analytic examples. Within this article, we present Smart-SHARK and discuss our experiences regarding the use of SmartSHARK and the mentioned problems. å© 2016 ACM.",Smart data; Software analytics; Software mining,Conference Paper,Scopus,2-s2.0-84974527552
"Torrecilla-Salinas C.J., SedeÌ±o J., Escalona M.J., MejÌ_as M.","Agile, Web Engineering and Capability Maturity Model Integration: A systematic literature review",2016,Information and Software Technology,71,,,92,107,no,,,3,10.1016/j.infsof.2015.11.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952802072&doi=10.1016%2fj.infsof.2015.11.002&partnerID=40&md5=69aa0fcfb3fd1618edac71c02691459c,"Department of Computer Languages and Systems, University of Seville, Spain; Agencia Andaluza de Instituciones Culturales, Junta de AndalucÌ_a, Spain","Torrecilla-Salinas, C.J., Department of Computer Languages and Systems, University of Seville, Spain; SedeÌ±o, J., Department of Computer Languages and Systems, University of Seville, Spain, Agencia Andaluza de Instituciones Culturales, Junta de AndalucÌ_a, Spain; Escalona, M.J., Department of Computer Languages and Systems, University of Seville, Spain; MejÌ_as, M., Department of Computer Languages and Systems, University of Seville, Spain","Context Agile approaches are an alternative for organizations developing software, particularly for those who develop Web applications. Besides, CMMI (Capability Maturity Model Integration) models are well-established approaches focused on assessing the maturity of an organization that develops software. Web Engineering is the field of Software Engineering responsible for analyzing and studying the specific characteristics of the Web. The suitability of an Agile approach to help organizations reach a certain CMMI maturity level in Web environments will be very interesting, as they will be able to keep the ability to quickly react and adapt to changes as long as their development processes get mature. Objective This paper responds to whether it is feasible or not, for an organization developing Web systems, to achieve a certain maturity level of the CMMI-DEV model using Agile methods. Method The proposal is analyzed by means of a systematic literature review of the relevant approaches in the field, defining a characterization schema in order to compare them to introduce the current state-of-the-art. Results The results achieved after the systematic literature review are presented, analyzed and compared against the defined schema, extracting relevant conclusions for the different dimensions of the problem: compatibility, compliance, experience, maturity and Web. Conclusion It is concluded that although the definition of an Agile approach to meet the different CMMI maturity levels goals could be possible for an organization developing Web systems, there is still a lack of detailed studies and analysis on the field. å© 2015 Elsevier B.V. All rights reserved.",Agile; CMMI; Scrum; Software Engineering; Web Engineering,Article,Scopus,2-s2.0-84952802072
"Tian Y., Ali N., Lo D., Hassan A.E.",On the unreliability of bug severity data,2016,Empirical Software Engineering,21,6,,2298,2323,no,,,,10.1007/s10664-015-9409-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945288247&doi=10.1007%2fs10664-015-9409-1&partnerID=40&md5=fc4f16dc7bdfb6779377ef5c7e3d4482,"School of Information Systems, Singapore Management University, Singapore, Singapore; Electrical and Computer Engineering, University of Waterloo, Waterloo, Canada; Software Analysis and Intelligence Lab (SAIL), Queen‰Ûªs University, Kingston, Canada","Tian, Y., School of Information Systems, Singapore Management University, Singapore, Singapore; Ali, N., Electrical and Computer Engineering, University of Waterloo, Waterloo, Canada; Lo, D., School of Information Systems, Singapore Management University, Singapore, Singapore; Hassan, A.E., Software Analysis and Intelligence Lab (SAIL), Queen‰Ûªs University, Kingston, Canada","Severity levels, e.g., critical and minor, of bugs are often used to prioritize development efforts. Prior research efforts have proposed approaches to automatically assign the severity label to a bug report. All prior efforts verify the accuracy of their approaches using human-assigned bug reports data that is stored in software repositories. However, all prior efforts assume that such human-assigned data is reliable. Hence a perfect automated approach should be able to assign the same severity label as in the repository ‰ÛÒ achieving a 100% accuracy. Looking at duplicate bug reports (i.e., reports referring to the same problem) from three open-source software systems (OpenOffice, Mozilla, and Eclipse), we find that around 51 % of the duplicate bug reports have inconsistent human-assigned severity labels even though they refer to the same software problem. While our results do indicate that duplicate bug reports have unreliable severity labels, we believe that they send warning signals about the reliability of the full bug severity data (i.e., including non-duplicate reports). Future research efforts should explore if our findings generalize to the full dataset. Moreover, they should factor in the unreliable nature of the bug severity data. Given the unreliable nature of the severity data, classical metrics to assess the accuracy of models/learners should not be used for assessing the accuracy of approaches for automated assigning severity label. Hence, we propose a new approach to assess the performance of such models. Our new assessment approach shows that current automated approaches perform well ‰ÛÒ 77-86 % agreement with human-assigned severity labels. å© 2015, Springer Science+Business Media New York.",Bug report management; Data quality; Noise prediction; Performance evaluation; Severity prediction,Article,Scopus,2-s2.0-84945288247
"Tiakas E., Papadopoulos A.N., Manolopoulos Y.",Skyline queries: An introduction,2015,"IISA 2015 - 6th International Conference on Information, Intelligence, Systems and Applications",,,7388053,,,no,,,,10.1109/IISA.2015.7388053,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963846392&doi=10.1109%2fIISA.2015.7388053&partnerID=40&md5=f47a72cc270f4c6292715916a0be2a69,"Department of Informatics, Aristotle University, Thessaloniki, Greece","Tiakas, E., Department of Informatics, Aristotle University, Thessaloniki, Greece; Papadopoulos, A.N., Department of Informatics, Aristotle University, Thessaloniki, Greece; Manolopoulos, Y., Department of Informatics, Aristotle University, Thessaloniki, Greece","During the two past decades, skyline queries were used in several multi-criteria decision support applications. Given a dominance relationship in a dataset, a skyline query returns the objects that cannot be dominated by any other objects. Skyline queries were studied extensively in multidimensional spaces, in subspaces, in metric spaces, in dynamic spaces, in streaming environments, and in time-series data. Several algorithms were proposed for skyline query processing, such as window-based, progressive, distributed, geometric-based, index-based, divide- and-conquer, and dynamic programming algorithms. Moreover, several variations were proposed to solve application-specific problems like k-dominant skylines, top-k dominating queries, spatial skyline queries, and others. As the number of objects that are returned in a skyline query may become large, there is also an extensive study for the cardinality of skyline queries. This extensive research depicts the importance of skyline queries and their variations in modern applications. å© 2015 IEEE.",,Conference Paper,Scopus,2-s2.0-84963846392
Thomas D.,"Costs, benefits, and adoption of additive manufacturing: a supply chain perspective",2016,International Journal of Advanced Manufacturing Technology,85,5-Aug,,1857,1876,no,,,3,10.1007/s00170-015-7973-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947087075&doi=10.1007%2fs00170-015-7973-6&partnerID=40&md5=8d09ffb966b072e807d5005f6dc8336a,"National Institute of Standards and Technology, Gaithersburg, MD, United States","Thomas, D., National Institute of Standards and Technology, Gaithersburg, MD, United States","There are three primary aspects to the economics of additive manufacturing: measuring the value of goods produced, measuring the costs and benefits of using the technology, and estimating the adoption and diffusion of the technology. This paper provides an updated estimate of the value of goods produced. It then reviews the literature on additive manufacturing costs and identifies those instances in the literature where this technology is cost-effective. The paper then goes on to propose an approach for examining and understanding the societal costs and benefits of this technology both from a monetary viewpoint and a resource consumption viewpoint. The final section discusses the trends in the adoption of additive manufacturing. Globally, there is an estimated $667 million in value added produced using additive manufacturing, which equates to 0.01åÊ% of total global manufacturing value added. US value added is estimated as $241 million. Current research on additive manufacturing costs reveals that it is cost-effective for manufacturing small batches with continued centralized production; however, with increased automation distributed production may become cost-effective. Due to the complexities of measuring additive manufacturing costs and data limitations, current studies are limited in their scope. Many of the current studies examine the production of single parts and those that examine assemblies tend not to examine supply chain effects such as inventory and transportation costs along with decreased risk to supply disruption. The additive manufacturing system and the material costs constitute a significant portion of an additive manufactured product; however, these costs are declining over time. The current trends in costs and benefits have resulted in this technology representing 0.02åÊ% of the relevant manufacturing industries in the USA; however, as the costs of additive manufacturing systems decrease, this technology may become widely adopted and change the supplier, manufacturer, and consumer interactions. An examination in the adoption of additive manufacturing reveals that for this technology to exceed $4.4 billion in 2020, $16.0 billion in 2025, and $196.8 billion in 2035, it would need to deviate from its current trends of adoption. å© 2015, Springer-Verlag London (outside the USA).",Additive manufacturing; Manufacturing; Supply chain,Article,Scopus,2-s2.0-84947087075
"Teo T.-A., Cho K.-H.",BIM-oriented indoor network model for indoor and outdoor combined route planning,2016,Advanced Engineering Informatics,30,3,,268,282,no,,,2,10.1016/j.aei.2016.04.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966280684&doi=10.1016%2fj.aei.2016.04.007&partnerID=40&md5=862b1f1a3bd2d3c9fed51f95d3bf8096,"Department of Civil Engineering, National Chiao Tung University, No. 1001 University Road, Hsinchu, Taiwan","Teo, T.-A., Department of Civil Engineering, National Chiao Tung University, No. 1001 University Road, Hsinchu, Taiwan; Cho, K.-H., Department of Civil Engineering, National Chiao Tung University, No. 1001 University Road, Hsinchu, Taiwan","Emergency response and pedestrian route planning rely highly on indoor-outdoor geospatial data and a network model of the data; however, indoor geospatial data collection is time-consuming. Several studies have used the architecture, engineering, and construction (AEC) model to generate indoor network models. These models are subject to the input data types, and the attributes of interior building objects are usually incomplete; hence, the integration of building information modeling (BIM) and geographic information systems (GIS) can benefit indoor-outdoor integrated applications. To achieve data interoperability, an open BIM standard called Industry Foundation Classes (IFC) is maintained by buildingSMART. In this study, we propose a multi-purpose geometric network model (MGNM) based on BIM and explore the strategy of indoor and outdoor network connections. To achieve the goals, the IFC-to-MGNM conversion includes the following: (1) extraction of building information from IFC, (2) isolation of the MGNM information from the aforementioned building information, and (3) build up the topological relationships of MGNM into GIS Geodatabase. In addition, an entrance-to-street strategy is proposed to connect indoor networks, entrances, and outdoor networks for detailed route planning. The experimental results indicate that the MGNM could be generated from BIM automatically and applied to connect indoor and outdoor features for the multi-purpose application. Two use-case scenarios were developed to validate the proposed methods. Compared to actual distance, the relative error was improved by 5.1% and 65.5% in the horizontal and vertical routes, respectively, over the conventional indoor network model from 2D ground plan. In addition, the computational time taken by the proposed coarse-to-fine route planning method was 25% that of the traditional single-scale route planning method. å© 2016 Elsevier Ltd. All rights reserved.",Building information modeling; Geographic information systems; Indoor network; Industry foundation classes,Article,Scopus,2-s2.0-84966280684
"Tariq J., Kwong S., Yuan H.",HEVC intra mode selection based on Rate Distortion (RD) cost and Sum of Absolute Difference (SAD),2016,Journal of Visual Communication and Image Representation,35,,,112,119,no,,,1,10.1016/j.jvcir.2015.11.013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952332530&doi=10.1016%2fj.jvcir.2015.11.013&partnerID=40&md5=c7adfe3f50d30458f72e6e8b9dece08d,"Department of Computer Science, City University of Hong Kong, Kowloon, Hong Kong; School of Information Science and Engineering, Shandong University, China; City University of Hong Kong Shenzhen Research Institute, Shenzhen, China","Tariq, J., Department of Computer Science, City University of Hong Kong, Kowloon, Hong Kong; Kwong, S., Department of Computer Science, City University of Hong Kong, Kowloon, Hong Kong, City University of Hong Kong Shenzhen Research Institute, Shenzhen, China; Yuan, H., Department of Computer Science, City University of Hong Kong, Kowloon, Hong Kong, School of Information Science and Engineering, Shandong University, China","High Efficiency Video Coding (HEVC) encoder provides higher compression efficiency by offering 35 intra modes. However, the encoding complexity is increased due to more modes are involved in the decision process. Therefore, it is desired to build a fast intra prediction algorithm that is practical for real time application. In this paper, a quadratic approach for reducing intra coding complexity is proposed. Firstly, the relationship between the RD-cost and the SAD is investigated. Secondly, a model is proposed to estimate the RD-cost of all 35 intra modes using the quadratic relation, thus avoiding the computation of entropy coding, Hadamard cost, distortion, and transform. Experimental results demonstrate that the average time saving of the proposed approach is 31-38%, while the BD-Bit Rate increment is only 0.62-1.37%, respectively. å© 2015 Elsevier Inc. All rights reserved.",Fast intra prediction; High Efficiency Video Coding (HEVC); Image coding; Low complexity video coding; Rate distortion cost prediction; Rough mode decision; Video coding; Video compression,Article,Scopus,2-s2.0-84952332530
"Tanveer B., GuzmÌÁn L., Engel U.M.",Understanding and improving effort estimation in agile software development- An industrial case study,2016,"Proceedings - International Conference on Software and System Process, ICSSP 2016",,,2904373,41,49,no,,,,10.1145/2904354.2904373,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974529280&doi=10.1145%2f2904354.2904373&partnerID=40&md5=c6684ed21f5570fd8d37779694e8d046,"Fraunhofer Institute for Experimental Software Engineering IESE, Fraunhofer Platz-1, Kaiserslautern, Germany; SAP SE, Dietmar-Hopp-Allee 16, Walldorf, Germany; Institute for Theoretical Physics, University of MÌ_nster, Germany","Tanveer, B., Fraunhofer Institute for Experimental Software Engineering IESE, Fraunhofer Platz-1, Kaiserslautern, Germany; GuzmÌÁn, L., Fraunhofer Institute for Experimental Software Engineering IESE, Fraunhofer Platz-1, Kaiserslautern, Germany; Engel, U.M., SAP SE, Dietmar-Hopp-Allee 16, Walldorf, Germany, Institute for Theoretical Physics, University of MÌ_nster, Germany","Effort estimation is more challenging in an agile context, as instead of exerting strict control over changes in requirements, dynamism is embraced. Current practice relies on expert judgment, where the accuracy of estimates is sensitive to the expertise of practitioners and prone to bias. In order to improve the effectiveness of the effort estimation process, the goal of this research is to investigate and understand the estimation process with respect to its accuracy in the context of agile software development from the perspective of agile development teams. Using case study research, two observations and eleven interviews were conducted with three agile development teams at SAP SE, a German multinational software corporation. The results reveal that factors such as the developer's knowledge and experience and the complexity and impact of changes on the underlying system affect the magnitude as well as the accuracy of estimation. Moreover, if certain aspects of the estimation process, such as the potential impact of a change on the underlying system, are supported by a tool can help improve estimation accuracy. We conclude that explicit consideration of these factors in the estimation process can support experts in making accurate and informed estimates. Furthermore, there is a need for a tool that incorporates expert knowledge, enables explicit consideration of cost drivers by experts and visualizes this information in order to improve the effectiveness of the effort estimation process. å© 2016 ACM.",Agile development; Change impact; Effort estimation,Conference Paper,Scopus,2-s2.0-84974529280
Tanveer B.,Hybrid effort estimation of changes in agile software development,2016,Lecture Notes in Business Information Processing,251,,,316,320,no,,,,10.1007/978-3-319-33515-5_33,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971541390&doi=10.1007%2f978-3-319-33515-5_33&partnerID=40&md5=a3af2d9148e93471fe1e29d8a5f9d4fe,"Fraunhofer Institute for Experimental Software Engineering, Fraunhofer Platz-1, Kaiserslautern, Germany","Tanveer, B., Fraunhofer Institute for Experimental Software Engineering, Fraunhofer Platz-1, Kaiserslautern, Germany","Unlike traditional software development approaches, Agile embraces change. The resulting dynamism of requirements makes it challenging to estimate effort accurately. Current practice relies on expertjudgment that can be biased, labor intensive and inaccurate. Therefore, a systematic yet lightweight effort estimation methodology is needed to support expert judgment and improve its effectiveness. Such an approach will utilize the quantification of the impact of a requirement on software artifacts potentially affected by it. It will further introduce an explicit consideration of effort drivers that contribute to effort overhead. The aim is to synthesize research from three often orthogonal areas of research: (1) change impact analysis, (2) effort estimation (model and expert driven) and (3) software visualization. Hence, resulting in a hybrid methodology with tool support that incorporates expert knowledge, change impact analysis and enables an explicit consideration of cost drivers by experts to improve the effectiveness of effort estimation process. å© The Author(s) 2016.",Agile; Effort estimation; Expert judgment; Hybrid,Conference Paper,Scopus,2-s2.0-84971541390
"Tantithamthavorn C., McIntosh S., Hassan A.E., Matsumoto K.",An Empirical Comparison of Model Validation Techniques for Defect Prediction Models,2017,IEEE Transactions on Software Engineering,43,1,7497471,1,18,no,Defect prediction,,,10.1109/TSE.2016.2584050,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009874260&doi=10.1109%2fTSE.2016.2584050&partnerID=40&md5=8e0362aaaa6056608661217e88e5498f,"Graduate School of Information Science, Nara Institute of Science and Technology, Ikoma, Japan; Department of Electrical and Computer Engineering, McGill University, Montreal, QC, Canada; School of Computing, Queen's University, Kingston, ON, Canada","Tantithamthavorn, C., Graduate School of Information Science, Nara Institute of Science and Technology, Ikoma, Japan; McIntosh, S., Graduate School of Information Science, Nara Institute of Science and Technology, Ikoma, Japan; Hassan, A.E., School of Computing, Queen's University, Kingston, ON, Canada; Matsumoto, K., Department of Electrical and Computer Engineering, McGill University, Montreal, QC, Canada","Defect prediction models help software quality assurance teams to allocate their limited resources to the most defect-prone modules. Model validation techniques, such as k-fold cross-validation, use historical data to estimate how well a model will perform in the future. However, little is known about how accurate the estimates of model validation techniques tend to be. In this paper, we investigate the bias and variance of model validation techniques in the domain of defect prediction. Analysis of 101 public defect datasets suggests that 77 percent of them are highly susceptible to producing unstable results-selecting an appropriate model validation technique is a critical experimental design choice. Based on an analysis of 256 studies in the defect prediction literature, we select the 12 most commonly adopted model validation techniques for evaluation. Through a case study of 18 systems, we find that single-repetition holdout validation tends to produce estimates with 46-229 percent more bias and 53-863 percent more variance than the top-ranked model validation techniques. On the other hand, out-of-sample bootstrap validation yields the best balance between the bias and variance of estimates in the context of our study. Therefore, we recommend that future defect prediction studies avoid single-repetition holdout validation, and instead, use out-of-sample bootstrap validation. å© 1976-2012 IEEE.",bootstrap validation; cross validation; Defect prediction models; holdout validation; model validation techniques,Article,Scopus,2-s2.0-85009874260
"Tantithamthavorn C., McIntosh S., Hassan A.E., Matsumoto K.",Comments on Researcher Bias: The Use of Machine Learning in Software Defect Prediction,2016,IEEE Transactions on Software Engineering,42,11,7450669,1092,1094,no,Defect,,1,10.1109/TSE.2016.2553030,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997207340&doi=10.1109%2fTSE.2016.2553030&partnerID=40&md5=61c3127db2085af0910fe007d6244353,"Graduate School of Information Science, Nara Institute of Science and Technology, Nara, Japan; Department of Electrical and Computer Engineering, McGill University, Montreal, QC, Canada; School of Computing, Queen's University, Kingston, ON, Canada","Tantithamthavorn, C., Graduate School of Information Science, Nara Institute of Science and Technology, Nara, Japan; McIntosh, S., Department of Electrical and Computer Engineering, McGill University, Montreal, QC, Canada; Hassan, A.E., School of Computing, Queen's University, Kingston, ON, Canada; Matsumoto, K., Graduate School of Information Science, Nara Institute of Science and Technology, Nara, Japan","Shepperd et al. find that the reported performance of a defect prediction model shares a strong relationship with the group of researchers who construct the models. In this paper, we perform an alternative investigation of Shepperd et al.'s data. We observe that (a) research group shares a strong association with other explanatory variables (i.e., the dataset and metric families that are used to build a model); (b) the strong association among these explanatory variables makes it difficult to discern the impact of the research group on model performance; and (c) after mitigating the impact of this strong association, we find that the research group has a smaller impact than the metric family. These observations lead us to conclude that the relationship between the research group and the performance of a defect prediction model are more likely due to the tendency of researchers to reuse experimental components (e.g., datasets and metrics). We recommend that researchers experiment with a broader selection of datasets and metrics to combat any potential bias in their results. å© 2016 IEEE.",defect prediction; researcher bias; Software quality assurance,Article,Scopus,2-s2.0-84997207340
"Tantithamthavorn C., McIntosh S., Hassan A.E., Matsumoto K.",Automated parameter optimization of classification techniques for defect prediction models,2016,Proceedings - International Conference on Software Engineering,14-22-May-2016,,,321,332,no,Defects,,2,10.1145/2884781.2884857,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971421988&doi=10.1145%2f2884781.2884857&partnerID=40&md5=de2dcf923c036b9ec037fadf1ce24c2e,"Nara Institute of Science and Technology, Japan; McGill University, Canada; Queen's University, Canada","Tantithamthavorn, C., Nara Institute of Science and Technology, Japan; McIntosh, S., McGill University, Canada; Hassan, A.E., Queen's University, Canada; Matsumoto, K., Nara Institute of Science and Technology, Japan","Defect prediction models are classifiers that are trained to identify defect-prone software modules. Such classifiers have configurable parameters that control their characteristics (e.g., the number of trees in a random forest classifier). Recent studies show that these classifiers may underperform due to the use of suboptimal default parameter settings. However, it is impractical to assess all of the possible settings in the parameter spaces. In this paper, we investigate the performance of defect prediction models where Caret-an automated parameter optimization technique-has been applied. Through a case study of 18 datasets from systems that span both proprietary and open source domains, we find that (1) Caret improves the AUC performance of defect prediction models by as much as 40 percentage points; (2) Caret-optimized classifiers are at least as stable as (with 35% of them being more stable than) classifiers that are trained using the default settings; and (3) Caret increases the likelihood of producing a top-performing classifier by as much as 83%. Hence, we conclude that parameter settings can indeed have a large impact on the performance of defect prediction models, suggesting that researchers should experiment with the parameters of the classification techniques. Since automated parameter optimization techniques like Caret yield substantially benefits in terms of performance improvement and stability, while incurring a manageable additional computational cost, they should be included in future defect prediction studies. å© 2016 ACM.",Classification techniques; Experimental design; Parameter optimization; Software defect prediction,Conference Paper,Scopus,2-s2.0-84971421988
"Tanhaei M., Habibi J., Mirian-Hosseinabadi S.-H.",A Feature Model Based Framework for Refactoring Software Product Line Architecture,2016,Journal of Computer Science and Technology,31,5,,951,986,no,,,2,10.1007/s11390-016-1674-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984908557&doi=10.1007%2fs11390-016-1674-y&partnerID=40&md5=bb87ce0a2218e447cdac76a5b15fa444,"Department of Computer Engineering, Sharif University of Technology, Tehran, Iran","Tanhaei, M., Department of Computer Engineering, Sharif University of Technology, Tehran, Iran; Habibi, J., Department of Computer Engineering, Sharif University of Technology, Tehran, Iran; Mirian-Hosseinabadi, S.-H., Department of Computer Engineering, Sharif University of Technology, Tehran, Iran","Software product line (SPL) is an approach used to develop a range of software products with a high degree of similarity. In this approach, a feature model is usually used to keep track of similarities and differences. Over time, as modifications are made to the SPL, inconsistencies with the feature model could arise. The first approach to dealing with these inconsistencies is refactoring. Refactoring consists of small steps which, when accumulated, may lead to large-scale changes in the SPL, resulting in features being added to or eliminated from the SPL. In this paper, we propose a framework for refactoring SPLs, which helps keep SPLs consistent with the feature model. After some introductory remarks, we describe a formal model for representing the feature model. We express various refactoring patterns applicable to the feature model and the SPL formally, and then introduce an algorithm for finding them in the SPL. In the end, we use a real-world case study of an SPL to illustrate the applicability of the framework introduced in the paper. å© 2016, Springer Science+Business Media New York.",feature model; refactoring; software architecture; software product line,Article,Scopus,2-s2.0-84984908557
"Talukder S.C., Rahman M.M.",Customer requirements oriented component based software development life cycle model,2015,"Proceedings - 2015 International Conference on Computers, Communications and Systems, ICCCS 2015",,,7562873,61,68,no,,,,10.1109/CCOMS.2015.7562873,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988925570&doi=10.1109%2fCCOMS.2015.7562873&partnerID=40&md5=b294a24ef7b6bdad3655fdcfc92de5ee,"Computer Science and Engineering, Daffodil International University, Dhaka, Bangladesh; Dept. of Software Engineering, Daffodil International University, Dhaka, Bangladesh","Talukder, S.C., Computer Science and Engineering, Daffodil International University, Dhaka, Bangladesh; Rahman, M.M., Dept. of Software Engineering, Daffodil International University, Dhaka, Bangladesh","Today software development is a big challenge due to market policy, cost, quality and most importantly customer satisfaction. But the conventional system development is not enough to fulfill those requirements. It needs a new method of solution to overcome those challenges. Component Based Software Engineering (CBSE) can be a solution, in where Component Based Software development (CBSD) is a process that follows the design and construction of computer based systems using reusable components. To develop a customer effective system, it requires some additional criteria, must have to follow in development. Many researchers avail to develop many different models and methodologies to overcome those challenges. But those models are not enough to handle customer oriented issues properly as their actual needs and also a complete system development model including new component development by providing pre-analyzed component development phases. Therefore new model is required to overcome those challenges. In our research we proposed a system development life cycle model which has incorporated most importantly customer participation, customer support, customer feedback analysis and new component development phases by providing fully complete customer requirement oriented framework. Our propose model for CBSD named as Customer Requirement Oriented CBSD life cycle Model (CROM). å© 2015 IEEE.",component development; customer feedback analysis; customer oriented model; customer support; model evaluation,Conference Paper,Scopus,2-s2.0-84988925570
"Taba N., Ow S.",A new model for software inspection at the requirements analysis and design phases of software development,2016,International Arab Journal of Information Technology,13,6,,,,no,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994504708&partnerID=40&md5=f6d77067ef4ef4f8e0e8e7d0b63d0294,"Department of Software Engineering, University of Malaya, Malaysia; Department of Software Engineering, Faculty of Computer Science and Information Technology, University of Malaya, Malaysia","Taba, N., Department of Software Engineering, University of Malaya, Malaysia; Ow, S., Department of Software Engineering, Faculty of Computer Science and Information Technology, University of Malaya, Malaysia","Software inspection models have been remarkable development in over the past four decades, particularly in the field of automatic inspection of software codes and electronic sessions. A small number of improvements have been made in the field of system analysis and design. The amount of using formal inspection model which is based on single check lists and physical or electronic sessions shows the decrease in interest in it. As inspection, in system analysis phase, is a man-cantered issue, inspectors support using electronic tools will lead to higher efficiency of the inspection process. This paper proposes a comprehensive web-based tool aimed to accelerating the inspection process in the early phases of software development. In order to evaluate the efficiency of the proposed tool, two case studies were conducted to inspect the artifacts from six software projects of two software companies. Comparing the statistics related to the defects detected using this tool with those detected using the formal method shows the efficiency of the used tool. å© 2016, Zarka Private Univ. All rights reserved.",Inspection metrics; Software engineering improvement; Software inspection; Software inspection tool; Software test; Web-based solution,Article,Scopus,2-s2.0-84994504708
"Sulistiani H., Tjahyanto A.",Heterogeneous feature selection for classification of customer loyalty fast moving consumer goods (Case study: Instant noodle),2016,Journal of Theoretical and Applied Information Technology,94,1,,77,83,no,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006380510&partnerID=40&md5=3b184935c3c661a739e5155aa2add3d8,"Department of Information System, STMIK Teknokrat Lampung, Indonesia; Department of Information System, Institut Teknologi Sepuluh Nopember, Indonesia","Sulistiani, H., Department of Information System, STMIK Teknokrat Lampung, Indonesia; Tjahyanto, A., Department of Information System, Institut Teknologi Sepuluh Nopember, Indonesia","In the face of ASEAN open market, the actors (Fast Moving Consumer Goods industry) must increasingly explore patterns of business development because of tough competition and challenges in the market. One of strategy for surviving in high competition is retain the customer loyalty. The data were usually obtained from various sources and contains heterogeneous features, such as numerical and non-numerical features. The datasets with heterogeneous features can affect feature selection results that are not appropriate because of the difficulty of evaluating heterogeneous features concurrently. In this paper, we propose a method that combine the features transformation and subset selection based on mutual information to obtain feature subset that able to improve performance classification algorithm. Analysis comparative among before feature subset selection, dynamic mutual information (DMI) methods, p-value methods and researcher estimate were also done. Feature transformation (FT) is another way to handle the selection of heterogeneous features. The datasets were obtained from the survey of customers fast moving consumer goods, with a total of 386 respondents. By applying unsupervised feature transformation and dynamic mutual information methods, can be known the relevant features that affected the performance of decision tree algorithm. The accuracy and F-measure increased of the DMI-unsupervised-feature-transformation compared to all features (without features subset selection), p-value methods and manual features subset selection. The accuracy and F-measure for DMI-unsupervised-feature transformation are 76.68% and 73.5% respectively. å© 2005 - 2016 JATIT & LLS. All rights reserved.",Classification; Customer loyalty; Decision tree; Feature transformation; Mutual information,Article,Scopus,2-s2.0-85006380510
"Stylianou C., Andreou A.S.","Investigating the impact of developer productivity, task interdependence type and communication overhead in a multi-objective optimization approach for software project planning",2016,Advances in Engineering Software,98,,,79,96,no,,,,10.1016/j.advengsoft.2016.04.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963831276&doi=10.1016%2fj.advengsoft.2016.04.001&partnerID=40&md5=c6954676eb7e84c7745370adef40d619,"Department of Computer Science, University of Cyprus, 75 Kallipoleos Avenue, PO Box 20537, Lefkosia, Cyprus; Department of Electrical Engineering/Computer Engineering and Informatics, Cyprus University of Technology, 31 Archbishop Kyprianou Avenue, PO Box 50329, Lemesos, Cyprus","Stylianou, C., Department of Computer Science, University of Cyprus, 75 Kallipoleos Avenue, PO Box 20537, Lefkosia, Cyprus; Andreou, A.S., Department of Electrical Engineering/Computer Engineering and Informatics, Cyprus University of Technology, 31 Archbishop Kyprianou Avenue, PO Box 50329, Lemesos, Cyprus","One of the most important activities in software project planning involves scheduling tasks and assigning them to developers. Project managers must decide who will do what and when in a software project, with the aim of minimizing both its duration and cost. However, project managers often struggle to efficiently allocate developers and schedule tasks in a way that balances these conflicting goals. Furthermore, the different criteria used to select developers could lead to inaccurate estimation of the duration and cost of tasks, resulting in budget overruns, delays, or reduced software quality. This paper proposes an approach that makes use of multi-objective optimization to handle the simultaneous minimization of project cost and duration, taking into account several productivity-related attributes for better estimation of task duration and cost. In particular, we focus on dealing with the non-interchangeable nature of human resources and the different ways in which teams carry out work by considering the relationship between the type of task interdependence and the productivity rate of developers, as well as the communication overhead incurred among developers. The approach is applied to four well-known optimization algorithms, whose performance and scalability are compared using generated software project instances. Additionally, several real-world case studies are explored to help discuss the implications of such approach in the software development industry. The results and observations show positive indications that using a productivity-based multi-objective optimization approach has the potential to provide software project managers with more accurate developer allocation and task scheduling solutions in a more efficient manner. å© 2016 Elsevier Ltd. All rights reserved.",Communication overhead; Human resource allocation; Multi-objective optimization; Productivity-based software project planning; Task interdependence; Task scheduling,Article,Scopus,2-s2.0-84963831276
Siva Nageswara Rao G.,Real time scheduling for dynamic process execution using round robin switching,2016,Journal of Theoretical and Applied Information Technology,89,1,,60,66,no,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978677645&partnerID=40&md5=c36f5100f50cf7a7d59c0ae0675f248d,"Department of Computer Science Engineering, KL University, India","Siva Nageswara Rao, G., Department of Computer Science Engineering, KL University, India, Department of Computer Science Engineering, KL University, India","Scheduling in operating systems is allocate certain amount of CPU time with use of different processes. Task scheduling is also one of the key process in running of different processors in with in life time. Round Robin (RR) is a popular scheduling algorithm allows to utilize the CPU short time for individual task scheduling events in real time process execution. The advancement of RR scheduling performs fine tuning for time slice which do not stipulated time to allocate them in events proceedings based on CPU scheduling. RR also maintains turnaround time, waiting time and response time with processing frequency of context switches. In this paper we improve the performance of RR with integer programming to refine in arrival time analysis in process scheduling with proceedings of all the requirement CPU processes. Every process has reasonable response and arrival time analysis in allocation of scheduling in process allocation. A method the usage of integer programming has been proposed to resolve equations that determine Changeable Time Quantum (CTQ) value that is neither too massive nor too small such that each system has reasonable response time and the throughput of the system is not decreased because of unnecessarily context switches. å© 2005 - 2016 JATIT & LLS. All rights reserved.",Arrival Time; Burst Time; CPU Scheduling; Cyclic Queue; Number of Switches; Residual Time; Time Slice; Turnaround Time,Article,Scopus,2-s2.0-84978677645
"Sinha K., de Weck O.L.",Empirical Validation of Structural Complexity Metric and Complexity Management for Engineering Systems,2016,Systems Engineering,19,3,,193,206,no,,,,10.1002/sys.21356,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983667054&doi=10.1002%2fsys.21356&partnerID=40&md5=1349c777d3660ffdd303310d07600c40,"Engineering Systems Division, Massachusetts Institute of Technology, Cambridge, MA, United States","Sinha, K., Engineering Systems Division, Massachusetts Institute of Technology, Cambridge, MA, United States; de Weck, O.L., Engineering Systems Division, Massachusetts Institute of Technology, Cambridge, MA, United States","Quantitative assessment of structural complexity is essential for characterization of engineered complex systems. In this paper, we describe a quantitative measure for structural complexity, conduct an empirical validation study of the structural complexity metric, and introduce a complexity management framework for engineering system development. We perform empirical validation of the proposed complexity metric using simple experiments using ball and stick models and show that the development effort increases superlinearly with increasing structural complexity. The standard deviation of the build time for ball and stick models is observed to vary superlinearly with structural complexity. We also describe a generic statistical procedure for building such cost estimation relationships with structural complexity as the independent variable. We distinguish the notion of perception of complexity as an observer-dependent property and contrast that with complexity, which is a property of the system architecture. Finally, we introduce the notion of system value based on performance-complexity trade space and introduce a complexity management framework for system development. å© 2016 Wiley Periodicals, Inc.",complexity management framework; development effort; empirical validation; perception of complexity; structural complexity; system value,Article,Scopus,2-s2.0-84983667054
"Singh P.P., Madan J.",A computer-aided system for sustainability assessment for the die-casting process planning,2016,International Journal of Advanced Manufacturing Technology,87,5-Aug,,1283,1298,no,,,,10.1007/s00170-013-5232-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882372141&doi=10.1007%2fs00170-013-5232-2&partnerID=40&md5=21a1987a5c190706f03cb3099bd1b456,"Department of Mechanical Engineering, Lovely Professional University, Jalandhar, India; Department of Mechanical Engineering, Sant Longowal Institute of Engineering and Technology, Longowal, Sangrur, India","Singh, P.P., Department of Mechanical Engineering, Lovely Professional University, Jalandhar, India, Department of Mechanical Engineering, Sant Longowal Institute of Engineering and Technology, Longowal, Sangrur, India; Madan, J., Department of Mechanical Engineering, Sant Longowal Institute of Engineering and Technology, Longowal, Sangrur, India","The sustainability assessment of a product's manufacturing using a life cycle assessment tool utilizes the average sustainability performance of the manufacturing process. Because of using an average of sustainability performance, the crucial information related to manufacturing is lost. Therefore, such an assessment cannot be used to assess and compare the sustainability of a part made using different process plans. In this paper, we propose a new systematic approach for sustainability assessment of the die-casting process planning. A computer-aided system named Sustainability Assessor for Die-casting is presented. Here, we discuss the details of the architecture and working of the proposed system. We assess sustainability using three sustainability indicators, namely, energy use, solid waste, and carbon emissions. The proposed system is verified by comparing results with the actual data measured from the shop floor. The developed system is beneficial for sustainability assessment comparing different plans alongside material properties, ultimately helping the die-casting industry to reduce the carbon emissions and material waste, besides improving energy efficiency. å© 2013, Springer-Verlag London.",Air emission; Die-casting; Energy use; Process planning; Solid waste; Sustainability assessment,Article,Scopus,2-s2.0-84882372141
"Singh B.K., Punhani A., Tiwari S., Misra A.K.",Consideration of Similarity Factors in Integration of FP and SLOC for Software Project Estimation,2015,"Proceedings - 8th International Conference on Advanced Software Engineering and Its Applications, ASEA 2015",,,7433066,36,40,no,,,,10.1109/ASEA.2015.17,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964901403&doi=10.1109%2fASEA.2015.17&partnerID=40&md5=26292617f7134ce927a3107240e30a9e,"Department of CSE, FET, R.B.S. College, Agra, U. P., India; Department of CSE, GLA University, Mathura, U. P., India; Department of CSE, ITS Engg. College, Gr. Noida, U. P., India; Department of CSE, MNNIT, Allahabad, U. P., India","Singh, B.K., Department of CSE, FET, R.B.S. College, Agra, U. P., India; Punhani, A., Department of CSE, GLA University, Mathura, U. P., India; Tiwari, S., Department of CSE, ITS Engg. College, Gr. Noida, U. P., India; Misra, A.K., Department of CSE, MNNIT, Allahabad, U. P., India","This research paper includes the use and explanations related to advantages of two ""public domain"" costing methods i.e., Function Point and Source Lines of Codes for size estimation. Research paper demonstrates the effect of deviation between SLOC and FP and use of homogeneous data can provide the acceptable results by reducing deviation as established. It is demonstrated and established that the combination of physical size and functional size using the LOC and function points can affect the productivity. Such estimates are of very high degree of accuracy. å© 2015 IEEE.",COCOMO; Function Point; homogeneous data Productivity; Source Lines of Codes,Conference Paper,Scopus,2-s2.0-84964901403
"Singh B.K., Punhani A., Misra A.K.",Integrated approach of software project size estimation,2016,International Journal of Software Engineering and its Applications,10,2,,45,64,no,Sizing,,,10.14257/ijseia.2016.10.2.05,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960416496&doi=10.14257%2fijseia.2016.10.2.05&partnerID=40&md5=f90f44110d4bb981106e6305e1350cc2,"Deptt. of CSE, FET, R.B.S. College, Agra, India; Deptt. of CSE, GLA University, Mathura, India; Deptt. of CSE, MNNIT, Allahabad, India","Singh, B.K., Deptt. of CSE, FET, R.B.S. College, Agra, India; Punhani, A., Deptt. of CSE, GLA University, Mathura, India; Misra, A.K., Deptt. of CSE, MNNIT, Allahabad, India","In the past, some researchers have shown that using more than one technique can reduce the risk of trusting only one method. At present many estimators use function point input in estimations based on COCOMO. This research paper includes the use and explanations related to advantages of these two ""public domain"" costing methods and the proposal of an integrated single model for size estimation. The problem of language dependency is well addressed and the language-weighted function point is introduced as the solution domain. Research paper demonstrates that the use of backfiring conversion factors is inherently inaccurate as there is no effective relationship between SLOC and FP. The use of homogeneous data can provide the acceptable results as established. It is demonstrated and established that the combination of physical size and functional size using the LOC and function points can affect the productivity. Using weighted function points as the main input in a COCOMO-like power function enables the effect of the programming language which enables the model to be easily adapted to other development environments. Such estimates are of very high degree of accuracy. å© 2016 SERSC.",COCOMO; Function point; Language-weighted function point; Productivity,Article,Scopus,2-s2.0-84960416496
"Silva-De-Souza T., Da Silva T.M., Travassos G.H.","AnÌÁlise linguÌ_stico-cientÌ_fica dos termos ""esforÌ¤o"" e ""custo"" e sua relaÌ¤Ì£o com o software: Um estudo qualitativo apoiado pela linguÌ_stica de corpus",2016,CIBSE 2016 - XIX Ibero-American Conference on Software Engineering,,,,200,213,no,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988350532&partnerID=40&md5=15f1b87b91759f7fa615616972a1f5cf,"Programa de Engenharia de Sistemas e ComputaÌ¤Ì£o (PESC), Instituto Alberto Luiz Coimbra de PÌ_s-GraduaÌ¤Ì£o e Pesquisa de Engenharia (COPPE), Universidade Federal Do Rio de Janeiro (UFRJ), Rio-De-Janeiro-RJ, Brazil; Faculdade de Letras, Universidade Federal Do Rio de Janeiro (UFRJ), Rio-De-Janeiro-RJ, Brazil; Pesquisador CNPq, Brazil","Silva-De-Souza, T., Programa de Engenharia de Sistemas e ComputaÌ¤Ì£o (PESC), Instituto Alberto Luiz Coimbra de PÌ_s-GraduaÌ¤Ì£o e Pesquisa de Engenharia (COPPE), Universidade Federal Do Rio de Janeiro (UFRJ), Rio-De-Janeiro-RJ, Brazil; Da Silva, T.M., Faculdade de Letras, Universidade Federal Do Rio de Janeiro (UFRJ), Rio-De-Janeiro-RJ, Brazil; Travassos, G.H., Programa de Engenharia de Sistemas e ComputaÌ¤Ì£o (PESC), Instituto Alberto Luiz Coimbra de PÌ_s-GraduaÌ¤Ì£o e Pesquisa de Engenharia (COPPE), Universidade Federal Do Rio de Janeiro (UFRJ), Rio-De-Janeiro-RJ, Brazil, Pesquisador CNPq, Brazil",[No abstract available],,Conference Paper,Scopus,2-s2.0-84988350532
"Silva A., Pinheiro P., Albuquerque A.",A brief analysis of reported problems in the use of function points,2016,Advances in Intelligent Systems and Computing,465,,,117,126,no,,,,10.1007/978-3-319-33622-0_11,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964780406&doi=10.1007%2f978-3-319-33622-0_11&partnerID=40&md5=bc62c005c94b0b64d5e5dac729d03e28,"University of Fortaleza, Av. Washinton Soares, 1321, BL J, SL 30, Fortaleza, CearÌÁ, Brazil","Silva, A., University of Fortaleza, Av. Washinton Soares, 1321, BL J, SL 30, Fortaleza, CearÌÁ, Brazil; Pinheiro, P., University of Fortaleza, Av. Washinton Soares, 1321, BL J, SL 30, Fortaleza, CearÌÁ, Brazil; Albuquerque, A., University of Fortaleza, Av. Washinton Soares, 1321, BL J, SL 30, Fortaleza, CearÌÁ, Brazil","Know the software size is a key issue to guide the planning and management of a software project. In this context, Function Point Analysis (FPA) has been consolidated as a strategic tool for measuring the functional size of software. The function point metric is the most widespread in the world, but despite its growth has received several criticisms from its users. This paper presents an investigation of the problems and difficulties on the application of FPA. As a result, the reported problems were analyzed and proposed solutions to these problems were presented. å© Springer International Publishing Switzerland 2016.",Function point analysis; Reported problems; Software project,Conference Paper,Scopus,2-s2.0-84964780406
"Silhavy R., Silhavy P., Prokopova Z.",Analysis and selection of a regression model for the Use Case Points method using a stepwise approach,2017,Journal of Systems and Software,125,,,1,14,no,,,,10.1016/j.jss.2016.11.029,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85000774224&doi=10.1016%2fj.jss.2016.11.029&partnerID=40&md5=ca89c5cdc41c4cf92f713ed96523dfd4,"Tomas Bata University in Zlin, Faculty of Applied Infomatics, Nad Stranemi 4511, Zlin, Czech Republic","Silhavy, R., Tomas Bata University in Zlin, Faculty of Applied Infomatics, Nad Stranemi 4511, Zlin, Czech Republic; Silhavy, P., Tomas Bata University in Zlin, Faculty of Applied Infomatics, Nad Stranemi 4511, Zlin, Czech Republic; Prokopova, Z., Tomas Bata University in Zlin, Faculty of Applied Infomatics, Nad Stranemi 4511, Zlin, Czech Republic","This study investigates the significance of use case points (UCP) variables and the influence of the complexity of multiple linear regression models on software size estimation and accuracy. Stepwise multiple linear regression models and residual analysis were used to analyse the impact of model complexity. The impact of each variable was studied using correlation analysis. The estimated size of software depends mainly on the values of the weights of unadjusted UCP, which represent a number of use cases. Moreover, all other variables (unadjusted actors' weights, technical complexity factors, and environmental complexity factors) from the UCP method also have an impact on software size and therefore cannot be omitted from the regression model. The best performing model (Model D) contains an intercept, linear terms, and squared terms. The results of several evaluation measures show that this model's estimation ability is better than that of the other models tested. Model D also performs better when compared to the UCP model, whose Sum of Squared Error was 268,620 points on Dataset 1 and 87,055 on Dataset 2. Model D achieved a greater than 90% reduction in the Sum of Squared Errors compared to the Use Case Points method on Dataset 1 and a greater than 91% reduction on Dataset 2. The medians of the Sum of Squared Errors for both methods are significantly different at the 95% confidence level (p‰Ûä<‰Ûä0.01), while the medians for Model D (312 and 37.26) are lower than Use Case Points (3134 and 3712) on Datasets 1 and 2, respectively. å© 2016 The Authors",Dataset; Multiple linear regression; Software size estimation; Stepwise approach; Use Case Points; Variables analysis,Article,Scopus,2-s2.0-85000774224
"Sidhanta S., Mukhopadhyay S.",Infra: SLO aware elastic auto-scaling in the cloud for cost reduction,2016,"Proceedings - 2016 IEEE International Congress on Big Data, BigData Congress 2016",,,7584931,141,148,no,,,,10.1109/BigDataCongress.2016.25,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994681129&doi=10.1109%2fBigDataCongress.2016.25&partnerID=40&md5=28298556cf993dbb11e7ea0223193b97,"Louisiana State University, Baton Rouge, LO, United States","Sidhanta, S., Louisiana State University, Baton Rouge, LO, United States; Mukhopadhyay, S., Louisiana State University, Baton Rouge, LO, United States","Enterprises often host applications and services on clusters of virtual machine instances provided by cloud service providers, like Amazon, Rackspace, Microsoft, etc. Users pay a cloud usage cost on the basis of the hourly usage [1] of virtual machine instances composing the cluster. A cluster composition refers to the number of virtual machine instances of each type (from a predefined list of types) comprising a cluster. We present Infra, a cloud provisioning framework that can predict an (ìµ, ìÄ)-minimum cluster composition required to run a given application workload on a cloud under an SLO (i.e., Service Level Objective) deadline. This paper does not present a new approximation algorithm, instead we provide a tool that applies existing machine learning techniques to predict an (ìµ, ìÄ)-minimum cluster composition. An (ìµ, ìÄ)-minimum cluster composition specifies a cluster composition whose cost approximates that of the minimum cluster composition (i.e., the cluster composition that incurs the minimum cloud usage cost that must be incurred in executing a given application under an SLO deadline), the approximation bounds the error to a predefined threshold ìµ with a degree of confidence 100 ‰öÑ (1 - ìÄ)%. The degree of confidence 100 ‰öÑ (1 - ìÄ)% specifies that the probability of failure in achieving the error threshold ìµ for the above approximation is at most ìÄ. For ìµ = 0.1 and ìÄ = 0.02, we experimentally demonstrate that an (ìµìÄ)-minimum cluster composition predicted by Infra successfully approximates the minimum cluster composition, i.e., the accuracy of prediction of minimum cluster composition ranges from 93.1% to 97.99% (the error is bound by the error threshold of 0.1) with a 98% degree of confidence, since 100‰öÑ (1 - ìÄ) = 98%. Auto scaling refers to the process of automatically adding cloud instances to a cluster to adapt to an increase in application workload (increased request rate), and deleting instances from a cluster when there is a decrease in workload (reduced request rate). However, state-of-the-art auto scaling techniques have the following disadvantages: A) they require explicit policy definition for changing the cluster configuration and therefore lack the ability to automatically adapt a cluster with respect to changing workload, B) they do not compute the appropriate size of resources required, and therefore do not result in an 'optimal' cluster composition. Infra provides an auto scaler that automatically adapts a cloud infrastructure to changing application workload, scaling the cluster up/down based on predictions from the Infra provisioning tool. å© 2016 IEEE.",Cost optimal cluster composition; Elastic auto scaling; Provisioning,Conference Paper,Scopus,2-s2.0-84994681129
"Sidhanta S., Golab W., Mukhopadhyay S.",OptEx: A Deadline-Aware Cost Optimization Model for Spark,2016,"Proceedings - 2016 16th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing, CCGrid 2016",,,7515689,193,202,no,,,,10.1109/CCGrid.2016.10,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983417200&doi=10.1109%2fCCGrid.2016.10&partnerID=40&md5=a28d387df449cf78b23ec5d9cb5eadff,"Louisiana State University, Baton Rouge, LO, United States; University of Waterloo, Waterloo, ON, Canada","Sidhanta, S., Louisiana State University, Baton Rouge, LO, United States; Golab, W., University of Waterloo, Waterloo, ON, Canada; Mukhopadhyay, S., Louisiana State University, Baton Rouge, LO, United States","We present OptEx, a closed-form model of jobexecution on Apache Spark, a popular parallel processing engine. To the best of our knowledge, OptEx is the first work thatanalytically models job completion time on Spark. The model canbe used to estimate the completion time of a given Spark job ona cloud, with respect to the size of the input dataset, the numberof iterations, the number of nodes comprising the underlyingcluster. Experimental results demonstrate that OptEx yields amean relative error of 6% in estimating the job completion time. Furthermore, the model can be applied for estimating the costoptimal cluster composition for running a given Spark job ona cloud under a completion deadline specified in the SLO (i.e.,Service Level Objective). We show experimentally that OptEx isable to correctly estimate the cost optimal cluster compositionfor running a given Spark job under an SLO deadline with anaccuracy of 98%. å© 2016 IEEE.",analytical model; cost optimal cluster composition; job profile; optimal scheduling; Spark job execution,Conference Paper,Scopus,2-s2.0-84983417200
"Shuai B., Li H., Zhang L., Zhang Q., Tang C.",Software vulnerability detection based on code coverage and test cost,2015,"Proceedings - 2015 11th International Conference on Computational Intelligence and Security, CIS 2015",,,7397098,317,321,no,,,,10.1109/CIS.2015.84,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964413929&doi=10.1109%2fCIS.2015.84&partnerID=40&md5=1cbe30edc4e42a445e54b35c9b5467ec,"School of Electronic Science and Engineering, National University of Defense Technology, Changsha, Hunan, China","Shuai, B., School of Electronic Science and Engineering, National University of Defense Technology, Changsha, Hunan, China; Li, H., School of Electronic Science and Engineering, National University of Defense Technology, Changsha, Hunan, China; Zhang, L., School of Electronic Science and Engineering, National University of Defense Technology, Changsha, Hunan, China; Zhang, Q., School of Electronic Science and Engineering, National University of Defense Technology, Changsha, Hunan, China; Tang, C., School of Electronic Science and Engineering, National University of Defense Technology, Changsha, Hunan, China","In order to solve the problems of traditional Fuzzing technique for software vulnerability detection, a novel method based on code coverage and test cost is proposed. Firstly, static analysis is applied to calculate the code coverage information, including basic block coverage and new block coverage. In addition, test path diversity information is introduced to elevate path coverage, which is achieved based on the sequence alignment algorithm. Secondly, test cost is analyzed respectively from running time and loop structure. The loop structure is simplified using finite expansion manner. Thirdly, the genetic algorithm fitness function is constructed based on the code coverage and test cost to guide the test case generation. Experiments on realistic binary software show that the method could obtain higher vulnerability detection accuracy and efficiency than the traditional Fuzzing technique. å© 2015 IEEE.",Code coverage; Genetic algorithm; Test cost; Test path diversity,Conference Paper,Scopus,2-s2.0-84964413929
"Shmueli O., Pliskin N., Fink L.",Can the outside-view approach improve planning decisions in software development projects?,2016,Information Systems Journal,26,4,,395,418,no,,,1,10.1111/isj.12091,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945971918&doi=10.1111%2fisj.12091&partnerID=40&md5=c723d0dca50823f7cc8cb97384e74958,"Department of Industrial Engineering and Management, Ben-Gurion University of the Negev, Beer-Sheva, Israel","Shmueli, O., Department of Industrial Engineering and Management, Ben-Gurion University of the Negev, Beer-Sheva, Israel; Pliskin, N., Department of Industrial Engineering and Management, Ben-Gurion University of the Negev, Beer-Sheva, Israel; Fink, L., Department of Industrial Engineering and Management, Ben-Gurion University of the Negev, Beer-Sheva, Israel","This study empirically tackles the question of whether taking an outside-view approach, recommended for reducing the irrational behaviours associated with the planning fallacy, can also reduce the time underestimation, scope overload and over-requirement problems plaguing planning decisions in software development. Drawing on descriptive behavioural decision theory, this study examines whether the planning fallacy, a cognitive bias referring to the tendency of people to underestimate costs and overestimate benefits in evaluating a task to be performed, can provide a theoretical platform for mitigating irrational behaviours in the planning of software development projects. In particular, we argue that taking an outside-view approach in planning decisions for software development may have the same mitigating effects on time underestimation, scope overload and over-requirement it has been shown to have on cost underestimation and benefit overestimation. In an experiment investigating this argument, participants were randomly assigned to four groups by manipulating two outside-view mechanisms: reference information about past completion times (present/absent) and role perspective (developer/consultant). After being presented with a to-be-developed software project, they were requested to estimate development times of various software features and to recommend which features to include within project scope given a fixed duration for the entire project. The results confirm that the three problems of time underestimation, scope overload and over-requirement are manifested in planning decisions for fixed-schedule software development projects. Moreover, the results show that these problems are mitigated, yet not eliminated, by presenting reference information about past completion times and by having a consultant role. å© 2015 Wiley Publishing Ltd",experiment; inside view; outside view; over-requirement; planning decisions; planning fallacy; reference information; role perspective; scope overload; software development; time underestimation,Article,Scopus,2-s2.0-84945971918
"Shiny Angel T.S., Senthil Kumar G., Selvakumarasamy S., Maria Nancy A., Veena S.",Cost model for E-learning system,2016,International Journal of Control Theory and Applications,9,28,,65,70,no,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007416051&partnerID=40&md5=9c715550a1c8132cf67d88fbc4310ccd,"SRM University, Chennai, India","Shiny Angel, T.S., SRM University, Chennai, India; Senthil Kumar, G., SRM University, Chennai, India; Selvakumarasamy, S., SRM University, Chennai, India; Maria Nancy, A., SRM University, Chennai, India; Veena, S., SRM University, Chennai, India","E-Learning applies information technology concepts in teaching and learning process. Numerous sophisticated project cost estimation models are available that estimates the cost of a project based on size, schedule and components used. But developing e-learning system is still in crisis. Capability to exactly predict the cost of e-learning system is dubious because traditional cost estimation methods fail to address the e-learning requirements and features. This leads to either under-estimate or over-estimate the cost for developing an e-learning system. The need to develop a separate cost model for e-learning system is rising. Hence, in this paper, cost estimation model for e-learning system is proposed. The proposed method first differentiates ordinary project with e-learning systems and trace for independent variables. These variables are building blocks of e-learning system. Regression analysis, a mathematical tool is used to calculate the cost of each independent variable with already incurred dependent variables. A linear regression formula is derived which estimates the cost of e-learning system. The proposed method is compared with COCOMO, COCOMO-I and SLIM models. The experimental results show that the Proposed method estimates the e-learning system in an accurate way when compared to traditional methods. å© International Science Press.",COCOMO; Cost Estimation; E-learning; FPA; LOC; Regression; SLIM,Article,Scopus,2-s2.0-85007416051
"Sheng B., Zhang C., Yin X., Lu Q., Cheng Y., Xiao T., Liu H.",Common intelligent semantic matching engines of cloud manufacturing service based on OWL-S,2016,International Journal of Advanced Manufacturing Technology,84,1-Apr,,103,118,no,,,,10.1007/s00170-015-7996-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947440077&doi=10.1007%2fs00170-015-7996-z&partnerID=40&md5=1b8ae85e49dfa8f629652014ef4de98b,"School of Mechanical and Electronic Engineering, Wuhan University of Technology, Wuhan, China; School of Mechanical Engineering, Hubei University of Technology, Wuhan, China","Sheng, B., School of Mechanical and Electronic Engineering, Wuhan University of Technology, Wuhan, China; Zhang, C., School of Mechanical and Electronic Engineering, Wuhan University of Technology, Wuhan, China; Yin, X., School of Mechanical and Electronic Engineering, Wuhan University of Technology, Wuhan, China; Lu, Q., School of Mechanical and Electronic Engineering, Wuhan University of Technology, Wuhan, China; Cheng, Y., School of Mechanical and Electronic Engineering, Wuhan University of Technology, Wuhan, China; Xiao, T., School of Mechanical and Electronic Engineering, Wuhan University of Technology, Wuhan, China; Liu, H., School of Mechanical Engineering, Hubei University of Technology, Wuhan, China","In order to improve the practicability of the search engine of cloud manufacturing platform in small- and medium-sized enterprises (SMEs), the paper studied the matching process‰Ûª diversity, heterogeneity, and multi-constraints between the intelligent matching engine and cloud manufacturing (CMfg) service, then established an intelligent searching engine of CMfg service in SMEs which is based on ontology language for service (OWL-S), and analyzed its matching degree quantization problems of matching process between ontology concept parameters and constraint parameters. To verify its validity, we have applied it in automobile/motorcycle accessory industry. By combining a quantization method, a matching algorithm and semantic similarity, an automobile/motorcycle accessory ontology database is established. It has fulfilled the semantic inference of the service resources ontology and has realized the valid and rapid intelligent matching of cloud searching. å© 2015, Springer-Verlag London.",Cloud manufacturing (CMfg); Knowledge base; Ontology library; OWL-S; Semantic similarity; Small- and medium-sized enterprises (SMEs),Article,Scopus,2-s2.0-84947440077
"Shen X., Minku L.L., Bahsoon R., Yao X.",Dynamic Software Project Scheduling through a Proactive-Rescheduling Method,2016,IEEE Transactions on Software Engineering,42,7,7365465,658,686,no,,,,10.1109/TSE.2015.2512266,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979243475&doi=10.1109%2fTSE.2015.2512266&partnerID=40&md5=b88110846c55069cba9ee7d2e0f9bb58,"B-DAT, CICAEET, School of Information and Control, Nanjing University of Information Science and Technology, No.219, Ning-Liu Road, Pu-Kou District, Nanjing, China; Department of Computer Science, University of Leicester, University Road, Leicester, United Kingdom; CERCIA, University of Birmingham, Edgbaston, Birmingham, United Kingdom","Shen, X., B-DAT, CICAEET, School of Information and Control, Nanjing University of Information Science and Technology, No.219, Ning-Liu Road, Pu-Kou District, Nanjing, China; Minku, L.L., Department of Computer Science, University of Leicester, University Road, Leicester, United Kingdom; Bahsoon, R., CERCIA, University of Birmingham, Edgbaston, Birmingham, United Kingdom; Yao, X., CERCIA, University of Birmingham, Edgbaston, Birmingham, United Kingdom","Software project scheduling in dynamic and uncertain environments is of significant importance to real-world software development. Yet most studies schedule software projects by considering static and deterministic scenarios only, which may cause performance deterioration or even infeasibility when facing disruptions. In order to capture more dynamic features of software project scheduling than the previous work, this paper formulates the project scheduling problem by considering uncertainties and dynamic events that often occur during software project development, and constructs a mathematical model for the resulting multi-objective dynamic project scheduling problem (MODPSP), where the four objectives of project cost, duration, robustness and stability are considered simultaneously under a variety of practical constraints. In order to solve MODPSP appropriately, a multi-objective evolutionary algorithm based proactive-rescheduling method is proposed, which generates a robust schedule predictively and adapts the previous schedule in response to critical dynamic events during the project execution. Extensive experimental results on 21 problem instances, including three instances derived from real-world software projects, show that our novel method is very effective. By introducing the robustness and stability objectives, and incorporating the dynamic optimization strategies specifically designed for MODPSP, our proactive-rescheduling method achieves a very good overall performance in a dynamic environment. å© 2016 IEEE.",dynamic software project scheduling; mathematical modeling; multi-objective evolutionary algorithms; Schedule and organizational issues; search-based software engineering,Article,Scopus,2-s2.0-84979243475
Sharma Y.,Effort estimation for program modification in object oriented development,2016,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),9789,,,334,345,no,,,,10.1007/978-3-319-42089-9_24,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978312742&doi=10.1007%2f978-3-319-42089-9_24&partnerID=40&md5=c147de88fcc429ad7cebcc834cf96020,"CSIS Department, Bits Pilani, Pilani, Rajasthan, India","Sharma, Y., CSIS Department, Bits Pilani, Pilani, Rajasthan, India","One of the major problems faced by software developers and managers is the estimation of efforts for the development and maintenance of a programming system. In this paper, estimation of efforts needed to update programs according to a given requirement change has been discussed for the Object Oriented (OO) environment. Since the demand for these changes has to be met quickly, a method is required to estimate the efforts for making the required changes. Methods exist for estimating the efforts in OO environment but none of them cater to the needs of updating requirements. We have proposed an up-gradation to the approach for effort estimation, which makes use of certain characteristics of the OO paradigm, specifically Inheritance and Encapsulation. We found that the degree of inheritance has to be considered for effort estimation because it plays an important role for identifying which methods need to be modified and others to be reused as it is. å© Springer International Publishing Switzerland 2016.",,Conference Paper,Scopus,2-s2.0-84978312742
Sharma S.,Expanded cloud plumes hiding Big Data ecosystem,2016,Future Generation Computer Systems,59,,,63,92,no,,,6,10.1016/j.future.2016.01.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957948996&doi=10.1016%2fj.future.2016.01.003&partnerID=40&md5=10ed6c04dc266019696b279ec6b7d9de,"Center for Survey Statistics and Methodology, Iowa State University, Ames, IA, United States","Sharma, S., Center for Survey Statistics and Methodology, Iowa State University, Ames, IA, United States","Today, a paradigm shift is being observed in science, where the focus is gradually shifting away from operation to data, which is greatly influencing the decision making also. The data is being inundated proactively from several sources in various forms; especially social media and in modern data science vocabulary is being recognized as Big Data. Today, Big Data is permeating through the bigger aspect of human life for scientific and commercial dependencies, especially for massive scale data analytics of beyond the exabyte magnitude. As the footprint of Big Data applications is continuously expanding, the reliability on cloud environments is also increasing to obtain appropriate, robust and affordable services to deal with Big Data challenges. Cloud computing avoids any need to locally maintain the overly scaled computing infrastructure that include not only dedicated space, but the expensive hardware and software also. Several data models to process Big Data are already developed and a number of such models are still emerging, potentially relying on heterogeneous underlying storage technologies, including cloud computing. In this paper, we investigate the growing role of cloud computing in Big Data ecosystem. Also, we propose a novel XCLOUDX {XCloudX, X...X}classification to zoom in to gauge the intuitiveness of the scientific name of the cloud-assisted NoSQL Big Data models and analyze whether XCloudX always uses cloud computing underneath or vice versa. XCloudX symbolizes those NoSQL Big Data models that embody the term ""cloud"" in their name, where X is any alphanumeric variable. The discussion is strengthen by a set of important case studies. Furthermore, we study the emergence of as-a-Service era, motivated by cloud computing drive and explore the new members beyond traditional cloud computing stack, developed in the past couple of years. å© 2016 Elsevier B.V. All rights reserved.",as-a-Service; Big Data; Cloud; IoT; Smart Data and Lakes; XCLOUDX,Article,Scopus,2-s2.0-84957948996
"Sharma H.K., Tomar R., Dumka A., Aswal M.S.",OpenECOCOMO: The algorithms and implementation of Extended Cost Constructive Model (E-COCOMO),2015,"Proceedings on 2015 1st International Conference on Next Generation Computing Technologies, NGCT 2015",,,7375225,773,778,no,,,,10.1109/NGCT.2015.7375225,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964503269&doi=10.1109%2fNGCT.2015.7375225&partnerID=40&md5=bd56aee005b762158d3244a5919511b4,"Centre for Information Technology, UPES, Dehradun, India; Faculty of Engineering Technology, Gurukul Kangri University, Haridwar, India","Sharma, H.K., Centre for Information Technology, UPES, Dehradun, India; Tomar, R., Centre for Information Technology, UPES, Dehradun, India; Dumka, A., Centre for Information Technology, UPES, Dehradun, India; Aswal, M.S., Faculty of Engineering Technology, Gurukul Kangri University, Haridwar, India","Traditional software development approaches is obsoleting day by day and the place is being taken by some new evolved approaches like. Cleanroom development approach, Agile development approach etc. due to the evolution in development methodology there is a strong need of evolution in estimation models also. In this work we have proposed an extended version of COCOMO that is E-COCOMO and based on this model a tool has been implemented called OpenECOCOMO. The evolved model is proposed for the new development methodologies and includes some more factors for estimation used in these new approaches. å© 2015 IEEE.",Cleanroom Software Engineering; COCOMO; Cost Drivers; E-COCOMO; Effort Estimation; OpenECOCOMO; SDLC,Conference Paper,Scopus,2-s2.0-84964503269
"Shariat Yazdi H., Angelis L., Kehrer T., Kelter U.","A framework for capturing, statistically modeling and analyzing the evolution of software models",2016,Journal of Systems and Software,118,,,176,207,no,,,,10.1016/j.jss.2016.05.010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969190349&doi=10.1016%2fj.jss.2016.05.010&partnerID=40&md5=b1964cd7dcad36eed7033a55fa3dc980,"Software Engineering Group, University of Siegen, Germany; Department of Informatics, Aristotle University of Thessaloniki, Greece; Department of Electronics, Informatics and Bioengineering, Politecnico di Milano, Italy","Shariat Yazdi, H., Software Engineering Group, University of Siegen, Germany; Angelis, L., Department of Informatics, Aristotle University of Thessaloniki, Greece; Kehrer, T., Software Engineering Group, University of Siegen, Germany, Department of Electronics, Informatics and Bioengineering, Politecnico di Milano, Italy; Kelter, U., Software Engineering Group, University of Siegen, Germany","This paper presents a new methodological framework for capturing and statistically modeling the evolution of models in model-driven software development. The framework captures the changes between revisions of models in terms of both low-level (internal) and high-level (developer-visible) edit operations applied between revisions. In our approach, evolution is modeled statistically by using ARMA, GARCH and mixed ARMA-GARCH models. Forecasting and simulation aspects of these time series models are thoroughly assessed. The suitability of the framework is shown by applying it to a large set of design models of real Java systems. Our analysis shows that mixed ARMA-GARCH models are superior to ARMA models. A main motivation for, and application of, the resulting statistical models is to control the generation of realistic model histories which are intended to be used for testing model versioning tools. We present the architecture of the model generator and show how to generate random sequences from the statistical models which control the generation process. Further usages of the statistical models include various forecasting and simulation tasks. å© 2016 Elsevier Inc. All rights reserved.",Forecasting; Model driven engineering; Simulation; Software model evolution analysis; Test model generation; Time series analysis,Article,Scopus,2-s2.0-84969190349
"Shao Y., Heath T., Zhu Y.",Developing an economic estimation system for vertical farms,2016,International Journal of Agricultural and Environmental Information Systems,7,2,,26,51,no,,,,10.4018/IJAEIS.2016040102,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978219447&doi=10.4018%2fIJAEIS.2016040102&partnerID=40&md5=b0c84ea903b28ca9d12953440dcbd03f,"University of Nottingham, Nottingham, United Kingdom; Department of Architecture and Urban Design, University of Nottingham, Nottingham, United Kingdom; Department of Architecture and Built Environment, University of Nottingham, Nottingham, United Kingdom","Shao, Y., University of Nottingham, Nottingham, United Kingdom; Heath, T., Department of Architecture and Urban Design, University of Nottingham, Nottingham, United Kingdom; Zhu, Y., Department of Architecture and Urban Design, University of Nottingham, Nottingham, United Kingdom, Department of Architecture and Built Environment, University of Nottingham, Nottingham, United Kingdom","The concept of vertical farming is nearly twenty years old, however, there are only a few experimental prototypes despite its many advantages compared to conventional agriculture. Significantly, financial uncertainty has been identified as the largest barrier to the realization of a 'real' vertical farm. Some specialists have provided ways to calculate costs and return on investment, however, most of them are superficial with calculations based on particular contextual circumstances. To move the concept forwards a reliable and flexible estimating tool, specific to this new building typology, is clearly required. A computational system, software named VFer, has therefore been developed by the authors to provide such a solution. This paper examines this highly flexible, customised system and results from several typical vertical farm configurations in three mega-cities (Shanghai, London and Washington DC) are used to elucidate the potential economic return of vertical farms. å© Copyright 2016, IGI Global.",Agricultural economics; Agricultural information system; Cost modeling methodology; Financial analysis; Greenhouse; Hydroponics; Simulator; Urban agriculture; Vertical farming,Article,Scopus,2-s2.0-84978219447
"Sgroi M.F., Zedde F., Barbera O., Stassi A., SebastiÌÁn D., Lufrano F., Baglio V., AricÌ_ A.S., Bonde J.L., Schuster M.",Cost analysis of direct methanol fuel cell stacks for mass production,2016,Energies,9,12,1008,,,no,,,,10.3390/en9121008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85002951667&doi=10.3390%2fen9121008&partnerID=40&md5=93c65aa242356e3c8d155549e9afed05,"Centro Ricerche FIAT S.C.p.A., Strada Torino 50, Orbassano, Italy; CNR-ITAE, Via Salita S. Lucia sopra Contesse 5, Messina, Italy; IRD, A/S Emil Neckelmanns vej 15 A and B, Odense SÌ÷, Denmark; FUMATECH BWT GmbH, Carl-Benz-Strasse 4, Bietigheim-Bissingen, Germany","Sgroi, M.F., Centro Ricerche FIAT S.C.p.A., Strada Torino 50, Orbassano, Italy; Zedde, F., Centro Ricerche FIAT S.C.p.A., Strada Torino 50, Orbassano, Italy; Barbera, O., CNR-ITAE, Via Salita S. Lucia sopra Contesse 5, Messina, Italy; Stassi, A., CNR-ITAE, Via Salita S. Lucia sopra Contesse 5, Messina, Italy; SebastiÌÁn, D., CNR-ITAE, Via Salita S. Lucia sopra Contesse 5, Messina, Italy; Lufrano, F., CNR-ITAE, Via Salita S. Lucia sopra Contesse 5, Messina, Italy; Baglio, V., CNR-ITAE, Via Salita S. Lucia sopra Contesse 5, Messina, Italy; AricÌ_, A.S., CNR-ITAE, Via Salita S. Lucia sopra Contesse 5, Messina, Italy; Bonde, J.L., IRD, A/S Emil Neckelmanns vej 15 A and B, Odense SÌ÷, Denmark; Schuster, M., FUMATECH BWT GmbH, Carl-Benz-Strasse 4, Bietigheim-Bissingen, Germany","Fuel cells are very promising technologies for efficient electrical energy generation. The development of enhanced system components and new engineering solutions is fundamental for the large-scale deployment of these devices. Besides automotive and stationary applications, fuel cells can be widely used as auxiliary power units (APUs). The concept of a direct methanol fuel cell (DMFC) is based on the direct feed of a methanol solution to the fuel cell anode, thus simplifying safety, delivery, and fuel distribution issues typical of conventional hydrogen-fed polymer electrolyte fuel cells (PEMFCs). In order to evaluate the feasibility of concrete application of DMFC devices, a cost analysis study was carried out in the present work. A 200 W-prototype developed in the framework of a European Project (DURAMET) was selected as the model system. The DMFC stack had a modular structure allowing for a detailed evaluation of cost characteristics related to the specific components. A scale-down approach, focusing on the model device and projected to a mass production, was used. The data used in this analysis were obtained both from research laboratories and industry suppliers specialising in the manufacturing/production of specific stack components. This study demonstrates that mass production can give a concrete perspective for the large-scale diffusion of DMFCs as APUs. The results show that the cost derived for the DMFC stack is relatively close to that of competing technologies and that the introduction of innovative approaches can result in further cost savings. å© 2016 by the authors; licensee MDPI.",Catalysts; Cost analysis; Direct methanol fuel cell (DMFC); Membranes; Stack,Article,Scopus,2-s2.0-85002951667
"Schwartz A., Do H.",Cost-effective regression testing through Adaptive Test Prioritization strategies,2016,Journal of Systems and Software,115,,,61,81,no,,,,10.1016/j.jss.2016.01.018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960881560&doi=10.1016%2fj.jss.2016.01.018&partnerID=40&md5=9b0cfcfba1c7ed8b6ab89a52e5660c8c,"University of South Carolina Upstate, Chesnee, SC, United States; University of North Texas, Denton, TX, United States","Schwartz, A., University of South Carolina Upstate, Chesnee, SC, United States; Do, H., University of North Texas, Denton, TX, United States","Regression testing is an important part of the software development life cycle. It is also very expensive. Many different techniques have been proposed for reducing the cost of regression testing. However, research has shown that the effectiveness of different techniques varies under different testing environments and software change characteristics. In prior work, we developed strategies to investigate ways of choosing the most cost-effective regression testing technique for a particular regression testing session. In this work, we empirically study the existing strategies presented in prior work as well as develop two additional Adaptive Test Prioritization (ATP) strategies using fuzzy analytical hierarchy process (AHP) and the weighted sum model (WSM). We also provide a comparative study examining each of the ATP strategies presented to date. This research will provide researchers and practitioners with strategies to utilize in regression testing plans as well as provide data to use when deciding which of the strategies would best fit their testing needs. The empirical studies provided in this research show that utilizing these strategies can improve the cost-effectiveness of regression testing. å© 2016 Elsevier Inc. All rights reserved.",Adaptive regression testing strategy; Regression testing; Test case prioritization,Article,Scopus,2-s2.0-84960881560
"Schultheiss F., HÌ_gglund S., StÌ´hl J.-E.",Modeling the cost of varying surface finish demands during longitudinal turning operations,2016,International Journal of Advanced Manufacturing Technology,84,5-Aug,,1103,1114,no,,,,10.1007/s00170-015-7750-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941367056&doi=10.1007%2fs00170-015-7750-6&partnerID=40&md5=03325e6165d4e69641b8b31f8ff5f74a,"Division of Production and Materials Engineering, Lund University, Lund, Sweden; Seco Tools AB, Fagersta, Sweden","Schultheiss, F., Division of Production and Materials Engineering, Lund University, Lund, Sweden; HÌ_gglund, S., Seco Tools AB, Fagersta, Sweden; StÌ´hl, J.-E., Division of Production and Materials Engineering, Lund University, Lund, Sweden","Tolerances, including geometry, dimension, and surface roughness, are an important part of production where the desire to manufacture quality products have to be weighed against the increase of manufacturing costs. The desired tolerance will influence the choice of both manufacturing method and machine tool. Given that machining is an adequate production method, variation of the required tolerances will imply a variation of the part cost which needs to be taken into account during production planning. Thus, the term ‰ÛÏtolerance cost‰Û is introduced. The paper presents a model for evaluating the tolerance cost in respect to the surface roughness during longitudinal turning operations, enabling a better comparison between different production alternatives. Through knowledge of the required surface roughness, it is possible to estimate appropriate cutting conditions. Knowledge of the cutting conditions and the part geometry then makes it possible to calculate the cycle time, information which in turn may be used for calculating the corresponding part cost. Through using experimental data, it is proven that the required surface roughness has a significant influence on the attained manufacturing cost. For instance, while longitudinally turning AISI 4140, it was shown that an improvement of the surface roughness from Ra = 3.2åÊë_m to Ra = 1.6åÊë_m will entail an increase of the part cost by roughly 20åÊ%. Similarly, a decrease of the required surface quality (larger Ra value) will imply a significantly reduced part cost. å© 2015, Springer-Verlag London.",Machining; Part cost; Surface roughness; Turning,Article,Scopus,2-s2.0-84941367056
"Scholtes I., Mavrodiev P., Schweitzer F.",From Aristotle to Ringelmann: a large-scale analysis of team productivity and coordination in Open Source Software projects,2016,Empirical Software Engineering,21,2,,642,683,no,,,,10.1007/s10664-015-9406-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84950257604&doi=10.1007%2fs10664-015-9406-4&partnerID=40&md5=2b9f96df7857068da67b680731155cf5,"ETH ZÌ_rich, Chair of Systems Design, Weinbergstrasse 56/58, Zurich, Switzerland","Scholtes, I., ETH ZÌ_rich, Chair of Systems Design, Weinbergstrasse 56/58, Zurich, Switzerland; Mavrodiev, P., ETH ZÌ_rich, Chair of Systems Design, Weinbergstrasse 56/58, Zurich, Switzerland; Schweitzer, F., ETH ZÌ_rich, Chair of Systems Design, Weinbergstrasse 56/58, Zurich, Switzerland","Complex software development projects rely on the contribution of teams of developers, who are required to collaborate and coordinate their efforts. The productivity of such development teams, i.e., how their size is related to the produced output, is an important consideration for project and schedule management as well as for cost estimation. The majority of studies in empirical software engineering suggest that - due to coordination overhead - teams of collaborating developers become less productive as they grow in size. This phenomenon is commonly paraphrased as Brooks‰Ûª law of software project management, which states that ‰ÛÏadding manpower to a software project makes it later‰Û. Outside software engineering, the non-additive scaling of productivity in teams is often referred to as the Ringelmann effect, which is studied extensively in social psychology and organizational theory. Conversely, a recent study suggested that in Open Source Software (OSS) projects, the productivity of developers increases as the team grows in size. Attributing it to collective synergetic effects, this surprising finding was linked to the Aristotelian quote that ‰ÛÏthe whole is more than the sum of its parts‰Û. Using a data set of 58 OSS projects with more than 580,000 commits contributed by more than 30,000 developers, in this article we provide a large-scale analysis of the relation between size and productivity of software development teams. Our findings confirm the negative relation between team size and productivity previously suggested by empirical software engineering research, thus providing quantitative evidence for the presence of a strong Ringelmann effect. Using fine-grained data on the association between developers and source code files, we investigate possible explanations for the observed relations between team size and productivity. In particular, we take a network perspective on developer-code associations in software development teams and show that the magnitude of the decrease in productivity is likely to be related to the growth dynamics of co-editing networks which can be interpreted as a first-order approximation of coordination requirements. å© 2015, Springer Science+Business Media New York.",Coordination; Open source software; Productivity factors; Repository mining; Social aspects of software engineering; Software engineering,Article,Scopus,2-s2.0-84950257604
"Schmitz B., Duffort F., Satzger G.",Managing Uncertainty in Industrial Full Service Contracts: Digital Support for Design and Delivery,2016,Proceedings - CBI 2016: 18th IEEE Conference on Business Informatics,1,,7780307,123,132,no,,,,10.1109/CBI.2016.22,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010441758&doi=10.1109%2fCBI.2016.22&partnerID=40&md5=4f5986d535d776d6cd4f882b9517b0ff,"Karlsruhe Service Research Institute, Karlsruhe Institute of Technology, Karlsruhe, Germany","Schmitz, B., Karlsruhe Service Research Institute, Karlsruhe Institute of Technology, Karlsruhe, Germany; Duffort, F., Karlsruhe Service Research Institute, Karlsruhe Institute of Technology, Karlsruhe, Germany; Satzger, G., Karlsruhe Service Research Institute, Karlsruhe Institute of Technology, Karlsruhe, Germany","In industrial maintenance, the backbone of the industrial service business, full-service contracts form new and increasingly popular value propositions. For operators of industrial equipment these contracts offer peace of mind at a fixed price, as service providers bear the cost for any action required to maintain or repair their clients' equipment after (unexpected) failures. In order to ensure profitable offers, providers require methods to both assess uncertainties in the contract design phase and to reduce their adverse business impact during service delivery. So far, research has primarily focused on the identification of uncertainties for various types of contracts. There has been a lack of focus, though, on how digital technologies can support a risk-based design of service contracts - either by reducing uncertainties (e.g. by employing technologies for data collection) or by mitigating their business impact (e.g. by reacting more effectively to unexpected events during service delivery). We address this research gap by elaborating on the interdependencies between uncertainties, their adverse business impact and measures to mitigate this impact. We portray the current state of the art in uncertainty management based on a literature review and complement these results with findings from exploratory interviews with experts of full-service providers in the industrial sector. Building upon the results, we discuss how digital technologies can improve uncertainty management in full-service maintenance contracts and, more generally, in service contracts overall. å© 2016 IEEE.",contract design; industrial service contracts; risk management; uncertainy management,Conference Paper,Scopus,2-s2.0-85010441758
"Sandberg M., Tyapin I., Kokkolaras M., Lundbladh A., Isaksson O.",A knowledge-based master model approach exemplified with jet engine structural design,2017,Computers in Industry,85,,,31,38,no,,,,10.1016/j.compind.2016.12.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007480242&doi=10.1016%2fj.compind.2016.12.003&partnerID=40&md5=708def428201fcf3c3b01694ace7ccbd,"LuleÌ´ University of Technology, LuleÌ´, Sweden; University of Agder, Kristiansand, Norway; McGill University, Montreal, Canada; GKN Aerospace, TrollhÌ_ttan, Sweden; Chalmers University of Technology, GÌ¦teborg, Sweden","Sandberg, M., LuleÌ´ University of Technology, LuleÌ´, Sweden; Tyapin, I., University of Agder, Kristiansand, Norway; Kokkolaras, M., McGill University, Montreal, Canada; Lundbladh, A., GKN Aerospace, TrollhÌ_ttan, Sweden; Isaksson, O., GKN Aerospace, TrollhÌ_ttan, Sweden, Chalmers University of Technology, GÌ¦teborg, Sweden","Successful product development requires the consideration of multiple engineering disciplines and the quantification of tradeoffs among conflicting objectives from the very early design phases. The single-largest challenge to do so is the lack of detailed design information. A possible remedy of this issue is knowledge-based engineering. This paper presents a knowledge-based master model approach that enables the management of concurrent design and analysis models within different engineering disciplines in relation to the same governing product definition. The approach is exemplified on an early phase structural design of a turbo-fan jet engine. The model allows geometric-, structural mechanics- and rotor-dynamic- models to be concurrently integrated into a multi-disciplinary design and optimization loop. å© 2016 Elsevier B.V.",Jet engine; Knowledge-based engineering; Master model; Multidisciplinary analysis and design,Article,Scopus,2-s2.0-85007480242
"Salmi A., David P., Blanco E., Summers J.D.",A review of cost estimation models for determining assembly automation level,2016,Computers and Industrial Engineering,98,,,246,259,no,,,,10.1016/j.cie.2016.06.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974712811&doi=10.1016%2fj.cie.2016.06.007&partnerID=40&md5=80efc717b01e2f59d13353bdc0e5299b,"Univ. Grenoble Alpes, G-SCOP Laboratory, 46 Avenue Flix Viallet, Grenoble, France; CNRS, G-SCOP, Grenoble, France; Department of Mechanical Engineering, Clemson UniversitySC, United States","Salmi, A., Univ. Grenoble Alpes, G-SCOP Laboratory, 46 Avenue Flix Viallet, Grenoble, France, CNRS, G-SCOP, Grenoble, France; David, P., Univ. Grenoble Alpes, G-SCOP Laboratory, 46 Avenue Flix Viallet, Grenoble, France, CNRS, G-SCOP, Grenoble, France; Blanco, E., Univ. Grenoble Alpes, G-SCOP Laboratory, 46 Avenue Flix Viallet, Grenoble, France, CNRS, G-SCOP, Grenoble, France; Summers, J.D., Department of Mechanical Engineering, Clemson UniversitySC, United States","The aim of this research is to support early phase cost estimation for assembly systems design and automation decision. During this phase, various alternatives of assembly systems with different automation levels can be generated. The alternatives generation is performed using available information on product design, assembly sequences, and planned production information. The issue is to assess given alternatives, identify, and opt for the most appropriate one. Several criteria have to be considered in this decision. The economic aspect represents one of the most preponderant, including cost and profitability prediction. The importance and challenges of this complex issue are highlighted in this paper with feedbacks from manufacturers and the literature. The literature in the field is reviewed, presented, and analyzed. For this sake, a selection of cost models is performed covering a wide chronological range, journals, and fields including assembly and manufacturing models. Classification techniques of cost estimation works are presented and exploited in the proposed review. It is used to filter and discuss models suitability for the defined assembly automation decision issue. The most appropriate models are more thoroughly reviewed and discussed. Useful literature costing techniques, features, and relevant cost drivers are also identified. They cover multiple aspects as production information, resources features or performances. Finally, the review findings are illustrated by a cost estimation outline proposal to support early phase cost prediction for assembly systems design and automation decision. å© 2016 Elsevier Ltd.",Assembly; Cost estimation; Economic model; Level of automation; Systems design,Review,Scopus,2-s2.0-84974712811
"Saleh A.I., Rabie A.H., Abo-Al-Ez K.M.",A data mining based load forecasting strategy for smart electrical grids,2016,Advanced Engineering Informatics,30,3,,422,448,no,,,,10.1016/j.aei.2016.05.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974803159&doi=10.1016%2fj.aei.2016.05.005&partnerID=40&md5=560d38609253e42a9bfd0f1c08560c33,"Computer Engineering and Systems Department, Faculty of Engineering, Mansoura University, Mansoura, Egypt; Electrical Engineering Department, Faculty of Engineering, Mansoura University, Mansoura, Egypt","Saleh, A.I., Computer Engineering and Systems Department, Faculty of Engineering, Mansoura University, Mansoura, Egypt; Rabie, A.H., Computer Engineering and Systems Department, Faculty of Engineering, Mansoura University, Mansoura, Egypt; Abo-Al-Ez, K.M., Electrical Engineering Department, Faculty of Engineering, Mansoura University, Mansoura, Egypt","Smart electrical grids, which involve the application of intelligent information and communication technologies, are becoming the core ingredient in the ongoing modernization of the electricity delivery infrastructure. Thanks to data mining and artificial intelligence techniques that allow the accurate forecasting of power, which alleviates many of the cost and operational challenges because, power predictions become more certain. Load forecasting (LF) is a vital process for the electrical system operation and planning as it provides intelligence to energy management. In this paper, a novel LF strategy is proposed by employing data mining techniques. In addition to a novel load estimation, the proposed LF strategy employs new outlier rejection and feature selection methodologies. Outliers are rejected through a Distance Based Outlier Rejection (DBOR) methodology. On the other hand, selecting the effective features is accomplished through a Hybrid technique that combines evidence from two proposed feature selectors. The first is a Genetic Based Feature Selector (GBFS), while the second is a Rough set Base Feature Selector (RBFS). Then, the filtered data is used to give fast and accurate load prediction through a hybrid KN3B predictor, which combines KNN and NB classifiers. Experimental results have proven the effectiveness of the new outlier rejection, feature selection, and load estimation methodologies. Moreover, the proposed LF strategy has been compared against recent LF strategies. It is shown that the proposed LF strategy has a good impact in maximizing system reliability, resilience and stability as it introduces accurate load predictions. å© 2016 Elsevier Ltd. All rights reserved.",Data mining; Feature selection; Load forecasting; Outlier rejection; Smart grids,Article,Scopus,2-s2.0-84974803159
"Salam A., Bhuiyan N.",Estimating design effort using parametric models: A case study at Pratt & Whitney Canada,2016,Concurrent Engineering Research and Applications,24,2,,129,138,no,,,,10.1177/1063293X16631800,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973449271&doi=10.1177%2f1063293X16631800&partnerID=40&md5=44d8210903798d8e4c21235c9ece277f,"Concordia University, 1455 de Maisonneuve BlvdW, Montreal, QC, Canada","Salam, A., Concordia University, 1455 de Maisonneuve BlvdW, Montreal, QC, Canada; Bhuiyan, N., Concordia University, 1455 de Maisonneuve BlvdW, Montreal, QC, Canada","Aerospace is very important to the Canadian economy, with over 80,000 employees and generating over $20 billion dollars in revenue. However, this industry like many others is facing many challenges. One of them is the difficulty in being able to estimate design effort required in a design project, which impacts not only resource requirements and lead-time but also the final cost. This article presents the findings of a case study conducted for Pratt & Whitney Canada, recognized as a global leader in the design and manufacturing of aircraft engines. The study models parametric cost estimation relationships to estimate the design effort of integrated blade-rotor low-pressure compressor fans. Several effort drivers are selected to model the relationship. Comparative analyses of three types of models are conducted. The model with the best accuracy and significance in design estimation is retained. å© SAGE Publications.",design effort; parametric models; Pratt & Whitney Canada,Article,Scopus,2-s2.0-84973449271
"Saif A., Abbas S., Fayed Z.","Finding the best software project options by PDBO algorithm for improving software development effort, time and quality",2016,ACM International Conference Proceeding Series,09-11-May-2016,,2908448,304,311,no,,,,10.1145/2908446.2908448,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84998655273&doi=10.1145%2f2908446.2908448&partnerID=40&md5=7cdb2afbbdfd88e2614d6e72b8927edc,"CS Dept., Ain Shams Uni., Egypt","Saif, A., CS Dept., Ain Shams Uni., Egypt; Abbas, S., CS Dept., Ain Shams Uni., Egypt; Fayed, Z., CS Dept., Ain Shams Uni., Egypt","There are many project changes (project options) that affect project estimates (effort, time and quality). The goal is to find the best project changes that optimize those project estimates. This problem was solved by some techniques including STAR (AI search tool), however, this technique didn't produce stable results, and case-based reasoning (CBR) which requires local dataset to predict the estimates. In this paper, problem data based optimization (PDBO) algorithm is adapted to optimize project estimates for different project goals without using past dataset and there are little/no options available about the software project. This paper also compares PDBO with intelligent water drops (IWD) and genetic algorithm (GA) in terms of stability, solution quality and processing time. The experiments are conducted on a software project with size 25KSLOC and the Jet Propulsion Laboratory (JPL) flight software project data. The results are stable for several runs of PDBO and are satisfactory. å© 2016 ACM.",COCOMO models; Metaheuristic algorithms; Software quality optimization,Conference Paper,Scopus,2-s2.0-84998655273
"Saha A., Chongder A., Mandal S.B., Chakrabarti A.",Synthesis of Vertex Coloring Problem Using Grover's Algorithm,2015,"Proceedings - 2015 IEEE International Symposium on Nanoelectronic and Information Systems, iNIS 2015",,,7434406,101,106,no,,,,10.1109/iNIS.2015.55,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966662502&doi=10.1109%2fiNIS.2015.55&partnerID=40&md5=3aa20a0708cb0e10543e48c304ddabd4,,"Saha, A.; Chongder, A.; Mandal, S.B.; Chakrabarti, A.","Many applications such as Graph coloring, Triangle finding, Boolean satisfiability, Traveling salesman problem can be solved by Grover's search algorithm, which is one of the remarkable quantum computing algorithm. To design a quantum circuit for a given quantum algorithm, which involves Grover's search, we need to define an oracle circuit specific to the given algorithm and the diffusion operator for amplification of the desired quantum state. In this paper, we propose a quantum circuit implementation for the oracle of the vertex coloring problem based on Grover's algorithmic approach. To the best of our knowledge, this is a first of its kind approach in regards to the quantum circuit synthesis of the vertex coloring oracle in binary quantum domain. We have performed the synthesis of the proposed oracle circuit for the six commonly available Physical Machine Description (PMD) gate libraries using the Fault Tolerant Quantum Logic Synthesis (FTQLS) tool. The synthesis results have been presented to understand the cost estimation of the oracle circuit for the various PMDs in terms of number of quantum operations and number of cycles. å© 2015 IEEE.",Grover's Search; PMD; Vertex coloring,Conference Paper,Scopus,2-s2.0-84966662502
"Sabbagh Jafari S., Ziaaddini F.",Optimization of software cost estimation using harmony search algorithm,2016,"1st Conference on Swarm Intelligence and Evolutionary Computation, CSIEC 2016 - Proceedings",,,7482119,131,135,no,,,,10.1109/CSIEC.2016.7482119,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978437350&doi=10.1109%2fCSIEC.2016.7482119&partnerID=40&md5=59a242cf6cd5e2ae7b032f2f9e2d4198,"Department of Computer Engineering, University of Vali Asr Rafsanjan, Rafsanjan-Kerman, Iran; Department of Information Technology, Kerman Branch, Islamic Azad University, Kerman, Iran","Sabbagh Jafari, S., Department of Computer Engineering, University of Vali Asr Rafsanjan, Rafsanjan-Kerman, Iran; Ziaaddini, F., Department of Information Technology, Kerman Branch, Islamic Azad University, Kerman, Iran",Accurate estimation of software costs is one of the key and most important activities in development of software projects. Uncertainty and intricacy of software systems has made it difficult to efficiently and effectively develop software and has led to software systems' tendency to new optimal techniques. Prediction of the required effort for developing software has benefited significant progression owing the application of Meta-heuristic optimization Algorithms such algorithms have the potential to be applied as credible and useful tools in software cost estimation. In this paper the COCOMO effort estimation method is optimized using Meta-heuristic harmony search Algorithm. Nasa dataset was used in order to test the results. The purpose of optimization methods in software efforts estimation is to decrease the Mean Magnitude of Relative Error which in this case led to almost 21% optimization. å© 2016 IEEE.,COCOMO; Cost Estimation; Harmony Search Algorithm; Meta-Heuristic Algorithms,Conference Paper,Scopus,2-s2.0-84978437350
"Sabahat N., Malik A.A., Azam F.",Size estimation of open source board-based software games,2015,"ICOSST 2015 - 2015 International Conference on Open Source Systems and Technologies, Proceedings",,,7396414,126,131,no,,,,10.1109/ICOSST.2015.7396414,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964865216&doi=10.1109%2fICOSST.2015.7396414&partnerID=40&md5=b400263df0d7c5a662fbcc1b5e244e70,"National University of Science and Technology (NUST), Islamabad, Pakistan; National University of Computer and Emerging Sciences (NUCES), Lahore, Pakistan","Sabahat, N., National University of Science and Technology (NUST), Islamabad, Pakistan; Malik, A.A., National University of Computer and Emerging Sciences (NUCES), Lahore, Pakistan; Azam, F., National University of Science and Technology (NUST), Islamabad, Pakistan","Software effort, schedule, and cost estimation has the highest utility at the time of inception. Since software size is one of the most important determinant of software effort (and, hence, cost), it is extremely beneficial to estimate size early. This early estimation of size is likely to result in better planning for projects dealing with the development of software games. Yet, despite its utility, a survey of the existing literature does not reveal a technique developed for this purpose. In particular, this paper focuses on the size estimation of open source board-based software games. A new software size estimation model is proposed for this sub-domain. In this research, 52 open source board-based software games are thoroughly examined and analyzed. After shortlisting potential predictors of software size for this sub-domain, forward stepwise Multiple Linear Regression (MLR) is used for model fitting. The results show that our model has reasonable estimation accuracy as indicated by the value of the coefficient of determination (R2 = 0.699). By providing a reasonably accurate estimate of software size early in the life cycle, our model makes it easier and simpler to plan and manage the development of open source board-based software games. å© 2015 IEEE.",Game Sizing; MMRE; Multiple Linear Regression (MLR); Open source; PRED; Project Management; Project Planning; Simple Linear Regression (SLR); Software Size Estimation,Conference Paper,Scopus,2-s2.0-84964865216
"Roberts B., Mazzuchi T., Sarkani S.",Engineered Resilience for Complex Systems as a Predictor for Cost Overruns,2016,Systems Engineering,19,2,,111,132,no,,,,10.1002/sys.21339,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978115997&doi=10.1002%2fsys.21339&partnerID=40&md5=4b2080200581b0e8bd29a574f2d3f753,"Department of Engineering Management and Systems Engineering, The George Washington University, Washington, DC, United States","Roberts, B., Department of Engineering Management and Systems Engineering, The George Washington University, Washington, DC, United States; Mazzuchi, T., Department of Engineering Management and Systems Engineering, The George Washington University, Washington, DC, United States; Sarkani, S., Department of Engineering Management and Systems Engineering, The George Washington University, Washington, DC, United States","The ever-increasing complexity of large-scale, ubiquitous systems, and internetworked Systems of Systems (SoS), have resulted in non-linear challenges for systems engineering and program management such as interdependent, dynamic, and emergent behaviors that influence design and development cost. In order to invest, make decisions, and effectively manage technology developments, ‰ÛÏshould-cost‰Û estimates are used to baseline systems to what is considered affordable over the lifecycle. In addition to a higher likelihood for overrun, complex systems traditionally require major acquisitions, budgets, and development schedules where failure results in increased cost consequences. Behavioral properties (such as emergence) can influence performance resulting in dynamic and unpredictable effects on networked platform operation, system capability, and mission effectiveness. Data analyzed from 526 major weapons programs and from subject-matter expert judgment are explored to propose: (1) a new approach to quantify these risks using prescriptive measures related to complexity, adaptability, and resilience, (2) identify leading indicators to reduce the likelihood of cost overrun, (3) suggest measurements based on research findings and systems heuristics to improve lifecycle cost estimation, and (4) present applications of Engineering Resilience to meet the growing complexity, interdependence, and scale of systems under constrained budgets with a growing demand for affordability. å© 2016 Wiley Periodicals, Inc.",AS02 Government; AS09 Other Application Sectors; Defense & Security; SEE02 Complexity Science; SEE10 Project Planning/Assessment/Control; SEE11 Decision Analysis/Management; SEE22 Life-cycle Costing and Economic valuation; SEE25 Systems of Systems (SoS),Article,Scopus,2-s2.0-84978115997
"Ripperda S., Krause D.",Cost effects of modular product family structures: Methods and quantification of impacts to support decision making,2017,"Journal of Mechanical Design, Transactions of the ASME",139,2,2593216,,,no,,,,10.1115/1.4035430,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009112223&doi=10.1115%2f1.4035430&partnerID=40&md5=11dec0cdb902f01b5006dbbfcf2d870c,"Institute for Product Development and Mechanical Engineering Design (PKT), Hamburg University of Technology, DenickestraÌÙe 17, Hamburg, Germany","Ripperda, S., Institute for Product Development and Mechanical Engineering Design (PKT), Hamburg University of Technology, DenickestraÌÙe 17, Hamburg, Germany; Krause, D., Institute for Product Development and Mechanical Engineering Design (PKT), Hamburg University of Technology, DenickestraÌÙe 17, Hamburg, Germany","Customer demands and global markets prompt companies to offer increasing product variety. The use of modular product structures is a possible strategy for providing the necessary external variety to the market and reducing costs by lowering internal variety within the company. Current research provides several approaches for developing modular product family structures. As modularity is a gradual property, these methods generate different product structure concepts and companies have to decide at an early stage and without detailed information which concepts to implement. Most existing modularization methods offer only little or no support for decision making, particularly in terms of cost effects. This article illustrates the cost effects of variety and modular product family structures, the various cost impacts of variety management strategies and modularization methods in a literature review. A new approach to quantify these cost effects to support concept selection during modular product family design is introduced. å© 2017 by ASME.",Complexity cost; Decision making; Design method; Modular product structures; Product family,Article,Scopus,2-s2.0-85009112223
"Rezvani S., Moheimani N.R., Bahri P.A.",Techno-economic assessment of CO2 bio-fixation using microalgae in connection with three different state-of-the-art power plants,2016,Computers and Chemical Engineering,84,,,290,301,no,,,3,10.1016/j.compchemeng.2015.09.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942935942&doi=10.1016%2fj.compchemeng.2015.09.001&partnerID=40&md5=1bc9456a92b0a7e58e2fafb070601878,"School of Engineering and Information Technology, Murdoch University, Murdoch, WA, Australia; Algae R and D Center, School of School of Veterinary and Life Sciences, Murdoch University, Murdoch, WA, Australia","Rezvani, S., School of Engineering and Information Technology, Murdoch University, Murdoch, WA, Australia; Moheimani, N.R., Algae R and D Center, School of School of Veterinary and Life Sciences, Murdoch University, Murdoch, WA, Australia; Bahri, P.A., School of Engineering and Information Technology, Murdoch University, Murdoch, WA, Australia","Large-scale microalgae cultivations for CO2 bio-sequestration from power plants can be an alternative process to conventional technologies if the capital investments associated with carbon capture, transport and storage can be reduced or avoided. In order to counterbalance the costs required to operate massive microalgae cultivations, it is necessary to create additional revenues through biomass sales. This manuscript examines the techno-economics of microalgae cultivations for the integration with three power plant technologies using an artificial neural network model. The economics are estimated using the net present value approach. The assessment is carried out at photosynthesis efficiencies ranging from 2% to 6%. The sensitivity assessment shows microalgae selling prices in the range of $440-1028/t at a photosynthetic efficiency of 4% for low and high cost scenarios in order to achieve an electricity price similar to that from a conventional power plant without a CO2 capture and storage. å© 2015 Elsevier Ltd.",Artificial neural networks; CO2 bio-fixation; Microalgae cultivation; Photosynthetic efficiency; Solar radiation,Article,Scopus,2-s2.0-84942935942
"Raza M., Faria J.P.",A model for analyzing performance problems and root causes in the personal software process,2016,Journal of Software: Evolution and Process,28,4,,254,271,no,,,,10.1002/smr.1759,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947484002&doi=10.1002%2fsmr.1759&partnerID=40&md5=ac4f8e1d0d70004ccfff579db1639180,"INESC TEC, Department of Informatics Engineering, Faculty of Engineering, University of Porto, Porto, Portugal","Raza, M., INESC TEC, Department of Informatics Engineering, Faculty of Engineering, University of Porto, Porto, Portugal; Faria, J.P., INESC TEC, Department of Informatics Engineering, Faculty of Engineering, University of Porto, Porto, Portugal","High-maturity software development processes, such as the Team Software Process and the accompanying Personal Software Process (PSP), can generate significant amounts of data that can be periodically analyzed to identify performance problems, determine their root causes, and devise improvement actions. However, there is a lack of tool support for automating that type of analysis, and hence diminish the manual effort and expert knowledge required. So, we propose in this paper a comprehensive performance model, addressing time estimation accuracy, quality, and productivity, to enable the automated (tool based) analysis of performance data produced by PSP developers, namely, identify and rank performance problems and their root causes. A PSP data set referring to more than 30 000 projects was used to validate and calibrate the model. Copyright å© 2015 John Wiley & Sons, Ltd.",performance analysis; performance model; Personal Software Process,Conference Paper,Scopus,2-s2.0-84947484002
"Ramirez-Noriega A., Juarez-Ramirez R., Navarro R., Lopez-Martinez J.",Using Bayesian networks to obtain the task's parameters for schedule planning in scrum,2016,"Proceedings - 2016 4th International Conference in Software Engineering Research and Innovation, CONISOFT 2016",,,7477927,167,174,no,,,,10.1109/CONISOFT.2016.33,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978194647&doi=10.1109%2fCONISOFT.2016.33&partnerID=40&md5=7628f017c669768dc5c00d701e057730,"School of Chemical and Engineering, Autonomous University of Baja California, Tijuana, Baja California, Mexico","Ramirez-Noriega, A., School of Chemical and Engineering, Autonomous University of Baja California, Tijuana, Baja California, Mexico; Juarez-Ramirez, R., School of Chemical and Engineering, Autonomous University of Baja California, Tijuana, Baja California, Mexico; Navarro, R., School of Chemical and Engineering, Autonomous University of Baja California, Tijuana, Baja California, Mexico; Lopez-Martinez, J., School of Chemical and Engineering, Autonomous University of Baja California, Tijuana, Baja California, Mexico","Planning Poker is a consensus-based technique mostly used for estimating effort or relative size of software development goals. This is applicable to estimate the size ofuser stories, developing releases and iteration plans. It is used generally with Scrum. Planning Poker has a lot of benefits, however, this method is not entirely efficient because the result is always based on the observation of an expert. This paper proposes the identification and validation of the most important factors taken into account by Scrum teams to assign complexity and importance to tasks, which are two important factors to consider for task scheduling. We define a knowledge structure that represents different aspects and the influence of each factor in the final decision. We use a Bayesian Network to co-relate these factors in order to have accurate in the estimation. The Bayesian Network give us the complexity of a task according to Fibonacci scale. Using our technique, software teams can focus in the most important tasks to obtain a quality software. å© 2016 IEEE.",complexity; expert judgment; Planning poker; scrum; task estimation,Conference Paper,Scopus,2-s2.0-84978194647
"Rajan K., Kakadia D., Curino C., Krishnan S.",PerfOrator: Eloquent performance models for resource optimization,2016,"Proceedings of the 7th ACM Symposium on Cloud Computing, SoCC 2016",,,,415,427,no,,,,10.1145/2987550.2987566,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995622065&doi=10.1145%2f2987550.2987566&partnerID=40&md5=4f3f8c018a12d706f1049e7b1e0a6d33,"Microsoft, United States","Rajan, K., Microsoft, United States; Kakadia, D., Microsoft, United States; Curino, C., Microsoft, United States; Krishnan, S., Microsoft, United States","Query Optimization focuses on finding the best query execution plan, given fixed hardware resources. In BigData settings, both pay-as-you-go clouds and on-prem shared clusters, a complementary challenge emerges: Resource Optimization: find the best hardware resources, given an execution plan. In this world, provisioning is almost instantaneous and time-varying resources can be acquired on a per-query basis. This allows us to optimize allocations for completion time, resource usage, dollar cost, etc. These optimizations have a huge impact on performance and cost, and pivot around a core challenge: faithful resource-to-performance models for arbitrary BigData queries. This task is challenging for users and tools alike due to lack of good statistics (high-velocity, unstructured data), frequent use of UDFs, impact on performance of different hardware types and a lack of understanding of parallel execution at such a scale. We address this with PerfOrator, a novel approach to resource-to-performance modeling. PerfOrator employs nonlinear regression on profile runs to model arbitrary UDFs, calibration queries to generalize across hardware platforms, and analytical framework models to account for parallelism. The resulting estimates are orders of magnitude more accurate than existing approaches (e.g, Hive's optimizer), and have been successfully employed in two resource optimization scenarios: 1) optimize provisioning of clusters in cloud settings-with decisions within 1% of optimal, 2) reserve skyline of resources for SLA jobs-with accuracies over 10ÌÑ better than human experts. å© 2016 Copyright held by the owner/author(s).",,Conference Paper,Scopus,2-s2.0-84995622065
"Rahbari-Asr N., Chow M.-Y., Chen J., Deng R.",Distributed Real-Time Pricing Control for Large-Scale Unidirectional V2G with Multiple Energy Suppliers,2016,IEEE Transactions on Industrial Informatics,12,5,7470419,1953,1962,no,,,,10.1109/TII.2016.2569584,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84999188137&doi=10.1109%2fTII.2016.2569584&partnerID=40&md5=8b31efda91c283b05e76394345c45efa,"Department of Electrical and Computer Engineering, North Carolina State University, Raleigh, NC, United States; Department of Electrical and Computer Engineering, University of Alberta, Edmonton, AB, Canada; Department of Control Science and Engineering, State Key Laboratory of Industrial Control Technology, Institute of Industrial Process Control, Hangzhou, China","Rahbari-Asr, N., Department of Electrical and Computer Engineering, North Carolina State University, Raleigh, NC, United States; Chow, M.-Y., Department of Electrical and Computer Engineering, North Carolina State University, Raleigh, NC, United States; Chen, J., Department of Electrical and Computer Engineering, University of Alberta, Edmonton, AB, Canada; Deng, R., Department of Control Science and Engineering, State Key Laboratory of Industrial Control Technology, Institute of Industrial Process Control, Hangzhou, China","With the increasing trend in adoption of plug-in hybrid and plug-in electric vehicles, they will play a prominent role in the future electric energy market by acting as responsive loads to increase the grid stability and facilitate the integration of renewables. However, due to the large number of controllable devices in the future grid, central vehicle to grid (V2G) management would be challenging and vulnerable to single points of failure. This paper introduces a novel distributed approach for optimal management of unidirectional V2G considering multiple energy suppliers. Each charging station as well as each energy supplier is equipped with a local price regulator to control the price paid to the energy suppliers and the price paid by the vehicles through coordination with their neighbors. In response to the updated prices, the vehicles adjust their charging rates and energy suppliers adjust their production to maximize their benefit. The main advantages of the proposed approach are that it manages unidirectional V2G in a fully distributed way considering multiple energy suppliers and vehicles, and it converges to the global optimum despite the greedy behavior of the individuals. å© 2016 IEEE.",Consensus networks; demand response; distributed control; distributed optimization; Karush-Kuhn-Tucker (KKT) conditions; Unidirectional V2G,Article,Scopus,2-s2.0-84999188137
"Radosavljevic M., Georgakarakos G., Lafond S., Vukobratovic D.",Fast coding unit selection based on local texture characteristics for HEVC intra frame,2015,"2015 IEEE Global Conference on Signal and Information Processing, GlobalSIP 2015",,,7418424,1377,1381,no,,,,10.1109/GlobalSIP.2015.7418424,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964756522&doi=10.1109%2fGlobalSIP.2015.7418424&partnerID=40&md5=eee59c9ce78f8f690bc09b746316b029,"Embedded Systems Laboratory, Abo Akademi University, Finland; Dept. of Power, Electronics and Communication Engineering, University of Novi Sad, Serbia","Radosavljevic, M., Embedded Systems Laboratory, Abo Akademi University, Finland; Georgakarakos, G., Embedded Systems Laboratory, Abo Akademi University, Finland; Lafond, S., Embedded Systems Laboratory, Abo Akademi University, Finland; Vukobratovic, D., Dept. of Power, Electronics and Communication Engineering, University of Novi Sad, Serbia","High Efficiency Video Coding (HEVC) is a novel video compression standard. It provides significantly better performance than its predecessor, but introduces high computational complexity due its hierarchical block structure. Determining Coding Units (CU), Prediction Units (PU), and Transform Units (TU) size is a time consuming process, as block partitioning decisions are based on rate-distortion optimization (RDO). Taking into account all possible outcomes in terms of different block sizes and different prediction modes that HEVC can exploit, RDO is computationally very intense. To speed-up the block partitioning decisions we propose a novel approach for the CU size selection for intra frames based on local image characteristics. We use data from the source image itself to make decisions on whether to split CU or not. Split decisions are based on the histogram matching of the Local Binary Patterns on two consecutive CU depths. Results show that encoding time can be reduced compared to the HM16.2 reference software by bypassing RDO calculation for some specific CUs. Different performance-complexity configurations are investigated. Based on the proposed configuration, performance loss varies from almost negligible up to 0.87dB using BD-PSNR metric. However, speed-up is from 5.4% up to 80.2% on average. å© 2015 IEEE.",,Conference Paper,Scopus,2-s2.0-84964756522
"Radhakrishnan B.M., Srinivasan D., Mehta R.",Fuzzy-Based Multi-Agent System for Distributed Energy Management in Smart Grids,2016,"International Journal of Uncertainty, Fuzziness and Knowlege-Based Systems",24,5,,781,803,no,,,,10.1142/S0218488516500355,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991277973&doi=10.1142%2fS0218488516500355&partnerID=40&md5=f8b36220bc2f28e97383cc2937f9b697,"National University of Singapore, Lower Kent Ridge Road, Singapore, Singapore","Radhakrishnan, B.M., National University of Singapore, Lower Kent Ridge Road, Singapore, Singapore; Srinivasan, D., National University of Singapore, Lower Kent Ridge Road, Singapore, Singapore; Mehta, R., National University of Singapore, Lower Kent Ridge Road, Singapore, Singapore","Energy Management Systems have become an imperative aspect of smart grids, owing to the enormous challenges imposed due to real-time pricing, distributed generation and integration of intermittent renewables. Due to the uncertainty associated with renewable sources and prominent fluctuations in the load demand, it is extremely important to maintain the overall energy balance in such grids. In this paper, the distributed energy management is achieved using a Multi-agent System which provides a flexible and reliable solution to control and manage smart grids. Adaptive fuzzy systems are designed to instill intelligent decision making capability in the agents of multi-agent system. When renewable sources are inadequate, the sustainability of the system is not guaranteed and multi-agent system is capable of deciding the mode of operation such that the system reliability and performance is not compromised. The proposed algorithm maintains power balance in the system and also sustains desired values for the State of Charge of storage units in order to guarantee extended battery life. The Energy management system also implements a cost optimization algorithm based on the Particle Swarm Optimization technique, to minimize operating costs and maximize profits earned by the grid. The proposed energy management algorithm is tested and validated on a practical test system which inherits most of the features of a small-scale smart grid. å© 2016 World Scientific Publishing Company.",Adaptive fuzzy decision systems; cost optimization; distributed control; energy management systems; multi-agent systems,Article,Scopus,2-s2.0-84991277973
"Quesada-LÌ_pez C., Jenkins M.",Function point structure and applicability: A replicated study,2016,Journal of Object Technology,15,3,26,,,no,,,1,10.5381/jot.2016.15.3.a2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981165509&doi=10.5381%2fjot.2016.15.3.a2&partnerID=40&md5=1a73209682fd331f863c9a2f301bffbe,"Center for ICT Research (CITIC), University of Costa Rica, San JosÌ©, Costa Rica; Department of Computer Science, University of Costa Rica, Costa Rica","Quesada-LÌ_pez, C., Center for ICT Research (CITIC), University of Costa Rica, San JosÌ©, Costa Rica, Department of Computer Science, University of Costa Rica, Costa Rica; Jenkins, M., Center for ICT Research (CITIC), University of Costa Rica, San JosÌ©, Costa Rica","Background: The complexity of providing accurate functional software size and effort prediction models is well known in the software industry. Function point analysis (FPA) is currently one of the most accepted software functional size metrics in the industry, but it is hardly automatable and generally requires a lengthy and costly process. Objectives: This paper reports on a family of replications carried out on a subset of the International Software Benchmarking Standards Group dataset (ISBSG R12) to evaluate the structure and applicability of function points. The goal of this replication is to aggregate evidence about internal issues of FPA as a metric, and to confirm previous results using a different set of data. Methods: A subset of 202 business application projects from 2005 to 2011 was analyzed. FPA counting was analyzed in order to determine the extent to which the basic functional components (BFC) were independent of each other and thus appropriate for an additive model of size. The correlations among effort and BFCs and unadjusted function points (UFP) were assessed in order to determine whether a simplified sizing metric might be appropriate to simplify effort prediction models. Prediction models were constructed and evaluated in terms of accuracy. Results: The results confirmed that some BFCs of the FPA method are correlated. There is a relationship between BFCs and effort. That suggest that prediction models based on transactional functions (TF) or external inputs (EI) appears to be as good as a model based on UFP in this subset of projects. Conclusions: The results might suggest an improvement in the performance of the measurement process. Simplifying the FPA measurement process based on counting a subset of BFCs could allow savings in measurement effort, preserving the accuracy of effort estimates.",Empirical evaluation; Family of replications; Function point analysis; Software effort prediction,Article,Scopus,2-s2.0-84981165509
"Quarati A., Clematis A., D'Agostino D.","Delivering cloud services with QoS requirements: Business opportunities, architectural solutions and energy-saving aspects",2016,Future Generation Computer Systems,55,,,403,427,no,,,,10.1016/j.future.2015.02.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954386736&doi=10.1016%2fj.future.2015.02.009&partnerID=40&md5=6e7512cd59fdc8efecf238227c4d3536,"Institute of Applied Mathematics and Information Technologies, National Research Council of Italy, Genoa, Italy","Quarati, A., Institute of Applied Mathematics and Information Technologies, National Research Council of Italy, Genoa, Italy; Clematis, A., Institute of Applied Mathematics and Information Technologies, National Research Council of Italy, Genoa, Italy; D'Agostino, D., Institute of Applied Mathematics and Information Technologies, National Research Council of Italy, Genoa, Italy","The flexible and pay-as-you-go computing capabilities offered by Cloud infrastructures are nowadays very attractive, and widely adopted by many organizations and enterprises. In particular this is true for those having periodical or variable tasks to execute, and, choose to not or cannot afford the expenses of buying and managing computing facilities or software packages, that should remain underutilized for most of the time. For their ability to couple the scalability offered by public service providers, with the wider Quality of Service (QoS) provisions and ad-hoc customizations provided by private Clouds, Hybrid Clouds (HC) seem a particularly appealing solution for customers requiring something more than the mere availability of the service. The paper firstly introduces a Cloud brokering system leveraging on a promising architectural approach, based on the use of a gateway toolkit. This approach provides noticeably advantages, both to customers and to Cloud Brokers (CB), for its ability to hide all the intricacies related to the management of powerful, but often complex and heterogeneous infrastructures like the Cloud. Moreover such approach, through customized interfaces, facilitates customers in accessing Cloud resources thus easing the tailored deployment and execution of their applications and workflows. The major contribution of this work is given by the analysis of a set of brokering strategies for Hybrid Clouds, implemented by a brokering algorithm, aimed at the execution of various applications subject to different user requirements and computational conditions. With the objective of firstly maximize both user satisfaction and CB's revenues the algorithm also pursues profit increases through the reduction of energy costs by adopting energy saving mechanisms. A simulation model is used to evaluate performance, and the results show that differences among strategies depend on type and size of system loads and that the use of turn on and off techniques greatly improves energy savings at low and medium load rates thus indirectly increasing CB revenues without diminishing customers' satisfaction. å© 2015 Elsevier B.V. All rights reserved.",Cloud computing; Economics; IT architecture,Article,Scopus,2-s2.0-84954386736
"Qi F., Jing X.-Y., Zhu X., Wu F., Cheng L.",Privacy preserving via interval covering based subclass division and manifold learning based bi-directional obfuscation for effort estimation,2016,ASE 2016 - Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering,,,,75,86,no,,,,10.1145/2970276.2970302,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989196127&doi=10.1145%2f2970276.2970302&partnerID=40&md5=9fbde1ddb580947fb0ec249c8b9fcfdc,"State Key Laboratory of Software Engineering, School of Computer, Wuhan University, China; School of Automation, Nanjing University of Posts and Telecommunications, China; School of Computer and Information Engineering, Henan University, China","Qi, F., State Key Laboratory of Software Engineering, School of Computer, Wuhan University, China; Jing, X.-Y., State Key Laboratory of Software Engineering, School of Computer, Wuhan University, China, School of Automation, Nanjing University of Posts and Telecommunications, China; Zhu, X., State Key Laboratory of Software Engineering, School of Computer, Wuhan University, China, School of Computer and Information Engineering, Henan University, China; Wu, F., State Key Laboratory of Software Engineering, School of Computer, Wuhan University, China, School of Automation, Nanjing University of Posts and Telecommunications, China; Cheng, L., State Key Laboratory of Software Engineering, School of Computer, Wuhan University, China","When a company lacks local data in hand, engineers can build an effort model for the effort estimation of a new project by utilizing the training data shared by other companies. However, one of the most important obstacles for data sharing is the privacy concerns of software development organizations. In software engineering, most of existing privacy-preserving works mainly focus on the defect prediction, or debugging and testing, yet the privacypreserving data sharing problem has not been well studied in effort estimation. In this paper, we aim to provide data owners with an effective approach of privatizing their data before release. We firstly design an Interval Covering based Subclass Division (ICSD) strategy. ICSD can divide the target data into several subclasses by digging a new attribute (i.e., class label) from the effort data. And the obtained class label is beneficial to maintaining the distribution of the target data after obfuscation. Then, we propose a manifold learning based bi-directional data obfuscation (MLBDO) algorithm, which uses two nearest neighbors, which are selected respectively from the previous and next subclasses by utilizing the manifold learning based nearest neighbor selector, as the disturbances to obfuscate the target sample. We call the entire approach as ICSD & MLBDO. Experimental results on seven public effort datasets show that: 1) ICSD&MLBDO can guarantee the privacy and maintain the utility of obfuscated data. 2) ICSD&MLBDO can achieve better privacy and utility than the compared privacy-preserving methods. å© 2016 ACM.",Effort estimation; Locality preserving projection; Privacy-preserving; Subclass division,Conference Paper,Scopus,2-s2.0-84989196127
"Porto F., Simao A.",Feature subset selection for instance filtering methods on cross-project defect prediction,2016,CIBSE 2016 - XIX Ibero-American Conference on Software Engineering,,,,171,184,no,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988329636&partnerID=40&md5=9f2238a5dd3d605ba453c438de2cf6bc,"Instituto de CiÌ»ncias MatemÌÁticas e de ComputaÌ¤Ì£o, Universidade de SÌ£o Paulo, PO Box 668, SÌ£o Carlos, SP, Brazil","Porto, F., Instituto de CiÌ»ncias MatemÌÁticas e de ComputaÌ¤Ì£o, Universidade de SÌ£o Paulo, PO Box 668, SÌ£o Carlos, SP, Brazil; Simao, A., Instituto de CiÌ»ncias MatemÌÁticas e de ComputaÌ¤Ì£o, Universidade de SÌ£o Paulo, PO Box 668, SÌ£o Carlos, SP, Brazil","The defect prediction models can be a good tool on organizing the project's test resources. However, not all companies maintain an appropriate set of historical defect data. In this case, the companies can build an appropriate dataset from known external projects. This approach, called Cross-project Defect Prediction (CPDP), solves the lack of defect data, although introduces heterogeneity on data. This heterogeneity can compromises the performance of CPDP models. Recently, filtering methods were proposed in order to decrease the heterogeneity of data by selecting the most similar instances from the training dataset. This similarity between instances is calculated based on the available features of the dataset. On this study, we propose that using only the most relevant features on this calculation can result in more accurate filtered datasets and better prediction performances. We present an empirical evaluation of different methods used for selecting the most relevant features. We evaluate different configurations of four Feature Selection (FS) methods and two metric subsets. We used 36 versions of 11 open source projects on experiments. The results indicate that the defect prediction performance can be improved by using the evaluated approach. In addition, we investigated which methods present better performances. The results do not indicate a method with general better performances, i.e., the most appropriate method for a project can vary according to the project characteristics.",Code metrics; Cross-project defect prediction; Feature selection; Instance filtering; Network metrics; Software quality assurance,Conference Paper,Scopus,2-s2.0-84988329636
"Pitonakova L., Crowder R., Bullock S.",Information flow principles for plasticity in foraging robot swarms,2016,Swarm Intelligence,10,1,,33,63,no,,,,10.1007/s11721-016-0118-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959229988&doi=10.1007%2fs11721-016-0118-1&partnerID=40&md5=0ce1903b8c4e43e2ed0e66b28d59bda0,"Institute for Complex Systems Simulation and Department of Electronics and Computer Science, University of Southampton, Southampton, United Kingdom; School of Computer Science, University of Bristol, Bristol, United Kingdom","Pitonakova, L., Institute for Complex Systems Simulation and Department of Electronics and Computer Science, University of Southampton, Southampton, United Kingdom; Crowder, R., Institute for Complex Systems Simulation and Department of Electronics and Computer Science, University of Southampton, Southampton, United Kingdom; Bullock, S., School of Computer Science, University of Bristol, Bristol, United Kingdom","An important characteristic of a robot swarm that must operate in the real world is the ability to cope with changeable environments by exhibiting behavioural plasticity at the collective level. For example, a swarm of foraging robots should be able to repeatedly reorganise in order to exploit resource deposits that appear intermittently in different locations throughout their environment. In this paper, we report on simulation experiments with homogeneous foraging robot teams and show that analysing swarm behaviour in terms of information flow can help us to identify whether a particular behavioural strategy is likely to exhibit useful swarm plasticity in response to dynamic environments. While it is beneficial to maximise the rate at which robots share information when they make collective decisions in a static environment, plastic swarm behaviour in changeable environments requires regulated information transfer in order to achieve a balance between the exploitation of existing information and exploration leading to acquisition of new information. We give examples of how information flow analysis can help designers to decide on robot control strategies with relevance to a number of applications explored in the swarm robotics literature. å© 2016, The Author(s).",Communication; Foraging; Plasticity; Self-organisation; Swarm robotics,Article,Scopus,2-s2.0-84959229988
"Phongpaibul M., Aroonvatanaporn P.",Standardized cost estimation in Thai government's software development projects,2015,ICSEC 2015 - 19th International Computer Science and Engineering Conference: Hybrid Cloud Computing: A New Approach for Big Data Era,,,7401449,,,no,,,,10.1109/ICSEC.2015.7401449,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964317425&doi=10.1109%2fICSEC.2015.7401449&partnerID=40&md5=76d3b7f70aeed5863a53e05b884c30fb,"Department of Computer Science, Faculty of Science and Technology, Thammasat University (Rangsit Campus, Pathumthani, Thailand; MandP Consulting, Bangkok, Thailand","Phongpaibul, M., Department of Computer Science, Faculty of Science and Technology, Thammasat University (Rangsit Campus, Pathumthani, Thailand; Aroonvatanaporn, P., MandP Consulting, Bangkok, Thailand","Software features and costs are often unquantifiable due to the abstract nature of software. In many cases, this results in the estimated costs of software development projects to be potentially highly biased, highly inaccurate, or highly unjustified. Hence, current software estimation methodologies can open up areas for corruption as estimated budgets and costs are difficult to verify and validate. The Thai COCOMO Framework and cost estimation model were developed in order to overcome this problem in software development projects for Thai government by providing standard and transparency to the software estimation process with justification to the associated costs. This paper discusses the areas in software project estimation that are prone to corruption and ways that the COCOMO model and framework can be used to address them. å© 2015 IEEE.",COCOMO II; cost estimation; software engineering,Conference Paper,Scopus,2-s2.0-84964317425
"Phillipson F., Worm D., Neumann N., Sangers A., Wiarda S.-J.",Estimating bandwidth coverage using geometric models,2016,"2016 21st European Conference on Networks and Optical Communications, NOC 2016",,,7507003,152,156,no,,,,10.1109/NOC.2016.7507003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981244362&doi=10.1109%2fNOC.2016.7507003&partnerID=40&md5=50e47abee4bd3f467a2ad205d9824497,"TNO, POBox 96800, The Hague, Netherlands","Phillipson, F., TNO, POBox 96800, The Hague, Netherlands; Worm, D., TNO, POBox 96800, The Hague, Netherlands; Neumann, N., TNO, POBox 96800, The Hague, Netherlands; Sangers, A., TNO, POBox 96800, The Hague, Netherlands; Wiarda, S.-J., TNO, POBox 96800, The Hague, Netherlands","Geometric models are a proven aid for calculating the CapEx needed for a network deployment and used widely by telecommunication network operators for their access networks. However, for the economic viability an accurate estimation of the delivered bandwidth is important as well. In this paper a method is described to calculate a bandwidth profile for an area under investigation, given a single topology and using the geometric representation. Next to this single topology result, a method to estimate the bandwidth profile for a geometric model of a multi-layer network topology is presented. å© 2016 IEEE.",Bandwidth prediction; geometric models; network topology,Conference Paper,Scopus,2-s2.0-84981244362
"Peruzzini M., Pellicciari M.",Human-driven design-to-cost methodology for industrial cost optimization,2016,Advances in Transdisciplinary Engineering,4,,,715,724,no,,,,10.3233/978-1-61499-703-0-715,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994043941&doi=10.3233%2f978-1-61499-703-0-715&partnerID=40&md5=7f4ef6414a0e98fbd761d01e5cd9010d,"University of Modena and Reggio Emilia, Via Vivarelli 10, Modena, Italy","Peruzzini, M., University of Modena and Reggio Emilia, Via Vivarelli 10, Modena, Italy; Pellicciari, M., University of Modena and Reggio Emilia, Via Vivarelli 10, Modena, Italy","Over the years cost optimization has gained a strategic importance to realize competitive products. However, traditional approaches are no longer efficient in modern highly competitive industrial scenarios, where numerous factors have to be contemporarily considered and optimized. In order to be effective, design has to care about cost along all its phases. This paper presents a methodology that integrates Design-To-Cost (DTC), Design for Manufacturing and Assembly (DFMA), Human Factors (HF) and Feature-Based Costing (FBC) to include costs from the early conceptual design stages and properly drive the product design. Thanks to a structured knowledge base and a FBC approach, it predicts both manufacturing and assembly processes from the 3D geometrical models and estimate the global costs, more accurately than existing tools. The research demonstrates the method validity by an industrial case study focusing on cost optimization of packaging machines. Thanks to the proposed method, the main design inefficiencies are easily identified from the early design stages and optimization actions are taken in advanced, in respect to traditional design process. Such actions allowed reducing total industrial costs of 20%, improving machine assemblability and human ergonomics due to structure simplification, part number reduction, and production processes modification, and reducing the time spent for cost estimation (until -60%). å© 2016 The authors and IOS Press.",Cost modeling; Cost optimization; Design-to-Cost (DTC); Feature-Based Costing (FBC); Knowledge-Based engineering (KBE),Conference Paper,Scopus,2-s2.0-84994043941
"Pekmestzi K., Tsoumanis K., Efstathiou C.",Fused modulo 2n + 1 add-multiply unit for weighted operands,2016,"Proceedings - 2016 11th IEEE International Conference on Design and Technology of Integrated Systems in Nanoscale Era, DTIS 2016",,,7483875,,,no,,,,10.1109/DTIS.2016.7483875,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978397749&doi=10.1109%2fDTIS.2016.7483875&partnerID=40&md5=301449938335021768c343f395d1c0d8,"School of Electrical and Computer Engineering, National Technical University of Athens, Athens, Greece; Department of Electrical Engineering, Technological Institute of Athens, Athens, Greece","Pekmestzi, K., School of Electrical and Computer Engineering, National Technical University of Athens, Athens, Greece; Tsoumanis, K., School of Electrical and Computer Engineering, National Technical University of Athens, Athens, Greece; Efstathiou, C., Department of Electrical Engineering, Technological Institute of Athens, Athens, Greece","Digital Signal Processing (DSP) applications are dominated by complex arithmetic operations, which heavily degrade their performance. Targeting to accelerate the execution of Residue Number Systems (RNS)-based DSP applications, in this work, we focus on optimizing the design of the modulo 2n + 1 Add-Multiply (AM) operation with weighted operands. We incorporate in our design a new direct recoding of the modulo 2n + 1 sum of two weighted operands in its Modified Booth form. Compared to the conventional design of first instantiating an adder and then, driving its output to a multiplier, the proposed fused AM design yields considerable delay, area and power gains. å© 2016 IEEE.",,Conference Paper,Scopus,2-s2.0-84978397749
"PeÌ±a-GarcÌ_a A., GÌ_mez-Lorente D., EspÌ_n A., Rabaza O.",New rules of thumb maximizing energy efficiency in street lighting with discharge lamps: The general equations for lighting design,2016,Engineering Optimization,48,6,,1080,1089,no,,,1,10.1080/0305215X.2015.1085715,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945218378&doi=10.1080%2f0305215X.2015.1085715&partnerID=40&md5=82c02d52c9b895797494038aafeab8d5,"Lighting Technology for Safety and Sustainability Research Group, Department of Civil Engineering, University of Granada, Granada, Spain","PeÌ±a-GarcÌ_a, A., Lighting Technology for Safety and Sustainability Research Group, Department of Civil Engineering, University of Granada, Granada, Spain; GÌ_mez-Lorente, D., Lighting Technology for Safety and Sustainability Research Group, Department of Civil Engineering, University of Granada, Granada, Spain; EspÌ_n, A., Lighting Technology for Safety and Sustainability Research Group, Department of Civil Engineering, University of Granada, Granada, Spain; Rabaza, O., Lighting Technology for Safety and Sustainability Research Group, Department of Civil Engineering, University of Granada, Granada, Spain","New relationships between energy efficiency, illuminance uniformity, spacing and mounting height in public lighting installations were derived from the analysis of a large sample of outputs generated with a widely used software application for lighting design. These new relationships greatly facilitate the calculation of basic lighting installation parameters. The results obtained are also based on maximal energy efficiency and illuminance uniformity as a premise, which are not included in more conventional methods. However, these factors are crucial since they ensure the sustainability of the installations. This research formulated, applied and analysed these new equations. The results of this study highlight their usefulness in rapid planning and urban planning in developing countries or areas affected by natural disasters where engineering facilities and computer applications for this purpose are often unavailable. å© 2015 Taylor & Francis.",lighting design; optimization; street lighting,Article,Scopus,2-s2.0-84945218378
"Paul M.C., Sarkar S., Rahman M.M., Reza S.M., Kaiser M.S.",Low cost and portable patient monitoring system for e-Health services in Bangladesh,2016,"2016 International Conference on Computer Communication and Informatics, ICCCI 2016",,,7479974,,,no,,,,10.1109/ICCCI.2016.7479974,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978082118&doi=10.1109%2fICCCI.2016.7479974&partnerID=40&md5=6515524e0528677ad10040002303299a,"Institute of Information Technology, Jahangirnagar University, Savar, Dhaka, Bangladesh; Institute of Information Technology, Jahangirnagar University, Bangladesh","Paul, M.C., Institute of Information Technology, Jahangirnagar University, Savar, Dhaka, Bangladesh; Sarkar, S., Institute of Information Technology, Jahangirnagar University, Savar, Dhaka, Bangladesh; Rahman, M.M., Institute of Information Technology, Jahangirnagar University, Savar, Dhaka, Bangladesh; Reza, S.M., Institute of Information Technology, Jahangirnagar University, Savar, Dhaka, Bangladesh; Kaiser, M.S., Institute of Information Technology, Jahangirnagar University, Bangladesh","This paper proposes a low cost and portable patient monitoring system for e-Health services in Bangladesh. A raspberry pi microcomputer based system has been developed which can be used by paramedics for collecting different sensor data such as ECG signal, blood pressure signal, heart beat signal, Situation of Oxygen in Blood(SPO2), temperature and generating different signals from a patient and send these signals to specialist doctor who are in a centre or in a hospital. A web based application has been developed for both doctor and paramedics for efficient communicate with each other. It has been found that the system can be suitable for village health care centre of Bangladesh. å© 2016 IEEE.",blood pressure signal; e-health; ECG; raspberry pi; raspbian etc; Situation of Oxygen in Blood(SPO2),Conference Paper,Scopus,2-s2.0-84978082118
"Parthasarathy S., Daneva M.",An approach to estimation of degree of customization for ERP projects using prioritized requirements,2016,Journal of Systems and Software,117,,,471,487,no,,,,10.1016/j.jss.2016.04.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964203444&doi=10.1016%2fj.jss.2016.04.006&partnerID=40&md5=415df9ce3efc266a7990ee664a1a595a,"Department of Computer Applications, Thiagarajar College of Engineering, Madurai, Tamil Nadu, India; University of Twente, Enschede, Netherlands","Parthasarathy, S., Department of Computer Applications, Thiagarajar College of Engineering, Madurai, Tamil Nadu, India; Daneva, M., University of Twente, Enschede, Netherlands","Customization in ERP projects is a risky, but unavoidable undertaking that companies need to initiate in order to achieve alignment between their acquired ERP solution and their organizational goals and business processes. Conscious about the risks, many companies commit to leveraging the off-the-shelf built-in functionality in their chosen ERP package, keeping customization at a minimum level so that it does not jeopardize the project or the future projects that would build upon it. However, many organizations experience that once the project team enters the stage of implementing the solution, requests for customization increase in volume and diversity. Managing properly the process of customization gets increasingly harder. This paper addresses the problem of estimating the degree of customization at an early stage of ERP implementation. This will support customization decision makers in making value and cost trade-offs when approving requests for customization. We propose a solution approach in which customization requirements are reasoned in quantitative terms. Our approach uses client-prioritized requirements for the estimation of degree of customization during the ERP implementation. A case study is used to illustrate the application of the proposed approach. We also discuss the strengths and limitations of our approach as well as its implications for research and practice. å© 2016 Elsevier Inc. All rights reserved.",Case study; Customization; Decision making; Enterprise resource planning (ERP) projects; Requirements prioritization,Article,Scopus,2-s2.0-84964203444
"Park K., Kim D.Y., Yang D.R.",Cost-based analysis about a newly designed two-staged reverse osmosis process with draw solute,2016,Computer Aided Chemical Engineering,38,,,223,228,no,,,,10.1016/B978-0-444-63428-3.50042-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994386098&doi=10.1016%2fB978-0-444-63428-3.50042-4&partnerID=40&md5=566b02fdd3fb09b524386c04c702254a,"Korea University, Department of Chemical and Biological Engineering, Anam-dong 5-Ga, Seoungbuk-gu, Seoul, South Korea; Imperial College of London, Department of Chemical Engineering, South Kensington London, London, United Kingdom","Park, K., Korea University, Department of Chemical and Biological Engineering, Anam-dong 5-Ga, Seoungbuk-gu, Seoul, South Korea; Kim, D.Y., Imperial College of London, Department of Chemical Engineering, South Kensington London, London, United Kingdom; Yang, D.R., Korea University, Department of Chemical and Biological Engineering, Anam-dong 5-Ga, Seoungbuk-gu, Seoul, South Korea","Draw solution assisted reverse osmosis (DSARO) process is suggested and mathematically modelled in this study. The main concept of the process is osmotic pressure reduction in the RO process by utilizing draw solution which has lower concentration than seawater. Since the pressure requirement of the DSARO process is in the range of BWRO process, the plant capital cost data in BWRO process can be used for cost estimation model. The cost estimation results shows that the specific water cost in DSARO process is more than 10% lower than that of conventional RO process. So the DSARO process can be economically feasible. å© 2016 Elsevier B.V.",cost estimation; desalination; forward osmosis; modelling; process design; reverse osmosis,Book Chapter,Scopus,2-s2.0-84994386098
"Park B.K., Moon S.Y., Kim R.Y.C.",Improving Use Case Point (UCP) Based on Function Point (FP) Mechanism,2016,"2016 International Conference on Platform Technology and Service, PlatCon 2016 - Proceedings",,,7456803,,,no,,,,10.1109/PlatCon.2016.7456803,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968619213&doi=10.1109%2fPlatCon.2016.7456803&partnerID=40&md5=0120e1ba6794604a698827d709c42c5b,"SELab., Dept. of Computer and Information Communications, Hongik University, 2639 Sejong Campus, Hongik, South Korea; Dept. of Computer and Information Communications, Hongik University, 2639 Sejong Campus, Hongik, South Korea","Park, B.K., SELab., Dept. of Computer and Information Communications, Hongik University, 2639 Sejong Campus, Hongik, South Korea; Moon, S.Y., SELab., Dept. of Computer and Information Communications, Hongik University, 2639 Sejong Campus, Hongik, South Korea; Kim, R.Y.C., Dept. of Computer and Information Communications, Hongik University, 2639 Sejong Campus, Hongik, South Korea","The cost of error correction has been increasing exponentially with the advancement of software industry. To minimize software errors, it is necessary to extract accurate requirements in the early stage of software development. In the previous study, we extracted the priorities of requirements based on the Use Case Point (UCP), which however revealed the issues inherent to the existing UCP as follows. (i) The UCP failed to specify the structure of use cases or the method of write the use cases, and (ii) the number of transactions determined the use case weight in the UCP. Yet, efforts taken for implementation depend on the types and number of operations performed in each transaction. To address these issues, the present paper proposes an improved UCP and applies it to the prioritization. The proposed method enables more accurate measurement than the existing UCP-based prioritization. å© 2016 IEEE.",Function Point; Improved UCP; Use Case Point; Use Case Priority,Conference Paper,Scopus,2-s2.0-84968619213
"Park B.K., Moon S.Y., Kim K.D., Jang W.S., Kim R.Y.C., Carlson C.R.",Refining an Assessing Model for Simplified TMM,2016,"2016 International Conference on Platform Technology and Service, PlatCon 2016 - Proceedings",,,7456820,,,no,,,,10.1109/PlatCon.2016.7456820,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968626471&doi=10.1109%2fPlatCon.2016.7456820&partnerID=40&md5=1269111a275648790bd0a763a5f6a642,"SELab., Dept. of Computer and Information Communications, Hongik University, 2639 Sejong Campus, Hongik, South Korea; Telecommunications Technology Association, Gyeonggi, South Korea; Dept. of ITM, Illinois Institute of Technology, Chicago, United States","Park, B.K., SELab., Dept. of Computer and Information Communications, Hongik University, 2639 Sejong Campus, Hongik, South Korea; Moon, S.Y., SELab., Dept. of Computer and Information Communications, Hongik University, 2639 Sejong Campus, Hongik, South Korea; Kim, K.D., Telecommunications Technology Association, Gyeonggi, South Korea; Jang, W.S., SELab., Dept. of Computer and Information Communications, Hongik University, 2639 Sejong Campus, Hongik, South Korea; Kim, R.Y.C., SELab., Dept. of Computer and Information Communications, Hongik University, 2639 Sejong Campus, Hongik, South Korea; Carlson, C.R., Dept. of ITM, Illinois Institute of Technology, Chicago, United States","Certification models, e.g. CMMI and TMMi, are hard to use most small software development organizations in Korea. Moreover, some companies do not have their own teams in charge of testing. To solve this issue, we propose the assessment method of a simplified test maturity model for small SW development organizations in local environment and. For the proposed assessment method, we consider the environment surrounding the local software industry and development teams, and then determine how to establish the assessment method. We show the proposed assessment method to apply local SMEs for their conditions, and reduces cost and labor as well as shortens the time spent on assessment in contrast to the previous models. å© 2016 IEEE.",Assessment Model; CMMI; Simplified TMM; Test Maturity Model; TMMi,Conference Paper,Scopus,2-s2.0-84968626471
"Pangsrivinij S., McGuffin-Cawley J., Quinn R., Narayanan B., Zhang S., Denney P.",Calculation of energy balance and efficiency in Laser Hot-Wire (LHW) cladding process,2016,"International Symposium on Flexible Automation, ISFA 2016",,,7790164,217,222,no,,,,10.1109/ISFA.2016.7790164,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010628681&doi=10.1109%2fISFA.2016.7790164&partnerID=40&md5=63d057e8e53bfa91c163e416d3030de0,"Department of Materials Science and Engineering, Case Western Reserve University, White Building, 10900 Euclid Avenue, Cleveland, OH, United States; Department of Mechanical and Aerospace Engineering, Case Western Reserve University, White Building, 10900 Euclid Avenue, Cleveland, OH, United States; Lincoln Electric Company, 22801 St. Clair Ave, Cleveland, OH, United States","Pangsrivinij, S., Department of Materials Science and Engineering, Case Western Reserve University, White Building, 10900 Euclid Avenue, Cleveland, OH, United States; McGuffin-Cawley, J., Department of Materials Science and Engineering, Case Western Reserve University, White Building, 10900 Euclid Avenue, Cleveland, OH, United States; Quinn, R., Department of Mechanical and Aerospace Engineering, Case Western Reserve University, White Building, 10900 Euclid Avenue, Cleveland, OH, United States; Narayanan, B., Lincoln Electric Company, 22801 St. Clair Ave, Cleveland, OH, United States; Zhang, S., Lincoln Electric Company, 22801 St. Clair Ave, Cleveland, OH, United States; Denney, P., Lincoln Electric Company, 22801 St. Clair Ave, Cleveland, OH, United States","Laser Hot-Wire (LHW) processing is a subset of freeform welding, which in turn is an extension of the well-established wire-based robotic cladding. The process is adaptable as a viable additive manufacturing (AM) technology. In LHW two separate heat sources are combined - a laser impinges on the surface of the workpiece and the feed wire is resistively heated through an external DC circuit. The two materials used in the experiments reported here are Ti-6Al-4V and Nickel 625. Advantages of the process are high deposition rate, high energy efficiency, and the ability to produced controlled unique microstructures. Preliminary calculations of energy efficiency and energy balance of the process are reported. å© 2016 IEEE.",,Conference Paper,Scopus,2-s2.0-85010628681
"Pan Y., Wang Z.",Data Collection and Modeling of Personal Social Behaviors,2016,"Proceedings of International Conference on Service Science, ICSS",2016-February,,7400791,164,168,no,,,1,10.1109/ICSS.2015.32,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962247984&doi=10.1109%2fICSS.2015.32&partnerID=40&md5=2e32a997c8576274792995c1b0689ecf,"Research Center of Intelligent Computing for Enterprises and Services (ICES), School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China","Pan, Y., Research Center of Intelligent Computing for Enterprises and Services (ICES), School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; Wang, Z., Research Center of Intelligent Computing for Enterprises and Services (ICES), School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China","Social media has developed rapidly, which had already affected modern people's life style. Meanwhile, social media has produced huge amounts of data. In research, we find that there is no explicit definition of user's behavior pattern in social network. Therefore, we focused on the research of that and found people who have two types of behaviors which are inseparably intertwined. The two types that mentioned are functional behaviors and social behaviors. We have defined user's behavior, and regarded user's behavior as a vertical timeline which the object of study is individual. With adding some horizontal lines which stand for social behaviors between vertical timelines, the model linked different people. Experiment's research takes Sina Weibo as an object. For obtaining the social behavior pattern, we use the generalized sequential pattern algorithm (GSP) for data mining. And the pattern is represented by the form of [User Behavior User 2192; User Behavior User] n. According to the result and algorithm accuracy confirmation, the frequent behavioral patterns of a certain person can be obtained. After that we can also predict user's behaviors in the future. å© 2015 IEEE.",dada mining; data gathering; GSP algorithm; social behavior pattern,Conference Paper,Scopus,2-s2.0-84962247984
PÌÁdua W.,Evolution of a Model-driven Process Framework,2016,Electronic Notes in Theoretical Computer Science,321,,,41,65,no,,,,10.1016/j.entcs.2016.02.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964671862&doi=10.1016%2fj.entcs.2016.02.004&partnerID=40&md5=8fa1314a6e6393990d00cc2e06a7d949,"Computer Science Department, Federal University of Minas Gerais, Belo Horizonte, MG, Brazil","PÌÁdua, W., Computer Science Department, Federal University of Minas Gerais, Belo Horizonte, MG, Brazil","We discuss the evolution of Praxis, a model-driven process framework, building on feedback from educational and professional applications, along the past fifteen years. We follow the evolution from Praxis first version to the current one, discussing what was introduced in each. For past and current versions, we classify model improvements, discussing their nature and rationale, derived from received feedback. å© 2016 The Author.",CRUD transactions; framework; model transformations; model-driven development; persistent data; process; reuse,Conference Paper,Scopus,2-s2.0-84964671862
"Ova K., Watanabe T., Koga H.",Parametric spanning tree generation method for network topology design,2016,International Conference on Information Networking,2016-March,,7427110,178,183,no,,,,10.1109/ICOIN.2016.7427110,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963938686&doi=10.1109%2fICOIN.2016.7427110&partnerID=40&md5=7c8fcd0668ac0d1c401d577435140a60,"Graduate School of Information Systems, Univ. of Electro. - Communications, 1-5-1 Chofu-ga-oka, Chofu-shi, Tokyo, Japan","Ova, K., Graduate School of Information Systems, Univ. of Electro. - Communications, 1-5-1 Chofu-ga-oka, Chofu-shi, Tokyo, Japan; Watanabe, T., Graduate School of Information Systems, Univ. of Electro. - Communications, 1-5-1 Chofu-ga-oka, Chofu-shi, Tokyo, Japan; Koga, H., Graduate School of Information Systems, Univ. of Electro. - Communications, 1-5-1 Chofu-ga-oka, Chofu-shi, Tokyo, Japan","A typical network topological design problem is to determine link connections and their capacity to achieve high performance, low initial and operational costs, and high reliability under the given traffic and link length data between nodes. Because of the difficulties of this problem, approximate solutions such as probabilistic searches have long been studied. However, the real-world network topologies seem to be more type oriented than the above traditional computer based solutions. In fact, most real network topologies consist of a hierarchical combination of basic types such as the bus, the star, and the ring to avoid the difficulties of the design problem. In this paper, a new parametric method for localized spanning tree (ST) generation is proposed with good experimental results. The method performs node clustering and physical link generation in one step. This is realized by a new idea of the parameterized virtual node distance incorporating both the physical node distance and the traffic gravity between nodes with a parametric weight. A set of localized spanning trees can be generated on traditional Kruscal algorithm, by changing the weight. As the main computational costs are the MST generation and the depth-first shortest-route search, so this is a high-speed approximate solver of the network topology design problem. To assist selecting a good solution, a link capacity determination function to achieve the given mean delay time and the monthly cost estimation function are incorporated. Approximate mathematical discussions to prove the existence of a minimum cost solution in the generated candidates is given. å© 2016 IEEE.",clustering; computer network; localized star spanning tree; optimization; parametric method; topology design; type-respecting,Conference Paper,Scopus,2-s2.0-84963938686
"Orji I., Wei S.",A detailed calculation model for costing of green manufacturing,2016,Industrial Management and Data Systems,116,1,,65,86,no,,,,10.1108/IMDS-04-2015-0140,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954499054&doi=10.1108%2fIMDS-04-2015-0140&partnerID=40&md5=af3f231939a5cfcce1e1387d371d1ef4,"School of Mechanical Engineering, Dalian University of Technology, Dalian, China","Orji, I., School of Mechanical Engineering, Dalian University of Technology, Dalian, China; Wei, S., School of Mechanical Engineering, Dalian University of Technology, Dalian, China","Purpose - Manufacturing firms are expected to implement green manufacturing and increase product complexity at a competitive price. However, a major problem for engineering managers is to ascertain the costs of embarking on green manufacturing. Thus, a planning and control methodology for costing of green manufacturing at the early design stage is important for engineering managers. The paper aims to discuss these issues. Design/methodology/approach - This paper integrates ""green manufacturing,"" concepts of industrial dynamics, and product lifecycle aiming at developing a methodology for cost calculation. The methodology comprises of a process-based cost model and a systems dynamics (SD) model. The process-based cost model focusses mainly on carbon emission costs and energy-saving activities. Important metrics usually ignored in traditional static modeling were incorporated using SD model. Findings - Equipment costs and carbon emission costs are major components of costs in manufacturing. The total life cycle cost of product in green manufacturing is lower than that of same product in conventional manufacturing. Research limitations/implications - The specific results of this study are limited to the case company, but can hopefully contribute to further research on ascertaining cost of implementing ""green issues"" in manufacturing. The proposed cost calculation model can be efficiently applied in any manufacturing firm on the basis of accessibility of real cost data. This necessitates a comprehensive cost database. At the development of the model and database management system, time and cost resources could be demanding, but once installed, use of the model becomes less demanding. Practical implications - The cost model provides cost justifications of implementing green manufacturing. The reality is that green manufacturing will see its development peak with cost justifications. The results of the application show that the proposed detailed cost model can be effective in ascertaining costs of implementing green manufacturing. Manufacturing firms are recommended to adopt energy-saving activities based on the proposed detailed cost calculation model. Originality/value - The main contributions of the study includes: first, to help engineering managers more accurately understand how to allocate resources for energy-saving activities through appropriate cost drivers. Second, to simulate with SD the dynamic behavior of few important metrics, often ignored in traditional mathematical modeling. The detailed model provides a pre-manufacturing decision-making tool which will assist management in implementing green manufacturing by incorporating a life cycle assessment measurement into manufacturing cost management. å© Emerald Group Publishing Limited.",Carbon emission costs; Energy-saving activities; Green manufacturing; Process-based costing; Product life cycle; Systems dynamics,Article,Scopus,2-s2.0-84954499054
"Orcioni S., Giammarini M., Scavongelli C., Vece G.B., Conti M.",Energy estimation in SystemC with Powersim,2016,"Integration, the VLSI Journal",55,,,118,128,no,,,1,10.1016/j.vlsi.2016.04.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84965169015&doi=10.1016%2fj.vlsi.2016.04.006&partnerID=40&md5=f9b531fc1a14b626c41e73475aa1953a,"Dipartimento di Ingegneria Dell'Informazione, UniversitÌÊ Politecnica Delle Marche, Ancona, Italy","Orcioni, S., Dipartimento di Ingegneria Dell'Informazione, UniversitÌÊ Politecnica Delle Marche, Ancona, Italy; Giammarini, M., Dipartimento di Ingegneria Dell'Informazione, UniversitÌÊ Politecnica Delle Marche, Ancona, Italy; Scavongelli, C., Dipartimento di Ingegneria Dell'Informazione, UniversitÌÊ Politecnica Delle Marche, Ancona, Italy; Vece, G.B., Dipartimento di Ingegneria Dell'Informazione, UniversitÌÊ Politecnica Delle Marche, Ancona, Italy; Conti, M., Dipartimento di Ingegneria Dell'Informazione, UniversitÌÊ Politecnica Delle Marche, Ancona, Italy","This paper presents a methodology to estimate the dissipation of energy in hardware, at any level of abstraction, with Powersim. Powersim is a C++ class library aimed to the calculation of energy dissipation of hardware described in SystemC. To this end C++ operators are monitored and a different energy model is used for each data type. Energy models are functions of operator inputs, constant parameters and variables, like supply voltage. The main advantage of this approach is that energy estimation does not require any change in the source code describing the hardware. As application examples, the computational complexity of a JPEG encoder, implemented in a FPGA, and a FIR filter, implemented in a microcontroller, are presented. å© 2016 Elsevier. All rights reserved.",Energy estimation; Low-power; Power estimation; System level; SystemC,Article,Scopus,2-s2.0-84965169015
Nugent P.D.,The role of uncertainty in systems engineering practice: An empirical analysis of engineering peer reviews,2016,"10th Annual International Systems Conference, SysCon 2016 - Proceedings",,,7490567,,,no,,,,10.1109/SYSCON.2016.7490567,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979298735&doi=10.1109%2fSYSCON.2016.7490567&partnerID=40&md5=26abeb2a3dcf279f8d212ad5cfbb4716,"Western Connecticut State University, Management Information Systems Department, Danbury, CT, United States","Nugent, P.D., Western Connecticut State University, Management Information Systems Department, Danbury, CT, United States",Uncertainty is a theoretical concept that undergirds much of organizational theory and engineering theory. This paper analyzes systems engineering peer review data obtained from a large defense contracting company to better understand the gaps between theory and practice with respect to uncertainty. Preliminary results reveal that in practice uncertainty plays critical roles with respect to language and system ontology that heretofore have been neglected or underrepresented in theory. å© 2016 IEEE.,organization theory; peer reviews; Systems engineering; uncertainty,Conference Paper,Scopus,2-s2.0-84979298735
"Nishanth K.J., Ravi V.",Probabilistic neural network based categorical data imputation,2016,Neurocomputing,218,,,17,25,no,,,,10.1016/j.neucom.2016.08.044,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994176946&doi=10.1016%2fj.neucom.2016.08.044&partnerID=40&md5=7cb7b718fb30a2262a59971114c58056,"Center of Excellence in CRM and Analytics, Institute for Development and Research in Banking Technology (IDRBT), Castle Hills Road #1, Masab Tank, Hyderabad, India","Nishanth, K.J., Center of Excellence in CRM and Analytics, Institute for Development and Research in Banking Technology (IDRBT), Castle Hills Road #1, Masab Tank, Hyderabad, India; Ravi, V., Center of Excellence in CRM and Analytics, Institute for Development and Research in Banking Technology (IDRBT), Castle Hills Road #1, Masab Tank, Hyderabad, India","Real world datasets contain both numerical and categorical attributes. Very often missing values are present in both numerical and categorical attributes. The missing data has to be imputed as the inferences made from complete data are often more accurate and reliable than those made from incomplete data [15]. Also, most of the data mining algorithms cannot work with incomplete datasets. The paper proposes a novel soft computing architecture for categorical data imputation. The proposed imputation technique employs Probabilistic Neural Network (PNN) preceded by mode for imputing the missing categorical data. The effectiveness of the proposed imputation technique is tested on 4 benchmark datasets under the 10 fold-cross validation framework. In all datasets, except Mushroom, which are complete, some values, which are randomly removed, are treated as missing values. The performance of the proposed imputation technique is compared with that of 3 statistical and 3 machine learning methods for data imputation. The comparison of the mode+PNN imputation technique with mode, K-Nearest Neighbor (K-NN), Hot Deck (HD), Naive Bayes, Random Forest (RF) and J48 (Decision Tree) imputation techniques demonstrates that the proposed method is efficient, especially when the percentage of missing values is high, for records having more than one missing value and for records having a large number of categories for each categorical variable. å© 2016 Elsevier B.V.",Categorical data imputation; Decision Tree (DT); Probabilistic Neural Network (PNN); Random Forest (RF),Article,Scopus,2-s2.0-84994176946
"Nicolini M., Miller J., Wienke S., Schlottke-Lakemper M., Meinke M., MÌ_ller M.S.",Software cost analysis of GPU-accelerated aeroacoustics simulations in C++ with openACC,2016,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),9945 LNCS,,,524,543,no,,,,10.1007/978-3-319-46079-6_36,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992549157&doi=10.1007%2f978-3-319-46079-6_36&partnerID=40&md5=8bb90abd2019d1dc36e64b8caa7712f5,"IT Center, RWTH Aachen University, Aachen, Germany; JARA, High-Performance Computing, Aachen, Germany; Institute of Aerodynamics, RWTH Aachen University, Aachen, Germany","Nicolini, M., IT Center, RWTH Aachen University, Aachen, Germany; Miller, J., IT Center, RWTH Aachen University, Aachen, Germany; Wienke, S., IT Center, RWTH Aachen University, Aachen, Germany, JARA, High-Performance Computing, Aachen, Germany; Schlottke-Lakemper, M., JARA, High-Performance Computing, Aachen, Germany, Institute of Aerodynamics, RWTH Aachen University, Aachen, Germany; Meinke, M., Institute of Aerodynamics, RWTH Aachen University, Aachen, Germany; MÌ_ller, M.S., IT Center, RWTH Aachen University, Aachen, Germany, JARA, High-Performance Computing, Aachen, Germany","Aeroacoustics simulations leverage the tremendous computational power of today‰Ûªs supercomputers, e.g., to predict the noise emissions of airplanes. The emergence of GPUs that are usable through directive-based programming models like OpenACC promises a costefficient solution for flow-induced noise simulations with respect to hardware expenditure and development time. However, OpenACC‰Ûªs capabilities for real-world C++ codes have been scarcely investigated so far and software costs are rarely evaluated and modeled for this kind of highperformance projects. In this paper, we present our OpenACC parallelization of ZFS, an aeroacoustics simulation framework written in C++, and its early performance results. From our implementation work, we derive common pitfalls and lessons-learned for real-world C++ codes using OpenACC. Furthermore, we borrow software cost estimation techniques from software engineering to evaluate the development efforts needed in a directive-based HPC environment. We discuss applicability and challenges of the popular COCOMO II model applied to the parallelization of ZFS. å© Springer International Publishing AG 2016.",,Conference Paper,Scopus,2-s2.0-84992549157
"Nguyen V., Dang H.H., Do N.-K., Tran D.-T.",Enhancing team collaboration through integrating social interactions in a Web-based development environment,2016,Computer Applications in Engineering Education,24,4,,529,545,no,,,,10.1002/cae.21729,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978327317&doi=10.1002%2fcae.21729&partnerID=40&md5=53882aa9f10e59865a3ae086e889534a,"University of Science, VNU-HCMC, Ho Chi Minh, Viet Nam","Nguyen, V., University of Science, VNU-HCMC, Ho Chi Minh, Viet Nam; Dang, H.H., University of Science, VNU-HCMC, Ho Chi Minh, Viet Nam; Do, N.-K., University of Science, VNU-HCMC, Ho Chi Minh, Viet Nam; Tran, D.-T., University of Science, VNU-HCMC, Ho Chi Minh, Viet Nam","This paper presents the design and evaluation of a Web-based collaborative learning environment called EduCo for learning and practicing team-based exercises in computer science and software engineering courses. EduCo's defining characteristic is integrating a number of services for software development activities, for example, project management, requirements engineering, design, and programming into integrated shared workspaces with social-networking facilities to enhance collaboration among students and instructors. We conducted a controlled experiment to evaluate the effectiveness of the system in doing a three-iteration programming project. A total of 126 second-year students who attended an object-oriented programming course participated in the experiment. The results show that while the system was not found to affect student programming performance, it improved student engagement and satisfaction when working in teams. These results imply that the system's shared workspaces and social-networking services have the potential to enhance learning outcomes of students working in teams when these features are integrated into collaborative learning environments. Ìâå© 2016 Wiley Periodicals, Inc. Comput Appl Eng Educ 24:529‰ÛÒ545, 2016; View this article online at wileyonlinelibrary.com/journal/cae; DOI 10.1002/cae.21729. Ìâå© 2016 Wiley Periodicals, Inc.",collaborative environment; programming language; social environment; social network,Article,Scopus,2-s2.0-84978327317
"Nepal M.P., Staub-French S.",Supporting knowledge-intensive construction management tasks in BIM,2016,Journal of Information Technology in Construction,21,,,13,38,no,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979503379&partnerID=40&md5=2c70d8d5e40d7eae4071941fd9583941,"School of Civil Engineering and Built Environment, Queensland University of Technology, Brisbane, Australia; Department of Civil Engineering, University of British Columbia, Vancouver, Canada","Nepal, M.P., School of Civil Engineering and Built Environment, Queensland University of Technology, Brisbane, Australia; Staub-French, S., Department of Civil Engineering, University of British Columbia, Vancouver, Canada","The delivery of products and services for construction-based businesses is increasingly becoming knowledge-driven and information-intensive. The proliferation of building information modelling (BIM) has increased business opportunities as well as introduced new challenges for the architectural, engineering and construction and facilities management (AEC/FM) industry. As such, the effective use, sharing and exchange of building life cycle information and knowledge management in building design, construction, maintenance and operation assumes a position of paramount importance. This paper identifies a subset of construction management (CM) relevant knowledge for different design conditions of building components through a critical, comprehensive review of synthesized literature and other information gathering and knowledge acquisition techniques. It then explores how such domain knowledge can be formalized as ontologies and, subsequently, a query vocabulary in order to equip BIM users with the capacity to query digital models of a building for the retrieval of useful and relevant domain-specific information. The formalized construction knowledge is validated through interviews with domain experts in relation to four case study projects. Additionally, retrospective analyses of several design conditions are used to demonstrate the soundness (realism), completeness, and appeal of the knowledge base and query-based reasoning approach in relation to the state-of-the-art tools, Solibri Model Checker and Navisworks. The knowledge engineering process and the methods applied in this research for information representation and retrieval could provide useful mechanisms to leverage BIM in support of a number of knowledge intensive CM/FM tasks and functions.",BIM; Construction management; Design features; Knowledge management; Knowledge specification; Ontology,Article,Scopus,2-s2.0-84979503379
"Nayebi M., Farrahi H., Lee A., Cho H., Ruhe G.",More insight from being more focused : Analysis of clustered market apps,2016,"WAMA 2016 - Proceedings of the International Workshop on App Market Analytics, co-located with FSE 2016",,,,30,36,no,,,,10.1145/2993259.2993266,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007387538&doi=10.1145%2f2993259.2993266&partnerID=40&md5=9a607cc1d8e34da33ddd0ddb8bf35b77,"SEDS laboratory, University of Calgary, Calgary, Canada; Dept. of Engineering Science, University of Toronto, Toronto, Canada","Nayebi, M., SEDS laboratory, University of Calgary, Calgary, Canada; Farrahi, H., SEDS laboratory, University of Calgary, Calgary, Canada; Lee, A., SEDS laboratory, University of Calgary, Calgary, Canada; Cho, H., Dept. of Engineering Science, University of Toronto, Toronto, Canada; Ruhe, G., SEDS laboratory, University of Calgary, Calgary, Canada","The increasing attraction of mobile apps has inspired re- searchers to analyze apps from different perspectives. As any software product, apps have different attributes such as size, content maturity, rating, category or number of down- loads. Current research studies mostly consider sampling across all apps. This often results in comparisons of apps being quite different in nature and category (games com- pared with weather and calendar apps), also being different in size and complexity. Similar to proprietary software and web-based services, more specific results can be expected from looking at more homogeneous samples as they can be received as a result of applying clustering. In this paper, we target homogeneous samples of apps to increase to degree of insight gained from analytics. As a proof-of-concept, we applied clustering technique DBSCAN and subsequent correlation analysis between app attributes for a set of 940 open source mobile apps from F-Droid. We showed that (i) clusters of apps with similar characteristics provided more insight compared to applying the same to the whole data and (ii) defining similarity of apps based on similarity of topics as created from topic modeling technique Latent Dirichlet Allocation does not significantly improve clustering results. å© 2016 ACM.",App store analysis; Clustering; Data analytics; Mobile apps,Conference Paper,Scopus,2-s2.0-85007387538
"Nascimento S.M., De Macedo J.A.F., Chucre M.R.R.B., Casanova M.A., Machado J.",On computing temporal functions for time-dependent networks using trajectory data streams,2016,"Proceedings of the 7th ACM SIGSPATIAL International Workshop on GeoStreaming, IWGS 2016",,, a4,,,no,,,,10.1145/3003421.3003425,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85002050833&doi=10.1145%2f3003421.3003425&partnerID=40&md5=5a6d800841b3b309bdf27bfa8ff8d4c1,"Federal University of CearÌÁ, CearÌÁ, Brazil; Pontifical Catholic University of Rio de Janeiro, Rio de Janeiro, Brazil","Nascimento, S.M., Federal University of CearÌÁ, CearÌÁ, Brazil; De Macedo, J.A.F., Federal University of CearÌÁ, CearÌÁ, Brazil; Chucre, M.R.R.B., Federal University of CearÌÁ, CearÌÁ, Brazil; Casanova, M.A., Pontifical Catholic University of Rio de Janeiro, Rio de Janeiro, Brazil; Machado, J., Federal University of CearÌÁ, CearÌÁ, Brazil","Time dependent networks in mobility scenario are key for many applications that need to cope with real world dynamics. However, the quality of a time dependent network relies on the accuracy of its temporal functions. To this aim, we propose a new method for computing temporal functions for a time dependent network using Trajectory Data Streams. This proposal extends the previous Piecewise linear model, which uses a smooth curve approach, called LOESS, that can estimate where the breakpoints values occurs in a Piecewise linear function. A challenge faced by the use of trajectory data streams is related with the time constraint to update time dependent network time functions. Our model computes the time dependent network and update the temporal function that needs to reflect recent data and discard old data. We described our solution and present experimental results, which show that our approach is efficient and effective comparing to their competitors. å© 2016 ACM.",Data Stream; Piecewise; Temporal function; Time dependent network,Conference Paper,Scopus,2-s2.0-85002050833
"Nandal D., Sangwan O.P.",A Survey Report on Various Software Estimation Techniques and Practices,2016,International Journal of Control Theory and Applications,9,22,,75,83,no,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006931247&partnerID=40&md5=3773f0a7333409750fba13d2cdbab015,"GJUS and T, Hisar, India","Nandal, D., GJUS and T, Hisar, India; Sangwan, O.P., GJUS and T, Hisar, India","Paper offers thoroughly examine of software and project analysis methods established in industry and literature, its skills and flaws The Software Estimation is very important task for completing the project successfully. A successful software project development not only relies on the product efficiency but also the accurate estimation The estimation in software development depends on various factors especially on cost and effort factors for which further AI(Artificial Intelligence) and Algorithmic models have been put into usage. The low accuracy and non- reliable structures of the algorithmic models led to high risks of software projects. So, it is needed to estimate the cost of the project annually and compare it to the other techniques. The Metaheuristic algorithms have been developed well lately in software fields. Metaheuristics like Genetic Algorithms (GA) and Ant Colony Optimization (ACO) solve the problems according to the optimization of the problems and are very efficient in optimizing the algorithmic models and the effective factors in cost estimation. This review aims to discuss the methods for calculating and optimizing the metrices by using metaheuristic algorithms for software development. For this purpose survey of already implemented meta- heuristic algorithms like MOPSO, Bee Colony Optimization and Firefly is done. å© International Science Press.",Effort estimation; Metaheuristic optimization; Software quality,Article,Scopus,2-s2.0-85006931247
"Nam J., Kim S.",CLAMI: Defect prediction on unlabeled datasets,2015,"Proceedings - 2015 30th IEEE/ACM International Conference on Automated Software Engineering, ASE 2015",,,7372033,452,463,no,Defects,,2,10.1109/ASE.2015.56,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963818075&doi=10.1109%2fASE.2015.56&partnerID=40&md5=8e44aad9d9cf4a0535891f3746c09f44,"Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, Hong Kong","Nam, J., Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, Hong Kong; Kim, S., Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, Hong Kong","Defect prediction on new projects or projects with limited historical data is an interesting problem in software engineering. This is largely because it is difficult to collect defect information to label a dataset for training a prediction model. Cross-project defect prediction (CPDP) has tried to address this problem by reusing prediction models built by other projects that have enough historical data. However, CPDP does not always build a strong prediction model because of the different distributions among datasets. Approaches for defect prediction on unlabeled datasets have also tried to address the problem by adopting unsupervised learning but it has one major limitation, the necessity for manual effort. In this study, we propose novel approaches, CLA and CLAMI, that show the potential for defect prediction on unlabeled datasets in an automated manner without need for manual effort. The key idea of the CLA and CLAMI approaches is to label an unlabeled dataset by using the magnitude of metric values. In our empirical study on seven open-source projects, the CLAMI approach led to the promising prediction performances, 0.636 and 0.723 in average f-measure and AUC, that are comparable to those of defect prediction based on supervised learning. å© 2015 IEEE.",,Conference Paper,Scopus,2-s2.0-84963818075
"Nagarajan H.P.N., Malshe H.A., Haapala K.R., Pan Y.",Environmental performance evaluation of a fast mask image projection stereolithography process through time and energy modeling,2016,"Journal of Manufacturing Science and Engineering, Transactions of the ASME",138,10,101004,,,no,,,,10.1115/1.4033756,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009232029&doi=10.1115%2f1.4033756&partnerID=40&md5=18dd2546e079052591e8913515e26676,"School of Mechanical, Industrial, and Manufacturing Engineering, Oregon State University, Corvallis, OR, United States; Department of Mechanical and Industrial Engineering, University of Illinois at Chicago, 2039 Engineering Research Facility, Chicago, IL, United States","Nagarajan, H.P.N., School of Mechanical, Industrial, and Manufacturing Engineering, Oregon State University, Corvallis, OR, United States; Malshe, H.A., School of Mechanical, Industrial, and Manufacturing Engineering, Oregon State University, Corvallis, OR, United States; Haapala, K.R., School of Mechanical, Industrial, and Manufacturing Engineering, Oregon State University, Corvallis, OR, United States; Pan, Y., Department of Mechanical and Industrial Engineering, University of Illinois at Chicago, 2039 Engineering Research Facility, Chicago, IL, United States","The emergence of additive manufacturing (AM) has potential for dramatic changes in labor productivity and economic welfare. With the growth of AM, understanding of the sustainability performance of relevant technologies is required. Toward that goal, an environmental impact assessment (EIA) approach is undertaken to evaluate an AM process. A novel fast mask image projection stereolithography (MIP-SL) process is investigated for the production of six functional test parts. The materials, energy, and wastes are documented for parts fabricated using this process. The EIA is completed for human health, ecosystem diversity, and resource costs using the ReCiPe 2008 impact assessment method. It is noted that process energy, in the form of electricity, is the key contributor for all three damage types. The results are used to depict the underlying relationship between energy consumed and the environmental impact of the process. Thus, to facilitate prediction of process energy utilization, a mathematical model relating shape complexity and dimensional size of the part with respect to part build time and washing time is developed. The effectiveness of this model is validated using data from real-time process energy monitoring. This work quantifies the elemental influence of design features on AM process energy consumption and environmental impacts. While focused on the environmental performance of the fast MIP-SL process, the developed approach can be extended to evaluate other AM processes and can encompass a triple bottom line analysis approach for sustainable design by predicting environmental, economic, and social performance of products. Copyright å© 2016 by ASME.",,Article,Scopus,2-s2.0-85009232029
"Nadir S., Streitferdt D.",Software code generator in automotive field,2015,"Proceedings - 2015 International Conference on Computational Science and Computational Intelligence, CSCI 2015",,,7424056,13,17,no,,,,10.1109/CSCI.2015.186,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964434755&doi=10.1109%2fCSCI.2015.186&partnerID=40&md5=a22f82f0392feaa49c3b3924848708a5,"Technical University of Ilmenau, Ilmenau, Germany","Nadir, S., Technical University of Ilmenau, Ilmenau, Germany; Streitferdt, D., Technical University of Ilmenau, Ilmenau, Germany","Rapid development of new technology has resulted changes in IT world affecting the way we work. Software use has increased in different fields of technology. In automation field, for example, the usage of Electronics Control Unit (ECU) has increased resulting in a different size of software. Due to persistent role of software in technology, the software cost and quality has become an important field. In automation field a number of tools are being developed which help to generate software codes. These tools can be used in software cost reduction because of reuse of software modules. In automatic code generation, the size estimates can hide the true effort of a program. In this paper we discuss the effect of using code generators in automation sector and how these tools have an effect on the cost of the software. We will compare the effort of code written manually and by using a code generator. å© 2015 IEEE.",Automotive; Code effort; Code generator; Software engineering,Conference Paper,Scopus,2-s2.0-84964434755
"NÌ_rman P., Johnson P., Gingnell L.",Using enterprise architecture to analyse how organisational structure impact motivation and learning,2016,Enterprise Information Systems,10,5,,523,562,no,,,2,10.1080/17517575.2014.986211,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919933262&doi=10.1080%2f17517575.2014.986211&partnerID=40&md5=cd594a9bda5f70cc525eed8b696838d9,"Industrial Information and Control Systems, Royal Institute of Technology, Stockholm, Sweden","NÌ_rman, P., Industrial Information and Control Systems, Royal Institute of Technology, Stockholm, Sweden; Johnson, P., Industrial Information and Control Systems, Royal Institute of Technology, Stockholm, Sweden; Gingnell, L., Industrial Information and Control Systems, Royal Institute of Technology, Stockholm, Sweden","When technology, environment, or strategies change, organisations need to adjust their structures accordingly. These structural changes do not always enhance the organisational performance as intended partly because organisational developers do not understand the consequences of structural changes in performance. This article presents a model-based analysis framework for quantitative analysis of the effect of organisational structure on organisation performance in terms of employee motivation and learning. The model is based on Mintzberg‰Ûªs work on organisational structure. The quantitative analysis is formalised using the Object Constraint Language (OCL) and the Unified Modelling Language (UML) and implemented in an enterprise architecture tool. å© 2014 Taylor & Francis.",architecture analysis; contingency theory; enterprise architecture; enterprise modelling; organisational structure,Article,Scopus,2-s2.0-84919933262
"Mukhopadhyay A., Ameri F.",An ontological approach to engineering requirement representation and analysis,2016,"Artificial Intelligence for Engineering Design, Analysis and Manufacturing: AIEDAM",30,4,,337,352,no,,,,10.1017/S0890060416000330,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989866189&doi=10.1017%2fS0890060416000330&partnerID=40&md5=5f4143d6c0b19defd8b89d4b9034b8ba,"Engineering Informatics Research Group, Texas State University, San Marcos, TX, United States","Mukhopadhyay, A., Engineering Informatics Research Group, Texas State University, San Marcos, TX, United States; Ameri, F., Engineering Informatics Research Group, Texas State University, San Marcos, TX, United States","Requirement planning is one of the most critical tasks in the product development process. Despite its significant impact on the outcomes of the design process, engineering requirement planning is often conducted in an ad hoc manner without much structure. In particular, the requirement planning phase suffers from a lack of quantifiable measures for evaluating the quality of the generated requirements and also a lack of structure and formality in representing engineering requirements. The main objective of this research is to develop a formal Web Ontology Language ontology for standard representation of engineering requirements. The proposed ontology uses explicit semantics that makes the ontology amenable to automated reasoning. To demonstrate how the proposed ontology can support requirement analysis and evaluation in engineering design, three possible services enabled by the ontology are introduced in this paper. These services are information content measurement, specificity and completeness analysis, and requirement classification. The proposed ontology and its associated algorithms and tools are validated experimentally in this work. å© 2016 Cambridge University Press.",Engineering Requirements; Information Content; Ontological Reasoning; Ontology; Requirement Management,Article,Scopus,2-s2.0-84989866189
"Morimoto T., Kobayashi S., Nagao Y., Iwahori Y.",Learning curve constraint cell automaton model for the lean production of CFRP airframe components,2016,International Journal of Advanced Manufacturing Technology,84,9-Dec,,2127,2137,no,,,,10.1007/s00170-015-7868-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944596854&doi=10.1007%2fs00170-015-7868-6&partnerID=40&md5=4ea9cd59db8ef1e883b0db3293748589,"Japan Aerospace Exploration Agency (JAXA), Osawa 6-13-1, Mitaka-shi, Tokyo, Japan; Tokyo Metropolitan University, Minami-Osawa 1-1, Hachioji-shi, Tokyo, Japan; Kanagawa Institute of Technology, Shimo-Ogino 1030, Atsugi-shi, Kanagawa, Japan","Morimoto, T., Japan Aerospace Exploration Agency (JAXA), Osawa 6-13-1, Mitaka-shi, Tokyo, Japan; Kobayashi, S., Tokyo Metropolitan University, Minami-Osawa 1-1, Hachioji-shi, Tokyo, Japan; Nagao, Y., Kanagawa Institute of Technology, Shimo-Ogino 1030, Atsugi-shi, Kanagawa, Japan; Iwahori, Y., Japan Aerospace Exploration Agency (JAXA), Osawa 6-13-1, Mitaka-shi, Tokyo, Japan","A new model is proposed for the lean production of carbon fiber reinforced plastic (CFRP) airframes. Our method improves the production rate by determining the ideal human-capital balance and inventory density on the factory line. The proposed model is derived as a two-step process: First, an analytical solution for the learning rate shift with a human-capital ratio is obtained by merging the Wright learning curve model into the Cobb-Douglas production function. Second, the solution is factored by an asymmetric simple exclusion process (ASEP) cell automaton model to assess whether the inventory density negates the theoretical learning effect. Recent moves toward lean production mean that aerospace CFRPs have a limited shelf life, minimizing buffer periods under metastable and stable production in the time discrete ASEP model. The shift from metastable to stable changes the production rate. Combined with the fact that ASEP is known to drastically reduce throughput if the production steps are not harmonized, the shipment probability p at each step becomes less than 1. Therefore, the human learning effect, which can alter the shipment rate, must be controlled so that p=1 at each production step. This paper describes the analytical aspects of the apparent learning rate to determine adequate values for the human and capital resources, and thus harmonize the learning rates of the production steps. The analytical model shows that factory planning dominates the production rate of CFRP aerospace components. The model is applied to Boeing 787 production data, and it is found that a reduction in inventory density could improve the apparent delivery rate up to the maximum of the human potential. å© 2015, The Author(s).",Airframe component; Airframe production; Asymmetric simple exclusion process (ASEP); Cell automaton; CFRP; Cobb-Douglas production function; Human fraction; Lean production; Man-hour; Wolfram rule 184; Wright learning curve,Article,Scopus,2-s2.0-84944596854
Mori T.,Superposed naive bayes for accurate and interpretable prediction,2015,"Proceedings - 2015 IEEE 14th International Conference on Machine Learning and Applications, ICMLA 2015",,,7424489,1228,1233,no,Defect,,,10.1109/ICMLA.2015.147,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969651482&doi=10.1109%2fICMLA.2015.147&partnerID=40&md5=986a01af2a899eb1b84c5555ade439e4,"IoT Technology Center, Industrial ICT Solutions Company, TOSHIBA Corporation, 1 Komukai-Toshiba-cho, Saiwai-ku, Kawasaki, Japan","Mori, T., IoT Technology Center, Industrial ICT Solutions Company, TOSHIBA Corporation, 1 Komukai-Toshiba-cho, Saiwai-ku, Kawasaki, Japan","Background: Data mining and machine learning techniques have been widely applied in software engineering research. However, past research has mainly focused on only prediction accuracy. Aim: The interpretability of prediction results should be accorded greater emphasis in software engineering research. A prediction model that has high accuracy and explanatory power is required. Method: We propose a new algorithm of naÌøve Bayes ensemble, called superposed naÌøve Bayes (SNB), which firstly builds an ensemble model with high prediction accuracy and then transforms it into an interpretable naÌøve Bayes model. Results: We conducted an experiment with the NASA MDP datasets, in which the performance and interpretability of the proposed method were compared with those of other classification techniques. The results of the experiment indicate that the proposed method can produce balanced outputs that satisfy both performance and interpretability criteria. Conclusion: We confirmed the effectiveness of the proposed method in an experiment using software defect data. The model can be extensively applied to other application areas, where both performance and interpretability are required. å© 2015 IEEE.",Bagging; Boosting; Ensemble method; Interpretability; Model transformatiomn; Naive Bayes classifier; Performance,Conference Paper,Scopus,2-s2.0-84969651482
"Moldovan D., Truong H.-L., Dustdar S.",Cost-aware scalability of applications in public clouds,2016,"Proceedings - 2016 IEEE International Conference on Cloud Engineering, IC2E 2016: Co-located with the 1st IEEE International Conference on Internet-of-Things Design and Implementation, IoTDI 2016",,,7484167,79,88,no,,,,10.1109/IC2E.2016.23,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978085708&doi=10.1109%2fIC2E.2016.23&partnerID=40&md5=71e3b3b2a2ee982fb39cb36dae5a8c8b,"Distributed Systems Group, TU Wien, Austria","Moldovan, D., Distributed Systems Group, TU Wien, Austria; Truong, H.-L., Distributed Systems Group, TU Wien, Austria; Dustdar, S., Distributed Systems Group, TU Wien, Austria","Scalable applications deployed in public clouds can be built from a combination of custom software components and public cloud services. To meet performance and/or cost requirements, such applications can scale-out/in their components during run-time. When higher performance is required, new component instances can be deployed on newly allocated cloud services (e.g., virtual machines). When the instances are no longer needed, their services can be deallocated to decrease cost. However, public cloud services are usually billed over predefined time and/or usage intervals, e.g., per hour, per GB of I/O. Thus, it might not be cost efficient to scale-in public cloud applications at any moment in time, without considering their billing cycles. In this work we aid developers of scalable applications for public clouds to monitor their costs, and develop cost-aware scalability controllers. We introduce a model for capturing the pricing schemes of cloud services. Based on the model we determine and evaluate the application's costs depending on its used cloud services and their billing cycles. We further evaluate cost efficiency of cloud applications, analyzing which application component is cost efficient to deallocate and when. We evaluate our approach on a scalable platform for IoT, deployed in Flexiant1, one of the leading European public cloud providers. We show that cost-aware scalability can achieve higher application stability and performance, while reducing its operation costs. å© 2016 IEEE.",Cloud; Cost Efficiency; Elasticity; Run-Time Control; Scalability,Conference Paper,Scopus,2-s2.0-84978085708
Mo L.,The Engineering Cost Evaluation Based on IPSO,2015,"Proceedings - 8th International Conference on Intelligent Computation Technology and Automation, ICICTA 2015",,,7473462,962,966,no,,,,10.1109/ICICTA.2015.245,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007406312&doi=10.1109%2fICICTA.2015.245&partnerID=40&md5=71d7e18767d88c55b70b4bbdf570cac5,"Hunan City University, Yiyang, Hunan, China","Mo, L., Hunan City University, Yiyang, Hunan, China","The rough set theory was used to reduce the factors affecting construction engineering cost and optimize input variables of BP neural network. Then, the improved particle swarm algorithm with constriction factors is adopted to optimize the initial weights and thresholds. An engineering project in a city of Hunan is selected to make empirical analysis. It shows that based on the features of engineering, this new model enjoys a high practical value as it can be applied to make scientific evaluation of costs of construction engineering. å© 2015 IEEE.",Artificial Neural Networks; cost; IPSO,Conference Paper,Scopus,2-s2.0-85007406312
"Mittas N., Kakarontzas G., Bohlouli M., Angelis L., Stamelos I., Fathi M.",ComProFITS: A web-based platform for human resources competence assessment,2015,"IISA 2015 - 6th International Conference on Information, Intelligence, Systems and Applications",,,7388113,,,no,,,,10.1109/IISA.2015.7388113,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963860041&doi=10.1109%2fIISA.2015.7388113&partnerID=40&md5=07943f0446c52ef337804cdbf1bcc775,"Department of Informatics, Aristotle University of Thessaloniki, Thessaloniki, Greece; Department of Computer Science and Engineering, T.E.I. of Thessaly, Larissa, Greece; Faculty of Science and Engineering, University of Siegen, Siegen, Germany","Mittas, N., Department of Informatics, Aristotle University of Thessaloniki, Thessaloniki, Greece; Kakarontzas, G., Department of Informatics, Aristotle University of Thessaloniki, Thessaloniki, Greece, Department of Computer Science and Engineering, T.E.I. of Thessaly, Larissa, Greece; Bohlouli, M., Faculty of Science and Engineering, University of Siegen, Siegen, Germany; Angelis, L., Department of Informatics, Aristotle University of Thessaloniki, Thessaloniki, Greece; Stamelos, I., Department of Informatics, Aristotle University of Thessaloniki, Thessaloniki, Greece; Fathi, M., Faculty of Science and Engineering, University of Siegen, Siegen, Germany","An efficient assessment of human resource competences, followed by the goal oriented analysis of the results, support the identification of the competence gaps in organizations and the allocation of resources towards identified gaps. This paper presents the basic idea as well as the scientific and implementation results of the ComProFITS project as an innovative web-based platform for the evaluation of existing employees and the recruitment of new employees in organizations. The platform integrates research on the statistical assessment of competences, an innovative competence pyramid and many alternative methods of employee evaluation and recruitment. The initial version of the platform is being applied and used in the IT sector in a large organization in Spain with the goal of extending it to the Spanish and European wide enterprises in different sectors. å© 2015 IEEE.",,Conference Paper,Scopus,2-s2.0-84963860041
"Mishra M., Dhir S.",Appraisement of different software estimation models: A rumination and contradistinction,2015,"International Conference on Soft Computing Techniques and Implementations, ICSCTI 2015",,,7489622,152,155,no,,,,10.1109/ICSCTI.2015.7489622,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979695268&doi=10.1109%2fICSCTI.2015.7489622&partnerID=40&md5=ff76fbbd73fd1ac5870254b2e50b5fb1,"Amity University Uttar Pradesh, India","Mishra, M., Amity University Uttar Pradesh, India; Dhir, S., Amity University Uttar Pradesh, India","In software project development, proclamation of programming happens after a few programming, reporting, testing and bug settling. The coordinated philosophy helps the product venture improvement in giving distinct options for customary task administration by presenting a few effort and expense estimation models, yet the best technique to appraise productively has still not been found. As few models are agreement based and others are relapsing based so it gets to be troublesome for the analyzer to settle on a choice. In this paper, a few models on agile methodology are being looked at on a few parameters, in order to check the disadvantages and also the advantages of the same furthermore to become acquainted with the distinctive estimating models. å© 2015 IEEE.",Agile methodology; Estimation models; Software project development,Conference Paper,Scopus,2-s2.0-84979695268
"Mishra D., Mahanty B.","A study of software development project cost, schedule and quality by outsourcing to low cost destination",2016,Journal of Enterprise Information Management,29,3,,454,478,no,,,,10.1108/JEIM-08-2014-0080,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962625706&doi=10.1108%2fJEIM-08-2014-0080&partnerID=40&md5=c8edae356290a0bd80122390b49bb99f,"RG Indian Institute of Management, Shillong, India; Dept of Industrial and Systems Engineering, IIT Kharagpur, Kharagpur, India","Mishra, D., RG Indian Institute of Management, Shillong, India; Mahanty, B., Dept of Industrial and Systems Engineering, IIT Kharagpur, Kharagpur, India","Purpose ‰ÛÒ The purpose of this paper is to find good values of onsite-offshore team strength; number of hours of communication between business users and onsite team and between onsite and offshore team so as to reduce project cost and improve schedule in a global software development (GSD) environment for software development project. Design/methodology/approach ‰ÛÒ This study employs system dynamics simulation approach to study software project characteristics in both co-located and distributed development environments. The authors consulted 14 experts from Indian software outsourcing industry during our model construction and validation. Findings ‰ÛÒ The study results show that there is a drop in overall team productivity in outsourcing environment by considering the offshore options. But the project cost can be reduced by employing the offshore team for coding and testing work only with minimal training for imparting business knowledge. The research results show that there is a potential to save project cost by being flexible in project schedule. Research limitations/implications ‰ÛÒ The implication of the study is that the project management team should be careful not to keep high percentage of manpower at offshore location in distributed software environment. A large offshore team can increase project cost and schedule due to higher training overhead, lower productivity and higher error proneness. In GSD, the management effort should be to keep requirement analysis and design work at onsite location and involves the offshore team in coding and testing work. Practical implications ‰ÛÒ The software project manager can use the model results to divide the software team between onsite and offshore location during various phases of software development in distributed environment. Originality/value ‰ÛÒ The study is novel as there is little attempt at finding the team distribution between onsite and offshore location in GSD environment. å© 2016, å© Emerald Group Publishing Limited.",Global software development; Outsourcing; Team distribution,Article,Scopus,2-s2.0-84962625706
"Meng Q., Lou J., Zhu J., Bai X.",Standard cost setting and application of improved ant colony optimization algorithm,2016,Xitong Gongcheng Lilun yu Shijian/System Engineering Theory and Practice,36,7,,1719,1731,no,,,,10.12011/1000-6788(2016)07-1719-13,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984659279&doi=10.12011%2f1000-6788%282016%2907-1719-13&partnerID=40&md5=382a664a31c10db22b678c3352cfe323,"Faculty of Management and Economics, Dalian University of Technology, Dalian, China","Meng, Q., Faculty of Management and Economics, Dalian University of Technology, Dalian, China; Lou, J., Faculty of Management and Economics, Dalian University of Technology, Dalian, China; Zhu, J., Faculty of Management and Economics, Dalian University of Technology, Dalian, China; Bai, X., Faculty of Management and Economics, Dalian University of Technology, Dalian, China","Setting standard cost, while ignoring the requirements of product quality and efficiency in production processes, will weaken cost control ability of standard cost. To solve the problem, the optimization method of standard cost setting based on the relations among production time, product quality and production cost was studied. Then a mathematical model of standard cost setting was formulated with objectives to minimize product standard cost and differentials between actual standard production time and ideal production time simultaneously under the condition of product quality requirements. Space-partition ant colony optimization algorithm was designed, and an ant searching strategy was established to overcome the precocious phenomenon. Compared with ant colony optimization algorithm with variable weight, the improved algorithm outperforms the latter. Finally, using the actual cost data of some enterprise, the proposed method of standard cost setting was compared with the present method in the enterprise. The results of simulation experiment testify that the optimization method of standard cost setting has a better effect in enhancing product quality and production efficiency, decreasing the standard cost, and reflecting resource consumption. So it can help to control cost and support for cost lean management in production processes. å© 2016, Editorial Board of Journal of Systems Engineering Society of China. All right reserved.",Improved ant colony optimization algorithm; Production time; Requirements of quality control; Standard cost setting,Article,Scopus,2-s2.0-84984659279
"McRitchie K., Spiewak R.",Re-using open source software in your software delivery,2016,CrossTalk,29,1,,20,27,no,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959906541&partnerID=40&md5=b229c6e5a1d81771c412374820b1f3db,"Galorath Incorporated, United States; MITRE Corporation, United States","McRitchie, K., Galorath Incorporated, United States; Spiewak, R., MITRE Corporation, United States",Open source software is generally available with few restrictions (depending on license) to be reused by other developers. Reuse of software presents both potential cost and schedule savings and corresponding risks to both cost and schedule. The key defining characteristic which distinguishes between risk and reward in this scenario is the quality of the software to be re-used. Wellestablished metrics and best practices in software development can be applied and assessed when examining open-source software for potential re-use.,,Article,Scopus,2-s2.0-84959906541
"Marzouk M., Azab S., Metawie M.",Framework for sustainable low-income housing projects using building information modeling,2016,Journal of Environmental Informatics,28,1,,25,38,no,,,,10.3808/jei.201600332,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991511088&doi=10.3808%2fjei.201600332&partnerID=40&md5=9756890de4e4636dfa215bbb8f36fa8d,"Structural Engineering Department, Faculty of Engineering, Cairo University, Giza, Egypt; National Authority for Remote Sensing and Space Sciences (NARSS), Cairo, Egypt","Marzouk, M., Structural Engineering Department, Faculty of Engineering, Cairo University, Giza, Egypt; Azab, S., National Authority for Remote Sensing and Space Sciences (NARSS), Cairo, Egypt; Metawie, M., Structural Engineering Department, Faculty of Engineering, Cairo University, Giza, Egypt","Governments in developing countries serve low income people by constructing low income housing projects. Most of total life cycle cost (LCC) of these projects is incurred during the operational phase, making these projects lose their economic aspect of sustainability. Decreasing the cost of low income housing projects and taking into consideration the available resources are two cru-cial facets that should be taken into consideration. Therefore, this paper focuses on two aspects of the sustainability of building through its life cycle, which are environmental and economic aspects using environmentally friendly materials in construction. To accomplish this aim, a framework is developed that integrates Building Information Modeling (BIM) with computer simulation, optimization and system dynamics in Low Income Housing (LIH) projects. The developed framework helps in determining LIH project duration and selecting the optimum alternative with to material systems. The sustainability aspects of building are achieved by considering a LCC of these buildings and the number of points that can be awarded under the Leadership in Energy and Environmental Design (LEED) ra-ting system. It aids government and/or contractors in adopting BIM technology to minimize life cycle cost while achieving maximum LEED materials credits points for LIH projects. Social Housing project in Badr City-Egypt is considered as a case study to demon-strate the use of the developed prototype and to illustrate its essential features. The results have shown that selection of sustainable-building materials for the construction has a crucial role in the formation of a sustainable building as they affect the performance of building. Therefore, the Egyptian government should consider building materials with sustainable properties and a low LCC in the de-sign phase to mitigate the negative impacts of LIH projects. å© 2016 ISEIS. All rights reserved.",Building information modeling; Computer simulation; Low income housing; Optimization; System dynamics,Article,Scopus,2-s2.0-84991511088
"Martins S., Chucre M.R.R.B., De Macedo J.A.F., Monteiro J., Casanova M.A.",On computing temporal functions for a time-dependent networks using trajectory data,2016,ACM International Conference Proceeding Series,11-13-July-2016,,,236,241,no,,,,10.1145/2938503.2938542,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989237187&doi=10.1145%2f2938503.2938542&partnerID=40&md5=06d62835774a16ff10ead5970689d922,"Nascimento, Federal University of Ceara, Ceara, Brazil; Pontifical Catholic University of Rio de Janeiro, Rio de Janeiro, Brazil","Martins, S., Nascimento, Federal University of Ceara, Ceara, Brazil; Chucre, M.R.R.B., Nascimento, Federal University of Ceara, Ceara, Brazil; De Macedo, J.A.F., Nascimento, Federal University of Ceara, Ceara, Brazil; Monteiro, J., Nascimento, Federal University of Ceara, Ceara, Brazil; Casanova, M.A., Pontifical Catholic University of Rio de Janeiro, Rio de Janeiro, Brazil","Time dependent networks are of key importance to allow computing precise travel times taking into consideration moving object's departure time. However the computation of time functions that are used to annotate time dependent networks are challenging since we must cope with noisy and incomplete traffic data. Recent related works adopt approaches that build Piecewise linear functions, which do not cope with aforementioned problems. In this work, we propose a new method for generating Piecewise linear functions by applying a map-matching technique allied to a curve smoothing approach in order to treat outliers and complete data. We performed experiments using real trajectory data and compared our results with a baseline. Preliminary results show that our approach generates time functions with better approximation than the baseline competitor. å© ACM 2016.",Functions & piecewise; Time dependent network,Conference Paper,Scopus,2-s2.0-84989237187
"MartÌ_nez-Rojas M., MarÌ_n N., Vila M.A.",The role of information technologies to address data handling in construction project management,2016,Journal of Computing in Civil Engineering,30,4,4015064,,,no,,,2,10.1061/(ASCE)CP.1943-5487.0000538,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975300033&doi=10.1061%2f%28ASCE%29CP.1943-5487.0000538&partnerID=40&md5=9fccbd2a6d19808ca8eff5ca724bcf7a,"Dept. of Building Construction, Univ. of Granada, C/Severo Ochoa s/n, Granada, Spain; Dept. of Computer Science and Artificial Intelligence, Univ. of Granada, C/Periodista Daniel Saucedo Aranda s/n, Granada, Spain","MartÌ_nez-Rojas, M., Dept. of Building Construction, Univ. of Granada, C/Severo Ochoa s/n, Granada, Spain; MarÌ_n, N., Dept. of Computer Science and Artificial Intelligence, Univ. of Granada, C/Periodista Daniel Saucedo Aranda s/n, Granada, Spain; Vila, M.A., Dept. of Computer Science and Artificial Intelligence, Univ. of Granada, C/Periodista Daniel Saucedo Aranda s/n, Granada, Spain","Construction is an extremely information-dependent industry in which a project's success largely depends on good access to and management of data. Effective project management requires the characterization of its challenging issues and the use of appropriate tools for data handling. For this purpose, the construction industry is increasingly adopting the use of information and communication technologies (ICT) in recent years. Given the acknowledged potential of ICT to bring about improvements in other industries, many initiatives have been undertaken to develop appropriate tools to support various tasks during the construction project lifecycle. This paper focuses on the proposals that use ICT to provide access to the data and take advantage of this access to manage crucial issues within project management such as costs, planning, risks, safety, progress monitoring, and quality control. The authors will demonstrate that suitable data handling facilitates and improves the decision-making process and helps to carry out successful project management. å© 2015 American Society of Civil Engineers.",Construction project management; Data management; Information and communication technologies (ICT),Review,Scopus,2-s2.0-84975300033
"Marcondes Filho D., Sant‰ÛªAnna A.M.O.",Principal component regression-based control charts for monitoring count data,2016,International Journal of Advanced Manufacturing Technology,85,5-Aug,,1565,1574,no,,,,10.1007/s00170-015-8054-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946772137&doi=10.1007%2fs00170-015-8054-6&partnerID=40&md5=3f17ca0f0dc4de6fbd736e642566c98f,"Department of Statistics, Federal University of Rio Grande do Sul, Porto Alegre, Rio Grande do Sul, Brazil; Department of Mechanical Engineering, Federal University of Bahia, Salvador, Bahia, Brazil","Marcondes Filho, D., Department of Statistics, Federal University of Rio Grande do Sul, Porto Alegre, Rio Grande do Sul, Brazil; Sant‰ÛªAnna, A.M.O., Department of Mechanical Engineering, Federal University of Bahia, Salvador, Bahia, Brazil","Control charts based on regression models are appropriate for monitoring in which the quality characteristics of products vary depending on the behavior of predecessor variables. Its use enables monitoring the correlation structure between input variables and the response variable through residuals from the fitted model according to historical process data. However, such strategy is restricted to data from input variables which are not significantly correlated. Otherwise, colinear variables that hold substantial information on the variability of the response variable might be absent in the regression model adjustment. This paper proposes a strategy for monitoring count data combining Poisson regression and principal component analysis. In such strategy, colinear variables are turned into uncorrelated variables by principal component analysis and a Poisson regression is performed on principal component scores. A deviance residual control chart from the fitted model is then used to evaluate the process. The performance of that new approach is illustrated through a case study in a plastic plywood process with real and simulated data. å© 2015, Springer-Verlag London.",Count data; PCA; Poisson regression; Residual control charts; Statistical processes control,Article,Scopus,2-s2.0-84946772137
"Mao F., Cai X., Shen B., Xia Y., Jin B.",Operational pattern based code generation for management information system: An industrial case study,2016,"2016 IEEE/ACIS 17th International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing, SNPD 2016",,,7515935,425,430,no,,,,10.1109/SNPD.2016.7515935,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983315002&doi=10.1109%2fSNPD.2016.7515935&partnerID=40&md5=99b23f888879e45cfcc9c5ab0228edf7,"School of Software, Shanghai Jiao Tong University, Shanghai, China; IBM Client Innovation Center China, Shanghai, China; Third Research Institute of Ministry of Public Security, Shanghai, China","Mao, F., School of Software, Shanghai Jiao Tong University, Shanghai, China; Cai, X., School of Software, Shanghai Jiao Tong University, Shanghai, China; Shen, B., School of Software, Shanghai Jiao Tong University, Shanghai, China; Xia, Y., IBM Client Innovation Center China, Shanghai, China; Jin, B., Third Research Institute of Ministry of Public Security, Shanghai, China","Code generation technology can significantly improve productivity and software quality. However, due to limited financial and human resources in most of small and medium software enterprises, there are many challenges when leveraging code generation approaches to large-scale software development. In this paper, an operational pattern based code generation approach is proposed for rapid development of domain-specific management information system. We demonstrate the approach with details: (I) semi-automatically extracting operational patterns from requirement documents, (II) building feature models to manage the commonalities and variability of each operational pattern, (III) mapping operational patterns into skeleton code with a template-based code generation technique, etc. Then we conduct an industrial case study in asset information management domain at CancoSoft Company for about 2 years, to analyze its feasibility and efficiency. 14 operational patterns are successfully extracted from 355 initial key phrases, and a code generator is implemented and applied to develop new Web applications. Preliminary findings show that the software development based on our approach yields a nearly 30% higher productivity as compared to traditional software development. Through code analysis, we find that around 70% of code can be automatically generated, and the generated code is also effective. å© 2016 IEEE.",Code Generation; Domain Engineering; Exploratory Case Study; Operational Pattern; Software Product Line,Conference Paper,Scopus,2-s2.0-84983315002
"Mansor Z., Arshad N.H., Yahya S., Razali R., Yahaya J.",Ruler for effective cost management practices in agile software development projects,2016,Advanced Science Letters,22,8,,1977,1980,no,,,,10.1166/asl.2016.7751,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984984936&doi=10.1166%2fasl.2016.7751&partnerID=40&md5=951af0774326779808bf1d1f79a02a96,"Research Center for Software Technology and Management, Faculty of Information Science and Technology, Universiti Kebangsaan Malaysia, Bangi, Selangor Darul Ehsan, Malaysia; Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA, Shah Alam, Selangor Darul Ehsan, Malaysia","Mansor, Z., Research Center for Software Technology and Management, Faculty of Information Science and Technology, Universiti Kebangsaan Malaysia, Bangi, Selangor Darul Ehsan, Malaysia; Arshad, N.H., Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA, Shah Alam, Selangor Darul Ehsan, Malaysia; Yahya, S., Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA, Shah Alam, Selangor Darul Ehsan, Malaysia; Razali, R., Research Center for Software Technology and Management, Faculty of Information Science and Technology, Universiti Kebangsaan Malaysia, Bangi, Selangor Darul Ehsan, Malaysia; Yahaya, J., Research Center for Software Technology and Management, Faculty of Information Science and Technology, Universiti Kebangsaan Malaysia, Bangi, Selangor Darul Ehsan, Malaysia","Managing cost in any software development project become more challenging and sensitive these days. This is due to the complexity of the processes and the demand by the stakeholders that really requires high competent of project managers to manage the project cost effectively. Most of project managers agreed that managing cost effectively is very important things to note. Effective cost management practices determine the level of project success. Therefore, this paper discusses on effective cost management practices and develop a ruler for cost management practices in agile software development projects towards few variables such as customer involvement, changes of requirements, simplicity and others. The findings show that most of the project managers are practicing cost management process in agile software development project. However, the result shows the difference levels of practicing for effective cost management in agile software development projects due to lack of experiences, lack of knowledge, lack of automated tool and other. The differences can be seen thru the ruler that has been developed in this research. The ruler helps organization in determining the competent project managers for effectively manage the project cost and finally enhance the success of project. å© 2016 American Scientific Publishers. All rights reserved.",Customer involvement; Effective; Rasch measurement; Requirement; Simplicity; Software project management; Stakeholders,Article,Scopus,2-s2.0-84984984936
"Manogharan G., Wysk R.A., Harrysson O.L.A.",Additive manufacturing-integrated hybrid manufacturing and subtractive processes: Economic model and analysis,2016,International Journal of Computer Integrated Manufacturing,29,5,,473,488,no,,,2,10.1080/0951192X.2015.1067920,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960249693&doi=10.1080%2f0951192X.2015.1067920&partnerID=40&md5=813487acf5f62b3e69af1614b9f13bdc,"Department of Mechanical and Industrial Engineering, Youngstown State University, Youngstown, OH, United States; Department of Industrial and Systems Engineering, North Carolina State University, Raleigh, NC, United States","Manogharan, G., Department of Mechanical and Industrial Engineering, Youngstown State University, Youngstown, OH, United States; Wysk, R.A., Department of Industrial and Systems Engineering, North Carolina State University, Raleigh, NC, United States; Harrysson, O.L.A., Department of Industrial and Systems Engineering, North Carolina State University, Raleigh, NC, United States","This article presents economic models for a new hybrid method where additive manufacturing (AM) and subtractive methods (SMs) are integrated through composite process planning. Although AM and SM offer several unique advantages, there are technological limitations such as tolerance and surface finish requirements; tooling and fixturing, etc. that cannot be met by a single type of manufacturing. The intent of this article is not to show a new manufacturing method, but rather to provide economic context to additive and subtractive methods as the best practice provides, and look at the corresponding economics of each of those methods as a function of production batch size, machinability, cost of the material, part geometry and tolerance requirements. Basic models of fixed and variable costs associated with additive, subtractive and hybrid methods to produce parts are also presented. An experimental design is used to study the influence of production volume, material and operating cost, batch size, machinability of the material and impact of reducing AM processing time. A composite response model for the unit cost is computed for the various levels associated with such engineering requirements. The developed models provide insight into how these variables affect the costs associated with engineering a mechanical product that will be produced using AM and SM methods. From the results, it appears that batch size, AM processing time and AM processing cost were the major cost factors. It was shown that the cost of producing near-net shape through SM and AM was the decision criteria; which will be critical for tough-to-machine alloys and at multi-batch size. å© 2015 Taylor & Francis.",AIMS; CNC-RP; EBM; Economic analysis and additive manufacturing; Hybrid manufacturing,Article,Scopus,2-s2.0-84960249693
"Mannion M., Savolainen J.",Choosing reusable software strategies,2016,ACM International Conference Proceeding Series,16-23-September-2016,,,227,231,no,,,,10.1145/2934466.2934494,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991610671&doi=10.1145%2f2934466.2934494&partnerID=40&md5=617c37fd46f15d098723ffb1c15ae11c,"Glasgow Caledonian University, 70 Cowcaddens Road, Glasgow, United Kingdom; Roche Diagnostics International Ltd, Forrenstrasse 2, Rotkreuz, Switzerland","Mannion, M., Glasgow Caledonian University, 70 Cowcaddens Road, Glasgow, United Kingdom; Savolainen, J., Roche Diagnostics International Ltd, Forrenstrasse 2, Rotkreuz, Switzerland","For many organisations, choosing a reusable software strategy such as whether to be developing products, platforms or components, or some combination of these is not straightforward. The appropriateness of the choice can also change as an organisation's internal and external business environment context changes. In this paper we provide a management tool to help guide that decision making. We set out four broad types of business strategy and map these against four different types of reusable software development strategy. The four types of business strategy correspond to different business environments which are in turn characterised by different combinations of market predictability (low to high) and an organisation's ability to influence it (low to high). To demonstrate the framework as an analytical tool we have mapped examples of different organisations reusable software strategies and explained some circumstances in which that organisation's strategy may change. å© 2016 ACM.",Reuse; Strategy,Conference Paper,Scopus,2-s2.0-84991610671
"MagoulÌ¬s F., Zhao H.-X.",Data Mining and Machine Learning in Building Energy Analysis,2016,Data Mining and Machine Learning in Building Energy Analysis,,,,1,164,no,,,1,10.1002/9781118577691,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983001953&doi=10.1002%2f9781118577691&partnerID=40&md5=4bcb41dac801d186f010ee4c04fcb222,,"MagoulÌ¬s, F.; Zhao, H.-X.","Focusing on up-to-date artificial intelligence models to solve building energy problems, Artificial Intelligence for Building Energy Analysis reviews recently developed models for solving these issues, including detailed and simplified engineering methods, statistical methods, and artificial intelligence methods. The text also simulates energy consumption profiles for single and multiple buildings. Based on these datasets, Support Vector Machine (SVM) models are trained and tested to do the prediction. Suitable for novice, intermediate, and advanced readers, this is a vital resource for building designers, engineers, and students. å© ISTE Ltd 2016. All rights reserved.",,Book,Scopus,2-s2.0-84983001953
Maciol A.,Knowledge-based methods for cost estimation of metal casts,2016,International Journal of Advanced Manufacturing Technology,,,,1,16,no,,,,10.1007/s00170-016-9704-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996848898&doi=10.1007%2fs00170-016-9704-z&partnerID=40&md5=30c1edf7dd2c4925409a3fabbf4da26f,"Faculty of Management, AGH University of Science and Technology, ul. Gramatyka 10, KrakÌ_w, Poland","Maciol, A., Faculty of Management, AGH University of Science and Technology, ul. Gramatyka 10, KrakÌ_w, Poland","The aim of the research presented in this paper was to verify the hypothesis that the problem of estimating the variable costs of metal casts can be solved by using knowledge-based systems. The estimation of variable costs determines the adequate pricing of products, which is a crucial marketing problem in enterprises. The problem of estimating the direct costs is especially important in companies whose participation in the market is defined as versatile manufacturing. This problem was the target of many studies presented in this paper. The assumptions that the direct costs of a metal cast are dependent on the physical features of the cast and the most effective method of cost estimation is an intuitive qualitative technique based on a rule-based system were considered. The nature of the relationships between the casts‰Ûª features and the direct costs of production could be effectively formed by experts as reasoning rules. In this paper, the results of experiments conducted on the basis of the example of the production program of a foundry that manufactures water-system fittings is presented. Three methods of knowledge-definition and reasoning techniques were examined: the classical method of crisp reasoning, fuzzy reasoning with the Mamdani‰Ûªs method, and fuzzy reasoning with the Takagi-Sugeno model. Proper experiments were conducted. The results of the experiments confirmed the hypothesis, which allows the assessment of different reasoning methods. å© 2016 The Author(s)",Cost estimation; Fuzzy reasoning; Knowledge based reasoning; Metal casts; Versatile manufacturing,Article in Press,Scopus,2-s2.0-84996848898
"Maabout S., Ordonez C., Wanko P.K., Hanusse N.",Skycube materialization using the topmost skyline or functional dependencies,2016,ACM Transactions on Database Systems,41,4,25,,,no,,,,10.1145/2955092,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996614639&doi=10.1145%2f2955092&partnerID=40&md5=76f4b79957487b6598880afa97de0455,"LaBRI-University of Bordeaux, 351 cours de la LibÌ©ration, Talence, France; University of Houston, Department of Computer ScienceTX, United States; University of Bordeaux-CNRS, France","Maabout, S., LaBRI-University of Bordeaux, 351 cours de la LibÌ©ration, Talence, France; Ordonez, C., University of Houston, Department of Computer ScienceTX, United States; Wanko, P.K., LaBRI-University of Bordeaux, 351 cours de la LibÌ©ration, Talence, France; Hanusse, N., LaBRI-University of Bordeaux, 351 cours de la LibÌ©ration, Talence, France, University of Bordeaux-CNRS, France","Given a table T(Id, D1, . . . , Dd), the skycube of T is the set of skylines with respect to to all nonempty subsets (subspaces) of the set of all dimensions {D1, . . . , Dd}. To optimize the evaluation of any skyline query, the solutions proposed so far in the literature either (i) precompute all of the skylines or (ii) use compression techniques so that the derivation of any skyline can be done with little effort. Even though solutions (i) are appealing because skyline queries have optimal execution time, they suffer from time and space scalability because the number of skylines to be materialized is exponential with respect to d. On the other hand, solutions (ii) are attractive in terms of memory consumption, but as we show, they also have a high time complexity. In this article, we make contributions to both kinds of solutions. We first observe that skyline patterns are monotonic. This property leads to a simple yet efficient solution for full and partial skycube materialization when the skyline with respect to all dimensions, the topmost skyline, is small. On the other hand, when the topmost skyline is large relative to the size of the input table, it turns out that functional dependencies, a fundamental concept in databases, uncover a monotonic property between skylines. Equipped with this information, we show that closed attributes sets are fundamental for partial and full skycube materialization. Extensive experiments with real and synthetic datasets show that our solutions generally outperform state-of-the-art algorithms. å© 2016 ACM.",Implementation; Materialization,Article,Scopus,2-s2.0-84996614639
"Ma Z., Liu Z., Wei Z.",Formalized Representation of Specifications for Construction Cost Estimation by Using Ontology,2016,Computer-Aided Civil and Infrastructure Engineering,31,1,,4,17,no,,,3,10.1111/mice.12175,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955191085&doi=10.1111%2fmice.12175&partnerID=40&md5=86d1edd242cba8febb6b17e7cc4a095a,"Department of Civil Engineering, Tsinghua University, Haidian District, Beijing, China","Ma, Z., Department of Civil Engineering, Tsinghua University, Haidian District, Beijing, China; Liu, Z., Department of Civil Engineering, Tsinghua University, Haidian District, Beijing, China; Wei, Z., Department of Civil Engineering, Tsinghua University, Haidian District, Beijing, China","Construction cost estimation, which is normally labor-intensive and error-prone, is one of the most important works concerned by multiparticipants during a project's life cycle. However, the proficiency of estimators on specifications for construction cost estimation greatly affects the efficiency and accuracy of cost estimation. By formalizing specifications for construction cost estimation, such specifications can be better implemented in computer programs so that the working efficiency and accuracy of estimation can be greatly improved. This study aims to establish an effective approach to formalize such specifications by using ontology. First, two typical specifications for construction cost estimation are analyzed and relevant models are established. Then, an ontology-based representation of two specifications is established based on the models. Next, a prototype tool for facilitating the establishment, modification, and extension of the ontology-based representation for estimators is presented and the use cases of the tool are illustrated. Finally, the applicability of the approach is discussed. It is concluded that the formalized representation can be used to classify the building components into items for bill of quantities and quota items automatically by computer programs to accelerate cost estimation. å©2015 Computer-Aided Civil and Infrastructure Engineering.",,Article,Scopus,2-s2.0-84955191085
"Lyu C., Lu Y., Ji D., Chen B.",Deep learning for textual entailment recognition,2016,"Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI",2016-January,,7372131,154,161,no,,,1,10.1109/ICTAI.2015.35,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963542618&doi=10.1109%2fICTAI.2015.35&partnerID=40&md5=e8e148cecacf79b654566ea03a0cc61a,"Computer School, Wuhan University, Wuhan, China","Lyu, C., Computer School, Wuhan University, Wuhan, China; Lu, Y., Computer School, Wuhan University, Wuhan, China; Ji, D., Computer School, Wuhan University, Wuhan, China; Chen, B., Computer School, Wuhan University, Wuhan, China","In this paper we propose a novel two-step procedure to recognize textual entailment. Firstly, we build a joint Restricted Boltzmann Machines (RBM) layer to learn the joint representation of the text-hypothesis pairs. Then the reconstruction error is calculated by comparing the original representation with reconstructed representation derived from the joint layer for each pair to recognize textual entailment. The joint RBM training data is automatically generated from a large news corpus. Experiment results show the contribution of the idea to the performance on textual entailment. å© 2015 IEEE.",Deep learning; Joint representation; Joint Restricted Boltzmann Machines; Textual entailment,Conference Paper,Scopus,2-s2.0-84963542618
"Lv Z., Zhao J., Liu Y., Wang W.",Data imputation for gas flow data in steel industry based on non-equal-length granules correlation coefficient,2016,Information Sciences,367-368,,,311,323,no,,,,10.1016/j.ins.2016.05.046,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975231515&doi=10.1016%2fj.ins.2016.05.046&partnerID=40&md5=b1230650cce9d720cc04f843733418fa,"School of Control Science and Engineering, Dalian University of Technology, China","Lv, Z., School of Control Science and Engineering, Dalian University of Technology, China; Zhao, J., School of Control Science and Engineering, Dalian University of Technology, China; Liu, Y., School of Control Science and Engineering, Dalian University of Technology, China; Wang, W., School of Control Science and Engineering, Dalian University of Technology, China","In the field of data-driven based modeling and optimization, the completeness and the accuracy of data samples are the foundations for further research tasks. Since the byproduct gas system of steel industry is rather complicated and its data-acquisition process might be frequently affected by the unexpected operational factors, the data-missing phenomenon usually occurs, which might lead to the failure of model establishment or inaccurate information discovery. In this study, a data imputation method based on the manufacturing characteristics is proposed for resolving the data-missing problem in steel industry. A novel correlation analysis, named by non-equal-length granules correlation coefficient (NGCC), is reported, and the corresponding model based on Estimation of Distribution Algorithm (EDA) is established to study the correlation of the similar procedures. To verify the performance of the proposed method, this study considers three typical features of the gas flow data with different missing ratios. The experiment results indicate that it is greatly effective for the missing data imputation of byproduct gas, and exhibits better performance on the accuracy compared to the other methods. å© 2016 Elsevier Inc. All rights reserved.",Byproduct gas of steel industry; Data imputation; Estimation of distribution algorithm; Non-equal-length granules correlation coefficient,Article,Scopus,2-s2.0-84975231515
"Lv X., El-Gohary N.M.",Enhanced context-based document relevance assessment and ranking for improved information retrieval to support environmental decision making,2016,Advanced Engineering Informatics,30,4,,737,750,no,,,,10.1016/j.aei.2016.08.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994659845&doi=10.1016%2fj.aei.2016.08.004&partnerID=40&md5=c5e76d2bb47f490a3dff917ebde47e12,"Department of Civil and Environmental Engineering, University of Illinois at Urbana-Champaign, 205 N. Mathews Ave., Urbana, IL, United States","Lv, X., Department of Civil and Environmental Engineering, University of Illinois at Urbana-Champaign, 205 N. Mathews Ave., Urbana, IL, United States; El-Gohary, N.M., Department of Civil and Environmental Engineering, University of Illinois at Urbana-Champaign, 205 N. Mathews Ave., Urbana, IL, United States","There is a need for enhanced context-based document relevance assessment and ranking to facilitate the retrieval of more relevant information for supporting environmental decision making. This paper proposes a new context-based relevance assessment method, which allows for enhanced context representation and context-based document relevance recognition through: (1) a context-aware and deep semantic concept indexing approach, and (2) a deep and semantically-sensitive relevance estimation approach. The proposed relevance assessment method was integrated into two widely-used document ranking models [vector space model (VSM) and statistical language model (SLM)], resulting in two improved ranking methods: (1) a context-enhanced VSM-based method, and (2) a context-enhanced SLM-based method. The two context-enhanced document ranking methods were evaluated in retrieving webpages that are relevant to transportation project environmental review. The two context-enhanced methods were compared with each other and with their provenance methods (i.e., original VSM and SLM) in terms of mean precision (MP) and mean average precision (MAP). The context-enhanced VSM-based method outperformed the context-enhanced SLM-based method on every metric. It achieved 48% MAP, 79% MP at the top 10 retrieved documents, and over 65% MP at the top 50 retrieved documents, on the testing data. It also showed significant improvement over the state-of-the-art keyword-based VSM method. å© 2016 Elsevier Ltd",Context-based relevance assessment; Context-enhanced document ranking; Information retrieval; Project environmental review; Statistical language model; Vector space model,Article,Scopus,2-s2.0-84994659845
"Lunghi P., Botarelli M., Brizioli D.",An innovative cost engineering model for ETO companies,2007,"2007 IEEE International Technology Management Conference, ICE 2007",,,7458698,,,no,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978151303&partnerID=40&md5=0a29fecd0e6d78f37dd26d81553d027a,"University of Perugia, Via Duranti 67, Perugia, Italy","Lunghi, P., University of Perugia, Via Duranti 67, Perugia, Italy; Botarelli, M., University of Perugia, Via Duranti 67, Perugia, Italy; Brizioli, D., University of Perugia, Via Duranti 67, Perugia, Italy","The new product cost estimation activity is considered by enterprises as a very critical issue. In our work we are going to analyse the situation of Engineering To Order (ETO) firms, i.e. of those firms in which design and production phases are pulled by a specific customer order. By the term designing phase we include also light or heacy customization of a specific product. One of the problems that those firms have to face during their activity is the correct economic evaluation of a job order. In this work we present an innovative cost engineering model to estimate customized product cost; based on the division of costs in two categories, 'variant' (the dependent variable) and 'invariant' (the independent variable), and the definition of a relationship between them. The paper is supported by a case study describing the implementation of the proposed model in an ETO company. å© 2007 Nottingham University.",Analogy Method; Cost engineering; Cost estimating; Detailed method; Engineering To Order; Parametric method,Conference Paper,Scopus,2-s2.0-84978151303
Lubas D.G.,System and software cost correlation to reliability,2016,Proceedings - Annual Reliability and Maintainability Symposium,2016-April,,7448052,,,no,,,,10.1109/RAMS.2016.7448052,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968784348&doi=10.1109%2fRAMS.2016.7448052&partnerID=40&md5=61cdf613ef76b4bea16597baa46d4d74,"Naval Undersea Warfare System, 1176 Howell Street, Newport, RI, United States","Lubas, D.G., Naval Undersea Warfare System, 1176 Howell Street, Newport, RI, United States","While technology evolves, challenges in system requirements, design and reliability modeling continue to keep pace with future underlying technology innovations and domain changes. Complex-software intense web-based information systems, with personal banking, retail marketing and retail sales evolved to online transactions and so did the need for secure information assurance to protect from illegal hacking. New electronics, computer technologies and software and system methodologies evolve to keep pace with military, aerospace, medical, human, manufacturing and cyber security requirements, system reliability evolve. In addition, shortened product development life-cycle in industry and the Department of Defense (DoD) went from three years to 18-months incorporating system reliability analysis and modeling for hardware, electronics, and software into the requirements, design, development, test and evaluation, and operations and support (O&S) sustainment phases. The challenge of technology evolution has increased the demand for systems to be built to meet reliability requirements and designed to meet the human need at least possible cost. Cost Estimation Model II (COCOMOII) estimates software engineering effort and cost of a software system based on source lines of code, engineering team capabilities, reliability and process maturity [1]. Cost Estimation System Engineering Model (COSYSMO) estimates system engineering effort and cost of a system based on system requirement counts, engineering team capabilities, reliability and process maturity.[1] Reliability and cost modeling for a continuous complex system using common reliability models, COCOMOII and COSYSMO, demonstrated a correlation between reliability with cost and process maturity with cost during developmental phases of the product lifecycle. The empirical reliability and developmental cost data analysis using COCOMOII and COSYSMO is recognized with limitations since the cost correlation, process maturity, reliability impact was built on only one system during developmental phases. Despite limited samples, these preliminary results indicate that it is possible to estimate developmental cost increase as a function of reliability improvements and developmental cost decrease as a function of process maturity. Additional research and analysis should be done to replicate the analysis. Since these COCOMOII and COSYSMO do not estimate effort and cost during the O&S phase, an O&S cost model was provided to demonstrate the correlation of system and software cost to reliability for an evaluation of total ownership costs (TOC) and return on investment. To model total TOC which includes O&S costs, the Army Logistics Support Activity Cost Analysis Strategy Assessment (CASA) model was run on Predator Unmanned Aerial Vehicle (UAV), Global Hawk UAV, MH-60S Fleet Combat Support Helicopter, CH-47F Improved Cargo Helicopter, and Force XXI Battle Command, and Brigade-And-Below (FBCB2) systems [2]. The CASA model estimates TOC for the entire life-cycle of a system, implemented by Navy, Army, Air Force, NASA, U.S. Coast Guard, and Marines. The CASA model with the five systems established a relationship between achieved reliability improvement and reduction in O&S cost. The empirical reliability, investment, and O&S data analysis is recognized with limitations since the reliability investment and reliability impact was built on only five systems. Additional research and analysis should be done to replicate the analysis. Despite limited samples, these preliminary results indicate that it is possible to estimate O&S cost reductions as a function of reliability investment and reliability improvements. This is significant since O&S costs account for over two-Thirds of the TOC [3]. Further research can be done to evaluate process maturity impact on the entire acquisition life-cycle TOC. It is also recognized that the applicability of the results may be limited to the specific systems under evaluation, although this data are typical of those for mission critical systems comprised of hardware and software. Further research is necessary to determine whether the results are applicable to systems in general or better specified to particular types of systems. Whether the results would be applicable to other DoD or commercial systems could only be determined by additional research. å© 2016 IEEE.",Cost Correlation to Reliability; O&S Costs; Software Reliability; System Reliability,Conference Paper,Scopus,2-s2.0-84968784348
"Low W.Z., vanden Broucke S.K.L.M., Wynn M.T., ter Hofstede A.H.M., De Weerdt J., van der Aalst W.M.P.",Revising history for cost-informed process improvement,2016,Computing,98,9,,895,921,no,,,,10.1007/s00607-015-0478-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949983602&doi=10.1007%2fs00607-015-0478-1&partnerID=40&md5=d3370b525d6bd76f192c631d4cd81a56,"Queensland University of Technology (QUT), Brisbane, Australia; Department of Decision Sciences and Information Management, KU Leuven, Leuven, Belgium; Department of Mathematics and Computer Science, Technische Universiteit Eindhoven (TU/e), Eindhoven, Netherlands","Low, W.Z., Queensland University of Technology (QUT), Brisbane, Australia; vanden Broucke, S.K.L.M., Department of Decision Sciences and Information Management, KU Leuven, Leuven, Belgium; Wynn, M.T., Queensland University of Technology (QUT), Brisbane, Australia; ter Hofstede, A.H.M., Queensland University of Technology (QUT), Brisbane, Australia, Department of Mathematics and Computer Science, Technische Universiteit Eindhoven (TU/e), Eindhoven, Netherlands; De Weerdt, J., Department of Decision Sciences and Information Management, KU Leuven, Leuven, Belgium; van der Aalst, W.M.P., Queensland University of Technology (QUT), Brisbane, Australia, Department of Mathematics and Computer Science, Technische Universiteit Eindhoven (TU/e), Eindhoven, Netherlands","Organisations are constantly seeking new ways to improve operational efficiencies. This study investigates a novel way to identify potential efficiency gains in business operations by observing how they were carried out in the past and then exploring better ways of executing them by taking into account trade-offs between time, cost and resource utilisation. This paper demonstrates how these trade-offs can be incorporated in the assessment of alternative process execution scenarios by making use of a cost environment. A number of optimisation techniques are proposed to explore and assess alternative execution scenarios. The objective function is represented by a cost structure that captures different process dimensions. An experimental evaluation is conducted to analyse the performance and scalability of the optimisation techniques: integer linear programming (ILP), hill climbing, tabu search, and our earlier proposed hybrid genetic algorithm approach. The findings demonstrate that the hybrid genetic algorithm is scalable and performs better compared to other techniques. Moreover, we argue that the use of ILP is unrealistic in this setup and cannot handle complex cost functions such as the ones we propose. Finally, we show how cost-related insights can be gained from improved execution scenarios and how these can be utilised to put forward recommendations for reducing process-related cost and overhead within organisations. å© 2015, Springer-Verlag Wien.",Business process analysis; Business process improvement; Cost-informed; Genetic algorithm; Optimisation; Process mining,Article,Scopus,2-s2.0-84949983602
"Lopez-Garcia P., Klemen M., Liqat U., Hermenegildo M.V.",A general framework for static profiling of parametric resource usage,2016,Theory and Practice of Logic Programming,16,5-Jun,,849,865,no,,,,10.1017/S1471068416000442,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991442140&doi=10.1017%2fS1471068416000442&partnerID=40&md5=f15bc3b99a9419431956e555328a5e6e,"IMDEA Software Institute, Spain; Spanish Council for Scientific Research (CSIC), Spain; Technical University of Madrid (UPM), Spain","Lopez-Garcia, P., IMDEA Software Institute, Spain, Spanish Council for Scientific Research (CSIC), Spain; Klemen, M., IMDEA Software Institute, Spain; Liqat, U., IMDEA Software Institute, Spain; Hermenegildo, M.V., IMDEA Software Institute, Spain, Technical University of Madrid (UPM), Spain","For some applications, standard resource analyses do not provide the information required. Such analyses estimate the total resource usage of a program (without executing it) as functions on input data sizes. However, some applications require knowing how such total resource usage is distributed over selected parts of a program. We propose a novel, general, and flexible framework for setting up cost equations/relations which can be instantiated for performing a wide range of resource usage analyses, including both static profiling and the inference of the standard notion of cost. We extend and generalize standard resource analysis techniques, so that the relations generated include additional Boolean control variables for switching on or off different terms in the relations, as required by the desired resource usage profile. We also instantiate our framework to perform static profiling of accumulated cost (also parameterized by input data sizes). Such information is much more useful to the software developer than the standard notion of cost: it identifies the parts of the program that have the greatest impact on the total program cost, and which therefore should be optimized first. We also report on an implementation of our framework within the CiaoPP system, and its instantiation for accumulated cost, and provide some experimental results. In addition to generality, our new method brings important advantages over our previous approach based on a program transformation, including support for non-deterministic programs, better and easier integration in the compiler, and higher efficiency. å© 2016 Cambridge University Press.",Complexity Analysis; Resource Usage Analysis; Static Analysis; Static Profiling,Conference Paper,Scopus,2-s2.0-84991442140
"Liu Z., Yu X., Gao Y., Chen S., Ji X., Wang D.",CU partition mode decision for HEVC hardwired intra encoder using convolution neural network,2016,IEEE Transactions on Image Processing,25,11,7547305,5088,5103,no,,,,10.1109/TIP.2016.2601264,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991736323&doi=10.1109%2fTIP.2016.2601264&partnerID=40&md5=88d252f4832e0a501ebfb58b122e3815,"Tsinghua National Laboratory for Information Science and Technology, Research Institute of Information Technology, Tsinghua University, Beijing, China; Institute of Microelectronics, Tsinghua University, Beijing, China; Department of Computer Science, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Huawei Technologies Company, Ltd., Shenzhen, China","Liu, Z., Tsinghua National Laboratory for Information Science and Technology, Research Institute of Information Technology, Tsinghua University, Beijing, China; Yu, X., Institute of Microelectronics, Tsinghua University, Beijing, China; Gao, Y., Department of Computer Science, Tsinghua University, Beijing, China; Chen, S., Huawei Technologies Company, Ltd., Shenzhen, China; Ji, X., Department of Automation, Tsinghua University, Beijing, China; Wang, D., Tsinghua National Laboratory for Information Science and Technology, Research Institute of Information Technology, Tsinghua University, Beijing, China","The intensive computation of High Efficiency Video Coding (HEVC) engenders challenges for the hardwired encoder in terms of the hardware overhead and the power dissipation. On the other hand, the constrains in hardwired encoder design seriously degrade the efficiency of software oriented fast coding unit (CU) partition mode decision algorithms. A fast algorithm is attributed as VLSI friendly, when it possesses the following properties. First, the maximum complexity of encoding a coding tree unit (CTU) could be reduced. Second, the parallelism of the hardwired encoder should not be deteriorated. Third, the process engine of the fast algorithm must be of low hardware- and power-overhead. In this paper, we devise the convolution neural network based fast algorithm to decrease no less than two CU partition modes in each CTU for full rate-distortion optimization (RDO) processing, thereby reducing the encoder's hardware complexity. As our algorithm does not depend on the correlations among CU depths or spatially nearby CUs, it is friendly to the parallel processing and does not deteriorate the rhythm of RDO pipelining. Experiments illustrated that, an averaged 61.1% intraencoding time was saved, whereas the Bjontegaard-Delta bit-rate augment is 2.67%. Capitalizing on the optimal arithmetic representation, we developed the high-speed [714 MHz in the worst conditions (125 å¡C, 0.9 V)] and low-cost (42.5k gate) accelerator for our fast algorithm by using TSMC 65-nm CMOS technology. One accelerator could support HD1080p at 55 frames/s real-time encoding. The corresponding power dissipation was 16.2 mW at 714 MHz. Finally, our accelerator is provided with good scalability. Four accelerators fulfill the throughput requirements of UltraHD-4K at 55 frames/s. å© 1992-2012 IEEE.",CNN; fast CU/PU mode decision; HEVC; intra encoding; VLSI,Article,Scopus,2-s2.0-84991736323
"Liu X., Li Z., Jiang S.",Ontology-based representation and reasoning in building construction cost estimation in China,2016,Future Internet,8,3,39,,,no,,,,10.3390/fi8030039,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006190085&doi=10.3390%2ffi8030039&partnerID=40&md5=2652bca6e1d89ae793c9449ba162a813,"Department of Construction Management, Dalian University of Technology, Dalian, China","Liu, X., Department of Construction Management, Dalian University of Technology, Dalian, China; Li, Z., Department of Construction Management, Dalian University of Technology, Dalian, China; Jiang, S., Department of Construction Management, Dalian University of Technology, Dalian, China","Cost estimation is one of the most critical tasks for building construction project management. The existing building construction cost estimation methods of many countries, including China, require information from several sources, including material, labor, and equipment, and tend to be manual, time-consuming, and error-prone. To solve these problems, a building construction cost estimation model based on ontology representation and reasoning is established, which includes three major components, i.e., concept model ontology, work item ontology, and construction condition ontology. Using this model, the cost estimation information is modeled into OWL axioms and SWRL rules that leverage the semantically rich ontology representation to reason about cost estimation. Based on OWL axioms and SWRL rules, the cost estimation information can be translated into a set of concept models, work items, and construction conditions associated with the specific construction conditions. The proposed method is demonstrated in ProtÌ©gÌ© 3.4.8 through case studies based on the Measurement Specifications of Building Construction and Decoration Engineering taken from GB 50500-2013 (the Chinese national mandatory specifications). Finally, this research discusses the limitations of the proposed method and future research directions. The proposed method can help a building construction cost estimator extract information more easily and quickly. å© 2016 by the authors.",Cost estimation; Ontology; Reasoning; Semantic relationships; Specification for cost estimation,Article,Scopus,2-s2.0-85006190085
"Liu W., Lim C.-C., Shi P., Xu S.",Sampled-data fuzzy control for a class of nonlinear systems with missing data and disturbances,2017,Fuzzy Sets and Systems,306,,,63,86,no,,,,10.1016/j.fss.2016.03.011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962467593&doi=10.1016%2fj.fss.2016.03.011&partnerID=40&md5=0fb428a5a860950ac1c1611df7800a0d,"School of Automation, Nanjing University of Science and Technology, Nanjing, China; School of Electrical and Electronic Engineering, The University of Adelaide, Adelaide, S.A., Australia; College of Engineering and Science, Victoria University, Melbourne, VIC, Australia","Liu, W., School of Automation, Nanjing University of Science and Technology, Nanjing, China, School of Electrical and Electronic Engineering, The University of Adelaide, Adelaide, S.A., Australia; Lim, C.-C., School of Electrical and Electronic Engineering, The University of Adelaide, Adelaide, S.A., Australia; Shi, P., School of Electrical and Electronic Engineering, The University of Adelaide, Adelaide, S.A., Australia, College of Engineering and Science, Victoria University, Melbourne, VIC, Australia; Xu, S., School of Automation, Nanjing University of Science and Technology, Nanjing, China",This paper considers the sampled-data fuzzy control of nonlinear systems in strict feedback form with disturbances and random missing input data. We propose a novel method in which a state observer and a disturbance observer are combined to construct a sampled-data fuzzy output feedback controller. The stochastic variables with a Bernoulli distributed sequence are used to model missing input data. Fuzzy logic systems are applied to approximate nonlinearities without requiring prior knowledge. The relation between observer gain and sampling period is established. The output feedback controller designed guarantees that the nonlinear system is globally stable. A simulation example of four degrees of freedom robotic arm is used to demonstrate the effectiveness and applicability of the proposed control scheme. å© 2016 Elsevier B.V.,Disturbance observer; Fuzzy logic systems; Missing input data; Sampled-data control; State observer,Article,Scopus,2-s2.0-84962467593
"Liu J., Chen Z., Wang J.",Solving cost prediction based search in symbolic execution,2016,Jisuanji Yanjiu yu Fazhan/Computer Research and Development,53,5,,1086,1094,no,,,,10.7544/issn1000-1239.2016.20148330,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971290249&doi=10.7544%2fissn1000-1239.2016.20148330&partnerID=40&md5=8cf4ae171b8835a7510ad905e206efb7,"College of Computer, National University of Defense Technology, Changsha, China; Science and Technology on Parallel and Distributed Processing Laboratory, College of Computer, National University of Defense Technology, Changsha, China; 95835 PLA Troops, Bayingolin, Xinjiang, China","Liu, J., College of Computer, National University of Defense Technology, Changsha, China, 95835 PLA Troops, Bayingolin, Xinjiang, China; Chen, Z., College of Computer, National University of Defense Technology, Changsha, China; Wang, J., College of Computer, National University of Defense Technology, Changsha, China, Science and Technology on Parallel and Distributed Processing Laboratory, College of Computer, National University of Defense Technology, Changsha, China","In symbolic execution, constraint solving needs a large proportion of execution time. The solving time of a constraint differs a lot with respect to the complexity, which happens a lot when analyzing the programs with complex numerical calculations. Solving more constraints within a specified time contributes to covering more statements and exploring more paths. Considering this feature, we propose a solving cost prediction based search strategy for symbolic execution. Based on the experimental data of constraint solving, we conclude an empirical formula to evaluate the complexity of constraints, and predict the solving cost of a constraint combined with historical solving cost data. The formula is used in our strategy to explore the paths with a lower solving cost with a higher priority. We have implemented our strategy in KLEE, a state-of-art symbolic executor for C, and carried out the experiments on the randomly selected 12 modules in GNU Scientific Library (GSL). The experimental results indicate that: in a same period, compared with the existing strategy, our strategy can explore averagely 24.34% more paths, without sacrificing the statement coverage; and our strategy can find more bugs. In addition, the time of using our strategy for finding same bugs decreases 44.43% in average. å© 2016, Science Press. All right reserved.",Bug finding; Constraint solving; Statement covering; Symbolic execution; Weighted random search,Article,Scopus,2-s2.0-84971290249
Liu J.,Guidelines for AM part consolidation,2016,Virtual and Physical Prototyping,11,2,,133,141,no,,,1,10.1080/17452759.2016.1175154,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964388860&doi=10.1080%2f17452759.2016.1175154&partnerID=40&md5=5502d28179040df047bf2f4317dc1f65,"Department of Mechanical Engineering, University of Alberta, Edmonton, Canada","Liu, J., Department of Mechanical Engineering, University of Alberta, Edmonton, Canada","This paper makes a comparative study about the structural performances of the consolidated AM (additive manufacturing) parts and the multi-piece assemblies. Generally, it is recognised that AM part consolidation can improve the structural performance compared to the traditional multi-piece assembly. However, this may not be true given that the AM materials are directionally weakened, especially in the build direction (BD). Hence, the comparative study performed in this work will provide some conclusions and at the same time, these conclusions could be valuable guidelines for designing AM part consolidation. For implementation details, the level set topology optimisation method will be applied to optimising the multi-piece assembly with rigid joints and the AM part with weakened material properties; and the same design domain will be shared in order to make a fair comparison. A set of different joint positions and material weakening levels will be studied in order to get comprehensive conclusions. å© 2016 Informa UK Limited, trading as Taylor & Francis Group.",Design for additive manufacturing; part consolidation; topology optimisation,Article,Scopus,2-s2.0-84964388860
"Liu H., Lu M., Al-Hussein M.",Ontology-based semantic approach for construction-oriented quantity take-off from BIM models in the light-frame building industry,2016,Advanced Engineering Informatics,30,2,,190,207,no,,,,10.1016/j.aei.2016.03.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962009851&doi=10.1016%2fj.aei.2016.03.001&partnerID=40&md5=75eb56c8d72375af583d0db395e6f17e,"Department of Civil and Environmental Engineering, University of Alberta, Edmonton, AB, Canada","Liu, H., Department of Civil and Environmental Engineering, University of Alberta, Edmonton, AB, Canada; Lu, M., Department of Civil and Environmental Engineering, University of Alberta, Edmonton, AB, Canada; Al-Hussein, M., Department of Civil and Environmental Engineering, University of Alberta, Edmonton, AB, Canada","In building information modeling (BIM), the model is a digital representation of physical and functional characteristics of a facility and contains enriched product information pertaining to the facility. This information is generally embedded into the BIM model as properties for parametric building objects, and is exchangeable among project stakeholders and BIM design programs - a key feature of BIM for enhancing communication and work efficiency. However, BIM itself is a purpose-built, product-centric information database and lacks domain semantics such that extracting construction-oriented quantity take-off information for the purpose of construction workface planning still remains a challenge. Moreover, some information crucial to construction practitioners, such as the topological relationships among building objects, remains implicit in the BIM design model. This restricts information extraction from the BIM model for downstream analyses in construction. To address identified limitations, this study proposes an ontology-based semantic approach to extracting construction-oriented quantity take-off information from a BIM design model. This approach allows users to semantically query the BIM design model using a domain vocabulary, capitalizing on building product ontology formalized from construction perspectives. As such, quantity take-off information relevant to construction practitioners can be readily extracted and visualized in 3D in order to serve application needs in the construction field. A prototype application is implemented in Autodesk Revit to demonstrate the effectiveness of the proposed new approach in the domain of light-frame building construction. å© 2016 Elsevier Ltd. All rights reserved.",Building information modeling; Domain vocabulary; Ontology; Semantic quantity take-off; SPARQL,Article,Scopus,2-s2.0-84962009851
Lin T.,"Cost based, integrated design optimization using a parametric CAD model",2016,Computer-Aided Design and Applications,,,,1,13,no,,,,10.1080/16864360.2016.1240455,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991434539&doi=10.1080%2f16864360.2016.1240455&partnerID=40&md5=b6a7e033cc1c40076dfe7f67a78be8a0,"School of Engineering & Technology (SET), Asian Institute of Technology, Thailand","Lin, T., School of Engineering & Technology (SET), Asian Institute of Technology, Thailand","This paper present a novel approach for developing a decision support tool for designers based on manufacturing cost. The approach focuses on exploiting the advantages offered by integrating parametric CAD, manufacturing processing time based cost estimation and optimization technique within an integrated framework. The methodology is then applied in optimizing the geometry for minimum manufacturing cost of rotor blade. å© 2016 CAD Solutions, LLC",CAD; Design for Manufacturing (DFM); Integrated Design; Optimization,Article in Press,Scopus,2-s2.0-84991434539
"Li W., Hayes J.H., Antoniol G., GuÌ©hÌ©neuc Y.-G., Adams B.",Error leakage and wasted time: sensitivity and effort analysis of a requirements consistency checking process,2016,Journal of Software: Evolution and Process,28,12,,1061,1080,no,,,,10.1002/smr.1819,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991821728&doi=10.1002%2fsmr.1819&partnerID=40&md5=c006359c6b5315d36544852da96d5cd4,"University of Kentucky, Lexington, KY, United States; Ecole Polytechnique de Montreal, Montreal, Canada","Li, W., University of Kentucky, Lexington, KY, United States; Hayes, J.H., University of Kentucky, Lexington, KY, United States; Antoniol, G., Ecole Polytechnique de Montreal, Montreal, Canada; GuÌ©hÌ©neuc, Y.-G., Ecole Polytechnique de Montreal, Montreal, Canada; Adams, B., Ecole Polytechnique de Montreal, Montreal, Canada","Several techniques are used by requirements engineering practitioners to address difficult problems such as specifying precise requirements while using inherently ambiguous natural language text and ensuring the consistency of requirements. Often, these problems are addressed by building processes/tools that combine multiple techniques where the output from 1 technique becomes the input to the next. While powerful, these techniques are not without problems. Inherent errors in each technique may leak into the subsequent step of the process. We model and study 1 such process, for checking the consistency of temporal requirements, and assess error leakage and wasted time. We perform an analysis of the input factors of our model to determine the effect that sources of uncertainty may have on the final accuracy of the consistency checking process. Convinced that error leakage exists and negatively impacts the results of the overall consistency checking process, we perform a second simulation to assess its impact on the analysts' efforts to check requirements consistency. We show that analyst's effort varies depending on the precision and recall of the subprocesses and that the number and capability of analysts affect their effort. We share insights gained and discuss applicability to other processes built of piped techniques. Copyright å© 2016 John Wiley & Sons, Ltd.",classification; consistency checking; error leakage; error propagation; genetic algorithms; information retrieval; machine learning; natural language processing; process model; requirements engineering; search-based software engineering; semantic role labeling; sensitivity analysis; work flow process,Conference Paper,Scopus,2-s2.0-84991821728
"Li F., Wu B., Yi K., Zhao Z.",Wander join: Online aggregation for joins,2016,Proceedings of the ACM SIGMOD International Conference on Management of Data,26-Jun-16,,,2121,2124,no,,,,10.1145/2882903.2899413,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979697403&doi=10.1145%2f2882903.2899413&partnerID=40&md5=88bb1e11a829a58cf101a34d6049a263,"University of Utah, United States; Hong Kong University of Science and Technology, Hong Kong; Shanghai Jiao Tong University, China","Li, F., University of Utah, United States; Wu, B., Hong Kong University of Science and Technology, Hong Kong; Yi, K., Hong Kong University of Science and Technology, Hong Kong; Zhao, Z., Shanghai Jiao Tong University, China","Joins are expensive, and online aggregation over joins was proposed to mitigate the cost, which offers a nice and flexible tradeoff between query effciency and accuracy in a continuous, online fashion. However, the state-of-the-art approach, in both internal and external memory, is based on ripple join, which is still very expensive and may also need very restrictive assumptions (e.g., tuples in a table are stored in random order). We introduce a new approach, wander join, to the online aggregation problem by performing random walks over the underlying join graph. We have also implemented and tested wander join in the latest PostgreSQL. å© 2016 ACM.",,Conference Paper,Scopus,2-s2.0-84979697403
"Li F., Wu B., Yi K., Zhao Z.",Wander join: Online aggregation via random walks,2016,Proceedings of the ACM SIGMOD International Conference on Management of Data,26-Jun-16,,,615,629,no,,,1,10.1145/2882903.2915235,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979696639&doi=10.1145%2f2882903.2915235&partnerID=40&md5=7ef03c6f5f9c5c2055b3d2f696969fb0,"University of Utah, Salt Lake City, United States; Hong Kong University of Science and Technology, Hong Kong, Hong Kong; Shanghai Jiao Tong University, Shanghai, China","Li, F., University of Utah, Salt Lake City, United States; Wu, B., Hong Kong University of Science and Technology, Hong Kong, Hong Kong; Yi, K., Hong Kong University of Science and Technology, Hong Kong, Hong Kong; Zhao, Z., Shanghai Jiao Tong University, Shanghai, China","Joins are expensive, and online aggregation over joins was proposed to mitigate the cost, which offers users a nice and flexible tradeoff between query efficiency and accuracy in a continuous, online fashion. However, the state-of-the-art approach, in both internal and external memory, is based on ripple join, which is still very expensive and even needs unrealistic assumptions (e.g., tuples in a table are stored in random order). This paper proposes a new approach, the wander join algorithm, to the online aggregation problem by performing random walks over the underlying join graph. We also design an optimizer that chooses the optimal plan for conducting the random walks without having to collect any statistics a priori. Compared with ripple join, wander join is particularly efficient for equality joins involving multiple tables, but also supports ëü-joins. Selection predicates and group-by clauses can be handled as well. Extensive experiments using the TPC-H benchmark have demonstrated the superior performance of wander join over ripple join. In particular, we have integrated and tested wander join in the latest version of PostgreSQL, demonstrating its practicality in a full-edged database system. å© 2016 ACM.",,Conference Paper,Scopus,2-s2.0-84979696639
"Leu J.-S., Hsieh W.-B., Yee Y.-S.",Implementing Billing as a Service by an IPDR Aggregator System,2016,Wireless Personal Communications,87,4,,1223,1240,no,,,,10.1007/s11277-015-3050-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939833067&doi=10.1007%2fs11277-015-3050-6&partnerID=40&md5=c810285d7f11f335e39a9eb3e75672a1,"Department of Electronic and Computer Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan","Leu, J.-S., Department of Electronic and Computer Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan; Hsieh, W.-B., Department of Electronic and Computer Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan; Yee, Y.-S., Department of Electronic and Computer Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan","In the past decade, mobile Internet applications provided by cellular operators have been shifted from an emphasis on one-way content delivery to attention to two-way multimedia interaction. IP-based multimedia application development is booming in the recent years. Without systematical integration among these business support systems, subscribers may receive several different bills from different service providers periodically so as to makes them bedazzled. Meanwhile, cloud computing typically involves provisioning of dynamically scalable resources to provide a powerful computation. Motivated by the aforementioned facts, we propose an Internet Protocol Detail Record based architecture, which can combine different service bills and utilize cloud computing to calculate collectively aggregated bill, to establish a Billing as a Service for cellular operators. In order to validate the efficiency of clouding computing for the aggregated billing calculation, we also conduct performance evaluations for the billing computation over the traditional relational database and our proposed system. å© 2015, Springer Science+Business Media New York.",Aggregated bill; Billing as a Service; Cellular operators; Cloud computing; Internet Protocol Detail Record,Article,Scopus,2-s2.0-84939833067
"Lenhard J., Wirtz G.",Portability of executable service-oriented processes: metrics and validation,2016,Service Oriented Computing and Applications,10,4,,391,411,no,,,,10.1007/s11761-016-0195-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978083260&doi=10.1007%2fs11761-016-0195-4&partnerID=40&md5=7ea37d00ef3718627fb172b8630044ca,"Department of Mathematics and Computer Science, Karlstad University, Karlstad, Sweden; Distributed Systems Group, University of Bamberg, Bamberg, Germany","Lenhard, J., Department of Mathematics and Computer Science, Karlstad University, Karlstad, Sweden; Wirtz, G., Distributed Systems Group, University of Bamberg, Bamberg, Germany","A key promise of process languages based on open standards, such as the Web Services Business Process Execution Language, is the avoidance of vendor lock-in through the portability of processes among runtime environments. Despite the fact that today various runtimes claim to support this language, every runtime implements a different subset, thus hampering portability and locking in their users. It is our intention to improve this situation by enabling the measurement of the portability of executable service-oriented processes. This helps developers to assess their implementations and to decide if it is feasible to invest in the effort of porting a process to another runtime. In this paper, we define several software quality metrics that quantify the degree of portability of an executable, service-oriented process from different viewpoints. When integrated into a development environment, such metrics can help to improve the portability of the outcome. We validate the metrics theoretically with respect to measurement theory and construct validity using two validation frameworks. The validation is complemented with an empirical evaluation of the metrics using a large set of processes coming from several process libraries. å© 2016, Springer-Verlag London.",Metrics; Portability; Process; SOA; Software quality,Article,Scopus,2-s2.0-84978083260
Lemmens S.,Cost engineering techniques and their applicability for cost estimation of organic rankine cycle systems,2016,Energies,9,7,485,,,no,,,1,10.3390/en9070485,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978083369&doi=10.3390%2fen9070485&partnerID=40&md5=3dddf7ba5aed3676f4038ad571a1a96d,"Department Engineering Management, University of Antwerp, Prinsstraat 13, Antwerp, Belgium","Lemmens, S., Department Engineering Management, University of Antwerp, Prinsstraat 13, Antwerp, Belgium","The potential of organic Rankine cycle (ORC) systems is acknowledged by both considerable research and development efforts and an increasing number of applications. Most research aims at improving ORC systems through technical performance optimization of various cycle architectures and working fluids. The assessment and optimization of technical feasibility is at the core of ORC development. Nonetheless, economic feasibility is often decisive when it comes down to considering practical instalments, and therefore an increasing number of publications include an estimate of the costs of the designed ORC system. Various methods are used to estimate ORC costs but the resulting values are rarely discussed with respect to accuracy and validity. The aim of this paper is to provide insight into the methods used to estimate these costs and open the discussion about the interpretation of these results. A review of cost engineering practices shows there has been a long tradition of industrial cost estimation. Several techniques have been developed, but the expected accuracy range of the best techniques used in research varies between 10% and 30%. The quality of the estimates could be improved by establishing up-to-date correlations for the ORC industry in particular. Secondly, the rapidly growing ORC cost literature is briefly reviewed. A graph summarizing the estimated ORC investment costs displays a pattern of decreasing costs for increasing power output. Knowledge on the actual costs of real ORC modules and projects remains scarce. Finally, the investment costs of a known heat recovery ORC system are discussed and the methodologies and accuracies of several approaches are demonstrated using this case as benchmark. The best results are obtained with factorial estimation techniques such as the module costing technique, but the accuracies may diverge by up to +30%. Development of correlations and multiplication factors for ORC technology in particular is likely to improve the quality of the estimates. å© 2016 by the author; licensee MDPI.",Case study; Cost estimate; Heat recovery; Investment costs; Organic Rankine cycle (ORC),Article,Scopus,2-s2.0-84978083369
"Lee H.Y., Kim H.-J.",Cost Estimation of Hybrid System Models in Simulation Based Acquisition,2015,"Proceedings - 2015 4th International Conference on Advanced Information Technology and Sensor Application, AITS 2015",,,7396445,55,58,no,,,,10.1109/AITS.2015.21,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964579455&doi=10.1109%2fAITS.2015.21&partnerID=40&md5=9a69059d93ae7713eed40b6d362481ae,"Seoul Women's University, South Korea","Lee, H.Y., Seoul Women's University, South Korea; Kim, H.-J., Seoul Women's University, South Korea","While a defense system is being acquired using simulation-based acquisition (SBA), part of the system would be described as hybrid systems that have both discrete and continuous components. Also we need to quantify the benefits of SBA somehow in order to clearly determine them. In this paper we propose a method for cost estimation of hybrid system models that are specified based on a formalism for modeling and simulation. Existing estimation methods designed for discrete or continuous system models cannot be directly applied to the estimation since they leave the interaction between the components out of consideration. On the contrary the proposed method estimates the cost in a 'rough order of magnitude' manner through measuring discrete and continuous factors of a model as well as another factor of their correlation. å© 2015 IEEE.",acquisition modeling and simulation; cost-benefit analysis; discrete event system specification; hybrid systems; simulation-based acquisition,Conference Paper,Scopus,2-s2.0-84964579455
"Lee B., Kim M.",A CU-Level Rate and Distortion Estimation Scheme for RDO of Hardware-Friendly HEVC Encoders Using Low-Complexity Integer DCTs,2016,IEEE Transactions on Image Processing,25,8,7488286,3787,3800,no,,,,10.1109/TIP.2016.2579559,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979074110&doi=10.1109%2fTIP.2016.2579559&partnerID=40&md5=1bb5d62cb022019987c2959e9f32369b,"School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea","Lee, B., School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Kim, M., School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea","In this paper, a low complexity coding unit (CU)-level rate and distortion estimation scheme is proposed for High Efficiency Video Coding (HEVC) hardware-friendly implementation where a Walsh-Hadamard transform (WHT)-based low-complexity integer discrete cosine transform (DCT) is employed for distortion estimation. Since HEVC adopts quadtree structures of coding blocks with hierarchical coding depths, it becomes more difficult to estimate accurate rate and distortion values without actually performing transform, quantization, inverse transform, de-quantization, and entropy coding. Furthermore, DCT for rate-distortion optimization (RDO) is computationally high, because it requires a number of multiplication and addition operations for various transform block sizes of 4-, 8-, 16-, and 32-orders and requires recursive computations to decide the optimal depths of CU or transform unit. Therefore, full RDO-based encoding is highly complex, especially for low-power implementation of HEVC encoders. In this paper, a rate and distortion estimation scheme is proposed in CU levels based on a low-complexity integer DCT that can be computed in terms of WHT whose coefficients are produced in prediction stages. For rate and distortion estimation in CU levels, two orthogonal matrices of 4ÌÑ 4 and 8ÌÑ 8 , which are applied to WHT that are newly designed in a butterfly structure only with addition and shift operations. By applying the integer DCT based on the WHT and newly designed transforms in each CU block, the texture rate can precisely be estimated after quantization using the number of non-zero quantized coefficients and the distortion can also be precisely estimated in transform domain without de-quantization and inverse transform required. In addition, a non-texture rate estimation is proposed by using a pseudoentropy code to obtain accurate total rate estimates. The proposed rate and the distortion estimation scheme can effectively be used for HW-friendly implementation of HEVC encoders with 9.8% loss over HEVC full RDO, which much less than 20.3% and 30.2% loss of a conventional approach and Hadamard-only scheme, respectively. å© 1992-2012 IEEE.",distortion estimation; Hadamard transform; HEVC; integer DCT; rate estimation; Rate-distortion optimization,Article,Scopus,2-s2.0-84979074110
"Langhammer M., Shahbazian A., Medvidovic N., Reussner R.H.",Automated extraction of rich software models from limited system information,2016,"Proceedings - 2016 13th Working IEEE/IFIP Conference on Software Architecture, WICSA 2016",,,7516816,99,108,no,,,,10.1109/WICSA.2016.35,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983347731&doi=10.1109%2fWICSA.2016.35&partnerID=40&md5=b589aa480b6fdaec14fd1bfd20755863,"Karlsruhe Institute of Technology, Karlsruhe, Germany; Computer Science Department, University of Southern California, Los Angeles, CA, United States","Langhammer, M., Karlsruhe Institute of Technology, Karlsruhe, Germany; Shahbazian, A., Computer Science Department, University of Southern California, Los Angeles, CA, United States; Medvidovic, N., Computer Science Department, University of Southern California, Los Angeles, CA, United States; Reussner, R.H., Karlsruhe Institute of Technology, Karlsruhe, Germany","Reverse engineering a software system is challenged by the typically very limited information available about existing systems. Useful reverse engineering tasks include recovering a system's architectural, behavioral, and usage models, which can then be leveraged to answer important questions about a system. For example, using such models to analyze and predict a system's non-functional properties would help to efficiently assess the system's current state, planned adaptations, scalability issues, etc. Existing approaches typically only extract a system's static architecture, omitting the dynamic information that is needed for such analyses. The contribution of this paper is an automated technique that extracts a system's static architecture, behavior, and usage models from very limited, but readily available information: source code and test cases. These models can then be fed into known performance, reliability, and cost prediction techniques. We evaluated our approach for accuracy against systems with already established usage models, and observed that our approach finds the correct, but more detailed usage models. We also analyzed 14 open source software systems spanning over 2 million lines of code to evaluate the scalability of our approach. å© 2016 IEEE.",,Conference Paper,Scopus,2-s2.0-84983347731
"Laatikainen G., Mazhelis O., Tyrvainen P.",Cost benefits of flexible hybrid cloud storage: Mitigating volume variation with shorter acquisition cycle,2016,Journal of Systems and Software,122,,,180,201,no,,,,10.1016/j.jss.2016.09.008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988643235&doi=10.1016%2fj.jss.2016.09.008&partnerID=40&md5=1246c7a78be1cb00d97617855e3f2e99,"University of Jyvaskyla, Department of Computer Science and Information Systems, P.O. Box 35, FI-40014 University of Jyvaskyla, Finland; University of Jyvaskyla, Agora Center, P.O. Box 35, FI-40014 University of Jyvaskyla, Finland","Laatikainen, G., University of Jyvaskyla, Department of Computer Science and Information Systems, P.O. Box 35, FI-40014 University of Jyvaskyla, Finland; Mazhelis, O., University of Jyvaskyla, Department of Computer Science and Information Systems, P.O. Box 35, FI-40014 University of Jyvaskyla, Finland; Tyrvainen, P., University of Jyvaskyla, Agora Center, P.O. Box 35, FI-40014 University of Jyvaskyla, Finland","Hybrid cloud storage combines cost-effective but inflexible private storage along with flexible but premium-priced public cloud storage. As a form of concurrent sourcing, it offers flexibility and cost benefits to organizations by allowing them to operate at a cost-optimal scale and scope under demand volume uncertainty. However, the extant literature offers limited analytical insight into the effect that the non-stationarity (i.e., variability) and non-determinism (i.e., uncertainty) of the demand volume ‰ÛÒ in other words, the demand variation ‰ÛÒ have on the cost-efficient mix of internal and external sourcing. In this paper, we focus on the reassessment interval ‰ÛÒ that is, the interval at which the organization re-assesses its storage needs and acquires additional resources ‰ÛÒ, as well as on the impacts it has on the optimal mix of sourcing. We introduce an analytical cost model that captures the compound effect of the reassessment interval and volume variation on the cost-efficiency of hybrid cloud storage. The model is analytically investigated and empirically evaluated in simulation studies reflecting real-life scenarios. The results confirm that shortening the reassessment interval allows volume variability to be reduced, yielding a reduction of the overall costs. The overall costs are further reduced if, by shortening the interval, the demand uncertainty is also reduced. å© 2016 Elsevier Inc.",Acquisition interval; Concurrent sourcing; Hybrid cloud storage; Plural governance; Reassessment interval; Volume variation,Article,Scopus,2-s2.0-84988643235
"Kyriklidis C., Dounias G.",Evolutionary computation for resource leveling optimization in project management,2016,Integrated Computer-Aided Engineering,23,2,,173,184,no,,,,10.3233/ICA-150508,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960948959&doi=10.3233%2fICA-150508&partnerID=40&md5=256fb50e499976d4a06834cab0df5ca0,"Management and Decision Engineering Laboratory (MDE-Lab), Department of Financial and Management Engineering, University of the Aegean, 41 Kountouriotou Street, Chios, Greece","Kyriklidis, C., Management and Decision Engineering Laboratory (MDE-Lab), Department of Financial and Management Engineering, University of the Aegean, 41 Kountouriotou Street, Chios, Greece; Dounias, G., Management and Decision Engineering Laboratory (MDE-Lab), Department of Financial and Management Engineering, University of the Aegean, 41 Kountouriotou Street, Chios, Greece","This paper proposes an evolutionary computation based approach for solving resource leveling optimization problems in project management. In modern management engineering, problems of this kind refer to the optimal handling of available resources in a candidate project and have emerged, as the result of the even increasing needs of project managers in facing project complexity, controlling related budgeting and finances and managing the construction production line. Standard approaches, such as exhaustive or greedy search methodologies, fail to provide near-optimum solutions in feasible time even for small scale problems, whereas intelligent approaches manage to quickly reach high quality near-optimal solutions. In this work, a new genetic algorithm is proposed which investigates the start time of the non-critical activities of a project, in order to optimally allocate its resources. The innovation of the proposed approach is related to certain genetic operations applied like crossover for the improvement of the solution quality from generation to generation. The presentation and performance comparison of all multi-objective functions for resource leveling that are available in literature is another interesting part of this work. Detailed experiments with small and medium size benchmark problems taken from publicly available project data resources produce highly accurate resource profiles. As shown in the experimental results, the proposed methodology proves capable of coping even with large size project management problems without the need to divide the original problem to sub-problems due to complexity. å© 2016 - IOS Press and the author(s). All rights reserved.",Genetic algorithms; Project management; Resource levelling; Time constraint project scheduling,Conference Paper,Scopus,2-s2.0-84960948959
"Kuo R.J., Li P.S.",Taiwanese export trade forecasting using firefly algorithm based K-means algorithm and SVR with wavelet transform,2016,Computers and Industrial Engineering,99,,,153,161,no,,,,10.1016/j.cie.2016.07.012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979643086&doi=10.1016%2fj.cie.2016.07.012&partnerID=40&md5=0e35a39fc916f8d1f5b061bfdbdc3c13,"Department of Industrial Management, National Taiwan University of Science and Technology, No. 43, Section 4, Kee-Lung Road, Taipei, Taiwan; AU Optronics Corporation, No. 1 Jhong-Ke Road, Central Taiwan Science Park, Taichung, Taiwan","Kuo, R.J., Department of Industrial Management, National Taiwan University of Science and Technology, No. 43, Section 4, Kee-Lung Road, Taipei, Taiwan; Li, P.S., AU Optronics Corporation, No. 1 Jhong-Ke Road, Central Taiwan Science Park, Taichung, Taiwan","In order to develop a prediction system for export trade value, this study proposes a three-stage forecasting model which integrates wavelet transform, firefly algorithm-based K-means algorithms and firefly algorithm-based support vector regression (SVR). First, wavelet transform is utilized to reduce the noise in data preprocessing. Then, the firefly algorithm-based K-means algorithm is employed for cluster analysis. Finally, a forecasting model is built for each cluster individually. For evaluation, this study compares methods with and without clustering. In addition, both non-wavelet transform and wavelet transform for data preprocessing are investigated. The experimental results indicate that the forecasting algorithm with both wavelet transform and clustering has better performance. Besides, firefly algorithm-based SVR outperforms the other algorithms. å© 2016 Elsevier Ltd",Cluster analysis; Firefly algorithm; Forecasting; Support vector regression; Wavelet transform,Article,Scopus,2-s2.0-84979643086
"Kumar S., Nag R.",An approach for multimedia software size estimation,2016,"Proceedings of the 10th INDIACom; 2016 3rd International Conference on Computing for Sustainable Global Development, INDIACom 2016",,,7724480,1321,1326,no,Size,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997235977&partnerID=40&md5=e81eaa8583d3be82831cf068b4b29c51,"Sharda University, Greater Noida, India; BIT, Extention Centre, Mesra, Noida, India","Kumar, S., Sharda University, Greater Noida, India; Nag, R., BIT, Extention Centre, Mesra, Noida, India","Although, the Function Point Analysis has offered an idea to estimate the size of software in both planned and existing one [1], use of multimedia technology has provided a different direction for delivering instruction. The interested users in the era of Graphics User Interfaces, are being attracted, gets benefited and have new learning capabilities and this is reason the two-way multimedia training is a process, rather than being termed as a technology. Multimedia software Developer should use suitable methods for designing the package which will not only enhance its capabilities but will also be user friendly. Mixed media projects can be utilized to present data in combined form in a number of ways to energize routes hypermedia strategies with guideline. Great presentations can be made when they depend on psychological targets that create attention on the learning of themes at distinctive levels of appreciation. All media segments, for example, design, sound and video and so forth it adds to learning. While developing multimedia programs, developer should focus mainly on three point; firstly grabbing the user's attention; secondly it should ease the user to find and organize all necessary information and finally to integrate all information into the user's knowledge data bank. All elements like text, graphics icon and audio visual elements need to be utilized at the maximum to create visual appeal and organized in structured program, and thus for the purpose of systematic organization it is convenient to divide the screen into functional areas by the Developers. Class and sequence diagrams were utilized solely for the generation of FP counts [2]. A similar technique for creating automated counts using COSMIC FFP methods likewise depended on UML graphs [3]. But, neither of these strategies measures will be used for the estimation mixed media software the developers. å© 2016 IEEE.",MCAF-Multimedia Complexity Adjustment Factor; MFPA-Multimedia Function Point Analysis; MLI-Level of Impact for Multimedia; MUFP-Multimedia Unadjusted Function Points; MVAF-Value Adjustment Factor for Multimedia,Conference Paper,Scopus,2-s2.0-84997235977
"Kuljaroenwirat N., Seresangtakul P.",Model for predicting cost overrun in small and medium-sized public construction project,2016,"2016 13th International Joint Conference on Computer Science and Software Engineering, JCSSE 2016",,,7748867,,,no,,,,10.1109/JCSSE.2016.7748867,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006934362&doi=10.1109%2fJCSSE.2016.7748867&partnerID=40&md5=262eaaf9b77879d8e9e2eea5caea1708,"Department of Computer Science, Faculty of Science, Khon Kaen University, Khon Kaen, Thailand","Kuljaroenwirat, N., Department of Computer Science, Faculty of Science, Khon Kaen University, Khon Kaen, Thailand; Seresangtakul, P., Department of Computer Science, Faculty of Science, Khon Kaen University, Khon Kaen, Thailand","Cost overrun is the important issue in the construction industry that gets attention in several countries. This study focuses on affected construction company to examine the cost overrun factors of a public construction project, such as dredge and road. The prediction model for cost overrun in pre-tender phase was developed base on artificial neural networks (ANN), case-based reasoning (CBR), and hybrid ANN-CBR by using actual information collected from the construction contract and experts interviewing. The evaluation results during models development process shown that ANN was more accurate than CBR. On the other hand, the result of each model validated with 12 unseen projects shown that ANN and CBR with factor weights averaged from the best ANN model also have acceptable accuracy. However, CBR is more useful to support constructors' decision, since CBR itself could produce results with similarity score included as the probability percentage. å© 2016 IEEE.",Artificial Neural Network; Case-based reasoning; Cost overrun,Conference Paper,Scopus,2-s2.0-85006934362
"Krishnan S., Wang J., Franklin M.J., Goldberg K., Kraska T.",PrivateClean: Data cleaning and differential privacy,2016,Proceedings of the ACM SIGMOD International Conference on Management of Data,26-Jun-16,,,937,951,no,,,,10.1145/2882903.2915248,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979656533&doi=10.1145%2f2882903.2915248&partnerID=40&md5=37807da0fcfc77aadc358a1799992906,"UC Berkeley, United States; Simon Fraser University, Canada; Brown University, United States","Krishnan, S., UC Berkeley, United States; Wang, J., Simon Fraser University, Canada; Franklin, M.J., UC Berkeley, United States; Goldberg, K., UC Berkeley, United States; Kraska, T., Brown University, United States","Recent advances in differential privacy make it possible to guarantee user privacy while preserving the main characteristics of the data. However, most differential privacy mechanisms assume that the underlying dataset is clean. This paper explores the link between data cleaning and differential privacy in a framework we call PrivateClean. PrivateClean includes a technique for creating private datasets of numerical and discrete-valued attributes, a formalism for privacy-preserving data cleaning, and techniques for answering sum, count, and avg queries after cleaning. We show: (1) how the degree of privacy affects subsequent aggregate query accuracy, (2) how privacy potentially amplifies certain types of errors in a dataset, and (3) how this analysis can be used to tune the degree of privacy. The key insight is to maintain a bipartite graph relating dirty values to clean values and use this graph to estimate biases due to the interaction between cleaning and privacy. We validate these results on four datasets with a variety of well-studied cleaning techniques including using functional dependencies, outlier filtering, and resolving inconsistent attributes. å© 2016 ACM.",,Conference Paper,Scopus,2-s2.0-84979656533
"Krishna R., Menzies T., Fu W.",Too much automation? the bellwether effect and its implications for transfer learning,2016,ASE 2016 - Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering,,,,122,131,no,,,,10.1145/2970276.2970339,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989206867&doi=10.1145%2f2970276.2970339&partnerID=40&md5=959e16066254267048c55abd1fb21f06,"Computer Science, North Carolina State University, United States","Krishna, R., Computer Science, North Carolina State University, United States; Menzies, T., Computer Science, North Carolina State University, United States; Fu, W., Computer Science, North Carolina State University, United States","""Transfer learning"": is the process of translating quality predictors learned in one data set to another. Transfer learning has been the subject of much recent research. In practice, that research means changing models all the time as transfer learners continually exchange new models to the current project. This paper offers a very simple ""bellwether"" transfer learner. Given N data sets, we find which one produces the best predictions on all the others. This ""bellwether"" data set is then used for all subsequent predictions (or, until such time as its predictions start failing-at which point it is wise to seek another bellwether). Bellwethers are interesting since they are very simple to find (just wrap a for-loop around standard data miners). Also, they simplify the task of making general policies in SE since as long as one bellwether remains useful, stable conclusions for N data sets can be achieved just by reasoning over that bellwether. From this, we conclude (1) this bellwether method is a useful (and very simple) transfer learning method; (2) ""bellwethers"" are a baseline method against which future transfer learners should be compared; (3) sometimes, when building increasingly complex automatic methods, researchers should pause and compare their supposedly more sophisticated method against simpler alternatives. å© 2016 ACM.",Data Mining; Defect Prediction; Transfer learning,Conference Paper,Scopus,2-s2.0-84989206867
"Krishna R., Menzies T.",Actionable = Cluster + contrast?,2015,"Proceedings - 2015 30th IEEE/ACM International Conference on Automated Software Engineering Workshops, ASEW 2015",,,7426630,14,17,no,,,,10.1109/ASEW.2015.23,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964412465&doi=10.1109%2fASEW.2015.23&partnerID=40&md5=2b8e2395ce620c73fd4e18f068356c2b,"Computer Science, North Carolina State University, United States","Krishna, R., Computer Science, North Carolina State University, United States; Menzies, T., Computer Science, North Carolina State University, United States","There are many algorithms for data classification such as C4.5, Naive Bayes, etc. Are these enough for learning actionable analytics? Or should we be supporting another kind of reasoning? This paper explores two approaches for learning minimal, yet effective, changes to software project artifacts. å© 2015 IEEE.",data mining; instance-based reasoning; model-based reasoning; planning; Prediction; software engineering,Conference Paper,Scopus,2-s2.0-84964412465
"Kretschmer R., Pfouga A., Rulhoff S., Stjepandi€à J.",Knowledge-based design for assembly in agile manufacturing by using Data Mining methods,2016,Advanced Engineering Informatics,,,,,,no,,,,10.1016/j.aei.2016.12.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009212122&doi=10.1016%2fj.aei.2016.12.006&partnerID=40&md5=8ba4b03c1cadd4969c4e4c73abc35034,"PROSTEP AG, Head of Business Unit 3D Product Creation, Dolivostraë_e 11, D-64293 Darmstadt, Germany; Miele and Cie. KG, Segment Professional Laundry, Director Segment Professional Laundry, Technology Lehrte, IndustriestraÌÙe 3, 31275 Lehrte, Germany","Kretschmer, R., Miele and Cie. KG, Segment Professional Laundry, Director Segment Professional Laundry, Technology Lehrte, IndustriestraÌÙe 3, 31275 Lehrte, Germany; Pfouga, A., PROSTEP AG, Head of Business Unit 3D Product Creation, Dolivostraë_e 11, D-64293 Darmstadt, Germany; Rulhoff, S., PROSTEP AG, Head of Business Unit 3D Product Creation, Dolivostraë_e 11, D-64293 Darmstadt, Germany; Stjepandi€à, J., PROSTEP AG, Head of Business Unit 3D Product Creation, Dolivostraë_e 11, D-64293 Darmstadt, Germany","Decision making in early production planning phases is typically based on a rough estimation due to lack of a comprehensive, reliable knowledge base. Virtual planning has been prevailed as a method used to evaluate risks and costs before the concrete realization of production processes. The process of product assembly, which yields a high share in total production costs, gets its particular importance. This paper introduces a new approach and its initial implementation for knowledge-based design for assembly in agile manufacturing by using data mining (DM) methods in the field of series production with high variance. The approach adopts the usage of bulk data with old, successful designs in order to extrapolate its scope for assembly processes. Especially linked product and process data allow the innovative usage of DM methods in order to facilitate the front loading in the product development. The concept presents an affordable assistance potential for development of new products variants along the product emergence process (PEP). With this approach an early cost estimation of assembly processes in series production can be conducted using advanced DM methods as shown in an industrial use case. Furthermore, design and planning processes can be supported effectively. å© 2016 Elsevier Ltd.",Agile manufacturing; Assembly; Data mining; Design for assembly; Digital factory; Process planning,Article in Press,Scopus,2-s2.0-85009212122
"KrÌ_ger J., Fenske W., Meinicke J., Leich T., Saake G.",Extracting software product lines: A cost estimation perspective,2016,ACM International Conference Proceeding Series,16-23-September-2016,,,354,361,no,,,,10.1145/2934466.2962731,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991632850&doi=10.1145%2f2934466.2962731&partnerID=40&md5=32d62975e344890cd22c6df1dc721b1c,"Hochschule Harz (FH), University of Applied Sciences, Germany; Otto-von-Guericke-University, Magdeburg, Germany","KrÌ_ger, J., Hochschule Harz (FH), University of Applied Sciences, Germany, Otto-von-Guericke-University, Magdeburg, Germany; Fenske, W., Otto-von-Guericke-University, Magdeburg, Germany; Meinicke, J., Otto-von-Guericke-University, Magdeburg, Germany; Leich, T., Hochschule Harz (FH), University of Applied Sciences, Germany; Saake, G., Otto-von-Guericke-University, Magdeburg, Germany","Companies are often forced to customize their software products. Thus, a common practice is to clone and adapt existing systems to new customer requirements. With the extractive approach, those derived variants can be migrated into a software product line. However, changing to a new development process is risky and may result in unnecessary costs. Therefore, companies apply cost estimations to predict whether another development approach is beneficial. Existing cost models for software-product-line engineering focus on development from scratch. Contrarily, the extractive approach is more common in practice but specialized models are missing. Thus, in this work we focus on product-line extraction from a set of legacy systems. We i) describe according cost factors, ii) put them in context with the development process and cost curves, and iii) identify open challenges in product-line economics. This way, our work supports cost estimations for the extractive approach and provides a basis for further research. å© 2016 ACM.",Cost estimation; Extractive approach; Investment analysis; Risk assessment; Software product line,Conference Paper,Scopus,2-s2.0-84991632850
"Koziolek H., Goldschmidt T., de Gooijer T., Domis D., Sehestedt S., Gamer T., Aleksy M.",Assessing software product line potential: an exploratory industrial case study,2016,Empirical Software Engineering,21,2,,411,448,no,,,,10.1007/s10664-014-9358-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922424792&doi=10.1007%2fs10664-014-9358-0&partnerID=40&md5=d52527a44c67d5bcbcaab488aa82cc99,"ABB Corporate Research, Ladenburg, Germany","Koziolek, H., ABB Corporate Research, Ladenburg, Germany; Goldschmidt, T., ABB Corporate Research, Ladenburg, Germany; de Gooijer, T., ABB Corporate Research, Ladenburg, Germany; Domis, D., ABB Corporate Research, Ladenburg, Germany; Sehestedt, S., ABB Corporate Research, Ladenburg, Germany; Gamer, T., ABB Corporate Research, Ladenburg, Germany; Aleksy, M., ABB Corporate Research, Ladenburg, Germany","Corporate organizations sometimes offer similar software products in certain domains due to former company mergers or due to the complexity of the organization. The functional overlap of such products is an opportunity for future systematic reuse to reduce software development and maintenance costs. Therefore, we have tailored existing domain analysis methods to our organization to identify commonalities and variabilities among such products and to assess the potential for software product line (SPL) approaches. As an exploratory case study, we report on our experiences and lessons learned from conducting the domain analysis in four application cases with large-scale software products. We learned that the outcome of a domain analysis was often a smaller integration scenario instead of an SPL and that business case calculations were less relevant for the stakeholders and managers from the business units during this phase. We also learned that architecture reconstruction using a simple block diagram notation aids domain analysis and that large parts of our approach were reusable across application cases. å© 2015, Springer Science+Business Media New York.",Business case; Domain analysis; Software product lines,Article,Scopus,2-s2.0-84922424792
"Koo C., Hong T., Yoon J., Jeong K.",Zoning-Based Vertical Transportation Optimization for Workers at Peak Time in a Skyscraper Construction,2016,Computer-Aided Civil and Infrastructure Engineering,31,11,,826,845,no,,,,10.1111/mice.12220,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978910807&doi=10.1111%2fmice.12220&partnerID=40&md5=b12fb4c3c880513042549377cb4b73af,"Division of Construction Engineering and Management, Purdue University, West Lafayette, IN, United States; Department of Architectural Engineering, Yonsei UniversitySeoul, South Korea; LOTTE Engineering & ConstructionSeoul, South Korea","Koo, C., Division of Construction Engineering and Management, Purdue University, West Lafayette, IN, United States, Department of Architectural Engineering, Yonsei UniversitySeoul, South Korea; Hong, T., Department of Architectural Engineering, Yonsei UniversitySeoul, South Korea; Yoon, J., LOTTE Engineering & ConstructionSeoul, South Korea; Jeong, K., Department of Architectural Engineering, Yonsei UniversitySeoul, South Korea","In a skyscraper construction, a great number of workers and materials must be vertically transported to the proper positions depending on their roles. Particularly, the optimal vertical transportation logistics of lift cars for workers at peak time should be established to enhance the entire project performance in a skyscraper construction. For achieving this objective, the zoning-based concept can be introduced to improve the effectiveness of the vertical transportation logistics of lift cars for workers at peak time in a skyscraper construction. In developing the zoning-based vertical transportation logistics, it is necessary to consider the minimization of the electricity consumption as an environmental index as well as the minimization of the operating time and the maximization of the cost effectiveness. Therefore, this study aims to develop a multi-objective optimization model for solving the time‰ÛÒcost‰ÛÒenvironment trade-off problem in establishing the zoning-based vertical transportation logistics of lift cars for workers at peak time in a skyscraper construction. å© 2016‰ÛâComputer-Aided Civil and Infrastructure Engineering",,Article,Scopus,2-s2.0-84978910807
"Klosowski G., Gola A.",Risk-based estimation of manufacturing order costs with artificial intelligence,2016,"Proceedings of the 2016 Federated Conference on Computer Science and Information Systems, FedCSIS 2016",,,7733321,729,732,no,,,,10.15439/2016F323,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007205646&doi=10.15439%2f2016F323&partnerID=40&md5=a6bd73bc99a80014effcf0bf8be66c36,"Lublin University of Technology, Faculty of Management, Department of Enterprise Organization, Poland; Lublin University of Technology, Faculty of Mechanical Engineering, Institute of Technological Systems of Information, Poland","Klosowski, G., Lublin University of Technology, Faculty of Management, Department of Enterprise Organization, Poland; Gola, A., Lublin University of Technology, Faculty of Mechanical Engineering, Institute of Technological Systems of Information, Poland","The following paper discusses the development of a risk-based cost estimation model for completing non-standard manufacturing orders. The model in question is a hybrid of Monte Carlo Simulation (MCS), which constitutes the main module of the applied model. Vector of order risk probability, which is the input data for the MCS module, is highly difficult to assess and is burdened to a considerable degree with subjectivity, therefore it was resolved that it should be generated with the application of artificial intelligence. Depending on the accessibility of historical data, the model incorporates fuzzy logic or artificial neural networks methods. The presented model could provide support to managers responsible for cost estimation, and moreover, after slight modification also in setting deadlines for non-standard manufacturing orders. å© 2016 Polish Information Processing Society.",,Conference Paper,Scopus,2-s2.0-85007205646
"Kless D., Jansen L., Milton S.",A content-focused method for re-engineering thesauri into semantically adequate ontologies using OWL,2016,Semantic Web,7,5,,543,576,no,,,,10.3233/SW-150194,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976484485&doi=10.3233%2fSW-150194&partnerID=40&md5=b2a7681607faa55c323c72dac234bd47,"Department of Computing and Information Systems, University of Melbourne, Parkville, VIC, Australia; Institute for Philosophy, University of Rostock, August-Bebel-StraÌÙe 28, Rostock, Germany","Kless, D., Department of Computing and Information Systems, University of Melbourne, Parkville, VIC, Australia; Jansen, L., Institute for Philosophy, University of Rostock, August-Bebel-StraÌÙe 28, Rostock, Germany; Milton, S., Department of Computing and Information Systems, University of Melbourne, Parkville, VIC, Australia","The re-engineering of vocabularies into ontologies can save considerable time in the development of ontologies. Current methods that guide the re-engineering of thesauri into ontologies often convert vocabularies merely syntactically and ignore problems arising from interpreting vocabularies as ontologies, i.e. as sets of statements of facts. Current re-engineering methods also do not make use of the semantic capabilities of formal languages in order to detect logical mistakes and improve vocabularies. In this paper, we introduce a content-focused method for building domain-specific ontologies based on a thesaurus, a popular type of vocabulary. Application of the method results in an ontology that not only adheres to the semantics of the description logic OWL, but also contains a semantically rich description of the modeled entities, enables non-trivial, automated reasoning, and can be integrated with other ontologies following the same development principles. We explain the motivation and sub-activities for each of the steps in our method and illustrate their application through a case study in the domain of agricultural fertilizers based on the ACROVOC Thesaurus. Our method shows, first and foremost, that a considerable manual effort is required to derive a semantically rich ontology from a thesaurus, particularly in connection with the alignment to a top-level ontology as well as for the identification and formal specification of membership conditions. Applying our method will likely change the structure of a thesaurus considerably. Our method is particularly useful where a highly reliable is-a hierarchy or consistent definitions are crucial. å© 2016 - IOS Press and the authors. All rights reserved.",Ontology development; OWL; Thesaurus re-engineering,Article,Scopus,2-s2.0-84976484485
"Kharadkar R.D., Hulle N.B.",FPGA Implementation of Modulo (231-1) Adder,2016,"International Conference on Emerging Trends in Engineering and Technology, ICETET",2016-March,,7425588,85,90,no,,,,10.1109/ICETET.2015.23,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962820701&doi=10.1109%2fICETET.2015.23&partnerID=40&md5=64c0119005eb13343c4d89f1d4d38696,"Dept. of e and TC, GHRIET Pune, Pune, India; Department of Electronics, GHRCE, Nagpur, India","Kharadkar, R.D., Dept. of e and TC, GHRIET Pune, Pune, India; Hulle, N.B., Department of Electronics, GHRCE, Nagpur, India","Modulo (231-1) adder is one of the important module in ZUC stream cipher. The paper presents compact, high performance architecture for modulo (231-1) adder using CLA. The proposed architecture is implemented by using VHDL language with CAD tool Xilinx ISE Design Suite 13.2 and target device is Xilinx Spartan3-xc3s1000, with package FG320. Presented result shows that proposed architecture minimizes the chip area, power consumption and increases computation speed of modulo (231-1) adder. å© 2015 IEEE.",chip area; CLA; FPGA; gate delay; generate function; modulo adder; propagate function,Conference Paper,Scopus,2-s2.0-84962820701
"Keskin M., Alptekin G.I.",Yazilim maliyet tahmininde iÙlev puani analizi ve yapay sinir a‚¤lari kullanim,2016,CEUR Workshop Proceedings,1721,,,466,471,no,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996562173&partnerID=40&md5=28a04cbd3af5989c6c59845730480d29,"Bilgisayar MÌ_hendisligi BÌ¦lÌ_mÌ_, Galatasaray ÌÏNiversitesi, Istanbul, Turkey","Keskin, M., Bilgisayar MÌ_hendisligi BÌ¦lÌ_mÌ_, Galatasaray ÌÏNiversitesi, Istanbul, Turkey; Alptekin, G.I., Bilgisayar MÌ_hendisligi BÌ¦lÌ_mÌ_, Galatasaray ÌÏNiversitesi, Istanbul, Turkey",[No abstract available],,Conference Paper,Scopus,2-s2.0-84996562173
"Karthikeyan T., Nandhini T.",Dependent component cost model of legacy application for hybrid cloud,2016,"Proceedings of IEEE International Conference on Circuit, Power and Computing Technologies, ICCPCT 2016",,,7530154,,,no,,,,10.1109/ICCPCT.2016.7530154,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992028021&doi=10.1109%2fICCPCT.2016.7530154&partnerID=40&md5=c6bfc5ee6d1020c862351f5b27fe81b9,"Department of Computer Science, PSG College of Arts and Science, Coimbatore, India; Bharathiar University, Coimbatore, India; Dept. of Computer Applications, Indo-American College, Cheyyar, India","Karthikeyan, T., Department of Computer Science, PSG College of Arts and Science, Coimbatore, India; Nandhini, T., Bharathiar University, Coimbatore, India, Dept. of Computer Applications, Indo-American College, Cheyyar, India","Cloud computing is an emerging technology employing pay-as-you-go technique helps many enterprises to concentrate on cloud migration. An affordable cost enables several legacy applications to opt for migration. Since legacy applications are made up of distributed components, the migration of all the components to the cloud result in the increase of cloud migration cost. In this paper, several pricing models and pricing types were studied. This paper analyzed several factors affecting the cost of migration and based on the selected cost parameters an optimal formula for estimation of cost was derived. å© 2016 IEEE.",cloud migration; cost estimation; dependent components; legacy application; pricing,Conference Paper,Scopus,2-s2.0-84992028021
"Karim M.R., Ruhe G., Rahman M.M., Garousi V., Zimmermann T.",An empirical investigation of single-objective and multiobjective evolutionary algorithms for developer's assignment to bugs,2016,Journal of Software: Evolution and Process,28,12,,1025,1060,no,,,,10.1002/smr.1777,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963642797&doi=10.1002%2fsmr.1777&partnerID=40&md5=1c8cfba08771f38c0550b2b6ade7e962,"Software Engineering Decision Support Laboratory, University of Calgary, Calgary, AB, Canada; MNP LLP, Calgary, AB, Canada; Software Engineering Research Group, Department of Computer Engineering, Hacettepe University, Ankara, Turkey; Microsoft Research, One Microsoft Way, Redmond, WA, United States","Karim, M.R., Software Engineering Decision Support Laboratory, University of Calgary, Calgary, AB, Canada; Ruhe, G., Software Engineering Decision Support Laboratory, University of Calgary, Calgary, AB, Canada; Rahman, M.M., MNP LLP, Calgary, AB, Canada; Garousi, V., Software Engineering Research Group, Department of Computer Engineering, Hacettepe University, Ankara, Turkey; Zimmermann, T., Microsoft Research, One Microsoft Way, Redmond, WA, United States","In this paper, the modeling of developers‰Ûª assignment to bugs (DAB) is studied. The problem is modeled both as a single objective (minimize bug fix time) and as a bi-objective (minimize bug fix time and cost) combinatorial optimization problem. Two models of developer assignment are considered where in the first model a single developer is assigned per bug (single developer model), while in the second model a single developer is assigned for each competency area of a bug (individual competency model). The latter model is proposed in this paper. For the single developer model, GA@DAB, an existing genetic algorithm-based approach, is extended to support precedence among bugs. For the individual competency model of DAB, one genetic algorithm-based approach (Competence@DAB) and one nondominated sorting genetic algorithm II-based approach (CompetenceMulti2@DAB) are proposed to generate solutions minimizing time and minimizing both time and cost, respectively. The performance of the proposed approaches was evaluated for 2040 bugs of 19 open-source milestone projects from the Eclipse platform. Our results and analysis show that the proposed individual competency model is far better than the single developer model, with average bug fix time reduction of 39.7% across all projects. Copyright å© 2016 John Wiley &amp; Sons, Ltd. Copyright å© 2016 John Wiley & Sons, Ltd.",developer's assignment to bugs; genetic algorithm; multiobjective optimization; Pareto optimal,Conference Paper,Scopus,2-s2.0-84963642797
Kang H.J.,Development of an nearly Zero Emission Building (nZEB) life cycle cost assessment tool for fast decision making in the early design phase,2017,Energies,10,1,59,,,no,,,,10.3390/en10010059,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009241592&doi=10.3390%2fen10010059&partnerID=40&md5=617181d79d5220a67c1cee2c9cd612ad,"SAMOO Architects and Engineers, Seoul, South Korea","Kang, H.J., SAMOO Architects and Engineers, Seoul, South Korea","An economic feasibility optimization method for the life cycle cost (LCC) has been developed to apply energy saving techniques in the early design stages of a building. The method was developed using default data (e.g., operation schedules), energy consumption prediction equations and cost prediction equations utilizing design variables considered in the early design phase. With certain equations developed, an LCC model was constructed using the computational program MATLAB, to create an automated optimization process. To verify the results from the newly developed assessment tool, a case study on an office building was performed to outline the results of the designer's proposed model and the cost optimal model.",Cost optimal model; Designer's proposed model; Investment cost; LCC analysis; Nearly Zero Emission Building; Operation cost,Article,Scopus,2-s2.0-85009241592
"Kallergis D., Tsantilis J., Douligeris C.",Performance evaluation of cloud systems: A behavioural approach,2015,"2015 IEEE International Symposium on Signal Processing and Information Technology, ISSPIT 2015",,,7394370,409,414,no,,,,10.1109/ISSPIT.2015.7394370,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963900708&doi=10.1109%2fISSPIT.2015.7394370&partnerID=40&md5=b17a9cb65c13115228867662c4d12b53,"Dept. of Informatics, University of Piraeus, 80, Karaoli Dimitriou Street, Piraeus, Greece","Kallergis, D., Dept. of Informatics, University of Piraeus, 80, Karaoli Dimitriou Street, Piraeus, Greece; Tsantilis, J., Dept. of Informatics, University of Piraeus, 80, Karaoli Dimitriou Street, Piraeus, Greece; Douligeris, C., Dept. of Informatics, University of Piraeus, 80, Karaoli Dimitriou Street, Piraeus, Greece","The concept of Cloud Engineering (CE) as a superset of performance engineering emerges in an extensive number of industrial architectural approaches and implementation flavours. To effectively manage cloud-based systems, it is crucial to monitor, to meter and then to allocate their structural behaviour and performance. In the context of this work, considerable number of experimental measurements are conducted in a distributed virtualised infrastructure and statistically analysed. The paper aims to evaluate cloud systems using multiple open-source benchmarking tools. Also, it addresses and discusses operational deviations of a commercial cloud system in respect to the service level objects (SLOs) per se. å© 2015 IEEE.",benchmarking; performance; statistical,Conference Paper,Scopus,2-s2.0-84963900708
"Kadam A.K., Joshi S.D., Bhattacharyya D., Kim H.-J.",Software superiority achievement through functional point and test point analysis,2016,International Journal of Software Engineering and its Applications,10,11,,181,192,no,,,,10.14257/ijseia.2016.10.11.16,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007035069&doi=10.14257%2fijseia.2016.10.11.16&partnerID=40&md5=32968e7168609592bf024f06621a057b,"Bharati Vidyapeeth University College of Engineering, Pune, India; Department of Computer Science and Engineering, Vignan's Institute of Information Technology, Visakhapatnam, India; Sungshin Women's University, 2 Bomun-ro 34da-gil, Seongbuk-gu, Seoul, South Korea","Kadam, A.K., Bharati Vidyapeeth University College of Engineering, Pune, India; Joshi, S.D., Bharati Vidyapeeth University College of Engineering, Pune, India; Bhattacharyya, D., Department of Computer Science and Engineering, Vignan's Institute of Information Technology, Visakhapatnam, India; Kim, H.-J., Sungshin Women's University, 2 Bomun-ro 34da-gil, Seongbuk-gu, Seoul, South Korea",Software cost estimation is the important activity while the development of the software. Expenditure assessment is bit complex task as it can be affected by many factors. This factors aids in the calculating of maintenance cost of software. In this paper we have implemented the function point analysis and test point analysis in order to discover the maintenance cost. This is accomplished by using the various techniques to calculate the function point analysis and test point analysis. Along with the calculation of maintenance cost we have also presented the module to assess the reliability of the software from the context of white box testing. Software reliability growth models are aids to evaluate the reliability of the software. This paper presented the analysis of code based SRGM to estimate the reliability. å© 2016 SERSC.,Function point analysis (FPA); Software reliability; Software reliability growth model; Test point analysis (TPA),Article,Scopus,2-s2.0-85007035069
"Jun G., Yongming Y., Bin Z., Qingmin M.",SLA-oriented research on prediction and evaluation of service components performance,2015,"Proceedings - 2015 12th Web Information System and Application Conference, WISA 2015",,,7396623,136,141,no,,,,10.1109/WISA.2015.33,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964247525&doi=10.1109%2fWISA.2015.33&partnerID=40&md5=bc7850eed9a8ba81bb19285f8125f4b4,"College of Information Science and Engineering, Northeastern University, Shenyang, China","Jun, G., College of Information Science and Engineering, Northeastern University, Shenyang, China; Yongming, Y., College of Information Science and Engineering, Northeastern University, Shenyang, China; Bin, Z., College of Information Science and Engineering, Northeastern University, Shenyang, China; Qingmin, M., College of Information Science and Engineering, Northeastern University, Shenyang, China","The application services which deployed in cloud environment have some characteristics, such as the variability of concurrent requests and the differences of resource demands among components, which bring certain influence on service performance even cause some potential problems. Therefore, making an effective service performance prediction mechanism has become a research hotspot for application services based cloud service components. This paper proposes SLA-oriented research on prediction of service component and introduces the fundamental process of performance prediction. It adopts the non-negative matrix factorization method for performance prediction and makes an evaluation on component performance. The experiment result indicates the method in this paper is feasible and valid, and the accuracy is higher than the traditional approaches. å© 2015 IEEE.",Cloud service component; Non-negative matrix factorization; Performance prediction; Response time sequence,Conference Paper,Scopus,2-s2.0-84964247525
"Joy J., Rajwade S., Gerla M.",Participation cost estimation: Private versus non-private study,2016,"2016 Mediterranean Ad Hoc Networking Workshop, Med-Hoc-Net 2016 - 15th IFIP MEDHOCNET 2016",,,7528423,,,no,,,,10.1109/MedHocNet.2016.7528423,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992163047&doi=10.1109%2fMedHocNet.2016.7528423&partnerID=40&md5=b3acafb680351df6e1ab60217d6b5699,"UCLA, United States","Joy, J., UCLA, United States; Rajwade, S., UCLA, United States; Gerla, M., UCLA, United States","In our study, we seek to learn the real-time crowd levels at popular points of interests based on users continually sharing their location data. We evaluate the benefits of users sharing their location data privately and non-privately, and show that suitable privacy-preserving mechanisms provide incentives for user participation in a private study as compared to a non-private study. å© 2016 IEEE.",,Conference Paper,Scopus,2-s2.0-84992163047
Jorgensen M.,A survey on the characteristics of projects with success in delivering client benefits,2016,Information and Software Technology,78,,,83,94,no,,,,10.1016/j.infsof.2016.05.008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971643798&doi=10.1016%2fj.infsof.2016.05.008&partnerID=40&md5=8431616f5b108c14524aae8a0f929bc8,"Simula Research Laboratory, P.O. Box 134, Lysaker, Norway","JÌürgensen, M., Simula Research Laboratory, P.O. Box 134, Lysaker, Norway","Context A large waste of resources in software development projects currently results from being unable to produce client benefits. Objective The main objective is to better understand the characteristics of successful software projects and contribute to software projects that are more likely to produce the planned client benefits. Method We asked 63 Norwegian software professionals, representing both the client and the provider role, to report information about their last completed project. In a follow-up survey with 64 Norwegian software professionals, we addressed selected findings from the first survey. Results The analysis of the project information showed the following: i) The project management triangle criteria of being on time, on budget, and having the specified functionality are poor correlates of the essential success dimension client benefits. ii) Benefit management planning before the project started and benefit management activities during project execution were connected with success in delivering client benefits. iii) Fixed-price projects and projects in which the selection of providers had a strong focus on low price were less successful in delivering project benefits than other projects. iv) Agile projects were in general more successful than other projects, but agile projects without flexible scope to reflect changed user needs and learning, or without frequent delivery to the client, had less than average success in delivering client benefits. Conclusions The software projects that were successful in delivering client benefits differed from the less successful ones in several ways. In particular, they applied benefit management practices during project execution, they avoided fixed-price contracts, they had less focus on low price in the selection of providers, and they applied the core agile practices - frequent delivery to the client and scope flexibility. å© 2016 Elsevier B.V. All rights reserved.",Client benefits; Project success factors; Software projects; Survey,Article,Scopus,2-s2.0-84971643798
Jorgensen M.,The effect of the time unit on software development effort estimates,2015,"SKIMA 2015 - 9th International Conference on Software, Knowledge, Information Management and Applications",,,7399992,,,no,,,,10.1109/SKIMA.2015.7399992,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964685756&doi=10.1109%2fSKIMA.2015.7399992&partnerID=40&md5=be9382e6e124f74cf32edc4123041e97,"Simula Research Laboratory, Norway; Kathmandu University, Nepal; University of Oslo, Norway","Jorgensen, M., Simula Research Laboratory, Norway, Kathmandu University, Nepal, University of Oslo, Norway","Estimates of software development effort are frequently inaccurate and over-optimistic. In this paper we describe how changes in the granularity of the unit of estimation, e.g., work-days instead of work-hours, affects the effort estimates. We describe four psychological mechanisms, how they interact and discuss the expected total effect of higher granularity units on effort estimates. We argue that the mechanisms in general imply that higher granularity effort units will result in higher effort estimates, e.g., that estimating software development work in work-days or weeks will lead to higher estimates than when estimating in work-hours. A possible implication of this predicted effect is that, in contexts where there is a tendency towards under-estimation, estimation in work-days or weeks instead of work-hours leads to more realistic estimates. å© 2015 IEEE.",cost estimation; human judgment; psychology,Conference Paper,Scopus,2-s2.0-84964685756
"Johri P., Nasar M., Das S., Kumar M.",Open source software reliability growth models for distributed environment based on component-specific testing-efforts,2016,ACM International Conference Proceeding Series,04-05-March-2016,, a75,,,no,,,,10.1145/2905055.2905283,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988601023&doi=10.1145%2f2905055.2905283&partnerID=40&md5=d200e5220e17596a8bebf076ab311357,"School of Computing Science and Engg, Galgotias University Gr. Noida, India","Johri, P., School of Computing Science and Engg, Galgotias University Gr. Noida, India; Nasar, M., School of Computing Science and Engg, Galgotias University Gr. Noida, India; Das, S., School of Computing Science and Engg, Galgotias University Gr. Noida, India; Kumar, M., School of Computing Science and Engg, Galgotias University Gr. Noida, India","Because of availability, redistributable, affordability, modifiability, of source code, free and no restriction in choice, open source is a favorite platform for lot of software industries and peoples, who consider using the power of extremely reliable and superior quality software. Numeouus SRGMs have been proposed to estimate the reliability of the software of OSSs; however, no one has proven to perform very well considering diverse project characteristics. In the models for OSSs, the error deletion experience for the reused and the newly developed components based on component-specific testing-effort is demonstrated. It is considered that there are several different types of faults for newly developed component and single type of faults for reused components for obtaining the unambiguous expressions for the mean number of individual types of errors. For OSSs system components testing-efforts have to be modeled separately for each and every component in the system. The total effort of the system is then calculated from the summation of component-specific testing-effort functions. We have employed MATLAB as implementation framework for performing all the estimations. Our approach partitions the testing effort with growth curves of varying nature among different components of the same OSS. To validate our analytical results, numerical illustrations have also been provided. å© 2016 ACM.",Distributed development environment (DDE); Non Homogeneous Poisson Process (NHPP); Open Source Software (OSS); Software Reliability Growth Models (SRGMs).; Testing-effort function (TEF),Conference Paper,Scopus,2-s2.0-84988601023
"Johnson P., LagerstrÌ¦m R., Franke U., Ekstedt M.",Modeling and analyzing systems-of-systems in the Multi-Attribute Prediction Language (MAPL),2016,"Proceedings - 4th International Workshop on Software Engineering for Systems-of-Systems, SESoS 2016",,,,1,7,no,,,,10.1145/2897829.2897830,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974573846&doi=10.1145%2f2897829.2897830&partnerID=40&md5=cb2a7e9c2832015edb7801e6ad519ac4,"Royal Institute of Technology, Stockholm, Sweden; Royal Institute of Technology Stockholm, Sweden; SICS Swedish ICT, Kista, Sweden","Johnson, P., Royal Institute of Technology, Stockholm, Sweden; LagerstrÌ¦m, R., Royal Institute of Technology Stockholm, Sweden; Franke, U., SICS Swedish ICT, Kista, Sweden; Ekstedt, M., Royal Institute of Technology, Stockholm, Sweden","The Multi-Attribute Prediction Language (MAPL), an analysis metamodel for non-functional qualities of systems-ofsystems, is introduced. MAPL features analysis in five nonfunctional areas: service cost, service availability, data accuracy, application coupling, and application size. In addition, MAPL explicitly includes utility modeling to make tradeoffs between the qualities. The paper introduces how each of the five non-functional qualities is modeled and quantitatively analyzed based on the ArchiMate standard for enterprise architecture modeling and the previously published Predictive, Probabilistic Architecture Modeling Framework, building on the well-known UML and OCL formalisms. The main contribution of MAPL lies in combining all five nonfunctional analyses into a single unified framework. å© 2016 ACM.",Architecture analysis; Architecture modeling; Systems-of-systems architecture,Conference Paper,Scopus,2-s2.0-84974573846
"Jing X.-Y., Qi F., Wu F., Xu B.",Missing data imputation based on low-rank recovery and semi-supervised regression for software effort estimation,2016,Proceedings - International Conference on Software Engineering,14-22-May-2016,,,607,618,no,,,1,10.1145/2884781.2884827,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971422020&doi=10.1145%2f2884781.2884827&partnerID=40&md5=ccf322a53bbf1ec3af7eda7e5946f4f7,"State Key Laboratory of Software Engineering, School of Computer, Wuhan University, China; School of Automation, Nanjing University of Posts and Telecommunications, China; Department of Computer Science and Technology, Nanjing University, China","Jing, X.-Y., State Key Laboratory of Software Engineering, School of Computer, Wuhan University, China, School of Automation, Nanjing University of Posts and Telecommunications, China; Qi, F., State Key Laboratory of Software Engineering, School of Computer, Wuhan University, China; Wu, F., State Key Laboratory of Software Engineering, School of Computer, Wuhan University, China, School of Automation, Nanjing University of Posts and Telecommunications, China; Xu, B., State Key Laboratory of Software Engineering, School of Computer, Wuhan University, China, Department of Computer Science and Technology, Nanjing University, China","Software effort estimation (SEE) is a crucial step in software development. Effort data missing usually occurs in real-world data collection. Focusing on the missing data problem, existing SEE methods employ the deletion, ignoring, or imputation strategy to address the problem, where the imputation strategy was found to be more helpful for improving the estimation performance. Current imputation methods in SEE use classical imputation techniques for missing data imputation, yet these imputation techniques have their respective disadvantages and might not be appropriate for effort data. In this paper, we aim to provide an effective solution for the effort data missing problem. Incompletion includes the drive factor missing case and effort label missing case. We introduce the low-rank recovery technique for addressing the drive factor missing case. And we employ the semi-supervised regression technique to perform imputation in the case of effort label missing. We then propose a novel effort data imputation approach, named low-rank recovery and semisupervised regression imputation (LRSRI). Experiments on 7 widely used software effort datasets indicate that: (1) the proposed approach can obtain better effort data imputation effects than other methods; (2) the imputed data using our approach can apply to multiple estimators well. å© 2016 ACM.",Drive factor missing case; Effort label missing case; Low-rank recovery and semi-supervised regression imputation (LRSRI); Missing data problem; Software effort estimation,Conference Paper,Scopus,2-s2.0-84971422020
"Jeong E.-J., Jeong S.-R.",Development and application of a stage-gate process to reduce the unerlying risks of it service projects,2016,Journal of Theoretical and Applied Information Technology,93,2,,233,245,no,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85001968687&partnerID=40&md5=a1be3f0db8c7cc8c165144bc7c87bcc2,"Kookmin University, Graduate School of Business IT, Seoul, South Korea","Jeong, E.-J., Kookmin University, Graduate School of Business IT, Seoul, South Korea; Jeong, S.-R., Kookmin University, Graduate School of Business IT, Seoul, South Korea","Recently IT service projects have increased to introduce new technology as like data analytics, IoT (Internet of Things), cloud and mobile computing and to change or improve business process of finance, manufacturing, service, and government and public organizations, but lots of projects were failed due to cost-overrun, schedule delay, and fail to pass user acceptance test on time, and fail to align company‰Ûªs objective and strategy. There are several critical factors of project failure, for example, incorrect project cost estimation, lack of enterprise-wide risk management, unfair contract agreement, and missing or incomplete user requirements, and low quality level of design and development, and lack of user participation or cooperation for user requirement definitions and user acceptance test. To prevent the critical risk factors of project, the risks should be identified and assessed during project lifecycle, and report to project governance board, and the project governance board should make the Go/No-Go decision at the end of each project stage. The purpose of this thesis is to develop the project SGP(Stage-Gate Process) for enterprise-wide risk management structure to reduce the project failure rate, and for helping to achieve the company‰Ûªs objective and strategy, and then to verify the effectiveness of the project SGP through application of the SGP to actual IT service projects. We can aware the SGP is very useful to reduce the failure rate of project through preventing the costoverrun, schedule delay, and failure to pass for user acceptance test. The SGP is consisting of assessment of deliverable by project management office and quality assurance, and Go/No-Go decision making based on quality criteria by executives for enterprise-wide risk and quality management at the end of each project stage. And we confirmed the effectiveness of SGP through FGI (Focus Group Interview), the result show that the SGP is very useful to manage cost, risk, and quality, but the effectiveness of SGP is dependent on company‰Ûªs project governance structure and process, and project governance board‰Ûªs attention and support to the SGP process. å© 2005-2016 JATIT & LLS. All rights reserved.",It service project; Project governance board; Project management office; Project stakeholder; Risk management; Stage-gate process,Article,Scopus,2-s2.0-85001968687
"Jensen U., Kugler P., Ring M., Eskofier B.M.",Approaching the accuracy‰ÛÒcost conflict in embedded classification system design,2016,Pattern Analysis and Applications,19,3,,839,855,no,,,,10.1007/s10044-015-0503-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977576782&doi=10.1007%2fs10044-015-0503-1&partnerID=40&md5=ebd5ec9e2733c7a976f1ae0b4a10f060,"Digital Sports Group, Pattern Recognition Lab, Friedrich-Alexander-UniversitÌ_t Erlangen-NÌ_rnberg (FAU), Haberstr. 2, Erlangen, Germany","Jensen, U., Digital Sports Group, Pattern Recognition Lab, Friedrich-Alexander-UniversitÌ_t Erlangen-NÌ_rnberg (FAU), Haberstr. 2, Erlangen, Germany; Kugler, P., Digital Sports Group, Pattern Recognition Lab, Friedrich-Alexander-UniversitÌ_t Erlangen-NÌ_rnberg (FAU), Haberstr. 2, Erlangen, Germany; Ring, M., Digital Sports Group, Pattern Recognition Lab, Friedrich-Alexander-UniversitÌ_t Erlangen-NÌ_rnberg (FAU), Haberstr. 2, Erlangen, Germany; Eskofier, B.M., Digital Sports Group, Pattern Recognition Lab, Friedrich-Alexander-UniversitÌ_t Erlangen-NÌ_rnberg (FAU), Haberstr. 2, Erlangen, Germany","Smart embedded systems often run sophisticated pattern recognition algorithms and are found in many areas like automotive, sports and medicine. The developer of such a system is often confronted with the accuracy‰ÛÒcost conflict as the resulting system should be as accurate as possible while being able to run on resource constraint hardware. This article introduces a method to support the solution of this design conflict with accuracy‰ÛÒcost reports. These reports compare classification systems regarding their classification rate (accuracy) and the mathematical operations and parameters of the working phase (cost). Our method is used to deduce the specific cost of various popular pattern recognition algorithms and to derive the overall cost of a classification system. We also show how our analysis can be used to estimate the computational cost for specific hardware architectures. A software toolbox to create accuracy‰ÛÒcost reports was implemented to facilitate the automatic classification system comparison with the presented methodology. The software is available for download and as supplementary material. We performed different experiments on synthetic and real-world data to underline the value of this analysis. Accurate and computationally cheap classification systems were easily identified. We were even able to find a better implementation candidate in an existing embedded classification problem. This work is the first step towards a comprehensive support tool for the design of embedded classification systems. å© 2015, Springer-Verlag London.",Classification system design; Cost estimation; Machine learning; Real-time systems,Article,Scopus,2-s2.0-84977576782
"Jeavons C.E., Baldwin J.S., McGourlay J., Purshouse R.C.",A value- ocussed decision framework for manufacturing research environments,2016,Advances in Transdisciplinary Engineering,4,,,662,671,no,,,,10.3233/978-1-61499-703-0-662,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84993929254&doi=10.3233%2f978-1-61499-703-0-662&partnerID=40&md5=aeb87fcadbf46f68aa97fd584d9d5c92,"IDC in Machining Science, AMRC with Boeing, University of Sheffield, United Kingdom; AMRC with Boeing, University of Sheffield, United Kingdom; Manufacturing Technology, Rolls-Royce plc, United Kingdom; Department of Automatic Control and Systems Engineering, University of Sheffield, United Kingdom","Jeavons, C.E., IDC in Machining Science, AMRC with Boeing, University of Sheffield, United Kingdom; Baldwin, J.S., AMRC with Boeing, University of Sheffield, United Kingdom; McGourlay, J., Manufacturing Technology, Rolls-Royce plc, United Kingdom; Purshouse, R.C., Department of Automatic Control and Systems Engineering, University of Sheffield, United Kingdom","Industrial research centres have a requirement to deliver new products, technologies and processes which can be applied in manufacturing environments. The dynamic nature of these centres has attracted a growing need from industry for value-focussed decision based systems to be in place that include accurate and reliable cost estimation techniques. A holistic solution which is able to manage the dynamic and complex nature of knowledge within these environments is required. This paper offers the potential to provide a solution for a value-focussed cost estimation and decision framework capable of supporting technology selection within these environments. å© 2016 The authors and IOS Press.",Cost engineering; Decision making; Uncertainty,Conference Paper,Scopus,2-s2.0-84993929254
JÌürgensen M.,Unit effects in software project effort estimation: Work-hours gives lower effort estimates than workdays,2016,Journal of Systems and Software,117,,,274,281,no,,,,10.1016/j.jss.2016.03.048,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962427440&doi=10.1016%2fj.jss.2016.03.048&partnerID=40&md5=7d02d67e8d1799bfb49c7159da570952,"Simula Research Laboratory, Norway","JÌürgensen, M., Simula Research Laboratory, Norway","Software development effort estimates are typically expert judgment-based and too low to reflect the actual use of effort. Our goal is to understand how the choice of effort unit affects expert judgement-based effort estimates, and to use this knowledge to increase the realism of effort estimates. We conducted two experiments where the software professionals were randomly instructed to estimate the effort of the same projects in work-hours or in workdays. In both experiment, the software professionals estimating in work-hours had much lower estimates (on average 33%-59% lower) than those estimating in workdays. We argue that the unitosity effect - i.e., that we tend to infer information about the quantity from the choice of unit - is the main explanation for the large difference in effort estimates. A practical implication of the unit effect is that, in contexts where there is a tendency toward effort under estimation, the instruction to estimate in higher granularity effort units, such as workdays instead of work-hours, is likely to lead to more realistic effort estimates. å© 2016 Elsevier Inc. All rights reserved.",Effort estimation; Effort unit; Judgment bias,Article,Scopus,2-s2.0-84962427440
JÌürgensen M.,The use of precision of software development effort estimates to communicate uncertainty,2016,Lecture Notes in Business Information Processing,238,,,156,168,no,,,,10.1007/978-3-319-27033-3_11,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952683979&doi=10.1007%2f978-3-319-27033-3_11&partnerID=40&md5=11efee615acbda3387dac72646049818,"Simula Research Laboratory, Fornebu, Norway","JÌürgensen, M., Simula Research Laboratory, Fornebu, Norway","The precision of estimates may be applied to communicate the uncertainty of required software development effort. The effort estimates 1000 and 975 work-hours, for example, communicate different levels of expected estimation accuracy. Through observational and experimental studies we found that software professionals (i) sometimes, but not in the majority of the examined projects, used estimate precision to convey effort uncertainty, (ii) tended to interpret overly precise, inaccurate effort estimates as indicating low developer competence and low trustworthiness of the estimates, while too narrow effort prediction intervals had the opposite effect. This difference remained even when the actual effort was known to be outside the narrow effort prediction interval. We identified several challenges related to the use of the precision of single value estimates to communicate effort uncertainty and recommend that software professionals use effort prediction intervals, and not the preciseness of single value estimates, to communicate effort uncertainty. å© Springer International Publishing Switzerland 2016.",Estimate precision; Estimation uncertainty; Human judgment; Software cost estimation,Conference Paper,Scopus,2-s2.0-84952683979
"Ismail U.M., Islam S., Islam S.",Towards cloud security monitoring: A case study,2016,"Proceedings - 2016 Cybersecurity and Cyberforensics Conference, CCC 2016",,,7600203,8,14,no,,,,10.1109/CCC.2016.8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994829900&doi=10.1109%2fCCC.2016.8&partnerID=40&md5=f4d8485c7ea870d4454ee6c0eecf3445,"School of Architecture, Computing and Engineering, University of East London, London, United Kingdom","Ismail, U.M., School of Architecture, Computing and Engineering, University of East London, London, United Kingdom; Islam, S., School of Architecture, Computing and Engineering, University of East London, London, United Kingdom; Islam, S., School of Architecture, Computing and Engineering, University of East London, London, United Kingdom","Cloud computing has become the norm in the provisioning of computing resources due to its flexible and proven reliability. Businesses perceive cloud services as a trend that presents enormous possibilities both in economic and technical terms. The growth in cloud services have also increased bottlenecks and security risks to business assets. Cloud security monitoring has remained relatively unexplored in security terms, a factor that has led businesses to be oblivious on the metrics to capture and the appropriate techniques to use. In this paper, we explore security monitoring in terms of tracking specific user requirements based on a case study. We identify various security tools that are practically relevant for addressing the requirements, and devise selection criteria for choosing the best tools. We present an evaluation of the tools and present a ranking for the tools that meet the particular requirements of the case study. The effort in this paper broadens the notion of cloud security monitoring and provides a methodical practical approach to solving a security related issue. å© 2016 IEEE.",Cloud Computing; Monitoring; Security Monitoring,Conference Paper,Scopus,2-s2.0-84994829900
"Ishii N., Takano Y., Muraki M.",A dynamic scheduling problem in cost estimation process of EPC projects,2016,"SIMULTECH 2016 - Proceedings of the 6th International Conference on Simulation and Modeling Methodologies, Technologies and Applications",,,,187,194,no,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991225588&partnerID=40&md5=5cd4874968650d6b88fed7ab730a0ca2,"Faculty of Engineering, Kanagawa University, 3-27-1 Rokkakubashi, Kanagawa-ku, Yokoham, Japan; School of Network Information, Senshu University, 2-1-1, Higashimita, Tama-ku, Kawasaki, Kanagawa, Japan; Graduate School of Decision Science and Technology, Tokyo Institute of Technology, Tokyo, Japan","Ishii, N., Faculty of Engineering, Kanagawa University, 3-27-1 Rokkakubashi, Kanagawa-ku, Yokoham, Japan; Takano, Y., School of Network Information, Senshu University, 2-1-1, Higashimita, Tama-ku, Kawasaki, Kanagawa, Japan; Muraki, M., Graduate School of Decision Science and Technology, Tokyo Institute of Technology, Tokyo, Japan","The cost estimation process, carried out by the contractor before the start of a project, is a critical activity for the contractor in accepting profitable EPC projects in competitive bidding situations. Thus, the contractor should devote significant time and resources to the accurate cost estimation of project orders from clients. However, it is impossible for any contractor to devote enough time and resources to all the orders because such resources are usually limited. For this reason, the contractor must dynamically decide bid or no-bid on the orders at each order arrival, and allocate the limited resources to the chosen orders. To maximize the contractor,s profits, this study devises a heuristic scheduling method for dynamically selecting orders and allocating the limited resources to them, on the basis of the resource requirement of the order, the contractor,s resource utilization, and the expected profit from the order. The effectiveness of our method is demonstrated through simulation experiments using a project cost estimation process model. å© Copyright 2016 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.",Competitive Bidding; Discrete Event Simulation; Project Selection; Resource Allocation,Conference Paper,Scopus,2-s2.0-84991225588
"Irshad M., Torkar R., Petersen K., Afzal W.",Capturing cost avoidance through reuse: Systematic literature review and industrial evaluation,2016,ACM International Conference Proceeding Series,01-03-June-2016,, a35,,,no,,,,10.1145/2915970.2915989,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978540065&doi=10.1145%2f2915970.2915989&partnerID=40&md5=96a74756ff73edff71ce2026000abd20,"Blekinge Institute of Technology, Karlskrona, Sweden; Chalmers University of Technology, Gothenburg, Sweden; Malardalen University, Box 883, VÌ_steras, Sweden","Irshad, M., Blekinge Institute of Technology, Karlskrona, Sweden; Torkar, R., Chalmers University of Technology, Gothenburg, Sweden; Petersen, K., Blekinge Institute of Technology, Karlskrona, Sweden; Afzal, W., Malardalen University, Box 883, VÌ_steras, Sweden","Background: Cost avoidance through reuse shows the benefits gained by the software organisations when reusing an artefact. Cost avoidance captures benefits that are not captured by cost savings e.g. spending that would have increased in the absence of the cost avoidance activity. This type of benefit can be combined with quality aspects of the product e.g. costs avoided because of defect prevention. Cost avoidance is a key driver for software reuse. Objectives: The main objectives of this study are: (1) To assess the status of capturing cost avoidance through reuse in the academia; (2) Based on the first objective, propose improvements in capturing of reuse cost avoidance, integrate these into an instrument, and evaluate the instrument in the software industry. Method: The study starts with a systematic literature review (SLR) on capturing of cost avoidance through reuse. Later, a solution is proposed and evaluated in the industry to address the shortcomings identified during the systematic literature review. Results: The results of a systematic literature review describe three previous studies on reuse cost avoidance and show that no solution, to capture reuse cost avoidance, was validated in industry. Afterwards, an instrument and a data collection form are proposed that can be used to capture the cost avoided by reusing any type of reuse artefact. The instrument and data collection form (describing guidelines) were demonstrated to a focus group, as part of static evaluation. Based on the feedback, the instrument was updated and evaluated in industry at 6 development sites, in 3 different countries, covering 24 projects in total. Conclusion: The proposed solution performed well in industrial evaluation. With this solution, practitioners were able to do calculations for reuse costs avoidance and use the results as decision support for identifying potential artefacts to reuse. å© 2016 ACM.",Cost avoidance; Cost savings; Software reuse,Conference Paper,Scopus,2-s2.0-84978540065
"Imtiaz A., Burger T., Krenge J.",Value estimation framework for collaborative knowledge workspaces,2010,"2010 IEEE International Technology Management Conference, ICE 2010",,,7477039,,,no,,,,10.1109/ICE.2010.7477039,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978119366&doi=10.1109%2fICE.2010.7477039&partnerID=40&md5=891898a2609cf5480bbed41cc15badb4,"Forschungsinstitut fÌ_r Rationalisierung E.V., Pontdriesch 14/16, Aachen, Germany; Salzburg Research, Jakob-Haringer-Str. 5/III, Salzburg, Austria","Imtiaz, A., Forschungsinstitut fÌ_r Rationalisierung E.V., Pontdriesch 14/16, Aachen, Germany; Burger, T., Salzburg Research, Jakob-Haringer-Str. 5/III, Salzburg, Austria; Krenge, J., Forschungsinstitut fÌ_r Rationalisierung E.V., Pontdriesch 14/16, Aachen, Germany","The paper presents an implementation of EVEKS, a framework developed for value estimation within enterprise knowledge workspaces. The core of the paper is to show EVEKS in a scenario to illustrate its application in an enterprise. For that purpose, a scenario will be created and all steps of EVEKS will be executed. In the first pillar, functionalities of the software about to be deployed will be identified. Afterwards assets in the company being value drivers will be generated and set into relation to the functionalities in the second pillar. The third pillar consists of a weighting of the interrelations found in the pillar before. The last pillar performed in this section will be pillar 4, the assessment of the value of the ontology. å© 2010 IEEE.",Collaboration; Enterprise; Value; Workspaces,Conference Paper,Scopus,2-s2.0-84978119366
"Ibrahim O., El-Seoud M.S.A.",Towards formally prioritizing the activities of group course work inside student teams,2017,Advances in Intelligent Systems and Computing,545,,,210,220,no,,,,10.1007/978-3-319-50340-0_17,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010046629&doi=10.1007%2f978-3-319-50340-0_17&partnerID=40&md5=c58f2efaf74e6d6772aed9d05d45701e,"Faculty of Informatics and Computer Science, The British University in Egypt, Cairo, Egypt","Ibrahim, O., Faculty of Informatics and Computer Science, The British University in Egypt, Cairo, Egypt; El-Seoud, M.S.A., Faculty of Informatics and Computer Science, The British University in Egypt, Cairo, Egypt","Learning to prioritize different activities effectively can help many students to become more efficient towards their work, managing time, energy, and stress. This paper presents an automated tool that helps students and teaches them how, as a team, can prioritize group work. The tool is based on a formal prioritization mechanism that encompasses a model and related process that takes into account the relative importance of each activity from each individual of the group. From individual judgment, the tool works out a prioritized list that reflects the group prioritization. The model is based on a sound mathematical basis to provide the automated support for the prioritization process that will be conducted by each team member while keeping the subjective viewpoint of each member. As a final step, the tool automatically computes the group final prioritized list of activities that reflects the team consensus. å© Springer International Publishing AG 2017.",Automated support; Group work prioritization; Mathematical modeling; Software tools,Conference Paper,Scopus,2-s2.0-85010046629
"Huijgens H., Vogelezang F.",Do estimators learn? on the effect of a positively skewed distribution of effort data on software portfolio productivity,2016,"Proceedings - 7th International Workshop on Emerging Trends in Software Metrics, WETSoM 2016",,,,8,14,no,,,,10.1145/2897695.2897698,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974575554&doi=10.1145%2f2897695.2897698&partnerID=40&md5=60a66ed315ac86dfa800bc3b20426c4d,"Delft University of Technology and Goverdson, Delft, Netherlands; Ordina, Nieuwegein, Netherlands","Huijgens, H., Delft University of Technology and Goverdson, Delft, Netherlands; Vogelezang, F., Ordina, Nieuwegein, Netherlands","We study whether an assumed positively skewed distribution of effort data prevents software estimators to learn over time; leading to increasing differences between planned and actual effort and a deteriorating (worsening) trend on productivity. We analyze data of 25 software releases of one application, collected over a period of six years in a public sector institution in The Netherlands. We statistically test for distribution, trend on differences between planned versus actual effort over time, and productivity of software portfolios. The key contributions of this paper are that we show that a proposed assumption that assumes any relation between a positively skewed distribution of effort data and a deteriorating productivity is not applicable to the subject dataset. We find that the effort data is to be characterized as positively skewed distributed, and we do see a shift over time from under-estimation to over-estimation. We do not find evidence for a deteriorating productivity; on the contrary productivity improves over time, indicating that estimators in the subject organization did learn. å© 2016 ACM.",Function Point Analysis; Software Economics; Software Estimation,Conference Paper,Scopus,2-s2.0-84974575554
Huijgens H.,Evidence-based software portfolio management: A tool description and evaluation,2016,ACM International Conference Proceeding Series,01-03-June-2016,, a17,,,no,,,,10.1145/2915970.2916012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978499578&doi=10.1145%2f2915970.2916012&partnerID=40&md5=836ae4cc61159ffc9979005d6a0cf39c,"Delft University of Technology, Goverdson, Netherlands","Huijgens, H., Delft University of Technology, Goverdson, Netherlands","Context: In this paper we describe and evaluate a tool for Evidence-Based Software Portfolio Management (EBSPM) that we developed over time in close cooperation with software practitioners from The Netherlands and Belgium. Objectives: The goal of the EBSPM-tool is to measure, analyze, and benchmark the performance of interconnected sets of software projects in terms of size, cost, duration, and number of defects, in order to support innovation of a company's software delivery capability. The tool supports building and maintaining a research repository of finalized software projects from different companies, business domains, and delivery approaches. Method: The tool consists of two parts. First, a Research Repository, at this moment holding data of for now 490 finalized software projects, from four different companies. Second, a Performance Dashboard, built from a so-called Cost Duration Matrix. Results: We evaluated the tool by describing its use in two practical applications in case studies in industry. Conclusions: We show that the EBSPM-tool can be used successfully in an industrial context, especially regarding its benchmarking and visualization purposes. å© 2016 ACM.",Benchmarking; EBSPM; EBSPM-tool; Evidence-based software portfolio management; Software portfolio management,Conference Paper,Scopus,2-s2.0-84978499578
"Hugues J., Delange J.",Model-based design and automated validation of ARINC653 architectures,2016,"Proceedings - IEEE International Symposium on Rapid System Prototyping, RSP",2016-February,,7416539,3,9,no,,,,10.1109/RSP.2015.7416539,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963759662&doi=10.1109%2fRSP.2015.7416539&partnerID=40&md5=d808ea6e9acc047d1919bd26b90a3cb6,"ISAE, SUPAERO, Universite de Toulouse, 10 Avenue E. Belin, Toulouse, France; Carnegie Mellon Software Engineering Institute, 4500 Fifth Avenue, Pittsburgh, PA, United States","Hugues, J., ISAE, SUPAERO, Universite de Toulouse, 10 Avenue E. Belin, Toulouse, France; Delange, J., Carnegie Mellon Software Engineering Institute, 4500 Fifth Avenue, Pittsburgh, PA, United States","Safety-Critical Systems as used in avionics systems are now extremely software-reliant. As these systems are life-or mission-critical, software must be carefully designed and certified according to stringent standards. One typical pitfalls of such project is the late detection of safety issues or bugs at integration time that impose to redo development steps. Model-Based Engineering aims at capturing system concerns with a specific notations and use models to drive the development process through all its phases-design, validation, implementation and ultimately, certification. Through a single consistent notation, such an approach would avoid undefined assumption and traditional hurdles due to informal, text-based, specifications. In this paper, we present recent contributions we pushed forward in the AADL architecture description language for the design and validation of Integrated Modular Avionics systems. First, we review modeling patterns to support abstractions for IMA systems. We then introduce capabilities to check all ARINC653 patterns are enforced at model-level. In addition, we review errror modeling and safety analysis capabilities towards the production of safety reports conforming to ARP4761 recommandations. å© 2015 IEEE.",,Conference Paper,Scopus,2-s2.0-84963759662
"Huang J., Sun H.",Grey Relational Analysis Based k Nearest Neighbor Missing Data Imputation for Software Quality Datasets,2016,"Proceedings - 2016 IEEE International Conference on Software Quality, Reliability and Security, QRS 2016",,,7589788,86,91,no,Faults,,,10.1109/QRS.2016.20,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995467585&doi=10.1109%2fQRS.2016.20&partnerID=40&md5=53989c3d3790af2bed2bd72809247d01,"Department of Systems Engineering and Engineering Management, City University of Hong Kong, Hong Kong, Hong Kong","Huang, J., Department of Systems Engineering and Engineering Management, City University of Hong Kong, Hong Kong, Hong Kong; Sun, H., Department of Systems Engineering and Engineering Management, City University of Hong Kong, Hong Kong, Hong Kong","Software quality estimation is important yet difficult in software engineering studies. Historical quality datasets are used to build classification models for estimating fault-proneness. However, the missing values in the datasets severely affect the estimation ability and therefore, cause inconclusive decision-making. Among the single imputation approaches, k nearest neighbor (kNN) imputation is popular in empirical studies due to the relatively high accuracy. However, researchers are still calling for the optimal parameter setting of kNN imputation. In this study, a novel grey relational analysis based incomplete-instance kNN imputation is built for software quality data. An evaluation is conducted on four quality datasets with different simulated missingness scenarios to analyze the performance of the proposed imputation. The empirical results show that the proposed approach is superior to traditional kNN imputation and mean imputation in most cases. Moreover, the classification accuracy can be maintained or even improved by using this approach in classification tasks. å© 2016 IEEE.",empirical software engineering estimation; imputation; kNN; missing data,Conference Paper,Scopus,2-s2.0-84995467585
"Huang G., Chaiprapat S., Waiyagan K.",Automated process planning and cost estimation under material quality uncertainty,2016,International Journal of Advanced Manufacturing Technology,86,1-Apr,,323,335,no,,,,10.1007/s00170-015-8180-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949485248&doi=10.1007%2fs00170-015-8180-1&partnerID=40&md5=3bcec008df93a1c0a5cee772882b64a4,"Department of Industrial Engineering, Faculty of Engineering, Prince of Songkla University, Hat Yai, Songkhla, Thailand; Department of Agro-Industrial Technology, Faculty of Agro-Industry, Prince of Songkla University, Hat Yai, Songkhla, Thailand","Huang, G., Department of Industrial Engineering, Faculty of Engineering, Prince of Songkla University, Hat Yai, Songkhla, Thailand; Chaiprapat, S., Department of Industrial Engineering, Faculty of Engineering, Prince of Songkla University, Hat Yai, Songkhla, Thailand; Waiyagan, K., Department of Agro-Industrial Technology, Faculty of Agro-Industry, Prince of Songkla University, Hat Yai, Songkhla, Thailand","In mass customization production environment, once an order is placed, a product quotation must be fed back to the customer in a relatively short time. Estimation of the product cost, however, can only be performed upon finalization of process planning. In wooden product manufacturing, optimization of process planning is complicated due to the uncertainty of raw material quality. Manual arrangement of process parameters always leads to non-optimality. In this study, an automated system was developed to shorten the process lead time, to ensure an optimal cutting process plan, and to estimate accurate material costs. The architectural structure of the proposed system is composed of three modules: (1) a digital image processing module to accelerate product recognition; (2) a cutting plan generation module to evaluate raw materials for their applicability, allowing a set of feasible plans to establish; and (3) an optimization module to quantify the amount of material loss. A process plan which satisfies a preferred objective function (either minimum cost or minimum loss) could subsequently be derived. The model developed in this study exhibits superior performance in predicting loss and estimating material cost over the other reported models. å© 2015, Springer-Verlag London.",Cost estimation; Mass customization; Process planning; Uncertainty; Wood loss,Article,Scopus,2-s2.0-84949485248
"Hu C.-Z., Xie N.-M., Yin S.-M.",Civil Aircraft Cost Drive Parameters Selecting Method Based on Grey Clustering Model,2015,"Proceedings - 2015 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2015",,,7379253,635,640,no,,,,10.1109/SMC.2015.121,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964499356&doi=10.1109%2fSMC.2015.121&partnerID=40&md5=5268420b2bb70b4c51fb3e0126dd84f9,"College of Economics and Management, Nanjing University of Aeronautics and Astronautics, Nanjing, China","Hu, C.-Z., College of Economics and Management, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Xie, N.-M., College of Economics and Management, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Yin, S.-M., College of Economics and Management, Nanjing University of Aeronautics and Astronautics, Nanjing, China","The scientific cost drive Parameters identification and screening can improve the civil airplane development cost estimation accuracy, and then realize control and management of the development cost effectively. According to the estimation of the conceptual design stage of civil aircraft cost is studied, considering the lack of the information of civil aircraft cost parameters, parameters have been collected exist the problem of Multicollinearity, firstly, through qualitative identification method of civil aircraft cost parameter system and construct a generalized grey relative correlation model and relational clustering model combination method of screening cost drive parameters, algorithm steps are given; and collected to verify the model at home and abroad of 9 different kinds of civil aircraft information, the result shows that the proposed model can effectively screening exit key cost drive parameters. The results show that the prediction method established in this paper has improved significantly on the accuracy, illustrating the effectiveness and scientific of this method. å© 2015 IEEE.",cost drive parameter; generalized grey relative correlation; Grey clustering model; parameter system,Conference Paper,Scopus,2-s2.0-84964499356
Hou C.,Research on the construction of engineering cost estimation system based on fuzzy prediction technology,2016,RISTI - Revista Iberica de Sistemas e Tecnologias de Informacao,2016,17,,129,139,no,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963670096&partnerID=40&md5=2a93fdff87fc74ea825f131879f814b9,"Shijiazhuang Vocational Technology Institute, Shijiazhuang, Hebei, China","Hou, C., Shijiazhuang Vocational Technology Institute, Shijiazhuang, Hebei, China","With the development of market economy in our country, the project cost budget has been more and more complicated, and it's an important link to establish a real and reliable factor market price and cost index database, choose feasible forecast method, and improve the accuracy of project cost prediction. For such engineering problems, this paper introduces the general situation of the engineering cost and the function of the construction cost, and establishes the fuzzy exponential smoothing forecasting model, combining with the fuzzy forecast technology, at the same time, the engineering cost management information system is constructed. And finally the fuzzy forecast technology of engineering cost is verified from the aspect of highway infrastructure construction, through the analysis of experimental data it's found that fuzzy prediction technique can effectively evaluate the engineering construction.",Cost index database; Fuzzy forecast technology; Market economy; Project cost budget,Article,Scopus,2-s2.0-84963670096
"Hira A., Sharma S., Boehm B.",Calibrating COCOMOå¨ II for projects with high personnel turnover,2016,"Proceedings - International Conference on Software and System Process, ICSSP 2016",,,2904367,51,55,no,,,,10.1145/2904354.2904367,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974588902&doi=10.1145%2f2904354.2904367&partnerID=40&md5=d35b005251113c865dfd6d046329eba4,"University of Southern California, 941 W 37th Pl, SAL 329, Los Angeles, CA, United States; University of Southern California, 941 W 37th Pl, SAL 330, Los Angeles, CA, United States","Hira, A., University of Southern California, 941 W 37th Pl, SAL 329, Los Angeles, CA, United States; Sharma, S., University of Southern California, 941 W 37th Pl, SAL 329, Los Angeles, CA, United States; Boehm, B., University of Southern California, 941 W 37th Pl, SAL 330, Los Angeles, CA, United States","Software cost and effort estimation is a necessary step in the software development lifecycle to track progress, manage resources, and negotiate. Though many accepted cost models exist, local calibration results in more accurate estimates. Locally calibrating Unified Code Count (UCC)'s dataset based on COCOMO (Constructive Cost Model)å¨ II helped UCC's development team learn which factors affected the effort, the amount of fixed costs associated with training new personnel and required deliverables, and resulted in a well-fitting effort estimation model. These insights give the development team a better understanding of the environment and where improvements are most necessary and possible. å© 2016 ACM.",COCOMOå¨ II; Cost estimation; Effort estimation; Fixed costs; Local calibration; Personnel turnover,Conference Paper,Scopus,2-s2.0-84974588902
"Hira A., Boehm B.",Function Point Analysis for Software Maintenance,2016,International Symposium on Empirical Software Engineering and Measurement,08-09-September-2016,, a48,,,no,,,,10.1145/2961111.2962613,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991593721&doi=10.1145%2f2961111.2962613&partnerID=40&md5=9b3c53222364b8a2a7263ed8dac52f5f,"University of Southern California, Los Angeles, United States","Hira, A., University of Southern California, Los Angeles, United States; Boehm, B., University of Southern California, Los Angeles, United States","Context: Software maintenance is required to fix defects, adapt to changes in the environment, and meet new or changed user requirements. The effort of these tasks need to be estimated to track progress, manage resources, and make decisions. Most widely used cost models use source lines of code (SLOC) as the software size input measure, due to its quantifiability and high correlation with effort. Estimating the SLOC of a project is very difficult in early stages of the software lifecycle. Function Points (FPs) represents software size by functions or modifications to functions, making them easier to calculate early in the lifecycle for new development projects or maintenance tasks. Several cost estimators use FPs to estimate the SLOC of a project to take advantage of existing cost models. Goal: Through empirical analysis, the authors want to determine whether FPs can effectively estimate maintenance tasks, as a better alternative to using SLOC as a software size metric. Additionally, the authors will demonstrate that FPs to SLOC ratios add uncertainty to effort estimates. Method: The empirical analysis will be run on Unified Code Count (UCC)'s dataset, a software tool maintained by University of Southern California (USC). Results: The analyses found that separating projects adding new functions from those modifying existing functions resulted in improved estimation models using FPs. The effort estimation model for projects adding functions to UCC had high prediction accuracy statistics, but less impressive results for projects modifying existing functions in UCC. The effort estimation accuracy became unsatisfactorily low when using a FPs to SLOC ratio. Conclusions: Cost estimators should not use FPs to SLOC ratios for effort estimation due to low prediction accuracy. FPs is only an effective size measure for a portion of UCC's maintenance tasks-specifically for the projects adding new functions to UCC. Another size measure may need to be considered that might be more effective independently or in conjunction with FPs for all of UCC's maintenance tasks. å© 2016 ACM.",Backfiring; Cost Estimation; Effort Estimation; Function Point Analysis; Local Calibration; Project Management; Software Maintenance,Conference Paper,Scopus,2-s2.0-84991593721
"Hira A., Boehm B.",Using Software Non-Functional Assessment Process to Complement Function Points for Software Maintenance,2016,International Symposium on Empirical Software Engineering and Measurement,08-09-September-2016,, a50,,,no,,,,10.1145/2961111.2962615,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991705903&doi=10.1145%2f2961111.2962615&partnerID=40&md5=80df0aca7491b47c35f5e70f2a09e737,"University of Southern California, Los Angeles, CA, United States","Hira, A., University of Southern California, Los Angeles, CA, United States; Boehm, B., University of Southern California, Los Angeles, CA, United States","Context: Most widely used cost models use source lines of code (SLOC) as the software size input measure, due to its quantifiability and high correlation with effort. Estimating the SLOC of a project is very difficult in early stages of the software lifecycle, especially for software maintenance tasks. Depending on the reuse model being used, one would need to size the existing code that needs modifications and the size of the changes being made in SLOC. Functional size measures, such as Function Points (FPs) and the Software Non-functional Assessment Process (SNAP), have been developed to improve the ability to estimate project size early in the lifecycle for both development and maintenance projects. While FPs represent software size by functions; SNAP complements FPs by sizing non-functional requirements, such as data operations and interface design. Goal: SNAP complements Function Points by sizing non-functional requirements, such as data operations and interface design. Through an empirical analysis, the authors want to determine whether SNAP might be an effective software size measure individually or in conjunction with FPs to improve effort estimation accuracy. Method: The empirical analysis will be run on Unified Code Count (UCC)'s dataset, a software tool maintained by University of Southern California (USC). Results: The analyses found that separating projects adding new functions from those modifying existing functions resulted in improved estimation models using SNAP. The effort estimation model for projects modifying functions in UCC had high prediction accuracy statistics, but less impressive results for projects adding existing functions to UCC. The effort estimation accuracy were satisfactory when using SNAP in conjunction with FPs for both groups of projects. Conclusions: SNAP, indeed, complements FPs in terms of the requirements that are considered and sized. Both size metrics should be treated as individual metrics, but can be used together for acceptably accurate cost models in UCC's development environment. å© 2016 ACM.",Cost Estimation; Effort Estimation; Function Point Analysis; Local Calibration; Project Management; SNAP; Software Maintenance; Software Non-Functional Assessment Process,Conference Paper,Scopus,2-s2.0-84991705903
"Heradio R., Perez-Morago H., Fernandez-Amoros D., Javier Cabrerizo F., Herrera-Viedma E.",A bibliometric analysis of 20 years of research on software product lines,2016,Information and Software Technology,72,,,1,15,no,,,4,10.1016/j.infsof.2015.11.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958657669&doi=10.1016%2fj.infsof.2015.11.004&partnerID=40&md5=0c077b201a23a4e9a410e3a7191dcdd5,"Department of Software Engineering and Computer Systems, Universidad Nacional de Educacion A Distancia(UNED), Madrid, Spain; Department of Computer Science and Artificial Intelligence, University of Granada, Granada, Spain","Heradio, R., Department of Software Engineering and Computer Systems, Universidad Nacional de Educacion A Distancia(UNED), Madrid, Spain; Perez-Morago, H., Department of Software Engineering and Computer Systems, Universidad Nacional de Educacion A Distancia(UNED), Madrid, Spain; Fernandez-Amoros, D., Department of Software Engineering and Computer Systems, Universidad Nacional de Educacion A Distancia(UNED), Madrid, Spain; Javier Cabrerizo, F., Department of Software Engineering and Computer Systems, Universidad Nacional de Educacion A Distancia(UNED), Madrid, Spain; Herrera-Viedma, E., Department of Computer Science and Artificial Intelligence, University of Granada, Granada, Spain","Context: Software product line engineering has proven to be an efficient paradigm to developing families of similar software systems at lower costs, in shorter time, and with higher quality. Objective: This paper analyzes the literature on product lines from 1995 to 2014, identifying the most influential publications, the most researched topics, and how the interest in those topics has evolved along the way. Method: Bibliographic data have been gathered from ISI Web of Science and Scopus. The data have been examined using two prominent bibliometric approaches: science mapping and performance analysis. Results: According to the study carried out, (i) software architecture was the initial motor of research in SPL; (ii) work on systematic software reuse has been essential for the development of the area; and (iii) feature modeling has been the most important topic for the last fifteen years, having the best evolution behavior in terms of number of published papers and received citations. Conclusion: Science mapping has been used to identify the main researched topics, the evolution of the interest in those topics and the relationships among topics. Performance analysis has been used to recognize the most influential papers, the journals and conferences that have published most papers, how numerous is the literature on product lines and what is its distribution over time. å© 2015 Elsevier B.V. All rights reserved.",Bibliometrics; Performance analysis; Science mapping; Software product lines,Article,Scopus,2-s2.0-84958657669
"Henpraserttae S., Leong H.W., Ratprasatpon P., Moolgate J., Toochinda P.",Effect of acidic sites of support to nickle catalysts for ammonia decomposition,2016,"2016 2nd Asian Conference on Defence Technology, ACDT 2016",,,7437663,167,169,no,,,,10.1109/ACDT.2016.7437663,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966577640&doi=10.1109%2fACDT.2016.7437663&partnerID=40&md5=4029d5e621d79e9722bd1e4fa6414e0e,"School of Bio-chemical Engineering and Technology, Sirindhorn International Institute of Technology, Thammasat University, Pathumthani, Thailand","Henpraserttae, S., School of Bio-chemical Engineering and Technology, Sirindhorn International Institute of Technology, Thammasat University, Pathumthani, Thailand; Leong, H.W., School of Bio-chemical Engineering and Technology, Sirindhorn International Institute of Technology, Thammasat University, Pathumthani, Thailand; Ratprasatpon, P., School of Bio-chemical Engineering and Technology, Sirindhorn International Institute of Technology, Thammasat University, Pathumthani, Thailand; Moolgate, J., School of Bio-chemical Engineering and Technology, Sirindhorn International Institute of Technology, Thammasat University, Pathumthani, Thailand; Toochinda, P., School of Bio-chemical Engineering and Technology, Sirindhorn International Institute of Technology, Thammasat University, Pathumthani, Thailand",Ammonia decomposition is an interesting reaction for clean H2 production. Ni-based catalysts were improved by doping of ZrO2 in Al2O3 support that can enhance the catalytic activity of catalyst for ammonia decomposition. The highest hydrogen production rate was obtained from Ni/ZrO2-Al2O3 catalyst at 873 K with 98% of NH3 conversion. The characteristics of supports were investigated using BET and TPD-NH3 techniques. The surface area of Ni on catalysts was also investigated using CO-pulse technique. These data clearly explain the high catalytic activities of Ni/ZrO2-Al2O3 catalyst. å© 2016 IEEE.,Acidic sites; NH3 decomposition; Ni based catalyst; ZrO2 doped Al2O3,Conference Paper,Scopus,2-s2.0-84966577640
"Heberle F., BrÌ_ggemann D.",Thermo-economic analysis of zeotropic mixtures and pure working fluids in Organic Rankine Cycles for waste heat recovery,2016,Energies,9,4,226,,,no,,,2,10.3390/en9040226,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963620689&doi=10.3390%2fen9040226&partnerID=40&md5=8ed9b2a108e349d212ee8818e02d990d,"Institute of Engineering Thermodynamics (LTTT), Center of Energy Technology (ZET), University of Bayreuth, Bayreuth, Germany","Heberle, F., Institute of Engineering Thermodynamics (LTTT), Center of Energy Technology (ZET), University of Bayreuth, Bayreuth, Germany; BrÌ_ggemann, D., Institute of Engineering Thermodynamics (LTTT), Center of Energy Technology (ZET), University of Bayreuth, Bayreuth, Germany","We present a thermo-economic analysis of an Organic Rankine Cycle (ORC) for waste heat recovery. A case study for a heat source temperature of 150 å¡C and a subcritical, saturated cycle is performed. As working fluids R245fa, isobutane, isopentane, and the mixture of isobutane and isopentane are considered. The minimal temperature difference in the evaporator and the condenser, as well as the mixture composition are chosen as variables in order to identify the most suitable working fluid in combination with optimal process parameters under thermo-economic criteria. In general, the results show that cost-effective systems have a high minimal temperature difference ëÓTPP,C at the pinch-point of the condenser and a low minimal temperature difference ëÓTPP,E at the pinch-point of the evaporator. Choosing isobutane as the working fluid leads to the lowest costs per unit exergy with 52.0 ‰âÂ/GJ (ëÓTPP,E = 1.2 K; ëÓTPP,C = 14 K). Considering the major components of the ORC, specific costs range between 1150 ‰âÂ/kW and 2250 ‰âÂ/kW. For the zeotropic mixture, a mole fraction of 90% isobutane leads to the lowest specific costs per unit exergy. A further analysis of the ORC system using isobutane shows high sensitivity of the costs per unit exergy for the selected cost estimation methods and for the isentropic efficiency of the turbine. å© 2016 by the authors.",Energy conversion systems; Organic Rankine Cycle (ORC); Thermo-economic analysis; Waste heat recovery; Working fluids; Zeotropic mixtures,Article,Scopus,2-s2.0-84963620689
"He Q., Kamineni R., Zhang Z.",Traffic signal control with partial grade separation for oversaturated conditions,2016,Transportation Research Part C: Emerging Technologies,71,,,267,283,no,,,1,10.1016/j.trc.2016.08.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982131520&doi=10.1016%2fj.trc.2016.08.001&partnerID=40&md5=f97b80a846e5da6764bc5b6002184b08,"Department of Civil, Structural and Environmental Engineering, University at Buffalo, The State University of New York, Buffalo, NY, United States; Department of Industrial and Systems Engineering, University at Buffalo, The State University of New York, Buffalo, NY, United States","He, Q., Department of Civil, Structural and Environmental Engineering, University at Buffalo, The State University of New York, Buffalo, NY, United States, Department of Industrial and Systems Engineering, University at Buffalo, The State University of New York, Buffalo, NY, United States; Kamineni, R., Department of Civil, Structural and Environmental Engineering, University at Buffalo, The State University of New York, Buffalo, NY, United States; Zhang, Z., Department of Civil, Structural and Environmental Engineering, University at Buffalo, The State University of New York, Buffalo, NY, United States","Increasing individual vehicular traffic is a major concern all around the world. This leads to more and more oversaturated intersections. Traffic signal control under oversaturated condition is a long-lasting challenge. To address this challenge thoroughly, this paper introduces grade separation at signalized intersections. A lane-based optimization model is developed for the integrated design of grade-separated lanes (e.g. tunnels), lane markings (e.g. left turns, through traffic, right turns, etc.) and signal timing settings. We take into account two types of lane configurations. One is conventional surface lanes controlled by signals, and the other is grade-separated lanes. This problem is formulated as a Mixed Integer Linear Program (MILP), and this can be solved using the regular branch-and-bound methods. The integer decision variables help in finding if the movement is on grade-separated or surface lanes, and also the successor functions to govern the order of signal display. The continuous variables include the assigned lane flow, common flow multiplier, cycle length, and start and duration of green for traffic movements and lanes. The optimized signal time settings and lane configurations are then represented in Vissim simulation. Numerical examples, along with a benefit-cost analysis show the good savings of the proposed optimization model for oversaturated traffic conditions. The benefit-cost ratio for installing 4 grade-separated lanes (as a tunnel) at a heavily oversaturated intersection (intersection capacity utilization rate equal to 1.57) exceeds 5.4. å© 2016 Elsevier Ltd",Mixed-integer programming; Oversaturated traffic; Partial grade separation; Traffic signal control,Article,Scopus,2-s2.0-84982131520
"Haoues M., Sellami A., Ben-Abdallah H.",Functional change impact analysis in use cases: An approach based on COSMIC functional size measurement,2017,Science of Computer Programming,135,,,88,104,no,,,,10.1016/j.scico.2016.09.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008953501&doi=10.1016%2fj.scico.2016.09.005&partnerID=40&md5=7c58d6e97f2c21dac7aad817cbbc5dc1,"Mir@cl Laboratory, University of Sfax, Tunisia; King Abdulaziz University, Jeddah, Saudi Arabia","Haoues, M., Mir@cl Laboratory, University of Sfax, Tunisia; Sellami, A., Mir@cl Laboratory, University of Sfax, Tunisia; Ben-Abdallah, H., King Abdulaziz University, Jeddah, Saudi Arabia","Context: Because functional changes are inevitable throughout the software life-cycle, it is essential to assess and evaluate whether a Functional Change (FC) can be handled within the estimated project budget and time. Thus, a well-defined change measure and control process is vital for the success of the development project. In fact, the size measurement is widely used for effort and cost estimation. This paper explores the COSMIC Functional Size Measurement (FSM) method to evaluate a FC. Objective: The objective of this paper is to assess a FC in terms of COSMIC Function Point units in order to identify changes incurring potential impact on the software development progress. Thus, we suppose that the functional size of a FC is one of the factors that must be taken into consideration to analyze the impact of changes on the development progress. This helps managers in making decisions to answer a FC request. Method: The method proposed in this paper analyzes the FC impact in use case diagrams, the de-facto standard for modeling the Functional User Requirements (FUR). It distinguishes between internal and external FC to a use case. It proposes to quantify the FC impact on the functional size of a use case diagram in terms of COSMIC Function Point (CFP) units. In addition, it evaluates the status of a FC according to its functional size. Furthermore, we propose an algorithm for prioritizing changes in the case where more than one functional change is proposed. The algorithm is based on a set of heuristics where the objective is to minimize the effort required to answer the changes. It accounts for the most important factors when prioritizing changes (the functional size of the FC, and the preference of the change requester). These two factors are identified based on an investigation with the international COSMIC community. Originality: Several techniques have been used to identify and analyze the impact of a FC during various phases of the software life-cycle. For example, some researches used the colored commit graphs (at the implementation phase), other studies used the consistency rules between UML diagrams (at the design phase), etc. Compared to other approaches focused on impact change analysis, our proposed approach uses the COSMIC-FSM method. Quantifying the functional size of a FC in terms of CFP units allows us to evaluate more realistically the impact of a FC on the software development progress. Results: We propose six categories of FC status according to their sizes. Based on the FC status, it is possible to identify the FC impact on the software development progress. The FC status will help not only designers/developers in analyzing the impact of FC on the size of their work products but also managers to decide to accept or forgo a FC and maintain the development schedule. Moreover, the experiment conducted with 30 computer engineering students showed that students‰Ûª ratings can be approximately evaluated when measuring FUR in terms of CFP units based on the FC status. Conclusion: Based on the finding of this study, it is now feasible to identify FC leading to potential impact on the software development progress. More specifically, evaluating FC in the use case diagrams will help in making quick decisions about accepting or rejecting a FC. Regardless of the used software life-cycle process, use cases are popular for specifying the software functionality. In practice, our proposed approach is applicable not only in the software development industry but also in academia. å© 2016 Elsevier B.V.",COSMIC; Functional change; Functional size measurement; Impact analysis; Requirements change; Use case description,Article,Scopus,2-s2.0-85008953501
"Hanusse N., Wanko P.K., Maabout S.",Using histograms for skyline size estimation,2016,ACM International Conference Proceeding Series,11-13-July-2016,,,125,134,no,,,,10.1145/2938503.2938531,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989233958&doi=10.1145%2f2938503.2938531&partnerID=40&md5=475202303053c49b10b251df523d69ae,"LaBRI, UniversitÌ© de Bordeaux, CNRS, France","Hanusse, N., LaBRI, UniversitÌ© de Bordeaux, CNRS, France; Wanko, P.K., LaBRI, UniversitÌ© de Bordeaux, CNRS, France; Maabout, S., LaBRI, UniversitÌ© de Bordeaux, CNRS, France","Let T be a table of n points described by a set of d attributes/ dimensions. Let p; q 2 T. p dominates q iff it is better than q in every dimension and there exists at least one attribute for which p is strictly better than p. p is a skyline point of T iff it is not dominated by any point of T. A skyline query returns the set of all skyline points. In order to integrate Skyline queries into database management systems, deriving an estimation of the skyline cardinality is important for query optimization purposes. We propose techniques for estimating skyline cardinality when data distribution is known. We first provide an unbiased estimator which requires one traversal of the whole data which is much faster than computing the exact skyline. Then, we show that this estimator can be used on a sample of the underlying data while preserving the estimation quality, i.e., it is still unbiased. Next, we provide a convergent estimator which does not require any data access but the data distribution. It estimates skyline cardinality expectation for those data sets respecting data distribution. The advantages of these solutions are their ease of implementation and, by contrast to other proposals, no costly subskyline queries are required. Our solutions are implemented and some experiments are reported showing both the accuracy of the estimations and the execution time efficiency by which they are obtained. å© ACM 2016.",Algorithms; Estimation; Expectation; Sampling; Skyline cardinality,Conference Paper,Scopus,2-s2.0-84989233958
"Hallowell M.R., Hardison D., Desvignes M.",Information technology and safety,2016,Construction Innovation,16,3,,323,347,no,,,,10.1108/CI-09-2015-0047,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978245433&doi=10.1108%2fCI-09-2015-0047&partnerID=40&md5=5eb4f891d690b4d0244a19b0e9ba7e00,"Civil, Environmental, and Architectural Engineering Department, University of Colorado at Boulder, Boulder, CO, United States; Construction Engineering Management Department, University of Colorado at Boulder, Boulder, CO, United States","Hallowell, M.R., Civil, Environmental, and Architectural Engineering Department, University of Colorado at Boulder, Boulder, CO, United States; Hardison, D., Construction Engineering Management Department, University of Colorado at Boulder, Boulder, CO, United States; Desvignes, M., Civil, Environmental, and Architectural Engineering Department, University of Colorado at Boulder, Boulder, CO, United States","Purpose - The architecture, engineering and construction industry is known to account for a disproportionate rate of disabling injuries and fatalities. Information technologies show promise for improving safety performance. This paper aims to describe the current state of knowledge in this domain and introduces a framework to integrate attribute-level safety risk data within existing technologies for the first time. Design/methodology/approach - The framework is demonstrated by integrating attribute safety risk data with information retrieval, location and tracking systems, augmented reality and building information models. Findings - Fundamental attributes of a work environment can be assigned to construction elements during design and planning. Once assigned, existing risk and predictive models can be leveraged to provide a user with objective, empirically driven feedback including quantity of safety risk, predictions of safety outcomes and clashes among incompatible attributes. Practical implications - This framework can provide designers, planners and managers with unbiased safety feedback that increases in detail and accuracy as the project develops. Such information can support prevention through design and safety management in advanced work packaging. Originality/value - The framework is the first to integrate empirical risk-based safety data with construction information technologies. The results provide users with insight that is unexpected, counter-intuitive or otherwise thought-provoking. å© Emerald Group Publishing Limited.",Health and safety; IT building design per cent 26 construction; IT management; IT strategies; IT/CAD/VR; IT/Design,Article,Scopus,2-s2.0-84978245433
"Gupta S., Sharma K.",Optimization of revenue generated by hydro power plant by Bat Algorithm(BA),2016,"Proceedings of the 10th INDIACom; 2016 3rd International Conference on Computing for Sustainable Global Development, INDIACom 2016",,,7724928,3576,3580,no,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997503444&partnerID=40&md5=a6bf6c8fd640ac0a946fe69d904ad832,"Department of Computer Engineering, Delhi Technological University, Delhi, India","Gupta, S., Department of Computer Engineering, Delhi Technological University, Delhi, India; Sharma, K., Department of Computer Engineering, Delhi Technological University, Delhi, India","Electricity is prime driver of growth and has varied range of uses spanning transportation, production of goods, industries, domestic uses and many. In countries, like India, the demand for electricity has always been more than the supply and hence the country face power shortage. Electricity is generated by non-renewable resources like coal, oil, fossil fuels, nuclear power and natural gas and from renewable resources like wind energy, hydroelectric power plants, biomass, solar energy, reservoirs. Hydroelectric power plants are major source of electricity in India with presently installed capacity of 41,997.42 MW as on July 31, 2015 which is 15.22% of total electricity generation in India. Hydroelectricity means production of electrical power through the use of the gravitational force of falling or flowing water. It is significant to optimize the revenue generated by the hydroelectric power plant to meet the implementation cost and for expansion of hydroelectric power plants. A lot of research work has been done in optimization of hydroelectric flow. In this paper, a new calibrated revenue optimization model is developed with Multi-Objective Bat Algorithm (MOBA) for hydro power plants. This model can be used to increase the revenue generated by the hydroelectric flow from the dam wih respect to given constraints in less time. Bat Algorithm is the latest metaheuristic algorithm which is based on the behaviour of bats. Bat Algorithm has received increased attention in many research fields recently. Bat algorithm based optimization model can predict the flow of water through turbine in hydroelectric power plant in order to maximize the revenue. å© 2016 IEEE.",Application of bat-algorithm; Bat algorithm; Electricity production; Hydroelectric flow; Optimization; Revenue maximization,Conference Paper,Scopus,2-s2.0-84997503444
"Guoda T., Huaguang Z., Jingge W., Hongjing L.",Output regulation for heterogeneous second-order linear multi-agent systems,2016,"2016 3rd International Conference on Informative and Cybernetics for Computational Social Systems, ICCSS 2016",,,7586457,235,240,no,,,,10.1109/ICCSS.2016.7586457,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994409978&doi=10.1109%2fICCSS.2016.7586457&partnerID=40&md5=8a7ea607597122950781058d9f04e265,"School of Information Science and Engineering, Northeastern University, Shenyang, Liaoning, China","Guoda, T., School of Information Science and Engineering, Northeastern University, Shenyang, Liaoning, China; Huaguang, Z., School of Information Science and Engineering, Northeastern University, Shenyang, Liaoning, China; Jingge, W., School of Information Science and Engineering, Northeastern University, Shenyang, Liaoning, China; Hongjing, L., School of Information Science and Engineering, Northeastern University, Shenyang, Liaoning, China","In this conference paper, we discuss a special kind of regulation problems for particular linear second-order control systems. At first, we propose a control protocol utilizing the information of agents themselves and their neighbours. Then we present a sufficient and necessary criterion for judging the stability of decentralized systems. Besides, we analyze the conditions that need to be satisfied for agents to track the output of the exosystem. In the end, examples and simulations are given to justify the control theory proposed by us. å© 2016 IEEE.",,Conference Paper,Scopus,2-s2.0-84994409978
"Guo Y., SpÌ_nola R.O., Seaman C.",Exploring the costs of technical debt management ‰ÛÒ a case study,2016,Empirical Software Engineering,21,1,,159,182,no,,,4,10.1007/s10664-014-9351-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955207062&doi=10.1007%2fs10664-014-9351-7&partnerID=40&md5=e6ced7ae7a319d762dcdf61414621a18,"Department of Information Systems, University of Maryland Baltimore County, Baltimore, MD, United States; Department of Systems and Computing, University of Salvador, Salvador, Bahia, Brazil; Fraunhofer Project Center for Software and System Engineering at Federal University of Bahia, Salvador, Bahia, Brazil","Guo, Y., Department of Information Systems, University of Maryland Baltimore County, Baltimore, MD, United States; SpÌ_nola, R.O., Department of Systems and Computing, University of Salvador, Salvador, Bahia, Brazil, Fraunhofer Project Center for Software and System Engineering at Federal University of Bahia, Salvador, Bahia, Brazil; Seaman, C., Department of Information Systems, University of Maryland Baltimore County, Baltimore, MD, United States","Technical debt is a metaphor for delayed software maintenance tasks. Incurring technical debt may bring short-term benefits to a project, but such benefits are often achieved at the cost of extra work in future, analogous to paying interest on the debt. Currently technical debt is managed implicitly, if at all. However, on large systems, it is too easy to lose track of delayed tasks or to misunderstand their impact. Therefore, we have proposed a new approach to managing technical debt, which we believe to be helpful for software managers to make informed decisions. In this study we explored the costs of the new approach by tracking the technical debt management activities in an on-going software project. The results from the study provided insights into the impact of technical debt management on software projects. In particular, we found that there is a significant start-up cost when beginning to track and monitor technical debt, but the cost of ongoing management soon declines to very reasonable levels. å© 2014, Springer Science+Business Media New York.",Case study; Cost; Decision making; Technical debt,Article,Scopus,2-s2.0-84955207062
"Govindan K., Khodaverdi R., Vafadarnikjoo A.",A grey DEMATEL approach to develop third-party logistics provider selection criteria,2016,Industrial Management and Data Systems,116,4,,690,722,no,,,4,10.1108/IMDS-05-2015-0180,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968610536&doi=10.1108%2fIMDS-05-2015-0180&partnerID=40&md5=f7615bbc9a31d69ead049e994da8b6b2,"Department of Technology and Innovation, University of Southern Denmark, Odense, Denmark; Faculty of Management and Accounting, Allame Tabataba'i University Business School (ATUBS), Tehran, Iran","Govindan, K., Department of Technology and Innovation, University of Southern Denmark, Odense, Denmark; Khodaverdi, R., Faculty of Management and Accounting, Allame Tabataba'i University Business School (ATUBS), Tehran, Iran; Vafadarnikjoo, A., Faculty of Management and Accounting, Allame Tabataba'i University Business School (ATUBS), Tehran, Iran","Purpose - Third-party logistics (3PL) plays a main role in supply chain management and, as a result, has experienced remarkable growth. The demand for 3PL providers has become a main approach for companies to offer better customer service, reduce costs, and gain competitive advantage. This paper identifies important criteria for 3PL provider selection and evaluation, and the purpose of this paper is to select 3PL providers from the viewpoint of firms which were already outsourcing their logistics services. Design/methodology/approach - This study utilized the grey decision-making trial and evaluation laboratory (DEMATEL) method to develop 3PL provider selection criteria. Because human judgments are vague and complicated to depict by accurate numerical values, the grey system theory is used to handle this problem. Findings - The findings revealed the structure and interrelationships between criteria and identified the main criteria for 3PL provider selection. The most important criteria for 3PL provider selection are on time delivery performance, technological capability, financial stability, human resource policies, service quality, and customer service, respectively. Practical implications - The paper's results help managers of automotive industries, particularly in developing countries, to outsource logistics activities to 3PL providers effectively and to create a significant competitive advantage. Originality/value - The main contributions of this paper are twofold. First, this paper proposes an integrated grey DEMATEL method to consider interdependent relationships among the 3PL provider selection criteria. Second, this study is one of the first studies to consider 3PL provider selection in a developing country like Iran. å© Emerald Group Publishing Limited.",DEMATEL; Grey systems theory; Supply chain management; Third-party logistics,Article,Scopus,2-s2.0-84968610536
"Gorzaâczany M.B., Rudziãski F.","A multi-objective genetic optimization for fast, fuzzy rule-based credit classification with balanced accuracy and interpretability",2016,Applied Soft Computing Journal,40,,,206,220,no,,,7,10.1016/j.asoc.2015.11.037,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955258864&doi=10.1016%2fj.asoc.2015.11.037&partnerID=40&md5=7c643b5d41826922a7f2d49191485936,"Department of Electrical and Computer Engineering, Kielce University of Technology, Al. 1000-lecia P.P. 7, Kielce, Poland","Gorzaâczany, M.B., Department of Electrical and Computer Engineering, Kielce University of Technology, Al. 1000-lecia P.P. 7, Kielce, Poland; Rudziãski, F., Department of Electrical and Computer Engineering, Kielce University of Technology, Al. 1000-lecia P.P. 7, Kielce, Poland","Credit classification is an important component of critical financial decision making tasks such as credit scoring and bankruptcy prediction. Credit classification methods are usually evaluated in terms of their accuracy, interpretability, and computational efficiency. In this paper, we propose an approach for automatic designing of fuzzy rule-based classifiers (FRBCs) from financial data using multi-objective evolutionary optimization algorithms (MOEOAs). Our method generates, in a single experiment, an optimized collection of solutions (financial FRBCs) characterized by various levels of accuracy-interpretability trade-off. In our approach we address the complexity- and semantics-related interpretability issues, we introduce original genetic operators for the classifier's rule base processing, and we implement our ideas in the context of Non-dominated Sorting Genetic Algorithm II (NSGA-II), i.e., one of the presently most advanced MOEOAs. A significant part of the paper is devoted to an extensive comparative analysis of our approach and 24 alternative methods applied to three standard financial benchmark data sets, i.e., Statlog (Australian Credit Approval), Statlog (German Credit Approval), and Credit Approval (also referred to as Japanese Credit) sets available from the UCI repository of machine learning databases (http://archive.ics.uci.edu/ml). Several performance measures including accuracy, sensitivity, specificity, and some number of interpretability measures are employed in order to evaluate the obtained systems. Our approach significantly outperforms the alternative methods in terms of the interpretability of the obtained financial data classifiers while remaining either competitive or superior in terms of their accuracy and the speed of decision making. å© 2015 Elsevier B.V. All rights reserved.",Accuracy and interpretability of credit classification systems; Financial decision support; Fuzzy rule-based systems; Genetic computations; Multi-objective evolutionary optimization,Article,Scopus,2-s2.0-84955258864
"Gjerde O., KjÌülle G., Jakobsen S.H., Vadlamudi V.V.",Enhanced method for reliability of supply assessment - An integrated approach,2016,"19th Power Systems Computation Conference, PSCC 2016",,,7540989,,,no,,,,10.1109/PSCC.2016.7540989,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986596622&doi=10.1109%2fPSCC.2016.7540989&partnerID=40&md5=a9828e75feec958a25751c1c03e16125,"Energy Systems, SINTEF Energy Research, Trondheim, Norway; Dept. of Electric Power Engineering, NTNU, Trondheim, Norway","Gjerde, O., Energy Systems, SINTEF Energy Research, Trondheim, Norway; KjÌülle, G., Energy Systems, SINTEF Energy Research, Trondheim, Norway; Jakobsen, S.H., Energy Systems, SINTEF Energy Research, Trondheim, Norway; Vadlamudi, V.V., Dept. of Electric Power Engineering, NTNU, Trondheim, Norway","A comprehensive analysis of reliability of electricity supply to end users requires a unique set of models, methods and tools. Of special significance are long term power market models for predicting future operating states (generation and load patterns) and more detailed network simulation models for analysing consequences of contingencies. This paper describes an integrated approach for reliability of supply analysis, assessing reliability and interruption costs down to the specifics of different kinds of end users at different delivery points in the network model. Furthermore, the effects of power system protection, temporal variation of parameters, and corrective actions as part of the consequence analysis, are taken into account in the presented integrated approach. The results are illustrated through a case study on a realistic hydro dominated four-area meshed test system. å© 2016 Power Systems Computation Conference.",Contingency analysis; interruption cost; protection system; reliability; temporal variation; transmission system,Conference Paper,Scopus,2-s2.0-84986596622
"Gharib M., Salnitri M., Paja E., Giorgini P., Mouratidis H., Pavlidis M., Ruiz J.F., Fernandez S., Siria A.D.",Privacy Requirements: Findings and Lessons Learned in Developing a Privacy Platform,2016,"Proceedings - 2016 IEEE 24th International Requirements Engineering Conference, RE 2016",,,7765531,256,265,no,,,,10.1109/RE.2016.13,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007190421&doi=10.1109%2fRE.2016.13&partnerID=40&md5=67c532af9488bf98cd13ec6b6cc2a7cd,"University of Trento, Trento, Italy; University of Brighton, Brighton, United Kingdom; Atos, Madrid, Spain; Bambino GesÌ¼ Children's Hospital, Rome, Italy; Business-e, Rome, Italy","Gharib, M., University of Trento, Trento, Italy; Salnitri, M., University of Trento, Trento, Italy; Paja, E., University of Trento, Trento, Italy; Giorgini, P., University of Trento, Trento, Italy; Mouratidis, H., University of Brighton, Brighton, United Kingdom; Pavlidis, M., University of Brighton, Brighton, United Kingdom; Ruiz, J.F., Atos, Madrid, Spain; Fernandez, S., Bambino GesÌ¼ Children's Hospital, Rome, Italy; Siria, A.D., Business-e, Rome, Italy","Information practices and systems that make use of personal and health-related information are governed by European laws and regulations to prevent unauthorized use and disclosure. Failure to comply with these laws and regulations results in huge monetary sanctions, which both private companies and public administrations want to avoid. How to comply with these laws, requires understanding the privacy requirements imposed on information systems. A holistic approach to privacy requirements specification calls for understanding not only the requirements derived from law, but also citizens' needs with respect to privacy. In this paper, we report on our experience in conducting privacy requirements engineering as part of a H2020 European Project, namely VisiOn (Visual Privacy Management in User Centric Open Requirements) for the development of a privacy platform to improve the interaction between Public Administrations (PA) and citizens, while guarding the privacy of the latter. Specifically, we present the process for eliciting, classifying, prioritizing, and validating privacy requirements for the two types of users, namely PA and citizen. The process is applied to different cases spanning from healthcare to other e-governmental initiatives, with the active involvement of the corresponding PAs. We report on findings and lessons learned from this experience. å© 2016 IEEE.",classification; elicitation; prioritization; Privacy requirements; requirements engineering; validation,Conference Paper,Scopus,2-s2.0-85007190421
"Gasparic M., Janes A.",What recommendation systems for software engineering recommend: A systematic literature review,2016,Journal of Systems and Software,113,,,101,113,no,,,4,10.1016/j.jss.2015.11.036,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962376126&doi=10.1016%2fj.jss.2015.11.036&partnerID=40&md5=949412aeaf85932acff3fbdef86ec0cc,"Free University of Bolzano, Italy","Gasparic, M., Free University of Bolzano, Italy; Janes, A., Free University of Bolzano, Italy","A recommendation system for software engineering (RSSE) is a software application that provides information items estimated to be valuable for a software engineering task in a given context. Present the results of a systematic literature review to reveal the typical functionality offered by existing RSSEs, research gaps, and possible research directions. We evaluated 46 papers studying the benefits, the data requirements, the information and recommendation types, and the effort requirements of RSSE systems. We include papers describing tools that support source code related development published between 2003 and 2013. The results show that RSSEs typically visualize source code artifacts. They aim to improve system quality, make the development process more efficient and less expensive, lower developer's cognitive load, and help developers to make better decisions. They mainly support reuse actions and debugging, implementation, and maintenance phases. The majority of the systems are reactive. Unexploited opportunities lie in the development of recommender systems outside the source code domain. Furthermore, current RSSE systems use very limited context information and rely on simple models. Context-adapted and proactive behavior could improve the acceptance of RSSE systems in practice. å© 2015 Elsevier Inc. Allrights reserved.",Recommendation system for software engineering; Systematic literature review,Review,Scopus,2-s2.0-84962376126
"GarcÌ_a-GalÌÁn J., Trinidad P., Rana O.F., Ruiz-CortÌ©s A.",Automated configuration support for infrastructure migration to the cloud,2016,Future Generation Computer Systems,55,,,200,212,no,,,9,10.1016/j.future.2015.03.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954127344&doi=10.1016%2fj.future.2015.03.006&partnerID=40&md5=cb18f74a9b6fa429c50caa19591b030d,"Escuela TÌ©cnica Superior de IngenierÌ_a InformÌÁtica, University of Seville, Avda. Reina Mercedes s/n, Seville, Spain; School of Computer Science and Informatics, Cardiff University, Queen's Buildings, Newport Road, Cardiff, United Kingdom","GarcÌ_a-GalÌÁn, J., Escuela TÌ©cnica Superior de IngenierÌ_a InformÌÁtica, University of Seville, Avda. Reina Mercedes s/n, Seville, Spain; Trinidad, P., Escuela TÌ©cnica Superior de IngenierÌ_a InformÌÁtica, University of Seville, Avda. Reina Mercedes s/n, Seville, Spain; Rana, O.F., School of Computer Science and Informatics, Cardiff University, Queen's Buildings, Newport Road, Cardiff, United Kingdom; Ruiz-CortÌ©s, A., Escuela TÌ©cnica Superior de IngenierÌ_a InformÌÁtica, University of Seville, Avda. Reina Mercedes s/n, Seville, Spain","With an increasing number of cloud computing offerings in the market, migrating an existing computational infrastructure to the cloud requires comparison of different offers in order to find the most suitable configuration. Cloud providers offer many configuration options, such as location, purchasing mode, redundancy, and extra storage. Often, the information about such options is not well organised. This leads to large and unstructured configuration spaces, and turns the comparison into a tedious, error-prone search problem for the customers. In this work we focus on supporting customer decision making for selecting the most suitable cloud configuration - in terms of infrastructural requirements and cost. We achieve this by means of variability modelling and analysis techniques. Firstly, we structure the configuration space of an IaaS using feature models, usually employed for the modelling of variability-intensive systems, and present the case study of the Amazon EC2. Secondly, we assist the configuration search process. Feature models enable the use of different analysis operations that, among others, automate the search of optimal configurations. Results of our analysis show how our approach, with a negligible analysis time, outperforms commercial approaches in terms of expressiveness and accuracy. å© 2015 Elsevier B.V. All rights reserved.",Automated analysis; Cloud migration; EC2; Feature model; IaaS,Article,Scopus,2-s2.0-84954127344
"FernÌÁndez-SÌÁnchez C., Garbajosa J., YagÌ_e A., Perez J.",Identification and analysis of the elements required to manage technical debt by means of a systematic mapping study,2017,Journal of Systems and Software,124,,,22,38,no,,,,10.1016/j.jss.2016.10.018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994877375&doi=10.1016%2fj.jss.2016.10.018&partnerID=40&md5=e7f9284a43254542c30289b900d2a138,"Technical University of Madrid (UPM), CITSEM, ctra. de Valencia Km7, Madrid, Spain; Technical University of Madrid (UPM), ETSISI, ctra. de Valencia Km7, Madrid, Spain","FernÌÁndez-SÌÁnchez, C., Technical University of Madrid (UPM), CITSEM, ctra. de Valencia Km7, Madrid, Spain; Garbajosa, J., Technical University of Madrid (UPM), CITSEM, ctra. de Valencia Km7, Madrid, Spain, Technical University of Madrid (UPM), ETSISI, ctra. de Valencia Km7, Madrid, Spain; YagÌ_e, A., Technical University of Madrid (UPM), CITSEM, ctra. de Valencia Km7, Madrid, Spain, Technical University of Madrid (UPM), ETSISI, ctra. de Valencia Km7, Madrid, Spain; Perez, J., Technical University of Madrid (UPM), CITSEM, ctra. de Valencia Km7, Madrid, Spain, Technical University of Madrid (UPM), ETSISI, ctra. de Valencia Km7, Madrid, Spain","Technical debt, a metaphor for the long-term consequences of weak software development, must be managed to keep it under control. The main goal of this article is to identify and analyze the elements required to manage technical debt. The research method used to identify the elements is a systematic mapping, including a synthesis step to synthesize the elements definitions. Our perspective differs from previous literature reviews because it focused on the elements required to manage technical debt and not on the phenomenon of technical debt or the activities used in performing technical debt management. Additionally, the rigor and relevance for industry of the current techniques used to manage technical debt are studied. The elements were classified into three groups (basic decision-making factors, cost estimation techniques, practices and techniques for decision-making) and mapped according three stakeholders‰Ûª points of view (engineering, engineering management, and business-organizational management). The definitions, classification, and analysis of the elements provide a framework that can be deployed to help in the development of models that are adapted to the specific stakeholders‰Ûª interests to assist the decision-making required in technical debt management and to assess existing models and methods. The analysis indicated that technical debt management is context dependent. å© 2016 Elsevier Inc.",Basic decision-making factors; Business-organizational management; Cost estimation techniques; Decision making; Engineering; Engineering management; Framework; Practices and techniques for decision-making; Stakeholders‰Ûª points of view; Systematic mapping; Technical debt; Technical debt management,Article,Scopus,2-s2.0-84994877375
"Felizardo K.R., Mendes E., Kalinowski M., Souza E.F., Vijaykumar N.L.",Using Forward Snowballing to update Systematic Reviews in Software Engineering,2016,International Symposium on Empirical Software Engineering and Measurement,08-09-September-2016,, a53,,,no,,,,10.1145/2961111.2962630,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991596539&doi=10.1145%2f2961111.2962630&partnerID=40&md5=39569ba9bb49bf0671af1947985b08af,"Federal Technological, University of ParanÌÁ, Departament of Computer, CornÌ©lio ProcÌ_pio, Brazil; Blekinge Institute of Technology, Depart. of Software Engineering, University of Oulu, Karlskrona, Sweden; Fluminense Federal University, Graduate Programm in Computing, NiteroÌ_, Brazil; Department of Comp. and Applied Mathematics, National Institute for Space Research, INPE, JosÌ© dos Campos, Brazil","Felizardo, K.R., Federal Technological, University of ParanÌÁ, Departament of Computer, CornÌ©lio ProcÌ_pio, Brazil; Mendes, E., Blekinge Institute of Technology, Depart. of Software Engineering, University of Oulu, Karlskrona, Sweden; Kalinowski, M., Fluminense Federal University, Graduate Programm in Computing, NiteroÌ_, Brazil; Souza, E.F., Federal Technological, University of ParanÌÁ, Departament of Computer, CornÌ©lio ProcÌ_pio, Brazil; Vijaykumar, N.L., Department of Comp. and Applied Mathematics, National Institute for Space Research, INPE, JosÌ© dos Campos, Brazil","Background: A Systematic Literature Review (SLR) is a methodology used to aggregate relevant evidence related to one or more research questions. Whenever new evidence is published after the completion of a SLR, this SLR should be updated in order to preserve its value. However, updating SLRs involves significant effort. Objective: The goal of this paper is to investigate the application of forward snowballing to support the update of SLRs. Method: We compare outcomes of an update achieved using the forward snowballing versus a published update using the search-based approach, i.e., searching for studies in electronic databases using a search string. Results: Forward snowballing showed a higher precision and a slightly lower recall. It reduced in more than five times the number of primary studies to filter however missed one relevant study. Conclusions: Due to its high precision, we believe that the use of forward snowballing considerably reduces the effort in updating SLRs in Software Engineering; however the risk of missing relevant papers should not be underrated. å© 2016 ACM.",forward snowballing; Systematic literature reviews,Conference Paper,Scopus,2-s2.0-84991596539
"Elsayed N., Al-Wahedi K.",Utilizing Task Partitioning for Self-Organized Allocation of Partially Sequential Tasks,2015,"Proceedings - 2015 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2015",,,7379659,3030,3035,no,,,,10.1109/SMC.2015.527,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964555726&doi=10.1109%2fSMC.2015.527&partnerID=40&md5=1ea15f64df1ad94404a8d78f2d0c58d9,"Department of Electrical Engineering, Petroleum Institute, Abu Dhabi, United Arab Emirates","Elsayed, N., Department of Electrical Engineering, Petroleum Institute, Abu Dhabi, United Arab Emirates; Al-Wahedi, K., Department of Electrical Engineering, Petroleum Institute, Abu Dhabi, United Arab Emirates","Task partitioning in multi-robot systems involves breaking down tasks or partitioning them into smaller tasks tackled by different robots in the system. Some of the benefits of this approach is less interference among the individual agents as they become more segregated, an improved scalability, and an improved transport efficiency. This approach allows for a better overall group performance, leads to specialization and AIDS in parallel task execution. In this paper, a new problem involving self-organized task allocation for partially sequential tasks in a two-robot environment is investigated. This partially sequential nature comes from the fact that some tasks are to be executed in a specific order while others can be executed in any order. The two robots are nonidentical. The first is equipped to do all the necessary computations and it has the ability to decide on the optimal order of executing the tasks, let's call him ""Brain"", while the other has a set of simple behaviors that it keeps following at all times, let's call him ""Pinky"". There is no explicit communication between the two robots, instead, indirect communication is achieved through the concept of stigmergy. å© 2015 IEEE.",Multi Robot Systems; Partially Sequential Tasks; Self-Organized Task Allocation; Stigmergy; Swarm Robotics; Task Allocation; Task Partitioning,Conference Paper,Scopus,2-s2.0-84964555726
"Elmidaoui S., Cheikhi L., Idri A.",A survey of empirical studies in software product maintainability prediction models,2016,SITA 2016 - 11th International Conference on Intelligent Systems: Theories and Applications,,,7772267,,,no,Maintainability,,,10.1109/SITA.2016.7772267,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010468522&doi=10.1109%2fSITA.2016.7772267&partnerID=40&md5=8184d51d9fbbba990544f8dc158d2375,"Software Project Management Team (SPM) ENSIAS, University Mohamed v of Rabat, Morocco","Elmidaoui, S., Software Project Management Team (SPM) ENSIAS, University Mohamed v of Rabat, Morocco; Cheikhi, L., Software Project Management Team (SPM) ENSIAS, University Mohamed v of Rabat, Morocco; Idri, A., Software Project Management Team (SPM) ENSIAS, University Mohamed v of Rabat, Morocco","Software product maintainability is critical to the achievement of the software product quality. In order to keep the software useful as long as possible, software product maintainability prediction (SPMP) has become an important endeavor. The objective of this paper is to identify and present the current research on SPMP. The search was conducted using digital libraries to And as much research papers as possible. Selected papers are classified according to the following survey classification criteria (SCs): research type, empirical type, publication year and channel. Based on the results of the survey, we provide a discussion of the current state of the art in software maintainability prediction models or techniques. We believe that this study will be a reliable basis for further research in software maintainability studies. å© 2016 IEEE.",Prediction Models; Software Product Maintainability; Survey,Conference Paper,Scopus,2-s2.0-85010468522
"El-Sappagh S., Elmogy M.M.",Medical case based reasoning frameworks: Current developments and future directions,2016,International Journal of Decision Support System Technology,8,3,,31,62,no,,,,10.4018/IJDSST.2016070103,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979994218&doi=10.4018%2fIJDSST.2016070103&partnerID=40&md5=aa1b1a0f370dc458b51a1d48d1178b10,"Faculty of Computers and Information, Minia University, Egypt; Faculty of Computers and Information, Mansoura University, Egypt","El-Sappagh, S., Faculty of Computers and Information, Minia University, Egypt; Elmogy, M.M., Faculty of Computers and Information, Mansoura University, Egypt","Case-Based Reasoning (CBR) is one of the most suitable AI techniques for building clinical decision support systems. Medical domain complexity introduces many challenges for building these systems. Building the systems' knowledge base from the Electronic Health Record (EHR), the encoding of case-base knowledge with standard medical ontology, and the handling of vague data are examples of these challenges. Although several advantages of using CBR in medicine have been identified, there are no real systems acceptable to physicians. This systematic review examines the current state of CBR and its limitations in the medical domain, especially for diabetes mellitus. The critical evaluation of the status of diabetes CBR systems presents unique opportunities for improving these systems. The literature review covers most of the English language studies extracted from relevant databases by using search terms relating CBR, ontology, Fuzzy, and standard terminology concepts. The authors identify 38 articles published between 1999 and 15 January 2015, which represent original researches in CBR domain. The study includes 15 (39.5%) non-medical studies and 23 (60.5%) medical studies with 22% for diabetes CBR. A list of 18 integrated evaluation metrics has been proposed and used in the analysis. The results show that the non-medical CBR systems achieved higher advances (50%) than medical systems (42.9%). In addition, the diabetes management CBR systems achieve the lowest advances (21.4%) compared to other systems. These shortages explain the question ""why CBR paradigm are not fully utilized in the commercial medical systems?"" As a result, there is a distinct need for more comprehensive enhancements in clinical CBR especially diabetes systems. Copyright å© 2016, IGI Global.",Case-based reasoning; Clinical decision support system (CDSS); Diabetes management; Electronic health record (EHR); Ontology; Patient care,Article,Scopus,2-s2.0-84979994218
"Eder K., Gallagher J.P., LÌ_pez-GarcÌ_a P., Muller H., Bankovi€à Z., Georgiou K., HaemmerlÌ© R., Hermenegildo M.V., Kafle B., Kerrison S., Kirkeby M., Klemen M., Li X., Liqat U., Morse J., Rhiger M., Rosendahl M.",ENTRA: Whole-systems energy transparency,2016,Microprocessors and Microsystems,47,,,278,286,no,,,,10.1016/j.micpro.2016.07.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84998814297&doi=10.1016%2fj.micpro.2016.07.003&partnerID=40&md5=370bdd806adc531f9768b0f2cf469afe,"University of Bristol, United Kingdom; Roskilde University, Denmark; IMDEA Software Institute, Spain; XMOS Ltd., Bristol, United Kingdom; Spanish Council for Scientific Research, Spain; Technical University of Madrid, Spain","Eder, K., University of Bristol, United Kingdom; Gallagher, J.P., Roskilde University, Denmark, IMDEA Software Institute, Spain; LÌ_pez-GarcÌ_a, P., IMDEA Software Institute, Spain, Spanish Council for Scientific Research, Spain; Muller, H., XMOS Ltd., Bristol, United Kingdom; Bankovi€à, Z., IMDEA Software Institute, Spain; Georgiou, K., University of Bristol, United Kingdom; HaemmerlÌ©, R., IMDEA Software Institute, Spain; Hermenegildo, M.V., IMDEA Software Institute, Spain, Technical University of Madrid, Spain; Kafle, B., Roskilde University, Denmark; Kerrison, S., University of Bristol, United Kingdom; Kirkeby, M., Roskilde University, Denmark; Klemen, M., IMDEA Software Institute, Spain; Li, X., Roskilde University, Denmark; Liqat, U., IMDEA Software Institute, Spain; Morse, J., University of Bristol, United Kingdom; Rhiger, M., Roskilde University, Denmark; Rosendahl, M., Roskilde University, Denmark","Promoting energy efficiency to a first class system design goal is an important research challenge. Although more energy-efficient hardware can be designed, it is software that controls the hardware; for a given system the potential for energy savings is likely to be much greater at the higher levels of abstraction in the system stack. Thus the greatest savings are expected from energy-aware software development, which is the vision of the EU ENTRA project. This article presents the concept of energy transparency as a foundation for energy-aware software development. We show how energy modelling of hardware is combined with static analysis to allow the programmer to understand the energy consumption of a program without executing it, thus enabling exploration of the design space taking energy into consideration. The paper concludes by summarising the current and future challenges identified in the ENTRA project. å© 2016 Elsevier B.V.",Energy modelling; Energy transparency; Energy-aware software development; Resource analysis; Static analysis,Article,Scopus,2-s2.0-84998814297
"Duan G., Ding D., Tian Y., You X.",An improved medical decision model based on decision tree algorithms,2016,"Proceedings - 2016 IEEE International Conferences on Big Data and Cloud Computing, BDCloud 2016, Social Computing and Networking, SocialCom 2016 and Sustainable Computing and Communications, SustainCom 2016",,,7723687,151,156,no,,,,10.1109/BDCloud-SocialCom-SustainCom.2016.33,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85000898337&doi=10.1109%2fBDCloud-SocialCom-SustainCom.2016.33&partnerID=40&md5=d84c29327bc90871c25f86b884659d7f,"School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Physical Electronics, University of Electronic Science and Technology of China, Chengdu, China; NO.32 Institute of China Electronic Technology Group Corporation, Shanghai, China; School of Software Engineering, Tongji University, Shanghai, China","Duan, G., School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; Ding, D., School of Physical Electronics, University of Electronic Science and Technology of China, Chengdu, China; Tian, Y., School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; You, X., NO.32 Institute of China Electronic Technology Group Corporation, Shanghai, China, School of Software Engineering, Tongji University, Shanghai, China","In the medical decision field, the conventional analysis methods of medical insurance data are lack of flexibility and efficiency. When coping with huge and redundant medical dataset, it is difficult to extract the candidate attributes if only using some limited professional knowledge. Therefore, the correlations between the medical insurance cost and relevant factors (e.g. diagnosis results) need to be mined by techniques such as the FP-Growth algorithm, which uses an extended prefix-tree structure for building attributes automatically, storing compressed and crucial information. In this work, we propose an improved decision tree algorithm to effectively model medical data, which includes processes of pre-pruning and post-pruning. Classification based on three criteria such as accuracy, stability and complexity are then utilized to improve the effectiveness of our approach. Simulation results show that our proposed model can improve the decision making with higher flexibility and efficiency. å© 2016 IEEE.",Decision tree algorithm; FPGrowth algorithm; Medical decision model; Pruning method,Conference Paper,Scopus,2-s2.0-85000898337
Dewanto V.,Learning the search heuristic for combined task and motion planning,2015,"ICACSIS 2015 - 2015 International Conference on Advanced Computer Science and Information Systems, Proceedings",,,7415160,309,316,no,,,,10.1109/ICACSIS.2015.7415160,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964478211&doi=10.1109%2fICACSIS.2015.7415160&partnerID=40&md5=9385f6a49c03cf6b64fe71f24249609d,"Department of Computer Science, Bogor Agricultural University, Bogor, Indonesia","Dewanto, V., Department of Computer Science, Bogor Agricultural University, Bogor, Indonesia","Autonomous robots have to plan two intricately dependent levels: task and motion. One promising approach is to plan task and motion simultaneously, yielding a sequence of high level actions that is guaranteed to have valid motion plans. In this paper, we present our work on such planning system whose backbone is the ability to estimate the cost of action sequences. This cost essentially encodes information about motion feasibility and optimality criteria. Concretely, the cost prediction serves as the heuristic for search over a task motion multigraph. The experiment results show that the proposed approach makes the planning progressively more efficient as well as ìµ-optimal. It means that the wasted computations are more and more reduced over planning attempts and that the complete plans found are guaranteed to have costs no more than a factor of (1 +ìµ) greater than the optimal. This suggests that the heuristic along with its learning formulation are justifiable and that the designed feature vector is sufficient for learning. In addition, we found that online learning during search offers better utility than the offline. å© 2015 IEEE.",,Conference Paper,Scopus,2-s2.0-84964478211
"Desai A.B., Parmar J.K.",Refactoring Cost Estimation (RCE) Model for Object Oriented System,2016,"Proceedings - 6th International Advanced Computing Conference, IACC 2016",,,7544837,214,218,no,,,,10.1109/IACC.2016.48,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987725518&doi=10.1109%2fIACC.2016.48&partnerID=40&md5=0b4503da086827f56eab3ecbf7343492,"Institute of Engg. and Technology, Ahmedabad University, Ahmedabad, India; Dept. of Comp. Sci. and Engg., BITS edu campus, Vadodara, India","Desai, A.B., Institute of Engg. and Technology, Ahmedabad University, Ahmedabad, India; Parmar, J.K., Dept. of Comp. Sci. and Engg., BITS edu campus, Vadodara, India","Successful software systems must be developedto evolve or die. Although object-oriented softwaresystems are to be built to last over the time but they willdegrade as much as any legacy software system. As aconsequence, one may identify various reengineeringpatterns which capture best practice in reverse and re-engineering object-oriented legacy systems. Software reengineeringbasically focuses on re-implementing oldersystems to improve or make it more maintainable. Refactoring is on kind of re-engineering with-in an Object-Oriented context. In this paper, with the given object-oriented refactoring opportunities, the cost ofrefactoring is resembled using RCE. The opportunities areclass misuse, violation of the principle of encapsulation, lack of use of inheritance concept, misuse of inheritance, misplaced polymorphism. å© 2016 IEEE.",RCE Model; Refactoring; Refactoring Cost,Conference Paper,Scopus,2-s2.0-84987725518
"Dabbagh M., Lee S.P., Parizi R.M.","Functional and non-functional requirements prioritization: empirical evaluation of IPA, AHP-based, and HAM-based approaches",2016,Soft Computing,20,11,,4497,4520,no,,,,10.1007/s00500-015-1760-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84936818484&doi=10.1007%2fs00500-015-1760-z&partnerID=40&md5=1f53a57d34a906ea79d654b6a51ff6b3,"Faculty of Computer Science and Information Technology, University of Malaya, Kuala Lumpur, Malaysia; School of Computing and IT, Taylor‰Ûªs University, Subang Jaya, Selangor, Malaysia","Dabbagh, M., Faculty of Computer Science and Information Technology, University of Malaya, Kuala Lumpur, Malaysia; Lee, S.P., Faculty of Computer Science and Information Technology, University of Malaya, Kuala Lumpur, Malaysia; Parizi, R.M., School of Computing and IT, Taylor‰Ûªs University, Subang Jaya, Selangor, Malaysia","Throughout the requirements engineering phase, the process of giving precedence to one requirement over another is beneficial to accomplish projects on a predefined schedule. This process is referred to as requirements prioritization. Although plenty of research has been dedicated to proposing various approaches to perform the requirements prioritization, only a small number of prioritization approaches have been recently reported with the aim of considering both functional and non-functional requirements during the prioritization stage. However, it is not a straightforward task to decide which of these approaches could be selected for a given prioritization problem unless the main properties of these approaches are well-evaluated. Hence, a detailed evaluation of the recently proposed approaches in an empirical manner would be needed. In this paper, we performed two successive controlled experiments with the aim of evaluating the current requirements prioritization approaches. In the first experiment, we compared the integrated prioritization approach (IPA) with the other approach, called AHP-based approach, whereas in the second experiment, IPA was compared with the other state-of-the-art alternative, named HAM-based approach. In the experiments, evaluation was based on measuring three properties: actual time-consumption, accuracy of results, and ease of use. Statistical analysis of the results obtained from the two experiments showed a better performance of IPA on all the measured properties compared to both AHP-based approach and HAM-based approach. The findings would be useful for practitioners to choose the most appropriate approach for a given prioritization problem, and also could be used as a guideline by interested researchers for identifying trends before conducting a study in future. å© 2015, Springer-Verlag Berlin Heidelberg.",Analytic hierarchy process (AHP); Empirical study; Functional and non-functional requirements; Hybrid assessment method (HAM); Integrated prioritization approach (IPA); Requirements prioritization,Article,Scopus,2-s2.0-84936818484
"Coelho M.A.N., Borges C.C.H., Neto R.F.",A dual method for solving the nonlinear structured prediction problem,2016,Pattern Recognition Letters,75,,,55,62,no,,,,10.1016/j.patrec.2016.03.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962768361&doi=10.1016%2fj.patrec.2016.03.009&partnerID=40&md5=759e35e15148493d03db051ab3d6b0a9,"Department of Computer Science, Federal University of Juiz de Fora, Juiz de Fora, Minas Gerais, Brazil","Coelho, M.A.N., Department of Computer Science, Federal University of Juiz de Fora, Juiz de Fora, Minas Gerais, Brazil; Borges, C.C.H., Department of Computer Science, Federal University of Juiz de Fora, Juiz de Fora, Minas Gerais, Brazil; Neto, R.F., Department of Computer Science, Federal University of Juiz de Fora, Juiz de Fora, Minas Gerais, Brazil","In this paper, we present a perceptron-based algorithm and have developed a dual formulation to solve the nonlinear structured prediction problem, which we called Dual Structured Incremental Margin Algorithm (DSIMA). The proposed formulation allows the introduction of kernel functions enabling the efficient solution of nonlinear problems. In order to verify the correctness and applicability of the algorithm, we consider an inverse approach to the path planning problem. The problem mapped on a grid environment can be solved by a search process that essentially depends on the definition of the transition costs between states. In this context, we develop and apply a learning algorithm that is able to perform the reverse path, i.e., the prediction of these costs in a direct space for the linear form. However, considering the nonlinear form, the problem is solved in a space of high dimension and where it is possible to learn a path instead of the transition costs. This learning problem is usually formulated as a convex optimization problem of maximum margin. Several tests to solve the costs prediction problem were carried out and the results compared to other structured prediction techniques. The proposed algorithm demonstrated greater efficiency in terms of computational effort and quality of prediction. å© 2016 Elsevier B.V. All rights reserved.",Dual perceptron; Inverse path planning; Learning strategy; Nonlinear structured prediction,Article,Scopus,2-s2.0-84962768361
"Cicirelli F., Forestiero A., Giordano A., Mastroianni C.",Transparent and efficient parallelization of swarm algorithms,2016,ACM Transactions on Autonomous and Adaptive Systems,11,2,14,,,no,,,,10.1145/2897373,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974624960&doi=10.1145%2f2897373&partnerID=40&md5=1c52d19cb20e5c020920f8ae54e79e6f,"CNR Institute for High Performance Computing and Networks, Via P. Bucci 41C, Rende (CS), Italy","Cicirelli, F., CNR Institute for High Performance Computing and Networks, Via P. Bucci 41C, Rende (CS), Italy; Forestiero, A., CNR Institute for High Performance Computing and Networks, Via P. Bucci 41C, Rende (CS), Italy; Giordano, A., CNR Institute for High Performance Computing and Networks, Via P. Bucci 41C, Rende (CS), Italy; Mastroianni, C., CNR Institute for High Performance Computing and Networks, Via P. Bucci 41C, Rende (CS), Italy","This article presents an approach for the efficient and transparent parallelization of a large class of swarm algorithms, specifically those where the multiagent paradigm is used to implement the functionalities of bioinspired entities, such as ants and birds. Parallelization is achieved by partitioning the space on which agents operate onto multiple regions and assigning each region to a different computing node. Data consistency and conflict issues, which can arise when several agents concurrently access shared data, are handled using a purposely developed notion of logical time. This approach enables a transparent porting onto parallel/ distributed architectures, as the developer is only in charge of defining the behavior of the agents, without having to cope with issues related to parallel programming and performance optimization. The approach has been evaluated for a very popular swarm algorithm, the ant-based spatial clustering and sorting of items, and results show good performance and scalability. å© 2016 ACM.",Ant-based clustering and sorting; Logical time; Swarm algorithms,Article,Scopus,2-s2.0-84974624960
"Ciancio C., Ambrogio G., Gagliardi F., Musmanno R.",Heuristic techniques to optimize neural network architecture in manufacturing applications,2016,Neural Computing and Applications,27,7,,2001,2015,no,,,,10.1007/s00521-015-1994-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938545201&doi=10.1007%2fs00521-015-1994-9&partnerID=40&md5=b2e9c1352b770ccc745e7107549b7181,"Department of Mechanical, Energy and Management Engineering, University of Calabria, Rende, Italy","Ciancio, C., Department of Mechanical, Energy and Management Engineering, University of Calabria, Rende, Italy; Ambrogio, G., Department of Mechanical, Energy and Management Engineering, University of Calabria, Rende, Italy; Gagliardi, F., Department of Mechanical, Energy and Management Engineering, University of Calabria, Rende, Italy; Musmanno, R., Department of Mechanical, Energy and Management Engineering, University of Calabria, Rende, Italy","Nowadays application of neural networks in the manufacturing field is widely assessed even if this type of problem is typically characterized by an insufficient availability of data for a robust network training. Satisfactory results can be found in the literature, in both forming and machining operations, regarding the use of a neural network as a predictive tool. Nevertheless, the research of the optimal network configuration is still based on trial-and-error approaches, rather than on the application of specific techniques. As a consequence, the best method to determine the optimal neural network configuration is still a lack of knowledge in the literature overview. According to that, a comparative analysis is proposed in this work. More in detail four different approaches have been used to increase the generalization abilities of a neural network. These methods are based, respectively, on the use of genetic algorithms, Taguchi, tabu search and decision trees. The parameters taken into account in this work are the training algorithm, the number of hidden layers, the number of neurons and the activation function of each hidden layer. These techniques have been firstly tested on three different datasets, generated through numerical simulations in the Deform2D environment, in an attempt to map the input‰ÛÒoutput relationship for an extrusion, a rolling and a shearing process. Subsequently, the same approach has been validated on a fourth dataset derived from the literature review for a complex industrial process to widely generalize and asses the proposed methodology in the whole manufacturing field. Four tests were carried out for each dataset modifying the original data with a random noise with zero mean and standard deviation of one, two and five per cent. The results show that the use of a suitable technique for determining the architecture of a neural network can generate a significant performance improvement compared to a trial-and-error approach. å© 2015, The Natural Computing Applications Forum.",2D numerical simulations; Decision trees; Genetic algorithm; Neural network architecture design; Tabu search; Taguchi,Article,Scopus,2-s2.0-84938545201
Chun Y.-H.,A simulation study on cost factors of smartphone-embedded softwares,2016,International Journal of Software Engineering and its Applications,10,7,,53,60,no,,,,10.14257/ijseia.2016.10.7.06,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983027505&doi=10.14257%2fijseia.2016.10.7.06&partnerID=40&md5=fc50997c59a448451c84235c58e8dc04,"School of Computer Science and Engineering, Yongin University, Gyeonggi, South Korea","Chun, Y.-H., School of Computer Science and Engineering, Yongin University, Gyeonggi, South Korea","With the rapid spread of smartphones, there have been diverse changes in the IT industry to activate the use of smartphones. Among them, the importance of smartphone-embedded softwares is getting greater. There have been a lot of studies about the service factors affecting on the users' choice of phone. But, the influence of smartphone-embedded S/Ws on the cost has been rarely researched, though the softwares have become a hot issue in the IT industry along with the rapid expansion of smartphone users. Therefore, this study researches the cost factors of smartphone-embedded S/Ws and makes the simulation test with them by using the business dynamics to check the influence of S/Ws on the smartphone cost. å© 2016 SERSC.",Business dynamics; Cost engineering; Mobile business; Smartphone,Article,Scopus,2-s2.0-84983027505
"Chu W., Li Y., Liu C., Mou W., Tang L.",Collaborative manufacturing of aircraft structural parts based on machining features and software agents,2016,International Journal of Advanced Manufacturing Technology,87,5-Aug,,1421,1434,no,,,,10.1007/s00170-013-4976-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875990233&doi=10.1007%2fs00170-013-4976-z&partnerID=40&md5=ed71eaee9991804c3076bf93610e533a,"College of Mechanical and Electrical Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; NC Machining Workshop, Chengdu Aircraft Industrial (Group) Co., Ltd, Chengdu, China","Chu, W., College of Mechanical and Electrical Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China, NC Machining Workshop, Chengdu Aircraft Industrial (Group) Co., Ltd, Chengdu, China; Li, Y., College of Mechanical and Electrical Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Liu, C., College of Mechanical and Electrical Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Mou, W., NC Machining Workshop, Chengdu Aircraft Industrial (Group) Co., Ltd, Chengdu, China; Tang, L., NC Machining Workshop, Chengdu Aircraft Industrial (Group) Co., Ltd, Chengdu, China","Manufacturing of aircraft structural parts have the characteristics of complex process, large variety, small batch, and frequent change of production status. In order to shorten lead time and reduce cost, high-efficiency communication and collaboration among manufacturing departments are required. However, in the existing research literature, tool/fixture design, manufacturing simulation, and online machining process monitoring are not fully taken into account for collaboration. In order to address these challenging issues, this paper proposes a collaborative manufacturing framework based on machining features and intelligent software agents. The components of the proposed framework include a machining process planning agent, a numerical control (NC) programming agent, a simulation and verification agent, a tool/fixture design agent, a cost estimation agent, a production management agent, and an online machining process monitoring agent. Machining features are used as information carrier for communication and collaboration among these agents. This paper particularly focuses on the collaboration between tool/fixture design and NC programming, as well as the collaboration between online machining processes and related departments. The proposed approach has been implemented through a prototype system and tested in a large aircraft manufacturing enterprise with some very promising results. å© 2013, Springer-Verlag London.",Aircraft structural parts; Collaborative manufacturing; Machining features; Software agents,Article,Scopus,2-s2.0-84875990233
"Christa S., Madhusudhan V., Suma V., Rao J.J.",Software maintenance: From the perspective of effort and cost requirement,2017,Advances in Intelligent Systems and Computing,469,,,759,768,no,,,,10.1007/978-981-10-1678-3_73,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984994066&doi=10.1007%2f978-981-10-1678-3_73&partnerID=40&md5=e04b114004483d1f2ddd58d83c2e544a,"Department of Information Science and Engineering, Dayananda Sagar College of Engineering, Bengaluru, India; Department of Industrial Engineering and Management, Dayananda Sagar College of Engineering, Bengaluru, India","Christa, S., Department of Information Science and Engineering, Dayananda Sagar College of Engineering, Bengaluru, India; Madhusudhan, V., Department of Information Science and Engineering, Dayananda Sagar College of Engineering, Bengaluru, India; Suma, V., Department of Information Science and Engineering, Dayananda Sagar College of Engineering, Bengaluru, India; Rao, J.J., Department of Industrial Engineering and Management, Dayananda Sagar College of Engineering, Bengaluru, India","Software and software deliverables have high impact on all fields. Once software is deployed, it has to be maintained continuously, till it becomes obsolete. Various activities that come under maintenance include adaptive, corrective, and predictive maintenance. Even though software maintenance is not tagged as a core field in software engineering compared to other software-related activities, almost 70 % of time and resources are allotted for maintenance activities. According to the related work, very little research is going on in the field of maintenance and this article highlights the scope of maintenance-related research. Identifying the factors that directly and indirectly affect the maintenance activity will in turn make the estimation activities easy. Implementation of an effective software maintenance model will have a very high impact in the quality of software and thereby with customer satisfaction. This article aims to project an effective maintenance model which reduces cost of rework and improves customer satisfaction index. å© Springer Science+Business Media Singapore 2017.",Adaptive maintenance; Effort estimation; Maintenance cost estimation; Software maintenance,Conference Paper,Scopus,2-s2.0-84984994066
"Choi J.H., Lee J.Y., Shin H., Nasridinov A.",An efficient computation of skyline queries using hash tables,2016,Advanced Science Letters,22,9,,2348,2353,no,,,,10.1166/asl.2016.7840,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007505087&doi=10.1166%2fasl.2016.7840&partnerID=40&md5=9a0e4e35ab295c1968578dff36bed1fa,"Department of Computer Science, Chungbuk National University, South Korea; Emotion Recognition IoT Research Section, ETRI, South Korea","Choi, J.H., Department of Computer Science, Chungbuk National University, South Korea; Lee, J.Y., Department of Computer Science, Chungbuk National University, South Korea; Shin, H., Emotion Recognition IoT Research Section, ETRI, South Korea; Nasridinov, A., Department of Computer Science, Chungbuk National University, South Korea","Skyline is a set of tuples that are not dominated by any other tuples in the database. The state-of-the-art methods to construct skyline are not efficient as these require presorting a dataset based on entropy score of each tuple. For large and high-dimensional databases, this may impose additional processing overload and enlarge the overall skyline construction time. In this paper, we propose an efficient method to compute skyline, called Hash Layer Skyline (HLS), which reduces the construction time of skyline. That is, we define criteria to classify candidate tuples into layers during comparison stage and store the classified tuples in buckets of hash tables. HLS overcomes the drawbacks of the state-of-the-art methods by avoiding presorting the dataset and reducing a number of comparisons between candidate tuples. The experiment results show that HLS reduces skyline construction time by several times comparing to the state-of-the-art methods. å© 2016 American Scientific Publishers. All rights reserved.",Hash tables; High-dimensional databases; Skyline queries,Article,Scopus,2-s2.0-85007505087
"Cherdantseva Y., Burnap P., Blyth A., Eden P., Jones K., Soulsby H., Stoddart K.",A review of cyber security risk assessment methods for SCADA systems,2016,Computers and Security,56,,,1,27,no,,,8,10.1016/j.cose.2015.09.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946126337&doi=10.1016%2fj.cose.2015.09.009&partnerID=40&md5=5573f0aaa631a4bcd4369d90e6940055,"School of Computer Science and Informatics, Cardiff University, United Kingdom; Faculty of Computing, Engineering and Science, University of South Wales, United Kingdom; Cyber Operations, Airbus Group Innovations, United Kingdom; Department of International Politics, Aberystwyth University, United Kingdom","Cherdantseva, Y., School of Computer Science and Informatics, Cardiff University, United Kingdom; Burnap, P., School of Computer Science and Informatics, Cardiff University, United Kingdom; Blyth, A., Faculty of Computing, Engineering and Science, University of South Wales, United Kingdom; Eden, P., Faculty of Computing, Engineering and Science, University of South Wales, United Kingdom; Jones, K., Cyber Operations, Airbus Group Innovations, United Kingdom; Soulsby, H., Cyber Operations, Airbus Group Innovations, United Kingdom; Stoddart, K., Department of International Politics, Aberystwyth University, United Kingdom","This paper reviews the state of the art in cyber security risk assessment of Supervisory Control and Data Acquisition (SCADA) systems. We select and in-detail examine twenty-four risk assessment methods developed for or applied in the context of a SCADA system. We describe the essence of the methods and then analyse them in terms of aim; application domain; the stages of risk management addressed; key risk management concepts covered; impact measurement; sources of probabilistic data; evaluation and tool support. Based on the analysis, we suggest an intuitive scheme for the categorisation of cyber security risk assessment methods for SCADA systems. We also outline five research challenges facing the domain and point out the approaches that might be taken. å© 2015 The Authors.",Cyber security; ICS; Review; Risk analysis; Risk assessment methods; Risk management; SCADA,Review,Scopus,2-s2.0-84946126337
"Cheng M.-Y., Hoang N.-D.",A Self-Adaptive Fuzzy Inference Model Based on Least Squares SVM for Estimating Compressive Strength of Rubberized Concrete,2016,International Journal of Information Technology and Decision Making,15,3,,603,619,no,,,1,10.1142/S0219622016500140,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963705467&doi=10.1142%2fS0219622016500140&partnerID=40&md5=acfa4a8e289fc5d536d99eb31ec0eae4,"Department of Civil and Construction Engineering, National Taiwan University of Science and Technology, #43, Section 4, Keelung Road, Daan District, Taipei, Taiwan; Institute of Research and Development, Faculty of Civil Engineering, Duy Tan University, P809 - K7/25 Quang Trung, Danang, Viet Nam","Cheng, M.-Y., Department of Civil and Construction Engineering, National Taiwan University of Science and Technology, #43, Section 4, Keelung Road, Daan District, Taipei, Taiwan; Hoang, N.-D., Institute of Research and Development, Faculty of Civil Engineering, Duy Tan University, P809 - K7/25 Quang Trung, Danang, Viet Nam","This paper presents an AI approach named as self-Adaptive fuzzy least squares support vector machines inference model (SFLSIM) for predicting compressive strength of rubberized concrete. The SFLSIM consists of a fuzzification process for converting crisp input data into membership grades and an inference engine which is constructed based on least squares support vector machines (LS-SVM). Moreover, the proposed inference model integrates differential evolution (DE) to adaptively search for the most appropriate profiles of fuzzy membership functions (MFs) as well as the LS-SVM's tuning parameters. In this study, 70 concrete mix samples are utilized to train and test the SFLSIM. According to experimental results, the SFLSIM can achieve a comparatively low MAPE which is less than 2%. å© 2016 World Scientific Publishing Company.",differential evolution; fuzzy logic; least squares support vector machines; Rubberized concrete; strength estimate,Article,Scopus,2-s2.0-84963705467
"Cheng A.-C., Jiang I.H.-R., Jou J.-Y.",Resource-aware functional ECO patch generation,2016,"Proceedings of the 2016 Design, Automation and Test in Europe Conference and Exhibition, DATE 2016",,,7459462,1036,1041,no,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973659218&partnerID=40&md5=e49a6524a8ab303165eac8adf5769e47,"Department of Electronics Engineering, National Chiao Tung University, Hsinchu, Taiwan; Department of Electrical Engineering, National Central University, Taoyuan, Taiwan","Cheng, A.-C., Department of Electronics Engineering, National Chiao Tung University, Hsinchu, Taiwan; Jiang, I.H.-R., Department of Electronics Engineering, National Chiao Tung University, Hsinchu, Taiwan; Jou, J.-Y., Department of Electronics Engineering, National Chiao Tung University, Hsinchu, Taiwan, Department of Electrical Engineering, National Central University, Taoyuan, Taiwan","Functional Engineering Change Order (ECO) is necessary for logic rectification at late design stages. Existing works mainly focus on identifying a minimal logic difference between the original netlist and the revised netlist, which is called a patch. The patch is then implemented by technology mapping using spare cells. However, there may be insufficient spare cells around the physical location of the patch, or the wires connecting spare cells are too long, thus causing timing violations and routing congestion. In this paper, we propose a resource-aware functional patch generation approach by gate count and wiring cost estimations. In particular, we estimate the number of spare cells required by a patch and define a cost of wire length on it, which considers the physical location of the patch and a set of nearby spare cells. As a result, the patch with minimal wiring cost instead of minimal size is produced. The experiments are conducted on nine industrial testcases. These testcases reflect real problems faced by designers, and the results show our method is promising. å© 2016 EDAA.",,Conference Paper,Scopus,2-s2.0-84973659218
Chen Y.-S.,A comprehensive identification-evidence based alternative for HIV/AIDS treatment with HAART in the healthcare industries,2016,Computer Methods and Programs in Biomedicine,131,,,111,126,no,,,,10.1016/j.cmpb.2016.04.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963499962&doi=10.1016%2fj.cmpb.2016.04.001&partnerID=40&md5=65bb7569e642d300d9e93a1b4f1a33da,"Department of Information Management, Hwa Hsia University of Technology, 111, Gongzhuan Rd., Zhonghe Dist., New Taipei City, Taiwan","Chen, Y.-S., Department of Information Management, Hwa Hsia University of Technology, 111, Gongzhuan Rd., Zhonghe Dist., New Taipei City, Taiwan","Background and Objective: The HIV/AIDS-related issue has given rise to a priority concern in which potential new therapies are increasingly highlighted to lessen the negative impact of highly active anti-retroviral therapy (HAART) in the healthcare industry. With the motivation of ""medical applications,"" this study focuses on the main advanced feature selection techniques and classification approaches that reflect a new architecture, and a trial to build a hybrid model for interested parties. Methods: This study first uses an integrated linear-nonlinear feature selection technique to identify the determinants influencing HAART medication and utilizes organizations of different condition-attributes to generate a hybrid model based on a rough set classifier to study evolving HIV/AIDS research in order to improve classification performance. Results: The proposed model makes use of a real data set from Taiwan's specialist medical center. The experimental results show that the proposed model yields a satisfactory result that is superior to the listed methods, and the core condition-attributes PVL, CD4, Code, Age, Year, PLT, and Sex were identified in the HIV/AIDS data set. In addition, the decision rule set created can be referenced as a knowledge-based healthcare service system as the best of evidence-based practices in the workflow of current clinical diagnosis. Conclusions: This study highlights the importance of these key factors and provides the rationale that the proposed model is an effective alternative to analyzing sustained HAART medication in follow-up studies of HIV/AIDS treatment in practice. å© 2016 Elsevier Ireland Ltd.",Acquired immune deficiency syndrome (AIDS); Highly active anti-retroviral therapy (HAART); Human immunodeficiency virus (HIV); Hybrid model; Linear-nonlinear feature selection,Article,Scopus,2-s2.0-84963499962
"Chatziasimidis F., Stamelos I.",Data collection and analysis of GitHub repositories and users,2015,"IISA 2015 - 6th International Conference on Information, Intelligence, Systems and Applications",,,7388026,,,no,,,,10.1109/IISA.2015.7388026,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963857299&doi=10.1109%2fIISA.2015.7388026&partnerID=40&md5=66222e9a974ce80d622a71d0e5e0240a,"Gnomon Informatics SA, 21 Anton is Tritsis Str., Thessaloniki, Greece; Department of Informatics, Aristotle University of Thessaloniki, Thessaloniki, Greece","Chatziasimidis, F., Gnomon Informatics SA, 21 Anton is Tritsis Str., Thessaloniki, Greece; Stamelos, I., Department of Informatics, Aristotle University of Thessaloniki, Thessaloniki, Greece","In this paper, we present the collection and mining of GitHub data, aiming to understand GitHub user behavior and project success factors. We collected information about approximately 100K projects and 10K GitHub users//owners of these projects, via GitHub API. Subsequently, we statistically analyzed such data, discretized values of features via k-means algorithm, and finally we applied apriori algorithm via weka in order to find out association rules. Having assumed that project success could be measured by the cardinality of downloads we kept only the rules which had as right par a download cardinality higher than a threshold of 1000 downloads. The results provide intersting insight in the GitHub ecosystem and seven success rules for GitHub projects. å© 2015 IEEE.",association rules; discretization; GitHub; GitHub API; k-means; open source; project success,Conference Paper,Scopus,2-s2.0-84963857299
"Chandrakumar T., Parthasarathy S.",An approach to estimate the size of ERP package using package points,2016,Computer Standards and Interfaces,47,,,100,107,no,Not comparative,,1,10.1016/j.csi.2015.10.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963723960&doi=10.1016%2fj.csi.2015.10.003&partnerID=40&md5=b971c307c25683c4a146dc1aee0164ee,"Department of Computer Applications, Thiagarajar College of Engineering, Madurai, India","Chandrakumar, T., Department of Computer Applications, Thiagarajar College of Engineering, Madurai, India; Parthasarathy, S., Department of Computer Applications, Thiagarajar College of Engineering, Madurai, India","Enterprise Resource Planning (ERP) packages are information systems that automate the business processes of organizations thereby improving their operational efficiency substantially. ERP projects that involve customization are often affected by inaccurate estimation of efforts. Size of the software forms the basis for effort estimation. Methods used for effort estimation either employ function points (FP) or lines of code (LOC) to measure the size of customized ERP packages. Literature review reveals that the existing software size methods which are meant for custom-built software products may not be suitable for COTS products such as customized ERP packages. Hence, the effort estimation using conventional methods for customized ERP packages may not be accurate. This paper proposes a new approach to estimate the size of customized ERP packages using Package Points (PP). The proposed approach was validated with data collected from 14 ERP projects delivered by the same company. A positive correlation was observed between Package Points (PP) and the efforts of these projects. This result indicates the feasibility of our proposed approach as well as the positive climate for its utility by the project managers of future ERP projects. Lastly, we examine the implication of these results for practice and future research scope. å© 2015 Elsevier B.V. All rights reserved.",Customization; Efforts; ERP projects; Function points; Package points,Article,Scopus,2-s2.0-84963723960
"Busch A., Koziolek A.",Considering not-quantified quality attributes in an automated design space exploration,2016,"Proceedings - 2016 12th International ACM SIGSOFT Conference on Quality of Software Architectures, QoSA 2016",,,7515435,50,59,no,,,,10.1109/QoSA.2016.10,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983455304&doi=10.1109%2fQoSA.2016.10&partnerID=40&md5=eacd8395d17a69644a8cd62c72cafd45,"Karlsruhe Institute of Technology, Karlsruhe, Germany","Busch, A., Karlsruhe Institute of Technology, Karlsruhe, Germany; Koziolek, A., Karlsruhe Institute of Technology, Karlsruhe, Germany","In a software design process, the quality of the resulting software system is highly driven by the quality of its software architecture (SA). In such a process trade-off decisions must be made between multiple quality attributes (QAs), such as performance or security, that are often competing. Several approaches exist to improve SAs either quantitatively or qualitatively. The first group of approaches requires to quantify each single QA to be considered in the design process, while the latter group of approaches are often fully manual processes. However, time and cost constraints often make it impossible to either quantify all relevant QAs or manually evaluate candidate architectures. Our approach to the problem is to quantify several most important quality requirements, combine them with several not-quantified QAs and use them together in an automated design space exploration process. As our basis, we used the PerOpteryx design space exploration approach, which requires quantified measures for its optimization engine, and extended it in order to combine them with not-quantified QAs. By this, our approach allows optimizing the design space by considering even QAs that can not be quantified due to cost constraints or lack of quantification methodologies. We applied our approach to two case studies to demonstrate its benefits. We showed how performance can be balanced against not-quantified QAs, such as security, using an example derived from an industry case study. å© 2016 IEEE.",Architecture; Automated; Decision; Design; Not-Quantified; Requirements; Trade-off,Conference Paper,Scopus,2-s2.0-84983455304
"Buciakowski M., Puig V.",Guaranteed cost estimation and control for nonlinear system using LMI optimization,2016,"2016 21st International Conference on Methods and Models in Automation and Robotics, MMAR 2016",,,7575311,1211,1216,no,,,,10.1109/MMAR.2016.7575311,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991824672&doi=10.1109%2fMMAR.2016.7575311&partnerID=40&md5=3962ea32f714f47adffba87c9bd0800c,"Institute of Control and Computation Engineering, University of Zielona GÌ_ra, ul. PodgÌ_rna 50, Zielona GÌ_ra, Poland; Automatic Control Department, Universidad PolitÌ©cnica de CataluÌ±a, Rambla Sant Nebridi, 10, Terrassa, Spain","Buciakowski, M., Institute of Control and Computation Engineering, University of Zielona GÌ_ra, ul. PodgÌ_rna 50, Zielona GÌ_ra, Poland; Puig, V., Automatic Control Department, Universidad PolitÌ©cnica de CataluÌ±a, Rambla Sant Nebridi, 10, Terrassa, Spain","In the paper, a methodology for the guaranteed cost estimation and control for nonlinear system discrete-time systems is proposed. To solve such a challenging problem, the article starts with a general description of the system and assumptions regarding its nonlinearities. The subsequent part of the paper describes the design methodology of the robust observer and controller for the predefined cost function using linear matrix inequalities. The final part of the paper presents an illustrative example oriented towards a practical application to the multiple tank system, which illustrates the performance of the proposed approach. å© 2016 IEEE.",,Conference Paper,Scopus,2-s2.0-84991824672
Breskovic D.,Real options analysis for HetNet and WLAN based FiWi access networks deployment,2016,"2016 24th International Conference on Software, Telecommunications and Computer Networks, SoftCOM 2016",,,7772175,,,no,,,,10.1109/SOFTCOM.2016.7772175,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010426972&doi=10.1109%2fSOFTCOM.2016.7772175&partnerID=40&md5=3d6aab16b5ae11052a4421532f18f9c8,"Ericsson Nikola Tesla, Split, Croatia","Breskovic, D., Ericsson Nikola Tesla, Split, Croatia","Hybrid optical-wireless (FiWi) access networks prove to be a competitive solution for smart access networks as they provide broadband services to the users while maintaining lower CAPEX cost. Thorough techno-economic analysis of FiWi access network based on new technologies are needed in order to support the project planning. In this paper we present the techno-economic analysis of FiWi networks comprising the GPON and HetNet/WLAN segments. First the NPV calculation is performed based on real-life scenario. After that real options analysis is conducted in order to include uncertainties and flexibility into the project planning. Based on the obtained results the conclusions on network planning using HetNet/WLAN technologies have been drawn. å© 2016 University of Split, FESB.",access networks; CAPEX; cost estimation; FiWi; GPON; HetNet; Hybrid optical-wireless; NPV; Real options analysis; WLAN,Conference Paper,Scopus,2-s2.0-85010426972
"Bougoudis I., Demertzis K., Iliadis L.",Fast and low cost prediction of extreme air pollution values with hybrid unsupervised learning,2016,Integrated Computer-Aided Engineering,23,2,,115,127,no,,,1,10.3233/ICA-150505,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960949985&doi=10.3233%2fICA-150505&partnerID=40&md5=5058b2bc7ffc793d27d0af1389580678,"Department of Forestry and Management of the Environment and Natural Resources, Democritus University of Thrace, 193 Pandazidou st., N Orestiada, Greece","Bougoudis, I., Department of Forestry and Management of the Environment and Natural Resources, Democritus University of Thrace, 193 Pandazidou st., N Orestiada, Greece; Demertzis, K., Department of Forestry and Management of the Environment and Natural Resources, Democritus University of Thrace, 193 Pandazidou st., N Orestiada, Greece; Iliadis, L., Department of Forestry and Management of the Environment and Natural Resources, Democritus University of Thrace, 193 Pandazidou st., N Orestiada, Greece","Air pollution is the problem of adding harmful substances or other agents into the atmosphere and it is caused by industrial, transport or household activities. It is one of the most serious problems of our times and the determination of the conditions under which we have extreme pollutants' values is a crucial challenge for the modern scientific community. The innovative and effective hybrid algorithm designed and employed in this research effort is entitled Easy Hybrid Forecasting (EHF). The main advantage of the EHF is that each forecasting does not require measurements from sensors, other hardware devices or data that require the use of expensive software. This was done intentionally because the motivation for this work was the development of a hybrid application that can be downloaded for free and used easily by everyday common people with no additional financial cost, running in devices like smart phones. From this point of view it does not require data from sensors or specialized software and it can offer people reliable information about extreme cases. å© 2016 - IOS Press and the author(s). All rights reserved.",Air pollution; Feature selection; Feed forward neural networks; Fuzzy C-means; Neural gas; Random forest; Self-organizing maps,Conference Paper,Scopus,2-s2.0-84960949985
"Borandag E., Yucalar F., Erdogan S.Z.",A case study for the software size estimation through MK II FPA and FP methods,2016,International Journal of Computer Applications in Technology,53,4,,309,314,no,Sizing,,1,10.1504/IJCAT.2016.076777,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973345270&doi=10.1504%2fIJCAT.2016.076777&partnerID=40&md5=a2cd47d73ea3e5d15c14a05e7da3749f,"Department of Software Engineering, Celal Bayar University, Turgutlu, Manisa, Turkey; Department of Software Engineering, Maltepe University, Maltepe, Istanbul, Turkey","Borandag, E., Department of Software Engineering, Celal Bayar University, Turgutlu, Manisa, Turkey; Yucalar, F., Department of Software Engineering, Celal Bayar University, Turgutlu, Manisa, Turkey; Erdogan, S.Z., Department of Software Engineering, Maltepe University, Maltepe, Istanbul, Turkey","Software size estimation is one of the most crucial and daunting tasks for a software project manager. It is very important for the accurate planning and calculation of the software project. The importance of software size estimation becomes critical at the beginning of the software life cycle. Software size estimation is a basic input for the software effort and cost estimation. There are many different approaches and various methods for software size estimation up to now such as the function points method, e.g., IFPUG FPA, Mk II FPA, COSMIC FFP. In this paper, the size of the software project was calculated through the function points method and Mark II FPA. The same software project was implemented by different software development teams. Early estimations were performed for the software project and the data obtained as a result of the teams' studies were compared. Copyright å© 2016 Inderscience Enterprises Ltd.",Function point method; Mark II FPA; Software size estimation,Article,Scopus,2-s2.0-84973345270
"Bohlouli M., Mittas N., Kakarontzas G., Theodosiou T., Angelis L., Fathi M.",Competence assessment as an expert system for human resource management: A mathematical approach,2017,Expert Systems with Applications,70,,,83,102,no,,,,10.1016/j.eswa.2016.10.046,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996720572&doi=10.1016%2fj.eswa.2016.10.046&partnerID=40&md5=22cb0c2540ba0358aea784a997414dab,"Institute of Knowledge Based Systems, University of Siegen, Hoelderlinstr, Siegen, Germany; Department of Informatics, Aristotle University of Thessaloniki, Thessaloniki, Greece; Department of Computer Science and Engineering, T.E.I. of Thessaly, Larissa, Greece","Bohlouli, M., Institute of Knowledge Based Systems, University of Siegen, Hoelderlinstr, Siegen, Germany; Mittas, N., Department of Informatics, Aristotle University of Thessaloniki, Thessaloniki, Greece; Kakarontzas, G., Department of Informatics, Aristotle University of Thessaloniki, Thessaloniki, Greece, Department of Computer Science and Engineering, T.E.I. of Thessaly, Larissa, Greece; Theodosiou, T., Department of Informatics, Aristotle University of Thessaloniki, Thessaloniki, Greece; Angelis, L., Department of Informatics, Aristotle University of Thessaloniki, Thessaloniki, Greece; Fathi, M., Institute of Knowledge Based Systems, University of Siegen, Hoelderlinstr, Siegen, Germany","Efficient human resource management needs accurate assessment and representation of available competences as well as effective mapping of required competences for specific jobs and positions. In this regard, appropriate definition and identification of competence gaps express differences between acquired and required competences. Using a detailed quantification scheme together with a mathematical approach is a way to support accurate competence analytics, which can be applied in a wide variety of sectors and fields. This article describes the combined use of software technologies and mathematical and statistical methods for assessing and analyzing competences in human resource information systems. Based on a standard competence model, which is called a Professional, Innovative and Social competence tree, the proposed framework offers flexible tools to experts in real enterprise environments, either for evaluation of employees towards an optimal job assignment and vocational training or for recruitment processes. The system has been tested with real human resource data sets in the frame of the European project called ComProFITS. å© 2016",Competence analysis; Competence assessment; Competence reference model; Human resource information system; Mathematical representation of competences,Article,Scopus,2-s2.0-84996720572
"Bilal M., Oyedele L.O., Qadir J., Munir K., Ajayi S.O., Akinade O.O., Owolabi H.A., Alaka H.A., Pasha M.","Big Data in the construction industry: A review of present status, opportunities, and future trends",2016,Advanced Engineering Informatics,30,3,,500,521,no,,,1,10.1016/j.aei.2016.07.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978529080&doi=10.1016%2fj.aei.2016.07.001&partnerID=40&md5=2e3c4aa24b24f44b04d3ef411258aa75,"Bristol Enterprise, Research and Innovation Centre (BERIC), Bristol Business School, University of West of the England, Bristol, United Kingdom; School of Electrical Engineering & Computer Science (SEECS), National University of Sciences & Technology (NUST), Islamabad, Pakistan; Department of Information Technology, Bahauddin Zakariya University, Multan, Pakistan","Bilal, M., Bristol Enterprise, Research and Innovation Centre (BERIC), Bristol Business School, University of West of the England, Bristol, United Kingdom; Oyedele, L.O., Bristol Enterprise, Research and Innovation Centre (BERIC), Bristol Business School, University of West of the England, Bristol, United Kingdom; Qadir, J., School of Electrical Engineering & Computer Science (SEECS), National University of Sciences & Technology (NUST), Islamabad, Pakistan; Munir, K., Bristol Enterprise, Research and Innovation Centre (BERIC), Bristol Business School, University of West of the England, Bristol, United Kingdom; Ajayi, S.O., Bristol Enterprise, Research and Innovation Centre (BERIC), Bristol Business School, University of West of the England, Bristol, United Kingdom; Akinade, O.O., Bristol Enterprise, Research and Innovation Centre (BERIC), Bristol Business School, University of West of the England, Bristol, United Kingdom; Owolabi, H.A., Bristol Enterprise, Research and Innovation Centre (BERIC), Bristol Business School, University of West of the England, Bristol, United Kingdom; Alaka, H.A., Bristol Enterprise, Research and Innovation Centre (BERIC), Bristol Business School, University of West of the England, Bristol, United Kingdom; Pasha, M., Department of Information Technology, Bahauddin Zakariya University, Multan, Pakistan","The ability to process large amounts of data and to extract useful insights from data has revolutionised society. This phenomenon‰ÛÓdubbed as Big Data‰ÛÓhas applications for a wide assortment of industries, including the construction industry. The construction industry already deals with large volumes of heterogeneous data; which is expected to increase exponentially as technologies such as sensor networks and the Internet of Things are commoditised. In this paper, we present a detailed survey of the literature, investigating the application of Big Data techniques in the construction industry. We reviewed related works published in the databases of American Association of Civil Engineers (ASCE), Institute of Electrical and Electronics Engineers (IEEE), Association of Computing Machinery (ACM), and Elsevier Science Direct Digital Library. While the application of data analytics in the construction industry is not new, the adoption of Big Data technologies in this industry remains at a nascent stage and lags the broad uptake of these technologies in other fields. To the best of our knowledge, there is currently no comprehensive survey of Big Data techniques in the context of the construction industry. This paper fills the void and presents a wide-ranging interdisciplinary review of literature of fields such as statistics, data mining and warehousing, machine learning, and Big Data Analytics in the context of the construction industry. We discuss the current state of adoption of Big Data in the construction industry and discuss the future potential of such technologies across the multiple domain-specific sub-areas of the construction industry. We also propose open issues and directions for future work along with potential pitfalls associated with Big Data adoption in the industry. å© 2016 Elsevier Ltd",Big Data Analytics; Big Data Engineering; Construction industry; Machine learning,Review,Scopus,2-s2.0-84978529080
"Bhatt D., Gite S.",Novel driver behavior model analysis using hidden markov model to increase road safety in smart cities,2016,ACM International Conference Proceeding Series,04-05-March-2016,, a125,,,no,,,,10.1145/2905055.2905185,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988650181&doi=10.1145%2f2905055.2905185&partnerID=40&md5=79324b88f6c7736e9d36e7f3cc01bacb,"Symbiosis Institute of Technology, Pune, India","Bhatt, D., Symbiosis Institute of Technology, Pune, India; Gite, S., Symbiosis Institute of Technology, Pune, India","The ability to categorize the driver behavior is very essentia for advance driver assistance system (ADAS). A metho for identifying driver's behavior is important to assist operativ mode transition between the driver and autonomou vehicles. We propose a driver behavior recognition metho in terms of steering and brake using Hidden Markov Mode (HMM). We are also proposing a HMM-based actuation behavio model for obstacle avoidance using collision avoidanc algorithm. We plan to analyze di-erent driver behavior based on HMM model and produce the results. Afte generating results, we intend to empower it further wit Dempster-Shafer Theory (DST) in order to improve system' performance and accuracy. å© 2016 ACM.",ADAS; Collision Avoidance Algorithm; Dempster-Shafer Theory; Driver Behavior; Hidden Markov Model,Conference Paper,Scopus,2-s2.0-84988650181
"Besbes G., Baazaoui-Zghal H.",Personalized and context-aware retrieval based on fuzzy ontology profiling,2017,Integrated Computer-Aided Engineering,24,1,,87,103,no,,,,10.3233/ICA-160525,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009895181&doi=10.3233%2fICA-160525&partnerID=40&md5=f77866bc175d6166407b46261517576f,"Riadi Laboratory, University of Manouba, Tunis, Tunisia","Besbes, G., Riadi Laboratory, University of Manouba, Tunis, Tunisia; Baazaoui-Zghal, H., Riadi Laboratory, University of Manouba, Tunis, Tunisia","Web Information retrieval aims to satisfy users' search needsSS which are highly dependent on their interests and preferences. In this context, web personalization provides an adapted information retrieval. Specific computational modeling is required to address the large amount of heterogeneous electronic documents and the lack of semantic annotations on the web which makes knowledge discovery challenging. In this paper, a novel system, based on fuzzy ontological user profile is proposed. The latter, is composed of history, positive and negative preferences. An implicit relevance judgments method is also introduced. Furthermore, the system is context-aware by integrating novel contextual similarity measures and supporting semantic fuzzification. Our proposal has been implemented and has endured a twofold evaluation. The results show that the proposed system can provide more personalized results and confirm the interest of the context-aware search. å© 2017 - IOS Press and the author(s). All rights reserved.",contextual similarity measures; Fuzzy modular ontologies; information retrieval; user profiling,Review,Scopus,2-s2.0-85009895181
"Bellet A., Bernabeu J.F., Habrard A., Sebban M.",Learning discriminative tree edit similarities for linear classification‰ÛÓApplication to melody recognition,2016,Neurocomputing,214,,,155,161,no,,,,10.1016/j.neucom.2016.06.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992530924&doi=10.1016%2fj.neucom.2016.06.006&partnerID=40&md5=f2a0d47d6d6e5d8d617325b5113da787,"University of Southern California, Department of Computer Science, Los Angeles, CA, United States; Dept. Lenguajes y Sistemas Informaticos, Universidad de Alicante, Spain; UniversitÌ© Jean Monnet de Saint-Etienne, Laboratoire Hubert Curien, UMR CNRS 5516, France","Bellet, A., University of Southern California, Department of Computer Science, Los Angeles, CA, United States; Bernabeu, J.F., Dept. Lenguajes y Sistemas Informaticos, Universidad de Alicante, Spain; Habrard, A., UniversitÌ© Jean Monnet de Saint-Etienne, Laboratoire Hubert Curien, UMR CNRS 5516, France; Sebban, M., UniversitÌ© Jean Monnet de Saint-Etienne, Laboratoire Hubert Curien, UMR CNRS 5516, France","Similarity functions are a fundamental component of many learning algorithms. When dealing with string or tree-structured data, measures based on the edit distance are widely used, and there exist a few methods for learning them from data. In this context, we recently proposed GESL (Bellet et al., 2012 [3]), an approach to string edit similarity learning based on loss minimization which offers theoretical guarantees as to the generalization ability and discriminative power of the learned similarities. In this paper, we argue that GESL, which has been originally dedicated to deal with strings, can be extended to trees and lead to powerful and competitive similarities. We illustrate this claim on a music recognition task, namely melody classification, where each piece is represented as a tree modeling its structure as well as rhythm and pitch information. The results show that GESL outperforms standard as well as probabilistically-learned edit distances and that it is able to describe consistently the underlying melodic similarity model. å© 2016 Elsevier B.V.",Convex optimization; Edit distance; Melody recognition; Tree-structured data,Article,Scopus,2-s2.0-84992530924
"Bardsiri A.K., Hashemi S.M., Razzazi M.",GVSEE: A Global Village Service Effort Estimator to Estimate Software Services Development Effort,2016,Applied Artificial Intelligence,30,5,,396,428,no,,,,10.1080/08839514.2016.1185858,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977151089&doi=10.1080%2f08839514.2016.1185858&partnerID=40&md5=12dd4d4c44e131acd55b89c339d6d6d0,"Computer Engineering Department, Science and Research Branch, Islamic Azad University, Tehran, Iran; Computer Engineering and IT Department, Amirkabir University of Technology, Tehran, Iran","Bardsiri, A.K., Computer Engineering Department, Science and Research Branch, Islamic Azad University, Tehran, Iran; Hashemi, S.M., Computer Engineering Department, Science and Research Branch, Islamic Azad University, Tehran, Iran; Razzazi, M., Computer Engineering and IT Department, Amirkabir University of Technology, Tehran, Iran","As software services have become a main and basic part of companies in recent years, accurate and efficient estimates of required effort for their development has turned into a major concern. Furthermore, the great variety, complexity, nonnormality, and inconsistency of software services have made estimation of the needed development effort a very difficult task. In spite of the numerous studies conducted, and improvements made, in the past, no single model has yet been introduced that can reliably estimate the required effort. All the proposed methods enjoy suitable performance under specific conditions but lack satisfactory accuracy in a general and global space. Therefore, apparently, it is impossible to introduce a global and efficient model for all types of services. This article proposes a new model called GVSEE that emphasizes the idea ‰ÛÏThink locally, act globally.‰Û Unlike previous studies, this model does not rely on a specific method and, in addition to combining methods, takes a local look at each software service with the help of fuzzy clustering. The model was evaluated on the real dataset ISBSG and on two artificial datasets, and results obtained indicate its tangible efficiency and the lack of accuracy of other models. As well as its greater accuracy, other advantages of the proposed model over other models are its adaptability and flexibility in confronting complexities and uncertainties present in the area of software services. å© 2016 Taylor & Francis.",,Article,Scopus,2-s2.0-84977151089
"Badri M., Badri L., Flageol W.",Source and test code size prediction a comparison between Use Case Metrics and Objective Class Points,2016,ENASE 2016 - Proceedings of the 11th International Conference on Evaluation of Novel Software Approaches to Software Engineering,,,,172,180,no,Sizing,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979517079&partnerID=40&md5=8642c08ad2a49a2ff108ede5a7ec1a14,"Software Engineering Research Laboratory, Department of Mathematics and Computer Science, University of Quebec, Trois-RiviÌ¬res, QC, Canada","Badri, M., Software Engineering Research Laboratory, Department of Mathematics and Computer Science, University of Quebec, Trois-RiviÌ¬res, QC, Canada; Badri, L., Software Engineering Research Laboratory, Department of Mathematics and Computer Science, University of Quebec, Trois-RiviÌ¬res, QC, Canada; Flageol, W., Software Engineering Research Laboratory, Department of Mathematics and Computer Science, University of Quebec, Trois-RiviÌ¬res, QC, Canada","Source code size, in terms of SLOC (Source Lines of Code), is an important parameter of many parametric software development effort estimation methods. Moreover, test code size, in terms of TLOC (Test Lines of Code), has been used in many studies to indicate the effort involved in testing. This paper aims at comparing empirically the Use Case Metrics (UCM) method, a use case model based method that we proposed in previous work, and the Objective Class Points (OCP) method in terms of early prediction of SLOC and TLOC for object-oriented software. We used both simple and multiple linear regression methods to build the prediction models. An empirical comparison, using data collected from four open source Java projects, is reported in the paper. Overall, results provide evidence that the multiple linear regression model, based on the combination of the use case metrics, is more accurate in terms of early prediction of SLOC and TLOC than: (1) the simple linear regression models based on each use case metric, and (2) the simple linear regression model based on the OCP method. Copyright å© 2016 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.",Class diagrams; Linear regression; Objective Class Points; Prediction models; Source code size; Test code size; Use Case Metrics; Use cases,Conference Paper,Scopus,2-s2.0-84979517079
"Ayoung A.D., Sigweni B., Abbott P.",Case-based reasoning system for predicting the sustainability of a telecentre,2015,"2015 10th International Conference for Internet Technology and Secured Transactions, ICITST 2015",,,7412072,125,130,no,,,,10.1109/ICITST.2015.7412072,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964466973&doi=10.1109%2fICITST.2015.7412072&partnerID=40&md5=492f24192e47724dc040f90b10562872,"Department of Computer Science, Brunel University London, United Kingdom","Ayoung, A.D., Department of Computer Science, Brunel University London, United Kingdom; Sigweni, B., Department of Computer Science, Brunel University London, United Kingdom; Abbott, P., Department of Computer Science, Brunel University London, United Kingdom","Telecentre implementation has been inundated with failure in developing countries. This has necessitated the need for evaluations to unearth reasons for such occurrences. Various methods have been proposed to evaluate IT projects. However, very few of these methods have been used to predict telecentre failure. We combine two approaches (ICT4D evaluation and Machine Learning) to predict the likelihood for failure of a telecentre. Through the use of a case study, this paper uses the well-established Case-Based Reasoning (CBR) methodology to predict failure of telecentres. We apply CBR on real life dataset to predict the Design-Reality Gap score (DRGS). We compare three CBR methods with a naÌøve benchmark using ArchANGEL. We demonstrate through our experiments that CBR can be used to predict DRGS. This gives a refreshing indication suggesting that it may be feasible to use CBR to evaluate ICT initiatives and to predict adequately outcome of an initiative. Through this mechanism, it may be possible for managers and owners of telecentres to pre-empt an outcome and have the advantage to take mitigating steps. This affords managers an opportunity for remedial action for sustainability. å© 2015 Infonomics Society.",case-based reasoning; design-reality gap score estimation; ICT4D; sustainability; telecentre,Conference Paper,Scopus,2-s2.0-84964466973
"Avrachenkov K., Ribeiro B., Sreedharan J.K.",Inference in OSNs via lightweight partial crawls,2016,SIGMETRICS/ Performance 2016 - Proceedings of the SIGMETRICS/Performance Joint International Conference on Measurement and Modeling of Computer Science,,,,165,177,no,,,,10.1145/2896377.2901477,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978764768&doi=10.1145%2f2896377.2901477&partnerID=40&md5=90b0ed54d61810648928671871bd4677,"INRIA, Sophia Antipolis, France; Dept. of Computer Science, Purdue University, West Lafayette, IN, United States","Avrachenkov, K., INRIA, Sophia Antipolis, France; Ribeiro, B., Dept. of Computer Science, Purdue University, West Lafayette, IN, United States; Sreedharan, J.K., INRIA, Sophia Antipolis, France","Are Online Social Network (OSN) A users more likely to form friendships with those with similar attributes? Do users at an OSN B score content more favorably than OSN C users? Such questions frequently arise in the context of So- cial Network Analysis (SNA) but often crawling an OSN net-work via its Application Programming Interface (API) is the only way to gather data from a third party. To date, these partial API crawls are the majority of public datasets and the synonym of lack of statistical guarantees in incomplete- data comparisons, severely limiting SNA research progress. Using regenerative properties of the random walks, we pro- pose estimation techniques based on short crawls that have proven statistical guarantees. Moreover, our short crawls can be implemented in massively distributed algorithms. We also provide an adaptive crawler that makes our method parameter-free, significantly improving our statistical guar- Antees. We then derive the Bayesian approximation of the posterior of the estimates, and in addition, obtain an estima-tor for the expected value of node and edge statistics in an equivalent configuration model or Chung-Lu random graph model of the given network (where nodes are connected ran- domly) and use it as a basis for testing null hypotheses. The theoretical results are supported with simulations on a variety of real-world networks. c 2016 ACM.",Bayesian inference; Graph sampling; Random walk on graphs; Social network analysis,Conference Paper,Scopus,2-s2.0-84978764768
"Atapattu S., Liyanage N., Menuka N., Perera I., Pasqual A.",Real time all intra HEVC HD encoder on FPGA,2016,"Proceedings of the International Conference on Application-Specific Systems, Architectures and Processors",2016-November,,7760792,191,195,no,,,,10.1109/ASAP.2016.7760792,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006915758&doi=10.1109%2fASAP.2016.7760792&partnerID=40&md5=c60c218bc8a2435114b9534db9163c8b,"Dept. of Electronic and Telecommunication Engineering, University of Moratuwa, Sri Lanka","Atapattu, S., Dept. of Electronic and Telecommunication Engineering, University of Moratuwa, Sri Lanka; Liyanage, N., Dept. of Electronic and Telecommunication Engineering, University of Moratuwa, Sri Lanka; Menuka, N., Dept. of Electronic and Telecommunication Engineering, University of Moratuwa, Sri Lanka; Perera, I., Dept. of Electronic and Telecommunication Engineering, University of Moratuwa, Sri Lanka; Pasqual, A., Dept. of Electronic and Telecommunication Engineering, University of Moratuwa, Sri Lanka","Higher compression efficiency in HEVC encoders comes with increased computational complexity, making real time encoding of high resolution videos a challenging task. This challenge can be addressed by software, yet hardware solutions are more appealing due to their superior performance and low power consumption. This paper presents an FPGA based hardware implementation of an all intra HEVC encoder, which can encode 8 bits per sample, 1920ÌÑ1080 resolution, 30 frames per second raw video, that is viable in real time even at low operating frequencies. A major obstacle to real time encoding in available architectures is the dependency created by reference generation. Moreover, each coding unit (CU) has to be processed in multiple configurations to determine the most efficient split and prediction mode representation, based on the bit stream generated. We propose a new three stage architecture to reduce these dependencies and increase parallelism. Feedback needed for CU split and prediction direction decision from binarization is avoided by a Hadamard based early decision method. Feedback constrained coefficient and reconstruction derivation module exploits several optimization techniques. All modules can operate at 200 MHz and the encoder can achieve real time encoding with a minimum operating frequency of 140 MHz. The design consumes 83K LUTs, 28K registers, and 34 DSPs when implemented on Xilinx Zynq ZC706. å© 2016 IEEE.",early CU partitioning; early mode decision; high efficiency video coding; intra coding; low latency encoding; video coding on FPGA,Conference Paper,Scopus,2-s2.0-85006915758
"Assar S., Borg M., Pfahl D.",Using text clustering to predict defect resolution time: a conceptual replication and an evaluation of prediction accuracy,2016,Empirical Software Engineering,21,4,,1437,1475,no,,,,10.1007/s10664-015-9391-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933556607&doi=10.1007%2fs10664-015-9391-7&partnerID=40&md5=6dc1fb96c4763d2f085822b925949370,"Ecole de Management, Institut Mines-Telecom, 9, rue C. Fourier, Evry, France; Department of Computer Science, Lund University, Box 118, Lund, Sweden; Institute of Computer Science, University of Tartu, J. Liivi 2, Tartu, Estonia","Assar, S., Ecole de Management, Institut Mines-Telecom, 9, rue C. Fourier, Evry, France; Borg, M., Department of Computer Science, Lund University, Box 118, Lund, Sweden; Pfahl, D., Institute of Computer Science, University of Tartu, J. Liivi 2, Tartu, Estonia","Defect management is a central task in software maintenance. When a defect is reported, appropriate resources must be allocated to analyze and resolve the defect. An important issue in resource allocation is the estimation of Defect Resolution Time (DRT). Prior research has considered different approaches for DRT prediction exploiting information retrieval techniques and similarity in textual defect descriptions. In this article, we investigate the potential of text clustering for DRT prediction. We build on a study published by Raja (2013) which demonstrated that clusters of similar defect reports had statistically significant differences in DRT. Raja‰Ûªs study also suggested that this difference between clusters could be used for DRT prediction. Our aims are twofold: First, to conceptually replicate Raja‰Ûªs study and to assess the repeatability of its results in different settings; Second, to investigate the potential of textual clustering of issue reports for DRT prediction with focus on accuracy. Using different data sets and a different text mining tool and clustering technique, we first conduct an independent replication of the original study. Then we design a fully automated prediction method based on clustering with a simulated test scenario to check the accuracy of our method. The results of our independent replication are comparable to those of the original study and we confirm the initial findings regarding significant differences in DRT between clusters of defect reports. However, the simulated test scenario used to assess our prediction method yields poor results in terms of DRT prediction accuracy. Although our replication confirms the main finding from the original study, our attempt to use text clustering as the basis for DRT prediction did not achieve practically useful levels of accuracy. å© 2015, Springer Science+Business Media New York.",Data clustering; Defect resolution time; Independent replication; Prediction; Simulation; Text mining,Article,Scopus,2-s2.0-84933556607
"Asgarpour M., Dalsgaard Sorensen J.",O&M modeling of offshore wind farms-State of the art and future developments,2016,Proceedings - Annual Reliability and Maintainability Symposium,2016-April,,7448057,,,no,,,,10.1109/RAMS.2016.7448057,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968750579&doi=10.1109%2fRAMS.2016.7448057&partnerID=40&md5=a40b9dcdd5d77dbf6ffd11e10749ac9d,"Aalborg University and ECN, Westerduinweg 1, Petten, Netherlands; Aalborg University, Sofiendalsvej 11, Aalborg SV, Denmark","Asgarpour, M., Aalborg University and ECN, Westerduinweg 1, Petten, Netherlands; Dalsgaard Sorensen, J., Aalborg University, Sofiendalsvej 11, Aalborg SV, Denmark","In this paper the state of the art in O&M models for O&M cost estimation of offshore wind farms is discussed and then, a case study for O&M cost estimation of an 800 MW reference offshore wind farm is given. Moreover, a framework for an ideal O&M strategy optimizer to achieve the maximum possible O&M costs reduction during operational years of an offshore wind farm is described and recommendations are given. å© 2016 IEEE.",health monitoring plan; inspection plan; Maintenance strategy optimization; O&M cost estimation; offshore wind farm; operation and maintenance,Conference Paper,Scopus,2-s2.0-84968750579
"Arzate E., Huitzil P., MartÌ_nez B.E., Grossmann I.",Water and Energy. Why Not Optimize them Simultaneously?,2016,Computer Aided Chemical Engineering,38,,,2139,2144,no,,,,10.1016/B978-0-444-63428-3.50361-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994386000&doi=10.1016%2fB978-0-444-63428-3.50361-1&partnerID=40&md5=0a639d01725dd71830f58cde6d4f0db2,"Instituto Mexicano del PetrÌ_leo, Eje Central LÌÁzaro CÌÁrdenas No. 152, Edif. 19-A, 2å¡ Piso, 19-A 232, Col. San Bartolo Atepehuacan, C. P., MÌ©xico, D. F., United States; Department of Chemical Engineering, Carnegie Mellon University, Pittsburg, PA, United States","Arzate, E., Instituto Mexicano del PetrÌ_leo, Eje Central LÌÁzaro CÌÁrdenas No. 152, Edif. 19-A, 2å¡ Piso, 19-A 232, Col. San Bartolo Atepehuacan, C. P., MÌ©xico, D. F., United States; Huitzil, P., Instituto Mexicano del PetrÌ_leo, Eje Central LÌÁzaro CÌÁrdenas No. 152, Edif. 19-A, 2å¡ Piso, 19-A 232, Col. San Bartolo Atepehuacan, C. P., MÌ©xico, D. F., United States; MartÌ_nez, B.E., Instituto Mexicano del PetrÌ_leo, Eje Central LÌÁzaro CÌÁrdenas No. 152, Edif. 19-A, 2å¡ Piso, 19-A 232, Col. San Bartolo Atepehuacan, C. P., MÌ©xico, D. F., United States; Grossmann, I., Department of Chemical Engineering, Carnegie Mellon University, Pittsburg, PA, United States","This paper describes the optimization of the energy intensity in industrial complexes that corresponds to minimizing this parameter (energy intensity) which is a ratio between energy and water consumption. There are some modern technologies/techniques that can reduce water consumption but they have the downside of requiring more energy up to the point that, if used, they would increase the overall energy consumption. This paper describes a methodology to reduce water consumption, while optimizing the energy intensity (kWh/M3) taking into account the treatment and transportation processes. This methodology finds good feasible solutions by following 7 steps, where each of them adds more constraints to the system. All the equations describing the costs were annualized and linearized and then compared to the results obtained with the commercial software ‰ÛÏAspen Capital Cost Estimator‰Û. After validating the equations they were added to the optimization model so a solution can be more easily obtained. In this paper we present case studies of a petrochemical complex, from cooling towers to pluvial treatment, showing the energy intensity in each one of them. å© 2016 Elsevier B.V.",Process integration; Water Network in Oil Industry; Water optimization: Methodology to find optimal solutions,Book Chapter,Scopus,2-s2.0-84994386000
"Arulmurugan R., Venkatesan T.",Performance calculation and probability lifecycle price investigation of standalone photovoltaic power supply for a residential applications,2015,IC-GET 2015 - Proceedings of 2015 Online International Conference on Green Engineering and Technologies,,,7453785,,,no,,,,10.1109/GET.2015.7453785,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966267437&doi=10.1109%2fGET.2015.7453785&partnerID=40&md5=e9b57e566ac513190fadd742ea075782,"Dept of EEE, Anna University, Chennai, Tamil Nadu, India; Dept of EEE, Sasurie Academy of Engineering, Coimbatore, TamilNadu, India; Dept of EEE, K.S. Rangasamy College of Technology, Tiruchengode, India","Arulmurugan, R., Dept of EEE, Anna University, Chennai, Tamil Nadu, India, Dept of EEE, Sasurie Academy of Engineering, Coimbatore, TamilNadu, India; Venkatesan, T., Dept of EEE, K.S. Rangasamy College of Technology, Tiruchengode, India","This paper offerings rigorous experimental outside performance of a 200 Wp standalone photovoltaic (PV)energy scheme in Salem, Tamil Nadu, India for clear weather data in month of 12th March 2014. The daily energy produced from the proposing standalone PV scheme was experimentally originating in the variety of 140-160 W h/day contingents on the usual sky circumstances. The number of times and everyday power produced corresponding to different weather state in March month were used to found yearly power creation from the proposing photovoltaic scheme. There are dissimilar load profiles with and without ground to air heat exchanger appropriate for varied periods like winter, rainy and summer. The hourly effectiveness of the standalone PV scheme components are found and offered in this paper. The lifespan price analysis for the proposing classic standalone photovoltaic scheme is conceded out to determine unit price of power. The effect of yearly degradation rate of photovoltaic scheme efficiency is also obtainable in this work. å© 2015 IEEE.",Lifecycle price estimation; performance evaluation; Photovoltaic energy scheme; Solar Energy,Conference Paper,Scopus,2-s2.0-84966267437
"Andrews A.A., Beaver P., Lucente J.",Towards better help desk planning: Predicting incidents and required effort,2016,Journal of Systems and Software,117,,,426,449,no,,,,10.1016/j.jss.2016.03.063,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964344416&doi=10.1016%2fj.jss.2016.03.063&partnerID=40&md5=003cbd9e5810db7001268ffc921d06ce,"Department of Computer Science, University of Denver, Denver, CO, United States; Daniels College of Business, University of Denver, Denver, CO, United States","Andrews, A.A., Department of Computer Science, University of Denver, Denver, CO, United States; Beaver, P., Daniels College of Business, University of Denver, Denver, CO, United States; Lucente, J., Department of Computer Science, University of Denver, Denver, CO, United States","In this case study, a cost model for help desk operations is developed. The cost model relates predicted incidents to labor costs. Since incident estimation for hundreds of products is time-consuming, we use cluster analysis to group similarly behaving products in clusters, for which we then estimate incidents based on the representative product in the cluster. Incidents are predicted using software reliability growth models. The cost to resolve the incidents is predicted using historical labor data for the resolution of incidents. Cluster analysis is used to group products with similar help desk incident characteristics. We use Principal Components Analysis to determine one product per cluster for the prediction of incidents for all members of the cluster, so as to reduce estimation cost. We were able to predict incidents for a cluster based on this product alone and do so successfully for all clusters with accuracy comparable to making predictions for each product in the portfolio. Linear regression is used with cost data for the resolution of incidents to relate incident predictions to help desk labor costs. The cost model is then validated by successfully demonstrating cost prediction accuracy for one month prediction intervals over a 22 month period. å© 2016 Elsevier Inc. All rights reserved.",Cluster analysis; Cost model; Help desk; IT operations; Principal components analysis; Software reliability growth model,Article,Scopus,2-s2.0-84964344416
"An M., Wang Z., Zhang Y.",Demonstration and verification system for artificial intelligent swarm control,2016,Jiqiren/Robot,38,3,,265,275,no,,,,10.13973/j.cnki.robot.2016.0265,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983749088&doi=10.13973%2fj.cnki.robot.2016.0265&partnerID=40&md5=a404f7bb8a4b033a875729d3b07ba1ac,"School of Aerospace Engineering, Tsinghua University, Beijing, China; Astronaut Center of China, Beijing, China","An, M., School of Aerospace Engineering, Tsinghua University, Beijing, China, Astronaut Center of China, Beijing, China; Wang, Z., School of Aerospace Engineering, Tsinghua University, Beijing, China; Zhang, Y., School of Aerospace Engineering, Tsinghua University, Beijing, China","A low-cost demonstration and verification system is developed and established in laboratory condition to demonstrate and verify the self-organizing control strategy of an intelligent swarm. The system consists of an arena, multiple mobile individuals, the cooperative identification logo and the identification unit, and the control and information allocating unit. The cooperative identification and the identification unit provides the accurate identification and the high-precision position and direction data of the multiple mobile individuals. The control and information allocating unit simulates the rules of the information exchanging and control among the individuals of an intelligent swarm. Finally, the mobile self-organized detecting swarm is taken as a demonstration case, and the self-organizing control strategy based on artificial potential field is demonstrated and verified in the system. The demonstration results show that the proposed system can demonstrate and verify the operation process of an intelligent swarm in laboratory condition, and the demonstration can provide the actual performance of an intelligent swarm. å© 2016, Science Press. All right reserved.",Artificial intelligent swarm; Demonstration and verification system; Self-organizing; Visual fiducial identification,Article,Scopus,2-s2.0-84983749088
"Ali K.N., Mustaffa N.E., Keat Q.J., Enegbuma W.I.",Building Information Modelling (BIM) educational framework for quantity surveying students: The Malaysian perspective,2016,Journal of Information Technology in Construction,21,,,140,151,no,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991510659&partnerID=40&md5=f60522785cce591d6a0cf49c88a42dbb,"Department of Quantity Surveying, Faculty of Built Environment, Universiti Teknologi Malaysia, Malaysia; Royal Institution of Surveyors Malaysia (RISM), Malaysia","Ali, K.N., Department of Quantity Surveying, Faculty of Built Environment, Universiti Teknologi Malaysia, Malaysia; Mustaffa, N.E., Department of Quantity Surveying, Faculty of Built Environment, Universiti Teknologi Malaysia, Malaysia; Keat, Q.J., Royal Institution of Surveyors Malaysia (RISM), Malaysia; Enegbuma, W.I., Royal Institution of Surveyors Malaysia (RISM), Malaysia","For the past few years, the wave of Building Information Modelling (BIM) has been hitting the shores of Malaysian construction industry. The unprecedented change it brings to the design responsibilities of construction professionals in Malaysia has led to a pre-emptive strategic focus for Quantity Surveying (QS) profession. The QS profession adheres to the 5th dimension of BIM, which invariably translates to the context of costing, offering the capability to generate quantity take-off, counts and measurement directly from a model. BIM digitalized data lead to accurate automated estimation which reduces variability in cost estimation. From the academic point of view, requirements to meet this paradigm shift to BIM requires an enhancement to the existing set of skills and knowledge available in Malaysian institutions of higher learning. The promotion of BIM educational framework for the QS graduates have been professional body led. This is carried out by the Royal Institution of Surveyors Malaysia (RISM) which has been actively involved in establishing the educational framework which in turn has been referred to by the higher institutions that offers quantity surveying program. This paper describes the educational framework for the QS in the context of BIM implementation that charts a route on how knowledge on BIM principles and its application can be imparted to the whole-life interdisciplinary design and construction with prime focus on the QS scope of work. The primary aim of the framework lies in equipping QS graduates with the necessary skills in project delivery through the use of BIM by focusing on four spheres of attainment level and two different level of knowledge acquisition. å© 2016 The author.",Building information modelling; Education; Framework; Malaysia; Quantity surveying,Article,Scopus,2-s2.0-84991510659
"Alemam A., Li S.",Matrix-based quality tools for concept generation in eco-design,2016,Concurrent Engineering Research and Applications,24,2,,113,128,no,,,,10.1177/1063293X15625097,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973502231&doi=10.1177%2f1063293X15625097&partnerID=40&md5=16855705e5fbba1d63549bc43436018e,"Department of Mechanical and Industrial Engineering, Concordia University, Montreal, QC, Canada; Department of Mechanical and Manufacturing Engineering, University of Calgary, Calgary, AB, Canada","Alemam, A., Department of Mechanical and Industrial Engineering, Concordia University, Montreal, QC, Canada; Li, S., Department of Mechanical and Manufacturing Engineering, University of Calgary, Calgary, AB, Canada","While quality management systems are familiar to industries for continuous improvements of products, the associated tools can also make significant contributions to address environmental concerns, aligning eco-design in the product improvement process. In this context, this article focuses on the adaptions of quality tools for supporting the generation of eco-design concepts. Particularly, this article develops a method that integrates quality function deployment and functional analysis via relational matrices. The proposed method has three steps. In step 1, an existing design is analyzed, and the associations between design entities are captured in three types of relational matrices: requirements and metrics, metrics and components, and functions and components. In step 2, the mapping between requirements and functions are determined via matrix multiplications, and then a morphological chart is established to generate possible design concepts. In step 3, the generated concepts are evaluated using Pugh charts via the delegated engineering metrics. A hair dryer has been selected as an application to demonstrate the proposed method for supporting eco-design. å© SAGE Publications.",decomposition; eco-design; functional analysis; morphological chart; quality function deployment; relational matrices,Article,Scopus,2-s2.0-84973502231
"Ajay Prakash B.V., Ashoka D.V., Manjunath Aradhya V.N.",A novel framework for integrating data mining techniques to software development phases,2016,Advances in Intelligent Systems and Computing,434,,,485,493,no,,,,10.1007/978-81-322-2752-6_47,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959036722&doi=10.1007%2f978-81-322-2752-6_47&partnerID=40&md5=5eb0b37f9dc86e8ff5f88548fe0fd22f,"Department of Computer Science and Engineering, SJBIT, Bengaluru, India; Department of Computer Science and Engineering, JSSATE, Bengaluru, India; Department of MCA, Sri Jayachamarajendra College of Engineering, Mysuru, India","Ajay Prakash, B.V., Department of Computer Science and Engineering, SJBIT, Bengaluru, India; Ashoka, D.V., Department of Computer Science and Engineering, JSSATE, Bengaluru, India; Manjunath Aradhya, V.N., Department of MCA, Sri Jayachamarajendra College of Engineering, Mysuru, India","In software development process, phases such as development effort estimation, code optimization, source code defect detection and software reuse are very important in order to improve the productivity and quality of the software. Software repository data produced in each phases have increased as component of software development process and new data analysis techniques have emerged in order to optimize the software development process. There is a gap between the software project management practices and the need of valuable data from software repository. To overcome this gap, a novel integrated framework is proposed, which integrates data mining techniques to extract valuable information from software repository and software metrics are used in different phases of software development process. Integrated framework can be used by software development project managers to improve quality of software and reduce time in predicting effort estimation, optimizing source code, defect detection and classification. å© Springer India 2016.",Code optimization; Defect prediction and classification; Software development process; Software effort estimation; Software reuse,Conference Paper,Scopus,2-s2.0-84959036722
"Ajay J., Rathore A.S., Song C., Zhou C., Xu W.",Don't forget your electricity bills! An empirical study of characterizing energy consumption of 3D printers,2016,"Proceedings of the 7th ACM SIGOPS Asia-Pacific Workshop on Systems, APSys 2016",,,2967377,,,no,,,,10.1145/2967360.2967377,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986626362&doi=10.1145%2f2967360.2967377&partnerID=40&md5=5f6e289737b42cf63c98720bd307f9d6,"University at Buffalo, United States","Ajay, J., University at Buffalo, United States; Rathore, A.S., University at Buffalo, United States; Song, C., University at Buffalo, United States; Zhou, C., University at Buffalo, United States; Xu, W., University at Buffalo, United States","3D printing is an emerging technique in product manufacturing. Its applications have been expanding vastly in homebased production. Compared to traditional manufacturing techniques, such as Computerized Numeric Control (CNC) machine tools, it is believed that 3D printing is more cost-effective in fabricating personalized products. The product cost estimation in 3D printing mainly takes material expenditure into account, and extensive studies have been performed for reducing filament expense or development of recyclable filaments. However, electricity expenditure is another inevitable cost in the 3D printing process yet an omitted factor in the cost estimation. To this end, this paper introduces the first in-depth study to understand the energy consumption in 3D printing. Specifically, our study comprises of two parts. The first part quantifies both material and electricity use in the 3D printing, and find that the electricity takes upto 32% of the total cost. The second part characterizes the energy consumption and identifies the sensitivity of various parameters. We also share insights and potential solutions to optimize the power consumption of 3D printers. Copyright å© 2016 held by owner/author(s).",,Conference Paper,Scopus,2-s2.0-84986626362
"Ahn Y.-J., Sim D.",Square-type-first inter-CU tree search algorithm for acceleration of HEVC encoder,2016,Journal of Real-Time Image Processing,12,2,,419,432,no,,,2,10.1007/s11554-015-0487-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922979514&doi=10.1007%2fs11554-015-0487-5&partnerID=40&md5=d1b4f4898ae01efe1c6df5c75f6f54cd,"Department of Computer Engineering, Kwangwoon University, 20, Gwangun-ro, Nowon-gu, Seoul, South Korea","Ahn, Y.-J., Department of Computer Engineering, Kwangwoon University, 20, Gwangun-ro, Nowon-gu, Seoul, South Korea; Sim, D., Department of Computer Engineering, Kwangwoon University, 20, Gwangun-ro, Nowon-gu, Seoul, South Korea","In this paper, a fast inter-coding algorithm is proposed to reduce the computational load of HEVC encoders. The HEVC reference model (HM) employs the recursive depth-first-search (DFS) of the quad-tree search in terms of rate-distortion optimization in selecting the best coding modes for the best CU, PU, TU partitions, and many associated coding modes. The proposed algorithm evaluates the RD costs of the current CU only for its square-type PUs in the top-down search of the DFS. When the CU partition with the square-type PU is better than its sub-level CU partitions in terms of RD cost in bottom-up search of the DFS, the square type of current CU partition, along with its coding mode, is selected as the best partition. Otherwise, non-square-type PUs for the current CU level are evaluated. If the sub-partition is better than the CU with the non-square PUs, the sub-partition is finally selected as the optimum PU. Otherwise, the best non-square PU is selected as the best PU for the current level. Experimental results demonstrate that the proposed square-type-first inter-PU search can reduce the computational load in average encoding time by 66.7åÊ% with 1‰ÛÒ2åÊ% BD loss over HM reference software. In addition, the proposed algorithm can yield an additional average time saving of 26.8‰ÛÒ35.5åÊ% against the three fast encoding algorithms adopted in HM. å© 2015, Springer-Verlag Berlin Heidelberg.",Fast encoding algorithm; Index HEVC; Inter-CU decision; Square-type mode; Video encoder,Article,Scopus,2-s2.0-84922979514
"Abdelmalek R., Mnasri Z.",High quality Arabic text-to-speech synthesis using unit selection,2016,"13th International Multi-Conference on Systems, Signals and Devices, SSD 2016",,,7473681,,,no,,,,10.1109/SSD.2016.7473681,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974588998&doi=10.1109%2fSSD.2016.7473681&partnerID=40&md5=3426dd8a8a02bd44726bfc914a72428d,"Signal, Image and Technology of Information Laboratory, Ecole Nationale d'IngÌ©nieurs de Tunis-ENIT, University Tunis El-Manar, El-Manar, Tunisia","Abdelmalek, R., Signal, Image and Technology of Information Laboratory, Ecole Nationale d'IngÌ©nieurs de Tunis-ENIT, University Tunis El-Manar, El-Manar, Tunisia; Mnasri, Z., Signal, Image and Technology of Information Laboratory, Ecole Nationale d'IngÌ©nieurs de Tunis-ENIT, University Tunis El-Manar, El-Manar, Tunisia","This work aims to develop a high quality Arabic concatenative speech synthesis system based on unit selection. The original unit selection algorithm was modified to integrate more phonological, linguistic and contextual features in order to improve the selection cost calculation from one side, and more prosodic parameters for more exact concatenation cost estimation from the other side. The subjective assessments based on MOS (Mean Opinion Score) tests and visual comparison of waveforms show satisfactory results. å© 2016 IEEE.",Arabic speech synthesis; Concatenation cost; Selection cost; Unit selection,Conference Paper,Scopus,2-s2.0-84974588998
[No author name available],"2016 Mediterranean Ad Hoc Networking Workshop, Med-Hoc-Net 2016 - 15th IFIP MEDHOCNET 2016",2016,"2016 Mediterranean Ad Hoc Networking Workshop, Med-Hoc-Net 2016 - 15th IFIP MEDHOCNET 2016",,,,,,no,,139,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992029463&partnerID=40&md5=f0a79b6d1677ac8e0482646c4f3abf0d,,,The proceedings contain 20 papers. The topics discussed include: hybrid spectrum access for mmWave networks; QoS based aggregation in high speed IEEE802.11 wireless networks; distributed clustering algorithm in dense group-based ad hoc networks; a dynamic programming algorithm for resolving transmit-ambiguities in the localization of WSN; exploiting state information to support QoS in software-defined WSNs; CNSMO: a network services manager/orchestrator tool for cloud federated environments; participation cost estimation: private versus non-private study; and fog-to-cloud computing (F2C): the key technology enabler for dependable e-health services deployment.,,Conference Review,Scopus,2-s2.0-84992029463
[No author name available],"35th Annual International Conference on Theory and Applications of Cryptographic Techniques, EUROCRYPT 2016",2016,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),9665,,,1,852,no,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979018679&partnerID=40&md5=981749d51e9cd9b5e0ece373d1cf513c,,,"The proceedings contain 31 papers. The special focus in this conference is on Cryptanalysis, Masking, Fully Homomorphic Encryption, Number Theory and Hash Functions. The topics include: Tightly CCA-secure encryption without pairings; indistinguishability obfuscation from constant-degree graded encoding schemes; essentially optimal robust secret sharing with maximal corruptions; provably robust sponge-based PRNGS and KDFS; reusable fuzzy extractors for low-entropy distributions; provably weak instances of ring-LWE revisited; faster algorithms for solving LPN; provable security evaluation of structures against impossible differential and zero correlation linear cryptanalysis; from improved leakage detection to the detection of points of interests in leakage traces; improved masking for tweakable blockciphers with applications to authenticated encryption; towards stream ciphers for efficient FHE with low-noise ciphertexts; improved differential-linear cryptanalysis of 7-round chaskey with partitioning; complete addition formulas for prime order elliptic curves; new attacks on the concatenation and XOR hash combiners; cryptanalysis of the new CLT multilinear map over the integers; on the influence of message length in PMAC‰Ûªs security bounds; safely exporting keys from secure channels; retaining security when randomness fails; honey encryption beyond message recovery security; improved progressive BKZ algorithms and their precise cost estimation by sharp simulator and practical, predictable lattice basis reduction.",,Conference Review,Scopus,2-s2.0-84979018679
[No author name available],"SIMULTECH 2016 - Proceedings of the 6th International Conference on Simulation and Modeling Methodologies, Technologies and Applications",2016,"SIMULTECH 2016 - Proceedings of the 6th International Conference on Simulation and Modeling Methodologies, Technologies and Applications",,,,,,no,,396,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991209591&partnerID=40&md5=5f9f0765d7cdfcaae7a7594518b4ad5e,,,"The proceedings contain 46 papers. The topics discussed include: combining harvesting operation optimisations using strategy-based simulation; simulating spark cluster for deployment planning, evaluation and optimization; an image generator platform to improve cell tracking algorithms - simulation of objects of various morphologies, kinetics and clustering; searching vaccination strategy with surrogate-assisted evolutionary computing; a globally convergent method for generalized resistive systems and its application to stationary problems in gas transport networks; molecular dynamics use in personalized cancer medicine - example of MET Y501C mutation; modelling population growth, shrinkage and aging using a hybrid simulation approach: application to healthcare; agent-based modeling and simulation software architecture for health care; subtask scheduling and predictive-delay control - comparison and hybridization; parameter identification of canalyzing Boolean functions with ternary vectors for gene networks; cache aware instruction accurate simulation of a 3-D coastal ocean model on low power hardware; cooperative radio resources allocation in LTE_a networks within MIH framework: a scheme and simulation analysis; modeling and simulation of pedestrian behaviour - as planning support for building design; an experimental and CFD analysis of a two-phase flow air induction nozzle with agricultural application; and a dynamic scheduling problem in cost estimation process of EPC projects.",,Conference Review,Scopus,2-s2.0-84991209591
[No author name available],"12th International Conference on Economics of Grids, Clouds, Systems and Services, GECON 2015",2016,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),9512,,,1,322,no,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979022684&partnerID=40&md5=60a206cb8cadf7f15947715cbabf0f6d,,,"The proceedings contain 21 papers. The special focus in this conference is on Resource Allocation, Service Selection in Clouds, Energy Conservation, Smart Grids and Community Networks. The topics include: Optimizing multi-tenant cloud resource pools via allocation of reusable time slots; dynamic scheduling in real time with budget constraints in hybrid clouds; cost-aware VM placement across distributed DCS using Bayesian networks; cost-efficient CPU provisioning for scientific workflows on clouds; cost estimation for the provisioning of computing resources to execute bag-of-tasks applications in the Amazon cloud; service quality assurance in multi-clouds; employing relevance feedback to embed content and service importance into the selection process of composite cloud services; business-pertinent cloud service discovery, assessment, and selection; optimizing data centres operation to provide ancillary services on-demand; a specification language for performance and economical analysis of short term data intensive energy management services; utility-based smartphone energy consumption optimization for cloud-based and on-device application uses; optimizing the data center energy consumption using a particle swarm optimization-based approach; towards an energy-aware cloud architecture for smart grids; automatic performance space exploration of web applications; towards incentive-compatible pricing for bandwidth reservation in community network clouds; evolution of the global knowledge network; network analysis of information and communication technologies‰Ûª patents and a revised model of the cloud computing ecosystem.",,Conference Review,Scopus,2-s2.0-84979022684
[No author name available],"International Conference on Practical Applications of Agents and Multi-Agent Systems, PAAMS 2016",2016,Communications in Computer and Information Science,616,,,1,445,no,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977178240&partnerID=40&md5=f72b1bcfd50d1a187bfe8ff3c4650246,,,"The proceedings contain 37 papers. The special focus in this conference is on Agents-Based Solutions for Manufacturing, Supply Chain, MAS for Complex Networks and Social Computation. The topics include: Results of a pilot study with a robot instructor for group exercise at a senior living community; multi agent application for chronic patients; monitoring and detection of remote anomalous situations; improving the distribution of services in MAS; a multi-level and multi-agent approach to modeling and solving supply chain problems; supply chain logistics platform as a supply chain coordination support; a multi-agent framework for cost estimation of product design; an approach to represent material handlers as agents in discrete-event simulation models; overview of case studies on adapting MABS models to GPU programming; holonic multiagent simulation of complex adaptive systems; multiagent social influence detection based on facial emotion recognition; self-regulation of social exchange processes; an immune multi-agent based decision support system for the control of public transportation systems; argumentation-based reasoning with preferences; conjoint measurement theory and score-based bargaining solutions; tracking users mobility at public transportation; energy planning decision-making under uncertainty based on the evidential reasoning approach; orientation system based on speculative computation and trajectory mining; the effect of decision satisfaction prediction in argumentation-based negotiation; experiments with multiple BDI agents with dynamic learning capabilities; measuring heterogeneous user behaviors during the interaction with dialog systems and belief-based argumentation in intelligence analysis and decision making.",,Conference Review,Scopus,2-s2.0-84977178240
[No author name available],"9th International Conference on Advanced Computing and Communication Technologies, ICACCT 2015",2016,Advances in Intelligent Systems and Computing,452,,,1,594,no,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976515935&partnerID=40&md5=390efc926b88b84d529a5084ead7c24d,,,"The proceedings contain 58 papers. The special focus in this conference is on Advanced Computing and Communication Technologies. The topics include: Variation of stacking interactions along with twist parameter in DNA and RNA; branch and bound algorithm for vertex bisection minimization problem; comparative study of 7T, 8T, 9T and 10T SRAM with conventional 6T SRAM cell using 180 nm technology; a survey of hardware and software co-design issues for system on chip design; safety validation of an embedded real-time system at hardware-software integration test environment; an efficient algorithm for tracing minimum leakage current vector in deep-sub micron circuits; age invariant face recognition using minimal geometrical facial features; intelligent prediction of properties of wheat grains using soft computing algorithms; an approach to improve the classification accuracy of leaf images with dorsal and ventral sides by adding directionality features with statistical feature sets; analyzing the effect of chaos functions in solving stochastic dynamic facility layout problem using CSA; ranking of stocks using a hybrid DS-fuzzy system; empirical assessment and optimization of software cost estimation using soft computing techniques; a hybrid algorithm with modified inver-over operator and genetic algorithm search for traveling salesman problem; exploration of feature reduction of MFCC spectral features in speaker recognition; MFAST processing model for occlusion and illumination invariant facial recognition; fuzzy soft set theory and its application in group decision making and discriminating confident and non-confidant speaker based on acoustic speech features.",,Conference Review,Scopus,2-s2.0-84976515935
[No author name available],"Proceedings - 2015 30th IEEE/ACM International Conference on Automated Software Engineering Workshops, ASEW 2015",2015,"Proceedings - 2015 30th IEEE/ACM International Conference on Automated Software Engineering Workshops, ASEW 2015",,,,,,no,,135,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964414238&partnerID=40&md5=91a7360e06e84ee7c3bb1ba0487324b3,,,The proceedings contain 22 papers. The topics discussed include: extending manual GUI testing beyond defects by building mental models of software behavior; software development analytics: experiences and the way forward; a conceptual framework for the comparison of fully automated GUI testing techniques; testing approach for mobile applications through reverse engineering of UI patterns; data mining methods and cost estimation models; empirical analysis on parallel tasks in crowdsourcing software development; analytics for software project management - where are we and where do we go?; a method to evaluate estimates produced by the capture-recapture model; using collective intelligence to support multi-objective decisions: collaborative and online preferences; RepMine: a system for transferrable analyses of collaboration activities in software engineering; an automated contextual collaboration approach for distributed agile delivery; and comparing model coverage and code coverage in model driven testing: an exploratory study.,,Conference Review,Scopus,2-s2.0-84964414238
"Zare F., Khademi Zare H., Fallahnezhad M.S.",Software effort estimation based on the optimal Bayesian belief network,2016,Applied Soft Computing Journal,49,,,968,980,yes,,,,10.1016/j.asoc.2016.08.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994803389&doi=10.1016%2fj.asoc.2016.08.004&partnerID=40&md5=9dd4f75f2a76d9fa4a26c19f29ada560,"Department of Industrial Engineering, Yazd University, Yazd, Iran","Zare, F., Department of Industrial Engineering, Yazd University, Yazd, Iran; Khademi Zare, H., Department of Industrial Engineering, Yazd University, Yazd, Iran; Fallahnezhad, M.S., Department of Industrial Engineering, Yazd University, Yazd, Iran","In this paper, we present a model for software effort (person-month) estimation based on three levels Bayesian network and 15 components of COCOMO and software size. The Bayesian network works with discrete intervals for nodes. However, we consider the intervals of all nodes of network as fuzzy numbers. Also, we obtain the optimal updating coefficient of effort estimation based on the concept of optimal control using Genetic algorithm and Particle swarm optimization for the COCOMO NASA database. In the other words, estimated value of effort is modified by determining the optimal coefficient. Also, we estimate the software effort with considering software quality in terms of the number of defects which is detected and removed in three steps of requirements specification, design and coding. If the number of defects is more than the specified threshold then the model is returned to the current step and an additional effort is added to the estimated effort. The results of model indicate that optimal updating coefficient obtained by genetic algorithm increases the accuracy of estimation significantly. Also, results of comparing the proposed model with the other ones indicate that the accuracy of the model is more than the other models. å© 2016 Elsevier B.V.",Bayesian belief network; Optimal control; Software effort estimation; Software quality,Article,Scopus,2-s2.0-84994803389
"Yucalar F., Kilinc D., Borandag E., Ozcift A.",Regression Analysis Based Software Effort Estimation Method,2016,International Journal of Software Engineering and Knowledge Engineering,26,5,,807,826,yes,,,,10.1142/S0218194016500261,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977489996&doi=10.1142%2fS0218194016500261&partnerID=40&md5=c19f075ce1fd023fdbd4a25b4335ebae,"Department of Software Engineering, Celal Bayar University, Manisa, Turkey","YÌ_calar, F., Department of Software Engineering, Celal Bayar University, Manisa, Turkey; Kilinc, D., Department of Software Engineering, Celal Bayar University, Manisa, Turkey; Borandag, E., Department of Software Engineering, Celal Bayar University, Manisa, Turkey; Ozcift, A., Department of Software Engineering, Celal Bayar University, Manisa, Turkey","Estimating the development effort of a software project in the early stages of the software life cycle is a significant task. Accurate estimates help project managers to overcome the problems regarding budget and time overruns. This paper proposes a new multiple linear regression analysis based effort estimation method, which has brought a different perspective to the software effort estimation methods and increased the success of software effort estimation processes. The proposed method is compared with standard Use Case Point (UCP) method, which is a well-known method in this area, and simple linear regression based effort estimation method developed by Nassif et al. In order to evaluate and compare the proposed method, the data of 10 software projects developed by four well-established software companies in Turkey were collected and datasets were created. When effort estimations obtained from datasets and actual efforts spent to complete the projects are compared with each other, it has been observed that the proposed method has higher effort estimation accuracy compared to the other methods. å© 2016 World Scientific Publishing Company.",Software effort estimation; software size estimation; use-case point method,Article,Scopus,2-s2.0-84977489996
"Wani Z.H., Quadri S.M.K.",Artificial bee colony-trained functional link artificial neural network model for software cost estimation,2016,Advances in Intelligent Systems and Computing,437,,,729,741,yes,,,,10.1007/978-981-10-0451-3_65,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964844151&doi=10.1007%2f978-981-10-0451-3_65&partnerID=40&md5=945260e165b2f0a1ed7f4ec9298c2bd5,"Department of Computer Sciences, University of Kashmir, Srinagar, India","Wani, Z.H., Department of Computer Sciences, University of Kashmir, Srinagar, India; Quadri, S.M.K., Department of Computer Sciences, University of Kashmir, Srinagar, India","Software cost estimation is forecasting the amount of developmental effort and time needed, while developing any software system. A good volume of software cost prediction models ranging from the very old algorithmic models to expert judgement to non-algorithmic models have been proposed so far. Each of these models has their advantage and disadvantage in estimating the development effort. Recently, the usage of meta-heuristic techniques for software cost estimations is increasingly growing. So in this paper, we are proposing an approach, which consists of functional link ANN and artificial bee colony algorithm as its training algorithm for delivering most accurate software cost estimation. FLANN reduces the computational complexity in multilayer neural network, and does not has any hidden layer, and thus has got fast learning ability. In our model, we are using MRE, MMRE and MdMRE as a measure of performance index to simply weigh the obtained quality of estimation. After an extensive evaluation of results, it showed that training a FLANN with ABC for the problem of software cost prediction yields a highly improved set of results. Besides this, the proposed model involves less computation during its training because of zero hidden layers and thus is structurally simple. å© Springer Science+Business Media Singapore 2016.",Artificial bee colony (ABC); Artificial neural network (ANN); Functional link artificial neural network (FLANN); Software cost estimation (SCE),Conference Paper,Scopus,2-s2.0-84964844151
"Velarde H., Santiesteban C., Garcia A., Casillas J.",Software Development Effort Estimation based-on multiple classifier system and Lines of Code,2016,IEEE Latin America Transactions,14,8,7786379,3907,3913,yes,,,,10.1109/TLA.2016.7786379,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007495794&doi=10.1109%2fTLA.2016.7786379&partnerID=40&md5=a8cb1ec2d8fbe611c55a0f4bba808f0a,"Facultad de Ciencias e IngenierÌ_as. FÌ_sicas y Formales, Universidad CatÌ_lica de Santa MarÌ_a, Arequipa, Peru; Centro de Bioplantas, Universidad de Ciego de Ìvila, Cuba; DivisiÌ_n Territorial Villa Clara, DESOFT, Cuba; Departamento de Ciencias de la ComputaciÌ_n e Inteligencia Artificial, Universidad de Granada, Spain","Velarde, H., Facultad de Ciencias e IngenierÌ_as. FÌ_sicas y Formales, Universidad CatÌ_lica de Santa MarÌ_a, Arequipa, Peru; Santiesteban, C., Centro de Bioplantas, Universidad de Ciego de Ìvila, Cuba; Garcia, A., DivisiÌ_n Territorial Villa Clara, DESOFT, Cuba; Casillas, J., Departamento de Ciencias de la ComputaciÌ_n e Inteligencia Artificial, Universidad de Granada, Spain","The development effort estimation is one of the most difficult problems in software project management. It is one of the most critical aspects in the early stages of the software project. Several software development effort estimation models have been proposed, however, these models are not able to obtain more than a 25 percent of accuracy, neither provide an understandable model for experts in the application area. Therefore, in this paper, we present EEpred, an explanatory model to estimate the development effort based on data of known software projects. It is a serial multiple classifier system based-on several decision trees. The model performance was evaluated by an internal validation procedure, analyzing their robustness and predictive performance. This procedure demonstrates that EEpred is able to estimate the software development effort with a 71 percent of precision. The main advantage of EEpred, regarding to other algorithms, is its ability to translate the process into a collection of simple decision rules, providing more easily interpretable knowledge that can help software engineer to improve decision-making on development planning. å© 2003-2012 IEEE.",Decision Trees; Effort estimation; Explanatory models; Multiple Classifiers; Regression Trees; Software metrics; Software projects,Article,Scopus,2-s2.0-85007495794
"Velarde H., Santiesteban C., Garcia A., Casillas J.",Analyzing the effect of variables in the software development effort estimation,2016,IEEE Latin America Transactions,14,8,7786366,3797,3803,yes,,,,10.1109/TLA.2016.7786366,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007449376&doi=10.1109%2fTLA.2016.7786366&partnerID=40&md5=7161663901bbe4659c1b27c5f86bbae7,"Facultad de Ciencias e IngenierÌ_as. FÌ_sicas y Formales, Universidad CatÌ_lica de Santa MarÌ_a, Arequipa, Peru; Centro de Bioplantas, Universidad de Ciego de Ìvila, Cuba; DivisiÌ_n Territorial Villa Clara, DESOFT, Cuba; Departamento de Ciencias de la ComputaciÌ_n e Inteligencia Artificial, Universidad de Granada, Spain","Velarde, H., Facultad de Ciencias e IngenierÌ_as. FÌ_sicas y Formales, Universidad CatÌ_lica de Santa MarÌ_a, Arequipa, Peru; Santiesteban, C., Centro de Bioplantas, Universidad de Ciego de Ìvila, Cuba; Garcia, A., DivisiÌ_n Territorial Villa Clara, DESOFT, Cuba; Casillas, J., Departamento de Ciencias de la ComputaciÌ_n e Inteligencia Artificial, Universidad de Granada, Spain","Due to the variability of variables, the estimation of effort in software development is considered as a very difficult task. Maybe due to small databases, uncertain and very subjective information. For these reason, in this paper, we introduce a study of the effect of variables in the software development effort estimation process. The estimation employing lines of codes, function points and engineering task, were taken as reference. With the use of datamining and machine learning technics, were demonstrated that the effort estimation using engineering task is more accurate. And, by the other hand, was established the influence of each variable, in the software development effort estimation process. å© 2003-2012 IEEE.",Datamining; Decision Trees; Effort estimation; Regression Trees; Software metrics; Software projects; Variables selection,Article,Scopus,2-s2.0-85007449376
"Urbanek T., Prokopova Z., Silhavy R., Kuncar A.",Using analytical programming for software effort estimation,2016,Advances in Intelligent Systems and Computing,465,,,261,272,yes,,,,10.1007/978-3-319-33622-0_24,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964788339&doi=10.1007%2f978-3-319-33622-0_24&partnerID=40&md5=c6bd7ce267c67f1c41ed69d433ea474a,"Faculty of Applied Informatics, Tomas Bata University in Zlin, Nad Stranemi, ZlÌ_n, Czech Republic","Urbanek, T., Faculty of Applied Informatics, Tomas Bata University in Zlin, Nad Stranemi, ZlÌ_n, Czech Republic; Prokopova, Z., Faculty of Applied Informatics, Tomas Bata University in Zlin, Nad Stranemi, ZlÌ_n, Czech Republic; Silhavy, R., Faculty of Applied Informatics, Tomas Bata University in Zlin, Nad Stranemi, ZlÌ_n, Czech Republic; Kuncar, A., Faculty of Applied Informatics, Tomas Bata University in Zlin, Nad Stranemi, ZlÌ_n, Czech Republic","This paper evaluates the usage of analytical programming for software effort estimation. Analytical programming and differential evolution generate regression models. The new model was generated by analytical programming and it was tested and compared with Karner‰Ûªs model to assess insight to its properties. Mean Magnitude of Relative Error and k-fold cross validation were used to assess the reliability to this experiment. The experimental results shows that the new model generated by analytical programming outperforms the Karner‰Ûªs equation about 12% MMRE. Moreover, this work shows that analytical programming method is viable method for calibrating Use Case Points method. All results were evaluated by standard approach: visual inspection and statistical significance testing. å© Springer International Publishing Switzerland 2016.",Analytical programming; Differential evolution; Effort estimation; Use case points,Conference Paper,Scopus,2-s2.0-84964788339
"Sree S.R., Ramesh S.N.S.V.S.C.",Analytical structure of a fuzzy logic controller for software development effort estimation,2016,Advances in Intelligent Systems and Computing,410,,,209,216,yes,,,,10.1007/978-81-322-2734-2_22,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952032054&doi=10.1007%2f978-81-322-2734-2_22&partnerID=40&md5=9bba3982f2f79e35bcab6d85dd3cb02d,"Department of CSE, Aditya Engineering College, JNTUK, Kakinada, AP, India; Department of CSE, Sri Sai Aditya Institute of Science and Technology, JNTUK, Kakinada, AP, India","Sree, S.R., Department of CSE, Aditya Engineering College, JNTUK, Kakinada, AP, India; Ramesh, S.N.S.V.S.C., Department of CSE, Sri Sai Aditya Institute of Science and Technology, JNTUK, Kakinada, AP, India","Most recently, attention has turned towards Machine learning techniques to predict software development cost as they are more apt when vague and inaccurate information is to be used. Based on the existing evidences, it is proved that a few of the problems associated with previous models are addressed by soft computing techniques. But, the need for accurate cost prediction in software project management is a challenge till today. In this paper, the analytical structure of a Takagi-Sugeno Fuzzy Logic Controller with two inputs and one output for software development effort estimation with a case study on NASA 93 dataset is discussed. The analytical study is also presented with two sample inputs. The Fuzzy models are developed using triangular and GBell membership functions. The results are compared using various assessment criteria. It has been observed that the fuzzy model with triangular membership function performed better than the other models. å© Springer India 2016.",Analytical study; Criteria for assessment; Effort estimation; Fuzzy logic controller; Fuzzy rules,Conference Paper,Scopus,2-s2.0-84952032054
"Sree P.R., Ramesh S.N.S.V.S.C.",Improving Efficiency of Fuzzy Models for Effort Estimation by Cascading & Clustering Techniques,2016,Procedia Computer Science,85,,,278,285,yes,,,,10.1016/j.procs.2016.05.234,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978634495&doi=10.1016%2fj.procs.2016.05.234&partnerID=40&md5=de3b6356c17b316a5aee194341b0c58d,"Department of CSE, Aditya Engineering College, JNTUK, Kakinada, India; Department of CSE, Sri Sai Aditya Institute of Science and Technology, JNTUK, Kakinada, India","Sree, P.R., Department of CSE, Aditya Engineering College, JNTUK, Kakinada, India; Ramesh, S.N.S.V.S.C., Department of CSE, Sri Sai Aditya Institute of Science and Technology, JNTUK, Kakinada, India","The main challenge in software industry is the process of estimating the cost required for the development or maintenance of a project. Various models have been proposed for constructing a relationship between size of software and its development effort. Various algorithmic cost estimation models exists with their own pros and cons for estimation. Now a days, attention has turned towards Machine learning techniques as few of the problems associated with previous models are being addressed by the soft computing techniques. But, the need for accurate effort estimation in software project management is still a challenge. The literature shows the usage of Fuzzy Logic Controller for Software Effort Estimation, but the computational time is very high as the rulebase is large. The main aim is to reduce the rulebase and improve the efficiency by cascading of Fuzzy Logic Controllers. A case study on NASA 93 dataset is taken for this purpose. The Proposed work is carried out by cascading the Fuzzy Logic Controllers in a stage of two and six for Software Effort Estimation. By increasing the cascading of Fuzzy Logic Controllers, the efficiency has been improved with the minimized size in rulebase. The limitation to this process is to find out the correct number of cascaded Fuzzy Logic Controllers. To overcome this a Fuzzy Model using Subtractive Clustering has been proposed. The rulebase of the models developed by using Subtractive Clustering is further reduced. Considering the rule minimization and the various criteria for assessment, Fuzzy Models developed using Subtractive Clustering provides better software development effort estimates. å© 2016 Published by Elsevier B.V.",Cascading Fuzzy Logic Controllers; Effort Estimation; Fuzzy Logic Controller; NASA 93 dataset; Rulebase; Subtractive Clustering,Conference Paper,Scopus,2-s2.0-84978634495
"Soltanveis F., Alizadeh S.H.",Using parametric regression and KNN algorithm with missing handling for software effort prediction,2016,"2016 Artificial Intelligence and Robotics, IRANOPEN 2016",,,7529494,77,84,yes,,,,10.1109/RIOS.2016.7529494,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992190516&doi=10.1109%2fRIOS.2016.7529494&partnerID=40&md5=c70700d74ad617a902db54ab181f709b,"Faculty of Electronic, Computer and IT, Islamic Azad University, Qazvin Branch, Qazvin, Iran","Soltanveis, F., Faculty of Electronic, Computer and IT, Islamic Azad University, Qazvin Branch, Qazvin, Iran; Alizadeh, S.H., Faculty of Electronic, Computer and IT, Islamic Azad University, Qazvin Branch, Qazvin, Iran","Estimating the software development costs, budget and resources such as the time and effort is one of the most important activities in the software project management. The error rate, at the estimating costs, has a sizable portion in success or fail of a project. In general, it is used from similar project histories for project estimation. One of the challenges in this approach is missing values. in this research, first, for handling missing values the K nearest neighbor (KNN) algorithm and Mean Imputation has been used, then for effort prediction, the parametric model based methods, the nonlinear and polynomial regression(quadratic) is used. The proposed method is performed on the CM1 dataset and the results show that the combination of KNN and nonlinear regression (quadratic) has the best response, signifying accuracy improvement and relative error reduction, in comparing with other approaches. å© 2016 IEEE.",effort estimation; missing value; regression,Conference Paper,Scopus,2-s2.0-84992190516
"Silhavy R., Prokopova Z., Silhavy P.",Algorithmic optimization method for effort estimation,2016,Programming and Computer Software,42,3,,161,166,yes,,,,10.1134/S0361768816030087,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971223685&doi=10.1134%2fS0361768816030087&partnerID=40&md5=19e8356d7e1238f3e97bce9d6178b360,"Faculty of Applied Informatics, Tomas Bata University, T.G. Masaryka 5555, Zlin, Czech Republic","Silhavy, R., Faculty of Applied Informatics, Tomas Bata University, T.G. Masaryka 5555, Zlin, Czech Republic; Prokopova, Z., Faculty of Applied Informatics, Tomas Bata University, T.G. Masaryka 5555, Zlin, Czech Republic; Silhavy, P., Faculty of Applied Informatics, Tomas Bata University, T.G. Masaryka 5555, Zlin, Czech Republic","This paper presents an improvement of an effort estimation method that can be used to predict the level of effort for software development projects. A new estimation approach based on a two-phase algorithm is used. In the first phase, we apply a calculation based on use case points (UCPs). In the second phase, we add correction values (a1, a2) obtained via least squares regression. This approach employs historical project data to refine the estimate. By applying the least squares regression approach, the algorithm filters out estimation errors caused by human factors and company practice. å© 2016, Pleiades Publishing, Ltd.",,Article,Scopus,2-s2.0-84971223685
"Sigweni B., Shepperd M., Turchi T.",Realistic assessment of software effort estimation models,2016,ACM International Conference Proceeding Series,01-03-June-2016,, a41,,,yes,,,,10.1145/2915970.2916005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978535952&doi=10.1145%2f2915970.2916005&partnerID=40&md5=b0692d02a66de707e06979ec8adea12d,"Department of Computer Science, Brunel University London, United Kingdom","Sigweni, B., Department of Computer Science, Brunel University London, United Kingdom; Shepperd, M., Department of Computer Science, Brunel University London, United Kingdom; Turchi, T., Department of Computer Science, Brunel University London, United Kingdom","Context: It is unclear that current approaches to evaluating or comparing competing software cost or effort models give a realistic picture of how they would perform in actual use. Specifically, we're concerned that the usual practice of using all data with some holdout strategy is at variance with the reality of a data set growing as projects complete. Objective: This study investigates the impact of using unrealistic, though possibly convenient to the researchers, ways to compare models on commercial data sets. Our questions are does this lead to different conclusions in terms of the comparisons and if so,are the results biased e.g., more optimistic than those that might realistically be achieved in practice. Method: We compare a traditional approach based on leave one out cross-validation with growing the data set chronologically using the Finnish and Desharnais data sets. Results: Our realistic, time-based approach to validation is significantly more conservative than leave-one-out cross-validation (LOOCV) for both data sets. Conclusion: If we want our research to lead to actionable findings it's incumbent upon the researchers to evaluate their models in realistic ways. This means a departure from LOOCV techniques, while further investigation is needed for other validation techniques, such as k-fold validation. å© 2016 ACM.",Cross-validation approaches; Software effort estimation; Software engineering experimentation,Conference Paper,Scopus,2-s2.0-84978535952
"Sholiq, Widodo A.P., Sutanto T., Subriadi A.P.",A model to determine cost estimation for software development projects of small and medium scales using use case points,2016,Journal of Theoretical and Applied Information Technology,85,1,,87,94,yes,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960395675&partnerID=40&md5=b448e0f052615fec9671daeb45d9393e,"Department of Information Systems, Sepuluh Nopember Institute of Technology (ITS), Indonesia; Department of Information System, STMIK STIKOM, Surabaya, Indonesia","Sholiq, Department of Information Systems, Sepuluh Nopember Institute of Technology (ITS), Indonesia; Widodo, A.P., Department of Information Systems, Sepuluh Nopember Institute of Technology (ITS), Indonesia, Department of Information System, STMIK STIKOM, Surabaya, Indonesia; Sutanto, T., Department of Information Systems, Sepuluh Nopember Institute of Technology (ITS), Indonesia, Department of Information System, STMIK STIKOM, Surabaya, Indonesia; Subriadi, A.P., Department of Information Systems, Sepuluh Nopember Institute of Technology (ITS), Indonesia","The purpose of this study was to develop a cost estimation model for software development projects of small and medium scale. The model was derived from the Use Case Points, which is usually used to estimate software development effort. The development of our cost estimation model was based on the need for a reference to the estimated costs for software development projects, particularly software development projects of small‰ÛÒmedium scale. The cost estimation was used to estimate the allocation of resources spent on covering personnel resources, money, and time to complete the project. The result of this study was a cost estimation model for software development projects of small‰ÛÒmedium scale that had been tested with four software projects that had been completed. Testing of the model was performed using data from four projects got level deviations between the estimated cost and the actual cost amounted to 6.89%. å© 2005-2016 JATIT & LLS. All rights reserved.",Cost estimation model; Effort estimation; Software development project; Use case points,Article,Scopus,2-s2.0-84960395675
"Sharma M., Sharma V., Jain P.",Software development time estimation using neuro- fuzzy approach: A review,2016,International Journal of Control Theory and Applications,9,18,,8919,8925,yes,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006355826&partnerID=40&md5=70dc7cff7b12eb4c14ccab4722b30345,"Department of Computer Science and Engineering, Krishna Institute of Engineering and Technology, Ghaziabad, U.P., India","Sharma, M., Department of Computer Science and Engineering, Krishna Institute of Engineering and Technology, Ghaziabad, U.P., India; Sharma, V., Department of Computer Science and Engineering, Krishna Institute of Engineering and Technology, Ghaziabad, U.P., India; Jain, P., Department of Computer Science and Engineering, Krishna Institute of Engineering and Technology, Ghaziabad, U.P., India","If we have to make a project efficiently and effectively, it is essential for any organization that the project should be completed under the budget, on time and the project should have mandatory excellence. Software processes should be managed efficiently and that requires a measurement of quantification and modelling. The pitiable causes for any software failure may be related to the inefficient estimation of resources and the resources may be like effort, cost, development time etc. So it should be managed effectively and efficiently. In this paper, we deals with a hybrid approach by combining neural networks and fuzzy i,e The Adaptive Neuro-Fuzzy approach for the development time estimation of a software . The proposed approach is based on the estimation of development time of a software and the results will be compared with the existing system. å© 2016 International Science Press.",ANFIS; Cost and effort estimation; Development time estimation; Software estimation,Review,Scopus,2-s2.0-85006355826
"Satapathy S.M., Kumar M., Rath S.K.",Optimised class point approach for software effort estimation using adaptive neuro-fuzzy inference system model,2016,International Journal of Computer Applications in Technology,54,4,,323,333,yes,,,1,10.1504/IJCAT.2016.080488,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85000770450&doi=10.1504%2fIJCAT.2016.080488&partnerID=40&md5=297638b6601144ff848ac484d00c63a8,"Department of Computer Science and Engineering, National Institute of Technology Rourkela, Rourkela, India","Satapathy, S.M., Department of Computer Science and Engineering, National Institute of Technology Rourkela, Rourkela, India; Kumar, M., Department of Computer Science and Engineering, National Institute of Technology Rourkela, Rourkela, India; Rath, S.K., Department of Computer Science and Engineering, National Institute of Technology Rourkela, Rourkela, India","The success of software development depends very much on proper estimation of effort required to develop the software. There is no simple way to make an accurate estimate of these parameters required to develop a software system. There are basically some points approach which are available for software effort estimation such as function point, use case point, class point, object point, etc. In this paper, our aim is to estimate the cost of various software projects using class point approach. The parameters are optimised using various soft computing techniques such as fuzzy logic and adaptive neuro-fuzzy logic so as to achieve better accuracy. Also, a comparative analysis of software effort estimation using the techniques such as artificial neural network (ANN), fuzzy logic (FL) and adaptive neuro-fuzzy inference system (ANFIS) has been provided. å© Copyright 2016 Inderscience Enterprises Ltd.",Adaptive neuro-fuzzy inference system; ANFIS; ANN; Artificial neural network; Class point; Function point; Fuzzy logic; Object-oriented system; Software effort estimation,Article,Scopus,2-s2.0-85000770450
"Satapathy S.M., Acharya B.P., Rath S.K.",Early stage software effort estimation using random forest technique based on use case points,2016,IET Software,10,1,,10,17,yes,,,2,10.1049/iet-sen.2014.0122,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958179169&doi=10.1049%2fiet-sen.2014.0122&partnerID=40&md5=89ec15def096f19a2d1ab5ad70b3a3ca,"Department of Computer Science and Engineering, National Institute of Technology Rourkela, Rourkela, Odisha, India","Satapathy, S.M., Department of Computer Science and Engineering, National Institute of Technology Rourkela, Rourkela, Odisha, India; Acharya, B.P., Department of Computer Science and Engineering, National Institute of Technology Rourkela, Rourkela, Odisha, India; Rath, S.K., Department of Computer Science and Engineering, National Institute of Technology Rourkela, Rourkela, Odisha, India","Due to the increasing complexity of software development activities, the need for effective effort estimation techniques has arisen. Underestimation leads to disruption in the project's estimated cost and delivery. On the other hand, overestimation causes outbidding and financial losses in business. Effective software effort estimation techniques enable project managers to schedule the software life cycle activities appropriately. Correctly assessing the effort needed to develop a software product is a major concern in software industries. Random forest (RF) technique is a popularly used machine learning technique that helps in improving the prediction values. The main objective of this study is to precisely assess the software projects development effort by utilising the use case point approach. The effort parameters are optimised utilising the RF technique to acquire higher prediction accuracy. Moreover, the results acquired applying the RF technique is compared with the multi-layer perceptron, radial basis function network, stochastic gradient boosting and log-linear regression techniques to highlight the performance attained by each technique. å© 2016 The Institution of Engineering and Technology.",,Article,Scopus,2-s2.0-84958179169
"Sarro F., Petrozziello A., Harman M.",Multi-objective software effort estimation,2016,Proceedings - International Conference on Software Engineering,14-22-May-2016,,,619,630,yes,,,3,10.1145/2884781.2884830,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971435118&doi=10.1145%2f2884781.2884830&partnerID=40&md5=485453ddecdacdba35f819b10ed637b3,"University College London, London, United Kingdom; University of Portsmouth, Portsmouth, United Kingdom","Sarro, F., University College London, London, United Kingdom; Petrozziello, A., University of Portsmouth, Portsmouth, United Kingdom; Harman, M., University College London, London, United Kingdom","We introduce a bi-objective effort estimation algorithm that combines Confidence Interval Analysis and assessment of Mean Absolute Error. We evaluate our proposed algorithm on three different alternative formulations, baseline comparators and current state-of-the-art effort estimators applied to five real-world datasets from the PROMISE repository, involving 724 different software projects in total. The results reveal that our algorithm outperforms the baseline, state-of-the-art and all three alternative formulations, statistically significantly (p &lt; 0:001) and with large effect size (A12‰ä´ 0:9) over all five datasets. We also provide evidence that our algorithm creates a new state-of-the-art, which lies within currently claimed industrial human-expert-based thresholds, thereby demonstrating that our findings have actionable conclusions for practicing software engineers. å© 2016 ACM.",Confidence interval; Estimates uncertainty; Multi-objective evolutionary algorithm; Software effort estimation,Conference Paper,Scopus,2-s2.0-84971435118
"Sarno R., Sidabutar J., Sarwosri",Improving the accuracy of COCOMO's effort estimation based on neural networks and fuzzy logic model,2015,"Proceedings of 2015 International Conference on Information and Communication Technology and Systems, ICTS 2015",,,7379898,197,202,yes,,,,10.1109/ICTS.2015.7379898,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964956263&doi=10.1109%2fICTS.2015.7379898&partnerID=40&md5=285da64b5195a7713bd803b8b655ffca,"Department of Informatics Engineering, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia","Sarno, R., Department of Informatics Engineering, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia; Sidabutar, J., Department of Informatics Engineering, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia; Sarwosri, Department of Informatics Engineering, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia","Constructive Cost Model II (COCOMO II) is investigated as the most popular model for software cost estimation. COCOMO II depends on several variables or Cost Drivers (CD). This research investigates the role of Effort Multiplier (EM) and Line of Code (LOC) to improve the accuracy of cost estimation. Fuzzy Logic has been implemented to the COCOMO II to represent the EM. Furthermore, in order to produce better estimation, this research uses Gaussian Membership Function to redesign the Effort Multiplier by studying the behavior of COCOMO II. This research also applies Neural Network (NN) approach to increase the accuracy of software effort estimation by training the software development datasets. The result is proposed model gives contribution to decrease error significantly. å© 2015 IEEE.",COCOMO; cost driver; fuzzy; Gaussian membership; neural networks; software cost estimation,Conference Paper,Scopus,2-s2.0-84964956263
"Sarno R., Sidabutar J., Sarwosri",Comparison of different Neural Network architectures for software cost estimation,2015,"Proceeding - 2015 International Conference on Computer, Control, Informatics and Its Applications: Emerging Trends in the Era of Internet of Things, IC3INA 2015",,,7377748,68,73,yes,,,,10.1109/IC3INA.2015.7377748,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963677738&doi=10.1109%2fIC3INA.2015.7377748&partnerID=40&md5=ef680229b5b8d0064ebc5cd5afce7785,"Department of Informatics Engineering, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia","Sarno, R., Department of Informatics Engineering, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia; Sidabutar, J., Department of Informatics Engineering, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia; Sarwosri, Department of Informatics Engineering, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia","This research will observe the use of Artificial Neural Networks (ANN) for estimating software cost. Constructive Cost Model (COCOMO) is the most famous estimating model for software cost, which will be used in this research. The model estimates software cost by calculating several variables which are created by expert with some equations. Furthermore, ANN helps to estimate COCOMO effort accurately. This research offers multilayer feed-forward neural network to adjust COCOMO effort estimation parameters. Also, an algorithm such as Back-propagation is applied to improve the architecture by comparing actual effort with estimated effort and updating the network. However, there are several types of neural network architecture. This research tries to compare several types of architecture by testing each architecture model to dataset. This paper concerns with two different architectures. The difference of this two architecture is basic architecture only uses effort multipliers as input layer while modified architecture divides input layer into two categories such as effort multipliers and scale factors. The result is the proposed model increases the accuracy and each model has different result. å© 2015 IEEE.",artificial neural network; back propagation; COCOMO; cost driver; effort estimation; feed forward; neural networks; software cost estimation,Conference Paper,Scopus,2-s2.0-84963677738
"Samareh Moosavi S.H., Khatibi Bardsiri V.",Satin bowerbird optimizer: A new optimization algorithm to optimize ANFIS for software development effort estimation,2017,Engineering Applications of Artificial Intelligence,60,,,1,15,yes,,,,10.1016/j.engappai.2017.01.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009902128&doi=10.1016%2fj.engappai.2017.01.006&partnerID=40&md5=56daa2d00cb38c98853ebba04374dad9,"Department of Computer Engineering, Kerman Branch, Islamic Azad University, Kerman, Iran","Samareh Moosavi, S.H., Department of Computer Engineering, Kerman Branch, Islamic Azad University, Kerman, Iran; Khatibi Bardsiri, V., Department of Computer Engineering, Kerman Branch, Islamic Azad University, Kerman, Iran","Accurate software development effort estimation is crucial to efficient planning of software projects. Due to complex nature of software projects, development effort estimation has become a challenging issue which must be seriously considered at the early stages of project. Insufficient information and uncertain requirements are the main reasons behind unreliable estimations in this area. Although numerous effort estimation models have been proposed during the last decade, accuracy level is not satisfying enough. This paper presents a new model based on a combination of adaptive neuro-fuzzy inference system (ANFIS) and satin bower bird optimization algorithm (SBO) to reach more accurate software development effort estimations. SBO is a novel optimization algorithm proposed to adjust the components of ANFIS through applying small and reasonable changes in variables. The proposed hybrid model is an optimized neuro-fuzzy based estimation model which is capable of producing accurate estimations in a wide range of software projects. The proposed optimization algorithm is compared against other bio inspired optimization algorithms using 13 standard test functions including unimodal and multimodal functions. Moreover, the proposed hybrid model is evaluated using three real data sets. Results show that the proposed model can significantly improve the performance metrics. å© 2017 Elsevier Ltd",Adaptive Neuro-fuzzy inference system; Development effort estimation; Satin bowerbird optimization algorithm; Software project,Article,Scopus,2-s2.0-85009902128
"Sabrjoo S., Khalili M., Nazari M.",Comparison of the accuracy of effort estimation methods,2015,"Conference Proceedings of 2015 2nd International Conference on Knowledge-Based Engineering and Innovation, KBEI 2015",,,7436134,724,728,yes,,,,10.1109/KBEI.2015.7436134,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971422855&doi=10.1109%2fKBEI.2015.7436134&partnerID=40&md5=7f2bedcbe86dbfc43f062cac9c1bcde1,"Dept. of Computer and Informatics Engineering, Payam-e-Noor University, Tehran, Iran","Sabrjoo, S., Dept. of Computer and Informatics Engineering, Payam-e-Noor University, Tehran, Iran; Khalili, M., Dept. of Computer and Informatics Engineering, Payam-e-Noor University, Tehran, Iran; Nazari, M., Dept. of Computer and Informatics Engineering, Payam-e-Noor University, Tehran, Iran","The COCOMO Model is one of the best software cost estimation models. This model is a kind of experimental model which has been made up through gathering data on different software projects, and mainly takes cost and schedule of the project under consideration. But hypotheses presented in early phases will be changing during time, and variable estimations might increase or decrease pre-suggested budget of the project. In this paper, COCOMO Model has been studied as a suggested method of cost and effort estimation. At first, different software effort estimation methods are described and finally, according to accomplished evaluations findings, the optimal COCOMO model is presented. å© 2015 IEEE.",COCOMO Model; Eybrid Technique Cost Estimation Model; Software Effort Estimation,Conference Paper,Scopus,2-s2.0-84971422855
"Rijwani P., Jain S.",Enhanced Software Effort Estimation Using Multi Layered Feed Forward Artificial Neural Network Technique,2016,Procedia Computer Science,89,,,307,312,yes,,,1,10.1016/j.procs.2016.06.073,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986550115&doi=10.1016%2fj.procs.2016.06.073&partnerID=40&md5=9415a97579817ac053702a5f99be01ff,"Institute of Engineering and Technology, JK Lakshmipat University, Jaipur, India","Rijwani, P., Institute of Engineering and Technology, JK Lakshmipat University, Jaipur, India; Jain, S., Institute of Engineering and Technology, JK Lakshmipat University, Jaipur, India",Software Effort Estimation models are hot topic of study over 3 decades. Several models have been developed in these decades. Providing accurate estimations of software is still very challenging. The major reason for such disappointments in projects are because of inaccurate software development norms; effort estimation is one such practice. Dynamically fluctuating environment of technology in software development industry make effort estimation further perplexing. One of the most commonly used algorithmic model for estimating effort in industry is COCOMO. Capability of machine learning particularly Artificial Neural Networks is to adjust a complex set of bond among the various independent and dependent variables. The paper proposes usage of ANN (Artificial Neural Network) based model technologically advanced using Multi Layered Feed Forward Neural Network which is given training with Back Propagation training method. COCOMO data-set is accustomed to test and train the network. Mean-Square-Error (MSE) and Mean Magnitude of Relative-Error (MMRE) are used as performance measurement indices. The experiment outputs suggest that the suggested model can provide better results and accurately forecast the software development effort. å© 2016 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license.,Artificial Neural Networks; Back Propagation; COCOMO; Effort Estimation,Conference Paper,Scopus,2-s2.0-84986550115
"Rasool R., Malik A.A.",Effort estimation of ETL projects using Forward Stepwise Regression,2015,"Proceedings of 2015 International Conference on Emerging Technologies, ICET 2015",,,7389209,,,yes,,,,10.1109/ICET.2015.7389209,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963547188&doi=10.1109%2fICET.2015.7389209&partnerID=40&md5=cb912de0121ccb683e73b6aaa13c890b,"Department of Computer Science, National University of Computer and Emerging Sciences, Lahore, Pakistan","Rasool, R., Department of Computer Science, National University of Computer and Emerging Sciences, Lahore, Pakistan; Malik, A.A., Department of Computer Science, National University of Computer and Emerging Sciences, Lahore, Pakistan","Effort estimation is a key component of planning a software development project. In the past, there has been a lot of research on estimation methods for traditional applications but, unfortunately, these methods do not apply to Extract Transform Load (ETL) projects. Coming up with a systematic effort estimate for ETL projects is a challenging task since ETL development does not follow the traditional Software Development Life Cycle (SDLC). Traditional application development is requirements-driven whereas ETL application development is data-driven. This research paper describes the development of an effort estimation model for ETL projects and compares this model with the most widely used algorithmic effort estimation model i.e. COCOMO II. A dataset comprising 220 industrial projects from five different software houses is used to build this effort estimation model using Forward Stepwise Regression. After eliminating 20 outliers from this dataset, the adjusted R2 (i.e. goodness of fit) of our model is 0.87. The prediction and training accuracy of this model is measured using the de-facto standard accuracy measures such as MMRE and PRED(25). On a training dataset of 200 projects, the training accuracy value of PRED(25) is 81.16% and MMRE is 0.16. Results show that our proposed estimation model provides considerably better estimation accuracy as compared to COCOMO II. On a validation dataset of 58 projects, the value of PRED(25) was 49% for our model as compared to 21% for COCOMO II. Furthermore, the MMRE of our model is 0.31 as compared to 0.99 for COCOMO II. å© 2015 IEEE.",COCOMO II; Data Warehousing; Effort Estimation; Estimation Accuracy; ETL; Forward Stepwise Regression; Project Management; Software Cost Estimation,Conference Paper,Scopus,2-s2.0-84963547188
"Ramacharan S., Venu Gopala Rao K.",Software effort estimation of GSD Projects Using Calibrated parametric estimation models,2016,ACM International Conference Proceeding Series,04-05-March-2016,, a117,,,yes,,,,10.1145/2905055.2905177,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988569512&doi=10.1145%2f2905055.2905177&partnerID=40&md5=d92784f149799967190374fd070e0890,"G. Narayanamma Institute of Technology and Science, Hyderabad, India","Ramacharan, S., G. Narayanamma Institute of Technology and Science, Hyderabad, India; Venu Gopala Rao, K., G. Narayanamma Institute of Technology and Science, Hyderabad, India","Software Engineering is the domain developed for designing, coding and testing of various software projects of computer and other electronic devices. GSD is the environment for developing projects at geographically isolated areas beyond cultural peripheries in a harmonized vogue comprising synchronous and asynchronous communication. The conventional methods are utilized to estimate effort for co-located projects which are not efficient for GSD-projects. The parametric effort estimation models can estimate effort for both co-located and GSD-projects but it has impact factors on GSD-projects based on their cultural difference and distance. Here we have introduced Scheduling-based model for effort estimation. In this research, three models are calibrated for estimating effort in GSD-environment. The performance can be evaluated based on the accuracy of effort measured from the deviation of actual effort from completed projects and estimated effort before and after calibration. The calibrated estimation shows better accuracy in GSD-projects than the estimation before calibration. å© 2016 ACM.",Calibration; COCOMO II; Effort Estimation; GSD; Scheduling-based models; SLIM,Conference Paper,Scopus,2-s2.0-84988569512
"Ramacharan S., Rao K.V.G.",Scheduling based cost estimation model: An effective empirical approach for GSD project,2016,"IFIP International Conference on Wireless and Optical Communications Networks, WOCN",2016-November,,7759881,,,yes,,,,10.1109/WOCN.2016.7759881,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85005952256&doi=10.1109%2fWOCN.2016.7759881&partnerID=40&md5=761ea1af91f081c9b659c5c16a5c0a13,"G. Narayanamma Institute of Technology and Science, Hyderabad, India","Ramacharan, S., G. Narayanamma Institute of Technology and Science, Hyderabad, India; Rao, K.V.G., G. Narayanamma Institute of Technology and Science, Hyderabad, India","Trustworthy effort estimation for a software project remains one of the biggest challenges for developer. Software effort estimation is the initial phase between the client and the business enterprise. So the association begins with effort estimation. Trustworthiness increases accurate estimation. Organizations turned toward GSD because it benefits with shorter development cycle. This paper presents a preliminary study of the consequences of intercultural factors. GSD involves geographical, temporal and sociocultural distances which affects communication, coordination and control of development processes. This paper deals with parameters for accurate estimation using Scheduling based method and also recommends few ideas for future research. å© 2016 IEEE.",Geographical distance; Global Software Development; Scheduling based Estimation; sociocultural distance; temporal distance,Conference Paper,Scopus,2-s2.0-85005952256
"Puri R., Kaur I.",Human opinion dynamics for software cost estimation,2016,Advances in Intelligent Systems and Computing,437,,,375,386,yes,,,,10.1007/978-981-10-0451-3_35,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964915763&doi=10.1007%2f978-981-10-0451-3_35&partnerID=40&md5=7c1594bfde2823a4fd2d2fb2c5bf303c,"Department of Computer Science Engineering, Chandigarh University, Chandigarh, India","Puri, R., Department of Computer Science Engineering, Chandigarh University, Chandigarh, India; Kaur, I., Department of Computer Science Engineering, Chandigarh University, Chandigarh, India",Human opinion dynamics is a novel approach to solve complex optimization problems. In this paper we propose and implement human opinion dynamics for tuning the parameters of the COCOMO model for software cost estimation. The input is coding size or lines of code and the output is effort in person-months. Mean absolute relative error and prediction are the two objectives considered for fine-tuning of parameters. The dataset considered is COCOMO. The current paper demonstrates that use of human opinion dynamics illustrates promising results. It has been observed that when compared with standard COCOMO it gives better results. å© Springer Science+Business Media Singapore 2016.,COCOMO; Human opinion dynamics; MARE; Social influence; Update rule,Conference Paper,Scopus,2-s2.0-84964915763
"Phannachitta P., Monden A., Keung J., Matsumoto K.",LSA-X: Exploiting productivity factors in linear size adaptation for analogy-based software effort estimation,2016,IEICE Transactions on Information and Systems,E99D,1,,151,162,yes,,,,10.1587/transinf.2015EDP7237,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84953335851&doi=10.1587%2ftransinf.2015EDP7237&partnerID=40&md5=0ad91a33ab47dce11fd45fc74c9cce5e,"Graduate School of Information Science, Nara Institute of Science and Technology, Ikoma-shi, Japan; Graduate School of Natural Science and Technology, Okayama University, Okayama-shi, Japan; Department of Computer Science, City University of Hong Kong, China","Phannachitta, P., Graduate School of Information Science, Nara Institute of Science and Technology, Ikoma-shi, Japan; Monden, A., Graduate School of Natural Science and Technology, Okayama University, Okayama-shi, Japan; Keung, J., Department of Computer Science, City University of Hong Kong, China; Matsumoto, K., Graduate School of Information Science, Nara Institute of Science and Technology, Ikoma-shi, Japan","Analogy-based software effort estimation has gained a considerable amount of attention in current research and practice. Its excellent estimation accuracy relies on its solution adaptation stage, where an effort estimate is produced from similar past projects. This study proposes a solution adaptation technique named LSA-X that introduces an approach to exploit the potential of productivity factors, i.e., project variables with a high correlation with software productivity, in the solution adaptation stage. The LSA-X technique tailors the exploitation of the productivity factors with a procedure based on the Linear Size Adaptation (LSA) technique. The results, based on 19 datasets show that in circumstances where a dataset exhibits a high correlation coefficient between productivity and a related factor (r ‰ä´ 0.30), the proposed LSA-X technique statistically outperformed (95% confidence) the other 8 commonly used techniques compared in this study. In other circumstances, our results suggest using any linear adaptation technique based on software size to compensate for the limitations of the LSA-X technique. Copyright å© 2016 The Institute of Electronics, Information and Communication Engineers.",Adaptation; Analogy; Empirical experiments; Productivity factor; Software development effort estimation,Article,Scopus,2-s2.0-84953335851
"Phannachitta P., Keung J., Monden A., Matsumoto K.",A stability assessment of solution adaptation techniques for analogy-based software effort estimation,2017,Empirical Software Engineering,22,1,,474,504,yes,,,,10.1007/s10664-016-9434-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973160570&doi=10.1007%2fs10664-016-9434-8&partnerID=40&md5=7137f3ea89fed0d651e73d103e04dfd1,"Graduate School of Information Science, Nara Institute of Science and Technology, Nara, Japan; Department of Computer Science, City University of Hong Kong, Hong Kong, China; Graduate School of Natural Science and Technology, Okayama University, Okayama, Japan","Phannachitta, P., Graduate School of Information Science, Nara Institute of Science and Technology, Nara, Japan; Keung, J., Department of Computer Science, City University of Hong Kong, Hong Kong, China; Monden, A., Graduate School of Natural Science and Technology, Okayama University, Okayama, Japan; Matsumoto, K., Graduate School of Natural Science and Technology, Okayama University, Okayama, Japan","Among numerous possible choices of effort estimation methods, analogy-based software effort estimation based on Case-based reasoning is one of the most adopted methods in both the industry and research communities. Solution adaptation is the final step of analogy-based estimation, employed to aggregate and adapt to solutions derived during the case-based reasoning process. Variants of solution adaptation techniques have been proposed in previous studies; however, the ranking of these techniques is not conclusive and shows conflicting results, since different studies rank these techniques in different ways. This paper aims to find a stable ranking of solution adaptation techniques for analogy-based estimation. Compared with the existing studies, we evaluate 8 commonly adopted solution techniques with more datasets (12), more feature selection techniques included (4), and more stable error measures (5) to a robust statistical test method based on the Brunner test. This comprehensive experimental procedure allows us to discover a stable ranking of the techniques applied, and to observe similar behaviors from techniques with similar adaptation mechanisms. In general, the linear adaptation techniques based on the functions of size and productivity (e.g., regression towards the mean technique) outperform the other techniques in a more robust experimental setting adopted in this study. Our empirical results show that project features with strong correlation to effort, such as software size or productivity, should be utilized in the solution adaptation step to achieve desirable performance. Designing a solution adaptation strategy in analogy-based software effort estimation requires careful consideration of those influential features to ensure its prediction is of relevant and accurate. å© 2016, Springer Science+Business Media New York.",Analogy-based estimation; Ranking instability; Robust statistical method; Software effort estimation; Solution adaptation techniques,Article,Scopus,2-s2.0-84973160570
"Ono K., Tsunoda M., Monden A., Matsumoto K.",Influence of outliers on analogy based software development effort estimation,2016,"2016 IEEE/ACIS 15th International Conference on Computer and Information Science, ICIS 2016 - Proceedings",,,7550865,,,yes,,,,10.1109/ICIS.2016.7550865,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987967929&doi=10.1109%2fICIS.2016.7550865&partnerID=40&md5=b5874036a32595164caab38190a85f64,"Graduate School of Information Science, Nara Institute of Science and Technology, Nara, Japan; Department of Informatics, Kindai University, Osaka, Japan; Graduate School of Natural Science and Technology, Okayama University, Okayama, Japan","Ono, K., Graduate School of Information Science, Nara Institute of Science and Technology, Nara, Japan; Tsunoda, M., Department of Informatics, Kindai University, Osaka, Japan; Monden, A., Graduate School of Natural Science and Technology, Okayama University, Okayama, Japan; Matsumoto, K., Graduate School of Information Science, Nara Institute of Science and Technology, Nara, Japan","In a software development project, project management is indispensable, and effort estimation is one of the important factors on the management. To improve estimation accuracy, outliers are often removed from dataset used for estimation. However, the influence of the outliers to the estimation accuracy is not clear. In this study, we added outliers to dataset experimentally, to analyze the influence. In the analysis, we changed the percentage of outliers, the extent of outliers, variable including outliers, and location of outliers on the dataset. After that, effort was estimated using the dataset. In the experiment, the influence of outliers was not very large, when they were included in the software size metric, the percentage of outliers was 10%, and the extent of outliers was 100%. å© 2016 IEEE.",abnormal value; case based reasoning; effort prediction,Conference Paper,Scopus,2-s2.0-84987967929
"Nwaiwu J.C., Oluwadare S.A.",Analytic study of fuzzy-based model for software cost estimation,2016,CEUR Workshop Proceedings,1755,,,22,30,yes,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009376475&partnerID=40&md5=108c6aadc8734ce283fffae33f57c39c,"Computer Science Department, Federal University of Technology Akure, Nigeria","Nwaiwu, J.C., Computer Science Department, Federal University of Technology Akure, Nigeria; Oluwadare, S.A., Computer Science Department, Federal University of Technology Akure, Nigeria","The need for successful software projects has been a major area of discourse amongst researchers and software developers in academia and software industry respectively. Failure of software projects has been tied to flawed estimation at the early stages of software development life cycle. Recently, soft computing techniques such as Fuzzy logic models has been seen as an alternative to handle uncertainties and vagueness of input parameters to the early software estimation models. In order to analyze the various conditions which affect estimation accuracy of fuzzy-based models, a sample of 93 COCOMO NASA projects was used to develop two groups of fuzzy models. One was the controlled group while the other was the experimental group varying in conditions of model structure, linguistic variables, parameters of input and output variables. A comparative analysis of the Mean Magnitude of Relative Error (MMRE) and Prediction accuracy Pred(l) evaluation criteria for the models was made and findings recorded. Results from the experiments show that the performance of a fuzzy-based software cost estimation model utilizing Takagi-Sugeno inference, Gaussian/Sigmoid membership function with more number of input variables and linguistics variables is more efficient. Copyright å© 2016 Ibadan ACM.",Fuzzy Inference System; Fuzzy model; Membership function (MF); Performance; Software cost estimation (SCE),Conference Paper,Scopus,2-s2.0-85009376475
"Nassif A.B., Azzeh M., Capretz L.F., Ho D.",Neural network models for software development effort estimation: a comparative study,2016,Neural Computing and Applications,27,8,,2369,2381,yes,,,1,10.1007/s00521-015-2127-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947554861&doi=10.1007%2fs00521-015-2127-1&partnerID=40&md5=652d9fb3cb82d2405b9c39990e37f2df,"Department of Electrical and Computer Engineering, University of Sharjah, Sharjah, United Arab Emirates; Department of Software Engineering, Applied Science University, PO BOX 166, Amman, Jordan; Department of Electrical and Computer Engineering, University of Western Ontario, London, ON, Canada; NFA Estimation Inc., Richmond Hill, ON, Canada","Nassif, A.B., Department of Electrical and Computer Engineering, University of Sharjah, Sharjah, United Arab Emirates; Azzeh, M., Department of Software Engineering, Applied Science University, PO BOX 166, Amman, Jordan; Capretz, L.F., Department of Electrical and Computer Engineering, University of Western Ontario, London, ON, Canada; Ho, D., NFA Estimation Inc., Richmond Hill, ON, Canada","Software development effort estimation (SDEE) is one of the main tasks in software project management. It is crucial for a project manager to efficiently predict the effort or cost of a software project in a bidding process, since overestimation will lead to bidding loss and underestimation will cause the company to lose money. Several SDEE models exist; machine learning models, especially neural network models, are among the most prominent in the field. In this study, four different neural network models‰ÛÓmultilayer perceptron, general regression neural network, radial basis function neural network, and cascade correlation neural network‰ÛÓare compared with each other based on: (1) predictive accuracy centred on the mean absolute error criterion, (2) whether such a model tends to overestimate or underestimate, and (3) how each model classifies the importance of its inputs. Industrial datasets from the International Software Benchmarking Standards Group (ISBSG) are used to train and validate the four models. The main ISBSG dataset was filtered and then divided into five datasets based on the productivity value of each project. Results show that the four models tend to overestimate in 80åÊ% of the datasets, and the significance of the model inputs varies based on the selected model. Furthermore, the cascade correlation neural network outperforms the other three models in the majority of the datasets constructed on the mean absolute residual criterion. å© 2015, The Natural Computing Applications Forum.",Cascade correlation neural network; General regression neural network; Multilayer perceptron; Neural network model; Radial basis function neural network; Software development effort estimation,Article,Scopus,2-s2.0-84947554861
"Murillo-Morera J., Quesada-LÌ_pez C., Castro-Herrera C., Jenkins M.",An empirical validation of an automated genetic software effort prediction framework using the ISBSG dataset,2016,CIBSE 2016 - XIX Ibero-American Conference on Software Engineering,,,,185,199,yes,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988350386&partnerID=40&md5=09eb6bbb921cb2cc3c422cc8de6c0115,"Center for ICT Research, University of Costa Rica, Costa Rica; Intel Corporation, Costa Rica","Murillo-Morera, J., Center for ICT Research, University of Costa Rica, Costa Rica; Quesada-LÌ_pez, C., Center for ICT Research, University of Costa Rica, Costa Rica; Castro-Herrera, C., Intel Corporation, Costa Rica; Jenkins, M., Center for ICT Research, University of Costa Rica, Costa Rica","Background: The complexity of providing accurate software effort prediction models is well known in the software industry. Several prediction models have been proposed in the literature using different techniques, with different results, in different contexts. Objectives: This paper reports a benchmarking study using a genetic approach that automatically generates and compares different learning schemes (preprocessing+ attribute selection+learning algorithms). The effectiveness of the software development effort prediction models (using function points) were validated using the ISBSG R12 dataset. Methods: Eight subsets of projects were analyzed running a MÌÑN-fold cross-validation. We used a genetic approach to automatically select the components of the learning schemes, to evaluate, and to report the learning scheme with the best performance. Results: In total, 150 learning schemes were studied (2 data preprocessors, 5 attribute selectors, and 15 modeling techniques). The most common learning schemes were: Log+ForwardSelection+M5-Rules, Log+BestFirst+M5-Rules, Log+LinearForwardSelection+SMOreg, ForwardSelection+ SMOreg and ForwardSelection+ SMOreg, BackwardElimination+ SMOreg, LinearForwardSelection+SMOreg, and Log+Best First+SMOreg. Conclusions: The results show that we should select a different learning schemes for each datasets. Our results support previous findings regarding that the setup applied in evaluations can completely reverse findings. A genetic approach that automatically selects best combination based on a specific dataset could improve the performance of software effort prediction models.",Effort prediction model; Experiment; Function points; Genetic approach; ISBSG dataset; Learning schemes,Conference Paper,Scopus,2-s2.0-84988350386
"Mendes E., Vaz V.T., Muradas F.",An expert-based requirements effort estimation model using bayesian networks,2016,Lecture Notes in Business Information Processing,238,,,79,93,yes,,,,10.1007/978-3-319-27033-3_6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952646318&doi=10.1007%2f978-3-319-27033-3_6&partnerID=40&md5=de2982f30a4a5721497c61320feb9caa,"BTH ‰ÛÒ Blekinge Institute of Technology, Karlskrona, Sweden; University of Oulu, Oulu, Finland; UFRJ ‰ÛÒ Federal University of Rio de Janeiro, P.O. Box 68511, Rio de Janeiro, Brazil; Naval Systems Analysis Centre, San Diego, Brazil","Mendes, E., BTH ‰ÛÒ Blekinge Institute of Technology, Karlskrona, Sweden, University of Oulu, Oulu, Finland; Vaz, V.T., UFRJ ‰ÛÒ Federal University of Rio de Janeiro, P.O. Box 68511, Rio de Janeiro, Brazil; Muradas, F., Naval Systems Analysis Centre, San Diego, Brazil","[Motivation]: There are numerous software companies worldwide that split the software development life cycle into at least two separate projects ‰ÛÒ an initial project where a requirements specification document is prepared; and a follow-up project where the previously prepared requirements document is used as input to developing a software application. These follow-up projects can also be delegated to a third party, as occurs in numerous global software development scenarios. Effort estimation is one of the cornerstones of any type of project management; however, a systematic literature review on requirements effort estimation found hardly any empirical study investigating this topic. [Objective]: The goal of this paper is to describe an industrial case study where an expert-based requirements effort estimation model was built and validated for the Brazilian Navy. [Method]: A knowledge engineering of Bayesian networks process was employed to build the requirements effort estimation model. [Results]: The expert-based requirements effort estimation model was built with the participation of seven software requirements analysts and project managers, leading to 28 prediction factors and 30+ relationships. The model was validated based on real data from 11 large requirements specification projects. The model was incorporated into the Brazilian navy‰Ûªs quality assurance process to be used by their software requirements analysts and managers. [Conclusion]: This paper details a case study where an expert-based requirements effort estimation model based solely on knowledge from requirements analysts and project managers was successfully built to help the Brazilian Navy estimate the requirements effort for their projects. å© Springer International Publishing Switzerland 2016.",Bayesian networks; Cost estimation; Industrial case study; Requirements effort estimation; Requirements engineering,Conference Paper,Scopus,2-s2.0-84952646318
"Malhotra R., Khanna M., Raje R.R.",On the application of search-based techniques for software engineering predictive modeling: A systematic review and future directions,2017,Swarm and Evolutionary Computation,32,,,85,109,yes,,,,10.1016/j.swevo.2016.10.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85002466321&doi=10.1016%2fj.swevo.2016.10.002&partnerID=40&md5=01a5d51962f9955f474e107a6c5e5e7e,"Department of Software Engineering, Delhi Technological University, India; Delhi Technological University, India; Sri Guru Gobind Singh College of Commerce, University of Delhi, India; Department of Computer and Information Science, Indiana University-Purdue University Indianapolis, Indiana, IN, United States","Malhotra, R., Department of Software Engineering, Delhi Technological University, India; Khanna, M., Delhi Technological University, India, Sri Guru Gobind Singh College of Commerce, University of Delhi, India; Raje, R.R., Department of Computer and Information Science, Indiana University-Purdue University Indianapolis, Indiana, IN, United States","Software engineering predictive modeling involves construction of models, with the help of software metrics, for estimating quality attributes. Recently, the use of search-based techniques have gained importance as they help the developers and project-managers in the identification of optimal solutions for developing effective prediction models. In this paper, we perform a systematic review of 78 primary studies from January 1992 to December 2015 which analyze the predictive capability of search-based techniques for ascertaining four predominant software quality attributes, i.e., effort, defect proneness, maintainability and change proneness. The review analyses the effective use and application of search-based techniques by evaluating appropriate specifications of fitness functions, parameter settings, validation methods, accounting for their stochastic natures and the evaluation of developmental models with the use of well-known statistical tests. Furthermore, we compare the effectiveness of different models, developed using the various search-based techniques amongst themselves, and also with the prevalent machine learning techniques used in literature. Although there are very few studies which use search-based techniques for predicting maintainability and change proneness, we found that the results of the application of search-based techniques for effort estimation and defect prediction are encouraging. Hence, this comprehensive study and the associated results will provide guidelines to practitioners and researchers and will enable them to make proper choices for applying the search-based techniques to their specific situations. å© 2016 Elsevier B.V.",Change prediction; Defect prediction; Effort estimation; Maintainability prediction; Search-based techniques; Software quality,Article,Scopus,2-s2.0-85002466321
"Kumari S., Pushkar S.",Software cost estimation using cuckoo search,2017,Advances in Intelligent Systems and Computing,509,,,167,175,yes,,,,10.1007/978-981-10-2525-9_17,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996956350&doi=10.1007%2f978-981-10-2525-9_17&partnerID=40&md5=37d7419ae2d743127624e559a467a38d,"Department of CSE, BIT, Mesra, Ranchi, India","Kumari, S., Department of CSE, BIT, Mesra, Ranchi, India; Pushkar, S., Department of CSE, BIT, Mesra, Ranchi, India","One of the important aspects of any software organization is to use models that can accurately estimate the software development effort. However, development of accurate estimation model is still a challenging issue for software engineering research community. This paper proposes a new model for software cost estimation that uses Cuckoo Search (CS) algorithm for finding the optimal parameter of the cost estimation model. The proposed model has been tested on NASA software project datasets. Experimental results show that the proposed model has improved the performance of the estimated effort with respect to MMRE (Mean Magnitude of Relative Error) and PRED (Prediction). å© Springer Science+Business Media Singapore 2017.",COCOMO model; CS algorithm; MMRE and PRED; Software cost estimation,Conference Paper,Scopus,2-s2.0-84996956350
"Kumar G., Bhatia P.K.",Empirical assessment and optimization of software cost estimation using soft computing techniques,2016,Advances in Intelligent Systems and Computing,452,,,117,130,yes,,,,10.1007/978-981-10-1023-1_12,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976496079&doi=10.1007%2f978-981-10-1023-1_12&partnerID=40&md5=9c3f9d7140d44b25e8b0976682b1efb9,"Department of Computer Science and Engineering, Guru Jambheshwar University of Science & Technology, Hisar, Haryana, India","Kumar, G., Department of Computer Science and Engineering, Guru Jambheshwar University of Science & Technology, Hisar, Haryana, India; Bhatia, P.K., Department of Computer Science and Engineering, Guru Jambheshwar University of Science & Technology, Hisar, Haryana, India","Software Engineering especially project planning, scheduling, monitoring and control are based on accurate estimate of the cost and effort. In the initial stage of Software Development Life Cycle (SDLC), it is hard to accurately measure software effort that may lead to possibility of project failure. Here, an empirical comparison of existing software cost estimation models based on the techniques used in those models has been elaborated using statistical criteria. On the basis of findings of empirical evaluation of existing models, a Neuro-Fuzzy Software Cost Estimation model has been proposed to hold best practices found in other models and to optimize software cost estimation. Proposed model gives good result as compared to other considered software cost estimation methods for the defined parameters in overall but it is also dependent on type of project, data and technique used in implementation. å© Springer Science+Business Media Singapore 2016.",Back propagation neural network (BPNN); Constructive cost model (COCOMO); Function point (FP); Fuzzy logic (FL); Genetic algorithm (GA); Particle swarm optimization (PSO); Software cost estimation (SCE),Conference Paper,Scopus,2-s2.0-84976496079
"Kondo M., Mizuno O.",Analysis on Causal-Effect Relationship in Effort Metrics Using Bayesian LiNGAM,2016,"Proceedings - 2016 IEEE 27th International Symposium on Software Reliability Engineering Workshops, ISSREW 2016",,,7789376,47,48,yes,,,,10.1109/ISSREW.2016.18,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009823670&doi=10.1109%2fISSREW.2016.18&partnerID=40&md5=f2d945e8d46f7e78ac627e8ae15304eb,"School of Science and Technology, Kyoto Institute of Technology, Kyoto, Japan; Faculty of Information and Human Sciences, Kyoto Institute of Technology, Kyoto, Japan","Kondo, M., School of Science and Technology, Kyoto Institute of Technology, Kyoto, Japan; Mizuno, O., Faculty of Information and Human Sciences, Kyoto Institute of Technology, Kyoto, Japan","In the effort estimation studies, we can obtain open datasets from the past research. Those datasets are either within-company or cross-company dataset. On effort estimation, it was long discussed which dataset is appropriate for building accurate model. To find a new viewpoint in this discussion, we introduce the causal-effect relationship estimation technique. We use a simple Bayesian approach that is defined by the data generation model in a Linear Non-Gaussian Acyclic Model (LiNGAM). This model is applied to the function point and effort metrics in both within-company and cross-company datasets. We assume that if a dataset is appropriate for effort estimation, causal-effect relationships between metrics and effort will be extracted more. The result of case study shows that we can extract more causal-effect relationships from the cross-company dataset than that of from the within-company dataset. å© 2016 IEEE.",,Conference Paper,Scopus,2-s2.0-85009823670
"Khuat T.T., Le M.H.",Optimizing parameters of software effort estimation models using directed artificial bee colony algorithm,2016,Informatica (Slovenia),40,4,,427,436,yes,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009455591&partnerID=40&md5=937f8043ecd6285e01e160c2c3daeabb,"University of Danang, University of Science and Technology, Danang, Viet Nam","Khuat, T.T., University of Danang, University of Science and Technology, Danang, Viet Nam; Le, M.H., University of Danang, University of Science and Technology, Danang, Viet Nam","Effective software effort estimation is one of the challenging tasks in software engineering. There have been various alternatives introduced to enhance the accuracy of predictions. In this respect, estimation approaches based on algorithmic models have been widely used. These models consider modeling software effort as a function of the size of the developed project. However, most approaches sharing a common thread of complex mathematical models face the difficulties in parameters calibration and tuning. This study proposes using a directed artificial bee colony algorithm in order to tune the values of model parameters based on past actual effort. The proposed methods were verified with NASA software dataset and the obtained results were compared to the existing models in other literature. The results indicated that our proposal has significantly improved the performance of the estimations.",Directed artificial bee colony; Estimation models; Optimization; Software effort estimation; Swarm intelligence,Conference Paper,Scopus,2-s2.0-85009455591
"Khatri S.K., Malhotra S., Johri P.",Use case point estimation technique in software development,2016,"2016 5th International Conference on Reliability, Infocom Technologies and Optimization, ICRITO 2016: Trends and Future Directions",,,7784938,123,128,yes,,,,10.1109/ICRITO.2016.7784938,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010496187&doi=10.1109%2fICRITO.2016.7784938&partnerID=40&md5=a7453ed92f1c24d6a1d30d9d83320bdf,"Amity University Uttar Pradesh, India; Galgotias University, Greater Noida, India","Khatri, S.K., Amity University Uttar Pradesh, India; Malhotra, S., Amity University Uttar Pradesh, India; Johri, P., Galgotias University, Greater Noida, India","Software Projects are developed with the prior requirements and should be capable to complete on time under a fixed budget but it gets late to delivered, gets over-budget and even not able to meet user expectations. In agile approach, the estimation of software depends on expert opinion or on any historical data which is used as the input to previous methods like planning poker. The accuracy in estimation is the primary goal of any development but various factors related to environment and technical complexity which may further alleviate the size and effort of a project. Previously proposed estimation models were successful in estimation but lacks due to some obstacles such as less accuracy and customer satisfaction as per the requirement, other factors such as complexity, risk tracking and estimation. This paper emphasizes on a new algorithmic approach to estimate considering Environment and Technical factors so as to have a more accuracy with the use cases under agile development. å© 2016 IEEE.",Agile Development; Environmental Factors; Technical Complexity Factors; Use Case; Use Case Point Estimation,Conference Paper,Scopus,2-s2.0-85010496187
"Khatibi Bardsiri A., Hashemi S.M.",A differential evolution-based model to estimate the software services development effort,2016,Journal of Software: Evolution and Process,28,1,,57,77,yes,,,,10.1002/smr.1765,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956590025&doi=10.1002%2fsmr.1765&partnerID=40&md5=baab5687e09e901513096860c878a166,"Computer Engineering Department, Science and Research Branch, Islamic Azad University, Tehran, Iran","Khatibi Bardsiri, A., Computer Engineering Department, Science and Research Branch, Islamic Azad University, Tehran, Iran; Hashemi, S.M., Computer Engineering Department, Science and Research Branch, Islamic Azad University, Tehran, Iran","Accurate estimation of software service development effort is a great challenge both in industry and for academia. The concept of effort is an important and effective parameter in process development and software service management. The reliable estimation of effort helps the project managers to allocate the resources better and manage cost and time so that the project will be finished in the determined time and budget. One of the most popular effort estimation methods is analogy-based estimation (ABE) to compare a service with similar historical cases. Unfortunately, ABE is not capable of generating accurate results unless determining weights for service features. Therefore, this paper aims to make an efficient and reliable model through combining ABE method and differential evolution algorithm to estimate the software services development effort. In fact, the differential evolution algorithm was utilized for weighing features in the similarity function of the ABE method. This weighing process could help determining the importance level of the various service features and extracting the best similar historical case. The proposed hybrid model has been evaluated on two real datasets and two artificial datasets. The obtained results were compared with common effort estimation methods. This comparison showed more accuracy, faster convergence, and lower cost of the proposed model. Examine 11 most popular effort-estimation models. Propose new and efficient weighting model. Use many data sets and performance metrics. Help to choice and a better understanding of software-service effort-estimation methods. Copyright å© 2015 John Wiley & Sons, Ltd.",analogy based estimation; development effort; differential evolution algorithm; software services,Article,Scopus,2-s2.0-84956590025
"Kaushik A., Tayal D.K., Yadav K., Kaur A.",Integrating firefly algorithm in artificial neural network models for accurate software cost predictions,2016,Journal of Software: Evolution and Process,28,8,,665,688,yes,,,,10.1002/smr.1792,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973901088&doi=10.1002%2fsmr.1792&partnerID=40&md5=eca487671642ac37dbc5ba48ae7e2404,"Department of IT, Maharaja Surajmal Institute of Technology, New Delhi, India; Department of Computer Science, Indira Gandhi Delhi Technical University for Women, Delhi, India; Department of IT, Indira Gandhi Delhi Technical University for Women, Delhi, India; University School of Information and Communication Technology, GGSIP University, New Delhi, India; Indira Gandhi Delhi Technical University for Women, Research Scholar, India","Kaushik, A., Department of IT, Maharaja Surajmal Institute of Technology, New Delhi, India, Indira Gandhi Delhi Technical University for Women, Research Scholar, India; Tayal, D.K., Department of Computer Science, Indira Gandhi Delhi Technical University for Women, Delhi, India; Yadav, K., Department of IT, Indira Gandhi Delhi Technical University for Women, Delhi, India; Kaur, A., University School of Information and Communication Technology, GGSIP University, New Delhi, India","Human effort is one of the main resources of software cost estimation. A successful software project development primarily relies on accurate effort prediction at an early stage of development. There are many effort prediction models in the literature. Deciding which model to choose is a challenge for the project managers. This paper investigates whether it is possible to improve the accuracy of software cost estimations by coupling firefly algorithm with the existing artificial neural network (ANN) models used in software cost predictions. The firefly algorithm is one of the recent evolutionary computing models inspired by the behaviour of fireflies in nature. This is compared with particle swarm optimization used already in literature for software cost estimations. The ANN models examined in this work include radial basis function network and functional link artificial neural networks models. The experimental results show that ANN models perform extremely well by incorporating firefly algorithm and intuitionistic fuzzy C-means for data preprocessing. The proposed approach is empirically validated through a statistical framework. Copyright å© 2016 John Wiley & Sons, Ltd. Copyright å© 2016 John Wiley & Sons, Ltd.",firefly algorithm; functional link artificial neural network; intuitionistic fuzzy C means; particle swarm optimization; radial basis function neural network; software cost estimation,Article,Scopus,2-s2.0-84973901088
Iraji M.S.,Hypermedia web software effort estimate with adaptive neuro fuzzy inference system,2016,Journal of Theoretical and Applied Information Technology,93,1,,133,142,yes,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995676366&partnerID=40&md5=d5dda17f602f141b5186ea16d31a0714,"Faculty Member of Department of Computer Engineering and Information Technology, Payame Noor University, Iran","Iraji, M.S., Faculty Member of Department of Computer Engineering and Information Technology, Payame Noor University, Iran","Accurate software cost estimates are an important factor in the stability of the software companies in the world competitive and efficient use of resources. Nature and structure of web applications is quite different from traditional software. In 2003, The estimated cost of hypermedia web projects was based on seven features were obtained best results, using case base reasoning (CBR) using Stepwise Regression approaches with MMRE on 37 web hypermedia projects. We considered count of html, count of media files and count of inner links features, presented in this paper proposed approach to reduce predicted effort Error than the actual amount for web hypermedia projects and calculate average relative deviations (AAD), through adaptive neuro fuzzy system (ANFIS) Method that is achieved better and more accurate results. å© 2005 - 2016 JATIT & LLS. All rights reserved.",Cost; Effort; Fuzzy; Hypermedia web software; Neural network,Article,Scopus,2-s2.0-84995676366
"Idri A., Hosni M., Abran A.",Improved estimation of software development effort using Classical and Fuzzy Analogy ensembles,2016,Applied Soft Computing Journal,49,,,990,1019,yes,,,1,10.1016/j.asoc.2016.08.012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997417515&doi=10.1016%2fj.asoc.2016.08.012&partnerID=40&md5=b29d5f00b8c137b8fef1ba5d0c2a68a1,"Software Projects Management Research Team, ENSIAS, Mohammed V University, Rabat, Morocco; Department of Software Engineering, Ìäcole d e Technologie SupÌ©rieure, MontrÌ©al, Canada","Idri, A., Software Projects Management Research Team, ENSIAS, Mohammed V University, Rabat, Morocco; Hosni, M., Software Projects Management Research Team, ENSIAS, Mohammed V University, Rabat, Morocco; Abran, A., Department of Software Engineering, Ìäcole d e Technologie SupÌ©rieure, MontrÌ©al, Canada","Delivering an accurate estimate of software development effort plays a decisive role in successful management of a software project. Therefore, several effort estimation techniques have been proposed including analogy based techniques. However, despite the large number of proposed techniques, none has outperformed the others in all circumstances and previous studies have recommended generating estimation from ensembles of various single techniques rather than using only one solo technique. Hence, this paper proposes two types of homogeneous ensembles based on single Classical Analogy or single Fuzzy Analogy for the first time. To evaluate this proposal, we conducted an empirical study with 100/60 variants of Classical/Fuzzy Analogy techniques respectively. These variants were assessed using standardized accuracy and effect size criteria over seven datasets. Thereafter, these variants were clustered using the Scott-Knott statistical test and ranked using four unbiased errors measures. Moreover, three linear combiners were used to combine the single estimates. The results show that there is no best single Classical/Fuzzy Analogy technique across all datasets, and the constructed ensembles (Classical/Fuzzy Analogy ensembles) are often ranked first and their performances are, in general, higher than the single techniques. Furthermore, Fuzzy Analogy ensembles achieve better performance than Classical Analogy ensembles and there is no best Classical/Fuzzy ensemble across all datasets and no evidence concerning the best combiner. å© 2016 Elsevier B.V.",Analogy; Ensemble effort estimation; Fuzzy logic; Software development effort estimation,Article,Scopus,2-s2.0-84997417515
"Idri A., Hosni M., Abran A.",Systematic literature review of ensemble effort estimation,2016,Journal of Systems and Software,118,,,151,175,yes,,,1,10.1016/j.jss.2016.05.016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969531836&doi=10.1016%2fj.jss.2016.05.016&partnerID=40&md5=4217c2107911348615a692540959501e,"Software Projects Management Research Team, ENSIAS, Mohammed v University in Rabat, Morocco; Department of Software Engineering, Ìäcole de Technologie SupÌ©rieure, MontrÌ©al, Canada","Idri, A., Software Projects Management Research Team, ENSIAS, Mohammed v University in Rabat, Morocco; Hosni, M., Software Projects Management Research Team, ENSIAS, Mohammed v University in Rabat, Morocco; Abran, A., Department of Software Engineering, Ìäcole de Technologie SupÌ©rieure, MontrÌ©al, Canada","The need to overcome the weaknesses of single estimation techniques for prediction tasks has given rise to ensemble methods in software development effort estimation (SDEE). An ensemble effort estimation (EEE) technique combines several of the single/classical models found in the SDEE literature. However, to the best of our knowledge, no systematic review has yet been performed with a focus on the use of EEE techniques in SDEE. The purpose of this review is to analyze EEE techniques from six viewpoints: single models used to construct ensembles, ensemble estimation accuracy, rules used to combine single estimates, accuracy comparison of EEE techniques with single models, accuracy comparison between EEE techniques and methodologies used to construct ensemble methods. We performed a systematic review of EEE studies published between 2000 and 2016, and we selected 24 of them to address the questions raised in this review. We found that EEE techniques may be separated into two types: homogeneous and heterogeneous, and that the machine learning single models are the most frequently employed in constructing EEE techniques. We also found that EEE techniques usually yield acceptable estimation accuracy, and in fact are more accurate than single models. å© 2016 Elsevier Inc. All rights reserved.",Ensemble effort estimation; Software development effort estimation; Systematic literature review,Article,Scopus,2-s2.0-84969531836
"Idri A., Hosni M., Abran A.",Systematic mapping study of ensemble effort estimation,2016,ENASE 2016 - Proceedings of the 11th International Conference on Evaluation of Novel Software Approaches to Software Engineering,,,,132,139,yes,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979496940&partnerID=40&md5=8108d3f05a4d4c689c6237a0019141b7,"Software Project Management Research Team, ENSIAS, Mohammed V University in Rabat, Rabat, Morocco; Department of Software Engineering, ETS, MontrÌ©al, Canada","Idri, A., Software Project Management Research Team, ENSIAS, Mohammed V University in Rabat, Rabat, Morocco; Hosni, M., Software Project Management Research Team, ENSIAS, Mohammed V University in Rabat, Rabat, Morocco; Abran, A., Department of Software Engineering, ETS, MontrÌ©al, Canada","Ensemble methods have been used recently for prediction in data mining area in order to overcome the weaknesses of single estimation techniques. This approach consists on combining more than one single technique to predict a dependent variable and has attracted the attention of the software development effort estimation (SDEE) community. An ensemble effort estimation (EEE) technique combines several existing single/classical models. In this study, a systematic mapping study was carried out to identify the papers based on EEE techniques published in the period 2000-2015 and classified them according to five classification criteria: research type, research approach, EEE type, single models used to construct EEE techniques, and rule used the combine single estimates into an EEE technique. Publication channels and trends were also identified. Within the 16 studies selected, homogeneous EEE techniques were the most investigated. Furthermore, the machine learning single models were the most frequently employed to construct EEE techniques and two types of combiner (linear and non-linear) have been used to get the prediction value of an ensemble. Copyright å© 2016 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.",Ensemble effort estimation; Software development effort estimation; Systematic mapping study,Conference Paper,Scopus,2-s2.0-84979496940
"Idri A., Hassani A., Abran A.",RBFN network based models for estimating software development effort: A cross-validation study,2015,"Proceedings - 2015 IEEE Symposium Series on Computational Intelligence, SSCI 2015",,,7376718,976,983,yes,,,,10.1109/SSCI.2015.142,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964949962&doi=10.1109%2fSSCI.2015.142&partnerID=40&md5=af0dde3af393745967961e241dd05385,"Department of Software Engineering ENSIAS, Mohammed v University, Rabat, Morocco; Department of Software Engineering, Ecole de Technologie SupÌ©rieure, MontrÌ©al, Canada","Idri, A., Department of Software Engineering ENSIAS, Mohammed v University, Rabat, Morocco; Hassani, A., Department of Software Engineering ENSIAS, Mohammed v University, Rabat, Morocco; Abran, A., Department of Software Engineering, Ecole de Technologie SupÌ©rieure, MontrÌ©al, Canada","Software effort estimation is very crucial and there is always a need to improve its accuracy as much as possible. Several estimation techniques have been developed in this regard and it is difficult to determine which model gives more accurate estimation on which dataset. Among all proposed methods, the Radial Basis Function Neural (RBFN) networks models have presented promising results in software effort estimation. The main objective of this research is to evaluate the RBFN networks construction based on both hard and fuzzy C-means clustering algorithms using cross-validation approach. The objective of this replication study is to investigate if the RBFN-based models learned from the training data are able to estimate accurately the efforts of yet unseen data. This evaluation uses two historical datasets, namely COCOMO81 and ISBSG R8. å© 2015 IEEE.",,Conference Paper,Scopus,2-s2.0-84964949962
"Idri A., Abnane I., Abran A.",Missing data techniques in analogy-based software development effort estimation,2016,Journal of Systems and Software,117,,,595,611,yes,,,,10.1016/j.jss.2016.04.058,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966340503&doi=10.1016%2fj.jss.2016.04.058&partnerID=40&md5=014ecb6ca07ef9fbb58c0cbfb8c1dc3a,"Software Project Management Research Team, ENSIAS, Mohamed v University of Rabat, Rabat, Morocco; Department of Software Engineering, Ecole de Technologie SupÌ©rieure, MontrÌ©al, Canada","Idri, A., Software Project Management Research Team, ENSIAS, Mohamed v University of Rabat, Rabat, Morocco; Abnane, I., Software Project Management Research Team, ENSIAS, Mohamed v University of Rabat, Rabat, Morocco; Abran, A., Department of Software Engineering, Ecole de Technologie SupÌ©rieure, MontrÌ©al, Canada","Missing Data (MD) is a widespread problem that can affect the ability to use data to construct effective software development effort prediction systems. This paper investigates the use of missing data (MD) techniques with two analogy-based software development effort estimation techniques: Classical Analogy and Fuzzy Analogy. More specifically, we analyze the predictive performance of these two analogy-based techniques when using toleration, deletion or k-nearest neighbors (KNN) imputation techniques. A total of 1512 experiments were conducted involving seven data sets, three MD techniques (toleration, deletion and KNN imputation), three missingness mechanisms (MCAR: missing completely at random, MAR: missing at random, NIM: non-ignorable missing), and MD percentages from 10 percent to 90 percent. The results suggest that Fuzzy Analogy generates more accurate estimates in terms of the Standardized Accuracy measure (SA) than Classical Analogy regardless of the MD technique, the data set used, the missingness mechanism or the MD percentage. Moreover, this study found that the use of KNN imputation, rather than toleration or deletion, may improve the prediction accuracy of both analogy-based techniques. However, toleration, deletion and KNN imputation are affected by the missingness mechanism and the MD percentage, both of which have a strong negative impact upon effort prediction accuracy. å© 2016 Elsevier Inc. All rights reserved.",Analogy-based software development effort estimation; Imputation; Missing data,Article,Scopus,2-s2.0-84966340503
"Ibrahim H., Far B.H.",Clustering and artificial neural network ensembles based effort estimation,2016,"Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",2016-January,,,301,308,yes,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988447189&partnerID=40&md5=7d585c07ab6b3905ada23effec881515,"Department of Electrical and Computer Engineering, University of Calgary, Calgary, AB, Canada; Department of Information Systems, Menoufia University, Shebin Elkom, Egypt","Ibrahim, H., Department of Electrical and Computer Engineering, University of Calgary, Calgary, AB, Canada, Department of Information Systems, Menoufia University, Shebin Elkom, Egypt; Far, B.H., Department of Electrical and Computer Engineering, University of Calgary, Calgary, AB, Canada","Accurate effort estimation of software development projects plays a key role in project success. However, it is still a challenge activity to researchers and practitioners because of the nature of software products and dynamics in software industry and development environment. Artificial neural network (ANN) is as an effective method and has been widely used in various areas of software engineering. This paper proposes a new effort estimation method based on clustering and ANN ensembles. The contribution of the paper is twofold. First, the impact of clustering projects on the estimation accuracy is investigated. Second, the impact of using ANN ensembles instead of a single ANN is also investigated. The proposed method includes three phases called pre-processing, k-means clustering, and ANN ensembles effort estimation. The method starts with exploring the historical projects dataset. Afterward, k-means is used to cluster the projects. Finally, the proposed method as well as two other estimation methods (i.e. a single ANN and expert-based) were applied to the created clusters and results were compared using MMRE and PRED measures. The simulation results show that the proposed method significantly outperforms the two other estimation methods. Copyright å© 2016 by KSI Research Inc. and Knowledge Systems Institute Graduate School.",Artifical neural network ensembles; Clustering; Effort estimation; K-means; Mean magnitude relative error; Percentage of/predictions,Conference Paper,Scopus,2-s2.0-84988447189
"Huanca L.M., OrÌ© S.B.",Factors affecting the accuracy of use case points,2017,Advances in Intelligent Systems and Computing,537,,,133,142,yes,,,,10.1007/978-3-319-48523-2_13,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992458098&doi=10.1007%2f978-3-319-48523-2_13&partnerID=40&md5=f19d0e99e6f425085b5f9a078e8333f2,"Unidad de Posgrado de la Facultad de IngenierÌ_a de Sistemas e InformÌÁtica, Universidad Nacional Mayor de San Marcos (UNMSM), Av. GermÌÁn AmÌ©zaga s/n, Lima, Peru","Huanca, L.M., Unidad de Posgrado de la Facultad de IngenierÌ_a de Sistemas e InformÌÁtica, Universidad Nacional Mayor de San Marcos (UNMSM), Av. GermÌÁn AmÌ©zaga s/n, Lima, Peru; OrÌ©, S.B., Unidad de Posgrado de la Facultad de IngenierÌ_a de Sistemas e InformÌÁtica, Universidad Nacional Mayor de San Marcos (UNMSM), Av. GermÌÁn AmÌ©zaga s/n, Lima, Peru","The success of a software development project depends on that the retrieved product complies with the user‰Ûªs specifications and to be completed within the time and within the budget established. Many projects fail when they are not being developed within the time set due to a bad assessment of the effort or duration of the software project. In this article is presented the results of a literature review about the factors that affect the precision of the use case points method. A total of 37 primarily studies were selected. The results show that the environmental factors, the use cases complexity, the lack of use case standardization, technical factors and counting transactions are some factors that affect the use of the use case points method. å© Springer International Publishing AG 2017.",Effort estimation; Software development projects; Use case points; Use cases,Conference Paper,Scopus,2-s2.0-84992458098
"Hihn J., Menzies T.",Data mining methods and cost estimation models: Why is it so hard to infuse new ideas?,2015,"Proceedings - 2015 30th IEEE/ACM International Conference on Automated Software Engineering Workshops, ASEW 2015",,,7426628,5,9,yes,,,,10.1109/ASEW.2015.27,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964489773&doi=10.1109%2fASEW.2015.27&partnerID=40&md5=a73ec9e42ab76b5327daf5c9f4681eb0,"Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA, United States; Department of Computer Science, North Carolina State University, Raleigh, NC, United States","Hihn, J., Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA, United States; Menzies, T., Department of Computer Science, North Carolina State University, Raleigh, NC, United States","Infusing new technologies and methods is hard and can often be described as ""banging ones head on a brick wall"". The is especially true when trying to get project managers, systems, engineers and cost analysts to add a radically new tool to their tool box. In this paper we suggest that the underlying causes are rooted in the fact that the different players have fundamental differences in mental models, vocabulary and objectives. We based this work on lessons learned from ten years of working on the infusion of software costing models into NASA. The good news is that, lately, a crack has begun to appear in what was previously a brick wall. å© 2015 IEEE.",cost estimation; data mining; effort estimation; software; technology infusion,Conference Paper,Scopus,2-s2.0-84964489773
"Gupta P., Arora I., Saha A.",A review of applications of search based software engineering techniques in last decade,2016,"2016 5th International Conference on Reliability, Infocom Technologies and Optimization, ICRITO 2016: Trends and Future Directions",,,7785022,584,589,yes,,,,10.1109/ICRITO.2016.7785022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010572543&doi=10.1109%2fICRITO.2016.7785022&partnerID=40&md5=07f886b93f1e6029848e5ffeaeac1e9f,"Department of Computer Science and Engineering, Northern India Engineering College, New Delhi, India; University School of Information and Communication Technology, Guru Gobind Singh Indraprastha University, Sector-16C, Delhi, Dwarka, India","Gupta, P., Department of Computer Science and Engineering, Northern India Engineering College, New Delhi, India; Arora, I., University School of Information and Communication Technology, Guru Gobind Singh Indraprastha University, Sector-16C, Delhi, Dwarka, India; Saha, A., University School of Information and Communication Technology, Guru Gobind Singh Indraprastha University, Sector-16C, Delhi, Dwarka, India","Optimization and testing in software engineering using traditional techniques has become a tedious task now-a-days. In order to fasten this process, search based-software engineering (SBSE) techniques are introduced to solve real world large scale problems efficiently. Also, previous researches have shown a significant contribution of SBSE techniques in the domain of software testing, reliability, cost and effort estimation. This work reviews the application of five major SBSE methods and the issues solved by different practitioners in the last decade. It also addresses the problems in the implementation of these techniques which are still open for further research. å© 2016 IEEE.",Metaheuristics; Search Based Software Engineering; Software Engineering; Software Quality; Software Testing,Conference Paper,Scopus,2-s2.0-85010572543
"Gonzalez-Ladron-de-Guevara F., Fernandez-Diego M., Lokan C.",The usage of ISBSG data fields in software effort estimation: A systematic mapping study,2016,Journal of Systems and Software,113,,,188,215,yes,,,,10.1016/j.jss.2015.11.040,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962343712&doi=10.1016%2fj.jss.2015.11.040&partnerID=40&md5=2e17a7b891f2c30d95c2fe597c7a526d,"Department of Business Organisation, Universitat PolitÌ¬cnica de ValÌ¬ncia, Camino de Vera, s/n, Valencia, Spain; School of Engineering and Information Technology, UNSW Canberra, Northcott Drive, Canberra, ACT, Australia","GonzÌÁlez-LadrÌ_n-de-Guevara, F., Department of Business Organisation, Universitat PolitÌ¬cnica de ValÌ¬ncia, Camino de Vera, s/n, Valencia, Spain; FernÌÁndez-Diego, M., Department of Business Organisation, Universitat PolitÌ¬cnica de ValÌ¬ncia, Camino de Vera, s/n, Valencia, Spain; Lokan, C., School of Engineering and Information Technology, UNSW Canberra, Northcott Drive, Canberra, ACT, Australia","The International Software Benchmarking Standards Group (ISBSG) maintains a repository of data about completed software projects. A common use of the ISBSG dataset is to investigate models to estimate a software project's size, effort, duration, and cost. The aim of this paper is to determine which and to what extent variables in the ISBSG dataset have been used in software engineering to build effort estimation models. For that purpose a systematic mapping study was applied to 107 research papers, obtained after a filtering process, that were published from 2000 until the end of 2013, and which listed the independent variables used in the effort estimation models. The usage of ISBSG variables for filtering, as dependent variables, and as independent variables is described. The 20 variables (out of 71) mostly used as independent variables for effort estimation are identified and analysed in detail, with reference to the papers and types of estimation methods that used them. We propose guidelines that can help researchers make informed decisions about which ISBSG variables to select for their effort estimation models. å© 2015 Elsevier Inc. All rights reserved.",ISBSG data field; Software effort estimation; Systematic mapping study,Article,Scopus,2-s2.0-84962343712
"Girotra S., Sharma K.",Tuning of software cost drivers using BAT algorithm,2016,"Proceedings of the 10th INDIACom; 2016 3rd International Conference on Computing for Sustainable Global Development, INDIACom 2016",,,7724422,1051,1056,yes,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997498690&partnerID=40&md5=3a3acae38dd30147002c641ad1d06b0b,"Department of Computer Engineering, Delhi Technological University, Delhi, India","Girotra, S., Department of Computer Engineering, Delhi Technological University, Delhi, India; Sharma, K., Department of Computer Engineering, Delhi Technological University, Delhi, India","Software effort estimation is an important process in software development as it predicts the no. of resources required to build the project. Lack of its accuracy and precision will impact timely delivery of project and its budget. There are numerous methods to estimate software effort. COCOMO II has been employed because of its wide acceptance as an industry standard and its applicability at diverse stages of software Engineering. And Accuracy in COCOMO II software effort estimation is highly dependent on its input parameters such as size of project, coefficients and cost drivers. Thus small changes in these parameters bring huge differences in Effort estimation. In this paper, we used Nature Inspired-Meta Heuristic Bat Algorithm to fine tune values of 15 cost drivers, which can effectively reduce error (MMRE) in effort estimation. It has been validated using NASA 93 dataset and its results are found better than COCOMO and genetic algorithm. å© 2016 IEEE.",Bat Algorithm; COCOMO II; Cost Drivers; MMRE; MRE; NASA 93 Dataset; Software Effort Estimation,Conference Paper,Scopus,2-s2.0-84997498690
"Gabrani G., Saini N.",Effort estimation models using evolutionary learning algorithms for software development,2016,"2016 Symposium on Colossal Data Analysis and Networking, CDAN 2016",,,7570916,,,yes,,,,10.1109/CDAN.2016.7570916,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992202478&doi=10.1109%2fCDAN.2016.7570916&partnerID=40&md5=1e2a0ece44e37d28a1cee865d3e73867,"Department of Computer Engineering, BML Munjal University, Gurgaon, India","Gabrani, G., Department of Computer Engineering, BML Munjal University, Gurgaon, India; Saini, N., Department of Computer Engineering, BML Munjal University, Gurgaon, India","Software effort estimation is a complicated task being carried out by software developers as very little information is available to them in the early phases of software development. The information collected about various attributes of software needs to be subjective, which otherwise can lead to uncertainty. Inaccurate software effort estimation can be disastrous as both underestimation and overestimation may result in schedule overruns and incorrect estimation of budget. This paper focuses on the comparative study of various non-algorithmic techniques used for estimating the software effort by empirical evaluation of five different evolutionary learning algorithms. The accuracy of these algorithms is found out and the behavior of these algorithms is analyzed with respect to the size and the type of data. All the five techniques are applied on three different datasets and various paramenters such as MMRE, PRED(25), PRED(50), PRED(75) are calculated. The proposed results are compared to other machine learning methods like SVR, ANFIS etc. The results show that evolutionary learning algorithms give more accurate results than machine learning algorithms. å© 2016 IEEE.",Evolutionary Learning Algorithms; Machine Learning Algorithms; Mean Magnitude of the Relative Error MMRE; Percentage of Predictions(x) PRED(x); Software Effort Estimation,Conference Paper,Scopus,2-s2.0-84992202478
"Ferreira-Santiago A., LÌ_pez-MartÌ_n C., YÌÁÌ±ez-MÌÁrquez C.",Metaheuristic optimization of multivariate adaptive regression splines for predicting the schedule of software projects,2016,Neural Computing and Applications,27,8,,2229,2240,yes,,,,10.1007/s00521-015-2003-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939493349&doi=10.1007%2fs00521-015-2003-z&partnerID=40&md5=3599f441378fadc23d19d5a27432a25b,"Laboratorio de Redes Neuronales y CÌ_mputo no Convencional, Instituto PolitÌ©cnico Nacional, Centro de InvestigaciÌ_n en ComputaciÌ_n, Mexico City, Mexico; Department of Information Systems, Universidad de Guadalajara, Guadalajara, Mexico","Ferreira-Santiago, A., Laboratorio de Redes Neuronales y CÌ_mputo no Convencional, Instituto PolitÌ©cnico Nacional, Centro de InvestigaciÌ_n en ComputaciÌ_n, Mexico City, Mexico; LÌ_pez-MartÌ_n, C., Department of Information Systems, Universidad de Guadalajara, Guadalajara, Mexico; YÌÁÌ±ez-MÌÁrquez, C., Laboratorio de Redes Neuronales y CÌ_mputo no Convencional, Instituto PolitÌ©cnico Nacional, Centro de InvestigaciÌ_n en ComputaciÌ_n, Mexico City, Mexico","A qualitative common perception of the software industry is that it finishes its projects late and over budget, whereas from a quantitative point of view, only 39åÊ% of software projects are finished on time compared to the schedule when the project started. This low percentage has been attributed to factors such as unrealistic time frames and lack of planning regarding poor prediction. The main techniques used for predicting project schedule have mainly been based on expert judgment and mathematical models. In this study, a new model, derived from the multivariate adaptive regression splines (MARS) model, is proposed. This new model, optimized MARS (OMARS), uses a simulated annealing process to find a transformation of the input data space prior to applying MARS in order to improve accuracy when predicting the schedule of software projects. The prediction accuracy of the OMARS model is compared to that of stand-alone MARS and a multiple linear regression (MLR) model with a logarithmic transformation. The two independent variables used for training and testing the models are functional size, which corresponds to a composite value of 19 independent variables, and the maximum size of the team of developers. The data set of projects was obtained from the International Software Benchmarking Standards Group (ISBSG) Release 11. Results based on the absolute residuals and t paired and Wilcoxon statistical tests showed that prediction accuracy with OMARS is statistically better than that with the MARS and MLR models. å© 2015, The Natural Computing Applications Forum.",ISBSG; Multivariate adaptive regression splines; Simulated annealing; Software project schedule prediction; Statistical regression,Article,Scopus,2-s2.0-84939493349
"Dolado J.J., Rodriguez D., Harman M., Langdon W.B., Sarro F.",Evaluation of estimation models using the Minimum Interval of Equivalence,2016,Applied Soft Computing Journal,49,,,956,967,yes,,,,10.1016/j.asoc.2016.03.026,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964595263&doi=10.1016%2fj.asoc.2016.03.026&partnerID=40&md5=d8cf7b0b03d186ac65967a33b9a5706d,"Facultad de InformÌÁtica, UPV/EHU, University of the Basque Country, Spain; Dept. of Computer Science, University of AlcalÌÁ, Spain; CREST, University College London, United Kingdom","Dolado, J.J., Facultad de InformÌÁtica, UPV/EHU, University of the Basque Country, Spain; Rodriguez, D., Dept. of Computer Science, University of AlcalÌÁ, Spain; Harman, M., CREST, University College London, United Kingdom; Langdon, W.B., CREST, University College London, United Kingdom; Sarro, F., CREST, University College London, United Kingdom","This article proposes a new measure to compare soft computing methods for software estimation. This new measure is based on the concepts of Equivalence Hypothesis Testing (EHT). Using the ideas of EHT, a dimensionless measure is defined using the Minimum Interval of Equivalence and a random estimation. The dimensionless nature of the metric allows us to compare methods independently of the data samples used. The motivation of the current proposal comes from the biases that other criteria show when applied to the comparison of software estimation methods. In this work, the level of error for comparing the equivalence of methods is set using EHT. Several soft computing methods are compared, including genetic programming, neural networks, regression and model trees, linear regression (ordinary and least mean squares) and instance-based methods. The experimental work has been performed on several publicly available datasets. Given a dataset and an estimation method we compute the upper point of Minimum Interval of Equivalence, MIEu, on the confidence intervals of the errors. Afterwards, the new measure, MIEratio, is calculated as the relative distance of the MIEu to the random estimation. Finally, the data distributions of the MIEratios are analysed by means of probability intervals, showing the viability of this approach. In this experimental work, it can be observed that there is an advantage for the genetic programming and linear regression methods by comparing the values of the intervals. å© 2016 Elsevier B.V.",Bootstrap; Credible intervals; Equivalence Hypothesis Testing; Soft computing; Software estimations,Article,Scopus,2-s2.0-84964595263
"Di Martino S., Ferrucci F., Gravino C., Sarro F.",Web Effort Estimation: Function Point Analysis vs. COSMIC,2016,Information and Software Technology,72,,,90,109,yes,,,2,10.1016/j.infsof.2015.12.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958604486&doi=10.1016%2fj.infsof.2015.12.001&partnerID=40&md5=51ffaf3fc1e230c41b4174c3960e725b,"Dipartimento di Ingegneria Elettrica e Delle Tecnologie dell'Informazione, University of Napoli Federico II, Italy; Department of Computer Science, University of Salerno, Italy; CREST, Department of Computer Science, University College London, United Kingdom","Di Martino, S., Dipartimento di Ingegneria Elettrica e Delle Tecnologie dell'Informazione, University of Napoli Federico II, Italy; Ferrucci, F., Department of Computer Science, University of Salerno, Italy; Gravino, C., Department of Computer Science, University of Salerno, Italy; Sarro, F., CREST, Department of Computer Science, University College London, United Kingdom","Context: software development effort estimation is a crucial management task that critically depends on the adopted size measure. Several Functional Size Measurement (FSM) methods have been proposed. COSMIC is considered a 2nd generation FSM method, to differentiate it from Function Point Analysis (FPA) and its variants, considered as 1st generation ones. In the context of Web applications, few investigations have been performed to compare the effectiveness of the two generations. Software companies could benefit from this analysis to evaluate if it is worth to migrate from a 1st generation method to a 2nd one. Objective: the main goal of the paper is to empirically investigate if COSMIC is more effective than FPA for Web effort estimation. Since software companies using FPA cannot build an estimation model based on COSMIC as long as they do not have enough COSMIC data, the second goal of the paper is to investigate if conversion equations can be exploited to support the migration from FPA to COSMIC. Method: two empirical studies have been carried out by employing an industrial data set. The first one compared the effort prediction accuracy obtained with Function Points (FPs) and COSMIC, using two estimation techniques (Simple Linear Regression and Case-Based Reasoning). The second study assessed the effectiveness of a two-step strategy that first exploits a conversion equation to transform historical FPs data into COSMIC, and then builds a new prediction model based on those estimated COSMIC sizes. Results: the first study revealed that, on our data set, COSMIC was significantly more accurate than FPs in estimating the development effort. The second study revealed that the effectiveness of the analyzed two-step process critically depends on the employed conversion equation. Conclusion: for Web effort estimation COSMIC can be significantly more effective than FPA. Nevertheless, additional research must be conducted to identify suitable conversion equations so that the two-step strategy can be effectively employed for a smooth migration from FPA to COSMIC. å© 2015 Elsevier B.V. All rights reserved.",COSMIC; Functional Size Measures; IFPUG Function Point Analysis; Web Effort Estimation,Article,Scopus,2-s2.0-84958604486
"Del Refugio Ofelia Luna Sandoval M., Ascencio J.R.",MUREM: A multiplicative regression method for software development effort estimation [MUREM: Un mÌ©todo multiplicativo de regresiÌ_n para estimar el esfuerzo de desarrollo de software],2016,Computacion y Sistemas,20,4,,763,787,yes,,,,10.13053/CyS-20-4-2378,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007335675&doi=10.13053%2fCyS-20-4-2378&partnerID=40&md5=d4e459d6d0b4ecc030e2d8a66c9ee99c,"CENIDET, Departamento de Ciencias Computacionales, Cuernavaca, Mexico","Del Refugio Ofelia Luna Sandoval, M., CENIDET, Departamento de Ciencias Computacionales, Cuernavaca, Mexico; Ascencio, J.R., CENIDET, Departamento de Ciencias Computacionales, Cuernavaca, Mexico","In this paper a multiplicative regression method to estimate software development effort is presented. This method, which we call MUREM, is a result of, on the one hand, a set of initial conditions to frame the process of estimating software development effort and, on the other hand, a set of restrictions to be satisfied by the development effort as a function of software size. To evaluate the performance of MUREM, it was compared with three regression models which are considered as important methods for estimating software development effort. In this comparison a battery of hypothesis and standard statistical tests is applied to twelve samples taken from well-known public databases. These databases serve as benchmarks for comparing methods to estimate the software development effort. In the experimentation it was found that MUREM generates more accurate point estimates of the development effort than those achieved by the other methods. MUREM corrects the heteroscedasticity and increases the proportion of samples whose residuals show normality. MUREM thus generates more appropriate confidence and prediction intervals than those obtained by the other methods. An important result is that residuals obtained by the regression model of MUREM satisfy the test for zero mean additive white gaussian noise which is proof that the estimation error of this model is random.",Estimating method; Method for estimating software development effort; Multiplicative method; Regression model; Software development effort estimation,Article,Scopus,2-s2.0-85007335675
"Daramola O., Ajala I., Akinyemi I.",An experimental comparison of three machine learning techniques for web cost estimation,2016,International Journal of Software Engineering and its Applications,10,2,,191,206,yes,,,,10.14257/ijseia.2016.10.2.16,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960455414&doi=10.14257%2fijseia.2016.10.2.16&partnerID=40&md5=6b7274c7588264db521949fcbea88ad2,"Department of Computer and Information Sciences, Covenant University, Ota, Nigeria","Daramola, O., Department of Computer and Information Sciences, Covenant University, Ota, Nigeria; Ajala, I., Department of Computer and Information Sciences, Covenant University, Ota, Nigeria; Akinyemi, I., Department of Computer and Information Sciences, Covenant University, Ota, Nigeria","Many comparative studies on the performance of machine learning (ML) techniques for web cost estimation (WCE) have been reported in the literature. However, not much attention have been given to understanding the conceptual differences and similarities that exist in the application of these ML techniques for WCE, which could provide credible guide for upcoming practitioners and researchers in predicting the cost of new web projects. This paper presents a comparative analysis of three prominent machine learning Techniques-Case-Based Reasoning (CBR), Support Vector Regression (SVR) and Artificial Neural Network (ANN)-In terms of performance, applicability, and their conceptual differences and similarities for WCE by using data obtained from a public dataset (www.tukutuku.com). Results from experiments show that SVR and ANN provides more accurate predictions of effort, although SVR require fewer parameters to generate good predictions than ANN. CBR was not as accurate, but its good explanation attribute gives it a higher descriptive value. The study also outlined specific characteristics of the 3 ML techniques that could foster or inhibit their adoption for WCE. å© 2016 SERSC.",Artificial neural networks; Case based reasoning; Machine learning; Support vector regression; Web cost estimation,Article,Scopus,2-s2.0-84960455414
"Chinthanet B., Leelaprute P., Rungsawang A., Phannachitta P., Ubayashi N., Kamei Y., Matsumoto K.",A review and comparison of methods for determining the best analogies in analogy-based software effort estimation,2016,Proceedings of the ACM Symposium on Applied Computing,04-08-April-2016,,,1554,1557,yes,,,,10.1145/2851613.2851974,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975883812&doi=10.1145%2f2851613.2851974&partnerID=40&md5=c77b6bb11aa9096e9e6ffd99cb606d5e,"Faculty of Engineering, Kasetsart University, Thailand; Nara Institute of Science and Technology, Japan; Kyushu University, Japan","Chinthanet, B., Faculty of Engineering, Kasetsart University, Thailand; Leelaprute, P., Faculty of Engineering, Kasetsart University, Thailand; Rungsawang, A., Faculty of Engineering, Kasetsart University, Thailand; Phannachitta, P., Nara Institute of Science and Technology, Japan; Ubayashi, N., Kyushu University, Japan; Kamei, Y., Kyushu University, Japan; Matsumoto, K., Nara Institute of Science and Technology, Japan","Analogy-based effort estimation (ABE) is a commonly used software development effort estimation method. The processes of ABE are based on a reuse of effort values from similar past projects, where the appropriate numbers of past projects (k values) to be reused is one of the long-standing debates in ABE research studies. To date, many approaches to find this k value have been continually proposed. One important reason for this inconclusive debate is that different studies appear to produce different conclusions of the k value to be appropriate. Therefore, in this study, we revisit 8 common approaches to the k value being most appropriate in general situations. With a more robust and comprehensive evaluation methodology using 5 robust error measures subject to the Wilcoxon rank-sum statistical test, we found that conflicting results in the previous studies were not mainly due to the use of different methodologies nor different datasets, but the performance of the different approaches are actually varied widely. å© 2016 ACM.",Analogy-based effort estimation; Dynamic-k; Evaluation criteria; Fixed-k; Software effort estimation,Conference Paper,Scopus,2-s2.0-84975883812
"Britto R., Mendes E., Wohlin C.",A specialized global software engineering taxonomy for effort estimation,2016,"Proceedings - 11th IEEE International Conference on Global Software Engineering, ICGSE 2016",,,7577433,154,163,yes,,,,10.1109/ICGSE.2016.11,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994579892&doi=10.1109%2fICGSE.2016.11&partnerID=40&md5=ea00c3eefbda33c57fa98f6b8eec7d5b,"Department of Software Engineering, Blekinge Institute of Technology, Sweden; Department of Computer Science and Engineering, Blekinge Institute of Technology, Sweden","Britto, R., Department of Software Engineering, Blekinge Institute of Technology, Sweden; Mendes, E., Department of Computer Science and Engineering, Blekinge Institute of Technology, Sweden; Wohlin, C., Department of Software Engineering, Blekinge Institute of Technology, Sweden","To facilitate the sharing and combination of knowledge by Global Software Engineering (GSE) researchers and practitioners, the need for a common terminology and knowledge classification scheme has been identified, and as a consequence, a taxonomy and an extension were proposed. In addition, one systematic literature review and a survey on respectively the state of the art and practice of effort estimation in GSE were conducted, showing that despite its importance in practice, the GSE effort estimation literature is rare and reported in an ad-hoc way. Therefore, this paper proposes a specialized GSE taxonomy for effort estimation, which was built on the recently proposed general GSE taxonomy (including the extension) and was also based on the findings from two empirical studies and expert knowledge. The specialized taxonomy was validated using data from eight finished GSE projects. Our effort estimation taxonomy for GSE can help both researchers and practitioners by supporting the reporting of new GSE effort estimation studies, i.e. new studies are to be easier to identify, compare, aggregate and synthesize. Further, it can also help practitioners by providing them with an initial set of factors that can be considered when estimating effort for GSE projects. å© 2016 IEEE.",Effort Estimation; Global Software Development; Global Software Engineering; Taxonomy,Conference Paper,Scopus,2-s2.0-84994579892
"Bisi M., Goyal N.K.",Software development efforts prediction using artificial neural network,2016,IET Software,10,3,,63,71,yes,,,,10.1049/iet-sen.2015.0061,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973370327&doi=10.1049%2fiet-sen.2015.0061&partnerID=40&md5=c6f436eb89625d4bd62b257e54041960,"Reliability Engineering Centre, Indian Institute of Technology, Kharagpur, India","Bisi, M., Reliability Engineering Centre, Indian Institute of Technology, Kharagpur, India; Goyal, N.K., Reliability Engineering Centre, Indian Institute of Technology, Kharagpur, India","Software project managers need an accurate assessment of software development efforts to achieve reliable software within development budget and schedule. A single layer neural network (SLP) is reported to predict software development efforts from software quality metrics. Particle swarm optimisation for training, principal component analysis (PCA) for dimension reduction of input features and genetic algorithm for optimising artificial neural network architecture are used. Literature reported datasets are tested and the results are acceptable within the limits. However, SLP-NN without pre-processing with PCA is adequate and in some cases, reduction approach may be dropped. å© The Institution of Engineering and Technology 2016.",,Article,Scopus,2-s2.0-84973370327
"Bishnu P.S., Bhattacherjee V.",Software cost estimation based on modified K-Modes clustering Algorithm,2016,Natural Computing,15,3,,415,422,yes,,,,10.1007/s11047-015-9492-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922567132&doi=10.1007%2fs11047-015-9492-7&partnerID=40&md5=2819c4e5856f138a5b439829a6fb82dc,"Birla Institute of Technology, Ranchi, India","Bishnu, P.S., Birla Institute of Technology, Ranchi, India; Bhattacherjee, V., Birla Institute of Technology, Ranchi, India","Unsupervised technique like clustering may be used for software cost estimation in situations where parametric models are difficult to develop. This paper presents a software cost estimation model based on a modified K-Modes clustering algorithm. The aims of this paper are: first, the modified K-Modes clustering which is an enhancement over the simple K-Modes algorithm using a proper dissimilarity measure for mixed data types, is presented and second, the proposed K-Modes algorithm is applied for software cost estimation. We have compared our modified K-Modes algorithm with existing algorithms on different software cost estimation datasets, and results showed the effectiveness of our proposed algorithm. å© 2015, Springer Science+Business Media Dordrecht.",Clustering; Data mining; K-Modes clustering; Software cost estimation,Article,Scopus,2-s2.0-84922567132
"Bhatia P., Mishra K.K., Misra A.K.",An approach to software cost estimation by improved - Time variant acceleration coefficient based PSO,2016,Journal of Multiple-Valued Logic and Soft Computing,27,1,,63,74,yes,,,1,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973443928&partnerID=40&md5=bffbcadc2001b0868ede1f369f9ae810,"Computer Science and Engineering Department, Motilal Nehru National Institute of Technology, Allahabad, India","Bhatia, P., Computer Science and Engineering Department, Motilal Nehru National Institute of Technology, Allahabad, India; Mishra, K.K., Computer Science and Engineering Department, Motilal Nehru National Institute of Technology, Allahabad, India; Misra, A.K., Computer Science and Engineering Department, Motilal Nehru National Institute of Technology, Allahabad, India","Software Cost estimation plays an important role in its development. A software cost (effort) estimation error can ruin any software. Number of models already exist that defines relationship between software size (input) and effort (output); however still these models fails to estimate the cost of software because of uncertainties, and imprecision associated with it. In this paper the parameters of existing cost estimation model (COCOMO) are tuned using improved Time Varying Acceleration coefficient based PSO and is tested on 10 NASA projects. The experimental results prove that the parameter tuned cost estimation model (COCOMO) has better estimation capabilities as compared to other existing cost estimations models. å© 2016 Old City Publishing, Inc.",KLOC-kilo lines of codes; Parameter tuning; Particle swarm optimization; PM-person month; Software cost estimation; Time-variant acceleration coefficient,Article,Scopus,2-s2.0-84973443928
Bhandari S.,FCM based conceptual framework for software effort estimation,2016,"Proceedings of the 10th INDIACom; 2016 3rd International Conference on Computing for Sustainable Global Development, INDIACom 2016",,,7724729,2584,2588,yes,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997124328&partnerID=40&md5=4595a6479a6ee73e7413dd1cccca2aab,"HMV, Jalandhar, India","Bhandari, S., HMV, Jalandhar, India","Software project management is tedious due to inherent complexities in the various tasks involved in it. Success of a project greatly depends on accurate estimates of cost and time it will take, which is hard to determine at the initial stages. Many models have been developed over the period of time for estimating development effort and cost. The initial effort estimate is basically based on the human judgment so different approaches based on rule induction, fuzzy systems, regression trees, genetic algorithms, Bayesian networks, artificial neural networks, and evolutionary computation have been evolved. COCOMO is well accepted model in industry. COCOMO provides basic factors which are mostly involved in effort estimation calculations. Because of the increasing complexity of domain and high stakes involved there is scope of improvement. In this paper we have proposed representation of factors used in COCOMO as FCM for better software effort estimation. å© 2016 IEEE.",COCOMO; Effort multipliers; FCM; Fuzzy inference system; Software development effort,Conference Paper,Scopus,2-s2.0-84997124328
"Basri S., Kama N., Haneem F., Ismail S.A.",Predicting effort for requirement changes during software development,2016,ACM International Conference Proceeding Series,08-09-December-2016,,,380,387,yes,,,,10.1145/3011077.3011096,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007560310&doi=10.1145%2f3011077.3011096&partnerID=40&md5=71a31a4c59af54efaed1d54ef4f932a8,"Advanced Informatics School, Universiti Teknologi Malaysia, Kuala Lumpur, Malaysia","Basri, S., Advanced Informatics School, Universiti Teknologi Malaysia, Kuala Lumpur, Malaysia; Kama, N., Advanced Informatics School, Universiti Teknologi Malaysia, Kuala Lumpur, Malaysia; Haneem, F., Advanced Informatics School, Universiti Teknologi Malaysia, Kuala Lumpur, Malaysia; Ismail, S.A., Advanced Informatics School, Universiti Teknologi Malaysia, Kuala Lumpur, Malaysia","In any software development life cycle, requirement and software changes are inevitable. One of the factors that influences the effectiveness of the change acceptance decision is the accuracy of the effort prediction for requirement changes. There are two current models that have been widely used to predict rework effort for requirement changes which are algorithmic and nonalgorithmic models. The algorithmic model is known for its formal and structural way of prediction and best suited for Traditional software development methodology. While nonalgorithmic model is widely adopted for Agile software development methodology of software projects due to its easiness and requires less work in term of effort predictability. Nevertheless, none of the existing effort prediction models for requirement changes are proven to suit both, Traditional and Agile software development methodology. Thus, this paper proposes an algorithmic-based effort prediction model for requirement changes that uses change impact analysis method which is applicable for both Traditional and Agile software development methodologies. The proposed model uses a current selected change impact analysis method for software development phase. The proposed model is evaluated through an extensive experimental validation using case study of six real Traditional and Agile methodologies software projects. The evaluation results confirmed a significance accuracy improvement of the proposed model over the existing approaches for both Traditional and Agile methodologies. å© 2016 ACM.",Change Effort Estimation; Change Impact Analysis; Effort Estimation; Impact Analysis,Conference Paper,Scopus,2-s2.0-85007560310
"Azzeh M., Nassif A.B., Banitaan S., Almasalha F.",Pareto efficient multi-objective optimization for local tuning of analogy-based estimation,2016,Neural Computing and Applications,27,8,,2241,2265,yes,,,,10.1007/s00521-015-2004-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940487623&doi=10.1007%2fs00521-015-2004-y&partnerID=40&md5=84e5ffe23e383b9c88baa4ec4ec921ca,"Department of Software Engineering, Applied Science University, POBOX 166, Amman, Jordan; Department of Electrical and Computer Engineering, University of Sharjah, Sharjah, United Arab Emirates; Department of Mathematics, Computer Science and Software Engineering, University of Detroit Mercy, Detroit, MI, United States; Department of Computer Science, Applied Science University, POBOX 166, Amman, Jordan","Azzeh, M., Department of Software Engineering, Applied Science University, POBOX 166, Amman, Jordan; Nassif, A.B., Department of Electrical and Computer Engineering, University of Sharjah, Sharjah, United Arab Emirates; Banitaan, S., Department of Mathematics, Computer Science and Software Engineering, University of Detroit Mercy, Detroit, MI, United States; Almasalha, F., Department of Computer Science, Applied Science University, POBOX 166, Amman, Jordan","Analogy-based effort estimation (ABE) is one of the prominent methods for software effort estimation. The fundamental concept of ABE is closer to the mentality of expert estimation but with an automated procedure in which the final estimate is generated by reusing similar historical projects. The main key issue when using ABE is how to adapt the effort of the retrieved nearest neighbors. The adaptation process is an essential part of ABE to generate more successful accurate estimation based on tuning the selected raw solutions, using some adaptation strategy. In this study, we show that there are three interrelated decision variables that have great impact on the success of adaptation method: (1) number of nearest analogies (k), (2) optimum feature set needed for adaptation and (3) adaptation weights. To find the right decision regarding these variables, one need to study all possible combinations and evaluate them individually to select the one that can improve all prediction evaluation measures. The existing evaluation measures usually behave differently, presenting sometimes opposite trends in evaluating prediction methods. This means that changing one decision variable could improve one evaluation measure while it is decreasing the others. Therefore, the main theme of this research is how to come up with best decision variables that improve adaptation strategy and thus the overall evaluation measures without degrading the others. The impact of these decisions together has not been investigated before; therefore, we propose to view the building of adaptation procedure as a multi-objective optimization problem. The Particle swarm optimization algorithm (PSO) is utilized to find the optimum solutions for such decision variables based on optimizing multiple evaluation measures. We evaluated the proposed approaches over 15 datasets and using four evaluation measures. After extensive experimentation, we found that: (1) predictive performance of ABE has noticeably been improved, (2) optimizing all decision variables together is more efficient than ignoring any one of them, and (3) optimizing decision variables for each project individually yields better accuracy than optimizing them for the whole dataset. å© 2015, The Natural Computing Applications Forum.",Adaptation strategy; Analogy-based effort estimation; Multi-objective optimization; Particle swarm optimization,Article,Scopus,2-s2.0-84940487623
"Azzeh M., Nassif A.B.",A hybrid model for estimating software project effort from Use Case Points,2016,Applied Soft Computing Journal,49,,,981,989,yes,,,2,10.1016/j.asoc.2016.05.008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966783554&doi=10.1016%2fj.asoc.2016.05.008&partnerID=40&md5=1f6b6d23c4cc8e5e27e7753315b86803,"Department of Software Engineering, Applied Science University, P.O. BOX 166, Amman, Jordan; Department of Electrical and Computer Engineering, University of SharjahSharjah, United Arab Emirates","Azzeh, M., Department of Software Engineering, Applied Science University, P.O. BOX 166, Amman, Jordan; Nassif, A.B., Department of Electrical and Computer Engineering, University of SharjahSharjah, United Arab Emirates","Early software effort estimation is a hallmark of successful software project management. Building a reliable effort estimation model usually requires historical data. Unfortunately, since the information available at early stages of software development is scarce, it is recommended to use software size metrics as key cost factor of effort estimation. Use Case Points (UCP) is a prominent size measure designed mainly for object-oriented projects. Nevertheless, there are no established models that can translate UCP into its corresponding effort; therefore, most models use productivity as a second cost driver. The productivity in those models is usually guessed by experts and does not depend on historical data, which makes it subject to uncertainty. Thus, these models were not well examined using a large number of historical data. In this paper, we designed a hybrid model that consists of classification and prediction stages using a support vector machine and radial basis neural networks. The proposed model was constructed over a large number of observations collected from industrial and student projects. The proposed model was compared against previous UCP prediction models. The validation and empirical results demonstrated that the proposed model significantly surpasses these models on all datasets. The main conclusion is that the environmental factors of UCP can be used to classify and estimate productivity. å© 2016 Elsevier B.V.",Effort estimation; Radial basis neural networks; Support vector machine; Use Case Points,Article,Scopus,2-s2.0-84966783554
"Amasaki S., Lokan C.",A replication study on the effects of weighted moving windows for software effort estimation,2016,ACM International Conference Proceeding Series,01-03-June-2016,, a40,,,yes,,,,10.1145/2915970.2915983,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978543424&doi=10.1145%2f2915970.2915983&partnerID=40&md5=cef99af6d3eb24329ad55caf69e3617a,"Okayama Prefectural University, 111 Kuboki, Soja, Okayama, Japan; UNSW Canberra, School of Engineering and Information Technology, Canberra, ACT, Australia","Amasaki, S., Okayama Prefectural University, 111 Kuboki, Soja, Okayama, Japan; Lokan, C., UNSW Canberra, School of Engineering and Information Technology, Canberra, ACT, Australia","Context: Recent studies have shown that estimation accuracy can be affected by only using a window of recent projects as training data for building an effort estimation model. The idea has been extended for regression-based estimation by weighting projects differently according to their order within the window. This significantly improved the accuracy of estimation in a single-company dataset from the ISBSG repository. Objective: To investigate the effects on estimation accuracy of using weighted moving windows with a new dataset, and compare results across datasets. Method: Using a dataset drawn from the Finnish dataset (studied previously with regard to windows but not with weighting), and using a fixed-size window policy, we examine the effect on estimation accuracy of using weighted moving windows. Results: The use of weighting functions could improve the estimation accuracy significantly, compared to using unweighted windows, with larger window sizes. The steepness of the weighting functions affects their effectiveness. However, in this dataset it is better to use a growing portfolio (retaining all past projects as training data) than to use windows. Conclusions: The results reinforce previous studies: the use of weighting functions can significantly improve the accuracy of regression-based estimation, compared to not using weighting, but in this dataset the use of moving windows reduces estimation accuracy. å© 2016 ACM.",Model-based effort estimation; Moving windows; Weighted functions,Conference Paper,Scopus,2-s2.0-84978543424
"Amasaki S., Lokan C.",Evaluation of Moving Window Policies with CART,2016,"Proceedings - 7th International Workshop on Empirical Software Engineering in Practice, IWESEP 2016",,,7464548,24,29,yes,,,,10.1109/IWESEP.2016.10,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971451915&doi=10.1109%2fIWESEP.2016.10&partnerID=40&md5=0f9c960a824b32646c62613d11375138,"Department of Systems Engineering, Okayama Prefectural University, Soja, Okayama, Japan; School of Engineering and Information Technology, UNSW Canberra, Canberra, ACT, Australia","Amasaki, S., Department of Systems Engineering, Okayama Prefectural University, Soja, Okayama, Japan; Lokan, C., School of Engineering and Information Technology, UNSW Canberra, Canberra, ACT, Australia","CONTEXT: Recent studies have shown that estimation accuracy can be affected by only using a window of recent projects (instead of all past projects) as training data for building an effort estimation model. The effect and its extent can be affected by the effort estimation methods used, and the windowing policy used (fixed size or fixed duration). The generality of the windowing approach remains uncertain, because only a few effort estimation methods have been examined with each policy. OBJECTIVE: To investigate the effect on estimation accuracy of using the fixed-duration window policy, particularly in comparison to the fixed-size window policy, when using Classification and Regression Trees (CART) as the estimation method. METHOD: Using a single-company ISBSG data set studied previously in similar research, we examine the effects of using a fixed-duration windowing policy on the accuracy of estimates using CART. RESULTS: Fixed-duration windows rarely improve the accuracy of estimates with CART, compared to using all past projects as training data. Few window sizes lead to statistically significant differences. The effect is smaller than when fixed-size windows are used. CONCLUSIONS: Fixed-duration windows are not helpful with this data set when using CART as the estimation method. The results support the preference for the fixed-size window policy that was found in previous research. This contributes further to understanding the effect of using windows. å© 2016 IEEE.",CART; fixed-duration; moving windows,Conference Paper,Scopus,2-s2.0-84971451915
"Alshalif S.A., Ibrahim N., Herawan T.",Artificial neural network with hyperbolic tangent activation function to improve the accuracy of COCOMO II model,2017,Advances in Intelligent Systems and Computing,549 AISC,,,81,90,yes,,,,10.1007/978-3-319-51281-5_9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009788884&doi=10.1007%2f978-3-319-51281-5_9&partnerID=40&md5=8f31036fa3f6d89088ced83192f38a40,"Universiti Tun Hussein Onn Malaysia, BatuPahat, Parit Raja, Johor, Malaysia; University of Malaya, Pantai Valley, Kuala Lumpur, Malaysia","Alshalif, S.A., Universiti Tun Hussein Onn Malaysia, BatuPahat, Parit Raja, Johor, Malaysia; Ibrahim, N., Universiti Tun Hussein Onn Malaysia, BatuPahat, Parit Raja, Johor, Malaysia; Herawan, T., University of Malaya, Pantai Valley, Kuala Lumpur, Malaysia","In software engineering, Constructive Cost Model II (COCOMO II) is one of the most cited, famous and widely used model to estimate and predict some important features of the software project such as effort, cost, time and manpower estimations. Lately, researchers incorporate it with soft computing techniques to solve and reduce the ambiguity and uncertainty of its software attributes. In this paper, Artificial Neural Network (ANN) with Hyperbolic Tangent Activation Function is used to improve the accuracy of the COCOMO II model and the backpropagation learning algorithm used in the training process. In the experiment, COCOMO II SDR dataset is used for training and testing the model. The result shows that eight out of twelve projects have a closer effort value of actual effort. It shows that the proposed model produces better performance comparing to sigmodal function. å© Springer International Publishing AG 2017.",Artificial neural Network; Backpropagation algorithm; COCOMO II; Hyperbolic tangent activation function; Software cost estimation,Conference Paper,Scopus,2-s2.0-85009788884
"Aljahdali S., Sheta A.F., Debnath N.C.","Estimating software effort and function point using regression, Support Vector Machine and Artificial Neural Networks models",2016,"Proceedings of IEEE/ACS International Conference on Computer Systems and Applications, AICCSA",2016-July,,7507149,,,yes,,,,10.1109/AICCSA.2015.7507149,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84980328379&doi=10.1109%2fAICCSA.2015.7507149&partnerID=40&md5=a4baafd3b2f43cee0fee8399b6a5e418,"Computer Science Department, Taif University, Saudi Arabia; Computers and Systems Department, Electronics Research Institute, Giza, Egypt; Computer Science Department, Winona State University, Winona, MN, United States","Aljahdali, S., Computer Science Department, Taif University, Saudi Arabia; Sheta, A.F., Computers and Systems Department, Electronics Research Institute, Giza, Egypt; Debnath, N.C., Computer Science Department, Winona State University, Winona, MN, United States","Accurate computation of software effort, cost and time required ahead would greatly reduce risk and maximize profit. Estimating software effort or computing the required function point helps project manager to better estimate the time and budget required for a project. Many statistical models were proposed in the past. These models suffer many problems related to parameter estimation and structure determination of the models. In this paper we presents two models for software effort estimation and one model for function points using Linear Regression (LR), Support Vector Machines (SVM) and Artificial Neural Networks (ANN). The proposed models have number of inputs and single output. The first model utilizes the Source Line Of Code (KLOC) as inputs; while the second model utilize the KLOC and development Methodology (ME) as inputs to estimate the Effort (E); while the third model utilize the Inputs, Outputs, Files, and User Inquiries as inputs to estimate the Function Point (FP). The proposed SVM and ANN models show better estimation capabilities compared to linear regression model models. These models are capable of providing better assistant to software project manager in computing the effort required of the number of function points. å© 2015 IEEE.",,Conference Paper,Scopus,2-s2.0-84980328379
"Agrawal P., Kumar S.",Early phase software effort estimation model,2016,"2016 Symposium on Colossal Data Analysis and Networking, CDAN 2016",,,7570914,,,yes,,,,10.1109/CDAN.2016.7570914,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992189585&doi=10.1109%2fCDAN.2016.7570914&partnerID=40&md5=7820b511e45abcb5318feeae616b8bb2,"CSE Department, Sushila Devi Bansal College of Technology, Indore(M.P.), India","Agrawal, P., CSE Department, Sushila Devi Bansal College of Technology, Indore(M.P.), India; Kumar, S., CSE Department, Sushila Devi Bansal College of Technology, Indore(M.P.), India","Software cost estimation is the measurement of the effort and resource required to develop a software system for a particular defined time period. From the last few years, many software cost estimation models have been proposed for the estimation of effort and development time. The cost of the software can be estimated easily in the mid of project development. But with further study, we found that the cost must be estimated before the start of the project, to satisfy customer as well as developer's needs. In earlier proposed cost estimation models, cost estimation is done with more than 20 parameters at the early conceptual phase and if input is not defined using logical approach, then the results of estimation are unpredictable. This paper presents simple approach for estimating software development effort with a minimum set of parameters yet sufficient, that can be easily identified at an early stage while considering all possible aspects. We also introduced a weight factor in the estimation of effort for improving the accuracy of the estimation and the proposed weight factor is calculated by expert learning system. Further, we developed a web based tool for the estimation of cost based on our proposed approach. Finally, we compare our result with previous works on early estimation and conclude with the points of accuracy that we observe while comparing the results with existing approaches. å© 2016 IEEE.",Cost Models; Early Phase Cost Estimation; Effort Estimation; KSLOC; Project Estimation; SDLC,Conference Paper,Scopus,2-s2.0-84992189585
"Agrawal A., Jain V., Sheikh M.",Quantitative estimation of cost drivers for intermediate COCOMO towards traditional and cloud based software development,2016,ACM International Conference Proceeding Series,21-23-October-2016,,2998488,85,95,yes,,,,10.1145/2998476.2998488,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996564572&doi=10.1145%2f2998476.2998488&partnerID=40&md5=921a83b7f683dc73b25baaed9ee13654,"Dept. of CSE, Medi-Caps University, Indore, M.P., India; Institute of Engineering and Technology, Devi Ahilya Vishwavidyala, Indore, M.P., India","Agrawal, A., Dept. of CSE, Medi-Caps University, Indore, M.P., India; Jain, V., Institute of Engineering and Technology, Devi Ahilya Vishwavidyala, Indore, M.P., India; Sheikh, M., Dept. of CSE, Medi-Caps University, Indore, M.P., India","Software project estimation is the process of analyzing the resource requirements for the given time duration of product development. Cost estimation models are used for calculating the associated amount required for developing the stakeholder's requirement within the defined time boundaries. Among several models available for the cost estimation of software projects, COCOMO is one of the well-known models which serve the field most. Resources applied for the given time will generate the rough estimates, but for more accurate values, various factors are analyzed. These factors are termed as cost drivers. Software estimation using COCOMO is performed by selecting values of cost drivers on a predefined scale. This approach solely depends on experience of a software analyst. However, there is a lack of a systematic approach available for the selection of values of these cost drivers. Our work suggests the quantification of cost drivers for intermediate COCOMO. Quantification will implicitly fetch the values from the system and its environment which reduces the manual selection of ranges of scaling factors. Hence the systems cost will be generated directly without analyst and selector logic. Finally, if the selection of correct scaling is performed, then the calculation of cost will definitely get improved. An experimental analysis is performed between the above suggested model and the Intermediate COCOMO. The results show that the ""COCOMOUP"" is performing well under the known conditions and in uncertain requirements conditions, the system is getting better predictions. å© 2016 ACM.",Cloud based software cost estimation; Cost drivers; Cost estimation; Intermediate COCOMO; KLOC (Kilo Line of Code); Quantification,Conference Paper,Scopus,2-s2.0-84996564572
"Êtrba R., Êtolfa S., Êtolfa J., VondrÌÁk I., SnÌÁÁel V.",An application of neural network in method for use case based effort estimation,2017,Frontiers in Artificial Intelligence and Applications,292,,,231,248,yes,,,,10.3233/978-1-61499-720-7-231,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85002828916&doi=10.3233%2f978-1-61499-720-7-231&partnerID=40&md5=272641aa613751b6110058120d3fe0e9,"Department of Computer Science, VÊB-Technical University of Ostrava, Faculty of Electrical Engineering and Computer Science, Ostrava-Poruba, Czech Republic","Êtrba, R., Department of Computer Science, VÊB-Technical University of Ostrava, Faculty of Electrical Engineering and Computer Science, Ostrava-Poruba, Czech Republic; Êtolfa, S., Department of Computer Science, VÊB-Technical University of Ostrava, Faculty of Electrical Engineering and Computer Science, Ostrava-Poruba, Czech Republic; Êtolfa, J., Department of Computer Science, VÊB-Technical University of Ostrava, Faculty of Electrical Engineering and Computer Science, Ostrava-Poruba, Czech Republic; VondrÌÁk, I., Department of Computer Science, VÊB-Technical University of Ostrava, Faculty of Electrical Engineering and Computer Science, Ostrava-Poruba, Czech Republic; SnÌÁÁel, V., Department of Computer Science, VÊB-Technical University of Ostrava, Faculty of Electrical Engineering and Computer Science, Ostrava-Poruba, Czech Republic","Effort overruns is common problem in software development. Our main intention is to support estimation by method for classification of use cases. The goal of this paper is to evaluate usage of the feed-forward neural network for the Use Case classification purposes. Experimental results show that the feed-forward neural network classifier, using softmax activation function in the output layer and hyperbolic tangent activation function in the hidden layer, offers the best classification performance. å© 2017 The authors and IOS Press.",Artificial Neural Network; Backpropagation Softmax Function; Effort Estimation; Hyperbolic Tangent Function; Use Case,Conference Paper,Scopus,2-s2.0-85002828916