Document Title,Abstract,Year,PDF Link,label
Proactive Detection of Computer Worms Using Model Checking,"Although recent estimates are speaking of 200,000 different viruses, worms, and Trojan horses, the majority of them are variants of previously existing malware. As these variants mostly differ in their binary representation rather than their functionality, they can be recognized by analyzing the program behavior, even though they are not covered by the signature databases of current antivirus tools. Proactive malware detectors mitigate this risk by detection procedures that use a single signature to detect whole classes of functionally related malware without signature updates. It is evident that the quality of proactive detection procedures depends on their ability to analyze the semantics of the binary. In this paper, we propose the use of model checking-a well-established software verification technique-for proactive malware detection. We describe a tool that extracts an annotated control flow graph from the binary and automatically verifies it against a formal malware specification. To this end, we introduce the new specification language CTPL, which balances the high expressive power needed for malware signatures with efficient model checking algorithms. Our experiments demonstrate that our technique indeed is able to recognize variants of existing malware with a low risk of false positives.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4689469,no
Wavelet Codes for Algorithm-Based Fault Tolerance Applications,"Algorithm-based fault tolerance (ABFT) methods, which use real number parity values computed in two separate comparable ways to detect computer-induced errors in numerical processing operations, can employ wavelet codes for establishing the necessary redundancy. Wavelet codes, one form of real number convolutional codes, determine the required parity values in a continuous fashion and can be intertwined naturally with normal data processing. Such codes are the transform coefficients associated with an analysis uniform filter bank which employs downsampling, while parity-checking operations are performed by a syndrome synthesis filter bank that includes upsampling. The data processing operations are merged effectively with the parity generating function to provide one set of parity values. Good wavelet codes can be designed starting from standard convolutional codes over finite fields by relating the field elements with the integers in the real number space. ABFT techniques are most efficient when employing a systematic form and methods for developing systematic codes are detailed. Bounds on the ABFT overhead computations are given and ABFT protection methods for processing that contains feedback are outlined. Analyzing syndromes' variances guide the selection of thresholds for syndrome comparisons. Simulations demonstrate the detection and miss probabilities for some high-rate wavelet codes.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4803849,no
Highly Available Intrusion-Tolerant Services with Proactive-Reactive Recovery,"In the past, some research has been done on how to use proactive recovery to build intrusion-tolerant replicated systems that are resilient to any number of faults, as long as recoveries are faster than an upper bound on fault production assumed at system deployment time. In this paper, we propose a complementary approach that enhances proactive recovery with additional reactive mechanisms giving correct replicas the capability of recovering other replicas that are detected or suspected of being compromised. One key feature of our proactive-reactive recovery approach is that, despite recoveries, it guarantees the availability of a minimum number of system replicas necessary to sustain correct operation of the system. We design a proactive-reactive recovery service based on a hybrid distributed system model and show, as a case study, how this service can effectively be used to increase the resilience of an intrusion-tolerant firewall adequate for the protection of critical infrastructures.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5010435,no
On the Quality of Service of Crash-Recovery Failure Detectors,We model the probabilistic behavior of a system comprising a failure detector and a monitored crash-recovery target. We extend failure detectors to take account of failure recovery in the target system. This involves extending QoS measures to include the recovery detection speed and proportion of failures detected. We also extend estimating the parameters of the failure detector to achieve a required QoS to configuring the crash-recovery failure detector. We investigate the impact of the dependability of the monitored process on the QoS of our failure detector. Our analysis indicates that variation in the MTTF and MTTR of the monitored process can have a significant impact on the QoS of our failure detector. Our analysis is supported by simulations that validate our theoretical results.,2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5210115,no
Fisher Information-Based Evaluation of Image Quality for Time-of-Flight PET,"The use of time-of-flight (TOF) information during positron emission tomography (PET) reconstruction has been found to improve the image quality. In this work we quantified this improvement using two existing methods: 1) a very simple analytical expression only valid for a central point in a large uniform disk source and 2) efficient analytical approximations for post-filtered maximum likelihood expectation maximization (MLEM) reconstruction with a fixed target resolution, predicting the image quality in a pixel or in a small region of interest based on the Fisher information matrix. Using this latter method the weighting function for filtered backprojection reconstruction of TOF PET data proposed by C. Watson can be derived. The image quality was investigated at different locations in various software phantoms. Simplified as well as realistic phantoms, measured both with TOF PET systems and with a conventional PET system, were simulated. Since the time resolution of the system is not always accurately known, the effect on the image quality of using an inaccurate kernel during reconstruction was also examined with the Fisher information-based method. First, we confirmed with this method that the variance improvement in the center of a large uniform disk source is proportional to the disk diameter and inversely proportional to the time resolution. Next, image quality improvement was observed in all pixels, but in eccentric and high-count regions the contrast-to-noise ratio (CNR) increased less than in central and low- or medium-count regions. Finally, the CNR was seen to decrease when the time resolution was inaccurately modeled (too narrow or too wide) during reconstruction. Although the maximum CNR is not very sensitive to the time resolution error, using an inaccurate TOF kernel tends to introduce artifacts in the reconstructed image.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5223575,no
Bayesian Approaches to Matching Architectural Diagrams,"IT system architectures and many other kinds of structured artifacts are often described by formal models or informal diagrams. In practice, there are often a number of versions of a model or diagram, such as a series of revisions, divergent variants, or multiple views of a system. Understanding how versions correspond or differ is crucial, and thus, automated assistance for matching models and diagrams is essential. We have designed a framework for finding these correspondences automatically based on Bayesian methods. We represent models and diagrams as graphs whose nodes have attributes such as name, type, connections to other nodes, and containment relations, and we have developed probabilistic models for rating the quality of candidate correspondences based on various features of the nodes in the graphs. Given the probabilistic models, we can find high-quality correspondences using search algorithms. Preliminary experiments focusing on architectural models suggest that the technique is promising.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5232811,no
Recursive Pseudo-Exhaustive Two-Pattern Generation,"Pseudo-exhaustive pattern generators for built-in self-test (BIST) provide high fault coverage of detectable combinational faults with much fewer test vectors than exhaustive generation. In (<i>n</i>, <i>k</i>)-adjacent bit pseudo-exhaustive test sets, all 2<i>k</i> binary combinations appear to all adjacent <i>k</i>-bit groups of inputs. With recursive pseudoexhaustive generation, all (<i>n</i>, <i>k</i>)-adjacent bit pseudoexhaustive tests are generated for <i>k</i> ?? <i>n</i> and more than one modules can be pseudo-exhaustively tested in parallel. In order to detect sequential (e.g., stuck-open) faults that occur into current CMOS circuits, two-pattern tests are exercised. Also, delay testing, commonly used to assure correct circuit operation at clock speed requires two-pattern tests. In this paper a pseudoexhaustive two-pattern generator is presented, that recursively generates all two-pattern (<i>n</i>, <i>k</i>)-adjacent bit pseudoexhaustive tests for all <i>k</i> ?? <i>n</i>. To the best of our knowledge, this is the first time in the open literature that the subject of recursive pseudoexhaustive two-pattern testing is being dealt with. A software-based implementation with no hardware overhead is also presented.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5272395,no
Generating Event Sequence-Based Test Cases Using GUI Runtime State Feedback,"This paper presents a fully automatic model-driven technique to generate test cases for graphical user interfaces (GUIs)-based applications. The technique uses feedback from the execution of a ??seed test suite,?? which is generated automatically using an existing structural event interaction graph model of the GUI. During its execution, the runtime effect of each GUI event on all other events pinpoints event semantic interaction (ESI) relationships, which are used to automatically generate new test cases. Two studies on eight applications demonstrate that the feedback-based technique 1) is able to significantly improve existing techniques and helps identify serious problems in the software and 2) the ESI relationships captured via GUI state yield test suites that most often detect more faults than their code, event, and event-interaction-coverage equivalent counterparts.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306073,no
An Integrated Data-Driven Framework for Computing System Management,"With advancement in science and technology, computing systems are becoming increasingly more complex with a growing number of heterogeneous software and hardware components. They are thus becoming more difficult to monitor, manage, and maintain. Traditional approaches to system management have been largely based on domain experts through a knowledge acquisition solution that translates domain knowledge into operating rules and policies. This process has been well known as cumbersome, labor intensive, and error prone. In addition, traditional approaches for system management are difficult to keep up with the rapidly changing environments. There is a pressing need for automatic and efficient approaches to monitor and manage complex computing systems. In this paper, we propose an integrated data-driven framework for computing system management by acquiring the needed knowledge automatically from a large amount of historical log data. Specifically, we apply text mining techniques to automatically categorize the log messages into a set of canonical categories, incorporate temporal information to improve categorization performance, develop temporal mining techniques to discover the relationships between different events, and take a novel approach called event summarization to provide a concise interpretation of the temporal patterns.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5313888,no
Learning a Metric for Code Readability,"In this paper, we explore the concept of code readability and investigate its relation to software quality. With data collected from 120 human annotators, we derive associations between a simple set of local code features and human notions of readability. Using those features, we construct an automated readability measure and show that it can be 80 percent effective and better than a human, on average, at predicting readability judgments. Furthermore, we show that this metric correlates strongly with three measures of software quality: code changes, automated defect reports, and defect log messages. We measure these correlations on over 2.2 million lines of code, as well as longitudinally, over many releases of selected projects. Finally, we discuss the implications of this study on programming language design and engineering practice. For example, our data suggest that comments, in and of themselves, are less important than simple blank lines to local judgments of readability.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5332232,no
Architectural Enhancement and System Software Support for Program Code Integrity Monitoring in Application-Specific Instruction-Set Processors,"Program code in a computer system can be altered either by malicious security attacks or by various faults in microprocessors. At the instruction level, all code modifications are manifested as bit flips. In this paper, we present a generalized methodology for monitoring code integrity at run-time in application-specific instruction-set processors. We embed monitoring microoperations in machine instructions, so the processor is augmented with a hardware monitor automatically. The monitor observes the processor's execution trace at run-time, checks whether it aligns with the expected program behavior, and signals any mismatches. Since the monitor works at a level below the instructions, the monitoring mechanism cannot be bypassed by software or compromised by malicious users. We discuss the ability and limitation of such monitoring mechanism for detecting both soft errors and code injection attacks. We propose two different schemes for managing the monitor, the operating system (OS) managed and application controlled, and design the constituent components within the monitoring architecture. Experimental results show that with an effective hash function implementation, our microarchitectural support can detect program code integrity compromises at a high probability with small area overhead and little performance degradation.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5332238,no
The Future of Integrated Circuits: A Survey of Nanoelectronics,"While most of the electronics industry is dependent on the ever-decreasing size of lithographic transistors, this scaling cannot continue indefinitely. Nanoelectronics (circuits built with components on the scale of 10 nm) seem to be the most promising successor to lithographic based ICs. Molecular-scale devices including diodes, bistable switches, carbon nanotubes, and nanowires have been fabricated and characterized in chemistry labs. Techniques for self-assembling these devices into different architectures have also been demonstrated and used to build small-scale prototypes. While these devices and assembly techniques will lead to nanoscale electronics, they also have the drawback of being prone to defects and transient faults. Fault-tolerance techniques will be crucial to the use of nanoelectronics. Lastly, changes to the software tools that support the fabrication and use of ICs will be needed to extend them to support nanoelectronics. This paper introduces nanoelectronics and reviews the current progress made in research in the areas of technologies, architectures, fault tolerance, and software tools.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5337937,no
Low-Complexity Transcoding of JPEG Images With Near-Optimal Quality Using a Predictive Quality Factor and Scaling Parameters,"A common transcoding operation consists of reducing the file size of a JPEG image to meet bandwidth or device constraints. This can be achieved by reducing its quality factor (QF) or reducing its resolution, or both. In this paper, using the structural similarity (SSIM) index as the quality metric, we present a system capable of estimating the QF and scaling parameters to achieve optimal quality while meeting a device's constraints. We then propose a novel low-complexity JPEG transcoding system which delivers near-optimal quality. The system is capable of predicting the best combination of QF and scaling parameters for a wide range of device constraints and viewing conditions. Although its computational complexity is an order of magnitude smaller than the system providing optimal quality, the proposed system yields quality results very similar to those of the optimal system.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5342473,no
Modulation Quality Measurement in WiMAX Systems Through a Fully Digital Signal Processing Approach,"The performance assessment of worldwide interoperability for microwave access (WiMAX) systems is dealt with. A fully digital signal processing approach for modulation quality measurement is proposed, which is particularly addressed to transmitters based on orthogonal frequency-division multiplexing (OFDM) modulation. WiMAX technology deployment is rapidly increasing. To aid researchers, manufactures, and technicians in designing, realizing, and installing devices and apparatuses, some measurement solutions are already available, and new ones are being released on the market. All of them are arranged to complement an <i>ad hoc</i> digital signal processing software with an existing specialized measurement instrument such as a real-time spectrum analyzer or a vector signal analyzer. Furthermore, they strictly rely on a preliminary analog downconversion of the radio-frequency input signal, which is a basic front-end function provided by the cited instruments, to suitably digitize and digitally process the acquired samples. In the same way as the aforementioned solutions, the proposed approach takes advantage of existing instruments, but different from them, it provides for a direct digitization of the radio-frequency input signal. No downconversion is needed, and the use of general-purpose measurement hardware such as digital scopes or data acquisition systems is thus possible. A proper digital signal processing algorithm, which was designed and implemented by the authors, then demodulates the digitized signal, extracts the desired measurement information from its baseband components, and assesses its modulation quality. The results of several experiments conducted on laboratory WiMAX signals show the effectiveness and reliability of the approach with respect to the major competitive solutions; its superior performance in special physical-layer conditions is also highlighted.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5353716,no
Hybrid Simulated Annealing and Its Application to Optimization of Hidden Markov Models for Visual Speech Recognition,"We propose a novel stochastic optimization algorithm, <i>hybrid simulated annealing</i> (SA), to train hidden Markov models (HMMs) for visual speech recognition. In our algorithm, SA is combined with a local optimization operator that substitutes a better solution for the current one to improve the convergence speed and the quality of solutions. We mathematically prove that the sequence of the objective values converges in probability to the global optimum in the algorithm. The algorithm is applied to train HMMs that are used as visual speech recognizers. While the popular training method of HMMs, the expectation-maximization algorithm, achieves only local optima in the parameter space, the proposed method can perform global optimization of the parameters of HMMs and thereby obtain solutions yielding improved recognition performance. The superiority of the proposed algorithm to the conventional ones is demonstrated via isolated word recognition experiments.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5373955,no
Performability Analysis of Multistate Computing Systems Using Multivalued Decision Diagrams,"A distinct characteristic of multistate systems (MSS) is that the systems and/or their components may exhibit multiple performance levels (or states) varying from perfect operation to complete failure. MSS can model behaviors such as shared loads, performance degradation, imperfect fault coverage, standby redundancy, limited repair resources, and limited link capacities. The nonbinary state property of MSS and their components as well as dependencies existing among different states of the same component make the analysis of MSS difficult. This paper proposes efficient algorithms for analyzing MSS using multivalued decision diagrams (MDD). Various reliability, availability, and performability measures based on state probabilities or failure frequencies are considered. The application and advantages of the proposed algorithms are demonstrated through two examples. Furthermore, experimental results on a set of benchmark examples are presented to illustrate the advantages of the proposed MDD-based method for the performability analysis of MSS, as compared to the existing methods.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5374373,no
The Probabilistic Program Dependence Graph and Its Application to Fault Diagnosis,"This paper presents an innovative model of a program's internal behavior over a set of test inputs, called the probabilistic program dependence graph (PPDG), which facilitates probabilistic analysis and reasoning about uncertain program behavior, particularly that associated with faults. The PPDG construction augments the structural dependences represented by a program dependence graph with estimates of statistical dependences between node states, which are computed from the test set. The PPDG is based on the established framework of probabilistic graphical models, which are used widely in a variety of applications. This paper presents algorithms for constructing PPDGs and applying them to fault diagnosis. The paper also presents preliminary evidence indicating that a PPDG-based fault localization technique compares favorably with existing techniques. The paper also presents evidence indicating that PPDGs can be useful for fault comprehension.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5374423,no
Vulnerability Discovery with Attack Injection,"The increasing reliance put on networked computer systems demands higher levels of dependability. This is even more relevant as new threats and forms of attack are constantly being revealed, compromising the security of systems. This paper addresses this problem by presenting an attack injection methodology for the automatic discovery of vulnerabilities in software components. The proposed methodology, implemented in AJECT, follows an approach similar to hackers and security analysts to discover vulnerabilities in network-connected servers. AJECT uses a specification of the server's communication protocol and predefined test case generation algorithms to automatically create a large number of attacks. Then, while it injects these attacks through the network, it monitors the execution of the server in the target system and the responses returned to the clients. The observation of an unexpected behavior suggests the presence of a vulnerability that was triggered by some particular attack (or group of attacks). This attack can then be used to reproduce the anomaly and to assist the removal of the error. To assess the usefulness of this approach, several attack injection campaigns were performed with 16 publicly available POP and IMAP servers. The results show that AJECT could effectively be used to locate vulnerabilities, even on well-known servers tested throughout the years.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5374427,no
Evaluation of Accuracy in Design Pattern Occurrence Detection,"Detection of design pattern occurrences is part of several solutions to software engineering problems, and high accuracy of detection is important to help solve the actual problems. The improvement in accuracy of design pattern occurrence detection requires some way of evaluating various approaches. Currently, there are several different methods used in the community to evaluate accuracy. We show that these differences may greatly influence the accuracy results, which makes it nearly impossible to compare the quality of different techniques. We propose a benchmark suite to improve the situation and a community effort to contribute to, and evolve, the benchmark suite. Also, we propose fine-grained metrics assessing the accuracy of various approaches in the benchmark suite. This allows comparing the detection techniques and helps improve the accuracy of detecting design pattern occurrences.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5374428,no
Assessing Software Service Quality and Trustworthiness at Selection Time,"The integration of external software in project development is challenging and risky, notably because the execution quality of the software and the trustworthiness of the software provider may be unknown at integration time. This is a timely problem and of increasing importance with the advent of the SaaS model of service delivery. Therefore, in choosing the SaaS service to utilize, project managers must identify and evaluate the level of risk associated with each candidate. Trust is commonly assessed through reputation systems; however, existing systems rely on ratings provided by consumers. This raises numerous issues involving the subjectivity and unfairness of the service ratings. This paper describes a framework for reputation-aware software service selection and rating. A selection algorithm is devised for service recommendation, providing SaaS consumers with the best possible choices based on quality, cost, and trust. An automated rating model, based on the expectancy-disconfirmation theory from market science, is also defined to overcome feedback subjectivity issues. The proposed rating and selection models are validated through simulations, demonstrating that the system can effectively capture service behavior and recommend the best possible choices.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5383370,no
Exception Handling for Repair in Service-Based Processes,"This paper proposes a self-healing approach to handle exceptions in service-based processes and to repair the faulty activities with a model-based approach. In particular, a set of repair actions is defined in the process model, and repairability of the process is assessed by analyzing the process structure and the available repair actions. During execution, when an exception arises, repair plans are generated by taking into account constraints posed by the process structure, dependencies among data, and available repair actions. The paper also describes the main features of the prototype developed to validate the proposed repair approach for composed Web services; the self-healing architecture for repair handling and the experimental results are illustrated.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5383376,no
Using Launch-on-Capture for Testing BIST Designs Containing Synchronous and Asynchronous Clock Domains,"This paper presents a new at-speed logic built-in self-test (BIST) architecture supporting two launch-on-capture schemes, namely aligned double-capture and staggered double-capture, for testing multi-frequency synchronous and asynchronous clock domains in a scan-based BIST design. The proposed architecture also includes BIST debug and diagnosis circuitry to help locate BIST failures. The aligned scheme detects and allows diagnosis of structural and delay faults among all synchronous clock domains, whereas the staggered scheme detects and allows diagnosis of structural and delay faults among all asynchronous clock domains. Both schemes solve the long-standing problem of using the conventional one-hot scheme, which requires testing each clock domain one at a time, or the simultaneous scheme, which requires adding isolation logic to normal functional paths across interacting clock domains. Physical implementation is easily achieved by the proposed solution due to the use of a slow-speed, global scan enable signal and reduced timing-critical design requirements. Application results for industrial designs demonstrate the effectiveness of the proposed architecture.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5395739,no
A Traveling-Wave-Based Protection Technique Using Wavelet/PCA Analysis,"This paper proposes a powerful high-speed traveling-wave-based technique for the protection of power transmission lines. The proposed technique uses principal component analysis to identify the dominant pattern of the signals preprocessed by wavelet transform. The proposed protection algorithm presents a discriminating method based on the polarity, magnitude, and time interval between the detected traveling waves at the relay location. A supplemental algorithm consisting of a high-set overcurrent relay as well as an impedance-based relay is also proposed. This is done to overcome the well-known shortcomings of traveling-wave-based protection techniques for the detection of very close-in faults and single-phase-to-ground faults occurring at small voltage magnitudes. The proposed technique is evaluated for the protection of a two-terminal transmission line. Extensive simulation studies using PSCAD/EMTDC software indicate that the proposed approach is reliable for rapid and correct identification of various fault cases. It identifies most of the internal faults very rapidly in less than 2 ms. In addition, the proposed technique presents high noise immunity.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5404978,no
Efficient Software Verification: Statistical Testing Using Automated Search,"Statistical testing has been shown to be more efficient at detecting faults in software than other methods of dynamic testing such as random and structural testing. Test data are generated by sampling from a probability distribution chosen so that each element of the software's structure is exercised with a high probability. However, deriving a suitable distribution is difficult for all but the simplest of programs. This paper demonstrates that automated search is a practical method of finding near-optimal probability distributions for real-world programs, and that test sets generated from these distributions continue to show superior efficiency in detecting faults in the software.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5406530,no
Application of Prognostic and Health Management technology on aircraft fuel system,"Prognostic and health management (PHM), which could provide the ability of fault detection (FD), fault isolation (FI) and estimation of remaining useful life (RUL), has been applied to detect and diagnose device faults and assess its health status, with aiming to enhance device reliability, safety, and reduce its maintenance costs. In this paper, taking an aircraft fuel system as an example, with virtual instrument technology and computer simulation technology, an integrated approach of signal processing method and model-based method is introduced to build the virtual simulation software of aircraft fuel PHM system for overcoming the difficulty in obtaining the failures information from the real fuel system. During the process of constructing the aircraft fuel PHM system, the first step is to analyze the fuel system failure modes and status parameters that can identify the failure modes. The main failure modes are determined as joints looseness, pipe broken, nozzle clogging, and fuel tank leakage. The status parameters are fuel pressure and fuel flow. Then, the status parameter model is constructed to imitate the behavior of sensor which detecting fuel system status. On this basis, utilizing the signal processing module provided by Labview software, the outputs from the virtual sensors, which collect the failure data, are processed to realize the simulation of failure detection and failure diagnosis. All the result shows that the virtual simulation software well accomplishes the task of the aircraft fuel system failure detection and diagnosis.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5413340,no
A stochastic filtering based data driven approach for residual life prediction and condition based maintenance decision making support,"As an efficient means of detecting potential plant failure, condition monitoring is growing popular in industry with million's spent on condition monitoring hardware and software. The use of condition monitoring techniques will generally increase plant availability and reduce downtime costs, but in some cases it will also tend to over-maintaining the plant in question. There is obviously a need for appropriate decision support in plant maintenance planning utilising available condition monitoring information, but compared to the extensive literature on diagnosis, relatively little research has been done on the prognosis side of condition based maintenance. In plant prognosis, a key, but often uncertain quantity to be modelled is the residual life prediction based on available condition information to date. This paper shall focus upon such a residual life prediction of the monitored items in condition based maintenance and review the recent developments in modelling residual life prediction using stochastic filtering. We first demonstrate the role of residual life prediction in condition based maintenance decision making, which highlights the need for such a prediction. We then discuss in detail the basic filtering model we used for residual life prediction and the extensions we made. We finally present briefly the result of the comparative studies between the filtering based model and other models using empirical data. The results show that the filtering based approach is the best in terms of prediction accuracy and cost effectiveness.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5413485,no
Improving interpretation of component-based systems quality through visualisation techniques,"Component-based software development is increasingly more commonplace and is widely used in the development of commercial software systems. This has led to the existence of several research works focusing on software component-based systems quality. The majority of this research proposes quality models focused on component-based systems in which different measures are proposed. In general, the result of assessing the measures is a number, which is necessary to determine the component-based system quality level. However, understanding and interpreting the data set is not an easy task. In order to facilitate the interpretation of results, this study selects and adapts a specific visual metaphor with which to show component-based systems quality. A tool has additionally been developed which permits the automatic assessment of the measures to be carried out. The tool also shows the results visually and proposes corrective actions through which to improve the level of quality. A case study is used to assess and to show the quality of a real-world component-based software system in a graphic manner.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5415511,no
Computer-aided recoding for multi-core systems,"The design of embedded computing systems faces a serious productivity gap due to the increasing complexity of their hardware and software components. One solution to address this problem is the modeling at higher levels of abstraction. However, manually writing proper executable system models is challenging, error-prone, and very time-consuming. We aim to automate critical coding tasks in the creation of system models. This paper outlines a novel modeling technique called computer-aided recoding which automates the process of writing abstract models of embedded systems by use of advanced computer-aided design (CAD) techniques. Using an interactive, designer-controlled approach with automated source code transformations, our computer-aided recoding technique derives an executable parallel system model directly from available sequential reference code. Specifically, we describe three sets of source code transformations that create structural hierarchy, expose potential parallelism, and create explicit communication and synchronization. As a result, system modeling is significantly streamlined. Our experimental results demonstrate the shortened design time and higher productivity.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5419796,no
The Theory of Relative Dependency: Higher Coupling Concentration in Smaller Modules,"Our observations on several large-scale software products has consistently shown that smaller modules are proportionally more defect prone. These findings, challenge the common recommendations from the literature suggesting that quality assurance (QA) and quality control (QC) resources should focus on larger modules. Those recommendations are based on the unfounded assumption that a monotonically increasing linear relationship exists between module size and defects. Given that complexity is correlated with the size.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5420801,no
Assess Content Comprehensiveness of Ontologies,"This paper proposes a novel method to assess and evaluate content comprehensiveness of ontologies. Comparing to other researchers methods which just count the number of classes and properties, the method concerns about the actual content coverage of ontologies. By applying statistical analysis to a corpus, we assign different weights to different terms chosen from the corpus. These terms are then used for evaluating ontologies. Afterwards, a score is generated for each ontology to mark its content comprehensiveness. Experiments are then appropriately designed to evaluate the qualities of typical ontologies to show the effectiveness of the proposed evaluation method.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5421334,no
The end of the blur,"This paper describes the application of NASA's software that calculates optical aberrations (wavefront errors). The power of this software lies in its ability to use an optical system's existing camera as a sensor to detect its own error, without installing any separate devices. This software is expected to sharpen images from space and improving the image quality of the astronomical telescopes, it could also redefine perfect vision for humans.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5421901,no
Condition Monitoring of the Power Output of Wind Turbine Generators Using Wavelets,"With an increasing number of wind turbines being erected offshore, there is a need for cost-effective, predictive, and proactive maintenance. A large fraction of wind turbine downtime is due to bearing failures, particularly in the generator and gearbox. One way of assessing impending problems is to install vibration sensors in key positions on these subassemblies. Such equipment can be costly and requires sophisticated software for analysis of the data. An alternative approach, which does not require extra sensors, is investigated in this paper. This involves monitoring the power output of a variable-speed wind turbine generator and processing the data using a wavelet in order to extract the strength of particular frequency components, characteristic of faults. This has been done for doubly fed induction generators (DFIGs), commonly used in modern variable-speed wind turbines. The technique is first validated on a test rig under controlled fault conditions and then is applied to two operational wind turbine DFIGs where generator shaft misalignment was detected. For one of these turbines, the technique detected a problem 3 months before a bearing failure was recorded.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5422657,no
Predictive data mining model for software bug estimation using average weighted similarity,"Software bug estimation is a very essential activity for effective and proper software project planning. All the software bug related data are kept in software bug repositories. Software bug (defect) repositories contains lot of useful information related to the development of a project. Data mining techniques can be applied on these repositories to discover useful interesting patterns. In this paper a prediction data mining technique is proposed to predict the software bug estimation from a software bug repository. A two step prediction model is proposed In the first step bug for which estimation is required, its summary and description is matched against the summary and description of bugs available in bug repositories. A weighted similarity model is suggested to match the summary and description for a pair of software bugs. In the second step the fix duration of all the similar bugs are calculated and stored and its average is calculated, which indicates the predicted estimation of a bug. The proposed model is implemented using open source technologies and is explained with the help of illustrative example.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5422923,no
Towards Enhanced User Interaction to Qualify Web Resources for Higher-Layered Applications,"The Web offers autonomous and frequently useful resources in growing manner. User Generated Content (UGC) like Wikis, Weblogs or Webfeeds often do not have one responsible authorship or declared experts who checked the created content for e.g. accuracy, availability, objectivity or reputation. The user is not able easily, to control the quality of the content he receives. If we want to utilize the distributed information flood as a linked knowledge base for higher-layered applications - e.g. for knowledge transfer and learning - information quality (iq) is a very important and complex aspect to analyze, personalize and annotate resources. In general, low information quality is one of the main discriminators of data sources on the Web. Assessing information quality with measurable terms can offer a personalized and smart view on a broad, global knowledge base. We developed the qKAI application framework to utilize available, distributed data sets in a practically manner. In the following we present our adaption of information quality aspects to qualify Web resources based on a three-level assessment model. We deploy knowledge-related iq-criteria as tool to implement iq-mechanisms stepwise into the qKAI framework. Here, we exemplify selected criteria of information quality in qKAI like relevance or accuracy. We derived assessment methods for certain iq-criteria enabling rich, game-based user interaction and semantic resource annotation. Open Content is embedded into knowledge games to increase the users' access and learning motivation. As side effect the resources' quality is enhanced stepwise by ongoing user interaction.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5430000,no
Transparent Fault Tolerance of Device Drivers for Virtual Machines,"In a consolidated server system using virtualization, physical device accesses from guest virtual machines (VMs) need to be coordinated. In this environment, a separate driver VM is usually assigned to this task to enhance reliability and to reuse existing device drivers. This driver VM needs to be highly reliable, since it handles all the I/O requests. This paper describes a mechanism to detect and recover the driver VM from faults to enhance the reliability of the whole system. The proposed mechanism is transparent in that guest VMs cannot recognize the fault and the driver VM can recover and continue its I/O operations. Our mechanism provides a progress monitoring-based fault detection that is isolated from fault contamination with low monitoring overhead. When a fault occurs, the system recovers by switching the faulted driver VM to another one. The recovery is performed without service disconnection or data loss and with negligible delay by fully exploiting the I/O structure of the virtualized system.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5432158,no
Context-Aware Adaptive Applications: Fault Patterns and Their Automated Identification,"Applications running on mobile devices are intensely context-aware and adaptive. Streams of context values continuously drive these applications, making them very powerful but, at the same time, susceptible to undesired configurations. Such configurations are not easily exposed by existing validation techniques, thereby leading to new analysis and testing challenges. In this paper, we address some of these challenges by defining and applying a new model of adaptive behavior called an Adaptation Finite-State Machine (A-FSM) to enable the detection of faults caused by both erroneous adaptation logic and asynchronous updating of context information, with the latter leading to inconsistencies between the external physical context and its internal representation within an application. We identify a number of adaptation fault patterns, each describing a class of faulty behaviors. Finally, we describe three classes of algorithms to detect such faults automatically via analysis of the A-FSM. We evaluate our approach and the trade-offs between the classes of algorithms on a set of synthetically generated Context-Aware Adaptive Applications (CAAAs) and on a simple but realistic application in which a cell phone's configuration profile changes automatically as a result of changes to the user's location, speed, and surrounding environment. Our evaluation describes the faults our algorithms are able to detect and compares the algorithms in terms of their performance and storage requirements.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5432224,no
Low Overhead Incremental Checkpointing and Rollback Recovery Scheme on Windows Operating System,"Implementation of a low overhead incremental checkpointing and rollback recovery scheme that consists of incremental checkpointing combines copy-on-write technique and optimal checkpointing interval is addressed in this article. The checkpointing permits to save process state periodically during failure-free execution, and the recovery scheme maintains to normally execute the task when failure occurs in a PC-based computer-controlled system employed with Windows Operating System. Excess size of capturing state and arbitrary checkpointing results in either performance degradation or expensive recovery cost. For the objective of minimizing overhead, the checkpointing and recovery scheme is designed of Win32 API interception associated with incremental checkpointing and copy-on-write technique. Instead of saving entire process space, it only needs to save the modified pages and uses buffer to save state temporarily in the process of checkpointing so that the checkpointing overhead is reduced. While system is encountered with failure, the minimum expected time of the total overhead to complete a task is calculated by using probability to find the optimal checkpointing interval. From simulation results, the proposed checkpointing and rollback recovery scheme not only enhances the capability of the normal task executing but also reduces the overhead of checkpointing and recovery.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5432637,no
Unified Approach for Next Generation Multimedia Based Communication Components Integration with Signaling and Media Processing Framework,"Modern digital technology makes it possible to manipulate multi-dimensional signals with systems that range from simple digital circuits to advanced parallel computers. It allows the user to modify and gives the excellent results for user experience as well as other commercial and security applications. Communication Technologies like Circuit Switched Video Telephony, IMS based multimedia Applications like Video Share, VoIP, VVoIP, Video on Demand etc are Multimedia based method provides real time Audio, Video and Data, it is growing in the current mobile and Broadband technologies. Current Multimedia based communication technologies are supporting low bandwidth error prone communication mechanism between terminals. Video Telephony supports signaling and the data transmission from peer to peer over low data rate flow in the network. Media control components are required to integrate the communication signaling and media processing. This paper proposes generic approach across various multimedia framework and signaling modules. A new media control interface layer enables seamless data and signaling flow between various multimedia data processing frameworks and signaling modules and improves media communication processing. This layer increases the data flow and improves the quality of media data processing. It helps to integrate communication signaling module with media processing. This paper analyzes the media control interface based media communication based mechanisms like CS Video Telephony, IP based Multimedia Technologies like Video Share, VOIP etc. Media control Interface layer helps to integrate easily various Multimedia applications such as CS Video Telephony, Violet into various media processing multimedia framework such as DirectShow, GStreamer, Opencore etc. It also helps to integrate multimedia middleware stacks such as Media Transfer protocol [MTP], Digital Living Network Alliance [DLNA], Image Processing Pipeline Algorithms with the Application and Nativ- e Layer.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5432698,no
Evaluation of WS-* Standards Based Interoperability of SOA Products for the Hungarian e-Government Infrastructure,"The proposed architecture of the Hungarian e-government framework, mandating the functional co-operation of independent organizations, puts special emphasis on interoperability. WS-*standards have been created to reach uniformity and interoperability in the common middleware tasks for Web services such as security, reliable messaging and transactions. These standards, however, while existing for some time, have implementations slightly different in quality. In order to assess implementations, thorough tests should be performed, and relevant test cases ought to be accepted. For selecting mature SOA products for e-government application, a methodology of such an assessment is needed. We have defined a flexible and extensible test bed and a set of test cases for SOA products considering three aspects: compliance with standards, interoperability and development support.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5432813,no
Performance Evaluation of Handoff Queuing Schemes,"One of the main advantages of new wireless systems is the freedom to make and receive calls anywhere and at any time; handovers are considered a key element for providing this mobility. This paper presents the handoff queuing problem in cellular networks. We propose a model to study three handoff queuing schemes, and provide a performance evaluation of these ones. The paper begin by a presentation of the different handoff queuing scheme to evaluate. Then, gives an evaluation model with the different assumption considered in the simulations. The evaluation concerns the blocking probability for handoff and original calls. These simulations are conducted for each scheme, according to different offered loads, size of call (original and handoff) queue, and number of voice channels. A model is proposed and introduced in this paper for the study of three channel assignment schemes; namely, they are the non prioritized schemes (NPS), the original call queuing schemes, and the handoff call queuing schemes.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5437627,no
A Hermite Interpolation Based Motion Vector Recovery Algorithm for H.264/AVC,"Error Concealment is a very useful method to improve the video quality. It aims at using the maximum received video data to recover the lost information of the picture at the decoder. Lagrange interpolation algorithm is the most effective interpolation for error concealment while it lost some detail information. Hermite interpolation algorithm considers the change rate of the motion vector as well as the motion vector itself, which is more accurate. In this paper, we propose a novel method which uses hermite interpolation to predict the lost motion vectors. We take the change rate (derivative) of motion vector into account, and synthesizing the horizontal and vertical recovered motion vectors adaptively by the minimum distance method. The experimental result shows that our method achieves higher PSNR values than Lagrange interpolation.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5437635,no
Failure Detection and Localization in OTN Based on Optical Power Analysis,"In consideration of the new features of Optical Transport Networks (OTN), the failure detection and localization has become a new challenging issue in OTN management research area. This paper proposes a scheme to detect and locate the failures based on the optical power analysis. In failure detection section of the scheme, this paper propose a method to detect the performance degradation caused by possible failures based on real optical power analysis and build a status matrix which demonstrates the current optical power deviation of the fiber port of each node in OTN. In failure localization section of the scheme, this paper proposes the multiple failures location algorithm (MFLA), which deals with both single point failure and multi-point failures, to locate the multiple failures based on analyzing the status matrix and the switching relationship matrix. Then, an exemplary scenario is given to present the result of detecting and locating the fiber link failure and OXC device failure with the proposed scheme.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5437636,no
New Handover Scheme Based on User Profile: A Comparative Study,In this paper we have analyzed the number of handover required in different time stamp for a particular user on the quality of service parameters namely handover call blocking probability. This system model is based on the analysis of user mobility profile and assigns a weightage factor to each cell. The mathematical equation of the handover blocking probability derived from stochastic comparisons has-been used to compute upper bounds on dropping handover blocking probability by using Guard channel assignment scheme. A conceptual view is given to reduce the new call blocking probability. We use Dynamic Channel Reservation Scheme that can assign handover-reserved channels to new calls depending on the handover weightage factor to reduce new call blocking probability.,2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5437642,no
Priority-Based Service Differentiation Scheme for Medium and High Rate Sensor Networks,"In Medium and High Rate Wireless Sensor Networks (MHWSNs), a sensor node may gather different kinds of network traffic. To support quality of service (QoS) requirements for different kinds of network traffic, an effective and fare queuing and scheduling scheme is necessary. The paper present a queuing model to distinguish different network traffic based priority, and proposed a rate adjustment model to adjust output rate at a node. Simulation results shows the proposed priority-based service differentiation queuing model can guarantee low average delay for high priority real time traffic. Using rate adjustment model can achieve low packet loss probability, therefore increase the network throughput.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5437697,no
Security and Performance Aspects of an Agent-Based Link-Layer Vulnerability Discovery Mechanism,"The identification of vulnerable hosts and subsequent deployment of mitigation mechanisms such as service disabling or installation of patches is both time-critical and error-prone. This is in part owing to the fact that malicious worms can rapidly scan networks for vulnerable hosts, but is further exacerbated by the fact that network topologies are becoming more fluid and vulnerable hosts may only be visible intermittently for environments such as virtual machines or wireless edge networks. In this paper we therefore describe and evaluate an agent-based mechanism which uses the spanning tree protocol (STP) to gain knowledge of the underlying network topology to allow both rapid and resource-efficient traversal of the network by agents as well as residual scanning and mitigation techniques on edge nodes. We report performance results, comparing the mechanism against a random scanning worm and demonstrating that network immunity can be largely achieved despite a very limited warning interval. We also discuss mechanisms to protect the agent mechanism against subversion, noting that similar approaches are also increasingly deployed in case of malicious code.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5438039,no
Identifying Security Relevant Warnings from Static Code Analysis Tools through Code Tainting,"Static code analysis tools are often used by developers as early vulnerability detectors. Due to their automation they are less time-consuming and error-prone then manual reviews. However, they produce large quantities of warnings that developers have to manually examine and understand.In this paper, we look at a solution that makes static code analysis tools more useful as an early vulnerability detector. We use flow-sensitive, interprocedural and context-sensitive data flow analysis to determine the point of user input and its migration through the source code to the actual exploit. By determining a vulnerabilities point of entry we lower the number of warnings a tool produces and we provide the developer with more information why this warning could be a real security threat. We use our approach in three different ways depending on what tool we examined. First, With the commercial static code analysis tool, Coverity, we reanalyze its results and create a set of warnings that are specifically relevant from a security perspective. Secondly, we altered the open source analysis tool Findbugs to only analyze code that has been tainted by user input. Third, we created an own analysis tool that focuses on XSS vulnerabilities in Java code.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5438066,no
Estimating Error-probability and its Application for Optimizing Roll-back Recovery with Checkpointing,"The probability for errors to occur in electronic systems is not known in advance, but depends on many factors including influence from the environment where the system operates. In this paper, it is demonstrated that inaccurate estimates of the error probability lead to loss of performance in a well known fault tolerance technique, Roll-back Recovery with checkpointing (RRC). To regain the lost performance, a method for estimating the error probability along with an adjustment technique are proposed. Using a simulator tool that has been developed to enable experimentation, the proposed method is evaluated and the results show that the proposed method provides useful estimates of the error probability leading to near-optimal performance of the RRC fault-tolerant technique.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5438676,no
A Smart CMOS Image Sensor with On-chip Hot Pixel Correcting Readout Circuit for Biomedical Applications,"One of the most recent and exciting applications for CMOS image sensors is in the biomedical field. In such applications, these sensors often operate in harsh environments (high intensity, high pressure, long time exposure), which increase the probability for the occurrence of hot pixel defects over their lifetime. This paper presents a novel smart CMOS image sensor integrating hot pixel correcting readout circuit to preserve the quality of the captured images. With this approach, no extra non-volatile memory is required in the sensor device to store the locations of the hot pixels. In addition, the reliability of the sensor is ensured by maintaining a real-time detection of hot pixels during image capture.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5438703,no
Population-Based Algorithm Portfolios for Numerical Optimization,"In this paper, we consider the scenario that a population-based algorithm is applied to a numerical optimization problem and a solution needs to be presented within a given time budget. Although a wide range of population-based algorithms, such as evolutionary algorithms, particle swarm optimizers, and differential evolution, have been developed and studied under this scenario, the performance of an algorithm may vary significantly from problem to problem. This implies that there is an inherent risk associated with the selection of algorithms. We propose that, instead of choosing an existing algorithm and investing the entire time budget in it, it would be less risky to distribute the time among multiple different algorithms. A new approach named population-based algorithm portfolio (PAP), which takes multiple algorithms as its constituent algorithms, is proposed based upon this idea. PAP runs each constituent algorithm with a part of the given time budget and encourages interaction among the constituent algorithms with a migration scheme. As a general framework rather than a specific algorithm, PAP is easy to implement and can accommodate any existing population-based search algorithms. In addition, a metric is also proposed to compare the risks of any two algorithms on a problem set. We have comprehensively evaluated PAP via investigating 11 instantiations of it on 27 benchmark functions. Empirical results have shown that PAP outperforms its constituent algorithms in terms of solution quality, risk, and probability of finding the global optimum. Further analyses have revealed that the advantages of PAP are mostly credited to the synergy between constituent algorithms, which should complement each other either over a set of problems, or during different stages of an optimization process.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5439827,no
Applying an effective model for VNPT CDN,"Most operations of Content Distribution Network (CDN) have measured to evaluate the ability to serve users with content or services they want. Activity measurement process provides the ability to predict, monitor and ensure activities throughout the CDN. Five parameters or regular measurement units are often used by content providers to evaluate the operation of CDN including Cache hit ratio, reserved bandwidth, latency, surrogate server utilization and reliability. There are many ways to measure CDN activities, one which use simulation tools. The simulation CDN is implemented using software tools that have value for research and development, internal testing and diagnostic CDN performance, because of accessing real CDN traces and logs is not easy due to the proprietary nature of commercial CDN. In this article, we will apply a CDN simulation model (based on [CDNSim; 2007]) for design a CDN based on the network infrastructure of Vietnam Posts and Telecommunications Group (VNPT Network).",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5440141,no
Trends in Firewall Configuration Errors: Measuring the Holes in Swiss Cheese,"The first quantitative evaluation of the quality of corporate firewall configurations appeared in 2004, based on Check Point Firewall-1 rule sets. In general, that survey indicated that corporate firewalls often enforced poorly written rule sets. This article revisits the first survey. In addition to being larger, the current study includes configurations from two major vendors. It also introduces a firewall complexity. The study's findings validate the 2004 study's main observations: firewalls are (still) poorly configured, and a rule -set's complexity is (still) positively correlated with the number of detected configuration errors. However, unlike the 2004 study, the current study doesn't suggest that later software versions have fewer errors.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5440153,no
Design of fault tolerant system based on runtime behavior tracing,"The current researches to improve the reliability of operating systems have been focusing on the evolution of kernel architecture or protecting against device driver errors. In particularly, the device driver errors are critical to the most of the complementary operating systems that have a kernel level device driver. Especially on special purpose embedded system, because of its limited resources and variety of devices, more serious problems are induced. Preventing data corruption or blocking the arrogation of operational level is not enough to cover the entire problems. For examples, when using device drivers, the violation of function's call sequence can cause a malfunction. Also a violation of behavior rules on system level involves the same problem. This type of errors is difficult to be detected by the previous methods. Accordingly, we designed a system that traces system behavior at runtime and recovers optimally when errors are detected. We experimented in Linux 2.6.24 kernel operating on GP2X-WIZ mobile game player.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5440261,no
Assessing communication media richness in requirements negotiation,"A critical claim in software requirements negotiation regards the assertion that group performances improve when a medium with different richness level is used. Accordingly, the authors have conducted a study to compare traditional face-to-face communication, the richest medium and two less rich communication media, namely a distributed three-dimensional virtual environment and a text-based structured chat. This comparison has been performed with respect to the time needed to accomplish a negotiation. Furthermore, as the only assessment of the time could not be meaningful, the authors have also analysed the media effect on the issues arisen in the negotiation process and the quality of the negotiated software requirements.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5440854,no
Introducing Queuing Network-Based Performance Awareness in Autonomic Systems,"This paper advocates for the introduction of performance awareness in autonomic systems. The motivation is to be able to predict the performance of a target configuration when a self-* feature is planning a system reconfiguration.We propose a global and partially automated process based on queues and queuing networks models. This process includes decomposing a distributed application into black boxes, identifying the queue model for each black box and assembling these models into a queuing network according to the candidate target configuration. Finally, performance prediction is performed either through simulation or analysis.This paper sketches the global process and focuses on the black box model identification step. This step is automated thanks to a load testing platform enhanced with a workload control loop. Model identification is then based on statistical tests. The model identification process is illustrated by experimental results.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5442613,no
A Cubic 3-Axis Magnetic Sensor Array for Wirelessly Tracking Magnet Position and Orientation,"In medical diagnoses and treatments, e.g., endoscopy, dosage transition monitoring, it is often desirable to wirelessly track an object that moves through the human GI tract. In this paper, we propose a magnetic localization and orientation system for such applications. This system uses a small magnet enclosed in the object to serve as excitation source, so it does not require the connection wire and power supply for the excitation signal. When the magnet moves, it establishes a static magnetic field around, whose intensity is related to the magnet's position and orientation. With the magnetic sensors, the magnetic intensities in some predetermined spatial positions can be detected, and the magnet's position and orientation parameters can be computed based on an appropriate algorithm. Here, we propose a real-time tracking system developed by a cubic magnetic sensor array made of Honeywell 3-axis magnetic sensors, HMC1043. Using some efficient software modules and calibration methods, the system can achieve satisfactory tracking accuracy if the cubic sensor array has enough number of 3-axis magnetic sensors. The experimental results show that the average localization error is 1.8 mm.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5443691,no
An Efficient Duplicate Detection System for XML Documents,"Duplicate detection, which is an important subtask of data cleaning, is the task of identifying multiple representations of a same real-world object and necessary to improve data quality. Numerous approaches both for relational and XML data exist. As XML becomes increasingly popular for data exchange and data publishing on the Web, algorithms to detect duplicates in XML documents are required. Previous domain independent solutions to this problem relied on standard textual similarity functions (e.g., edit distance, cosine metric) between objects. However, such approaches result in large numbers of false positives if we want to identify domain-specific abbreviations and conventions. In this paper, we present the process of detecting duplicate includes three modules, such as selector, preprocessor and duplicate identifier which uses XML documents and candidate definition as input and produces duplicate objects as output. The aim of this research is to develop an efficient algorithm for detecting duplicate in complex XML documents and to reduce number of false positive by using MD5 algorithm. We illustrate the efficiency of this approach on several real-world datasets.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5445601,no
Performance Evaluation of the Judicial System in Taiwan Using Data Envelopment Analysis and Decision Trees,"A time-honored maxim says that the judicial system is the last line of defending justice. Its performance has a great impact on how the citizen trust or distrust their state apparatus in a democracy. Technically speaking, the judicial process and its procedures are very complicated and the purpose of the whole system is to go through the law and due process to protect civil liberties and rights and to defend the public good of the nation. Therefore, it is worthwhile to assess the performance of judicial institutions in order to advance the efficiency and quality of judicial verdict. This paper combines data envelopment analysis (DEA) and decision trees to achieve this objective. In particular, DEA is first of all used to evaluate the relative efficiency of 18 district courts in Taiwan. Then, the efficiency scores and the overall efficiency of each decision making units are then used to train a decision tree model. Specifically, C5.0, CART, and CHAID decision trees are constructed for comparisons. The decision rules in the best decision tree model can be used to distinguish between efficient units and inefficient units and allow us to understand important factors affecting the efficiency of judicial institutions. The experimental result shows that C5.0 performs the best for predicting (in) efficient judicial institutions, which provides 80.37% average accuracy.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5445658,no
RFOH: A New Fault Tolerant Job Scheduler in Grid Computing,"The goal of grid computing is to aggregate the power of widely distributed resources. Considering that the probability of failure is great in such systems, fault tolerance has become a crucial area in computational grid. In this paper, we propose a new strategy named RFOH for fault tolerant job scheduling in computational grid. This strategy maintains the history of fault occurrence of resources in Grid Information Server (GIS). Whenever a resource broker has jobs to schedule, it uses this information in Genetic Algorithm and finds a near optimal solution for the problem. Further, it increases the percentage of jobs executed within specified deadline. The experimental result shows that we can have a combination of user satisfaction and reliability. Using checkpoint techniques, the proposed strategy can make grid scheduling more reliable and efficient.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5445793,no
Model-based validation of safety-critical embedded systems,"Safety-critical systems have become increasingly software reliant and the current development process of Abuild, then integrateA has become unaffordable. This paper examines two major contributors to today's exponential growth in cost: system-level faults that are not discovered until late in the development process; and multiple truths of analysis results when predicting system properties through model-based analysis and validating them against system implementations. We discuss the root causes of such system-level problems, and an architecture-centric model-based analysis approach of different operational quality aspects from an architecture model. A key technology is the SAE Architecture Analysis & Design Language (AADL) standard for embedded software-reliant system. It supports a single source approach to analysis of operational qualities such as responsiveness, safety-criticality, security, and reliability through model annotations. The paper concludes with a summary of an industrial case study that demonstrates the feasibility of this approach.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5446809,no
Development of fault detection and reporting for non-central maintenance aircraft,"This paper describes how real-time faults can be automatically detected in Boeing 737 airplanes without significant hardware or software modifications, or potentially expensive system re-certification by employing a novel approach to Airplane Conditioning and Monitoring System (ACMS) usage. The ACMS is a function of the Digital Flight Data Acquisition Unit (DFDAU), which also collects aircraft parameters and transmits them to the Flight Data Recorder (FDR). The DFDAU receives digital and analog data from various airplane subsystems, which is also available to the ACMS. Exploiting customized ACMS software allows airline operators to specify collection and processing of various aircraft parameters for flight data monitoring, maintenance, and operational efficiency trending. Employing a rigorous systems engineering approach with detailed signal analysis, fault detection algorithms are created for software implementation within the ACMS to support ground-based reporting systems. To date, over 160 algorithms are in development based upon the existing Fault Reporting and Fault Isolation Manual (FRM/FIM) structure and availability of system signals for individual faults. Following successful field-testing and implementation, 737 airplane customers have access to a state of fault detection automation not previously available on aircraft without central maintenance monitoring.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5446830,no
A dual use fiber optic technology for enabling health management,"Advanced diagnostic and prognostic technology promises to reduce support costs and further improve safety of flight. The ability to detect fault precursors and predict remaining useful life can provide longer periods of uninterrupted operations as well as reduce support costs and improve crew response to degrading conditions. Two major barriers to realizing this potential include the cost of implementation in legacy and new aircraft and the lack of data to develop or mature algorithms. The cost of incorporating the additional sensing, data collection, processing and communications hardware into legacy aircraft is typically prohibitive, particularly if qualification/requalification of hardware and software are required. Boeing and Killdeer Mountain Manufacturing (KMM) developed a dual use technology to enable the low cost and footprint implementation of health management systems. The Chafing Protection System (CHAPS) uses optical fiber within wire bundles to detect and locate the wire chafing. An Optical Time Domain Reflectometry (OTDR) is used to locate the source of the chafing. Boeing has developed technology to also make use of the CHAPS fiber to communicate health management data onboard a vehicle. This technology includes a split ring connector which enables the implementation of a low cost, support critical, high bandwidth data network for health management.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5446838,no
Cassini spacecraft's in-flight Fault Protection redesign for unexpected regulator malfunction,"After the launch of the Cassini ?Mission-to-Saturn? Spacecraft, the volume of subsequent mission design modifications was expected to be minimal due to the rigorous testing and verification of the Flight Hardware and Flight Software. For known areas of risk where faults could potentially occur, component redundancy and/or autonomous Fault Protection (FP) routines were implemented to ensure that the integrity of the mission was maintained. The goal of Cassini's FP strategy is to ensure that no credible Single Point Failure (SPF) prevents attainment of mission objectives or results in a significantly degraded mission, with the exception of the class of faults which are exempted due to low probability of occurrence. In the case of Cassini's Propulsion Module Subsystem (PMS) design, a waiver was approved prior to launch for failure of the prime regulator to properly close; a potentially mission catastrophic single point failure. However, one month after Cassini's launch when the fuel & oxidizer tanks were pressurized for the first time, the prime regulator was determined to be leaking at a rate significant enough to require a considerable change in Main Engine (ME) burn strategy for the remainder of the mission. Crucial mission events such as the Saturn Orbit Insertion (SOI) burn task which required a characterization exercise for the PMS system 30 days before the maneuver were now impossible to achieve. This paper details the steps that were necessary to support the unexpected malfunction of the prime regulator, the introduction of new failure modes which required new FP design changes consisting of new/modified under-pressure & over-pressure algorithms; all which must be accomplished during the operation phase of the spacecraft, as a result of a presumed low probability, waived failure which occurred after launch.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5446845,no
Resilient Critical Infrastructure Management Using Service Oriented Architecture,"The SERSCIS project aims to support the use of interconnected systems of services in Critical Infrastructure (CI) applications. The problem of system interconnectedness is aptly demonstrated by 'Airport Collaborative Decision Making' (A-CDM). Failure or underperformance of any of the interlinked ICT systems may compromise the ability of airports to plan their use of resources to sustain high levels of air traffic, or to provide accurate aircraft movement forecasts to the wider European air traffic management systems. The proposed solution is to introduce further SERSCIS ICT components to manage dependability and interdependency. These use semantic models of the critical infrastructure, including its ICT services, to identify faults and potential risks and to increase human awareness of them. Semantics allows information and services to be described in such a way that makes them understandable to computers. Thus when a failure (or a threat of failure) is detected, SERSCIS components can take action to manage the consequences, including changing the interdependency relationships between services. In some cases, the components will be able to take action autonomously -- e.g. to manage 'local' issues such as the allocation of CPU time to maintain service performance, or the selection of services where there are redundant sources available. In other cases the components will alert human operators so they can take action instead. The goal of this paper is to describe a Service Oriented Architecture (SOA) that can be used to address the management of ICT components and interdependencies in critical infrastructure systems.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5447380,no
Improving the Effectiveness and the Efficiency of the Some Operations in Maintenance Processes Using Dynamic Taxonomies,"The purpose of this work is to increase the effectiveness and the efficiency of some operations carried out during the activities in the aeronautical maintenance and transformation processes. In particular, we examine the Non-Routine Card (NRC) Resolution and the Activity Planning Processes. An NRC is a fault, not expected, detected during the maintenance/transformation operations. The costs and the efforts of the NRC management are of the same order of magnitude of the a priori scheduled activities. The process requires that corrective actions come from applicable technical documentation. There are many kinds of defects and technical documentation does not cover all possible cases. Often, we observed the operators refer to similar NRC solved in the past. Therefore, the operators browse the set of NRCs. We found two major issues: the enormous number of NRC and the lack of a shared and concrete definition of similarity between NRCs. The latter is due to a distinctive feature of the NRCs. The operators find AsimilarA NRCs using their expertise. On other hand, the Activity Planning process requires the periodic re-calculation of the optimal schedule of the maintenance activities due to the presence of NRCs. The upgrading of the schedule is a very time consuming task. NRC management and activity planning share common problems. Both of them require searching and browsing large set of items by specialized technicians. We propose a common approach for the two problems. The application we developed relies on the concept of dynamic taxonomy.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5447400,no
Wavelet Coherence and Fuzzy Subtractive Clustering for Defect Classification in Aeronautic CFRP,"Despite their high specific stiffness and strength, carbon fiber reinforced polymers, stacked at different fiber orientations, are susceptible to interlaminar damages. They may occur in the form of micro-cracks and voids, and leads to a loss of performance. Within this framework, ultrasonic tests can be exploited in order to detect and classify the kind of defect. The main object of this work is to develop the evolution of a previous heuristic approach, based on the use of Support Vector Machines, proposed in order to recognize and classify the defect starting from the measured ultrasonic echoes. In this context, a real-time approach could be exploited to solve real industrial problems with enough accuracy and realistic computational efforts. Particularly, we discuss the cross wavelet transform and wavelet coherence for examining relationships in time-frequency domains between. For our aim, a software package has been developed, allowing users to perform the cross wavelet transform, the wavelet coherence and the Fuzzy Inference System. Since the ill-posedness of the inverse problem, Fuzzy Inference has been used to regularize the system, implementing a data-independent classifier. Obtained results assure good performances of the implemented classifier, with very interesting applications.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5447413,no
Localized QoS Routing with Admission Control for Congestion Avoidance,"Localized Quality of Service (QoS) routing has been recently proposed for supporting the requirements of multimedia applications and satisfying QoS constraints. Localized algorithms avoid the problems associated with the maintenance of global network state by using statistics of flow blocking probabilities. Using local information for routing avoids the overheads of global information with other nodes. However, localized QoS routing algorithms perform routing decisions based on information updated from path request to path request. This paper proposes to tackle a combined localized routing and admission control in order to avoid congestion. We introduce a new Congestion Avoidance Routing algorithm (CAR) in localized QoS routing which make a decision of routing in each connection request using an admission control to route traffic away from congestion. Simulations of various network topologies are used to illustrate the performance of the CAR. We compare the performance of the CAR algorithm against the Credit Based Routing (CBR) algorithm and the Quality Based Routing (QBR) under various ranges of traffic loads.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5447423,no
Large Scale Disaster Information System Based on P2P Overlay Network,"When a large-scale disaster occurs, information sharing among administration, residents, and volunteers is indispensable. However, as reported, a lot of examples show that it is difficult to use well. The Disaster Information System is not built on the infrastructure which the system failure was considered at the disaster is nominated for the cause. In this study, we focus on the operation usability of the Disaster Information Sharing Systems works at each area, and share the resources of those systems with the P2P overlay network, by decentralizing and integrating the disaster information to realize the redundancy of the system. For the disorder between the nodes and the communication links, we propose a mechanism to detect the faults in order to improve the robustness of the system.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5447435,no
Fault Tolerance and Recovery in Grid Workflow Management Systems,"Complex scientific workflows are now commonly executed on global grids. With the increasing scale complexity, heterogeneity and dynamism of grid environments the challenges of managing and scheduling these workflows are augmented by dependability issues due to the inherent unreliable nature of large-scale grid infrastructure. In addition to the traditional fault tolerance techniques, specific checkpoint-recovery schemes are needed in current grid workflow management systems to address these reliability challenges. Our research aims to design and develop mechanisms for building an autonomic workflow management system that will exhibit the ability to detect, diagnose, notify, react and recover automatically from failures of workflow execution. In this paper we present the development of a Fault Tolerance and Recovery component that extends the ActiveBPEL workflow engine. The detection mechanism relies on inspecting the messages exchanged between the workflow and the orchestrated Web Services in search of faults. The recovery of a process from a faulted state has been achieved by modifying the default behavior of ActiveBPEL and it basically represents a non-intrusive checkpointing mechanism. We present the results of several scenarios that demonstrate the functionality of the Fault Tolerance and Recovery component, outlining an increase in performance of about 50% in comparison to the traditional method of resubmitting the workflow.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5447462,no
A Failure Detection System for Large Scale Distributed Systems,"Failure detection is a fundamental building block for ensuring fault tolerance in large scale distributed systems. In this paper we present an innovative solution to this problem. The approach is based on adaptive, decentralized failure detectors, capable of working asynchronous and independent on the application flow. The proposed failure detectors are based on clustering, the use of a gossip-based algorithm for detection at local level and the use of a hierarchical structure among clusters of detectors along which traffic is channeled. In this we present result proving that the system is able to scale to a large number of nodes, while still considering the QoS requirements of both applications and resources, and it includes the fault tolerance and system orchestration mechanisms, added in order to assess the reliability and availability of distributed systems in an autonomic manner.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5447464,no
A Multidimensional Array Slicing DSL for Stream Programming,"Stream languages offer a simple multi-core programming model and achieve good performance. Yet expressing data rearrangement patterns (like a matrix block decomposition) in these languages is verbose and error prone. In this paper, we propose a high-level programming language to elegantly describe n-dimensional data reorganization patterns. We show how to compile it to stream languages.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5447483,no
Fault Tolerance by Quartile Method in Wireless Sensor and Actor Networks,"Recent technological advances have lead to the emergence of wireless sensor and actor networks (WSAN) which sensors gather the information for an event and actors perform the appropriate actions. Since sensors are prone to failure due to energy depletion, hardware failure, and communication link errors, designing an efficient fault tolerance mechanism becomes an important issue in WSAN. However, most research focus on communication link fault tolerance without considering sensing fault tolerance on paper survey. In this situation, actor may perform incorrect action by receiving error sensing data. To solve this issue, fault tolerance by quartile method (FTQM) is proposed in this paper. In FTQM, it not only determines the correct data range but also sifts the correct sensors by data discreteness. Therefore, actors could perform the appropriate actions in FTQM. Moreover, FTQM also could be integrated with communication link fault tolerance mechanism. In the simulation results, it demonstrates FTQM has better predicted rate of correct data, the detected tolerance rate of temperature, and the detected temperature compared with the traditional sensing fault tolerance mechanism. Moreover, FTQM has better performance when the real correct data rate and the threshold value of failure are varied.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5447511,no
Impact of disk corruption on open-source DBMS,"Despite the best intentions of disk and RAID manufacturers, on-disk data can still become corrupted. In this paper, we examine the effects of corruption on database management systems. Through injecting faults into the MySQL DBMS, we find that in certain cases, corruption can greatly harm the system, leading to untimely crashes, data loss, or even incorrect results. Overall, of 145 injected faults, 110 lead to serious problems. More detailed observations point us to three deficiencies: MySQL does not have the capability to detect some corruptions due to lack of redundant information, does not isolate corrupted data from valid data, and has inconsistent reactions to similar corruption scenarios. To detect and repair corruption, a DBMS is typically equipped with an offline checker. Unfortunately, the MySQL offline checker is not comprehensive in the checks it performs, misdiagnosing many corruption scenarios and missing others. Sometimes the checker itself crashes; more ominously, its incorrect checking can lead to incorrect repairs. Overall, we find that the checker does not behave correctly in 18 of 145 injected corruptions, and thus can leave the DBMS vulnerable to the problems described above.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5447821,no
Mini-Me: A min-repro system for database software,"Testing and debugging database software is often challenging and time consuming. A very arduous task for DB testers is finding a min-repro - the Asimplest possible setupA that reproduces the original problem. Currently, a great deal of searching for min-repros is carried out manually using non-database-specific tools, which is both slow and error-prone. We propose to demonstrate a system, called Mini-Me<sup>1</sup>, designed to ease and speed-up the task of finding min-repros in database-related products. Mini-Me employs several effective tools, including: the novel simplification transformations, the high-level language for creating search scripts and automation, the Arecord-and-replayA functionality, and the visualization of the search space and results. In addition to the standard application mode, the system can be interacted with in the game mode. The latter can provide an intrinsically motivating environment for developing successful search strategies by DB testers, which can be data-mined and recorded as patterns and used as recommendations for DB testers in the future. Potentially, a system like Mini-Me can save hours of time (for both customers and testers to isolate a problem), which could result in faster fixes and large cost savings to organizations.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5447933,no
An improved monte carlo method in fault tree analysis,"The Monte Carlo (MC) method is one of the most general ones in system reliability analysis, because it reflects the statistical nature of the problem. It is not restricted by type of failure models of system components, allows to capture the dynamic relationship between events and estimate the accuracy of obtained results by calculating standard error. However, it is rarely used in Fault Tree (FT) software, because a huge number of trials are required to reach a tolerable precision if the value of system probability is relatively small. Regrettably, this is the most important practical case, because nowadays highly reliable systems are ubiquitous. In the present paper we study several enhancements of the raw simulation method: variance reduction, parallel computing, and improvements based on simple preliminary information about FT structure. They are efficiently developed both for static and dynamic FTs. The effectiveness and accuracy of the improved MC method is confirmed by numerous calculations of complex industrial benchmarks.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5447989,no
Efficient analysis of imperfect coverage systems with functional dependence,"Traditional approaches to handling functional dependence in systems with imperfect fault coverage are based on Markov models, which are inefficient due to the well-known state space explosion problem. Also, the Markov-based methods typically assume exponential time-to-failure distributions for the system components. In this paper we propose a new combinatorial approach to handling functional dependence in the reliability analysis of imperfect coverage systems. Based on the total probability theorem and the divide-and-conquer strategy, the approach separates the effects of functional dependence and imperfect fault coverage from the combinatorics of the system solution. The proposed approach is efficient, accurate, and has no limitation on the type of time-to-failure distributions for the system components. The application and advantages of the proposed approach are illustrated through analyses of two examples.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5447994,no
Qualitative-Quantitative Bayesian Belief Networks for reliability and risk assessment,This paper presents an extension of Bayesian belief networks (BBN) enabling use of both qualitative and quantitative likelihood scales in inference. The proposed method is accordingly named QQBBN (Qualitative-Quantitative Bayesian Belief Networks). The inclusion of qualitative scales is especially useful when quantitative data for estimation of probabilities are lacking and experts are reluctant to express their opinions quantitatively. In reliability and risk analysis such situation occurs when for example human and organizational root causes of systems are modeled explicitly. Such causes are often not quantifiable due to limitations in the state of the art and lack of proper quantitative metrics. This paper describes the proposed QQBBN framework and demonstrates its uses through a simple example.,2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5448022,no
A Novel Method of Fault Diagnosis in Wind Power Generation System,"Along with environmental consciousness enhancement, conventional energy depletion, wind energy exploitation is expanding gradually due to the renewable merit, clean without any pollution and vast reserve features. Therefore, wind power generation (WPG) system equipped with Doubly Fed Induction Generators is emerging as mushroom. Larger rated capacity of power unit, the higher tower and the variable pitch are the main scope in WPG system. However, latent trouble will bring about likewise. If a fault occurred, it will be catastrophic for WPG system. Consequently, the technology of fault detection will play a more important role in WPG system. Based on the present status, a novel method is proposed in this paper after summarizing and analyzing the lack of previous methods. Then an example for detecting the inverter fault is studied using PSCAD software. Results indicated that the proposed method is effective and feasible.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5448346,no
Research on Online Static Risk Assessment for Urban Power System,"With the rapid development of urbanization, the importance of city power grids safety has been gradually recognized. Given a full consideration for the characteristics of city power grids, we design a complete set of risk evaluation index system based on probability theory by employing risk theory and analytic hierarchy process (AHP) in power system online static security risk assessment. Further, we have developed city grid online risk assessment software, and have provided a clear description of some key technologies of implementation and calculation process after installation. Finally, the test result shows the functionality and applicability of our software.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5448732,no
Numerical Simulation of the Unsteady Flow and Power of Horizontal Axis Wind Turbine using Sliding Mesh,Horizontal axis wind turbine (hereafter HAWT) is the common equipment in wind turbine generator systems in recent years. The paper relates to the numerical simulation of the unsteady airflow around a HAWT of the type Phase VI with emphasis on the power output. The rotor diameter is 10-m and the rotating speed is 72 rpm. The simulation was undertaken with the Computational Fluid Dynamics (CFD) software FLUENT 6.2 using the sliding meshes controlled by user-defined functions (UDF). Entire mesh node number is about 1.8 million and it is generated by GAMBIT 2.2 to achieve better mesh quality. The numerical results were compared with the experimental data from NREL UAE wind tunnel test. The comparisons show that the numerical simulation using sliding meshes can accurately predict the aerodynamic performance of the wind turbine rotor.,2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5448767,no
The Research of Power Quality Real Time Monitor for Coal Power System Based on Wavelet Analysis and ARM Chip,"In order to prevent coal power system fault for safety production, a novel power quality real time monitor is researched in this paper. According to the coal power system special characteristics and safety production standard, the harmonic of the coal power system is analyzed first based on the wavelet theory, and then the monitoring system is designed with ARM LPC2132 as the client computer to fulfill the common power parameter data acquisition. The whole monitoring system is composed of the signal transform module, data processing module, communication module, host computer interfaces and their function modules. The system software is designed with the platform of LabWindowns/CVI. The research result shows that the power quality monitor can detect the harmonic states in real time.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5448921,no
Study and Realizing of Method of AC Locating Fault in Distribution System,"The idea of AC locating fault method in distribution system is that inject an AC signal into the fault phase after a single line to ground fault happened and then diagnose the fault along the transmission line with the handled AC signal detector utilizing dichotomy method until the fault is determined. The frequency of the injected AC signal used in this study is 60 Hz. Compared with the injected S signal technique, this method is called low frequency AC signal injection method. In this paper, the hardware of the signal source and the software design are introduced. The SCM managing pulse is used in the control section of the hardware, and the application of PWM control technique in this hardware is discussed in this reference; as the software design, the PWM signal is generated by coding based on the relation between the injected signal and PWM waveforms. The high frequency PWM signal excited a couple of breakers in the inversion source and then the output terminal will get high stable injected signals by filtering and generate AC signal with invariable frequency and adjustable voltage, based on which the signal detector could detect the required signal easily. The proposed signal source device reduces the difficulty of high impedance to ground detecting and improves the accuracy and reliability of locating fault. This technique, which allows the ground detecting and is convenient for engineers' operation, reduces the locating fault time, improves its efficiency and is proved by simulation and analysis on its validity.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5449036,no
Notice of Retraction<BR>Design of Synchronous Sampling System Based on ATT7022C,"Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR>Higher and higher demands of power quality detection are asked approach to the further demand to understanding of power quality. Moreover, harmonics in the grid is detected how fast and synchronous appears especially important. An approach to detect harmonics is presented in this paper. The design is mainly composed of a single ATT7022C chip and a microcontroller. Working principle is introduced firstly. Then the structure of the system is discussed. A software design approach of data acquisition, based on the chip ATT7022C and low cost DSP TMS320F2812 (F2812), is given in this paper. This design is given to verify fast speed, high detection accuracy and poor calculation.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5449116,no
Model and implementation for runtime software monitoring system,"For complicated Software-Intensive System, it is always hard to guarantee the reliability and safety of software. Effective methods for detecting faults and isolating software fault from hardware fault are desiderated especially. In order to detect and isolate fault in SIS, a method called runtime software monitoring is studied, and a new kind of runtime software monitoring system(RSMS) is constructed in this paper. The RSMS can not only detect software fault by observing software behavior to determine whether it complies with its intended behavior, but also can assist to isolate software fault from hardware fault and to locate software fault based on fault symptoms acquired by our method. The software architecture of RSMS is presented from different views by using A4+1A view model and layer architectural style. The RSMS prototype is implemented through architecture-based software development method. By applying the prototype in practice, it proved that the RSMS prototype is feasible and effective for detecting and diagnosing faults in SIS.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5451251,no
A model for early prediction of faults in software systems,"Quality of a software component can be measured in terms of fault proneness of data. Quality estimations are made using fault proneness data available from previously developed similar type of projects and the training data consisting of software measurements. To predict faulty modules in software data different techniques have been proposed which includes statistical method, machine learning methods, neural network techniques and clustering techniques. Predicting faults early in the software life cycle can be used to improve software process control and achieve high software reliability. The aim of proposed approach is to investigate that whether metrics available in the early lifecycle (i.e. requirement metrics), metrics available in the late lifecycle (i.e. code metrics) and metrics available in the early lifecycle (i.e. requirement metrics) combined with metrics available in the late lifecycle (i.e. code metrics) can be used to identify fault prone modules using decision tree based Model in combination of K-means clustering as preprocessing technique. This approach has been tested with CM1 real time defect datasets of NASA software projects. The high accuracy of testing results show that the proposed Model can be used for the prediction of the fault proneness of software modules early in the software life cycle.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5451695,no
Analysing the need for autonomic behaviour in grid computing,"With the introduction of Grid computing, complexity of large scale distributed systems has become unmanageable because of the manual system adopted for the management these days. Due to dynamic nature of grid, manual management techniques are time-consuming, in-secure and more prone to errors. This leads to new paradigm of self-management through autonomic computing to pervade over the old manual system to begin the next generation of Grid computing. In this paper, we have discussed the basic concept of grid computing and the need for grid to be autonomic. A comparative analysis of different grid middleware has been provided to show the absence of autonomic behavior in current grid architecture. To conclude the discussion, we have mentioned the areas where research work has been lacking and what we believe the community should be considering.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5451880,no
Performance-effective operation below Vcc-min,"Continuous circuit miniaturization and increased process variability point to a future with diminishing returns from dynamic voltage scaling. Operation below Vcc-min has been proposed recently as a mean to reverse this trend. The goal of this paper is to minimize the performance loss due to reduced cache capacity when operating below Vcc-min. A simple method is proposed: disable faulty blocks at low voltage. The method is based on observations regarding the distributions of faults in an array according to probability theory. The key lesson, from the probability analysis, is that as the number of uniformly distributed random faulty cells in an array increases the faults increasingly occur in already faulty blocks. The probability analysis is also shown to be useful for obtaining insight about the reliability implications of other cache techniques. For one configuration used in this paper, block disabling is shown to have on the average 6.6% and up to 29% better performance than a previously proposed scheme for low voltage cache operation. Furthermore, block-disabling is simple and less costly to implement and does not degrade performance at or above Vcc-min operation. Finally, it is shown that a victim-cache enables higher and more deterministic performance for a block-disabled cache.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5452017,no
Fault-based attack of RSA authentication,"For any computing system to be secure, both hardware and software have to be trusted. If the hardware layer in a secure system is compromised, not only it would be possible to extract secret information about the software, but it would also be extremely hard for the software to detect that an attack is underway. In this work we detail a complete end-to-end fault-attack on a microprocessor system and practically demonstrate how hardware vulnerabilities can be exploited to target secure systems. We developed a theoretical attack to the RSA signature algorithm, and we realized it in practice against an FPGA implementation of the system under attack. To perpetrate the attack, we inject transient faults in the target machine by regulating the voltage supply of the system. Thus, our attack does not require access to the victim system's internal components, but simply proximity to it. The paper makes three important contributions: first, we develop a systematic fault-based attack on the modular exponentiation algorithm for RSA. Second, we expose and exploit a severe flaw on the implementation of the RSA signature algorithm on OpenSSL, a widely used package for SSL encryption and authentication. Third, we report on the first physical demonstration of a fault-based security attack of a complete microprocessor system running unmodified production software: we attack the original OpenSSL authentication library running on a SPARC Linux system implemented on FPGA, and extract the system's 1024-bit RSA private key in approximately 100 hours.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5456933,no
HW/SW co-detection of transient and permanent faults with fast recovery in statically scheduled data paths,"This paper describes a hardware-/software-based technique to make the data path of a statically scheduled super scalar processor fault tolerant. The results of concurrently executed operations can be compared with little hardware overhead in order to detect a transient or permanent fault. Furthermore, the hardware extension allows to recover from a fault within one to two clock cycles and to distinguish between transient and permanent faults. If a permanent fault was detected, this fault is masked for the rest of the program execution such that no further time is needed for recovering from that fault. The proposed extensions were implemented in the data path of a simple VLIW processor in order to prove the feasibility and to determine the hardware overhead. Finally a reliability analysis is presented. It shows that for medium and large scaled data paths our extension provides an up to 98% better reliability than triple modular redundancy.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5456957,no
ERSA: Error Resilient System Architecture for probabilistic applications,"There is a growing concern about the increasing vulnerability of future computing systems to errors in the underlying hardware. Traditional redundancy techniques are expensive for designing energy-efficient systems that are resilient to high error rates. We present Error Resilient System Architecture (ERSA), a low-cost robust system architecture for emerging killer probabilistic applications such as Recognition, Mining and Synthesis (RMS) applications. While resilience of such applications to errors in low-order bits of data is well-known, execution of such applications on error-prone hardware significantly degrades output quality (due to high-order bit errors and crashes). ERSA achieves high error resilience to high-order bit errors and control errors (in addition to low-order bit errors) using a judicious combination of 3 key ideas: (1) asymmetric reliability in many-core architectures, (2) error-resilient algorithms at the core of probabilistic applications, and (3) intelligent software optimizations. Error injection experiments on a multi-core ERSA hardware prototype demonstrate that, even at very high error rates of 20,000 errors/second/core or 2??10<sup>-4</sup> error/cycle/core (with errors injected in architecturally-visible registers), ERSA maintains 90% or better accuracy of output results, together with minimal impact on execution time, for probabilistic applications such as K-Means clustering, LDPC decoding and Bayesian networks. Moreover, we demonstrate the effectiveness of ERSA in tolerating high rates of static memory errors that are characteristic of emerging challenges such as Vccmin problems and erratic bit errors. Using the concept of configurable reliability, ERSA platforms may also be adapted for general-purpose applications that are less resilient to errors (but at higher costs).",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5457059,no
Continuous Verification of Large Embedded Software Using SMT-Based Bounded Model Checking,"The complexity of software in embedded systems has increased significantly over the last years so that software verification now plays an important role in ensuring the overall product quality. In this context, bounded model checking has been successfully applied to discover subtle errors, but for larger applications, it often suffers from the state space explosion problem. This paper describes a new approach called continuous verification to detect design errors as quickly as possible by exploiting information from the software configuration management system and by combining dynamic and static verification to reduce the state space to be explored. We also give a set of encodings that provide accurate support for program verification and use different background theories in order to improve scalability and precision in a completely automatic way. A case study from the telecommunications domain shows that the proposed approach improves the error-detection capability and reduces the overall verification time by up to 50%.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5457776,no
Generating Test Plans for Acceptance Tests from UML Activity Diagrams,"The Unified Modeling Language (UML) is the standard to specify the structure and behaviour of software systems. The created models are a constitutive part of the software specification that serves as guideline for the implementation and the test of software systems. In order to verify the functionality which is defined within the specification documents, the domain experts need to perform an acceptance test. Hence, they have to generate test cases for the acceptance test. Since domain experts usually have a low level of software engineering knowledge, the test case generation process is challenging and error-prone. In this paper we propose an approach to generate high-level acceptance test plans automatically from business processes. These processes are modeled as UML Activity Diagrams (ACD). Our method enables the application of an all-path coverage criterion to business processes for testing software systems.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5457786,no
Dynamic Workflow Management and Monitoring Using DDS,"Large scientific computing data-centers require a distributed dependability subsystem that can provide fault isolation and recovery and is capable of learning and predicting failures to improve the reliability of scientific workflows. This paper extends our previous work on the autonomic scientific workflow management systems by presenting a hierarchical dynamic workflow management system that tracks the state of job execution using timed state machines. Workflow monitoring is achieved using a reliable distributed monitoring framework, which employs publish-subscribe middleware built upon OMG Data Distribution Service standard. Failure recovery is achieved by stopping and restarting the failed portions of workflow directed acyclic graph.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5457822,no
Time Coordination of Distance Protections Using Probabilistic Fault Trees With Time Dependencies,"Distance protection of the electrical power system is analyzed in the paper. Electrical power transmission lines are divided into sections equipped with protective relaying system. Numerical protection relays use specialized digital signal processors as the computational hardware, together with the associated software tools. The input analogue signals are converted into a digital representation and processed according to the appropriate mathematical algorithms. The distance protection is based on local and remote relays. Hazard is the event: remote circuit breaker tripping provided the local circuit breaker can be opened. Coordination of operation of protection relays in time domain is an important and difficult problem. Incorrect values of time delays of protective relays can cause the hazard. In the paper, the time settings are performed using probabilistic fault trees with time dependencies (PFTTD). PFTTD is built for the above mentioned hazard. PFTTD are used in selection of time delays of primary (local) and backup (remote) protections. Results of computations of hazard probabilities as a function of time delay are given.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5458018,no
Micro-Computed Tomography analysis methods to assess sheep cancellous bone preservation,"The goal of this study was to determine if mineral dissolution from cancellous bone specimens alters stereology parameters determined by Micro-Computed Tomography (Micro-CT). Sheep cancellous bone cores were excised from lumbar vertebrae and randomized for immediate storage in one of four AbathingA solutions - Phosphate Buffered Saline (PBS), PBS supersaturated with Hydroxyapatite (HA) (PBS+HA), PBS supplemented with Protease Inhibitor cocktail solution (PBS+PI), and Supersaturated HA PBS supplemented with Protease Inhibitor Cocktail (PBS+HA+PI). An additional sample was stored in 70% Ethanol (EtOH) to provide reference values since samples are often stored and processed by micro-CT in EtOH prior to mechanical tests. Common micro-CT parameters used to examine bone structure and quality can assess vertebral cancellous bone changes due to storage and handling. Micro-CT data were collected while the samples were stored in their storage and bathing solutions at baseline (t=0 months). Following data collection, samples were stored at -20AC for 6 months. Micro-CT data were re-collected and degradation effects on the cancellous bone specimens were evaluated in stereology parameters between samples stored in different solutions and for both time points. Specimen geometry and selected volumes used for data analysis were assessed for possible computational differences due to the software algorithms used for stereology. No differences were seen at baseline due to the specimen size or the pre-determined analysis volumes. Stereology differences were seen between bathing media groups at 6 months. Additionally, threshold values used for processing were different between the bathing solutions, reflecting changes due to solution density differences.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5458148,no
Notice of Retraction<BR>Application of Homogeneous Markov Chain in Quantitative Analysis of Teaching Achievement Indicators in Physical Education,"Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR>Markov chain is a statistical analysis method which is based on probability theory and uses random mathematical models to analyze the quantitative relationship in the course of development and changes of objects. This paper explores the application of Markov chain analysis method in the assessment of the teaching effectiveness of physical education, based on the features and requirements of teaching activities in physical education,. As a method to quantify the teaching achievement indicators in physical education, limit distribution in Markov process solves the problems which arise in the evaluation of the teaching quality by using students' score due to students at different physical levels. This provides a good method for scientific teaching evaluation in physical education.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5458935,no
Study on Fault Tree Analysis of Fuel Cell Stack Malfunction,"In order to enhance the reliability and safety of Fuel Cell Engine(FCE), combined the composition of FCE developed by our group with the electrochemical reaction mechanism of fuel cell, the fault symptom of fuel cell stack malfunction was defined and analyzed from four aspects: hardware faults, software faults, environmental and man-made factors. Then its fault tree model was established, all the common fault causes were figured out qualitatively by Fussel Algorithm and were classified as 19 minimal cut sets. At last, the happening probability of top event, important degree of probability and key importance of each basic event were quantitatively calculated. Based on the study and analysis above, several effective rectification measures implied in practical work were put forward, which can provide helpful guiding significance to the control, management and maintenance of FCE in future.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5459114,no
Notice of Retraction<BR>Research and Application of the Data Mining Technology on the Quality of Teaching Evaluation,"Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR>The assessment of the quality of teaching is in accordance with the purpose and principles of teaching. By use of the evaluation of technical feasibility of the teaching process and the expected results, the value of the judgement can be given to provide some information and make some kind of assessment on the subject which need to be assessed. On the teaching quality of teachers, there are many kinds of the evaluation criteria and different index systems. In this paper, Grey Clustering comprehensive assessment of teaching quality in connection with the computer program to assess the quality of teaching for a teacher is used. Compared with the traditional paper-assessment method, the assessment has more scientific, accurate and convincing.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5459673,no
An Optimized Algorithm for Finding Approximate Tandem Repeats in DNA Sequences,"In gene analysis, finding approximate tandem repeats in DNA sequence is an important issue. MSATR is one of the latest methods for finding those repetitions, which suffers deficiencies of runtime cost and poor result quality. This paper proposes an optimized algorithm mMSATR for detecting approximate tandem repeats in genomic sequences more efficiently. By introducing the definition of CASM to reduce the searching scope and optimizing the original mechanism adopted by MSATR, mMSATR makes the detecting process more efficient and improves the result quality. The theoretical analysis and experiment results indicates that mMSATR is able to get more results within less runtime. Algorithm mMSATR is superior to other methods in finding results, and it greatly reduces the runtime cost, which is of benefit when the gene data becomes larger.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5459976,no
Non-contact Discharge Detection System for High Voltage Equipment Based on Solar-Blind Ultraviolet Photomultiplier,"Optical radiation is a very important character signal of high-voltage equipments surface discharge, and it can be used to characterize the equipment insulation condition, but usually, the equipment surface discharge is very weak, and the light signal mainly distributes below 400 nm ultraviolet (UV) band. To detect the solar-blind band UV signal, it can not only help us to find the early discharge, but also to detect the discharge in the daytime. In this paper, a discharge UV pulse detection system was designed, first, the detection principle is introduced, then the key hardware and software parts are introduced in detail, and finally it is tested in laboratory. Experiment shows that this system has the characters of long detection distance, high-sensitivity, and can effectively find the surface discharge. So it provides a new non-contact method to inspect the discharge phenomenal of high-voltage equipment.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5460264,no
A Data Mining Model to Predict Software Bug Complexity Using Bug Estimation and Clustering,"Software defect(bug) repositories are great source of knowledge. Data mining can be applied on these repositories to explore useful interesting patterns. Complexity of a bug helps the development team to plan future software build and releases. In this paper a prediction model is proposed to predict the bug's complexity. The proposed technique is a three step method. In the first step, fix duration for all the bugs stored in bug repository is calculated and complexity clusters are created based on the calculated bug fix duration. In second step, bug for which complexity is required its estimated fix time is calculated using bug estimation techniques. And in the third step based on the estimated fix time of bug it is mapped to a complexity cluster, which defines the complexity of the bug. The proposed model is implemented using open source technologies and is explained with the help of illustrative example.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5460784,no
Tourism emergency data mining and intelligent prediction based on networking autonomic system,"This paper introduces the key technologies and tourism applications of networking autonomic system. The paper focuses especially in the theories, architectures and algorithms being used. It discusses the requirements for networking autonomic system in China and introduces a data mining and intelligent predicting system of tourism emergency based on networking autonomic system, which concentrates on the methods, such as quantum immune clone, multi-level and multi-scale prediction model and local/global coordination mechanism of agent, in tourism data mining process.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5461495,no
Object oriented design metrics and tools a survey,"The most important measure that must be considered in any software product is its design quality. The design phase takes only 5-10 % of the total effort but a large part (up to 80%) of total effort goes into correcting bad design decisions. If bad design is not fixed, the cost for fixing it after software delivery is between 5 and 100 times or higher. Researches on object oriented design metrics have produced a large number of metrics that can be measured to identify design problems and assess design quality attributes. However the use of these design metrics is limited in practice due to the difficulty of measuring and using a large number of metrics. This paper presents a survey of object-oriented design metrics. The goal of this paper is to identify a limited set of metrics that have significant impact on design quality attributes. We adopt the notion of defining design metrics as independent variables that can be measured to assess their impact on design quality attributes as dependent variables. We also present survey of existing object oriented design metrics tools that can be used to automate the measurement process. We present our conclusions on the set of important object oriented design metrics that can be assessed using these tools.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5461764,no
An approach to measure the Hurst parameter for the Dhaka University network traffic,"The main goal of this work was to analyze the network traffic of the University of Dhaka and find out the Hurst parameter to assess the degree of self similarity. For this verification a number of tests and analyses were performed on the data collected from the University Gateway router. The conclusions were supported by a rigorous statistical analysis of 7.5 millions of data packets of high quality Ethernet traffic measurements collected between Aug '07 and March'08 and the data were analyzed using both visual and statistical experimentation. Busy hour traffic and non-busy hour traffic, both were considered. All the software was coded using MATLAB and can be used as a tool to determine the inherent self similarity of a data traffic.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5461793,no
Utilizing CK metrics suite to UML models: A case study of Microarray MIDAS software,"Software metrics provide essential means for software practitioners to assess its quality. However, to assess software quality, it is important to assess its UML models because of UML wide and recent usage as an object-oriented modeling language. But the issue is which type of software metrics can be utilized on UML models. One of the most important software metrics suite is Chidamber and Kemerer metrics suite, known by CK suite. In the current work, an automated tool is developed to compute the six CK metrics by gathering the required information from class diagrams, activity diagrams, and sequence diagrams. In addition, extra information is collected from system designer, such as the relation between methods and their corresponding activity diagrams and which attributes they use. The proposed automated tool operates on XMI standard file format to provide independence from a specific UML tool. To evaluate the applicability and quality of this tool, it has been applied to two examples: an online registration system and one of the bioinformatics Microarray tools (MIDAS).",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5461798,no
On Constructing Efficient Shared Decision Trees for Multiple Packet Filters,"Multiple packet filters serving different purposes (e.g., firewalling, QoS) and different virtual routers are often deployed on a single physical router. The HyperCuts decision tree is one efficient data structure for performing packet filter matching in software. Constructing a separate HyperCuts decision tree for each packet filter is not memory efficient. A natural alternative is to construct shared HyperCuts decision trees to more efficiently support multiple packet filters. However, we experimentally show that naively classifying packet filters into shared HyperCuts decision trees may significantly increase the memory consumption and the height of the trees. To help decide which subset of packet filters should share a HyperCuts decision tree, we first identify a number of important factors that collectively impact the efficiency of the resulted shared HyperCuts decision tree. Based on the identified factors, we then propose to use machine learning techniques to predict whether any pair of packet filters should share a tree. Given the pair-wise prediction matrix, a greedy heuristic algorithm is used to classify packets filters into a number of shared HyperCuts decision trees. Our experiments using both real packets filters and synthetic packet filters show that the shared HyperCuts decision trees consume considerably less memory.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5462124,no
An extensive comparison of bug prediction approaches,"Reliably predicting software defects is one of software engineering's holy grails. Researchers have devised and implemented a plethora of bug prediction approaches varying in terms of accuracy, complexity and the input data they require. However, the absence of an established benchmark makes it hard, if not impossible, to compare approaches. We present a benchmark for defect prediction, in the form of a publicly available data set consisting of several software systems, and provide an extensive comparison of the explanative and predictive power of well-known bug prediction approaches, together with novel approaches we devised. Based on the results, we discuss the performance and stability of the approaches with respect to our benchmark and deduce a number of insights on bug prediction models.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463279,no
Assessing the precision of FindBugs by mining Java projects developed at a university,"Software repositories are analyzed to extract useful information on software characteristics. One of them is external quality. A technique used to increase software quality is automatic static analysis, by means of bug finding tools. These tools promise to speed up the verification of source code; anyway, there are still many problems, especially the high number of false positives, that hinder their large adoption in software development industry. We studied the capability of a popular bug-finding tool, FindBugs, for defect prediction purposes, analyzing the issues revealed on a repository of university Java projects. Particularly, we focused on the percentage of them that indicates actual defects with respect to their category and priority, and we ranked them. We found that a very limited set of issues have high precision and therefore have a positive impact on code external quality.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463283,no
Assessing UML design metrics for predicting fault-prone classes in a Java system,"Identifying and fixing software problems before implementation are believed to be much cheaper than after implementation. Hence, it follows that predicting fault-proneness of software modules based on early software artifacts like software design is beneficial as it allows software engineers to perform early predictions to anticipate and avoid faults early enough. Taking this motivation into consideration, in this paper we evaluate the usefulness of UML design metrics to predict fault-proneness of Java classes. We use historical data of a significant industrial Java system to build and validate a UML-based prediction model. Based on the case study we have found that level of detail of messages and import coupling-both measured from sequence diagrams, are significant predictors of class fault-proneness. We also learn that the prediction model built exclusively using the UML design metrics demonstrates a better accuracy than the one built exclusively using code metrics.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463285,no
Assessment of issue handling efficiency,"We mined the issue database of GNOME to assess how issues are handled. How many issues are submitted and resolved? Does the backlog grow or decrease? How fast are issues resolved? Does issue resolution speed increase or decrease over time? In which subproject are issues handled most efficiently? To answer such questions, we apply several visualization and quantification instruments to the raw issue data. In particular, we aggregate issues into four risk categories, based on their resolution time. These categories are the basis both for visualizing and ranking, which are used in concert for issue database exploration.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463292,no
Validity of network analyses in Open Source Projects,"Social network methods are frequently used to analyze networks derived from Open Source Project communication and collaboration data. Such studies typically discover patterns in the information flow between contributors or contributions in these projects. Social network metrics have also been used to predict defect occurrence. However, such studies often ignore or side-step the issue of whether (and in what way) the metrics and networks of study are influenced by inadequate or missing data. In previous studies email archives of OSS projects have provided a useful trace of the communication and co-ordination activities of the participants. These traces have been used to construct social networks that are then subject to various types of analysis. However, during the construction of these networks, some assumptions are made, that may not always hold; this leads to incomplete, and sometimes incorrect networks. The question then becomes, do these errors affect the validity of the ensuing analysis? In this paper we specifically examine the stability of network metrics in the presence of inadequate and missing data. The issues that we study are: 1) the effect of paths with broken information flow (i.e. consecutive edges which are out of temporal order) on measures of centrality of nodes in the network, and 2) the effect of missing links on such measures. We demonstrate on three different OSS projects that while these issues do change network topology, the metrics used in the analysis are stable with respect to such changes.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463342,no
Clones: What is that smell?,"Clones are generally considered bad programming practice in software engineering folklore. They are identified as a bad smell and a major contributor to project maintenance difficulties. Clones inherently cause code bloat, thus increasing project size and maintenance costs. In this work, we try to validate the conventional wisdom empirically to see whether cloning makes code more defect prone. This paper analyses relationship between cloning and defect proneness. We find that, first, the great majority of bugs are not significantly associated with clones. Second, we find that clones may be less defect prone than non-cloned code. Finally, we find little evidence that clones with more copies are actually more error prone. Our findings do not support the claim that clones are really a Abad smellA. Perhaps we can clone, and breathe easy, at the same time.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463343,no
THEX: Mining metapatterns from java,"Design patterns are codified solutions to common object-oriented design (OOD) problems in software development. One of the proclaimed benefits of the use of design patterns is that they decouple functionality and enable different parts of a system to change frequently without undue disruption throughout the system. These OOD patterns have received a wealth of attention in the research community since their introduction; however, identifying them in source code is a difficult problem. In contrast, metapatterns have similar effects on software design by enabling portions of the system to be extended or modified easily, but are purely structural in nature, and thus easier to detect. Our long-term goal is to evaluate the effects of different OOD patterns on coordination in software teams as well as outcomes such as developer productivity and software quality. we present THEX, a metapattern detector that scales to large codebases and works on any Java bytecode. We evaluate THEX by examining its performance on codebases with known design patterns (and therefore metapatterns) and find that it performs quite well, with recall of over 90%.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463349,no
Mutation Operators for Actor Systems,"Mutation testing is a well known technique for estimating and improving the quality of test suites. Given a test suite T for a system S, mutation testing systematically creates mutants of S and executes T to measure how many mutants T detects. If T does not detect some (non-equivalent) mutants, T can be improved by adding test cases that detect those mutants. Mutants are created by applying mutation operators. Mutation operators are important because they define the characteristics of the system that are tested as well as the characteristics that are improved in the test suite. While mutation operators are well defined for a number of programming paradigms such as sequential or multi-threaded, to the best of our knowledge, mutation operators have not been defined for the actor programming model. In this paper, we define and classify mutation operators that can be used for mutation testing of actor programs.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463644,no
Test Coverage Analysis of UML State Machines,"Software testing is a very important activity of the software development process. To expedite the testing process and improve the quality of the tests, models are increasingly used as a basis to derive test cases automatically - a technique known as model-based testing (MBT). Given a system model and a test suite derived automatically from the model or created by other process, the coverage of the model achieved by the test suite is important to assess the quality and completeness of the test suite early in the software development process. This paper presents a novel tool that shows visually the coverage achieved by a test suite on a UML state machine model. The tool receives as input a UML state machine model represented in XMI and a test suite represented in a XML format, and produces a colored UML state machine model that shows the coverage result. Model test coverage is determined by simulating the execution of the test suite over the model. An example is presented in order to show the features of the tool.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463661,no
A Demo on Using Visualization to Aid Run-Time Verification of Dynamic Service Systems,"Future software systems will be dynamic service oriented systems. Service-Oriented Architecture (SOA) provides an extensible and dynamic architecture to be used, for example, in smart environments. In such an environment, software has to adapt its behaviour dynamically. Thus, there is a need for Verifying and Validating (V & V) the adaptations at run-time. This paper contributes to that by introducing a novel visualization tool to be used with traditional V & V techniques to aid the software analysts in the verification process of dynamic software systems. When Quality of Service (QoS) of dynamic software systems varies due to the changing environment the Interactive Quality Visualization (IQVis) tool detects these changes and provides analysts an easier way of understanding the changed behaviour of the system.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463665,no
A Comparison of Constraint-Based and Sequence-Based Generation of Complex Input Data Structures,"Generation of complex input data structures is one of the challenging tasks in testing. Manual generation of such structures is tedious and error-prone. Automated generation approaches include those based on constraints, which generate structures at the concrete representation level, and those based on sequences of operations, which generate structures at the abstract representation level by inserting or removing elements to or from the structure. In this paper, we compare these two approaches for five complex data structures used in previous research studies. Our experiments show several interesting results. First, constraint-based generation can generate more structures than sequence-based generation. Second, the extra structures can lead to false alarms in testing. Third, some concrete representations of structures cannot be generated only with sequences of insert operations. Fourth, slightly different implementations of the same data structure can behave differently in testing.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463666,no
Applications of Optimization to Logic Testing,"A tradeoff exists in software logic testing between test set size and fault detection. Testers may want to minimize test set size subject to guaranteeing fault detection or they may want to maximize faults detection subject to a test set size. One way to guarantee fault detection is to use heuristics to produce tests that satisfy logic criteria. Some logic criteria have the property that they are satisfied by a test set if detection of certain faults is guaranteed by that test set. An empirical study is conducted to compare test set size and computation time for heuristics and optimization for various faults and criteria. The results show that optimization is a better choice for applications where each test has significant cost, because for a small difference in computation time, optimization reduces test set size. A second empirical study examined the percentage of faults detected in a best, random, and worst case, first for a test set size of one and then again for a test set size of ten. This study showed that if you have a limited number of tests from which to choose, the exact tests you choose have a large impact on fault detection.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463667,no
Towards Security Vulnerability Detection by Source Code Model Checking,"Security in code level is an important aspect to achieve high quality software. Various security programming guidelines are defined to improve the quality of software code. At the same time, enforcing mechanisms of these guidelines are needed. In this paper, we use source code model checking technique to check whether some security programming guidelines are followed, and correspondingly to detect related security vulnerabilities. Two SAP security programming guidelines related to logging sensitive information and Cross-Site Scripting attack are used as examples. In the case studies, Bandera Tool Set is used as source code model checker, and minimizing programmers' additional effort is set as one of the goals.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463672,no
Modelling Requirements to Support Testing of Product Lines,"The trend towards constantly growing numbers of product variants and features in industry makes the improvement of analysis and specification techniques a key efficiency enabler. The development of a single generic functional specification applicable to a whole product family can help to save costs and time to market significantly. However, the introduction of a product-line approach into a system manufacturer's electronics development process is a challenging task, prone to human error, with the risk of spreading a single fault across a whole platform of product variants. In this contribution, a combined approach on variant-management and model-based requirements analysis and validation is presented. The approach, process and tool presented are generally applicable to functional requirements analysis and specification, since informal specifications or only an abstract idea of the required function are demanded as an input. It has been experienced in several industrial projects that the presented approach may help to reduce redundancies and inconsistencies and as a consequence it may ease and improve subsequent analysis, design and testing activities. Furthermore, the application of the presented variant management approach may benefit from model-based specifications, due to their improved analysability and changeability. In this contribution we present our experiences and results using model-based and variant-management concepts for requirements specification to support system testing. Additionally, we present an extension to integrate testing into the variant-management concept. The presented approach and process are supported by the MERAN tool-suite, which has been developed as an add-in to IBM RationalDOORS.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463709,no
A Measurement Framework for Assessing Model-Based Testing Quality,This paper proposes a measurement framework for assessing the relative quality of alternative approaches to system level model-based testing. The motivation is to investigate the types of measures that the MBT community should apply. The purpose of this paper is to provide a basis for discussion by proposing some initial ideas on where we should probe for MBT quality measurement. The centerpiece of the proposal offered here is the concept of an operational profile (OP) and its relevance to model-based testing.,2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463712,no
Generating Minimal Fault Detecting Test Suites for Boolean Expressions,"New coverage criteria for Boolean expressions are regularly introduced with two goals: to detect specific classes of realistic faults and to produce as small as possible test suites. In this paper we investigate whether an approach targeting specific fault classes using several reduction policies can achieve that less test cases are generated than by previously introduced testing criteria. In our approach, the problem of finding fault detecting test cases can be formalized as a logical satisfiability problem, which can be efficiently solved by a SAT algorithm. We compare this approach with respect to the well-known MUMCUT and Minimal-MUMCUT strategies by applying it to a series of case studies commonly used as benchmarks, and show that it can reduce the number of test cases further than Minimal-MUMCUT.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463715,no
Numerical simulations of thermo-mechanical stresses during the casting of multi-crystalline silicon ingots,"Silicon is an important semiconductor substrate for manufacturing solar cells. The mechanical and electrical properties of multi-crystalline silicon (mc-Si) are primarily influenced by the quality of the feedstock material and the crystallization process. In this work, numerical calculations, applying finite element analysis (FEA) and finite volume methods (FVM) are presented, in order to predict thermo-mechanical stresses during the solidification of industrial size mc-Si ingots. A two-dimensional global model of an industrial multi-crystallization furnace was created for thermal stationary and time-dependent calculations using the software tool CrysMAS. Subsequent thermo-mechanical analyses of the silica crucible and the ingot were performed with the FEA code ANSYS, allowing additional calculations to define mechanical boundary conditions as well as material models. Our results show that thermal analyses are in good agreement with experimental measurements. Furthermore we show that our approach is suitable to describe the generation of thermo-mechanical stress within the silicon ingot.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5464525,no
Measurement and Analysis of Link Quality in Wireless Networks: An Application Perspective,"Estimating the quality of wireless link is vital to optimize several protocols and applications in wireless networks. In realistic wireless networks, link quality is generally predicted by measuring received signal strength and error rates. Understanding the temporal properties of these parameters is essential for the measured values to be representative, and for accurate prediction of performance of the system. In this paper, we analyze the received signal strength and error rates in an IEEE 802.11 indoor wireless mesh network, with special focus to understand its utility to measurement based protocols. We show that statistical distribution and memory properties vary across different links, but are predictable. Our experimental measurements also show that, due to the effect of fading, the packet error rates do not always monotonically decrease as the transmission rate is reduced. This has serious implications on many measurement-based protocols such as rate-adaptation algorithms. Finally, we describe real-time measurement framework that enables several applications on wireless testbed, and discuss the results from example applications that utilize measurement of signal strength and error rates.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5466673,no
Evolutionary Optimization of Software Quality Modeling with Multiple Repositories,"A novel search-based approach to software quality modeling with multiple software project repositories is presented. Training a software quality model with only one software measurement and defect data set may not effectively encapsulate quality trends of the development organization. The inclusion of additional software projects during the training process can provide a cross-project perspective on software quality modeling and prediction. The genetic-programming-based approach includes three strategies for modeling with multiple software projects: Baseline Classifier, Validation Classifier, and Validation-and-Voting Classifier. The latter is shown to provide better generalization and more robust software quality models. This is based on a case study of software metrics and defect data from seven real-world systems. A second case study considers 17 different (nonevolutionary) machine learners for modeling with multiple software data sets. Both case studies use a similar majority-voting approach for predicting fault-proneness class of program modules. It is shown that the total cost of misclassification of the search-based software quality models is consistently lower than those of the non-search-based models. This study provides clear guidance to practitioners interested in exploiting their organization's software measurement data repositories for improved software quality modeling.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5467094,yes
A Framework for Clustering Categorical Time-Evolving Data,"A fundamental assumption often made in unsupervised learning is that the problem is static, i.e., the description of the classes does not change with time. However, many practical clustering tasks involve changing environments. It is hence recognized that the methods and techniques to analyze the evolving trends for changing environments are of increasing interest and importance. Although the problem of clustering numerical time-evolving data is well-explored, the problem of clustering categorical time-evolving data remains as a challenging issue. In this paper, we propose a generalized clustering framework for categorical time-evolving data, which is composed of three algorithms: a drifting-concept detecting algorithm that detects the difference between the current sliding window and the last sliding window, a data-labeling algorithm that decides the most-appropriate cluster label for each object of the current sliding window based on the clustering results of the last sliding window, and a cluster-relationship-analysis algorithm that analyzes the relationship between clustering results at different time stamps. The time-complexity analysis indicates that these proposed algorithms are effective for large datasets. Experiments on a real dataset show that the proposed framework not only accurately detects the drifting concepts but also attains clustering results of better quality. Furthermore, compared with the other framework, the proposed one needs fewer parameters, which is favorable for specific applications.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5467229,no
Low-capture-power at-speed testing using partial launch-on-capture test scheme,"Most previous DFT-based techniques for low-capture-power broadside testing can only reduce test power in one of the two capture cycles, launch cycle and capture cycle. Even if some methods can reduce both of them, they may make some testable faults in standard broadside testing untestable. In this paper, a new test application scheme called partial launch-on-capture (PLOC) is proposed to solve the two problems. It allows only a part of scan flip-flops to be active in the launch cycle and capture cycle. In order to guarantee that all testable faults in the standard broadside testing can be detected in the new test scheme, extra efforts are required to check the overlapping part. In addition, calculation of the overlapping part is different from previous techniques for the stuck-at fault testing because broadside testing requires two consecutive capture cycles. Therefore, a new scan flip-flop partition algorithm is proposed to minimize the overlapping part. Sufficient experimental results are presented to demonstrate the efficiency of the proposed method.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5469590,no
Pin-count-aware online testing of digital microfluidic biochips,"On-line testing offers a promising method for detecting defects, fluidic abnormalities, and bioassay malfunctions in microfluidic biochips. To reduce product cost for disposable biochips, testing steps and functional fluidic operations must be implemented on pin-constrained designs. However, previous testing methods for pin-constrained designs do not optimize test schedules to reduce the number of control pins and test/assay completion time. We propose a pin-count-aware online testing method for pin-constrained designs to support the execution of both fault testing and the target bioassay protocol. The proposed method interleaves fault testing with the target bioassay protocol for online testing. It is aimed at significantly reducing the completion time for testing and for the bioassay, while keeping the number of control pins small. Two practical applications, namely a multiplexed bioassay and an interpolation-based mixing protocol, are used to evaluate the effectiveness of the proposed method.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5469602,no
Sustainability at Kluge Estate vineyard and winery,"Kluge Estate, a vineyard and winery in Charlottesville, Virginia, with one of the largest productions in the Commonwealth, is working to become a more sustainable business. Through implementing sustainable practices, Kluge Estate is seeking to benefit its business, the environment, and its community. However, due to a lack of relevant information about its environmental impact, Kluge Estate's decision-makers are unable to justify sustainable choices with quantified data. To resolve this problem, this paper focuses on assessing Kluge Estate's environmental impact. The Kluge Estate system is a complex combination of agriculture and manufacturing, making it difficult to assess the environmental impact throughout the life-cycle of its products. Life-cycle assessment (LCA) is a method that quantifies the environmental impact of a product or process; the life-cycle starts with the extraction of raw materials from the earth, continues through manufacturing, transportation, consumer use of the product, and concludes with disposal or recycling. To conduct the LCA, the team mapped the inputs, outputs and processes of each life-cycle stage of the Kluge Estate product Cru, an aperitif wine, with the goal of providing quantitative information about environmental impact to decision makers. SimaPro 7.1, an LCA software package, was used to perform the LCA for the production of Cru. SimaPro 7.1 utilizes databases containing comprehensive data and conversions gathered through research concerning the impacts of specific materials and processes that exist; specifically, the capstone group used the Ecoinvent Life Cycle Inventory database, which contains agricultural information. The comparison of LCA stages of a bottle of Cru shows that the disposal stage has the greatest contribution in human health (DALY) and ecosystem quality (PDF * m<sup>2</sup> * yr), but extraction has the greatest contribution in resources (MJ surplus). Further investigations into the extraction stage, compari- - ng product components, show that the glass bottle has the largest contribution in human health, due to the energy intensive process to generate new glass. The Cru has the largest impact in ecosystem quality, due to the processes needed to harvest and cut the wood as well as the generation of ethanol. The Foil has the largest contribution in resources due to the process to generate tin. A look into Kluge Estate's on-site operation shows the processes in the vineyard and winery have similar environmental impacts in human health. The major contributor in the vineyard is the Spraying of the crops due to heavy tractor and agrochemical use. The major contributors in the winery are Aging, Stabilization and Storing, which all require large amounts of electricity. Its operation measured in ecosystem quality, shows the greatest environmental impacts come from the vineyard processes, again due to the spraying.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5469654,no
A high-performance fault-tolerant software framework for memory on commodity GPUs,"As GPUs are increasingly used to accelerate HPC applications by allowing more flexibility and programmability, their fault tolerance is becoming much more important than before when they were used only for graphics. The current generation of GPUs, however, does not have standard error detection and correction capabilities, such as SEC-DED ECC for DRAM, which is almost always exercised in HPC servers. We present a high-performance software framework to enhance commodity off-the-shelf GPUs with DRAM fault tolerance. It combines data coding for detecting bit-flip errors and checkpointing for recovering computations when such errors are detected. We analyze performance of data coding in GPUs and present optimizations geared toward memory-intensive GPU applications. We present performance studies of the prototype implementation of the framework and show that the proposed framework can be realized with negligible overheads in compute intensive applications such as N-body problem and matrix multiplication, and as low as 35% in a highly-efficient memory intensive 3-D FFT kernel.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5470473,no
On the resolution of conflicts for collective pervasive context-aware applications,"The main goal of this ongoing dissertation is to define and evaluate an efficient and flexible methodology to detect and solve collective conflicts for pervasive context-aware applications. The solution proposed is flexible enough to be used by applications with different characteristics and also considers the resource constraints, which are typical in pervasive systems. One of the basic motivations to the development of this work is the existence of a great number of collective context-aware applications, specially the ones related to the pervasive computing area. Besides, to the best of the author's knowledge, until now there is no work in literature that could be applied to many different applications, and that also considers systematically the trade-off between quality of services (QoS) and resource consumption. In this work, QoS means the users' satisfaction with the application's tasks. A user is considered satisfied when he/she can perform the tasks he/she has previously demanded.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5470568,no
Resilient image sensor networks in lossy channels using compressed sensing,"Data loss in wireless communications greatly affects the reconstruction quality of wirelessly transmitted images. Conventionally, channel coding is performed at the encoder to enhance recovery of the image by adding known redundancy. While channel coding is effective, it can be very computationally expensive. For this reason, a new mechanism of handling data losses in wireless multimedia sensor networks (WMSN) using compressed sensing (CS) is introduced in this paper. This system uses compressed sensing to detect and compensate for data loss within a wireless network. A combination of oversampling and an adaptive parity (AP) scheme are used to determine which CS samples contain bit errors, remove these samples and transmit additional samples to maintain a target image quality. A study was done to test the combined use of adaptive parity and compressive oversampling to transmit and correctly recover image data in a lossy channel to maintain Quality of Information (QoI) of the resulting images. It is shown that by using the two components, an image can be correctly recovered even in a channel with very high loss rates of 10%. The AP portion of the system was also tested on a software defined radio testbed. It is shown that by transmitting images using a CS compression scheme with AP error detection, images can be successfully transmitted and received even in channels with very high bit error rates.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5470604,no
Comparison of exact static and dynamic Bayesian context inference methods for activity recognition,"This paper compares the performance of inference in static and dynamic Bayesian Networks. For the comparison both kinds of Bayesian networks are created for the exemplary application activity recognition. Probability and structure of the Bayesian Networks have been learnt automatically from a recorded data set consisting of acceleration data observed from an inertial measurement unit. Whereas dynamic networks incorporate temporal dependencies which affect the quality of the activity recognition, inference is less complex for dynamic networks. As performance indicators recall, precision and processing time of the activity recognition are studied in detail. The results show that dynamic Bayesian Networks provide considerably higher quality in the recognition but entail longer processing times.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5470671,no
Experimental responsiveness evaluation of decentralized service discovery,"Service discovery is a fundamental concept in service networks. It provides networks with the capability to publish, browse and locate service instances. Service discovery is thus the precondition for a service network to operate correctly and for the services to be available. In the last decade, decentralized service discovery mechanisms have become increasingly popular. Especially in ad-hoc scenarios - such as ad-hoc wireless networks - they are an integral part of auto-configuring service networks. Albeit the fact that auto-configuring networks are increasingly used in application domains where dependability is a major issue, these environments are inherently unreliable. In this paper, we examine the dependability of decentralized service discovery. We simulate service networks that are automatically configured by Zeroconf technologies. Since discovery is a time-critical operation, we evaluate responsiveness - the probability to perform some action on time even in the presence of faults - of domain name system (DNS) based service discovery under influence of packet loss. We show that responsiveness decreases significantly already with moderate packet loss and becomes practicably unacceptable with higher packet loss.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5470861,no
Optimizing RAID for long term data archives,We present new methods to extend data reliability of disks in RAID systems for applications like long term data archival. The proposed solutions extend existing algorithms to detect and correct errors in RAID systems by preventing accumulation of undetected errors in rarely accessed disk segments. Furthermore we show how to change the parity layout of a RAID system in order to improve the performance and reliability in case of partially defect disks. All methods benefit of a hierarchical monitoring scheme that stores reliability related information. Our proposal focuses on methods that do not need significant hardware changes.,2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5470870,no
An extension of GridSim for quality of service,GridSim is a well known and useful open software product through which users can simulate a Grid environment. At present Qualities of Service are not modeled in GridSim. When utilising a Grid a user may wish to make decisions about type of service to be contracted. For instance performance and security are two levels of service upon which different decisions may be made. Subsequently during operation a grid may not be able to fulfill its contractual obligations. In this case renegotiation is necessary. This paper describes an extension to GridSim that enables various Qualities of Service to be modeled together with Service Level Agreements and renegotiation of contract with associated costs. The extension is useful as it will allow users to make better estimates of potential costs and will also enable grid service suppliers to more accurately predict costs and thus provide better service to users.,2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5471948,no
The Study on the Fixed End Wave in Magnetostrictive Position Sensor,"The magnetostrictive position sensor is a kind of displacement sensor utilizing the magnetostrictive effect and inverse effect of magnetostrictive material. This essay discussed the influence of the fixed end of the sensor system on the detected signal with the driver impulse. The fixed end waves (a kind of elastic wave) were described and defined in this essay. The mechanisms of torsional magnetic field on the long magnetostrictive material line and fixed end waves were discussed, and relative theory models were constructed in this paper. Experiments showed that the fixed end wave of the system could be generated and transmit along the line with impulse current, which is also a kind of noise wave should be removed or weakened. Consequently, this essay should provide the theory basis and data for promoting the signal quality of the signal detection of the sensor.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5473256,no
Mining Frequent Patterns from Software Defect Repositories for Black-Box Testing,"Software defects are usually detected by inspection, black-box testing or white-box testing. Current software defect mining work focuses on mining frequent patterns without distinguishing these different kinds of defects, and mining with respect to defect type can only give limited guidance on software development due to overly broad classification of defect type. In this paper, we present four kinds of frequent patterns from defects detected by black-box testing (called black-box defect) based on a kind of detailed classification named ODC-BD (Orthogonal Defect Classification for Blackbox Defect). The frequent patterns include the top 10 conditions (data or operation) which most easily result in defects or severe defects, the top 10 defect phenomena which most frequently occur and have a great impact on users, association rules between function modules and defect types. We aim to help project managers, black-box testers and developers improve the efficiency of software defect detection and analysis using these frequent patterns. Our study is based on 5023 defect reports from 56 large industrial projects and 2 open source projects.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5473578,no
Voice Quality in VoIP Networks Based on Random Neural Networks,"The growth of Internet has led to the development of many new applications and technologies. Voice over Internet Protocol (VoIP) is one of the fastest growing applications. Calculating the quality of calls has been a complex task. The ITU E-Model gives a framework to measure quality of VoIP calls but the MOS element is a subjective measure. In this paper, we discuss a novel method using Random Neural Network (RNN) to accurately predict the perceived quality of voice and more importantly to perform this on real-time traffic to overcome the drawbacks of available methods. The novelty of this model is that RNN model provides a non-intrusive method to accurately predict and monitor perceived voice quality for both listening and conversational voice. This method has learning capabilities and this makes it possible for it to adapt to any network changes without human interference. Our novel model uses three input variables (neurons) delay, jitter, and packet loss and the codec used was G711.a. Results show a good degree of accuracy in calculating Mean Option Score (MOS), compared to Perceptual Evaluation of Speech Quality (PESQ) algorithm. WAN emulation software WANem was used to generate different samples for testing and training the RNN.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5473955,no
Designing Modular Hardware Accelerators in C with ROCCC 2.0,"While FPGA-based hardware accelerators have repeatedly been demonstrated as a viable option, their programmability remains a major barrier to their wider acceptance by application code developers. These platforms are typically programmed in a low level hardware description language, a skill not common among application developers and a process that is often tedious and error-prone. Programming FPGAs from high level languages would provide easier integration with software systems as well as open up hardware accelerators to a wider spectrum of application developers. In this paper, we present a major revision to the Riverside Optimizing Compiler for Configurable Circuits (ROCCC) designed to create hardware accelerators from C programs. Novel additions to ROCCC include (1) intuitive modular bottom-up design of circuits from C, and (2) separation of code generation from specific FPGA platforms. The additions we make do not introduce any new syntax to the C code and maintain the high level optimizations from the ROCCC system that generate efficient code. The modular code we support functions identically as software or hardware. Additionally, we enable user control of hardware optimizations such as systolic array generation and temporal common subexpression elimination. We evaluate the quality of the ROCCC 2.0 tool by comparing it to hand-written VHDL code. We show comparable clock frequencies and a 18% higher throughput. The productivity advantages of ROCCC 2.0 is evaluated using the metrics of lines of code and programming time showing an average of 15 improvement over hand-written VHDL.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5474060,no
Codesign and Simulated Fault Injection of Safety-Critical Embedded Systems Using SystemC,"The international safety standard IEC-61508 highly recommends fault injection techniques in all steps of the development process of safety-critical embedded systems, in order to analyze the reaction of the system in a faulty environment and to validate the correct implementation of fault tolerance mechanisms. Simulated fault injection enables an early dependability assessment that reduces the risk of late discovery of safety related design pitfalls and enables the analysis of fault tolerance mechanisms at each design refinement step using techniques such as failure mode and effect analysis. This paper presents a SystemC based executable modeling approach for the codesign and early dependability assessment by means of simulated fault injection of safety-critical embedded systems, which reduces the gap between the abstractions at which the system is designed and assessed. The effectiveness of this approach is examined in a train on-board safety-critical odometry example, which combines fault tolerance and sensor-fusion.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5474177,no
Early Consensus in Message-Passing Systems Enriched with a Perfect Failure Detector and Its Application in the Theta Model,"While lots of consensus algorithms have been proposed for crash-prone asynchronous message-passing systems enriched with a failure detector of the class  (the class of eventual leader failure detectors), very few algorithms have been proposed for systems enriched with a failure detector of the class P (the class of perfect failure detectors). Moreover, (to the best of our knowledge) the early decision and stopping notion has not been investigated in such systems. This paper presents an early-deciding/stopping P-based consensus algorithm. A process that does not crash decides (and stops) in at most min(f+2, t+1) rounds, where t is the maximum number of processes that may crash, and f the actual number of crashes (0 f t). Differently from what occurs in a synchronous system, a perfect failure detector notifies failures asynchronously. This makes the design of an early deciding (and stopping) algorithm not trivial. Interestingly enough, the proposed algorithm meets the lower bound on the number of rounds for early decision in synchronous systems. In that sense, it is optimal. The paper then presents an original algorithm that implements a perfect failure detector in the Theta model, an interesting model that achieves some form of synchrony without relying on physical clocks. Hence, the stacking of these algorithms provides an algorithm that solves consensus in the Theta model in min(f+2, t+1) communication rounds, i.e., in two rounds when there are no failures, which is clearly optimal.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5474189,no
Towards Understanding the Importance of Variables in Dependable Software,"A dependable software system contains two important components, namely, error detection mechanisms and error recovery mechanisms. An error detection mechanism attempts to detect the existence of an erroneous software state. If an erroneous state is detected, an error recovery mechanism will attempt to restore a correct state. This is done so that errors are not allowed to propagate throughout a software system, i.e., errors are contained. The design of these software artefacts is known to be very difficult. To detect and correct an erroneous state, the values held by some important variables must be ensured to be suitable. In this paper we develop an approach to capture the importance of variables in dependable software systems. We introduce a novel metric, called importance, which captures the impact a given variable has on the dependability of a software system. The importance metric enables the identification of critical variables whose values must be ensured to be correct.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5474191,no
Emulation of Transient Software Faults for Dependability Assessment: A Case Study,"Fault Tolerance Mechanisms (FTMs) are extensively used in software systems to counteract software faults, in particular against faults that manifest transiently, namely Mandelbugs. In this scenario, Software Fault Injection (SFI) plays a key role for the verification and the improvement of FTMs. However, no previous work investigated whether SFI techniques are able to emulate Mandelbugs adequately. This is an important concern for assessing critical systems, since Mandelbugs are a major cause of failures, and FTMs are specifically tailored for this class of software faults. In this paper, we analyze an existing state-of-the-art SFI technique, namely G-SWFIT, in the context of a real-world fault-tolerant system for Air Traffic Control (ATC). The analysis highlights limitations of G-SWFIT regarding its ability to emulate the transient nature of Mandelbugs, because most of injected faults are activated in the early phase of execution, and they deterministically affect process replicas in the system. We also notice that G-SWFIT leaves untested the 35% of states of the considered system. Moreover, by means of an experiment, we show how emulation of Mandelbugs is useful to improve SFI. In particular, we emulate concurrency faults, which are a critical sub-class of Mandelbugs, in a fully representative way. We show that proper fault triggering can increase the confidence in FTMs' testing, since it is possible to reduce the amount of untested states down to 5%.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5474200,no
Software Fault Prediction Model Based on Adaptive Dynamical and Median Particle Swarm Optimization,"Software quality prediction can play a role of importance in software management, and thus in improve the quality of software systems. By mining software with data mining technique, predictive models can be induced that software managers the insights they need to tackle these quality problems in an efficient way. This paper deals with the adaptive dynamic and median particle swarm optimization (ADMPSO) based on the PSO classification technique. ADMPSO can act as a valid data mining technique to predict erroneous software modules. The predictive model in this paper extracts the relationship rules of software quality and metrics. Information entropy approach is applied to simplify the extraction rule set. The empirical result shows that this method set of rules can be streamlined and the forecast accuracy can be improved.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5474404,no
Applications of Support Vector Mathine and Unsupervised Learning for Predicting Maintainability Using Object-Oriented Metrics,"Importance of software maintainability is increasing leading to development of new sophisticated techniques. This paper presents the applications of support vector machine and unsupervised learning in software maintainability prediction using object-oriented metrics. In this paper, the software maintainability predictor is performed. The dependent variable was maintenance effort. The independent variable were five OO metrics decided clustering technique. The results showed that the Mean Absolute Relative Error (MARE) was 0.218 of the predictor. Therefore, we found that SVM and clustering technique were useful in constructing software maintainability predictor. Novel predictor can be used in the similar software developed in the same environment.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5474411,no
Wireless Intrusion Detection System Using a Lightweight Agent,"The exponential growth in wireless network faults, vulnerabilities, and attacks make the Wireless Local Area Network (WLAN) security management a challenging research area. Deficiencies of security methods like cryptography (e.g. WEP) and firewalls, causes the use of more complex security systems, such as Intrusion Detection Systems, to be crucial. In this paper, we present a hybrid wireless intrusion detection system (WIDS). To implement the WIDS, we designed a simple lightweight agent. The proposed agent detect the most destroying and serious attacks; Man-In-The-Middle and Denial-of-Service; with the minimum selected feature set. To evaluate our proposed WIDS and its agent, we collect a complete data-set using open source attack generator softwares. Experimental results show that in comparison with similar systems, in addition of more simplicity, our WIDS provides high performance and precision.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5474532,no
Exploiting Spectrum Usage Patterns for Efficient Spectrum Management in Cognitive Radio Networks,"A cognitive radio (CR) is very significant technology to use a spectrum dynamically in wireless communication networks. However, very little has been done on using the spectrum usage patterns to handle with the problem of spectrum allocation in dynamic spectrum access. We suggest a scheme by exploiting spectrum usage patterns for the efficient spectrum management and reduce the communication cost in cognitive radio networks (CRNs). We propose the following three factors into account: spectrum sensing scheme with a sleep mode, spectrum decision scheme with a probability of spectrum access and spectrum handoff scheme with back-off time. All factors make use of spectrum usage patterns based on the statistical information. The first factor reduces the number of spectrum sensing. The second increases the opportunity of spectrum access and the last decreases the number of spectrum handoff. First of all, our proposed spectrum management scheme considers the analysis of the spectrum usage patterns and various factors obtained from the analysis is applied to lessen the communication cost in CRNs. The simulation results show that our proposed scheme improve the efficiency of spectrum management in dynamic spectrum access.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5474718,no
Active Data Selection for Sensor Networks with Faults and Changepoints,"We describe a Bayesian formalism for the intelligent selection of observations from sensor networks that may intermittently undergo faults or changepoints. Such active data selection is performed with the goal of taking as few observations as necessary in order to maintain a reasonable level of uncertainty about the variables of interest. The presence of faults/changepoints is not always obvious and therefore our algorithm must first detect their occurrence. Having done so, our selection of observations must be appropriately altered. Faults corrupt our observations, reducing their impact; changepoints (abrupt changes in the characteristics of data) may require the transition to an entirely different sampling schedule. Our solution is to employ a Gaussian process formalism that allows for sequential time-series prediction about variables of interest along with a decision theoretic approach to the problem of selecting observations.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5474748,no
"A Quadratic, Complete, and Minimal Consistency Diagnosis Process for Firewall ACLs","Developing and managing firewall Access Control Lists (ACLs) are hard, time-consuming, and error-prone tasks for a variety of reasons. Complexity of networks is constantly increasing, as it is the size of firewall ACLs. Networks have different access control requirements which must be translated by a network administrator into firewall ACLs. During this task, inconsistent rules can be introduced in the ACL. Furthermore, each time a rule is modified (e.g. updated, corrected when a fault is found, etc.) a new inconsistency with other rules can be introduced. An inconsistent firewall ACL implies, in general, a design or development fault, and indicates that the firewall is accepting traffic that should be denied or vice versa. In this paper we propose a complete and minimal consistency diagnosis process which has worst-case quadratic time complexity with the number of rules in a set of inconsistent rules. There are other proposals of consistency diagnosis algorithms. However they have different problems which can prevent their use with big, real-life, ACLs: on the one hand, the minimal ones have exponential worst-case time complexity; on the other hand, the polynomial ones are not minimal.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5474827,no
In Situ Software Visualisation,"Software engineers need to design, implement, comprehend and maintain large and complex software systems. Awareness of information about the properties and state of individual artifacts, and the process being enacted to produce them, can make these activities less error-prone and more efficient. In this paper we advocate the use of code colouring to augment development environments with rich information overlays. These in situ visualisations are delivered within the existing IDE interface and deliver valuable information with minimal overhead. We present CODERCHROME, a code colouring plug-in for Eclipse, and describe how it can be used to support and enhance software engineering activities.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5475058,no
Managing Structure-Related Software Project Risk: A New Role for Project Governance,"This paper extends recent research on the risk implications of software project organization structures by considering how structure-related risk might be managed. Projects, and other organizations involved in projects, are usually structured according to common forms. These organizational entities interact with each other, creating an environment in which risks relating to their structural forms can impact the project and its performance. This source of risk has previously been overlooked in software project research. The nature of the phenomenon is examined and an approach to managing structure-related risk is proposed, responsibility for which is assigned as a new role for project governance. This assignment is necessary because, due to the structural and relational nature of these risks, the project is poorly placed to manage such threats. The paper argues that risk management practices need to be augmented with additional analyses to identify, analyze and assess structural risks to improve project outcomes and the delivery of quality software. The argument is illustrated and initially validated with two project case studies. Implications for research and practice are drawn and directions for future research are suggested, including extending the theory to apply to other organization structures.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5475065,no
Identification and Analysis of Skype Peer-to-Peer Traffic,"More and more applications are adopting peer-to-peer (P2P) technology. Skype is a P2P based, popular VoIP software. The software works almost seamlessly across Network Address Translations (NATs) and firewalls and has better voice quality than most IM applications. The communication protocol and source code of Skype are undisclosed. It uses high strength encryption and random port number selection, which render the traditional flow identification solutions invalid. In this paper, we first obtain the Skype clients and super nodes by analyzing the process of login and calling in different network environments. Then we propose a method to identify Skype traffic based on Skype nodes and flow features. Our proposed method makes the previously hard-to-detect Skype traffic, especially voice service traffic, much easier to identify. We design an identification system utilizing the proposed method and implement the system in a LAN network. We also successfully identified Skype traffic in one of the largest Internet Providers over a period of 93 hours, during which over 30TB data were transmitted. Through experiments, we show that our proposed approach and implementations can indeed identify Skype traffic with higher accuracy and effectiveness.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5476757,no
Towards Fully Automated Test Management for Large Complex Systems,"Development of large and complex software intensive systems with continuous builds typically generates large volumes of information with complex patterns and relations. Systematic and automated approaches are needed for efficient handling of such large quantities of data in a comprehensible way. In this paper we present an approach and tool enabling autonomous behavior in an automated test management tool to gain efficiency in concurrent software development and test. By capturing the required quality criteria in the test specifications and automating the test execution, test management can potentially be performed to a great extent without manual intervention. This work contributes towards a more autonomous behavior within a distributed remote test strategy based on metrics for decision making in automated testing. These metrics optimize management of fault corrections and retest, giving consideration to the impact of the identified weaknesses, such as fault-prone areas in software.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477058,no
Searching for a Needle in a Haystack: Predicting Security Vulnerabilities for Windows Vista,"Many factors are believed to increase the vulnerability of software system; for example, the more widely deployed or popular is a software system the more likely it is to be attacked. Early identification of defects has been a widely investigated topic in software engineering research. Early identification of software vulnerabilities can help mitigate these attacks to a large degree by focusing better security verification efforts in these components. Predicting vulnerabilities is complicated by the fact that vulnerabilities are, most often, few in number and introduce significant bias by creating a sparse dataset in the population. As a result, vulnerability prediction can be thought of us preverbally searching for a needle in a haystack. In this paper, we present a large-scale empirical study on Windows Vista, where we empirically evaluate the efficacy of classical metrics like complexity, churn, coverage, dependency measures, and organizational structure of the company to predict vulnerabilities and assess how well these software measures correlate with vulnerabilities. We observed in our experiments that classical software measures predict vulnerabilities with a high precision but low recall values. The actual dependencies, however, predict vulnerabilities with a lower precision but substantially higher recall.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477059,no
Fault Detection Likelihood of Test Sequence Length,"Testing of graphical user interfaces is important due to its potential to reveal faults in operation and performance of the system under consideration. Most existing test approaches generate test cases as sequences of events of different length. The cost of the test process depends on the number and total length of those test sequences. One of the problems to be encountered is the determination of the test sequence length. Widely accepted hypothesis is that the longer the test sequences, the higher the chances to detect faults. However, there is no evidence that an increase of the test sequence length really affect the fault detection. This paper introduces a reliability theoretical approach to analyze the problem in the light of real-life case studies. Based on a reliability growth model the expected number of additional faults is predicted that will be detected when increasing the length of test sequences.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477061,no
A Counter-Example Testing Approach for Orchestrated Services,"Service oriented computing is based on a typical combination of features such as very late binding, run-time integration of software elements owned and managed by third parties, run-time changes. These characteristics generally make difficult both static and dynamic verification capabilities of service-centric systems. In this domain verification and testing research communities have to face new issues and revise existing solutions; possibly profiting of the new opportunities that the new paradigm makes available. In this paper, focusing on service orchestrations, we propose an approach to automatic test case generation aiming in particular at checking the behaviour of services participating in a given orchestration. The approach exploits the availability of a runnable model (the BPEL specification) and uses model checking techniques to derive test cases suitable to detect possible integration problems. The approach has been implemented in a plug-in for the Eclipse platform already released for public usage. In this way BPEL developers can easily derive, using a single environment, test suites for each participant service they would like to compose.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477062,no
Satisfying Test Preconditions through Guided Object Selection,"A random testing strategy can be effective at finding faults, but may leave some routines entirely untested if it never gets to call them on objects satisfying their preconditions. This limitation is particularly frustrating if the object pool does contain some precondition-satisfying objects but the strategy, which selects objects at random, does not use them. The extension of random testing described in this article addresses the problem. Experimentally, the resulting strategy succeeds in testing 56% of the routines that the pure random strategy missed; it tests hard routines 3.6 times more often; although it misses some of the faults detected by the original strategy, it finds 9.5% more faults overall; and it causes negligible overhead.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477072,no
"We're Finding Most of the Bugs, but What are We Missing?","We compare two types of model that have been used to predict software fault-proneness in the next release of a software system. Classification models make a binary prediction that a software entity such as a file or module is likely to be either faulty or not faulty in the next release. Ranking models order the entities according to their predicted number of faults. They are generally used to establish a priority for more intensive testing of the entities that occur early in the ranking. We investigate ways of assessing both classification models and ranking models, and the extent to which metrics appropriate for one type of model are also appropriate for the other. Previous work has shown that ranking models are capable of identifying relatively small sets of files that contain 75-95% of the faults detected in the next release of large legacy systems. In our studies of the rankings produced by these models, the faults not contained in the predicted most fault prone files are nearly always distributed across many of the remaining files; i.e., a single file that is in the lower portion of the ranking virtually never contains a large number of faults.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477073,no
An Application of Six Sigma and Simulation in Software Testing Risk Assessment,"The conventional approach to Risk Assessment in Software Testing is based on analytic models and statistical analysis. The analytic models are static, so they don't account for the inherent variability and uncertainty of the testing process, which is an apparent deficiency. This paper presents an application of Six Sigma and Simulation in Software Testing. DMAIC and simulation are applied to a testing process to assess and mitigate the risk to deliver the product on time, achieving the quality goals. DMAIC is used to improve the process and achieve required (higher) capability. Simulation is used to predict the quality (reliability) and considers the uncertainty and variability, which, in comparison with the analytic models, more accurately models the testing process. Presented experiments are applied on a real project using published data. The results are satisfactorily verified. This enhanced approach is compliant with CMMI<sup></sup> and provides for substantial Software Testing performance-driven improvements.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477075,no
An Empirical Evaluation of Regression Testing Based on Fix-Cache Recommendations,"Background: The fix-cache approach to regression test selection was proposed to identify the most fault-prone files and corresponding test cases through analysis of fixed defect reports. Aim: The study aims at evaluating the efficiency of this approach, compared to the previous regression test selection strategy in a major corporation, developing embedded systems. Method: We launched a post-hoc case study applying the fix-cache selection method during six iterations of development of a multi-million LOC product. The test case execution was monitored through the test management and defect reporting systems of the company. Results: From the observations, we conclude that the fix-cache method is more efficient in four iterations. The difference is statistically significant at  = 0.05. Conclusions: The new method is significantly more efficient in our case study. The study will be replicated in an environment with better control of the test execution.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477099,no
(Un-)Covering Equivalent Mutants,"Mutation testing measures the adequacy of a test suite by seeding artificial defects (mutations) into a program. If a test suite fails to detect a mutation, it may also fail to detect real defects-and hence should be improved. However, there also are mutations which keep the program semantics unchanged and thus cannot be detected by any test suite. Such equivalent mutants must be weeded out manually, which is a tedious task. In this paper, we examine whether changes in coverage can be used to detect non-equivalent mutants: If a mutant changes the coverage of a run, it is more likely to be non-equivalent. In a sample of 140 manually classified mutations of seven Java programs with 5,000 to 100,000 lines of code, we found that: (a) the problem is serious and widespread-about 45% of all undetected mutants turned out to be equivalent; (b) manual classification takes time-about 15 minutes per mutation; (c) coverage is a simple, efficient, and effective means to identify equivalent mutants-with a classification precision of 75% and a recall of 56%; and (d) coverage as an equivalence detector is superior to the state of the art, in particular violations of dynamic invariants. Our detectors have been released as part of the open source JAVALANCHE framework; the data set is publicly available for replication and extension of experiments.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477100,no
MuTMuT: Efficient Exploration for Mutation Testing of Multithreaded Code,"Mutation testing is a method for measuring the quality of test suites. Given a system under test and a test suite, mutations are systematically inserted into the system, and the test suite is executed to determine which mutants it detects. A major cost of mutation testing is the time required to execute the test suite on all the mutants. This cost is even greater when the system under test is multithreaded: not only are test cases from the test suite executed on many mutants, but also each test case is executed for multiple possible thread schedules. We introduce a general framework that can reduce the time for mutation testing of multithreaded code. We present four techniques within the general framework and implement two of them in a tool called MuTMuT. We evaluate MuTMuT on eight multithreaded programs. The results show that MuTMuT reduces the time for mutation testing, substantially over a straightforward mutant execution and up to 77% with the advanced technique over the basic technique.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477101,no
Automated Test Data Generation on the Analyses of Feature Models: A Metamorphic Testing Approach,"A Feature Model (FM) is a compact representation of all the products of a software product line. The automated extraction of information from FMs is a thriving research topic involving a number of analysis operations, algorithms, paradigms and tools. Implementing these operations is far from trivial and easily leads to errors and defects in analysis solutions. Current testing methods in this context mainly rely on the ability of the tester to decide whether the output of an analysis is correct. However, this is acknowledged to be time-consuming, error-prone and in most cases infeasible due to the combinatorial complexity of the analyses. In this paper, we present a set of relations (so-called metamorphic relations) between input FMs and their set of products and a test data generator relying on them. Given an FM and its known set of products, a set of neighbour FMs together with their corresponding set of products are automatically generated and used for testing different analyses. Complex FMs representing millions of products can be efficiently created applying this process iteratively. The evaluation of our approach using mutation testing as well as real faults and tools reveals that most faults can be automatically detected within a few seconds.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477103,no
Java code reviewer for verifying object-oriented design in class diagrams,"Verification and Validation (V&V) processes play an important role in quality control. The earlier defects are detected, the less rework incurs. According to the findings from literature, most of the defects occurred during the design and coding phases. Automatic detection of these defects would alleviate the problem. This research therefore invented an automatic code reviewer to examine Java source files against the object-oriented design described in UML class diagrams. Prior to the review process, the class diagrams are converted into XML format so that the information of classes and relations could be extracted and used to generate the review checklists. The code reviewer will then follow the checklist items to verify whether all defined classes exist in the code, the class structures with encapsulated methods and parameters are correctly implemented, all relations of associated classes are valid. Finally, the summary report will then be generated to notify the results.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477762,no
A software reliability prediction model based on benchmark measurement,Software reliability is a very important and active research field in software engineering. There have been one hundred of prediction model since the first prediction model published. But most of them are adapted after software test and only few of them can be used before test. The paper proposed an idea that to predict the software reliability making use of the similar projects measurement data based on software process benchmark. Its prediction uses benchmark measurement and software process data before software test.,2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5478245,no
Predict protein subnuclear location with ensemble adaboost classifier,"Protein function prediction with computational method is becoming an important research field in protein science and bioinformatics. In eukaryotic cells, the knowledge of subnuclear localization is essential for understanding the life function of nucleus. In this study, A novel ensemble classifier is designed incorporating three AdaBoost classifiers to predict protein subnuclear localization. The base classifier algorithms in AdaBoost classifier is fuzzy K nearest neighbors (FKNN). Three parts amino acid pair compositions with different spaces are computed to construct features vector for representing a protein sample. Jackknife cross-validation test are used to evaluate performance of proposed with two benchmark datasets. Compared with prior works, promising results obtained indicate that the proposed method is more effective and practical. Current approach may also be used to improve the prediction quality of other protein attributes. The software written in Matlab are available freely by contacting the corresponding author.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5478972,no
A Profile Approach to Using UML Models for Rich Form Generation,"The Model Driven Development (MDD) has provided a new way of engineering today's rapidly changing requirements into the implementation. However, the development of user interface (UI) part of an application has not benefit much from MDD although today's UIs are complex software components and they play an essential role in the usability of an application. As one of the most common UI examples, consider view forms that are used for collecting data from the user. View forms are usually generated with a lot of manual efforts after the implementation. For example, in case of Java 2 Enterprise Edition (Java EE) web applications, developers create all view forms manually by referring to entity beans to determine the content of forms, but such manual creation is pretty tedious and certainly very much error-prone and makes the system maintenance difficult. One promise in MDD is that we can generate code from UML models. Existing design models in MDD, however, cannot provide all class attributes that are required to generate the practical code of UI fragments. In this paper, we propose a UML profile for view form generation as an extension of the object relational mapping (ORM) profile. A profile form of hibernate validator is also introduced to implement the practical view form generation that includes an user input validation.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5480315,no
Software Maintenance Prediction Using Weighted Scenarios: An Architecture Perspective,"Software maintenance is considered one of the most important issues in software engineering which has some serious implications in term of cost and effort. It consumes enormous amount of organization's overall resources. On the other hand, software architecture of an application has considerable effect on quality factors such as maintainability, performance, reliability and flexibility etc. Using software architecture for quantification of certain quality factor will help organizations to plan resources accordingly. This paper is an attempt to predict software maintenance effort at architecture level. The method takes requirements, domain knowledge and general software engineering knowledge as input in order to prescribe application architecture. Once application architecture is prescribed, then weighted scenarios and certain factors (i.e. system novelty, turnover and maintenance staff ability, documentation quality, testing quality etc) that affect software maintenance are applied to application architecture to quantify maintenance effort. The technique is illustrated and evaluated using web content extraction application architecture.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5480420,no
Improvement and its Evaluation of Worker's Motion Monitor System in Automobile Assembly Factory,"Warranty of quality in industrial products depends on to confirm everything on their production process. There are some cases in assemble process that the process itself satisfies the regulation or not is very difficult. For example, to fix a part by using some bolts, order to screwing is often regulated to guarantee accuracy to attach the part. However, once it is fixed, even if the procedure is violated as long as screws are enough fastened, thus accuracy is not guaranteed and the violation cannot be detected. So we are developing a system that monitors worker's motions in routine works in factory, employing terrestrial magnetism and acceleration sensors. In this system, these sensors are attached onto tools, and the system judges the work is correctly done or not. In this paper, we describe a method to judge which uses Local Outlier Factor (LOF), and we show some evaluations on automobile assembly factory.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5480816,no
NLAR: A New Approach to AQM,"The traditional adaptive Random Early Detection (RED) algorithm allows network to achieve high throughput and low average delay. However it uses a linear dropping probability function, which causes high jitter in the core router. To overcome the drawbacks of queue jitter in the traditional adaptive RED algorithm, this paper proposes a new Non-Linear Adaptive RED (NLAR) approach based on the Active Queue Management (AQM) scheme, which provides a non-linear adaptation to the dropping probability function of the adaptive RED. NLAR enables the gradient of the dropping probability to vary along with the deviation that is between the average queue length and the target queue length, which contributes to a more stable algorithm. Empirical simulation with various data analysis have demonstrated that the NLAR algorithm outperforms the adaptive RED algorithm in most scenarios.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5480840,no
Intelligent Agents for Fault Tolerance: From Multi-agent Simulation to Cluster-Based Implementation,"Recent research in multi-agent systems incorporate fault tolerance concepts, but does not explore the extension and implementation of such ideas for large scale parallel computing systems. The work reported in this paper investigates a swarm array computing approach, namely 'Intelligent Agents'. A task to be executed on a parallel computing system is decomposed to sub-tasks and mapped onto agents that traverse an abstracted hardware layer. The agents intercommunicate across processors to share information during the event of a predicted core/processor failure and for successfully completing the task. The feasibility of the approach is validated by simulations on an FPGA using a multi-agent simulator, and implementation of a parallel reduction algorithm on a computer cluster using the Message Passing Interface.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5480939,no
Evaluation of Error Control Mechanisms Based on System Throughput and Video Playable Frame Rate on Wireless Channel,"Error control mechanisms are widely used in video communications over wireless channels. However for improving end-to-end video quality: they consume extra bandwidth and reduce effective system throughput. In this paper, considering the parameters of system throughput and playable frame rate as evaluating metrics, we investigate the efficiency of different error control mechanisms. We develop a throughput analytical model to present system effective throughput for different error control mechanisms under different conditions. For a given packet loss probability, both optimal retransmission times in adaptive ARQ and optimal number of redundant packets in adaptive FEC for each type of frames are derived by keeping the system throughput as a constant value. Also, end to end playable frame rates for the two schemes are computed. Then which error control scheme is the most suitable for which application condition is concluded. Finally empirical simulation experimental results with various data analysis are demonstrated.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5480940,no
Reverse engineering legacy code for finite element field computation in magnetics,"The development of code for finite elements-based field computation has been going on at a pace since the 1970s, yielding code that was not put through the formal software lifecycle. As a result, today we have legacy code running into millions of lines, implemented without planning and not using proper state-of-the-art software design tools. It is necessary to redo this code to exploit object oriented facilities and make corrections or run on the web in Java. Object oriented code's principal advantage is reusability. It is ideal for describing autonomous agents so that values inside a method are private unless otherwise so provided - that is encapsulation makes programming neat and less error-prone in unexpected situations. Recent advances in software make such reverse engineering/reengineering of this code into object oriented form possible. In this paper we reverse engineer legacy code in FORTRAN written decades ago for the computation of magnetic fields by the finite element method into the modern languages of Java and C++.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5481490,no
Proposal of a language for describing differentiable sizing models for electromagnetic devices design,"Sizing by optimization usually implies a model definition able to link the desired objective and constrained functions with the design parameters. When gradient based optimization algorithms are used, highly accurate derivatives are required. Without special software solutions, this procedure easily becomes time-consuming or error-prone. In this paper, our goal is to first observe the modeling needs of electromagnetic devices in the optimization context. A new modeling language is then proposed in order to satisfy and formalize these needs. The concepts are validated by the modeling and optimization procedures of an electromagnetic actuator.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5481845,no
"System and software assurance  rationalizing governance, engineering practice, and engineering economics","This paper discusses rationalizing governance, engineering practice, and engineering economics to produce conformant systems that meet their quality attribute targets for system and software assurance in an optimal, cost-effective fashion. It begins with a description of the governance landscape and addresses defining and trading off system quality characteristics, models for assessing the cost and value of software assurance, addressing multi-dimensional risk, and the delivery of value to the organization, its customers, and its stakeholders.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5482470,no
The Effects of Time Constraints on Test Case Prioritization: A Series of Controlled Experiments,"Regression testing is an expensive process used to validate modified software. Test case prioritization techniques improve the cost-effectiveness of regression testing by ordering test cases such that those that are more important are run earlier in the testing process. Many prioritization techniques have been proposed and evidence shows that they can be beneficial. It has been suggested, however, that the time constraints that can be imposed on regression testing by various software development processes can strongly affect the behavior of prioritization techniques. If this is correct, a better understanding of the effects of time constraints could lead to improved prioritization techniques and improved maintenance and testing processes. We therefore conducted a series of experiments to assess the effects of time constraints on the costs and benefits of prioritization techniques. Our first experiment manipulates time constraint levels and shows that time constraints do play a significant role in determining both the cost-effectiveness of prioritization and the relative cost-benefit trade-offs among techniques. Our second experiment replicates the first experiment, controlling for several threats to validity including numbers of faults present, and shows that the results generalize to this wider context. Our third experiment manipulates the number of faults present in programs to examine the effects of faultiness levels on prioritization and shows that faultiness level affects the relative cost-effectiveness of prioritization techniques. Taken together, these results have several implications for test engineers wishing to cost-effectively regression test their software systems. These include suggestions about when and when not to prioritize, what techniques to employ, and how differences in testing processes may relate to prioritization cost--effectiveness.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5482587,no
Reliability Analysis of Embedded Applications in Non-Uniform Fault Tolerant Processors,"Soft error analysis has been greatly aided by the concept of Architectural vulnerability Factor (AVF) and Architecturally Correct Execution (ACE). The AVF of a processor is defined as the probability that a bit flip in the processor architecture will result in a visible error in the final output of a program. In this work, we exploit the techniques of AVF analysis to introduce a software-level vulnerability analysis. This metric allows insight into the vulnerability of instruction and software to hardware faults with a micro-architectural involved fault injection method. The proposed metric can be used to make judgments about the reliability of different programs on different processors with regard to architectural and compiler guidelines for improving the processor reliability.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5482675,no
A fault section detection method using ZCT when a single phase to ground fault in ungrounded distribution system,"A single line to ground fault (SLG) detection in ungrounded network is very difficult, because fault current magnitude is very small. It is generated by a charging current between distribution line and ground. As it is very small, is not used for fault detection in case of SLG. So, SLG has normally been detected by switching sequence method which makes customers experience blackouts. A new fault detection algorithm based on comparison of zero-sequence current and line-to-line voltage phases is proposed. The algorithm uses ZCT installed to ungrounded distribution network. The proposed in this paper algorithm has the advantage that it can detect fault phase and distinguish a faulted section as well. The simulation tests of proposed algorithm were performed using Matlab Simulink and the results are presented in the paper.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5484544,no
Adaptive random testing of mobile application,"Mobile applications are becoming more and more powerful yet also more complex. While mobile application users expect the application to be reliable and secure, the complexity of the mobile application makes it prone to have faults. Mobile application engineers and testers use testing technique to ensure the quality of mobile application. However, the testing of mobile application is time-consuming and hard to automate. In this paper, we model the mobile application from a black box view and propose a distance metric for the test cases of mobile software. We further proposed an ART test case generation technique for mobile application. Our experiment shows our ART tool can both reduce the number of test cases and the time needed to expose first fault when compared with random technique.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5485442,no
Evaluation of software testing process based on Bayesian networks,"In this paper, we will introduce a Bayesian networks (BN) approach for probability evaluation method of software quality assurance. Then, we will present a method for transforming Fault Tree Analysis (FTA) to Bayesian networks and build an evaluation model based on Bayesian networks. Bayesian networks can perform forward risk prediction and backward diagnosis analysis by deduction on the model. Finally, we will illustrate the rationality and validity of the Bayesian networks through an example of evaluation of software testing process.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5485469,no
Defect association and complexity prediction by mining association and clustering rules,"Number of defects remaining in a system provides an insight into the quality of the system. Software defect prediction focuses on classifying the modules of a system into fault prone and non-fault prone modules. This paper focuses on predicting the fault prone modules as well as identifying the types of defects that occur in the fault prone modules. Software defect prediction is combined with association rule mining to determine the associations that occur among the detected defects and the effort required for isolating and correcting these defects. Clustering rules are used to classify the defects into groups indicating their complexity: SIMPLE, MODERATE and COMPLEX. Moreover the defects are used to predict the effect on the project schedules and the nature of risk concerning the completion of such projects.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5485608,no
Keystroke identification with a genetic fuzzy classifier,"This paper proposes the use of fuzzy if-then rules for Keystroke identification. The proposed methodology modifies Ishibuchi's genetic fuzzy classifier to handle high dimensional problems such as keystroke identification. High dimensional property of a problem increases the number of rules with low fitness. For decreasing them, rule initialization and coding are modified. Furthermore a new heuristic method is developed for improving the population quality while running GA. Experimental result demonstrates that we can achieve better running time, interpretability and accuracy with these modifications.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5485677,no
The application of multi-function interface MVB NIC in distributed locomotive fault detecting and recording system,"Locomotive condition monitoring and fault diagnosis system is an important component of modern locomotive, it needs a reliable, high-speed communication network to ensure that the system's reliable operation in the complex locomotive environment. The Controller Area Network (CAN) used in the existing distributed locomotive fault detecting and recording system is not suitable for vehicles bus, so the paper brought forward the scheme using the Multifunction Vehicle Bus (MVB). Firstly, it described the alteration of system structure and operating principle key design concepts in detail, next designed the multi-function interface MVB NIC using SOPC (system on a programmable chip) technology, given the realization of hardware and software, ultimately proceeded the network test in the lab, and verified the correctness and feasibility of the design. The improved network has farther transmission distance, higher rates, better reliability and real-time.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5486001,no
Semantic consistency checking for model transformations,"Model transformation, as a key technique of MDA, is error-prone because of conceptual flaws in design and man-made errors in manual transformation rules. So the consistency checking of model transformations is of great importance for MDA. In this paper, a framework of semantic consistency checking for model transformation is proposed and discussed. In this framework, a graph representation is required to describe model languages, model transformation rules, and source code. Then several semantic properties are selected to be studied, and algorithms based on critical pairs are given to check whether these properties are preserved by model transformations. At last, a case study is performed to demonstrate the feasibility.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5486198,no
Uplink array concept demonstration with the EPOXI spacecraft,"Uplink array technology is currently being developed for NASA's Deep Space Network (DSN), to provide greater range and data throughput for future NASA missions, including manned missions to Mars and exploratory missions to the outer planets, the Kuiper belt, and beyond. The DSN uplink arrays employ N microwave antennas transmitting at X-band to produce signals that add coherently at the spacecraft, thereby providing a power gain of N<sup>2</sup> over a single antenna. This gain can be traded off directly for N<sup>2</sup> higher data rate at a given distance such as Mars, providing, for example, HD quality video broadcast from earth to a future manned mission, or it can provide a given data-rate for commands and software uploads at a distance N times greater than possible with a single antenna. The uplink arraying concept has been recently demonstrated using the three operations 34-meter antennas of the Apollo complex at Goldstone, CA, which transmitted arrayed signals to the EPOXI spacecraft. Both two-element and three-element uplink arrays were configured, and the theoretical array gains of 6 dB and 9.5 dB, respectively, were demonstrated experimentally. This required initial phasing of the array elements, the generation of accurate frequency predicts to maintain phase from each antenna despite relative velocity components due to earth-rotation and spacecraft trajectory, and monitoring of the ground system phase for possible drifts caused by thermal effects over the 16 km fiber-optic signal distribution network. This provides a description of the equipment and techniques used to demonstrate the uplink arraying concept in a relevant operational environment. Data collected from the EPOXI spacecraft was analyzed to verify array calibration, array gain, and system stability over the entire five hour duration of this experiment.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5486539,no
A QoS prediction approach based on improved collaborative filtering,"Consumers need to make prediction on the quality of unused Web services before selecting from a large number of services. Usually, this prediction is made based on other consumers' experiences. Being aware of the similarity of consumers' assessment, this paper proposes a QoS prediction approach. This approach calculates the similarity among consumers, and then uses a improved collaborative filtering technology to predicting the QoS of the unused Web services. Experimental results show that with this approach preciseness of QoS prediction for Web services is higher than other prediction approaches, and it has good feasibility and effectiveness.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5486697,no
An improved software reliability model incorporating debugging time lag,"Software reliability growth models (SRGMs) based on the non-homogeneous Poisson process (NHPP) are quite successful tools that have been proposed to assess the software reliability. Most NHPP-SRGMs assume the detected faults are immediately corrected, but it is not the case in real environments. In this paper, incorporating the testing coverage, considering imperfect debugging and correcting time lag, we propose an improved NHPP-SRGM. Experimental results show that the proposed model fits the failure data quite well and has a fairly accurate prediction capability.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5486747,no
An efficient experimental approach for the uncertainty estimation of QoS parameters in communication networks,"In communication networks setup and tuning activities, a key issue is to assess the impact of a new service running on the network on the overall Quality of Service. To this aim suitable figures of merit and test beds have to be adopted and time-consuming measurement campaigns generally should be carried out. A preliminary issue to be accomplished for is the metrological characterization of the test set-up aimed to provide a confidence level and a variability interval to the measurement results. This allows identifying and evaluating the intrinsic uncertainty to be considered in the experimental measurement of Quality of Service parameters. This paper proposes an original experimental approach suitable for the purpose. The uncertainty components involved in the measurement process are identified and experimentally quantified by means of effective statistical analyses. The proposed approach takes into account the general characteristics of the network topology, the number and type of devices involved, the characteristics of the current services operating on the network, and of the new services to be implemented, as well as the intrinsic uncertainties related to the set-up and to the measurement method. As an application example, the proposed approach has been adopted to the measurement of the packet jitter on a test bed involving a real computer network displaced on several kilometers. The obtained results show the effectiveness of the proposal.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5488077,no
Self adaptive BCI as service-oriented information system for patients with communication disabilities,"A new service-oriented information architecture is presented that can help the communication-disabled to socialize in their private and public environment. The service is designed for physically handicapped people who cannot communicate without expensive custom-made tools. Statistics show that, e.g., in Belgium 1:500 persons suffer from some form of motor or speech disability, mostly due to stroke (aphasia patients). Patients with severe motor or speech disabilities need expensive tailor-made made devices and individualized protocols to communicate. About 1:6000 do have problems with information exchange in their daily practice, such as patients with severe autistic disorders, and Amyotrophic Lateral Sclerosis (ALS), Locked-in Syndrome (LIS) and Speech and Language Impaired (SLI) patients, and their communication is often limited to care takers and family, because the interaction with other people through electronic systems often fails. In fact, all these disabled yearn to basically participate in our society. Enhancing the amount of adapted devices and personal care takers has huge consequences and is mostly unfeasible by firm limits in specialists, infrastructure and budget. The quality of life can be graded up by a service-oriented information architecture that supports an on-line Mind Speller<sup></sup>, i.e., a Brain-Computer Interface (BCI) that enables subjects to spell text on a computer screen, and potentially have it voiced, without muscular activity, to assist or enable patients to communicate, but also to provide speech revalidation, as in autism spectrum disorder patients. The Mind Speller<sup></sup> operates non-invasively by detecting P300 signals in their EEG. With the support of predictive text algorithms, the mind spelled characters will be words and sentences, and even stories (story telling), enabling the communication-disabled to participate in either the physical environment as - by the Internet - the global digital world.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5488610,no
A runtime approach for software fault analysis based on interpolation,"In the application system, obtaining the information of the system at runtime and analyzing them are important for system adjustment. Many runtime metrics can be collected from software systems, and some statistical relationships exist among these metrics. Extracting the information of these metrics from the monitoring data and then analyzing the relationships between these metrics is an effective way to detect failure and diagnose fault. This paper proposes a fault analysis approach for the system at runtime which gets the information of the system by monitoring. We demonstrate this approach in a case study which shows that our approach is effective and is beneficial to find the relationship between the fault and the component.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5488642,no
Trustable web services with dynamic confidence time interval,"One part of trustfulness on web services application over the network is confidence of services that providers can guarantee to their customers. Therefore, after the development process, web services developers must be sure that the delivered services are qualified for availability and reliability during their execution. However, there is a critical problem when errors occur during the execution time of the service agent, such as the infinite loop problem in the service process. This unexpected problem of the web services software can cause critical damages in various aspects, especially lives and dead of people. Although there are various methods have been proposed to protect the unexpected errors, most of them are procedures in the verification and validation during the development processes. Nevertheless, these methods cannot completely solve the infinite loop problem since this problem is usually occurred by an unexpected values obtained from the execution of request and response processes . Therefore, this paper proposed a system architecture includes with a protection mechanism that completely detect and protect the unbound loop problem of web services when request services of each requester are under the dynamic situation. This proposed solution can guarantee that users will definitely be protected from a critical lost occurred from the unexpected infinite loop of the web services system. Consequently, all service agents with dynamic loop control condition can be trustable.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5488643,no
Interaction Testing: From Pairwise to Variable Strength Interaction,"Although desirable as an important activity for quality assurances and enhancing reliability, complete and exhaustive software testing is prohibitively impossible due to resources as well as timing constraints. While earlier work has indicated that uniform pairwise testing (i.e. based on 2-way interaction of variables) can be effective to detect most faults in a typical software system, a counter argument suggests such conclusion cannot be generalized to all software system faults. In some system, faults may also be non-uniform and caused by more than two parameters. Considering these issues, this paper explores the issues pertaining to t-way testing from pairwise to variable strength interaction in order to highlight the state-of-the-art as well as the current state-of-practice.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5489307,no
Predicting Software Reliability with Support Vector Machines,"Support vector machine (SVM) is a new method based on statistical learning theory. It has been successfully used to solve nonlinear regression and time series problems. However, SVM has rarely been applied to software reliability prediction. In this study, an SVM-based model for software reliability forecasting is proposed. In addition, the parameters of SVM are determined by Genetic Algorithm (GA). Empirical results show that the proposed model is more precise in its reliability prediction and is less dependent on the size of failure data comparing with the other forecasting models.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5489502,no
Enhance Fault Localization Using a 3D Surface Representation,"Debugging is a difficult and time-consuming task in software engineering. To locate faults in programs, a statistical fault localization technique makes use of program execution statistics and employs a suspiciousness function to assess the relation between program elements and faults. In this paper, we develop a novel localization technique by using a 3D surface to visualize previous suspiciousness functions and using fault patterns to enhance such a 3D surface. By clustering realistic faults, we determine various fault patterns and use 3D points to represent them. We employ spline method to construct a 3D surface from those 3D points and build our suspiciousness function. Empirical evaluation on a common data set, Siemens suite, shows that the result of our technique is more effective than four existing representative such techniques.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5489519,no
Study of ERP Test-Suite Reduction Based on Modified Condition/Decision Coverage,"Enterprise Resource Planning (ERP) systems represent a huge market in the commercial arena. Products from suppliers such as SAP, Oracle and more recently, Microsoft, dominate the software market. Testing in these projects is a significant effort but is hardly supported by methods and tools other than those provided by the suppliers themselves. Experience shows that testing in these projects is critical, but often neglected. Recent 'lessons learned' work by the Paul Gerrard indicates that a benefit, risk- and coverage-based test approach could significantly reduce the risk of failures. It is evidence that modified condition/decision coverage (MC/DC) is an effective verification method and can help to detect safety faults despite of its expensive cost. In regression testing, it is quite costly to return all of test cases in test suite because new test cases are added to test suite as the software evolves. Therefore, it is necessary to reduce the test suite to improve test efficiency and save test cost. Many existing test-suite reduction techniques are not effective to reduce MC/DC test suite. This paper proposes a new test-suite reduction technique for MC/DC: a bi-objective model that considers both the coverage degree of test case for test requirements and the capability of test cases to reveal error. Our experiment results show that the technique both reduces the size of test suite and better ensures the effectiveness of test suite to reveal error.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5489576,no
An Ontology-Based Framework for Designing a Sensor Quality of Information Software Library,"Assessing the quality of sensor-originated information is key for the effective and predictable operation sensor-enabled computerized applications. However, with increasing uncertainties due to alternative deployment scenarios, operational realities, and sensing resource use, it becomes very challenging in designing replicable software solutions that can be easily reused in a number of occasions with minimal customization. Leveraging semantic sensor web technologies, this paper presents an ontology-based design framework for organizing a library of quality of information (QoI) analysis algorithms specific to a data source, and interfacing to a library containing computational algorithms assessing quality of information. The ontology-based framework is broad enough to allow easy accommodation of new computational algorithms that domain experts may provide to the library as needed to reflect specific deployment, operational, sensing realities.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5489662,no
Detecting patterns and antipatterns in software using Prolog rules,"Program comprehension is a key prerequisite for the maintainance and analysis of legacy software systems. Knowing about the presence of design patterns or antipatterns in a software system can significantly improve the program comprehension. Unfortunately, in many cases the usage of certain patterns is seldom explicitly described in the software documentation, while antipatterns are never described as such in the documentation. Since manual inspection of the code of large software systems is difficult, automatic or semi-automatic procedures for discovering patterns and antipatterns from source code can be very helpful. In this article we propose detection methods for a set of patterns and antipatterns, using a logic-based approach. We define with help of Prolog predicates both structural and behavioural aspects of patterns and antipatters. The detection results obtained for a number of test systems are also presented.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5491288,no
Portable artificial nose system for assessing air quality in swine buildings,"To practice an efficient air quality management in livestocks, a standardized measurement technology has always been requested in order to assess the odor, of which results are acceptable by every party involved, i.e., the owner, the state and the public. This paper has reported on a prototype of portable electronic nose (e-nose) designed specially to assess malodors in swine buildings atmosphere in the pig farm. The briefcase formed e-nose consists of eight chemical gas sensors that are sensitive to gases usually presented in pig farm such as ammonia, hydrogen sulfide, hydrocarbons etc. The system contains gas flow controller, measurement circuit and data acquisition unit, all of which are automated and controlled by an in-house software on a notebook PC via a USB port. We have tested the functionality of this e-nose in a pig farm under a real project aimed specifically to reduce the odor emission from swine buildings in the pig farm. The e-nose was used to assess the air quality inside sampled swine buildings. Based on the results given in this paper, recommendations on appropriate feeding menu, buildings' cleaning schedule and emission control program have been made.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5491431,no
Synthesizing simulators for model checking microcontroller binary code,"Model checking of binary code is recognized as a promising tool for the verification of embedded software. Our approach, which is implemented in the [MC]SQUARE model checker, uses tailored simulators to build state spaces for model checking. Previously, these simulators have been generated by hand in a time-consuming and error-prone process. This paper proposes a method for synthesizing these simulators from a description of the hardware in an architecture description language in order to tackle these drawbacks. The application of this approach to the Atmel ATmega16 microcontroller is detailed in a case study.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5491761,no
SLA-Driven Dynamic Resource Management for Multi-tier Web Applications in a Cloud,"Current service-level agreements (SLAs) offered by cloud providers do not make guarantees about response time of Web applications hosted on the cloud. Satisfying a maximum average response time guarantee for Web applications is difficult due to unpredictable traffic patterns. The complex nature of multi-tier Web applications increases the difficulty of identifying bottlenecks and resolving them automatically. It may be possible to minimize the probability that tiers (hosted on virtual machines) become bottlenecks by optimizing the placement of the virtual machines in a cloud. This research focuses on enabling clouds to offer multi-tier Web application owners maximum response time guarantees while minimizing resource utilization. We present our basic approach, preliminary experiments, and results on a EUCALYPTUS-based testbed cloud. Our preliminary results shows that dynamic bottleneck detection and resolution for multi-tier Web application hosted on the cloud will help to offer SLAs that can offer response time guarantees.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5493374,no
Region-Based Prefetch Techniques for Software Distributed Shared Memory Systems,"Although shared memory programming models show good programmability compared to message passing programming models, their implementation by page-based software distributed shared memory systems usually suffers from high memory consistency costs. The major part of these costs is inter-node data transfer for keeping virtual shared memory consistent. A good prefetch strategy can reduce this cost. We develop two prefetch techniques, TReP and HReP, which are based on the execution history of each parallel region. These techniques are evaluated using offline simulations with the NAS Parallel Benchmarks and the LINPACK benchmark. On average, TReP achieves an efficiency (ratio of pages prefetched that were subsequently accessed) of 96% and a coverage (ratio of access faults avoided by prefetches) of 65%. HReP achieves an efficiency of 91% but has a coverage of 79%. Treating the cost of an incorrectly prefetched page to be equivalent to that of a miss, these techniques have an effective page miss rate of 63% and 71% respectively. Additionally, these two techniques are compared with two well-known software distributed shared memory (sDSM) prefetch techniques, Adaptive++ and TODFCM. TReP effectively reduces page miss rate by 53% and 34% more, and HReP effectively reduces page miss rate by 62% and 43% more, compared to Adaptive++ and TODFCM respectively. As for Adaptive++, these techniques also permit bulk prefetching for pages predicted using temporal locality, amortizing network communication costs and permitting bandwidth improvement from multi-rail network interfaces.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5493488,no
Body sets and lines: A reliable representation of images,"This paper proposes a novel definition of color lines and sets, based on the dichromatic model for lambertian objects. The ends of the body vectors are robustly detected, from the clearest to the darkest through to a multi-level 2D histogram analysis. Finally, instead of classically defining the topographic map along one sole luminance direction, our body lines are designed along each body vector. Compared to existing topographic maps, our method is more compact while better preserving the color quality. Furthermore, it is faster to compute than.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5495343,no
An effective nonparametric quickest detection procedure based on Q-Q distance,"Quickest detection schemes are geared toward detecting a change in the state of a data stream or a real-time process. Classical quickest detection schemes invariably assume knowledge of the pre-change and post-change distributions that may not be available in many applications. In this paper, we present a distribution free nonparametric quickest detection procedure based on a novel distance measure, referred to as the Q-Q distance calculated from the Q-Q plot, for detection of distribution changes. Through experimental study, we show that the Q-Q distance-based detection procedure presents comparable or better performance compared to classical parametric and other nonparametric procedures. The proposed procedure is most effective when detecting small changes.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5495849,no
Automatic synthesis of OSCI TLM-2.0 models into RTL bus-based IPs,"Transaction-level modeling (TLM) is the most promising technique to deal with the increasing complexity of modern embedded systems. TLM provides designers with high-level interfaces and communication protocols for abstract modeling and efficient simulation of system platforms. The Open SystemC Initiative (OSCI) has recently released the TLM-2.0 standard, to standardize the interface between component models for bus-based systems. The TLM standard aims at facilitating the interchange of models between suppliers and users, and thus encouraging the use of virtual platforms for fast simulation prior to the availability of register-transfer level (RTL) code. On the other hand, because a TLM IP description does not include the implementation details that must be added at the RTL, the process to synthesize TLM designs into RTL implementations is still manual, time spending and error prone. In this context, this paper presents a methodology for automating the TLM-to-RTL synthesis by applying the theory of high-level synthesis (HLS) to TLM, and proposes a protocol synthesis technique based on the extended finite state machine (EFSM) model for generating the RTL IP interface compliant with any RTL bus-based protocol.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5496652,no
Application of ANN in food safety early warning,"In recent years, frequent occurrence of food safety crisis has seriously affected people's health, which causes widespread concern around the world. To effectively track and trace food has become an extremely urgent global issue. Early warning of food safety can prevent food safety crisis. However, there is still very few automatic tracking systems for the entire food supply chain. In the paper we propose a data mining technique to predict food quality using back-propagation (BP) neural network. Some prediction errors could occur when predicted data are near threshold values. To reduce errors, data near the threshold values are selected to train our system. Special care of threshold values and performance of our proposed algorithm are discussed in the paper.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5497450,no
A novel resilient multi-path establishment scheme for sensor networks,"With the development of Wireless sensor networks, secure communication among sensors by establishing pair-wise keys becomes a critical issue for many sensor network applications. Due to sensors' special uses, limited capabilities and declining connection probability, the random key pre-distribution schemes are no longer suitable for networks with large size. Multi-path key establishment scheme becomes a great addition to these existing schemes. In this paper, we propose a novel resilient establishment scheme for sensor networks. In our scheme, the sender sensor first partitions the pair-wise key into many sub-keys, and computes hash value of all sub-keys and products a hash checkout tree. It then transmits each sub-key and its hash path value via different node-disjoint paths. The receiver first checkouts the integrity of sub-keys by using hash path value, identifies faulty key establishment paths, and it then recovers the original pair-wise key. The salient features of the scheme are its flexibility of trading transmission, efficient checkout and low information disclosure. Resilience and information leaking analysis is conducted to show that the proposed scheme is agile and efficient compared with the existing schemes. The other nice feature of the proposed scheme is that the parameters of the scheme can be set up to meet the requirements for different sensor network applications.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5497625,no
Ultrasonic Waveguides Detection-based approach to locate defect on workpiece,"Conventional ultrasonic techniques, such as pulse-echo, has been limited to testing relatively simple geometries or interrogating the region in the immediate vicinity of the transducer. A novel, efficiency methodology uses ultrasonic waveguides to examine structural components. The advantages of this technique include: its ability to detect the entire structure in a single measurement through long distance with little attenuation; and its capacity to test inaccessible regions of complex components. However, in practical work, this technique exists dispersion and mode conversion phenomena which makes poor signal to noise ratio, thereby, influences the actual application of this technique. In order to solve this problem, simulation with experiments can not only verifies the feasibility of this technique, but also has guiding significant for actual work. This paper reports on a novel approach in the simplification of the simulation of Ultrasonic Waveguides Detection. The first step is the selection of the frequency of signal which has the fastest group velocity and relatively small dispersion. The second step is the decision of  and l<sub>e</sub>. As the numerical analysis characteristics of general-purpose software ANSYS, two key parameters: time step t and mesh element size l<sub>e</sub> need to be carefully selected. This report finds the balance point between the accuracy of results and calculation time to determine two key parameters which significantly influence the result of the simulation result. Finally, this report show the experiment results on two-dimensional flat panel structure and three-dimensional triangle-iron structure respectively. From the result shown, the error between the simulation and actual value is less than 0.4%, perfectly prove the feasibility of this approach.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5498125,no
"Matrix-geometric solution of a heterogeneous two-Server M/(PH,M)/2 queueing system with server breakdowns","In this paper, we study a repairable queueing system with two different servers, where Server 1 is perfectly reliable and Server 2 is subject to breakdown. The service times of two servers are assumed to follow phase type (PH) distribution and exponential distribution, respectively. By establishing the quasi-birth-and-death (QBD) process of the system states, we first derive the equilibrium condition of the system, and then obtain the matrix-geometric solution for the steady-state probability vectors of the system. Finally, numerical results are presented.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5498245,no
Probabilistic fault prediction of incipient fault,"In this work, a probabilistic fault prediction approach is presented for prediction of incipient fault in an uncertain way. The approach has two stages. In the first stage, normal data is analyzed by principle component analysis (PCA) to get control limits of the statistics of T<sup>2</sup> and SPE. In the second stage, fault data starts by PCA so as to derive the statistics of T<sup>2</sup> and SPE. Then, the samplings of these two statistics obeying some certain prediction distribution are obtained using Bayesian AR model on the basis of the Winbugs software. At last, one-step prediction fault probabilities are estimated by kernel density estimation method according to the statistics' corresponding control limits. The prediction performance of this approach is illustrated using the data from the simulator of the Tennessee Eastman process.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5498474,no
Model Based Testing Using Software Architecture,"Software testing is an ultimate obstacle to the final release of software products. Software testing is also a leading cost factor in the overall construction of software products. On the one hand, model-based testing methods are new testing techniques aimed at increasing the reliability of software, and decreasing the cost by automatically generating a suite of test cases from a formal behavioral model of a system. On the other hand, the architectural specification of a system represents a gross structural and behavioral aspect of a system at the high level of abstraction. Formal architectural specifications of a system also have shown promises to detect faults during software back-end development. In this work, we discuss a hybrid testing method to generate test cases. Our proposed method combines the benefits of model-based testing with the benefits of software architecture in a unique way. A simple Client/Server system has been used to illustrate the practicality of our testing technique.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5501432,no
Provenance Collection in Reservoir Management Workflow Environments,"There has been a recent push towards applying information technology principles, such as workflows, to bring greater efficiency to reservoir management tasks. These workflows are data intensive in nature, and the data is derived from heterogenous data sources. This has placed an emphasis on the quality and reliability of data that is used in reservoir engineering applications. Data provenance is metadata that pertains to the history of the data and can be used to assess data quality. In this paper, we present an approach for collecting provenance information from application logs in the domain of reservoir engineering. In doing so, we address challenges due to: 1) the lack of a workflow orchestration framework in reservoir engineering and 2) the inability of many reservoir engineering applications to collect provenance information. We present an approach that uses the workflow instances detection algorithm and the Open Provenance Model (OPM) for capturing provenance information from the logs.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5501450,no
Improving Change Impact Analysis with a Tight Integrated Process and Tool,"Change impact analysis plays an immanent role in the maintenance and enhancement of software systems, especially for defect prevention. In our previous work we have developed approaches to detect logical dependencies among artifacts in repositories and calculated different metrics. But that is not enough, because in order to use change impact analysis a detailed process with guidelines on the one hand, and appropriate tools on the other hand will be needed. To show the importance of such an approach, we have gathered problems and analyzed requirements in the field of a social insurance company. Based on these requirements we have developed a process and a tool which helps analysts in performing activities along the defined process. In this paper we present the tool for change impact analysis and the significance of the tight integration of the tool into the process.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5501508,no
Towards Scalable Robustness Testing,"Several approaches have been developed to assess the robustness of a system. We propose a model-based approach to scalable testing the robustness of a software system using event sequence graphs (ESG) and decision tables (DT). Elementary modification operators are introduced to manipulate ESGs and DTs resulting in faulty models. Test cases generated from these faulty models are applied to the system under consideration to check its robustness. Thus, the approach enables the quantification of robustness with respect to a universe of erroneous inputs.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5502839,no
Self-Checked Metamorphic Testing of an Image Processing Program,"Metamorphic testing is an effective technique for testing systems that do not have test oracles, for which it is practically impossible to know the correct output of an arbitrary test input. In metamorphic testing, instead of checking the correctness of a test output, the satisfaction of metamorphic relation among test outputs is checked. If a violation of the metamorphic relation is found, the system implementation must have some defects. However, a randomly or accidently generated incorrect output may satisfy a metamorphic relation as well. Therefore, checking only metamorphic relations is not good enough to ensure the testing quality. In this paper, we propose a self-checked metamorphic testing approach, which integrates structural testing into metamorphic testing, to detect subtle defects in a system implementation. In our approach, metamorphic testing results are further verified by test coverage information, which is automatically produced during the metamorphic testing. The effectiveness of the approach has been investigated through testing an image processing program.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5502841,no
Testing Effort Dependent Software FDP and FCP Models with Consideration of Imperfect Debugging,"Software reliability can be enhanced considerably during testing with faults being detected and corrected by testers. The allocation of testing resources, such as man power and CPU hours, during testing phase can largely influence fault detection speed and the time to correct a detected fault. The testing resources allocation is usually depicted by testing effort function, which has been incorporated into software reliability models in some recent papers. Fault correction process (FCP) is usually modeled as a delayed process of fault detection process (FDP). In addition, debugging is usually not perfect and new faults can be introduced during testing. In this paper, flexible testing effort dependent paired models of FDP and FCP are derived with consideration of fault introduction. A real dataset is used to illustrate the application of proposed models.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5502847,no
Sensitivity of Two Coverage-Based Software Reliability Models to Variations in the Operational Profile,"Software in field use may be utilized by users with diverse profiles. The way software is used affects the reliability perceived by its users, that is, software reliability may not be the same for different operational profiles. Two software reliability growth models based on structural testing coverage were evaluated with respect to their sensitivity to variations in operational profile. An experiment was performed on a real program (SPACE) with real defects, submitted to three distinct operational profiles. Distinction among the operational profiles was assessed by applying the Kolmogorov-Smirnov test. Testing coverage was measured according to the following criteria: all-nodes, all-arcs, all-uses, and all-potential-uses. Reliability measured for each operational profile was compared to the reliabilities estimated by the two models, estimated reliabilities were obtained using the coverage for the four criteria. Results from the experiment show that the predictive ability of the two models is not affected by variations in the operational profile of the program.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5502848,no
System of Measuring the Sub-Pixel Edge of CCD Image's,"In order to improve the precision, speed, integration and reliability of the linear CCD system, which was used to detect the sub-pixel edge of picture, a new digital system based on auto focusing was designed. The system captures the image of the tested work piece through a CCD, puts the image data into computer, gathers coordinate of tested edge of work by the method of digital image processing and auto focusing. The data of signals memorized in computer were processed by using software system of data collecting. Thereby, the inspection of precise size and location of defects were achieved. The result shows that The resolving power of this method can reach 20 m and the error is less than 10%.The possibility that the impurity particles can be checked out is up to 100%.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5504485,no
Evaluation of depth compression and view synthesis distortions in multiview-video-plus-depth coding systems,"Several quality evaluation studies have been performed for video-plus-depth coding systems. In these studies, however, the distortions in the synthesized views have been quantified in experimental setups where both the texture and depth videos are compressed. Nevertheless, there are several factors that affect the quality of the synthesized view. Incorporating more than one source of distortion in the study could be misleading; one source of distortion could mask (or be masked by) the effect of other sources of distortion. In this paper, we conduct a quality evaluation study that aims to assess the distortions introduced by the view synthesis procedure and depth map compression in multiview-video-plus-depth coding systems. We report important findings that many of the existing studies have overlooked, yet are essential to the reliability of quality evaluation. In particular, we show that the view synthesis reference software yields high distortions that mask those due to depth map compression, when the distortion is measured by average luma peak signal-to-noise ratio. In addition, we show what quality metric to use in order to reliably quantify the effect of depth map compression on view synthesis quality. Experimental results that support these findings are provided for both synthetic and real multiview-video-plus-depth sequences.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5506511,no
A software-based receiver sampling frequency calibration technique and its application in GPS signal quality monitoring,"This paper has investigated the sampling frequency error impact on the signal processing in a software-correlator based GPS receiver as well as the periodic averaging technique in a pre-correlation GPS signal quality monitor. The refined signal model of receiver processing in the presence of clock error is established as the foundation of the performance analyses. A software-based method is developed to accurately calibrate both the digital IF and the sampling frequency simultaneously. The method requires no additional hardware other than the GPS receiver RF front end output samples. It enables inline calibration of the receiver measurements instead of complicated post-processing. The performance of the technique is evaluated using simulated signals as well as live GPS signals collected by several GPS data acquisition equipments, including clear time domain waveforms/eye patterns, amplitude probability density histograms, Power Spectrum Density (PSD) envelopes, and correlation-related characteristics. The results show that we can calibrate the sampling frequency with an accuracy resolution of 10<sup>9</sup> of the true sampling frequency online, and the pre-correlation SNR can be potentially improved by 39dB using periodic averaging.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5507224,no
Message analysis method based on a stream database for information system management,"The authors conducted a detailed survey of a large-scale data center. They found that automated monitoring of messages produced by operating systems, middleware, and applications was adopted as a key method for detecting system failures and malfunctions, and found that this was an effective method for improving the quality of system management operations. The survey revealed that message occurrences are not only monitored, they are also analyzed for particular patterns. Those patterns include multiple occurrences surpassing a threshold in a predefined time period, consecutive occurrences within a predefined time interval, or occurrences violating predefined sequences, and so on. A custom-made application is currently used for this message analysis. The authors found some problems in implementation of the application. To overcome these problems and aid information system management, the authors proposed a message analysis method using a stream database. An experiment proved the effectiveness of the method.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5507317,no
A Quality Model in a Quality Evaluation Framework for MDWE methodologies,"Nowadays, diverse development methodologies exist in the field of Model-Driven Web Engineering (MDWE), each of which covers different levels of abstraction on Model-Driven Architecture (MDA): CIM, PIM, PSM and Code. Given the high number of methodologies available, it is necessary to evaluate the quality of existing methodologies and provide helpful information to the developers. Furthermore, proposals are constantly appearing and the need may arise not only to evaluate the quality but also to find out how it can be improved. In this context, QuEF (Quality Evaluation Framework) can be employed to assess the quality of MDWE methodologies. This article presents the work being carried out and describes tasks to define a Quality Model component for QuEF. This component would be responsible for providing the basis for specifying quality requirements with the purpose of evaluating quality.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5507323,no
A framework based security-knowledge database for vulnerabilities detection of business logic,"This paper presents a framework for vulnerabilities detection of business logic in the software design phase. First, model the business logic in the design phase finite state machine, and extract relevant business processes from the model. Calculate the similarity degree between attack pattern and the business processes. Thus, find out if there are some vulnerabilities in the business logic and generate a report of threats analysis. Finally, Focusing on the business logic of user registration in the web application, we model it as a FSA then detect the model. By analyzing the detection result we conclude that the approach is correct and effective and can improve software security and reliability.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5508127,no
Fractal web service composition framework,"By using semantic web service composition, individual web services can be combined to create complex, but easily reconfigurable systems. Such systems can play a vital role in today's changing economic conditions, as they allow business to quickly adapt to market changes. Taking into consideration that manual web service composition is both time-consuming and error prone, the proposed framework uses a semi-automatic approach. The composition is done in a fractal manner as existing web service chains can easily be incorporated in new web service chains.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5509006,no
The quality of multiple VoIP calls in an encrypted wireless network,"We quantified the user-perceived quality for a VoIP application running in an encrypted wireless network and we experimentally determined the maximum number of parallel VoIP calls that can be achieved, at the best quality of the speech signal. We studied the behaviour of the application for four encryption mechanisms and for each of them we measured the bandwidth waste due to encryption. We used the ITU-T PESQ score to objectively assess the quality of the speech signal. In this paper we present the test setup and the tools we used for our experiments, as well as our experimental results and conclusions.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5510404,no
Algorithm-based fault tolerance for many-core architectures,"Modern many-core architectures with hundreds of cores provide a high computational potential. This makes them particularly interesting for scientific high-performance computing and simulation technology. Like all nano scaled semiconductor devices, many-core processors are prone to reliability harming factors like variations and soft errors. One way to improve the reliability of such systems is software-based hardware fault tolerance. Here, the software is able to detect and correct errors introduced by the hardware. In this work, we propose a software-based approach to improve the reliability of matrix operations on many-core processors. These operations are key components in many scientific applications.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5512738,no
A Practical Approach to Robust Design of a RFID Triple-Band PIFA Structure,"This paper presents a practical methodology of obtaining a robust optimal solution for a multi U-slot PIFA (Planar Inverted F-Antenna) structure with triple bands of 433 MHz, 912 MHz and 2.45 GHz. Using evolutionary strategy, a global optimum is first sought out in terms of the dimensions of the slots and shorting strip. Then, starting with the optimized values, Taguchi quality method is carried out in order to obtain a robust design of the antenna against the changes of uncontrollable factors such as material property and feed position. To prove the validity of the proposed method, the performance of the antenna is predicted by general-purpose electromagnetic software and also compared to experimental results.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5512937,no
Research on Failure Detector in Remote Disaster-tolerant System,"Failure detector is one of the key components for remote disaster-tolerant systems. In this paper, a flexible hierarchical architecture is designed and the detection component is divided from decision-making. In doing so, failure detector can be applied to remote disaster-tolerant system and can meet the requirements of different applications. An adaptable failure detection algorithm is put forward considering message delay and message loss and clock synchronization. This algorithm can adapt itself to the change of network condition and applications' requirements. The algorithm has been implemented in a remote disaster-tolerant system and the test results showed that the error rate is greatly decreased using the algorithm.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5514199,no
Correlation between Indoor Air Distribution and Pollutants in Natural Ventilation,"The indoor air quality of lecture room in the university located in Xi'an city centre was assessed. The primary aim was to obtain correlations between the air distribution and pollutants. It was found that air distribution had important effect on the IAQ, and rational air distribution has played an important part in pollutants dispersion and attenuation. The visual air-flow field of lecture room was obtained via the computational fluid dynamics (CFD) method and Fluent software, meanwhile, some parameters of indoor pollutants was gained by field testing and the lecture room characteristics were investigated. The results showed that indoor pollutants can't be removed just depending on natural ventilation when there are vortexes in the air distribution. Furthermore, reasonable suggestions for creating healthy teaching environment and better IAQ are offered.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5514673,no
A logic based approach to locate composite refactoring opportunities in object-oriented code,"In today's software engineering, more and more emphasis is put on the quality of object-oriented software design. It is commonly accepted that building a software system with maintainability and reusability issues in mind is far more important than just getting all the requirements fulfilled in one way or another. Design patterns are powerful means to obtain this goal. Tools have been built that automatically detect design patterns in object-oriented code and help in understanding the code. Other tools help in refactoring object-oriented code towards introducing design patterns, but human intelligence is needed to detect where these design patterns should be inserted. This paper proposes a logic approach to the automatic detection of places within object-oriented code where the Composite design pattern could have been used. Suspects identified by such a tool could very well be served as input data for other tools that automatically refactor the code as to introduce the missing design pattern.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5520665,no
Testing-oriented improvements of OCL specification patterns,"Detailed and unequivocal model specifications are a prerequisite for attaining the automated software development goal as promoted by the Model Driven Engineering paradigm. The use of Design by Contract assists in creating such model specifications. However, writing from scratch a large amount of assertions can be tedious, time-consuming, and error-prone. Consequently, a number of constraint patterns have been identified in the literature, and corresponding OCL specifications have been proposed. Automating their use in tools should speed the writing task and increase its correctness. Yet, no attention has been paid to the influence of such specifications on the testing process. We approach this area by proposing new OCL specification patterns for some of the existing constraint patterns. Our proposal should increase the efficiency while testing/debugging models and applications. Relevant examples and tool-support are used in order to explain and validate our approach.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5520815,no
Visual robot guidance in conveyor tracking with belt variables,"The paper presents a method and related algorithm for visual robot guidance in tracking objects moving on conveyor belts; the instantaneous location of the moving object is evaluated by a vision system consisting from a stationary, down looking monochrome video camera, a controller-integrated image processor and a vision extension of the structured V+ robot programming environment. The algorithm for visual tracking of the conveyor belt for """"on-the-fly"""" object grasping is partitioned in two stages: (i) visual planning of the instantaneous destination of the robot, (ii) dynamic re-planning of the robot's destination while tracking the object moving on the conveyor belt. The control method uses in addition the concept of Conveyor Belt Window - CBW. The paper discusses in detail the motion control algorithm for visually tracking the conveyor belt on which objects are detected, recognised and located by the vision part of the system. Experiments are finally reported in what concerns the statistics of object locating errors and motion planning errors function of the size of the objects and of the belt speed. These tests have been carried out on a 4-d.o.f. SCARA robot with artificial vision and the AVI AdeptVision software extension of the V+ high-level, structured programming environment.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5520880,no
Achieving Flow-Level Controllability in Network Intrusion Detection System,"Current network intrusion detection systems are lack of controllability, manifested as significant packet loss due to the long-term resources occupation by a single flow. The reasons can be classified into two kinds. The first kind is known as normal reasons, that is, the processing of mass arriving packets of a large flow can not be limited to a determinable period of time and thus makes other flows starved. The second kind, in which the CPU is trapped in a dead-loop like state due to processing some packets with particular content of a flow, is considered as abnormal reasons. In fact, it is a kind of software crashes. In this paper, we discuss the innate defects of traditional packet-driven NIDS, and implement a flow-driven framework which can achieve fine-grained controllability. An Active Two-threshold scheme based on ideal Exit-Point (ATEP) is proposed in order to diminish data preserving overhead during flow switches and to detect crash in time. A quick crash recovery mechanism is also given which can recover the trapped thread from 90% crashes in 0.2 ms. The experimental results show that our flow-driven framework with ATEP scheme can achieve higher throughput and less packet loss ratio than the uncontrollable packet-driven systems with less than 1% of extra CPU overhead. What's more, in the case of crash occurrence, the ATEP scheme is still able to maintain rather steady throughput without sudden decrease.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5521501,no
An Automatic Approach to Model Checking UML State Machines,"UML has become the dominant modeling language in software engineering arena. In order to reduce cost induced by design issues, it is crucial to detect model-level errors in the initial phase of software development. In this paper, we focus on the formal verification of dynamic behavior of UML diagrams. We present an approach to automatically verifying models composed of UML state machines. Our approach is to translate UML models to the input language of our home grown model checker PAT in such a way as to be transparent for users. Compared to previous efforts, our approach supports a more complete subset of state machine including fork, join, history and submachine features. It alleviates the state explosion problem by limiting the use of auxiliary variables. Additionally, this approach allows to check safety/liveness properties (with various fairness assumptions), trace refinement relationships and so on with the help of PAT.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5521548,no
An Approach to Achieving the Reliability in TV Embedded System,"In this paper we propose an approach to improving the reliability of a TV set through the systematical model-based automated testing procedure. The proposed approach defines a probabilistic transition model of a TV set based on user behavior. The test scenarios are derived from the usage model and conducted through the automated execution framework, in which the TV set under testing is treated as a black box. Based on test results, the reliability analysis is performed.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5521550,no
Are Longer Test Sequences Always Better? - A Reliability Theoretical Analysis,"One of the interesting questions currently discussed in software testing, both in practice and academia, is the role of test sequences on software testing, especially on fault detection. Previous work includes empirical research on rather small examples tested by relatively short test sequences. Belief is """"the longer the better"""", i.e., the longer test sequences are, the more faults are detected. This paper extends those approaches applied to a large commercial application using test sequences of increasing length, which are generated and selected by graph-model-based techniques. Experiments applying many software reliability models of different categories deliver surprising results.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5521564,no
Quantitative Evaluation of Related Web-Based Vulnerabilities,Current web application scanner reports contribute little to diagnosis and remediation when dealing with vulnerabilities that are related or vulnerability variants. We propose a quantitative framework that combines degree of confidence reports pre-computed from various scanners. The output is evaluated and mapped based on derived metrics to appropriate remediation for the detected vulnerabilities and vulnerability variants. The objective is to provide a trusted level of diagnosis and remediation that is appropriate. Examples based on commercial scanners and existing vulnerabilities and variants are used to demonstrate the framework's capability.,2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5521569,no
Studying the Impact of Social Structures on Software Quality,"Correcting software defects accounts for a significant amount of resources such as time, money and personnel. To be able to focus testing efforts where needed the most, researchers have studied statistical models to predict in which parts of a software future defects are likely to occur. By studying the mathematical relations between predictor variables used in these models, researchers can form an increased understanding of the important connections between development activities and software quality. Predictor variables used in past top-performing models are largely based on file-oriented measures, such as source code and churn metrics. However, source code is the end product of numerous interlaced and collaborative activities carried out by developers. Traces of such activities can be found in the repositories used to manage development efforts. In this paper, we investigate statistical models, to study the impact of social structures between developers and end-users on software quality. These models use predictor variables based on social information mined from the issue tracking and version control repositories of a large open-source software project. The results of our case study are promising and indicate that statistical models based on social information have a similar degree of explanatory power as traditional models. Furthermore, our findings suggest that social information does not substitute, but rather augments traditional product and process-based metrics used in defect prediction models.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5521754,no
A Technique for Just-in-Time Clone Detection in Large Scale Systems,"Existing clone tracking tools have limited support for sharing clone information between developers in a large scale system. Developers are not notified when new clones are introduced by other developers or when existing clones are modified. We propose a client-server architecture that centrally detects and maintains clone information for an entire software system stored in a version control system. Clients retrieve a list of clones relevant to the code they are working on from the server. Whenever an update is committed to the version control system, the server detects and incrementally updates clone information. We propose techniques to improve the speed of the incremental clone detection. In order to reduce the number of comparisons required for clone detection, we select representative clones from the existing clone list. We build a string-based technique to compare the newly committed code with the representative clones and to update the clone list. In a case study, we show that our approach significantly reduces the clone detection time, while supporting clone detection across the entire software system.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5521760,no
Adatpive single phase fault identification and selection technique for maintaining continued operaton of distributed generation,This paper presents the development of an adaptive rule based fault identification and phase selection technique to be used in the implementation of single phase auto-reclosing (SPAR) in power distribution network with distributed generators (DGs). The proposed method uses only the three line currents measured at the relay point. The waveform pattern of phase angle and symmetrical components of the three line currents during transient period of fault condition are analyzed using conditioning rules with IF-THEN in order to determine the type of single phase to ground fault and initiate single pole auto-reclosing command or three phase reclosing command for other type of faults. The proposed method is implemented and verified in PSCAD/EMTDC power system software. The test results show that the proposed method can correctly detects the faulty phase within one cycle in power distribution network with DGs under various network operating conditions.,2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5522214,no
Research on Modeling and Simulation of Activated Sludge Process,"Due to the complexity of wastewater treatment process, it is difficult to apply the existing mathematical models in practice. A new model is presented for the wastewater treatment process in this paper. This model is based on Benchmark Simulation Model no.1 (BSM1) modeling method, and then simplifies Activated Sludge Model No. 1 (ASM1) which was set up to connect the secondary settler model dynamically. Meanwhile, the parameters of the model are adjusted by the experiment data. Finally, the practical data was used to predict the COD values of the water quality. The results demonstrate that this proposed model is useful.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5522801,no
Design of Real-Time Monitoring System of Bridgman Single Crystal Growth Parameters,"A computer system has been developed to detect Bridgman single crystal growth parameters. The main hardware is a Personal Computer. Software is Kingview6.53. Detected parameters are the temperature gradient within a furnace chamber, the crucible rotation speed, the crucible pull-down rate, the Solid-liquid interface temperature and so on. Depend on the detection values can analyze the causes of a crystal growth lacuna existence. It provides a basis for improving a crystal growth method and enhancing a crystal growth quality.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5522858,no
Finite Element Modelling of Circumferential Magnetic Flux Leakage Inspection in Pipeline,"The axial magnetic flux leakage(MFL) inspection tools cannot reliably detect or size axially aligned cracks, such as SCC, longitudinal corrosion, long seam defects, and axially oriented mechanical damage. To focus on this problem, the circumferential MFL inspection tool is introduced. The finite element (FE) model is established by adopting ANSYS software to simulate magnetostatics. The results show that the amount of flux that is diverted out of the pipe depends on the geometry of the defect, the primary variables that affect the flux leakage are the ones that define the volume of the defect. The defect location can significantly affect flux leakage, the magnetic field magnitude arising due to the presence of the defect is immersed in the high field close to the permanent magnets. These results demonstrate the feasibility of detecting narrow axial defects and the practicality of developing a circumferential MFL tool.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5523141,no
Eyecharts: Constructive benchmarking of gate sizing heuristics,"Discrete gate sizing is one of the most commonly used, flexible, and powerful techniques for digital circuit optimization. The underlying problem has been proven to be NP-hard. Several (suboptimal) gate sizing heuristics have been proposed over the past two decades, but research has suffered from the lack of any systematic way of assessing the quality of the proposed algorithms. We develop a method to generate benchmark circuits (called eyecharts) of arbitrary size along with a method to compute their optimal solutions using dynamic programming. We evaluate the suboptimalities of some popular gate sizing algorithms. Eyecharts help diagnose the weaknesses of existing gate sizing algorithms, enable systematic and quantitative comparison of sizing algorithms, and catalyze further gate sizing research. Our results show that common sizing methods (including commercial tools) can be suboptimal by as much as 54% (V<sub>t</sub>-assignment), 46% (gate sizing) and 49% (gate-length biasing) for realistic libraries and circuit topologies.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5523536,no
Automated development tools for linux USB drivers,"USB devices are widely used in consumer electronics. Writing device drivers has always been a tedious and error-prone job. This paper presents assisting tools for developing USB drivers under Linux OS. The tool kit includes (1) a generic-skeleton generator that can automatically generate generic USB driver code skeleton according to user-specified configuration, (2) a flattened-HID-driver generator that can merge stacked HID drivers to a monolithic driver and prune C codes to reduce size and response time for embedded applications, and (3) an ECP (Extended C Preprocessor) compiler that provides type-checking capability for low-level I/O operation and makes the driver code more readable.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5523719,no
Objective video quality assessment of mobile television receivers,"The automated evaluation of mobile television receivers shall be facilitated by objective video quality assessment. Therefore, a test bench was set up, whose design and signal flow will be presented first. We describe and compare different full-reference video quality metrics and a simple no-reference metric from literature. The implemented metrics are evaluated and then used to assess the video quality of receivers for digital television broadcast by applying different RF scenarios. We present the achieved results with the different metrics for the purpose of receiver comparison.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5523722,no
A safety related analog input module based on diagnosis and redundancy,"This paper introduces a safety-related analog input module to achieve data acquisition of 4-20mA current signals. It is an integral part of the safety-instrumented systems (SIS) which is used to provide critical control and safety applications for automation users. In order to ensure the performance of analog input circuit in good condition, a combination of hardware and software diagnosis should be carried out periodically. These kinds of internal self-diagnosis allow the device to detect improper operation within itself. If potentially dangerous process occurs, the AI has redundancy to maintain operation even when parts fail. The article presents special hardware, diagnostic software and full fault injection testing of the complete design. The test result shows that the safety-related AI is capable of detecting and locating of mostly potential faults and internal component failures.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5524110,no
A study of medical image tampering detection,"Currently, methods of image tampering detection are divided into two categories, active detection and passive detection. In this paper, we try to review several detecting methods and hope this will offer some help to this field. We will focus on the passive detection method for medical images and show some results of our experiments in which we extract statistical features (IQM and HOWS based) of source images and their doctored version respectively. Manipulations we take to doctor the images include: brightness adjustment, rotation, scale, filtering, compression and so on, using fix manipulation parameter and random selected parameter. Different classifiers are chosen then to discriminate the source images from the doctored ones. We compare the performance of the classifiers to show that the passive detection methods are effective while dealing with medical image tapering detecting.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5528509,no
Overview of power system operational reliability,"The traditional reliability evaluation assesses the long-term performance of power system but its constant failure rate cannot reflect the time-varying performances in an operational time frame. This paper studies the operational reliability of power system in the real-time operating conditions based on online operation information obtained from the Energy Management System (EMS). A framework of operational reliability evaluation is proposed systematically. The effects of components' inherence conditions, environment conditions and operating electrical conditions on failure rates are considered in their operational reliability models. To meet the restrictive requirement of real-time evaluation, a special algorithm is presented. The indices for operational reliability are defined as well. The software, Operational Reliability Evaluation Tools (ORET), is developed to implement the above described functions. The work reported in this paper can be used to assess the system risk in an operational timeframe and give warnings when the system reliability is low.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5528993,no
Using Aurora Road Network Modeler for Active Traffic Management,"Active Traffic Management (ATM) is the ability to dynamically manage recurrent and nonrecurrent congestion based on prevailing traffic conditions. Focusing on trip reliability, it maximizes the effectiveness and efficiency of freeway corridors. ATM relies on fast and trustworthy traffic simulation software that can assess a large number of control strategies for a given road network, given various scenarios, in a matter of minutes. Effective traffic density estimation is crucial for the successful deployment of feedback algorithms for congestion control. Aurora Road Network Modeler (RNM) is an open-source macrosimulation tool set for operational planning and management of freeway corridors. Aurora RNM employs Cell Transmission Model (CTM) for road networks extended to support multiple vehicle classes. It allows dynamic filtering of measurement data coming from traffic sensors for the estimation of traffic density. In this capacity, it can be used for detection of faulty sensors. The virtual sensor infrastructure of Aurora RNM serves as an interface to the real world measurement devices, as well as a simulation of such measurement devices.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5530532,no
Failure prediction method for Network Management System by using Bayesian network and shared database,"Network Management System (NMS) is a service that employs a variety of tools, applications, and devices to assist network administrators on monitoring and maintaining network. Keeping the network in high quality of service is the main purpose of NMS. This paper proposed a method to solve the network problem by making a prediction of failure based on network-data behavior. The prediction represented by conditional probability generated by Bayesian network. Bayesian network is a probability graphical model for representing the probabilistic relationship among a large number of variables and doing probabilistic inference with those variables. In order to describe how the prediction works, we discuss the prediction result by simulation on network congestion.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5532294,no
The Application of Wireless Sensor Networks in Machinery Fault Diagnosis,"Wireless sensor network is a thriving information collecting and processing technology, which is widely used in military field, industry and environmental monitoring, etc. In a wireless sensor network which is made up of tens of thousands battery-powered sensor nodes, data fusion technology can be used to reduce communication traffic in order to save energy. With respect to large mechanical equipment, traditional wired sensors are commonly used for fault detection and diagnosis. There will be no wiring problem if wireless sensor networks are used, which is favorable to detect potential problems in mechanical equipment without affecting normal production of enterprises. In this paper, the application of wireless sensor networks in machinery fault diagnosis is studied, a data fusion model for machinery fault diagnosis in wireless sensor networks and PCA neural data fusion algorithm are proposed, and the effectiveness of the method is demonstrated in an experiment.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5532717,no
Formal Fault Tolerant Architecture,"This paper shows the need of development by refinement: from most abstract specification to the implementation, in order to ensure 1) the traceability of the needs and requirements, 2) a good management of the development and 3) a reliable and fault-tolerant design of systems. We propose a formal architecture of models and methods for critical requirements and fault-tolerance. System complexity increases and the choices of their implementation are numerous. So the architecture verification achieves a prominent role in the system design cycle. Fault detecting at this early level decreases the time and costs of correction. We show how a formal method, B method, may be used to write the abstract specification of a system then to product correct-by-construction architecture through many steps of formal refinement. During these steps, a fault scenario is injected with a suitable introspective reaction by the system. All refinement steps, including the introspective correction, should be proven to be correct and satisfy the initial specification of the system. At the lower levels, design is separated between hardware and software communities. But even at these levels many design traces could be captured to prove not only the consistency of each design unit but the coherence between the different sub-parts: software, digital or other technologies",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5532782,no
A Modified History Based Weighted Average Voting with Soft-Dynamic Threshold,"Voting is a widely used fault-masking technique for real time systems. Several voting algorithms exist in literature. In this paper, a survey on the few existing voting algorithms is presented and a modified history based weighted average voting algorithm with soft-dynamic threshold value is proposed with two different weight assignment techniques, which combines all the advantages of the surveyed voting algorithms but overcomes their deficiencies. The proposed algorithm with both type of weight assignment techniques, gives better performance compared to the existing history based weighted average voting algorithms in the presence of intermittent errors. In the presence of permanent errors, when all the modules are fault prone, the proposed algorithm with first type of weight assignment technique gives higher availability than all the surveyed voting algorithms. If at least one module is fault free, this algorithm gives almost 100 % safety and also higher range of availability than the other surveyed voting algorithms.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5532841,no
Software component quality prediction using KNN and Fuzzy logic,"Prediction of product quality within software engineering, preventive and corrective actions within the various project phases are constantly improved over the past decades. Practitioners and software companies were using various methods, different approaches and best practices in software development projects. Nevertheless, the issue of quality is pushing software companies to constantly invest in efforts to produce enough quality products that will arrive in time, with good enough quality to the customer. However, the quality is not for free, it has a price that is required at the time you notice about her. In this paper fuzzy logic and KNN classification method approaches are presented to predict Weibull distribution parameters shape, slope and the total number of faults in the system based on the software components individual contribution. Since the Weibull distribution is one of the most widely used probability distributions in the reliability engineering, predicting of its characteristics early in the software lifecycle might be useful input for the planning and control of verification activities.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5533413,no
Streamlining collection of training samples for object detection and classification in video,"This paper is concerned with object recognition and detection in computer vision. Many promising approaches in the field exploit the knowledge contained in a collection of manually annotated training samples. In the resulting paradigm, the recognition algorithm is automatically constructed by some machine learning technique. It has been shown that the quantity and quality of positive and negative training samples is critical for good performance of such approaches. However, collecting the samples requires tedious manual effort which is expensive in time and prone to error. In this paper we present design and implementation of a software system which addresses these problems. The system supports an iterative approach whereby the current state-of-the-art detection and recognition algorithms are used to streamline the collection of additional training samples. The presented experiments have been performed in the frame of a research project aiming at automatic detection and recognition of traffic signs in video.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5533507,no
Research on the 3D visual rapid design platform of belt conveyor based on AutoCAD,"Belt conveyors are widely used in the areas of coal, ports, electric power, chemical, etc, to transport bulk solid. The traditional design methods have many defects, such as long design cycle, prone to error, and it is unable to meet the needs of modern society. This paper presents a new method of designing belt conveyors, the 3D visual rapid design platform based on the AutoCAD software for the design of belt conveyors. This platform uses Visual Basic software to develop the rapid design program, establishes the stereo models of the belt conveyor by extending the functions of AutoCAD with ObjectARX. So it builds the three-dimensional design environment in which the designers can adjust the structure of the belt conveyor based on the actual environment, manage the parts of the belt conveyor and get the engineering drawing of the belt conveyor. This platform increases the design speed and improves the design quality.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5535529,no
Process research on cogging rolling of H beam,"H beam has been widely used in industry and civil steel structure. But there are many key technologies that must be solved in order to produce high quality H-beam. This paper describes an investigation into the rolling technology using MARC software to model reversing rolling process of H-beam and compare the results with PDA data. A FEM model involving in three-dimensional, elasto-plastic and thermo-mechanical coupling has been established successfully to predict this multi-pass rolling process. The analysis produces outputs such as deformation rules, rolling force. The mechanism that the web is built up during the rolling process is also discussed. The influence of web reduction, the ratio of web reduction ration and flange reduction ration are also discussed. A new rolling schedule is suggested in order to improve the quality of the final product and improve rolling efficiency.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5535568,no
Accurate diagnosis of rolling bearing based on wavelet packet and genetic-support vector machine,This paper studies on the combination usage of wavelet packet and artificial genetic-support vector machine in the fault diagnosis of ball bearing. Energy eigenvector of frequency domain is extracted using wavelet packet analysis method. Fault state of ball bearing is identified by using radial basis function genetic-support vector machine. The test results show that this GSVM model is effective to detect fault of ball bearing.,2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5535701,no
Development on preventive maintenance management system for expressway asphalt pavements,"In view of the status that there was no expressway pavement preventive maintenance management system at home and abroad at present, based on the technology theory obtained by the author and the demands and process of expressway asphalt pavement preventive maintenance management, preventive maintenance management system for expressway asphalt pavement (EPMMS (V1.0)) was developed. The work or functions of expressway maintenance quality evaluating, pavement performance predicting, optimum selecting of preventive maintenance treatment, the optimal timing determining, and post-evaluating could be realized or auxiliary realized, The theoretical foundation, development environment and the whole framework were expounded in the paper, the development and realization process of the five sub-systems was introduced in detail, the system testing and application were briefly introduced. The test results showed that this system could meet the demands of software products register test code. Primary application showed the system was correct and efficient, software support would be provided for expressway pavement preventive maintenance management, the pavement preventive maintenance management would be more standardized and convenient, and also the demands of the expressway maintenance management department could be greatly met by this system.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5536248,no
Automatic detection technology of surface defects on plastic products based on machine vision,"It is very necessary to detect surface defects on plastic products during production process and post treatment. The research and application of automatic detection technology of surface defects on plastic products is supposed to greatly liberate the human workforce, improve the automated production level, and has wide application prospect. The development of machine vision's key technologies such as illumination system, CCD camera, image enhancement, image segmentation, image recognition, and so on has been explained in detail. Its application on detection for surface defects on plastic products such as plastic electronic components, strips, PVC building materials, films, leather, bottles, and so on is also presented briefly. Especially, it mainly focuses on the automatic detection of surface defects for injection products, and the automatic detection system is proposed. It is composed of the conveyor belt device, the image acquisition and processing software and PLC control device, et al.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5536470,no
Design and implementation of a direct RF-to-digital UHF-TV multichannel transceiver,"This manuscript presents the design and implementation of a direct UHF digital transceiver that provides direct sampling of the UHF-TV input signal spectrum. Our SDR-based approach is based on a pair of high-speed ADC/DAC devices along with a channelizer, a dechannelizer and a channel management unit, which is capable of inserting, deleting and/or conmuting individual RF-TV channels. Simulation results and in-lab measurements assess that the proposed system is able to receive, manipulate and retransmit a real UHF-TV spectrum at a negligible quality penalty cost.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5537685,no
Urban land use change detection based on RS&GIS,"Urban land use change may influence natural phenomena and ecological processes. Decreasing of cultivated land area in Chengdu is one of the critical problems in recent years. The objective of this study is to detect land use changes between 1992 to 2008 using satellite images of Landsat TM/ETM+. The land use maps of 1992 waw collected from 1:50000 digital maps and Arc/View 3.2 software. TM/ETM+ satellite data were used to generate land use map. The images quality assessment and georeferencing were performed on images. Different suitable spectral transformations such as rationing, PCA, Tasseled Cap transformation and data fusion were performed on the images in ENVI4.0 and ERDAS IMAGE 8.5 software. Image classification was done using supervised classification maximum likelihood and minimum distance classifier utilizing original and synthetic bands resulted from diverse spectral transformation. The result of change detection shows that the cultivated land area decreased between 1992 and 2008 by 31.683% from 21.12710<sup>4</sup>hm<sup>2</sup> to 14.43310<sup>4</sup>hm<sup>2</sup>, and the built-land area increased between 1992 and 2008 by 66.318% from 4.42710<sup>4</sup>hm<sup>2</sup> to 7.36310<sup>4</sup>hm<sup>2</sup>. Also, the area with irrigated land farms have been decreased to 6.69410<sup>4</sup>hm<sup>2</sup> by 75.343% and the dry land farming area increased to 2.72810<sup>4</sup>hm<sup>2</sup> by 9.2%. The overlaying map of land use change shows that intensity of conversion of arable lands to built-land increased, and its speed of spatio-temporal change of urban land use is notable.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5538377,no
An efficient polling scheme for wireless LANs,"With the popularity of WLAN application how to meet the requirements of various applications and perform more efficiently on the limited bandwidth of wireless networks becomes a key problem. This paper suggest a new scheme of MAC protocol of WLAN, making possible the service of the two-class priority station polling system under the mixed policy of exhaustive and gated services, optimizing the services of the system in time of load change input via adjustment of the times of the access gated services, and strengthening the flexibility and fairness of multimedia transmissions in wireless networks. The theoretical model is established with discretetime Markov chain and probability generating function. The analyses demonstrate that this scheme enables an effective allocation of channel resources for different tasks, guarantees the quality of system services.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5540711,no
A SVM-based method for abnormity detection of log curves,"Rapid and accurate abnormity detection of log curves is critical in the quality control for logging industry. Traditional methods based on manual detection have been proven to be ineffective and unreliable. A machine learning method based on Support Vector Machine (SVM) is proposed in this paper to address this problem. The SVM classifiers are established according to the suspicious sections selected from log curves and the detected results given by experts. A genetic algorithm (GA) is introduced for optimization of parameters. With GA and SVM fusions, the optimal models for classifiers are determined to detect abnormity sections. Experimental results of China XiangJiang Oilfield show that an accuracy of 95% is achieved for suspicious straight sections and 96% is achieved for suspicious bouncing sections, which has proven the feasibility of this method.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5540755,no
Quality of learning analysis based on Bayesian Network,"The level of quality of learning is directly related to the competitiveness of students in the future social life, the overall quality and comprehensive national strength of Chinese citizens; the establishment and improvement of student learning quality analysis and guiding systems are the strategic starting point for promotion of education. The Bayesian Network (BN) proposed by Pearl is a new mechanism for uncertain knowledge representation and manipulation based on probability theory and graph theory. BN is network structure with clarity semantics. It exploits the structure of the domain to allow a compact representation of complex joint probability distribution. Its sound probabilistic semantics explicit encoding of relevance relationships, inference algorithms and learning algorithms that are fairly efficient and effective in practice, and decision-making mechanism of facility, have led BN to enter the Artificial Intelligence(AI) mainstream. The present thesis is to make an experimental analysis of the test paper based on Bayesian Network. The main toolkit used in this experiment is BNT software suite compiled with MATLAB. This software suite provides us with a lot of basic function sets for Bayes Network learning. It is suitable for the accurate and appropriate logics of various types of joints, and it also has the function of parameter learning and structure learning. From the experiment we come to the conclusion that five factors including absorption rate of teaching and work accuracy have great influence on quality of learning.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5540920,no
Research on fruit firmness test system based on virtual instrument technology,"A fruit firmness detection system was designed based on LabVIEW development environment which detects the displacement of the pendulum with eddy current displacement sensors according to the impact pendulum method and the signals were treated by the JKU-12 data acquisition cards. The whole system debug successfully in WinXP SP2, LabVIEW8.2 trial version of the Chinese environment. It could record the details of the probe penetrating into the fruit. The system was proved stable and reliable, and it provides a good technical method to test and analyze fruit firmness.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5541086,no
Probabilistic neural network and its application,"Nuclear marine apparatus is a huge complicated system, most of which equipments are of nonlinearity, time varying, coupling and inexactness. Neuralnet is widely applied in nuclear fault diagnosis for its approaching any kinds of nonlinearity mapping. At present, BP neural net is used more widely, but the layers of the net and the neurones on each layer can not be delimited easily; such net may fall into the minimum point in the course of training. In this essay, PNN proves effective in diagnosing faults on nuclearmarine apparatus.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5541345,no
Analysis of the effect of Java software faults on security vulnerabilities and their detection by commercial web vulnerability scanner tool,"Most software systems developed nowadays are highly complex and subject to strict time constraints, and are often deployed with critical software faults. In many cases, software faults are responsible for security vulnerabilities which are exploited by hackers. Automatic web vulnerability scanners can help to locate these vulnerabilities. Trustworthiness of the results that these tools provide is important; hence, relevance of the results must be assessed. We analyze the effect on security vulnerabilities of Java software faults injected on source code of Web applications. We assess how these faults affect the behavior of the scanner vulnerability tool, to validate the results of its application. Software fault injection techniques and attack trees models were used to support the experiments. The injected software faults influenced the application behavior and, consequently, the behavior of the scanner tool. High percentage of uncovered vulnerabilities as well as false positives points out the limitations of the tool.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5542602,no
An empirical study of the influence of software Trustworthy Attributes to Software Trustworthiness,"Software Trustworthiness is a hotspot problem in software engineering, and Software Trustworthy Attribute is the base of Software Trustworthiness. Software defect is the basic reason that influences Software Trustworthiness. Therefore, in this thesis we will attempt to utilize text classification technology to classify the historical software detects according to Software Trustworthy Attributes, in order to analyze the influence of Software Trustworthy Attributes to Software Trustworthiness, get the pivotal attribute of Software Trustworthy Attributes; and analyze the influence of Software Trustworthy Attributes to Software Trustworthiness in different version of Gentoo Linux.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5542851,no
A data mining based method: Detecting software defects in source code,"With the expansion of software size and complexity, how to detect defects becomes a challenging problem. This paper proposes a defect detection method which applies data mining techniques in source code to detect two types of defects in one process. The two types of defects are rule-violating defects and copy-paste related defects which may include semantic defects. During the process, this method can also extract implicit programming rules without prior knowledge of the software and detect copy-paste segments with different granularities. The method is evaluated with the Linux kernel that contains more than 4 million lines of C code. The result shows that the resulting system can quickly detect many programming rules and violations to the rules. After using the novel pruning techniques, it will greatly reduce the effort of manually checking violations so as a large number of false positives are effectively eliminated. As an illustrative example of its effectiveness, a case study shows that among the top 50 violations reported by the proposed model, 11 defects can be confirmed after examining the source code.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5542852,no
Time series analysis for bug number prediction,"Monitoring and predicting the increasing or decreasing trend of bug number in a software system is of great importance to both software project managers and software end-users. For software managers, accurate prediction of bug number of a software system will assist them in making timely decisions, such as effort investment and resource allocation. For software end-users, knowing possible bug number of their systems will enable them to take timely actions in coping with loss caused by possible system failures. To accomplish this goal, in this paper, we model the bug number data per month as time series and, use time series analysis algorithms as ARIMA and X12 enhanced ARIMA to predict bug number, in comparison with polynomial regression as the baseline. X12 is the widely used seasonal adjustment algorithm proposed by U.S. Census. The case study based on Debian bug data from March 1996 to August 2009 shows that X12 enhanced ARIMA can achieve the best performance in bug number prediction. Moreover, both ARIMA and X12 enhanced ARIMA outperform the baseline as polynomial regression.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5542853,no
Modeling risk factors dependence using Copula method for assessing software schedule risk,"Based on risk factor method, a model for building dependence among risk factors is discussed for assessing schedule risk of software project. Three main risk factors which influence software schedule are described. Copula method is used to model the dependence among risk factor, and Monte Carlo method is adopted to simulate the schedule risk model. The empirical results from a Communication Corporation show that schedule risk of the new software project is overestimated without modeling the dependence among risk factors. Moreover, the model with considering risk factor dependence by Copula can assess the schedule risk accurately.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5542857,no
3D protein model assessment using geometric and biological features,"Automatic prediction of protein three-dimensional structures from its amino acid sequence has become one of the most important researched fields in bioinformatics. With that increases the importance of determining the quality of these protein models. Protein three-dimensional structure evaluation is a complex problem in computational structure biology. We attempt to solve this problem using SVM and information from both sequence and structure of the protein. The goal is to generate a machine that understands structures from PDB and when given a new model, predicts whether it belongs to the same class as the PDB structures or not (correct or incorrect protein model). Here we show one such machine; results appear promising for further analysis. For the purpose of reducing computational overhead multiprocessor environment and basic feature selection method is used.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5542898,no
GUI test-case generation with macro-event contracts,"To perform a comprehensive GUI testing, a large number of test cases are needed. This paper proposes a GUI test-case generation approach that is suitable for system testing. The key idea is to extend high-level GUI scenarios with contracts and use the contracts to infer the ordering dependencies of the scenarios. From the ordering dependencies, a state machine of the system is constructed and used to generate test cases automatically. A case study is conducted to investigate the quality of the test cases generated by the proposed approach. The results showed that, in comparison to creating test cases manually, the proposed approach can detect more faults with less human effort.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5542936,no
A computer-vision-assisted system for Videodescription scripting,"We present an application of video indexing/summarization to produce Videodescription (VD) for the blinds. Audio and computer vision technologies can automatically detect and recognize many elements that are pertinent to VD which can speed-up the VD production process. We have developed and integrated many of them into a first computer-assisted VD production software. The paper presents the main outcomes of this R&D activity started 5 years ago in our laboratory. Up to now, usability performance on various video and TV series types have shown a reduction of up to 50% in the VD time production process.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5543575,no
Numerical simulation of metal interconnects of power semiconductor devices,"This paper presents a methodology and a software tool - R3D - for extraction, simulations, analysis, and optimization of metal interconnects of power semiconductor devices. This tool allows an automated calculation of large area device Rdson value, to analyze current density and potential distributions, to design sense device, and to optimize a layout to achieve a balanced and optimal design. R3D helps to reduce the probability of a layout error, and drastically speeds up and improves the quality of layout design.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5544002,no
Assessing and improving the effectiveness of logs for the analysis of software faults,"Event logs are the primary source of data to characterize the dependability behavior of a computing system during the operational phase. However, they are inadequate to provide evidence of software faults, which are nowadays among the main causes of system outages. This paper proposes an approach based on software fault injection to assess the effectiveness of logs to keep track of software faults triggered in the field. Injection results are used to provide guidelines to improve the ability of logging mechanisms to report the effects of software faults. The benefits of the approach are shown by means of experimental results on three widely used software systems.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5544279,no
An empirical investigation of fault types in space mission system software,"As space mission software becomes more complex, the ability to effectively deal with faults is increasingly important. The strategies that can be employed for fighting a software bug depend on its fault type. Bohrbugs are easily isolated and removed during software testing. Mandelbugs appear to behave chaotically. While it is more difficult to detect these faults during testing, it may not be necessary to correct them; a simple retry after a failure occurrence may work. Aging-related bugs, a sub-class of Mandelbugs, can cause an increasing failure rate. For these faults, proactive techniques may prevent future failures. In this paper, we analyze the faults discovered in the on-board software for 18 JPL/NASA space missions. We present the proportions of the various fault types and study how they have evolved over time. Moreover, we examine whether or not the fault type and attributes such as the failure effect are independent.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5544284,no
Application of a fault injection based dependability assessment process to a commercial safety critical nuclear reactor protection system,"Existing nuclear power generation facilities are currently seeking to replace obsolete analog Instrumentation and Control (I&C) systems with contemporary digital and processor based systems. However, as new technology is introduced into existing and new plants, it becomes vital to assess the impact of that technology on plant safety. From a regulatory point of view, the introduction or consideration of new digital I&C systems into nuclear power plants raises concerns regarding the possibility that the fielding of these I&C systems may introduce unknown or unanticipated failure modes. In this paper, we present a fault injection based safety assessment methodology that was applied to a commercial safety grade digital Reactor Protection System. Approximately 10,000 fault injections were applied to the system. This paper presents a overview of the research effort, lessons learned, and the results of the endeavor.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5544285,no
A study of the internal and external effects of concurrency bugs,"Concurrent programming is increasingly important for achieving performance gains in the multi-core era, but it is also a difficult and error-prone task. Concurrency bugs are particularly difficult to avoid and diagnose, and therefore in order to improve methods for handling such bugs, we need a better understanding of their characteristics. In this paper we present a study of concurrency bugs in MySQL, a widely used database server. While previous studies of real-world concurrency bugs exist, they have centered their attention on the causes of these bugs. In this paper we provide a complementary focus on their effects, which is important for understanding how to detect or tolerate such bugs at run-time. Our study uncovered several interesting facts, such as the existence of a significant number of latent concurrency bugs, which silently corrupt data structures and are exposed to the user potentially much later. We also highlight several implications of our findings for the design of reliable concurrent systems.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5544315,no
Volunteer-instigated connectivity restoration algorithm for Wireless Sensor and Actor Networks,"Due to their applications, Wireless Sensor and Actor Networks (WSANs) have recently been getting significant attention from the research community. In these networks, maintaining inter-actor connectivity is of a paramount concern in order to plan an optimal coordinated response to a detected event. Failure of an actor may partition the inter-actor network into disjoint segments, and may thus hinder inter-actor coordination. This paper presents VCR, a novel distributed algorithm that opts to repair severed connectivity while imposing minimal overhead on the nodes. In VCR the neighbors of the failed actor volunteer to restore connectivity by exploiting their partially utilized transmission range and by repositioning closer to the failed actor. Furthermore, a diffusion force is applied among the relocating actors based on transmission range in order to reduce potential of interference, and improve connectivity. VCR is validated through simulation and is shown to outperform contemporary schemes found in the literature.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5544679,no
Diverse Partial Memory Replication,"An important approach for software dependability is the use of diversity to detect and/or tolerate errors. We develop and evaluate an approach for automated program diversity called Diverse Partial Memory Replication (DPMR), aimed at detecting memory safety errors. DPMR is an automatic compiler transformation that replicates some subset of an executable's data memory and applies one or more diversity transformations to the replica. DPMR can detect any kind of memory safety errors in any part of a program's data memory. Moreover, DPMR is novel because it uses partial replication within a single address space, replicating (and comparing) only a subset of a program's memory. We also perform a detailed study of the diversity mechanisms and state comparison policies in DPMR (a first of its kind for such diversity approaches), which is valuable for exploiting the high flexibility of DPMR.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5545012,no
Assignments acceptance strategy in a Modified PSO Algorithm to elevate local optima in solving class scheduling problems,Local optima in optimization problems describes a state where no small modification of the current best solution will produce a solution that is better. This situation will make the optimization algorithm unable to find a way to global optimum and finally the quality of the generated solution is not as expected. This paper proposes an assignment acceptance strategy in a Modified PSO Algorithm to elevate local optima in solving class scheduling problems. The assignments which reduce the value of objective function will be totally accepted and the assignment which increases or maintains the value of objective function will be accepted based on acceptance probability. Five combinations of acceptance probabilities for both types of assignments were tested in order to see their effect in helping particles moving out from local optima and also their effect towards the final penalty of the solution. The performance of the proposed technique was measured based on percentage penalty reduction (%PR). Five sets of data from International Timetabling Competition were used in the experiment. The experimental results shows that the acceptance probability of 1 for neutral assignment and 0 for negative assignments managed to produce the highest percentage of penalty reduction. This combination of acceptance probability was able to elevate the particle stuck at the local optima which is one of the unwanted situations in solving optimization problems.,2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5545252,no
Proposal for a set of quality attributes relevant for Web 2.0 application success,"Quality and usability of Web applications are considered to be key aspects of their success. If these aspects are not adequately represented in a Web application, or if they are not appropriately combined, there will be little to prevent the users from browsing further in search of an application that will more effectively satisfy their needs. However, the main challenge is to identify key attributes that will retain users on a Web application longer or influence their decision to visit it again. There are many frameworks and methodologies that deal with this issue but very few of them have an emphasis on assessing the quality and usability of Web 2.0 applications. This paper contains a critical review of previous research in the field of Web quality assessment. It provides the theoretical basis for the development of a set of attributes that should be considered when measuring the quality of Web 2.0 applications.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5546431,no
Automatic Concurrency Management for distributed applications,"Building distributed applications is difficult mostly because of concurrency management. Existing approaches primarily include events and threads. Researchers and developers have been debating for decades to prove which is superior. Although the conclusion is far from obvious, this long debate clearly shows that neither of them is perfect. One of the problems is that they are both complex and error-prone. Both events and threads need the programmers to explicitly manage concurrency, and we believe it is just the source of difficulties. In this paper, we propose a novel approach-automatic concurrency management by the runtime system. It dynamically analyzes the programs to discover potential concurrency opportunities; and it dynamically schedules the communication and the computation tasks, resulting in automatic concurrent execution. This approach is inspired by the instruction scheduling technologies used in modern microprocessors, which dynamically exploits instruction-level parallelism. However, hardware scheduling algorithms do not fit software in many aspects, thus we have to design a new scheme completely from scratch. automatic concurrency management is a runtime technique with no modification to the language, compiler or byte code, so it is good at backward compatibility. It is essentially a dynamic optimization for networking programs.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5546705,no
Joint throughput and packet loss probability analysis of IEEE 802.11 networks,"Wireless networks have grown their popularity over the past number of years. Usually wireless networks operate according to IEEE 802.11, which specifies protocols of physical and MAC layers. A number of different studies have been conducted on performance of IEEE 802.11 wireless networks. However, questions of QoS control in such networks have not received sufficient attention from the research community. This paper considers modeling of QoS of IEEE 802.11 networks defined in terms of throughput requirements and packet loss probability limitations. An influence of sizes of packets being transmitted through the network on the QoS is investigated. Extensive simulations confirm results obtained from the mathematical model.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5546809,no
Resilient workflows for high-performance simulation platforms,"Workflows systems are considered here to support large-scale multiphysics simulations. Because the use of large distributed and parallel multi-core infrastructures is prone to software and hardware failures, the paper addresses the need for error recovery procedures. A new mechanism based on asymmetric checkpointing is presented. A rule-based implementation for a distributed workflow platform is detailed.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5547153,no
A systems approach to verification using hardware acceleration,"The increasing complexity of system-on-chip devices has made verification of these devices an extremely difficult task. Additionally, there is the pressure of time-to-market that is faced by all semiconductor companies. Some of the functional complexity of such devices has made it necessary to run system level sequences that were typically run only in post-silicon phase in the pre-silicon stages. However, the effective speeds of simulators used during functional verification do not lend well to running system level tests. In this paper we will describe how a hardware accelerator was used to execute system level tests. We will share some of the results seen and some of the design issues that were detected using such an approach. We have illustrated this approach choosing three distinct areas of (i) secure boot, (ii) built-in-self-test sequences, and (iii) scan testing. We also believe that going to a system level approach using hardware acceleration helps to find several difficult corner case issues that remain undetected using other verification approaches.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5548241,no
SES-based framework for fault-tolerant systems,"Embedded real-time systems are often used in harsh environments, for example engine control systems in automotive vehicles. In such ECUs (Engine Control Unit) faults can lead to serious accidents. In this paper we propose a safety embedded architecture based on coded processing. This framework only needs two channels to provide fault tolerance and allows the detection and identification of permanent and transient faults. Once a fault is detected by an observer unit the SES guard makes it visible and initiates a suitable failure reaction.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5548427,no
Fault-tolerant defect prediction in high-precision foundry,"High-precision foundry production is subjected to rigorous quality controls in order to ensure a proper result. Such exams, however, are extremely expensive and only achieve good results in a posteriori fashion. In previous works, we presented a defect prediction system that achieved a 99% success rate. Still, this approach did not take into account sufficiently the geometry of the casting part models, resulting in higher raw material requirements to guarantee an appropriate outcome. In this paper, we present here a fault-tolerant software solution for casting defect prediction that is able to detect possible defects directly in the design phase by analysing the volume of three-dimensional models. To this end, we propose advanced algorithms to recreate the topology of each foundry part, analyze its volume and simulate the casting procedure, all of them specifically designed for an robust implementation over the latest graphic hardware that ensures an interactive design process.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5549392,no
A component based approach for modeling expert knowledge in engine testing automation systems,"Test automation systems used for developing combustion engines comprise hardware components and software functionality they depend on. Such systems usually perform similar tasks; they comprise similar hardware and execute similar software. Regarding their details, however, literally no two systems are exactly the same. In order to support such variations, the automation system has to be customized accordingly. Without a tools that properly supports both, customization as well as standardization of functionality, customization can be time consuming and error-prone. In this paper we describe a modeling driven approach that is based on components with hard- and software view that allows defining standard functionality for physical hardware. We show how this way most of the automation system's standard functionality can be generated automatically, while still allowing to add custom functionality.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5549594,no
Fault location for a series compensated transmission line based on wavelet transform and an adaptive neuro-fuzzy inference system,"Fault diagnosis is a major area of investigation for power system and intelligent system applications. This paper proposes an efficient and practical algorithm based on using wavelet MRA coefficients for fault detection and classification, as well as accurate fault location. A three-phase transmission line with series compensation is simulated using MATLAB software. The line currents at both ends are processed using an online wavelet transform algorithm to obtain wavelet MRA for fault recognition. Directions and magnitudes of spikes in the wavelet coefficients are used for fault detection and classification. After identifying the fault section, the summation of the sixth level MRA coefficients of the currents are fed to adaptive neuro-fuzzy inference system (ANFIS) to obtain accurate fault location. The proposed scheme is able to detect all types of internal faults at different locations either before or after the series capacitor, at different inception angles, and at different fault resistances. It can also detect the faulty phase(s) and can differentiate between internal and external faults. The simulation results show that the proposed method has the characteristic of a simple and clear recognition process. We conclude that the algorithm is ready for series compensated transmission lines.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5549994,no
"Substation's switchgear's reliability evaluation as a part of transmission, distribution and generation modeling software",The paper gives the main idea of substation's switchgear's reliability evaluation module as a part of software that was developed in Riga Technical University in frames of project Information technology to ensure the sustainability of generation and transmission grid. The module allows easily evaluate switchgear's up-state's probability and interruption time for large amount of different types of switchgears by switchgear's bay's faults and elements standardization. Also it is possible to evaluate energy not supplied and costs related to energy not supplied due to switchgear's fault.,2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5550004,no
The limitations of software signature and basic block sizing in soft error fault coverage,"This paper presents a detailed analysis of the efficiency of software-only techniques to mitigate SEU and SET in microprocessors. A set of well-known rules is presented and implemented automatically to transform an unprotected program into a hardened one. SEU and SET are injected in all sensitive areas of MIPS-based microprocessor architecture. The efficiency of each rule and a combination of them are tested. Experimental results show the limitations of the control-flow techniques in detecting the majority of SEU and SET faults, even when different basic block sizes are evaluated. A further analysis on the undetected faults with control flow effect is done and five causes are explained. The conclusions can lead designers in developing more efficient techniques to detect these types of faults.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5550346,no
Evaluation of a new low cost software level fault tolerance technique to cope with soft errors,"Increasing soft error rates make the protection of combinational logic against transient faults in future technologies a major issue for the fault tolerance community. Since not every transient fault leads to an error at application level, software level fault tolerance has been proposed by several authors as a better approach. In this paper, a new software level technique to detect and correct errors due to transient faults is proposed and compared to a classic one, and the costs of detection and correction for both approaches are compared and discussed.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5550371,no
Model driven testing of embedded automotive systems with timed usage models,"Extended Automation Method 2.0 (EXAM) is employed at AUDI AG to perform the testing of automotive systems. The main drawback of EXAM is, that each test case must be devised and created individually. This procedure is apparently awkward and error-prone. Moreover, the development of increasingly complex functionality poses new challenges to the testing routine in industry. We employed Timed Usage Models to extend the EXAM test method. The usage model serves as the basis for the whole testing process, including test planning and test case generation. We derived automatically platform independent test cases for the execution in EXAM. Test-bench specific code was automatically generated for the test cases in EXAM, where they were executed on hardware-in-the-loop simulators (HILs). Usage models were created for functionalities from power train, comfort, and energy management. The application of usage models allowed the assessment of the test effort and the systematic generation of test cases.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5550938,no
A Hybrid Approach for Detection and Correction of Transient Faults in SoCs,"Critical applications based on Systems-on-Chip (SoCs) require suitable techniques that are able to ensure a sufficient level of reliability. Several techniques have been proposed to improve fault detection and correction capabilities of faults affecting SoCs. This paper proposes a hybrid approach able to detect and correct the effects of transient faults in SoC data memories and caches. The proposed solution combines some software modifications, which are easy to automate, with the introduction of a hardware module, which is independent of the specific application. The method is particularly suitable to fit in a typical SoC design flow and is shown to achieve a better trade-off between the achieved results and the required costs than corresponding purely hardware or software techniques. In fact, the proposed approach offers the same fault-detection and -correction capabilities as a purely software-based approach, while it introduces nearly the same low memory and performance overhead of a purely hardware-based one.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5551155,no
Automatic installation of software-based fault tolerance algorithms in programs generated by GCC compiler,"The problem of designing radiation-tolerant devices working in application critical systems becomes very important especially if human life depends on the reliability of control mechanisms. One of the possible solution of this problem are pure software protection methods. They constitute different category of techniques to detect transient faults and correct corresponding errors. Software fault tolerance schemes are cheaper to implement since they can be used with standard, commercial of-the-shelf (COTS) components. Additionally, they do not require any hardware modification. In this paper, author propose a new implementation mechanism for software based fault protection algorithms performed automatically during application compilation.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5551287,no
Usage of the safety-oriented real-time OASIS approach to build deterministic protection relays,"As any safety-related system, medium voltage protection relays have to comply with a Safety Integrated Level (SIL), as defined by the IEC 61508 standard. The safety-function of the software part of protection relays is first to detect any faults within the supervised power network, then ask the tripping of the circuit breakers in order to isolate the faulty portion of the network. However, it is required that detection and isolation of faults must occur within a given time, as specified by the IEC 60255 standard. Schneider Electric currently achieves the demonstration that a protection relay is performing its safety-function within such temporal constraints at the price of a costly phase of tests. The OASIS approach is a complete tool-chain to build safety-critical deterministic real-time systems, which enables the demonstration of the system timeliness. In this paper, we show how we apply the OASIS approach to build a deterministic protection relay system. We designed a software platform called OASISepam, based on an existing product from Schneider Electric, namely the Sepam 10. We show a preliminary evaluation of our implementation over a STR710 ARM-based board that runs the OASIS kernel. Notably, we show that the observed worst-case end-to-end detection time of OASISepam fulfils the specified constraint expressed in the design phase and translated in the OASIS programming model. Consequently, the temporal behaviour of protection relays is mastered, thus reducing application development costs and allowing the optimization of selectivity.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5551378,no
A die-based defect-limited yield methodology for line control,"Defect monitoring and control in the semiconductor fab has been well documented over the years. The methodologies typically described in the literature involve controls through full-wafer defect counts, or defect densities, with attempts to correlate defects to electrical fail modes in order to predict the yield impact. These wafer-based methodologies are not adequate for determining the impact of defects on yield. Most notably, severe complications arise when applying wafer-based methods on wafers with mixed distributions (mix of random and clustered defects). This paper describes the proper statistical treatment of defect data to estimate yield impact for mixed-distribution wafer maps. This die-based, defect-limited yield (DLY) methodology properly addresses random and clustered defects, and applies a die-based multi-stage sampling method to select defects for review. The estimated yield impact of defects on the die can then be determined. Additionally, a die normalization technique is described that permits application of this die-based methodology on multiple products with different die sizes.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5551412,no
A QoS constrained dynamic priority scheduling algorithm for process engine,"In this paper, a task scheduling algorithm in process engine is proposed to not only maximize overall customer satisfaction but also guarantee QoS requirement. This new algorithm dynamically assigns priority to a task based on the weighted utility value considering the predicted business value and left time to execute the process instance the task belongs to. The business value of each kind of process instance is modeled as a utility function of response time which reflects the inverse proportional relationship between the customer satisfaction and response time. Experiments show that the proposed algorithm promotes the total value of utility function with better QoS achievement compared to traditional algorithms.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5551623,no
Model-based dependency analysis in service delivery process management,"Build up well-defined and optimized service process is the key to deliver good service quality and service satisfaction. An effective method of dependency analysis in the service delivery process is the core to construct an optimized process. However, with increasingly complicated services, there is the large amount of the tasks elements with extreme complex dependency in the service process. Under this situation, set up correct relationship among tasks becomes time consuming and error prone. In this paper, we propose a model-based dependency analysis to automatically build up the dependency relationship among tasks in the service delivery process. It addresses the problem in analyzing dependency firstly and our approach is given. Based on the dependency analysis, we also propose several advanced analysis features on the process to guide user to optimize the process via reducing the process cost. Also a tool adopting our approach has been implemented and introduced. Based on the tool, a case study about the test service delivery process is illustrated to show the results.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5551624,no
A decision support system utilizing a semantic agent,"The adaptabilty, rapidity, and focus on high quality solutions offered by agile methodology have lead to a paradigm shift in the software development process in many enterprises. Agile methodology is iterative in nature, with each iteration i.e. timebox lasting 2-6 weeks. Iterations involve small teams comprising 9-19 developers working through the entire software development life cycle. Agile methodology works on two basic principles. The first being regular adaptation to changing circumstances and the second -focus on technical excellence and good design and high quality code. The first principle accommodates that tasks in an agile project cannot be predicted more than a week in advance. Thus the need arises for project teams to incorporate experts in the problem domain, such that they are better equipped to handle changes rapidly. However this methodology has been criticised as it may not bring about the benefits intended by the second principle unless practised by skilled programmers, who can create high quality code. Hence a project manager should be equipped with a highly skilled team. We propose the utilization of a semantic agent, which will act on behalf of the project manager and suggest experts based on a set of parameters. Our semantic agent is based on a semantic matching algorithm. This algorithm utilizes an ontology based similarity framework to make recommendations and suggest training paths to satisfy the requirements of the project manager. The agent uses this algorithm to recommend employees based on their expertise, past experience and availability. Further, based on recommendations made by the agent we classify employees as experts and non experts and suggest knowledge transfer methods to upgrade their skills.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5552340,no
A qualitative and quantitative assessment method for software process model,"Aimed at the problems of high-cost, non-completeness and ambiguity existed in the traditional assessment methods for Software Process Model (SPM), this paper proposes a qualitative and quantitative assessment method. On the basis of assessment theory and domain experience of SPM, the unclear goals in the project start-up phase are qualitatively described in the form of problem set and expert problem domain is constructed as the assessment criteria on the implementation capability of SPM in the proposed method. With the integration of qualitative analysis and quantitative analysis by using a multi-index synthetic assessment algorithm of AHP (Analytic Hierarchy Process), the weight vector of goals and the reciprocal comparative matrix of problem domain are calculated and then the assessment result in dimensionless index is obtained. In order to reduce the cost of assessment, questionnaires instead of project tracking and audit are adopted to extract project goals in the method. Meanwhile, SPM is comprehensively assessed from four aspects including personnel, method, product and process, and the assessment result can intuitively reflect the capability of different SPM in a numerical form. Finally, an example of how to assess and choose four classical SPMs in the initial stage of a practical project is given out to show the validity of this assessment method.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5552434,no
A bidirectional graph transformation approach to analysis of concurrent software models,"The application of model driven software development still faces strong challenges. One challenge we focus on here is analysis of concurrent software systems for detecting potential defects such as race conditions or atomicity violations. We adopt a BiG (Bidirectional Graph Transformation) approach to analysis of concurrent software models. The essential idea of our approach is that we choose labeled transition systems as the behavior model of the concurrent system, and then conduct model transformation and extract a labeled partial order view from the labeled transition systems for software analysis. The potential of BiG in this work is that model transformation is effectively supported based on queries, and models before and after transformation can also be synchronized automatically. This research is expected to benefit model-driven software development in that analysis of state models is lightweight and can be automated. It also provides the engineers with an interesting example about the application of bidirectional transformation to software analysis, which will encourage the improvement of BiG and its practical applications.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5552447,no
A Pattern-Driven Generation of Security Policies for Service-Oriented Architectures,"Service-oriented Architectures support the provision, discovery, and usage of services in different application contexts. The Web Service specifications provide a technical foundation to implement this paradigm. Moreover, mechanisms are provided to face the new security challenges raised by SOA. To enable the seamless usage of services, security requirements can be expressed as security policies (e.g. WS-Policy and WS-SecurityPolicy) that enable the negotiation of these requirements between clients and services. However, the codification of security policies is a difficult and error-prone task due to the complexity of the Web Service specifications. In this paper, we introduce our model-driven approach that facilitates the transformation of architecture models annotated with simple security intentions to security policies. This transformation is driven by security configuration patterns that provide expert knowledge on Web Service security. Therefore, we will introduce a formalised pattern structure and a domain-specific language to specify these patterns.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5552780,no
Detecting Data Inconsistency Failure of Composite Web Services Through Parametric Stateful Aspect,"Runtime monitoring of Web service compositions with WS-BPEL has been widely acknowledged as a significant approach to understand and guarantee the quality of services. However, most existing monitoring technologies only track patterns related to the execution of an individual process. As a result, the possible inconsistency failure caused by implicit interactions among concurrent process instances cannot be detected. To address this issue, this paper proposes an approach to specify the behavior properties related to shared resources for web service compositions and verify their consistency with the aid of a parametric stateful aspect extension to WS-BPEL. Parameters are introduced in pattern specification, which allows monitoring not only events but also their values bound to the parameters at runtime to keep track of data flow among concurrent process instances. An efficient implementation is also provided to reduce the runtime overhead of monitoring and event observation. Our experiments show that the proposed approach is promising.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5552802,no
Software project schedule variance prediction using Bayesian Network,"The major objective of software engineer is to guarantee to deliver high-quality software on time and within budget. But as the development of software technology and the rapid extension of application areas, the size and complexity of software is increasing so quickly that the cost and schedule is often out of control. However, few groups or researchers have proposed an effective method to help project manager make reasonable project plan, resource allocation and improvement actions. This paper proposes Bayesian Network to solve the problem of predicting and controlling of software schedule in order to achieve proactive management. Firstly, we choose factors influencing software schedule and determine some significant cause-effect relationship between factors. Then we analyze data using statistical analysis and deal with data using data discretization. Thirdly, we construct the Bayesian structure of software schedule and learn the condition probability table of the structure. The structure and condition probability table constitute the model for software schedule variance. The model can be used not only to help project manager predict probability of software schedule variance but also guide software developers to make reasonable improvement actions. At last, an application shows how to use the model and the result proves the validity of the model. In addition, a sensitivity analysis is developed with the model to locate the most important factor of software schedule variance.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5552847,no
The design of alarm and control system for electric fire prevention based on MSP430F149,"This paper introduces an electrical fire control system which is composed of stand-alone electrical fire detector and electrical fire monitoring equipment. The detector with fieldbus communication technologies uses MCU to realize the main route leakage protection for low-voltage 3-phase 4-wire system; at the same time, finish the measure, display and control of voltage, current, power, electricity, temperature and other parameters; through the monitoring equipments have been installed the management software written by VC++, the operation of the scene can be monitored and the alarm information can be detected in time. With RS-485 or Ethernet communication, the system can be associated with kinds of standard power monitoring software, circular monitoring and control more than 250 detects, and record 100 types of faults and data with more than 12 months storage time.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5553985,no
Neural network fault prediction and its application,"In this paper, the forecasting algorithm employs wavelet function to replace sigmoid function in the hidden layer of Back-Propagation Neural Network. And a Wavelet Neural Network prediction model is established to predict Anode Effect (the most typical fault) through forecasting the change rate of cell resistance. The authors have developed forecasting software based on platform of Visual Basic 6.0. The simulation results show that the proposed method not only has greatly improved fault prediction precision and real-time, but also improved the operation efficiency. That means we can increase energy efficiency and the safety of aluminum production process.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5554056,no
Exploitation of Multiple Hyperspace Dimensions to Realize Coexistence Optimized Wireless Automation Systems,"The need for multiple radio systems in overlapping regions of a factory floor introduces a coexistence problem. The current research challenge is to design and realize radio systems that should be able to achieve a desired quality-of-service (QoS) in harsh, time-varying, coexisting industrial environments. Conventional coexistence solutions attempt to accommodate coexisting systems in a single dimension, mostly in the frequency dimension. The concept of multidimensional electromagnetic (EM) space utilization provides optimal opportunities to achieve coexistence optimized solutions. It can revolutionarily augment the shareable capacity of the resource space and provide optimal coexisting capabilities of radio systems. A software defined radio (SDR)-based cognitive radio (CR) is realized which can exploit the frequency, time, and power dimensions of the EM space to improve coexistence in the 2.4 GHz industrial, scientific, and medical (ISM) band. Furthermore, a conventional hardware defined radio (HDR) and additional simulations are used to test and prove the feasibility of the triple EM space utilization. Joint results of these experiments are presented in this contribution. Additionally, we present a novel computational efficient algorithm to detect cyclic properties of industrial wireless systems.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5556023,no
"Smart phone based medicine in-take scheduler, reminder and monitor","Out-patient medication administration was identified as the most error-prone procedure in modern healthcare. Most medication administration errors were made when patients acquired prescribed and over-the-counter medicines from several drug stores and use them at home without proper guidance. In this paper, we introduce Wedjat, a smart phone application that helps patients to avoid these mistakes. Wedjat can remind its users to take the correct medicines on time and keep an in-take record for later review by healthcare professionals. Wedjat has two distinguished features: (1) it can alert the patients about potential drug-drug/drug-food interactions and plan an in-take schedule that avoids these adverse interactions; (2) it can revise an in-take schedule automatically when a dose was missed. In both cases, the software always produces the simplest schedule with least number of in-takes. Wedjat works with the calendar application available on most smart phones to issue medicine and meal reminders. It also shows pictures of the medicine and provides succinct in-take instructions. As a telemonitoring device, Wedjat can maintain medicine in-take records on board, synchronize them with a database on a host machine or upload them onto an electronic medical records (EMR) system. A prototype of Wedjat has been implemented on Window Mobile platform. This paper introduces the design concepts of Wedjat with emphasis on its medication scheduling and grouping algorithms.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5556577,no
Assessing the quality of scientific publications as educational content on digital libraries,"In recent years, new projects and research lines in several scientific areas have emerged in order to develop and improve the process of teaching/learning within the network of universities in Cuba. From a general viewpoint, applying information and communication technologies on the management of the information generated by higher education processes stimulates the use of digital libraries. In this paper, we aim at applying the LOQEVAL (Learning Object Quality EVALuation) proposal, which is based on the intensive use of ontologies, for assessing the quality of scientific publications as educational contents. The paper explains the whole process of assessment using the use case of the digital library of the Agrarian University of Havana, which is currently under development.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5556667,no
New Supervisory Control and Data Acquisition (SCADA) based fault isolation system for low voltage distribution systems,"This paper proposes a new supervisory control and data acquisition (SCADA) based fault isolation system on the low voltage (415/240 V) distribution system. It presents a customized distribution automation system (DAS) for automatic operation and secure fault isolation tested in the Malaysian utility distribution system; Tenaga Nasional Berhad (TNB) distribution system. It presents the first research work on customer side automation for operating and controlling between the consumer and the substation in an automated manner. The paper focuses on the development of very secure automated fault isolation work tested to TNB distribution operating principles as the fault is detected, identified, isolated and cleared in few seconds by just clicking the mouse of laptop or desktop connected to the system. Supervisory Control and Data Acquisition (SCADA) technique has been developed and utilized to build Human Machine Interface (HMI) that provides a Graphical User Interface (GUI) functions for the engineers and technicians to monitor and control the system. Microprocessor based Remote Monitoring Devices have been used for customized software integrated to the hardware. Power Line Carrier (PLC) has been used as communication media between the consumer and the substation. As a result, complete DAS and fault isolation system has been developed for remote automated operation, cost reduction, maintenance time saving and less human intervention during faults conditions.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5556777,no
Computational Intelligent Gait-Phase Detection System to Identify Pathological Gait,"An intelligent gait-phase detection algorithm based on kinematic and kinetic parameters is presented in this paper. The gait parameters do not vary distinctly for each gait phase; therefore, it is complex to differentiate gait phases with respect to a threshold value. To overcome this intricacy, the concept of fuzzy logic was applied to detect gait phases with respect to fuzzy membership values. A real-time data-acquisition system was developed consisting of four force-sensitive resistors and two inertial sensors to obtain foot-pressure patterns and knee flexion/extension angle, respectively. The detected gait phases could be further analyzed to identify abnormality occurrences, and hence, is applicable to determine accurate timing for feedback. The large amount of data required for quality gait analysis necessitates the utilization of information technology to store, manage, and extract required information. Therefore, a software application was developed for real-time acquisition of sensor data, data processing, database management, and a user-friendly graphical-user interface as a tool to simplify the task of clinicians. The experiments carried out to validate the proposed system are presented along with the results analysis for normal and pathological walking patterns.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5557818,no
Towards Self-Assisted Troubleshooting for the Deployment of Private Clouds,"Acquiring a private computing cloud is the first step that an enterprise would choose to enable the cloud model and get its considerable benefits while keeping the control within the enterprise. The enterprise level applications that provide the infrastructure enabling cloud computing services are typically built by integrating inter-related complex software components. Critical challenges of these applications are the increasing level of inter-component dependencies and the customized growth, which make recurrent deployment of such applications, as the one required in private clouds, labor intensive and error prone. In this paper we investigate the type of issues faced when deploying a cloud computing management infrastructure and propose a solution to self-assist the deployment. We show how by leveraging virtual image technologies we can detect faulty installations and their signatures early in the deployment process. We also propose a methodology to capture in a shared repository and update these signatures for reuse in subsequent deployments in the form of two level signature patterns. We explore the perspective of our solution and criteria of analysis.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5557997,no
Proving transaction and system-level properties of untimed SystemC TLM designs,"Electronic System Level (ESL) design manages the enormous complexity of todays systems by using abstract models. In this context Transaction Level Modeling (TLM) is state-of-the-art for describing complex communication without all the details. As ESL language, SystemC has become the de facto standard. Since the SystemC TLM models are used for early software development and as reference for hardware implementation their correct functional behavior is crucial. Admittedly, the best possible verification quality can be achieved with formal approaches. However, formal verification of TLM models is a hard task. Existing methods basically consider local properties or have extremely high run-time. In contrast, the approach proposed in this paper can verify true TLM properties, i.e. major TLM behavior like for instance the effect of a transaction and that the transaction is only started after a certain event can be proven. Our approach works as follows: After a fully automatic SystemC-to-C transformation, the TLM property is mapped to monitoring logic using C assertions and finite state machines. To detect a violation of the property the approach uses a BMC-based formulation over the outermost loop of the SystemC scheduler. In addition, we improve this verification method significantly by employing induction on the C model forming a complete and efficient approach. As shown by experiments state-of-the-art proof techniques allow proving important non-trivial behavior of SystemC TLM designs.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5558643,no
A Neural network based approach for modeling of severity of defects in function based software systems,"There is lot of work done in prediction of the fault proneness of the software systems. But, it is the severity of the faults that is more important than number of faults existing in the developed system as the major faults matters most for a developer and those major faults needs immediate attention. As, Neural networks, which have been already applied in software engineering applications to build reliability growth models predict the gross change or reusability metrics. Neural networks are non-linear sophisticated modeling techniques that are able to model complex functions. Neural network techniques are used when exact nature of input and outputs is not known. A key feature is that they learn the relationship between input and output through training. In this paper, five Neural Network Based techniques are explored and comparative analysis is performed for the modeling of severity of faults present in function based software systems. The NASA's public domain defect dataset is used for the modeling. The comparison of different algorithms is made on the basis of Mean Absolute Error, Root Mean Square Error and Accuracy Values. It is concluded that out of the five neural network based techniques Resilient Backpropagation algorithm based Neural Network is the best for modeling of the software components into different level of severity of the faults. Hence, the proposed algorithm can be used to identify modules that have major faults and require immediate attention.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5559743,no
A Density Based Clustering approach for early detection of fault prone modules,"Quality of a software component can be measured in terms of fault proneness of data. Quality estimations are made using fault proneness data available from previously developed similar type of projects and the training data consisting of software measurements. To predict fault-proneness of modules different techniques have been proposed which includes statistical methods, machine learning techniques, neural network techniques and clustering techniques. The aim of proposed approach is to investigate that whether metrics available in the early lifecycle (i.e. requirement metrics), metrics available in the late lifecycle (i.e. code metrics) and metrics available in the early lifecycle (i.e. requirement metrics) combined with metrics available in the late lifecycle (i.e. code metrics) can be used to identify fault prone modules by using Density Based Clustering technique. This approach has been tested with real time defect datasets of NASA software projects named as PC1. Predicting faults early in the software life cycle can be used to achieve high software quality. The results show that the fusion of requirement and code metric is the best prediction model for detecting the faults as compared with mostly used code based model.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5559753,no
A new low cost fault tolerant solution for mesh based NoCs,"In this paper a new fault tolerant routing algorithm with minimum hardware requirements and extremely high fault tolerance for 2D-mesh based NoCs is proposed. The LCFT (Low Cost Fault Tolerant) algorithm, removes the main limitations (forbidden turns) of the famous XY. So not only many new routes will be added to the list of selectable paths as well as deadlock freedom, but also it creates high level of fault tolerance. All these things are yielded only by the cost of adding one more virtual channel (for a total of two). Results show that LCFT algorithm can work well under almost bad conditions of faults in comparison with the already published methods.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5559766,no
A history data based traffic incident impact analyzing and predicting method,"Traffic incidents are a main factor that reduces capacity and service quality of roads. Due to the absence of efficient incident impact analyzing and predicting method, the traffic congestion and secondary accident brought by traffic incidents can hardly be avoided. In this paper we propose a history data based traffic incident impact analyzing and predicting method. By extracting regularity and volatility information from history data, we find a solution to analyze both parts that comprise the traffic flow status: the road condition without the incident, and the impact of the incident. Thereby we can predict the traffic condition under incidents by estimating the two components separately and adding them together. Experimental results show that our solution could simulate and predict the impact tendency of traffic incidents with high accuracy.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5559768,no
SBST for on-line detection of hard faults in multiprocessor applications under energy constraints,"Software-Based Self-Test (SBST) has emerged as an effective method for on-line testing of processors integrated in non safety-critical systems. However, especially for multi-core processors, the notion of dependability encompasses not only high quality on-line tests with minimum performance overhead but also methods for preventing the generation of excessive power and heat that exacerbate silicon aging mechanisms and can cause long term reliability problems. In this paper, we initially extend the capabilities of a multiprocessor simulator in order to evaluate the overhead in the execution of the useful application load in terms of both performance and energy consumption. We utilize the derived power evaluation framework to assess the overhead of SBST implemented as a test thread in a multiprocessor environment. A range of typical processor configurations is considered. The application load consists of some representative SPEC benchmarks, and various scenarios for the execution of the test thread are studied (sporadic or continuous execution). Finally, we apply in a multiprocessor context an energy optimization methodology that was originally proposed to increase battery life for battery-powered devices. The methodology reduces significantly the energy and performance overhead without affecting the test coverage of the SBST routines.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5560233,no
Identifying effective software engineering (SE) team personality types composition using rough set approach,"This paper presents an application of rough sets in identifying effective personality-type composition in software engineering (SE) teams. Identifying effective personality composition in teams is important for determining software project success. It was shown that a balance of the personality types Sensing (S), Intuitive (N), Thinking (T) and Feeling (F) assisted teams in achieving higher software quality. In addition, Extroverted (E) members also had an impact on team performance. Even though the size of empirical data was too small, the rough-set technique allows the generation of significant personality-type composition rules to assist decision makers in forming effective teams. Future works will include more empirical data in order to develop predicting model of teams' performance based on personality types.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5561479,no
A proposed reusability attribute model for aspect oriented software product line components,"Reusability assessment is vital for software product line due to reusable nature of its core components. Reusability being a high level quality attribute is more relevant to the software product lines as the entire set of individual products depend on the software product line core assets. Recent research proposes the use of aspect oriented techniques for product line development to provide better separation of concerns, treatment of crosscutting concerns and variability management. There are quality models available which relate the software reusability to its attributes. These models are intended to assess the reusability of software or a software component. The assessment of aspect oriented software and a core asset differs from the traditional software or component. There is need to develop a reusability model to relate the reusability attributes of aspect oriented software product line assets. This research work is an effort towards the development of reusability attribute model for software product line development using aspect oriented techniques.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5561503,no
Establishing a defect prediction model using a combination of product metrics as predictors via Six Sigma methodology,"Defect prediction is an important aspect of the Product Development Life Cycle. The rationale in knowing predicted number of functional defects earlier on in the lifecycle, rather than to just find as many defects as possible during testing phase is to determine when to stop testing and ensure all the in-phase defects have been found in-phase before a product is delivered to the intended end user. It also ensures that wider test coverage is put in place to discover the predicted defects. This research is aimed to achieve zero known post release defects of the software delivered to the end user by MIMOS Berhad. To achieve the target, the research effort focuses on establishing a test defect prediction model using Design for Six Sigma methodology in a controlled environment where all the factors contributing to the defects of the product is within MIMOS Berhad. It identifies the requirements for the prediction model and how the model can benefit them. It also outlines the possible predictors associated with defect discovery in the testing phase. Analysis of the repeatability and capability of test engineers in finding defects are demonstrated. This research also describes the process of identifying characteristics of data that need to be collected and how to obtain them. Relationship of customer needs with the technical requirements of the proposed model is then clearly analyzed and explained. Finally, the proposed test defect prediction model is demonstrated via multiple regression analysis. This is achieved by incorporating testing metrics and development-related metrics as the predictors. The achievement of the whole research effort is described at the end of this study together with challenges faced and recommendation for future research work.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5561516,no
Formalization of UML class diagram using description logics,"Unified Modelling Language (UML) is as a standard object-oriented modelling notation that is widely accepted and used in software development industry. In general, the UML notation is informally defined in term of natural language description (English) and Object Constraint Language (OCL) which makes difficult to formally analyzed and error-prone. In this paper, we elucidate the preliminary result on an approach to formally define UML class diagram using logic-based representation formalism. We represent how to define the UML class diagram using Description Logics (DLs).",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5561621,no
Software-Implemented Hardware Error Detection: Costs and Gains,"Commercial off-the-shelf (COTS) hardware is becoming less and less reliable because of the continuously decreasing feature sizes of integrated circuits. But due to economic constraints, more and more critical systems will be based on basically unreliable COTS hardware. Usually in such systems redundant execution is used to detect erroneous executions. However, arithmetic codes promise much higher error detection rates. Yet, they are generally assumed to generate very large slowdowns. In this paper, we assess and compare the runtime overhead and error detection capabilities of redundancy and several arithmetic codes. Our results demonstrate a clear trade-off between runtime costs and gained safety. However, unexpectedly the runtime costs for arithmetic codes compared to redundancy increase only linearly, while the gained safety increases exponentially.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562849,no
FTDIS: A Fault Tolerant Dynamic Instruction Scheduling,"In this work, we target the robustness for controller scheduler of type Tomasulo for SEU faults model. The proposed fault-tolerant dynamic scheduling unit is named FTDIS, in which critical control data of scheduler is protected from driving to an unwanted stage using Triple Modular Redundancy and majority voting approaches. Moreover, the feedbacks in voters produce recovery capability for detected faults in the FTDIS, enabling both fault mask and recovery for system. As the results of analytical evaluations demonstrate, the implemented FTDIS unit has over 99% fault detection coverage in the condition of existing less than 4 faults in critical bits. Furthermore, based on experiments, the FTDIS has a 200% hardware overhead comparing to the primitive dynamic scheduling control unit and about 50% overhead in comparision to a full CPU core. The proposed unit also has no performance penalty during simulation. In addition, the experiments show that FTDIS consumes 98% more power than the primitive unit.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562850,no
From Formal Specification in Event-B to Probabilistic Reliability Assessment,"Formal methods, in particular the B Method and its extension Event-B, have proven their worth in the development of many complex software-intensive systems. However, while providing us with a powerful development platform, these frameworks poorly support quantitative assessment of dependability attributes. Yet, such an assessment would facilitate not only system certification but also system development by guiding it towards the design optimal from the dependability point of view. In this paper we demonstrate how to integrate reliability assessment performed by model checking into refinement process in Event-B. Such an integration allows us to combine logical reasoning about functional correctness with probabilistic reasoning about reliability. Hence we obtain a method that enables building the systems that are not only correct-by-construction but also have a predicted level of reliability.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562853,no
Assessing Dependability for Mobile and Ubiquitous Systems: Is there a Role for Software Architectures?,A traditional research direction in SA and dependability is to deduce system dependability properties from the Knowledge of the system Software Architecture. This will reflect the fact that traditional systems are built by using the closed world assumption. In mobile and ubiquitous systems this line of reasoning becomes too restrictive to apply due to the inherent dynamicity and heterogeneity of the systems under consideration. Indeed these systems need to relax the closed world assumption and to consider an open world where the system/component/user context is not fixed. In other words the assumption that the system SA is known and fixed at an early stage of the system development does not apply anymore. On the contrary the ubiquitous scenario promotes the view that systems can be dynamically composed out of available components whose dependability can at most be assessed in terms of components assumptions on the system context. Moreover dependability cannot be anymore designed as an absolute context free property of the system rather it may change as long as it allows the satisfaction of the user's requirements and needs. In this setting SA can only be dynamically induced by taking into consideration the respective assumptions of the system components and the current user needs. The talk will illustrate this challenge and will discuss a set of possible future research directions.,2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562937,no
A Grouping-Based Strategy to Improve the Effectiveness of Fault Localization Techniques,"Fault localization is one of the most expensive activities of program debugging, which is why the recent years have witnessed the development of many different fault localization techniques. This paper proposes a grouping-based strategy that can be applied to various techniques in order to boost their fault localization effectiveness. The applicability of the strategy is assessed over - Tarantula and a radial basis function neural network-based technique; across three different sets of programs (the Siemens suite, grep and gzip). Results are suggestive that the grouping-based strategy is capable of significantly improving the fault localization effectiveness and is not limited to any particular fault localization technique. The proposed strategy does not require any additional information than what was already collected as input to the fault localization technique, and does not require the technique to be modified in any way.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562940,no
Mining Performance Regression Testing Repositories for Automated Performance Analysis,"Performance regression testing detects performance regressions in a system under load. Such regressions refer to situations where software performance degrades compared to previous releases, although the new version behaves correctly. In current practice, performance analysts must manually analyze performance regression testing data to uncover performance regressions. This process is both time-consuming and error-prone due to the large volume of metrics collected, the absence of formal performance objectives and the subjectivity of individual performance analysts. In this paper, we present an automated approach to detect potential performance regressions in a performance regression test. Our approach compares new test results against correlations pre-computed performance metrics extracted from performance regression testing repositories. Case studies show that our approach scales well to large industrial systems, and detects performance problems that are often overlooked by performance analysts.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562942,no
Scenarios-Based Testing of Systems with Distributed Ports,"Current distributed systems are usually composed of several distributed components that communicate through specific ports. When testing these systems we separately observe sequences of inputs and outputs at each port rather than a global sequence and potentially cannot reconstruct the global sequence that occurred. In this paper we concentrate on the problem of formally testing systems with distributed components that, in general, have independent behaviors but that at certain points of time synchronization can occur. These situations appear very often in large real systems that regularly go through maintenance and/or update operations. If we represent the specification of the global system by using a state-based notation, we say that a scenario is any sequence of events that happens between two of these operations; we encode these special operations by marking some of the states of the specification. In order to assess the appropriateness of our new framework, we show that it represents a conservative extension of previous implementation relations defined in the context of the distributed test architecture: If we consider that all the states are marked then we simply obtain ioco (the classical relation for single-port systems) while if no state is marked then we obtain dioco (our previous relation for multi-port systems).",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562944,no
Adaptive Random Testing by Exclusion through Test Profile,"One major objective of software testing is to reveal software failures such that program bugs can be removed. Random testing is a basic and simple software testing technique, but its failure-detection effectiveness is often controversial. Based on the common observation that program inputs causing software failures tend to cluster into contiguous regions, some researchers have proposed that an even spread of test cases should enhance the failure-detection effectiveness of random testing. Adaptive random testing refers to a family of algorithms to evenly spread random test cases based on various notions. Restricted random testing, an algorithm to implement adaptive random testing by the notion of exclusion, defines an exclusion region around each previously executed test case, and selects test cases only from outside all exclusion regions. Although having a high failure-detection effectiveness, restricted random testing has a very high computation overhead, and it rigidly discards all test cases inside any exclusion region, some of which may reveal software failures. In this paper, we propose a new method to implement adaptive random testing by exclusion, where test cases are simply selected based on a well-designed test profile. The new method has a low computation overhead and it does not omit any possible program inputs that can detect failures. Our experimental results show that the new method not only spreads test cases more evenly but also brings a higher failure-detection effectiveness than random testing.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562948,no
Fault Localization Based on Dynamic Slicing and Hitting-Set Computation,"Slicing is an effective method for focusing on relevant parts of a program in case of a detected misbehavior. Its application to fault localization alone and in combination with other methods has been reported. In this paper we combine dynamic slicing with model-based diagnosis, a method for fault localization, which originates from Artificial Intelligence. In particular, we show how diagnosis, i.e., root causes, can be extracted from the slices for erroneous variables detected when executing a program on a test suite. We use these diagnoses for computing fault probabilities of statements that give additional information to the user. Moreover, we present an empirical study based on our implementation JSDiagnosis and a set of Java programs of various size from 40 to more than 1,000 lines of code.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562955,no
Software Operational Profile Modeling and Reliability Prediction with an Open Environment,"With the continuous development of the internet, operational environments of software have undergone tremendous change. One of the significant changes in the software operational environment is more open. The operational profiles under open environment are much more complex and unpredictable. It is an important property for the operational profiles under open environment to change continually over time. In the traditional software operational profile modeling, there is an assumption that software operational profile should be defined in a certain probability space. However, for the open operational software systems, it is difficult to define such a certain probability space, because there is often not a limited boundary for the operations of such system. So, the question arises: how to model the software operational profile when software operational environment is open? In the paper, partial probability space is proposed to describe the varying probability space, and based on this concept an operational profile model under open operational environment is developed.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562963,no
A Methodology for Continuos Quality Assessment of Software Artefacts,"Although some methodologies for evaluating the quality of software artifacts exist, all of these are isolated proposals, which focus on specific artifacts and apply specific evaluation techniques. There is no generic and flexible methodology that allows quality evaluation of any kind of software artifact, regardless of type, much less a tool that supports this. To tackle that problem in this paper, we propose the CQA Environment, consisting of a methodology (CQA-Meth) and a tool that implements it (CQA-Tool). We began applying this environment in the evaluation of the quality of UML models (use cases, class and statechart diagrams). To do so, we have connected CQA-Tool to the different tools needed to assess the quality of models, which we also built ourselves. CQA-Tool, apart from implementing the methodology, provides the capacity for building a catalogue of evaluation techniques that integrates the evaluation techniques (e.g. metrics, checklists, modeling conventions, guidelines, etc.) which are available for each software artifact. CQA Environment is suitable for use by companies that offer software quality evaluation services, especially for clients who are software development organizations and who are outsourcing software construction. They will obtain an independent quality evaluation of the software products they acquire. Software development organizations that perform their own evaluation will be able to use it as well.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562967,no
Increasing System Availability with Local Recovery Based on Fault Localization,"Due to the fact that software systems cannot be tested exhaustively, software systems must cope with residual defects at run-time. Local recovery is an approach for recovering from errors, in which only the defective parts of the system are recovered while the other parts are kept operational. To be efficient, local recovery must be aware of which component is at fault. In this paper, we combine a fault localization technique (spectrum-based fault localization, SFL) with local recovery techniques to achieve fully autonomous fault detection, isolation, and recovery. A framework is used for decomposing the system into separate units that can be recovered in isolation, while SFL is used for monitoring the activities of these units and diagnose the faulty one whenever an error is detected. We have applied our approach to MPlayer, a large open-source software. We have observed that SFL can increase the system availability by 23.4% on average.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562970,no
Active Monitoring for Control Systems under Anticipatory Semantics,"As the increment of software complexity, traditional software analysis, verification and testing techniques can not fully guarantee the faultlessness of deployed systems. Therefore, runtime verification has been developed to continuously monitor the running system. Typically, runtime verification can detect property violations but cannot predict them, and consequently cannot prevent the failures from occurring. To remedy this weakness, active monitoring is proposed in this paper. Its purpose is not repairing the faults after failures have occurred, but predicting the possible faults in advance and triggering the necessary steering actions to prevent the software from violating the property. Anticipatory semantics of linear temporal logic is adopted in monitor construction here, and the information of system model is used for successful steering and prevention. The prediction and prevention will form a closed-loop feedback based on control theory. The approach can be regarded as an effective complement of traditional testing and verification techniques.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562977,no
An Integrated Support for Attributed Goal-Oriented Requirements Analysis Method and its Implementation,"This paper presents an integrated supporting tool for Attributed Goal-Oriented Requirements Analysis (AGORA), which is an extended version of goal-oriented analysis. Our tool assists seamlessly requirements analysts and stakeholders in their activities throughout AGORA steps including constructing goal graphs with group work, utilizing domain ontologies for goal graph construction, detecting various types of conflicts among goals, prioritizing goals, analyzing impacts when modifying a goal graph, and version control of goal graphs.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562985,no
A Novel Approach to Automatic Test Case Generation for Web Applications,"As the quantity and breadth of Web-based software systems continue to grow rapidly, it is becoming critical to assure the quality and reliability of a Web application. Web application testing is a challenging work owing to its dynamic behaviors and complex dependencies. Test case generation, in general, is costly and labor-intensive processes. How to automatically generate effective test case is important for Web applications testing. In this paper, we propose the two-phase approach to generate test cases automatically by analyzing structure of the Web application. We define the dependence relationships, data dependence and control dependence, in the Web application and detect the relationships from source code and improve the way of test case generation with analysis result. The experimental result show that our approach can reduce test case set in test case generation processes.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562993,no
Adaptive Interaction Fault Location Based on Combinatorial Testing,"Combinatorial testing aims to detect interaction faults, which are triggered by interaction among parameters in system, by covering some specific combinations of parametric values. Most works about combinatorial testing focus on detecting such interaction faults rather than locating them. Based on the model of interaction fault schema, in which the interaction fault is described as a minimum fault schema and several corresponding parent-schemas, we propose an iterative adaptive interaction fault location technique for combinatorial testing. In order to locate interaction faults that detected in combinatorial testing, such technique utilizes delta debugging strategy to filtrate suspicious schemas by generating and running additional test cases iteratively. The properties, which include both recall and precision, of adaptive interaction fault location techniques are also analyzed in this paper. Analytical results suggest that the high scores in both recall and precision are guaranteed. It means that such technique can provide an efficient guidance for the applications of combinatorial testing.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5563006,no
Hardware/software co-design to secure crypto-chip from side channel analysis at design time,"Side channel analysis (SCA) is a powerful physical cryptanalysis. In common industrial practice, the SCA security evaluation is performed after the devices are manufactured. However, the post-manufactured analysis is time consuming, error prone and expensive. Motivated by the spirit of hardware/software co-design, a design-time SCA analysis is proposed to improve the efficiency. Firstly, a general SCA leakage model is presented, and then the flow of design-time security analysis is described. After that, a flexible simulation and analytical environment is built to assess the different SCA leakage. Finally, an illustrative experiment, which performs a design-time SCA analysis on the crypto-chip with AES-128 core, is given.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5563592,no
Information flow metrics and complexity measurement,"Complexity metrics takes an important role to predict fault density and failure rate in the software deployment. Information flow represents the flow of data in collective procedures in the processes of a concrete system. The present static analysis of source code and the ability of metrics are incompetent to predict the actual amount of information flow complexity in the modules. In this paper, we propose metrics IF-C, F-(I+O), (C-L) and (P-C) focuses on the improved information flow complexity estimation method, which is used to evaluate the static measure of the source code and facilitates the limitation of the existing work. Using the IF-C method, the framework of information flow metrics can be significantly combined with external flow of the active procedure, which depends on various levels of procedure call in the code.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5563667,no
The research on the algorithm of inner and outer contours' distinguishing and the problems of processing domains' identification,"At the present time, the choice of processing domains in CAM softwares used by enterprises is the contour boundary's hand sorting or the single machined surface's selection. These ways are low efficiency and error-prone, when they are used in the choice of various domains at one time or the processing domains' boundaries are complex. According to the need for planning tool path on the mold of tire-sidewall patterns, inner and outer contours and processing domains must be distinguished at first, so an algorithm of inner and outer contours' distinguishing is proposed which based on the algorithm of ray cross-point. Firstly, the algorithm takes one point from each contour; secondly, the mutual position relationship between contours is obtained by determining the inclusion relation of a point on one contour with other contours; at last, some solution strategies are proposed according to the targeted analysis on the composition of various domains in the practical processing.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5564763,no
Notice of Retraction<BR>Accurate and efficient reliability Markov model analysis of predictive hybrid m-out-of-n systems,"Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR>In recent years we perceive many researches on new techniques for improving the performance of fault-tolerant control systems. Prediction of correct system output is one of these techniques which can calculate and predict the probable correct system output when the ordinary techniques are incapacitated to make decision. In this paper, a performance model of predictive m-out-of-n hybrid redundancy is introduced and analyzed. The results of equations and mathematical relations based on Markov model demonstrated that this approach can improve the system reliability in comparison with traditional m-out-of-n system.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5564803,no
Notice of Retraction<BR>Probability-based safety related requirements change impact analysis,"Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR>Software requires change throughout its lifetime which has been found to be a particular problem in terms of schedule, budget, and quality. The problems are most extreme important for critical software that needs to be validated if the changed requirements will affect safety. Therefore, the change impact analysis is so important in safety-critical system. This paper proposes a method to analyze the change impact analysis in a quantitative way with risk probability. First, a complete safety assessment model and a reliable model to trace the change are proposed based on probabilistic assessment. A case study on well-known systems, such as pressure systems, is analyzed. Then the application of our proposed traceability method is discussed which proves the applicability of our approach in change impact analysis.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5564993,no
Research on automatic detection for defect on bearing cylindrical surface,"At present, it is still by using manual method to detect the defects on micro bearing surface. The method is laborious and time consuming. Moreover, it has a low efficiency and high miss-detection rate. Due to the fact, an on-line automatic detection system is developed using linear CCD. The proposed system is composed of three subsystems: the detection environment setting subsystem, the automatic detection subsystem, and the data management subsystem. In order to lead the above subsystems to cooperate with each other, control software is developed with LabVIEW 8.5 as a platform. Experimental results indicate that the system realizes the predefined functions and caters to the requirements of stability, real-time performance, and accuracy. Thus it can be applied for actual production.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5565229,no
Fractal study on fault system of Carboniferous in Junggar Basin based on GIS,"Fault system is a significant evidence of tectonic movement during crust tectonic evolution and may play an more important role in oil-gas accumulation process than other tectonic types in sedimentary basin. Carboniferous surface faults in Junggar Basin developed well and varied in size and distribution. There are about 200 faults in Carboniferous, and 187 of them are thrust faults. Chaos-fractals theories have been widely investigated and great progress has been made in the past three decades. One of the important conception-fractal dimension had become a powerful tool for describing non-linearity dynamical system characteristic. The clustered objects in nature are often fractal and fault system distribution in space is inhomogeneous, always occurs in groups, so we can describe spatial distribution of faults from the point of fractal dimension. Fractal dimension of fault system is a comprehensive factor associated with fault number, size, combination modes and dynamics mechanism, so it can evaluate the complexity of fault system quantitatively. The relationship between fault system and oil-gas accumulation is a focus and difficulty problem in petroleum geology, and fractal dimension is a new tool for describing fault distribution and predicting potential areas of hydrocarbon resources. Geographic Information System (GIS) is a kind of technological system collecting, storing, managing, computing, analyzing, displaying and describing the geospatial information supported by computer software and hardware. In the last 15-20 years, GIS have been increasingly used to address a wide variety of geoscience problems. Weights-of-evidence models use the theory of conditional probability to quantify spatial association between fractal dimension and oil-gas accumulation. The weights of evidence are combined with the prior probability of occurrence of oil-gas accumulation using Bayes'rule in a loglinear form under an assumption of conditional independence of the dimension maps t- - o derive posterior probability of occurrence of oil-gas accumulation. In this paper, we first vectorize the fault system in Carboniferous of Junggar Basin in GIS software and store it as polyline layer in Geodatabase of GIS to manage and analyze, then calculate the fractal dimension of three types which are box dimension, information dimension and cumulative length dimension using spatial functions of GIS, in the last use weights-of-evidence model to calculate the correlation coefficients in GIS environment between oil-gas accumulation and three types of fractal dimension in order to quantity the importance of fault system.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5567793,no
Simulation of city development using cellular automata and agent based integrated model  A case study of Qingdao city,"This paper first developed an integrated model of urban land use changing using cellular automata (CA) and agent technique. In this model, every cell not only has the current state information of the cell, but also has the land use changing related information such as policy and natural condition information; the agents with different roles are used to process the information contained in the cell and then decide the state of the next time step of the cell. For instance, the natural condition information analysis agent is in charge of processing natural condition information; the policy information analysis agent is in charge of processing policy information; surrounding sensor agent is in charge of sensing the state of the cells in the neighborhood; and the logical decision agent makes the logical decision basing on the analytical results processed by the agents mentioned above, and decides whether the cell change its state in the next time step. This model can be extended according to the increasing understanding of the development mechanism of city which is known as the complex giant system. Thanks to the addition of multi-role agents, the model has a global vision of the focused area and thereby the model has complexity and flexibility in processing which can effectively simulate urban city development. Qingdao city has changed to an important seaport, tourist, information technology and ocean science city in China from a small fishery village in the past over 100 years. The city has experienced three main phases of development, and the driving forces behind each development are different. In this paper, the model is used to simulate the three development scenes in the Qingdao city history. In the simulation, the driving forces in each phase are quantified and implemented, and the agents with different roles are assigned specific algorithms correspondingly. The simulated results are compared with the maps or remote sensing. Although the quality of the fine compa- - rison of the simulated results and the real data is affected by the poor real data quality because the real data of Qingdao City before 1949 is hard to obtain, the results show that the model has a good capability to simulate the city's development with the selected parameters of Qingdao city. The model has a good potential to analyze the driving forces related to city development and predict the city development under different scenes.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5567860,no
A novel watermarking algorithm for protecting audio aggregation based on ICA,"This paper proposes an audio watermarking algorithm based on ICA, breaks through the traditional watermarking which only protects the copyright of a single audio, implements copyright protection for an audio aggregation. The implementation process of the algorithm is composed of following steps: firstly, the wavelet coefficients of each audio work in the aggregation are set as observation data, and independent components of the observation data are extracted using ICA separate matrix, and then two most important components are chosen to embed the common watermark; secondly, ICA is used to detect watermark. According to experimental results, after embedding watermark into original audio aggregation, there is little impact on audio aggregation's perceptual quality, and when detected audio aggregation suffered from various attacks, the algorithm can extract watermark from the attacked detected audio aggregation. The proposed algorithm has good imperceptibility, strong robustness, low computational complexity, and achieve blind detection.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5568632,no
An improved method to simplify software metric models constructed with incomplete data samples,"Software metric models are useful in predicting the target software metric(s) for any future software project based on the project's predictor metric(s). Obviously, the construction of such a model makes use of a data sample of such metrics from analogous past projects. However, incomplete data often appear in such data samples. Worse still, the necessity to include a particular continuous predictor metric or a particular category for a certain categorical predictor metric is most likely based on an experience-related intuition that the continuous predictor metric or the category matters to the target metric. However, in the presence of incomplete data, this intuition is traditionally not verifiable retrospectively after the model is constructed, leading to redundant continuous predictor metric(s) and/or excessive categorization for categorical predictor metrics. As an improvement of the author's previous work to solve all these problems, this paper proposes a methodology incorporating the k-nearest neighbors (k-NN) multiple imputation method, kernel smoothing, Monte Carlo simulation, and stepwise regression. This paper documents this methodology and one experiment on it.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5569384,no
The discovery of the fault location in NIGS,"A new method is discovered for calculating the fault distance of the overhead line of the Neutral Indirect Grounded System (NIGS) in power distribution networks, in which the single phase to ground fault point or distance is difficult to detect, because the zero sequence current is in lower value. It is found that the information of the fault distance is kept in the zero sequence voltage vector which may be measured at the tail terminal of the questioned line by digging the data. Then an algorithm to calculate the fault location on the overhead lines is proposed by considering that the zero sequence voltage vector at the tail terminal. The value of the zero sequence voltage is determined by the fault location, and the phase angle also contains the distance traveled by the load current to the fault point. The system analysis for parameters is conducted for the NIGS by considering line is actual and by the two terminals' parameters.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5569691,no
A Framework for QoS and Power Management in a Service Cloud Environment with Mobile Devices,"Service cloud integrates the concepts of cloud and service-oriented computing, providing users with tremendous opportunities for composing a large variety of services to achieve desired tasks. At the same time, rapid development in mobile devices has made it a typical instrument for accessing service clouds. However, the limited battery power can greatly impact the usage of mobile devices and their availability for accessing service clouds. Power management has long been an important issue for mobile devices. When considering accessing service clouds, power and QoS have to be considered together. For example, tasks may be delegated to the cloud to save the energy on mobile devices as long as the QoS constraints are satisfied. Current research has not considered these issues in an integrated view. In this paper, we propose a framework for handling QoS and power management for mobile devices in the service cloud environment. In this framework, the service QoS profiles capturing the services' QoS and power behaviors and user profiles capturing users' service usage patterns are defined. Based on the information, service QoS behaviors and power consumption patterns can be predicted to facilitate decisions regarding whether to run a service locally or remotely and how to configure the mobile device such that the power usage can be minimized without violating QoS requirements. Moreover, service migration technology is used to minimize the communication cost such that the latency can be minimized in case the user decides to invoke a remote service.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5569901,no
On an Automatic Simulation Environment Customizing Services for Cloud Simulation Center,"Simulation plays an important role in both academic research and industrial development and manufactory. The users' requirements for simulation are often complex and diverse. It is time consuming and error-prone for users to build different simulation environments manually to conduct their simulation tasks. An Automatic Simulation Environment Customizing Service for Cloud Simulation Center is presented in this paper to address this problem. The service offers an automatic simulation environment that can customize and configure service requirements of simulation users with high efficiency, flexibility and agility. The system is capable of processing diversified requests dynamically and responding the requests in real-time. The operation and experiments on the prototype system operation and experiment show higher availability, flexibility, and reusability of our method than the traditional simulation environment customizing method.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5569902,no
GOS: A Global Optimal Selection Approach for QoS-Aware Web Services Composition,"Services composition technology provides a promising way to create a new service in services-oriented architecture (SOA). However, some challenges are hindering the application of services composition. One of the greatest challenges for composite service developer is how to select a set of services to instantiate composite service with quality of service (QoS) assurance across different autonomous region (e.g. organization or business). To solve QoS-aware Web service composition problem, this paper proposes a global optimization selection (GOS) based on prediction mechanism for QoS values of local services. The GOS includes two parts. First, local service selection algorithm can be used to predict the change of service quality information. Second, GOS aims at enhancing the run-time performance of global selection by reducing to aggregation operation of QoS. The simulation results show that the GOS has lower execution cost than existing approaches.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5569936,no
Fault detection for high availability RAID system,"Designing storage systems to provide high availability in the face of failures needs the use of various data protection techniques, such as dual-controller RAID. The failure of controller may cause data inconsistencies of RAID storage system. Heartbeat is used to detect controllers whether survival. So, the heartbeat cycle's impact on the high availability of a dual-controller hot-standby system has become the key of current research. To address the problem of fixed setting heartbeat in building high availability system currently, an adaptive heartbeat fault detection model of dual controller, which can adjust heartbeat cycle based on the frequency of data read-write request, is designed to improve the high availability of dual-controller RAID storage system. Additionally, this heartbeat mechanism can be used for other applications in distributed settings such as detecting node failures, performance monitoring, and query optimization. Based on this model, the high availability stochastic Petri net model of fault detection was established and used to evaluate the effect of the availability. In addition, we define a AHA (Adaptive Heart Ability) parameter to scale the ability of system heartbeat cycle to adapt to the environment which is changing. The results show that, relatively speaking with fixed configuration, the design is valid and effective, and can enhance dual controller RAID system high availability.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5572373,no
A reliability improvement predictive approach to software testing with Bayesian method,"The capability of improving software reliability is one of the main objectives of software testing. However, the previous testing methods did not pay much attention to how to improve software testing strategy based on software reliability improvement. The relationship between software testing and software reliability is very complex and this is mainly due to the complexity of software products and development processes. The software testing strategy with improving reliability on line needs to possess the ability to predict reliability. Model predictive control provides a good framework to improve predictive effect on line. However, one of the main issues in model predictive control is how to estimate the concern parameter. In this case, Bayesian method is used to estimate the concern parameter: reliability. This proposed reliability improvement predictive approach to software testing with Bayesian method can optimize test allocation scheme on line. The case study shows that it is not definitely true for a software testing method that can find more defects than others can get higher reliability. And the case study also shows that the proposed approach can get better result in the sense of improving reliability than random testing.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5572883,no
Static slicing for PLC program with ladder transformation,"PLC (programmable logic controller) is one type of general industrial control platforms with high reliability, which has been widely used in many real-time control systems, such as transfer lines and continuous casting machines. With the increasing size and complexity of PLC programs, the traditional manual test cannot meet the needs of industrial fields due to its inefficiency and error-prone features. Program slicing is a method of program analysis and understanding. Based on some slicing criterion, it removes the irrelevant statements from the source code to obtain a group of interested program segments. In this way, the scope of the program under study is narrowed. In this paper, program slicing of PLC programs will be studied. The slicing of PLC programs needs special treatment which is usually not necessary in other software written in high level language or assembly language. For example, PLC programs run in cyclic operating mode, and they usually allow each ladder to include more than one output ports making the number of outputs in one statement much larger than that of those in other software. We first introduce a ladder transformation as a preparation for program slicing. Then algorithms for static slicing for PLC programs are proposed. A demo is given to show that this method can effectively reduce the scale of program.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5572887,no
An optimal release policy for software testing process,"In this paper, we discuss the dynamic release problem in software testing processes. If we stop testing too early, there may be too many defects in the software, resulting in too many failures during operation and leading to significant losses due to the failure penalty or user dissatisfaction. If we spend too much time in testing, there may be a high testing cost. Therefore, there is a tradeoff between software testing and releasing. The release time should be dynamically determined by the testing process. The more defects have been detected and removed, the less time will be used for further testing. A continuous time Markov process is proposed to model the testing process. By formulating with dynamic programming we obtain the Hamilton- Jacobi-Bellman equation of the optimal cost function, and derive the threshold structure of the optimal policy. Furthermore, the dynamic optimal release policy is compared with the static optimal release policy by numerical examples, showing that dynamic policy may outperforms static policy very much in some situations.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5573113,no
Two-dimensional bar code mobile commerce - Implementation and performance analysis,"Two-dimensional bar code is called in Japan, QR Code (Quick Response Code, quick response codes). Holding a camera phone in front of this small black box according to what, after the phone software to identify, quickly turned into a line on the site, connected to the site, you can get the information you need. Of course, not necessarily a website, small black box may also be represented by e-mail, image data or text data. The study of this system in different two-dimensional bar code Shoujizuoye. The Implementation and performance analysis, in which experiments can see, network quality of service is subject to various factors Ying Xiang, to send Therefore, in the Internet video O'clock, must consider the status of the network, to the decision to adopt the GOP pattern; on streams in the network, the packet error rate will change the packet loss probability, affect the image quality.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5573247,no
Condition-based reliability modeling for systems with partial and standby redundancy,"In this paper, we introduce a temporal approach for assessing the reliability of systems with components that are subject to change their conditions over time. Such changes could be due to poor working conditions, improper maintenance, or any unexpected malformations throughout the system's useful life. This approach is mainly based on adopting time-dependent reliability functions for assessing condition-based reliability. Our approach can be applied to common situations where the failure rates of the system components have dissimilar distributions.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5575155,no
Using search-based metric selection and oversampling to predict fault prone modules,"Predictive models can be used in the detection of fault prone modules using source code metrics as inputs for the classifier. However, there exist numerous structural measures that capture different aspects of size, coupling and complexity. Identifying a metric subset that enhances the performance for the predictive objective would not only improve the model but also provide insights into the structural properties that lead to problematic modules. Another difficulty in building predictive models comes from unbalanced datasets, which are common in empirical software engineering as a majority of the modules are not likely to be faulty. Oversampling attempts to overcome this deficiency by generating new training instances from the faulty modules. We present the results of applying search-based metric selection and oversampling to three NASA datasets. For these datasets, oversampling results in the largest improvement. Metric subset selection was able to reduce up to 52% of the metrics without decreasing the predictive performance gained with oversampling.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5575249,no
Scenario-Based Early Reliability Model for Distributed Software,"The ability to predict the reliability of software system early can help to improve the quality of system. A scenario-based early reliability model for distributed software system is proposed in this paper. The distributed system is composed of subsystems and components. Using the scenarios of the component interactions, which are described by sequence diagrams, we construct a simplified communication diagram to obtain the interaction parameters in the scenarios. The error propagation between the components is taken into account in our model, which is often overlooked in previous researches. An example is illustrated to show the effectiveness of the proposed model.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5575869,no
Fault-Tolerance in Dataflow-Based Scientific Workflow Management,"This paper addresses the challenges of providing fault-tolerance in scientific workflow management. The specification and handling of faults in scientific workflows should be defined precisely in order to ensure the consistent execution against the process-specific requirements. We identified a number of typical failure patterns that occur in real-life scientific workflow executions. Following the intuitive recovery strategies that correspond to the identified patterns, we developed the methodologies that integrate recovery fragments into fault-prone scientific workflow models. Compared to the existing fault-tolerance mechanisms, the propositions reduce the effort of workflow designers by defining recovery fragments automatically. Furthermore, the developed framework implements the necessary mechanisms to capture the faults from the different layers of a scientific workflow management architecture. Experience indicates that the framework can be employed effectively to model, capture and tolerate the typical failure patterns that we identified.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5577254,no
A Tree Based Strategy for Test Data Generation and Cost Calculation for Uniform and Non-Uniform Parametric Values,"Software testing is a very important phase of software development to ensure that the developed system is reliable. Due to huge number of possible combinations involved in testing and the limitation in the time and resources, it is usually too expensive and sometimes impossible to test systems exhaustively. To reduce the number of test cases to an acceptable level, combinatorial software interaction testing has been suggested and used by many researchers in the software testing field. It is also reported in literature that pairwise (2-way) combinatorial interaction testing can detect most of the software faults. In this paper we propose a new strategy for test data generation, a Tree Based Test Case Generation and Cost Calculation strategy (TBGCC) that supports uniform and non-uniform values, for input parameters (i.e. parameters with same and different number of values). Our strategy is distinct from others work since we include only the test cases which covers the maximum number of pairs in the covering array at every iteration. Additionally, the whole set of test cases will be checked as one block at every iteration only until the covering array is covered. Other strategies check each test case (N-1) times, where N is the maximum number of the input parameters. A detail description of the tree generation strategy, the iterative cost calculation strategy and efficient empirical results are presented.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5577848,no
Towards Estimating Physical Properties of Embedded Systems using Software Quality Metrics,"The complexity of embedded devices poses new challenges to embedded software development in addition to the traditional physical requirements. Therefore, the evaluation of the quality of embedded software and its impact on these traditional properties becomes increasingly relevant. Concepts such as reuse, abstraction, cohesion, coupling, and other software attributes have been used as quality metrics in the software engineering domain. However, they have not been used in the embedded software domain. In embedded systems development, another set of tools is used to estimate physical properties such as power consumption, memory footprint, and performance. These tools usually require costly synthesis-and-simulation design cycles. In current complex embedded devices, one must rely on tools that can help design space exploration at the highest possible level, identifying a solution that represents the best design strategy in terms of software quality, while simultaneously meeting physical requirements. We present an analysis of the cross-correlation between software quality metrics, which can be extracted before the final system is synthesized, and physical metrics for embedded software. Using a neural network, we investigate the use of these cross-correlations to predict the impact that a given modification on the software solution will have on embedded software physical metrics. This estimation can be used to guide design decisions towards improving physical properties of embedded systems, while maintaining an adequate trade-off regarding software quality.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5578300,no
Service Level Agreements in a Rental-based System,"In this paper, we investigate how Service Level Agreeements (SLAs) can be incorporated as part of the system's scheduling and rental decisions to satisfy the different performance promises of high performance computing (HPC) applications. Such SLAs are contracts which specify a set of application-driven requirements such as the estimated total load, contract duration, total utility value and the estimated total number of generated jobs. We present several scheduling and rental based policies that make use of these SLA parameters and demonstrate the effectiveness of such policies to accurately predict and plan for resource levels in a rental-based system.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5578541,no
An empirical approach for software fault prediction,"Measuring software quality in terms of fault proneness of data can help the tomorrow's programmers to predict the fault prone areas in the projects before development. Knowing the faulty areas early from previous developed projects can be used to allocate experienced professionals for development of fault prone modules. Experienced persons can emphasize the faulty areas and can get the solutions in minimum time and budget that in turn increases software quality and customer satisfaction. We have used Fuzzy C Means clustering technique for the prediction of faulty/ non-faulty modules in the project. The datasets used for training and testing modules available from NASA projects namely CM1, PC1 and JM1 include requirement and code metrics which are then combined to get a combination metric model. These three models are then compared with each other and the results show that combination metric model is found to be the best prediction model among three. Also, this approach is compared with others in the literature and is proved to be more accurate. This approach has been implemented in MATLAB 7.9.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5578698,no
Software security testing based on typical SSD:A case study,"Due to the increasing complexity of Web applications, traditional function security testing ways, which only test and validate software security mechanisms, are becoming ineffective to detect latent software security defects (SSD). The number of reported web application vulnerabilities is increasing dramatically. However, the most of vulnerabilities result from some typical SSD. Based on SSD, this paper presents an effective software security testing (SST) model, which extends traditional security testing process to defects behavior analysis which incorporates advantages of traditional testing method and SSD-based security testing methodology. Primary applications show the effectiveness of our test model.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5579101,no
Modeling and Evaluation of Control Flow Vulnerability in the Embedded System,"Faults in control flow-changing instructions are critical for correct execution because the faults could change the behavior of programs very differently from what they are expected to show. The conventional techniques to deal with control flow vulnerability typically add extra instructions to detect control flow-related faults, which increase both static and dynamic instructions, consequently, execution time and energy consumption. In contrast, we make our own control flow vulnerability model to evaluate the effects of different compiler optimizations. We find that different programs show very different degrees of control flow vulnerabilities and some compiler optimizations have high correlation to control flow vulnerability. The results observed in this work can be used to generate more resilient code against control flow-related faults.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5581492,no
Using Content and Text Classification Methods to Characterize Team Performance,"Because of the critical role that communication plays in a team's ability to coordinate action, the measurement and analysis of online transcripts in order to predict team performance is becoming increasingly important in domains such as global software development. Current approaches rely on human experts to classify and compare groups according to some prescribed categories, resulting in a laborious and error-prone process. To address some of these issues, the authors compared and evaluated two methods for analyzing content generated by student groups engaged in a software development project. A content analysis and semi-automated text classification methods were applied to the communication data from a global software student project involving students from the US, Panama, and Turkey. Both methods were evaluated in terms of the ability to predict team performance. Application of the communication analysis' methods revealed that high performing teams develop consistent patterns of communicating which can be contrasted to lower performing teams.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5581509,no
Effect of Replica Placement on the Reliability of Large-Scale Data Storage Systems,"Replication is a widely used method to protect large-scale data storage systems from data loss when storage nodes fail. It is well known that the placement of replicas of the different data blocks across the nodes affects the time to rebuild. Several systems described in the literature are designed based on the premise that minimizing the rebuild times maximizes the system reliability. Our results however indicate that the reliability is essentially unaffected by the replica placement scheme. We show that, for a replication factor of two, all possible placement schemes have mean times to data loss (MTTDLs) within a factor of two for practical values of the failure rate, storage capacity, and rebuild bandwidth of a storage node. The theoretical results are confirmed by means of event-driven simulation. For higher replication factors, an analytical derivation of MTTDL becomes intractable for a general placement scheme. We therefore use one of the alternate measures of reliability that have been proposed in the literature, namely, the probability of data loss during rebuild in the critical mode of the system. Whereas for a replication factor of two this measure can be directly translated into MTTDL, it is only speculative of the MTTDL behavior for higher replication factors. This measure of reliability is shown to lie within a factor of two for all possible placement schemes and any replication factor. We also show that for any replication factor, the clustered placement scheme has the lowest probability of data loss during rebuild in critical mode among all possible placement schemes, whereas the declustered placement scheme has the highest probability. Simulation results reveal however that these properties do not hold for the corresponding MTTDLs for a replication factor greater than two. This indicates that some alternate measures of reliability may not be appropriate for comparing the MTTDL of different placement schemes.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5581603,no
Use of differential evolution in low NO<inf>x</inf> combustion optimization of a coal-fired boiler,"The present work focuses on low NO<sub>x</sub> emissions combustion modification of a 300MW dual-furnaces coal-fired utility boiler through a combination of support vector regression (SVR) and a novel and modern differential evolution optimization technique (DE). SVR, used as a more versatile type of regression tool, was employed to build a complex model between NO<sub>x</sub> emissions and operating conditions by using available experimental results in a case boiler. The trained SVR model performed well in predicting the NO<sub>x</sub> emissions with an average relative error of less than 1.14% compared with the experimental results in the case boiler. The optimal ten inputs (namely operating conditions to be optimized by operators of the boiler) of NO<sub>x</sub> emissions characteristics model were regulated by DE so that low NO<sub>x</sub> emissions were achieved, given that the boiler load is determined. Two cases were optimized in this work to check the possibility of reducing NO<sub>x</sub> emissions by DE under high and low boiler load. The time response of DE was typical of 20 sec, at the same time with the better quality of optimized results. Remarkable good results were obtained when DE was used to optimize NO<sub>x</sub> emissions of this boiler, supporting its applicability for the development of an advanced on-line and real-time low NO<sub>x</sub> emissions combustion optimization software package in modern power plants.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5583524,no
Reliability-based structural integrity assessment of Liquefied Natural Gas tank with hydrogen blistering defects by MCS method,"Hydrogen blistering is one of the serious threats to safe operation of a Liquefied Natural Gas (LNG) tank, therefore safety analysis of hydrogen blistering defects is very important. In order to assess the reliability-based structural integrity of the LNG tank with defects of hydrogen blistering, the following steps were carried out. Firstly, Abaqus code, one of the Finite Element Method (FEM) software, was utilized to calculate 100 J-integral values of crack tip by defining directly. Secondly, the 100 J-integral values of crack tip were used as training data and testing data by Optimized Least Squares Support Vector Machine (OLS-SVM), Least Squares Support Vector Machine (LS-SVM) and Artificial Neural Networks (ANN) to get other 20000 J-integral values of crack tip. Finally, Monte-Carlo Simulation (MCS) was used to assess the reliability-based structural integrity analysis. The results showed that the hydrogen blistering defect with crack will propagate with about 14 percent chance in such a case. It also proved that MCS combined with FEM and SVM was an effective and prospective method for research and application of integrity assessment, which could overcome the data source problem.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5583691,no
Predicting mechanical properties of hot-rolling steel by using RBF network method based on complex network theory,"Recently, producing high-precision and high-quality steel products becomes the major aim of the large-scale iron and steel enterprises. Because of the internal multiplex components of products and complex changes in the production process, it is too difficult to achieve precise control in hot rolling production process. In this paper, radial basis function neural network is used to complete performance prediction. It has the advantage of fast training and high accuracy, and overcomes shortcomings of BP neural network used previously, such as local minimum. When determining the center of radial basis function we make use of complex network visualization which can clearly figure out the relationship between input vectors and receive the center and width according to the relationship of the nodes. Experiments show that the method that is combining community discovery algorithm and RBF enjoy high stability, small training time which means to be suitable to analysis large-scale data. More importantly, it can reach high accuracy.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5584387,no
A Quality Framework to check the applicability of engineering and statistical assumptions for automated gauges,"In high-volume part manufacturing, interactions between program data and program flow can depart significantly from the initial statistical assumptions used during software development. This is a particular challenge for industrial gauging systems used in automotive part production where the applicability of statistical models affects system correctness. This paper uses a Quality Framework to track high-level engineering and statistical assumptions during development. Statistical Process Control (SPC) metrics define an in-control region where the statistical assumptions apply, and an outlier region where they do not apply. The gauge is monitored on-line to verify that production corresponds to the area of the operation where the gauge algorithms are known to work. If outliers are detected in the on-line manufacturing process, then parts can be quarantined, improved gauging algorithms selected, and/or process improvement activities can be initiated.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5584605,no
Cooperative Co-evolution for large scale optimization through more frequent random grouping,"In this paper we propose three techniques to improve the performance of one of the major algorithms for large scale continuous global function optimization. Multilevel Cooperative Co-evolution (MLCC) is based on a Cooperative Co-evolutionary framework and employs a technique called random grouping in order to group interacting variables in one subcomponent. It also uses another technique called adaptive weighting for co-adaptation of subcomponents. We prove that the probability of grouping interacting variables in one subcomponent using random grouping drops significantly as the number of interacting variables increases. This calls for more frequent random grouping of variables. We show how to increase the frequency of random grouping without increasing the number of fitness evaluations. We also show that adaptive weighting is ineffective and in most cases fails to improve the quality of found solution, and hence wastes considerable amount of CPU time by extra evaluations of objective function. Finally we propose a new technique for self-adaptation of the subcomponent sizes in CC. We demonstrate how a substantial improvement can be gained by applying these three techniques.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5586127,no
The jMetal framework for multi-objective optimization: Design and architecture,"jMetal is a Java-based framework for multi-objective optimization using metaheuristics. It is a flexible, extensible, and easy-to-use software package that has been used in a wide range of applications. In this paper, we describe the design issues underlying jMetal, focusing mainly on its internal architecture, with the aim of offering a comprehensive view of its main features to interested researchers. Among the covered topics, we detail the basic components facilitating the implementation of multi-objective metaheuristics (solution representations, operators, problems, density estimators, archives), the included quality indicators to assess the performance of the algorithms, and jMetal's support to carry out full experimental studies.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5586354,no
Towards a formal framework for developing concurrent programs: Modeling dynamic behavior,"It is now widely accepted that programming concurrent software is a complex, error-prone task. Therefore, there is a big interest in the specification, verification and development of concurrent programs using formal methods. In our work-in-progress project, we are attempting to make a constructive framework for developing concurrent programs formally. In this paper, we first demonstrate how one can apply an intermediate artifact of our work, a Z-based formalism, to specify the dynamic behavior of a concurrent system. More precisely, we show how one can use this formalism to explicitly specify the nondeterministic interleaving of processes in a concurrent system. Such a specification will constructively result in a functional program involving all allowable interleaved executions of concurrent processes. As the second contribution of the paper, we introduce a verification method to prove safety properties of concurrent systems specified in the proposed Z-based formalism.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5586965,no
State of art and practice of COTS components search engines,"COTS-Based Software Development has emerged as an approach aiming to improve a number of drawbacks found in the software development industry. The main idea is the reuse of well-tested software products, known as Commercial-Off-The-Shelf (COTS) components, that will be assembled together in order to develop larger systems. The potential benefits of this approach are mainly its reduced costs and shorter development time, while ensuring the quality. One of the most critical activities in COTS-based development is the identification of the COTS candidates to be integrated into the system under development. Nowadays, the Web is the most used means to find COTS candidates. Thus, the use of search engines turns out to be crucial. This paper deals with existing search engines especially proposed to find COTS components satisfying some needs on the Web. It presents a state of the art and practice of search engines followed by a study assessing to which extent they are able to accomplish their objectives.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5587032,no
Proving Model Transformations,"Within the MDA context, model transformations (MT) play an important role as they ensure consistency and significant time savings. Several MT frameworks have been deployed and successfully used in practice. Like for any software, the development of MT programs is error prone. However there is limited support for verification and validation in current MDA technologies. This paper presents an approach to prove model transformations. Model transformations are firstly formalized in B. Then the B provers will be used to analyze and prove the correctness of transformation rules w.r.t. met models and transformation invariants. We also analyze and prove the consistency of transformation rules w.r.t. each other.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5587729,no
Probabilistic Model of System Survivability,"The paper completely formalizes the concept of system survivability on the basis of Knight's research. We present a computable probabilistic model of survivable system which is divided into two layers, i.e. the function and service. The probabilistic refinement is introduced to reason about the survivable system, which is modeled by a probabilistic choice of accepted services with respect to the operating environment. Furthermore, we present an elegant survivability specification and the differences with Knight's related works are discussed. The command-and-control example is also revisited in our framework.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5587742,no
The study of environmental pollution based on second-order partial differential equation model,"In order to forecast and simulate the quality of atmospheric environment, the author constructed the basic three-dimension model of environmental fluid mechanics, and consequently improved the transport model of emissions of air pollutants, based on the analysis of atmospheric pollution and water pollution process. Through the use of Matlab software, the number of days needed for each pollutant to reach the mark has been calculated. The model is able to be applied to a variety of pollution process researches, and can predict the location of a certain concentration of pollutants at a given moment. Finally, cost comparison has been generated, and strategies for environmental problems had been proposed.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5588927,no
Performance analysis of distributed software systems: A model-driven approach,"The design of complex software systems is a challenging task because it involves a wide range of quality attributes such as security, performance, reliability, to name a few. Dealing with each of these attributes requires specific set of skills, which quite often, involves making various trade-offs. This paper proposes a novel Model-Driven Software Performance Engineering (MDSPE) process that can be used for performance analysis requirements of distributed software systems. An example assessment is given to illustrate how our MDSPE process can comply with well-known performance models to assess the performance measures.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5589073,no
Power quality analysis of a Synchronous Static Series Compensator (SSSC) connected to a high voltage transmission network,"This paper presents the power quality analysis of a Synchronous Static Series Compensator (SSSC) connected to a high-voltage transmission network. The analysis employs a simplified harmonic equivalent model of the network that is described in the paper. To facilitate the calculations, a software toolset was developed to measure the voltage harmonics content introduced by the SSSC at the Point of Common Coupling (PCC). These harmonics are then evaluated against the stringent power quality legislation set on the Spanish transmission network. Subsequently, the compliance of an SSSC based on two types of Voltage Sourced Converters (VSC): a 48-pulse and a PWM one, usual in this type of applications, is assessed. Finally, a parameter sensitivity analysis is presented, which looks at the influence of parameters such as the line length and short circuit impedances on the PCC voltage harmonics.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5589923,no
Cause-effect modeling and simulation of power distribution fault events,"Modeling and simulation are important to study power distribution faults due to the limited actual data and high cost of experimentations. Although a number of software packages are available to simulate the electric signals, approaches for simulating fault events in different environments are not well developed yet. In this paper, we propose a framework for modeling and simulating fault events in power distribution systems based on environmental factors and cause-effect relations among them. The spatial and temporal aspects of significant environmental factors leading to various faults are modeled as raster maps and probability distributions, respectively. The cause-effect relations are expressed as fuzzy rules and a hierarchical fuzzy inference system is built to infer the probability of faults given the simulated environments. This work will be helpful in fault diagnosis for different local systems and provide a configurable data source to other researchers and engineers in similar areas as well. A sample fault simulator we have developed is used to illustrate the approaches.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5590028,no
Research on the Copy Detection Algorithm for Source Code Based on Program Organizational Structure Tree Matching,Code plagiarism is an ubiquitous phenomenon in the teaching of Programming Language. A large number of source code can be automatically detected and uses the similarity value to determine whether the copy is present. It can greatly improve the efficiency of teachers and promote teaching quality. A algorithm is provided that firstly match program organizational structure tree and then process the methods of program to calculate the similarity value. It not only applies to process-oriented programming languages but also applies to object-oriented programming language.,2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5590938,no
WSIM: Detecting Clone Pages Based on 3-Levels of Similarity Clues,"Code clones often result in code inconsistencies, which eventually increase cost and degrade quality. Web applications have higher rate of clones than normal software and it is more and more necessary to detect clones in web applications. In this paper, three levels of views in detecting clone pairs are suggested for a web application. The proposed technique utilizes relationships between web pages, passed parameters, and target entities as similarity clues. The results of the experiments also represent the trade-off between recall rate and accuracy. And then, two approaches, static and dynamic selection, are suggested for deciding candidates of clone pairs. As a result, the combined strategy of three levels of methods and two approaches of candidate selection is recommended. Finally, applicability of the proposed approach is shown from the experiments.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5591017,no
A Study on Defect Density of Open Source Software,"Open source software (OSS) development is considered an effective approach to ensuring acceptable levels of software quality. One facet of quality improvement involves the detection of potential relationship between defect density and other open source software metrics. This paper presents an empirical study of the relationship between defect density and download number, software size and developer number as three popular repository metrics. This relationship is explored by examining forty-four randomly selected open source software projects retrieved from SourceForge.net. By applying simple and multiple linear regression analysis, the results reveal a statistically significant relationship between defect density and number of developers and software size jointly. However, despite theoretical expectations, no significant relationship was found between defect density and number of downloads in OSS projects.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5591023,no
Detecting Altered Fingerprints,"The widespread deployment of Automated Fingerprint Identification Systems (AFIS) in law enforcement and border control applications has prompted some individuals with criminal background to evade identification by purposely altering their fingerprints. Available fingerprint quality assessment software cannot detect most of the altered fingerprints since the implicit image quality does not always degrade due to alteration. In this paper, we classify the alterations observed in an operational database into three categories and propose an algorithm to detect altered fingerprints. Experiments were conducted on both real-world altered fingerprints and synthetically generated altered fingerprints. At a false alarm rate of 7%, the proposed algorithm detected 92% of the altered fingerprints, while a well-known fingerprint quality software, NFIQ, only detected 20% of the altered fingerprints.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5595939,no
Software defect prediction using static code metrics underestimates defect-proneness,"Many studies have been carried out to predict the presence of software code defects using static code metrics. Such studies typically report how a classifier performs with real world data, but usually no analysis of the predictions is carried out. An analysis of this kind may be worthwhile as it can illuminate the motivation behind the predictions and the severity of the misclassifications. This investigation involves a manual analysis of the predictions made by Support Vector Machine classifiers using data from the NASA Metrics Data Program repository. The findings show that the predictions are generally well motivated and that the classifiers were, on average, more confident in the predictions they made which were correct.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5596650,no
Quality Models for Free/Libre Open Source Software Towards the Silver Bullet?,"Selecting the right software is of crucial importance for businesses. Free/Libre Open Source Software (FLOSS) quality models can ease this decision-making. This paper introduces a distinction between first and second generation quality models. The former are based on relatively few metrics, require deep insights into the assessed software, relying strongly on subjective human perception and manual labour. Second generation quality models strive to replace the human factor by relying on tools and a multitude of metrics. The key question this paper addresses is whether the emerging FLOSS quality models provide the silver bullet overcoming the shortcomings of first generation models. In order to answer this question, OpenBRR, a first generation quality model, and QualOSS, a second generation quality model, are used for a comparative assessment of Asterisk, a FLOSS implementation of a telephone private branch exchange. Results indicate significant progress, but apparently the silver bullet has not yet been found.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5598015,no
"An Analysis of the """"Inconclusive' Change Report Category in OSS Assisted by a Program Slicing Metric","In this paper, we investigate the Barcode open-source system (OSS) using one of Weiser's original slice-based metrics (Tightness) as a basis. In previous work, low numerical values of this slice-based metric were found to indicate fault-free (as opposed to fault-prone) functions. In the same work, we deliberately excluded from our analysis a category comprising 221 of the 775 observations representing `inconclusive' log reports extracted from the OSS change logs. These represented OSS change log descriptions where it was not entirely clear whether a fault had occurred or not in a function and, for that reason, could not reasonably be incorporated into our analysis. In this paper we present a methodology through which we can draw conclusions about that category of report.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5598109,no
AI-Based Models for Software Effort Estimation,"Decision making under uncertainty is a critical problem in the field of software engineering. Predicting the software quality or the cost/ effort requires high level expertise. AI based predictor models, on the other hand, are useful decision making tools that learn from past projects' data. In this study, we have built an effort estimation model for a multinational bank to predict the effort prior to projects' development lifecycle. We have collected process, product and resource metrics from past projects together with the effort values distributed among software life cycle phases, i.e. analysis & test, design & development. We have used Clustering approach to form consistent project groups and Support Vector Regression (SVR) to predict the effort. Our results validate the benefits of using AI methods in real life problems. We attain Pred(25) values as high as 78% in predicting future projects.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5598114,no
Optimization of forming process for transiting part of combustion chamber,"In deep drawing, one of the sheet metal forming processes is a widely used technique for producing parts from sheet metal blanks in a variety of fields such as aerospace and automobile. Predicting the behaviors of deformation during a forming process is one of the main challenges in cold forming. Instead of the traditional method called trial and error process, numerical simulation based on finite element analysis method could be used to achieve a better understanding of forming deformation during the process and to predict tools for several failure modes to reduce the number of costly experimental verification tests. In this paper, optimization of forming process for transiting part of combustion chamber is mainly performed by ABAQUS software. The numerical simulation model was constructed by CAD software. This part structure is very complex and the defects such as wrinkles and springback occur during forming. The present research work is aimed at avoiding the wrinkles. Using numerical simulation technology, the influences of friction coefficient and blankholder force in wrinkle have been investigated and optimized values for these two important process parameters are suggested in order to eliminate wrinkles. Moreover two criterions for fracture (FLD criterion and reduction of thickness criterion) are briefly presented.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5598376,no
Checkpointing vs. Migration for Post-Petascale Supercomputers,"An alternative to classical fault-tolerant approaches for large-scale clusters is failure avoidance, by which the occurrence of a fault is predicted and a preventive measure is taken. We develop analytical performance models for two types of preventive measures: preventive checkpointing and preventive migration. We also develop an analytical model of the performance of a standard periodic checkpoint fault-tolerant approach. We instantiate these models for platform scenarios representative of current and future technology trends. We find that preventive migration is the better approach in the short term by orders of magnitude. However, in the longer term, both approaches have comparable merit with a marginal advantage for preventive checkpointing. We also find that standard non-prediction-based fault tolerance achieves poor scaling when compared to prediction-based failure avoidance, thereby demonstrating the importance of failure prediction capabilities. Finally, our results show that achieving good utilization in truly large-scale machines (e.g., 2<sup>20</sup> nodes) for parallel workloads will require more than the failure avoidance techniques evaluated in this work.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5599161,no
Application of statistical learning theory to predict corrosion rate of injecting water pipeline,"Support Vector Machines (SVM) represents a new and very promising approach to pattern recognition based on small dataset. The approach is systematic and properly motivated by Statistical Learning Theory (SLT). Training involves separating the classes with a surface that maximizes the margin between them. An interesting property of this approach is that it is an approximate implementation of Structural Risk Minimization (SRM) induction principle, therefore, SVM is more generalized performance and accurate as compared to artificial neural network which embodies the Embodies Risk Minimization (ERM) principle. In this paper, according to corrosion rate complicated reflection relation with influence factors, we studied the theory and method of Support Vector Machines based the statistical learning theory and proposed a pattern recognition method based Support Vector Machine to predict corrosion rate of injecting water pipeline. The outline of the method is as follows: First, we researched the injecting water quality corrosion influence factors in given experimental zones with Gray correlation method; then we used the LibSVM software based Support Vector Machine to study the relationship of those injecting water quality corrosion influence factors, and set up the mode to predict corrosion rate of injecting water pipeline. Application and analysis of the experimental results in Shengli oilfield proved that SVM could achieve greater accuracy than the BP neural network do, which also proved that application of SVM to predict corrosion rate of injecting water pipeline, even to the other theme in petroleum engineering, is reliable, adaptable, precise and easy to operate.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5599754,no
Extend argumentation frameworks based on degree of attack,"To capture the quantity and quality properties of attacks in argumentation process, a fuzzy argumentation framework is presented in this paper. Degree of attack is introduced into argumentation, which contributes to formalizing the internal structure of arguments. Then based on probability theory, total attack is proposed to calculate the effect of all attacks. In addition, the extensional semantics for fuzzy argumentation frameworks are explored in the same way as for extended argumentation frameworks. Finally, the application of FAF in investigation is stated.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5599810,no
Intelligent agent-based system using dissolved gas analysis to detect incipient faults in power transformers,Condition monitoring and software-based diagnosis tools are central to the implementation of efficient maintenance management strategies for many engineering applications including power transformers.,2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5599977,no
MATLAB Design and Research of Fault Diagnosis Based on ANN for the C3I System,Artificial neural networks (ANN) are an information-processing method of a simulation of the structure for biological neurons. C<sup>3</sup>I system as a modern combat unit can control and command the army action and can communicate to others. This paper makes a research on the approach of the artificial neural network for fault diagnosis of C<sup>3</sup>I system and constructs a fault diagnosis system of C<sup>3</sup>I system with ANN. And the system can analyze fault phenomena and detect C<sup>3</sup>I system fault. It will greatly improve the response to the C<sup>3</sup>I system fault diagnosis and maintenance efficiency.,2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5600194,no
Testing the Verticality of Slow-Axis of the Dual Optical Fiber Based on Image Processing,"The polarization dual fiber collimator is one of the important passive devices, which could be used to transform divergent light from the fiber into parallel beam or focus parallel beam into the fiber so as to improve the coupling efficiency of fiber device. However, the quality of double-pigtail fiber used for assembling collimator directly impacts on the performance of fiber collimator. Therefore, it is necessary to detect the quality of double-pigtail fiber before collimators has been packaged to ensure the quality of fiber collimator. The paper has pointed out that the verticality of two slow-axis of the double-pigtail fiber is the major factor to affect the quality of fiber collimator. With """"Panda""""-type double-pigtail fiber as the research object, a novel method to detect the verticality of slow-axis is proposed. First, with the red light of LED as background light source, the clear images of the cross-section end surface of double-pigtail fiber has been obtained by micro-imaging systems; Then after a series of image pre-processing operations, the coordinate information of """"cat's eye"""" light heart can been extracted with the image centroid algorithm; Finally, the angle between the two slow axes of the double-pigtail fiber can be calculated quickly and accurately according to pre-established mathematical model. The detection system mainly consists of CCD, microscopic imaging device, image board and image processing software developed with VC++. The experimental results prove that the system is both practical and effective and can satisfy the collimator assembly precision demands.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5600654,no
Model Checking Security Vulnerabilities in Software Design,"Software faults in the design are frequent sources of security vulnerabilities. Mode checking shows the great promise in detecting and eradicating security vulnerabilities in the programs. The wide use of the system modeling language UML with precise syntax and semantics enables software engineers to analyze the design in details. We present a method of integrating the two techniques to detect design faults which may become security vulnerabilities in the software. Given a software design in UML and security policy, our method extracts the security properties and formally expresses them in temporal logic language. Combining with the security properties, we convert the UML models into PROMELA models, which are input of the model checker SPIN. The method either statically proves that the model satisfies the security property, or provides an execution path that exhibits a violation of the property. A case study shows the feasibility of the method.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5601288,no
New Conceptual Coupling and Cohesion Metrics for Object-Oriented Systems,"The paper presents two novel conceptual metrics for measuring coupling and cohesion in software systems. Our first metric, Conceptual Coupling between Object classes (CCBO), is based on the well-known CBO coupling metric, while the other metric, Conceptual Lack of Cohesion on Methods (CLCOM5), is based on the LCOM5 cohesion metric. One advantage of the proposed conceptual metrics is that they can be computed in a simpler (and in many cases, programming language independent) way as compared to some of the structural metrics. We empirically studied CCBO and CLCOM5 for predicting fault-proneness of classes in a large open source system and compared these metrics with a host of existing structural and conceptual metrics for the same task. As the result, we found that the proposed conceptual metrics, when used in conjunction, can predict bugs nearly as precisely as the 58 structural metrics available in the Columbus source code quality framework and can be effectively combined with these metrics to improve bug prediction.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5601833,no
Fully-automatic annotation of scene vidoes: Establish eye tracking effectively in various industrial applications,"Modern mobile eye-tracking systems record participants' gaze behavior while they move freely within the environment and have haptic contact with the objects of interest. Whereas these mobile systems can be easily set up and operated, the analysis of the stored gaze data is still quite difficult and cumbersome: the recorded scene video with overlaid gaze cursor has to be manually annotated - a very time consuming and error-prone process - preventing the use of eye-tracking techniques in various application fields. In order to overcome these problems, we developed a new software application (JVideoGazer) that uses translation, scale and rotation invariant object detection and tracking algorithms for a fully-automatic video analysis and annotation process. We evaluated our software by comparing its results to those of a manual annotation using scene videos of a typical day-by-day task. Preliminary results show that our software guarantees reliable automatic video analysis even under challenging recording conditions, while it significantly speeds up the annotation process. To the best of our knowledge, the JVideoGazer is the first software for a fully-automatic analysis of gaze videos. With it, modern eye-tracking techniques can be effectively applied to real world situations in various application fields, like research, control, quality management, human-machine interactions, as well as information processing and other industrial applications.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5602650,no
Benchmarking IP blacklists for financial botnet detection,"Every day, hundreds or even thousands of computers are infected with financial malware (i.e. Zeus) that forces them to become zombies or drones, capable of joining massive financial botnets that can be hired by well-organized cyber-criminals in order to steal online banking customers' credentials. Despite the fact that detection and mitigation mechanisms for spam and DDoS-related botnets have been widely researched and developed, it is true that the passive nature (i.e. low network traffic, fewer connections) of financial botnets greatly hinder their countermeasures. Therefore, cyber-criminals are still obtaining high economical profits at relatively low risk with financial botnets. In this paper we propose the use of publicly available IP blacklists to detect both drones and Command & Control nodes that are part of financial botnets. To prove this hypothesis we have developed a formal framework capable of evaluating the quality of a blacklist by comparing it versus a baseline and taking into account different metrics. The contributed framework has been tested with approximately 500 million IP addresses, retrieved during a one-month period from seven different well-known blacklist providers. Our experimental results showed that these IP blacklists are able to detect both drones and C&C related with the Zeus botnet and most important, that it is possible to assign different quality scores to each blacklist based on our metrics. Finally, we introduce the basics of a high-performance IP reputation system that uses the previously obtained blacklists' quality scores, in order to reply almost in real-time whether a certain IP is a member of a financial botnet or not. Our belief is that such a system can be easily integrated into e-banking anti-fraud systems.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5604040,no
Using vulnerability information and attack graphs for intrusion detection,"Intrusion Detection Systems (IDS) have been used widely to detect malicious behavior in network communication and hosts. IDS management is an important capability for distributed IDS solutions, which makes it possible to integrate and handle different types of sensors or collect and synthesize alerts generated from multiple hosts located in the distributed environment. Sophisticated attacks are difficult to detect and make it necessary to integrate multiple data sources for detection and correlation. Attack graph (AG) is used as an effective method to model, analyze, and evaluate the security of complicated computer systems or networks. The attack graph workflow consists of three parts: information gathering, attack graph construction, and visualization. This paper proposes the integration of the AG workflow with an IDS management system to improve alert and correlation quality. The vulnerability and system information is used to prioritize and tag the incoming IDS alerts. The AG is used during the correlation process to filter and optimize correlation results. A prototype is implemented using automatic vulnerability extraction and AG creation based on unified data models.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5604041,no
Cycle accurate simulator generator for NoGap,"Application Specific Instruction-set Processors (ASIPs) are needed to handle the future demand of flexible yet high performance computation in mobile devices. However designing an ASIP is complicated by the fact that not only the processor but, also tools such as assemblers, simulators, and compilers have to be designed. Novel Generator of Accelerators And Processors (NoGap), is a design automation tool for ASIP design that imposes very few limitations on the designer. Yet NoGap supports the designer by automating much of the tedious and error prone tasks associated with ASIP design. This paper will present the techniques used to generate a stand alone software simulator for a processor designed with NoGap. The focus will be on the core algorithms used. Two main problems had to be solved, simulation of a data path graph and simulation of leaf functional units. The concept of sequentialization is introduced and the algorithms used to perform both the leaf unit sequentialization and data path sequentialization is presented. A key component of the sequentialization process is the Micro Architecture Generation Essentials (Mage) dependency graph. The mage dependency graph and the algorithm used for its generation are also presented in this paper. A NoGap simulator was generated for a simple processor and the results were verified.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5604963,no
Weibull distribution in modeling component faults,"Cost efficiency and the issue of quality are pushing software companies to constantly invest in efforts to produce enough quality applications that will arrive in time, with good enough quality to the customer. Quality is not for free, it has a price. Using the different methods of prediction, characteristic parameters will be obtained and will lead to the conclusions about quality even prior the beginning of the project. The Weibull distribution is by far the world's most popular statistical model for life data. On the other hand, exponential distribution and Rayleigh distribution are special cases of Weibull distribution. If we want to model and predict software component quality with mentioned distribution we should take some assumption regarding them. Prediction of component quality will take us to preventive and corrective action in the organization. Based on the results of prediction and modeling of software components faults prior the project start, during project execution and finally during maintenance stage of the component lifecycle some conclusion can be made. In this paper software component prediction using different mathematical models will be presented.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5606115,no
Fault tolerant amplifier system using evolvable hardware,"This paper proposes the use evolvable hardware (EHW) for providing fault tolerance to an amplifier system in a signal-conditioning environment. The system has to maintain a given gain despite the presence of faults, without direct human intervention. The hardware setup includes a reconfigurable system on chip device and an external computer where a genetic algorithm is running. For detecting a gain fault, we propose a software-based built-in self-test strategy that establishes the actual values of gain achievable by the system. The performance evaluation of the fault tolerance strategy proposed is made by adopting two different types of fault-models. The fault simulation results show that the technique is robust and that the genetic algorithm finds the target gain with low error.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5606375,no
The study of effectiveness of computer-assisted instruction versus traditional lecture in probability and statistics,"The purpose of this study is to enhance teaching quality and effect of probability and statistics by computer-assisted instruction (CAI). Using multimedia technology and statistical software, the paper researched mainly three aspects of probability and statistics teaching. Firstly, the paper summarized traditional lecture of probability and statistics and pointed its shortcoming in enhancing teaching quality and effect. Secondly, the paper expatiated CAI in probability and statistics, including some skills of doing multimedia courseware, application of statistical software and how to make use of network. At last, using statistical package for social science (SPSS) 13.0, the paper stressed the effectiveness CAI versus traditional lecture in probability and statistics through a living example.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5607683,no
A Single-Network ANN-based Oracle to verify logical software modules,"Test Oracle is a mechanism to determine if an application executed correctly. In addition, it may be difficult to verify logical software modules due to the complexity of their structures. In this paper, an attempt has been made to study the applications of Artificial Neural Networks as Single-Network Oracles to verify logical modules. First, the logical module under test was modeled by the neural network using a training dataset generated based on the software specifications. Next, the proposed approach was applied to test a subject-registration application; meanwhile, the quality of the proposed oracle is measured by assessing its accuracy, precision, misclassification error and practicality in practice, using mutation testing by implementing two different versions of the case study: a Golden Version and a Mutated Version. The results indicate that neural networks may be reliable and applicative as oracles to verify logical modules.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5608808,no
A neutral network for identifying the out-of-control signals of MEWMA control charts,"Multivariate quality control charts show some advantages to monitor several variables in comparison with the simultaneous use of univariate charts, nevertheless, there are some disadvantages. The main problem is how to interpret the out-of-control signal of a multivariate chart. The MEWMA quality control chart is a very powerful scheme to detect small shifts in the mean vector. There are no previous specific works about the interpretation of the out-of-control signal of this chart. In this paper neural networks are designed to interpret the out-of-control signal of the MEWMA chart, and the percentage of correct classifications is studied for different cases.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5608846,no
An analytical model for performance evaluation of software architectural styles,"Software architecture is an abstract model that gives syntactic and semantic information about the components of a software system and the relationship among them. The success of the software depends on whether the system can satisfy the quality attributes. One of the most critical aspects of the quality attributes of a software system is its performance. Performance analysis can be useful for assessing whether a proposed architecture can meet the desired performance specifications and whether it can help in making key architectural decisions. An architecture style is a set of principles which an architect uses in designing software architecture. Since software architectural styles have frequently been used by architects, these styles have a specific effect on quality attributes. If this effect is measurable for each existing style, it will enable the architect to evaluate and make architectural decisions more easily and precisely. In this paper an effort has been made to introduce a model for investigating this attributes in architectural styles. So, our approach initially models the system as Discrete Time Markov Chain or DTMC, and then extracts the parameters to predict the response time of the system.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5608864,no
Web based ETL component extended with loading and reporting facilitations a financial application tool,"The data warehousing environment includes components that are inherently technical in nature. These cleansing components function to extract, clean, model, transform, transfer and load data from multiple operational systems into a single coherent data model hosted within the data warehouse. The analytical environment is the domain of the business users who use application to query, report, analyze and act upon data in the data warehouse. The conventional process of developing custom code or scripts for this is always a costly, error prone and time-consuming. In this paper, we propose a web based frame work model for representing extraction of data from one or more data sources and use transformation business logic, load the data within the data warehouse. The entire above mentioned have been modeled using UML because the structural and dynamic properties of an information system at the conceptual level are more natural than other classical approaches. New feature of entire loading process of data movement between source and target system is also made visible to the users. In addition a reporting capability to log all successful transformations is provided.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5608874,no
Exploratory failure analysis of open source software,"Reliability growth modeling in software system plays an important role in measuring and controlling software quality during software development. One main approach to reliability growth modeling is based on the statistical correlation of observed failure intensities versus estimated ones by the use of statistical models. Although there are a number of statistical models in the literature, this research concentrates on the following seven models: Weibull, Gamma, S-curve, Exponential, Lognormal, Cubic, and Schneidewind. The failure data collected are from five popular open source software (OSS) products. The objective is to determine which of the seven models best fits the failure data of the selected OSS products as well as predicting the future failure pattern based on partial failure history. The outcome reveals that the best model fitting the failure data is not necessarily the best predictor model.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5608887,no
Auto-generation and redundancy reduction of test cases for reactive systems,"Testing is the fundamental technique to assess the correctness of software systems, but it is cost-labored to generate test cases. One solution to change the situation is to automatize some parts of the testing process, especially the generation of test cases using formal theory and technology. The research work in the direction shows the good perspective. This paper targets on the automatic generation of test cases based on IOSTS, which is widely used to model reactive systems with data. When selecting test cases based on a set of test purposes specified by IOSTS or temporal logic, in general, the redundancy phenomena are unavoidable in the derived test suite. Hence, some strategies are presented for eliminating the redundancies in order to reduce the cost of implementing testing. More importantly, the strategies are directly applied to test cases in form of IOSTS, such that it can reduce not only the size of test suite, but also the cost of deriving test cases.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5608898,no
Using clone detection to identify bugs in concurrent software,"In this paper we propose an active testing approach that uses clone detection and rule evaluation as the foundation for detecting bug patterns in concurrent software. If we can identify a bug pattern as being present then we can localize our testing effort to the exploration of interleavings relevant to the potential bug. Furthermore, if the potential bug is indeed a real bug, then targeting specific thread interleavings instead of examining all possible executions can increase the probability of the bug being detected sooner.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5609529,no
Pairwise test set calculation using k-partite graphs,Many software faults are triggered by unusual combinations of input values and can be detected using pairwise test sets that cover each pair of input values. The generation of pairwise test sets with a minimal size is an NP-complete problem which implies that many algorithms are either expensive or based on a random process. In this paper we present a deterministic algorithm that exploits our observation that the pairwise testing problem can be modeled as a k-partite graph problem. We calculate the test set using well investigated graph algorithms that take advantage of properties of k-partite graphs. We present evaluation results that prove the applicability of our algorithm and discuss possible improvement of our approach.,2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5609653,no
Test generation via Dynamic Symbolic Execution for mutation testing,"Mutation testing has been used to assess and improve the quality of test inputs. Generating test inputs to achieve high mutant-killing ratios is important in mutation testing. However, existing test-generation techniques do not provide effective support for killing mutants in mutation testing. In this paper, we propose a general test-generation approach, called PexMutator, for mutation testing using Dynamic Symbolic Execution (DSE), a recent effective test-generation technique. Based on a set of transformation rules, PexMutator transforms a program under test to an instrumented meta-program that contains mutant-killing constraints. Then PexMutator uses DSE to generate test inputs for the meta-program. The mutant-killing constraints introduced via instrumentation guide DSE to generate test inputs to kill mutants automatically. We have implemented our approach as an extension for Pex, an automatic structural testing tool developed at Microsoft Research. Our preliminary experimental study shows that our approach is able to strongly kill more than 80% of all the mutants for the five studied subjects. In addition, PexMutator is able to outperform Pex, a state-of-the-art test-generation tool, in terms of strong mutant killing while achieving the same block coverage.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5609672,no
Understanding where requirements are implemented,"Trace links between requirements and code reveal where requirements are implemented. Such trace links are essential for code understanding and change management. The lack thereof is often cited as a key reason for software engineering failure. Unfortunately, the creation and maintenance of requirements-to-code traces remains a largely manual and error prone task due to the informal nature of requirements. This paper demonstrates that reasoning about requirements-to-code traces can be done, in part, by considering the calling relationships within the source code (call graph). We observed that requirements-to-code traces form regions along calling dependencies. Better knowledge about these regions has several direct benefits. For example, erroneous traces become detectable if a method inside a region does not trace to a requirement. Or, a missing trace (incompleteness) can be identified. Knowledge of requirement regions can also be used to help guide developers in establishing requirements-to-code traces in a more efficient manner. This paper discusses requirement regions and sketches their benefits.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5609699,no
An approach to improving software inspections performance,Software inspections allow finding and removing defects close to their point of injection and are considered a cheap and effective way to detect and remove defects. A lot of research work has focused on understanding the sources of variability and improving software inspections performance. In this paper we studied the impact of inspection review rate in process performance. The study was carried out in an industrial context effort of bridging the gap from CMMI level 3 to level 5. We supported a decision for process change and improvement based on statistical significant information. Study results led us to conclude that review rate is an important factor affecting code inspections performance and that the applicability of statistical methods was useful in modeling and predicting process performance.,2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5609700,no
Reverse engineering object-oriented distributed systems,"A significant part of the modern software systems are designed and implemented as object-oriented distributed applications, addressing the needs of a globally-connected society. While they can be analyzed focusing only on their object-oriented nature, their understanding and quality assessment require very specific, technology-dependent analysis approaches. This doctoral dissertation describes a methodology for understanding object-oriented distributed systems using a process of reverse engineering driven by the assessment of their technological and domain-specific particularities. The approach provides both system-wide and class-level characterizations, capturing the architectural traits of the systems, and assessing the impact of the distribution-aware features throughout the application. The methodology describes a mostly-automated analysis process fully supported by a tools infrastructure, providing means for detailed understanding of the distribution-related traits and including basic support for the potentially consequent system restructuring.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5609716,no
Fine-grained incremental learning and multi-feature tossing graphs to improve bug triaging,"Software bugs are inevitable and bug fixing is a difficult, expensive, and lengthy process. One of the primary reasons why bug fixing takes so long is the difficulty of accurately assigning a bug to the most competent developer for that bug kind or bug class. Assigning a bug to a potential developer, also known as bug triaging, is a labor-intensive, time-consuming and fault-prone process if done manually. Moreover, bugs frequently get reassigned to multiple developers before they are resolved, a process known as bug tossing. Researchers have proposed automated techniques to facilitate bug triaging and reduce bug tossing using machine learning-based prediction and tossing graphs. While these techniques achieve good prediction accuracy for triaging and reduce tossing paths, they are vulnerable to several issues: outdated training sets, inactive developers, and imprecise, single-attribute tossing graphs. In this paper we improve triaging accuracy and reduce tossing path lengths by employing several techniques such as refined classification using additional attributes and intra-fold updates during training, a precise ranking function for recommending potential tossees in tossing graphs, and multi-feature tossing graphs. We validate our approach on two large software projects, Mozilla and Eclipse, covering 856,259 bug reports and 21 cumulative years of development. We demonstrate that our techniques can achieve up to 83.62% prediction accuracy in bug triaging. Moreover, we reduce tossing path lengths to 1.5-2 tosses for most bugs, which represents a reduction of up to 86.31% compared to original tossing paths. Our improvements have the potential to significantly reduce the bug fixing effort, especially in the context of sizable projects with large numbers of testers and developers.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5609736,no
Model-driven detection of Design Patterns,"Tracing source code elements of an existing Object Oriented software system to the components of a Design Pattern is a key step in program comprehension or re-engineering. It helps, mainly for legacy systems, to discover the main design decisions and trade-offs that are often not documented. In this paper an approach is presented to automatically detect Design Patterns in existing Object Oriented systems by tracing system's source code components to the roles they play in the Patterns. Design Patterns are modelled by high level structural Properties (e.g. inheritance, dependency, invocation, delegation, type nesting and membership relationships) that are checked, by source code parsing, against the system structure and components. The approach allows to detect also Pattern variants, defined by overriding the Pattern structural properties. The approach was applied to some open-source systems to validate it. Results on the detected patterns, discovered variants and on the overall quality of the approach are provided and discussed.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5609740,no
Physical and conceptual identifier dispersion: Measures and relation to fault proneness,"Poorly-chosen identifiers have been reported in the literature as misleading and increasing the program comprehension effort. Identifiers are composed of terms, which can be dictionary words, acronyms, contractions, or simple strings. We conjecture that the use of identical terms in different contexts may increase the risk of faults. We investigate our conjecture using a measure combining term entropy and term context coverage to study whether certain terms increase the odds ratios of methods to be fault-prone. Entropy measures the physical dispersion of terms in a program: the higher the entropy, the more scattered across the program the terms. Context coverage measures the conceptual dispersion of terms: the higher their context coverage, the more unrelated the methods using them. We compute term entropy and context coverage of terms extracted from identifiers in Rhino 1.4R3 and ArgoUML 0.16. We show statistically that methods containing terms with high entropy and context coverage are more fault-prone than others.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5609748,no
Assessment of product maintainability for two space domain simulators,"The software life-cycle of applications supporting space missions follows a rigorous process in order to ensure the application compliance with all the specified requirements. Ensuring the correct behavior of the application is critical since an error can lead, ultimately, to the loss of a complete space mission. However, it is not only important to ensure the correct behavior of the application but also to achieve good product quality since the applications need to be maintained for several years. Then, the question arises, is a rigorous process enough to guarantee good product maintainability? In this paper we assess the software product maintainability of two simulators used to support space missions. The assessment is done using both a standardized analysis, using the SIG quality model for maintainability, and a customized copyright license analysis. The assessment results revealed several quality problems leading to three lessons. First, rigorous process requirements by themselves do not ensure product quality. Second, quality models can be used not only to pinpoint code problems but also to reveal team issues. Finally, tailored analyses, complementing quality models, are necessary for in-depth investigation of quality.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5609752,no
Influences of different excitation parameters upon PEC testing for deep-layered defect detection with rectangular sensor,"In pulsed eddy current testing, repetitive excitation signals with different parameters: duty-cycle, frequency and amplitude have different response representations. This work studies the influences of different excitation parameters on pulsed eddy current testing for deep-layered defects detection of stratified samples with rectangular sensor. The sensor had been proved to be superior in quantification and classification of defects in multi-layered structures compared with traditional circular ones. Experimental results show necessities to optimize the parameters of pulsed excitation signal, and advantages of obtaining better performances to enhance the POD of PEC testing.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5610253,no
FTA-based assessment methodology for adaptability of system under electromagnetic environment,"In order to develop a methodology for assessing the adaptability of system under electromagnetic environment especially RF environment, a Fault-Tree-Analysis-based method is proposed considering the possible performance degradation, disfunction and fault. A component-level and a system-level assessment are performed. An approach to describing logical and hypotactic relation of nodes in Fault-Tree Analysis is designed and illustrated in detail. A software platform is developed to implement the system-level assessment.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5610368,no
Enterprise services (business) collaboration using portal and SOA-based semantics,"With the spread of Internet technologies, the severe competition among businesses, many organizations are moving towards integrating their services online. Service Oriented Architecture (SOA) has shown a potential features in facilitating and managing services integration and expose them through a Portal application. The marriage of these two new standards will definitely lead to a full fledge service integration where key features of both paradigms will smoothen the heterogeneous service integration process. This is achieved by the mean of automatic service composition and orchestration feature of SOA and the automatic customization and profiling feature of the portal technology. This research endeavored to integrate Service Oriented Architecture of (SOA) with portal technology. In this paper, we propose an architecture to help integrating (composing) services online and exposing them via one single point of access known a portal. Composition of Web Services is semantically supported, and relies on user requirements and profile. To ensure smooth integration of services while ensuring good quality for instance high availability, good response time, and processing time, a monitoring technique has been proposed to detect and report if any QoS violation of service composition occurs. The designed architecture has been applied to a use case scenario: a e-commerce portal (ECP) and the results of a system prototype have been reported to demonstrate some relevant features of the proposed approach.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5610609,no
Power electronics health monitoring test platform for assessment of modern power drives and electric machines with regeneration capabilities,This work presents a power electronics health monitoring test platform for assessing modern power drives and electric machines with regeneration capabilities. This versatile platform combines data acquisition of critical system signals that are used for analysis as health indicators for the overall system and individual components such as power semiconductor devices. The test platform combines hardware and software in the loop to allow health monitoring and control techniques for fault tolerance.,2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613572,no
An Anomaly Detection Framework for Autonomic Management of Compute Cloud Systems,"In large-scale compute cloud systems, component failures become norms instead of exceptions. Failure occurrence as well as its impact on system performance and operation costs are becoming an increasingly important concern to system designers and administrators. When a system fails to function properly, health-related data are valuable for troubleshooting. However, it is challenging to effectively detect anomalies from the voluminous amount of noisy, high-dimensional data. The traditional manual approach is time-consuming, error-prone, and not scalable. In this paper, we present an autonomic mechanism for anomaly detection in compute cloud systems. A set of techniques is presented to automatically analyze collected data: data transformation to construct a uniform data format for data analysis, feature extraction to reduce data size, and unsupervised learning to detect the nodes acting differently from others. We evaluate our prototype implementation on an institute-wide compute cloud environment. The results show that our mechanism can effectively detect faulty nodes with high accuracy and low computation overhead.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5615245,no
Simulation of High-Performance Memory Allocators,"Current general-purpose memory allocators do not provide sufficient speed or flexibility for modern high-performance applications. To optimize metrics like performance, memory usage and energy consumption, software engineers often write custom allocators from scratch, which is a difficult and error-prone process. In this paper, we present a flexible and efficient simulator to study Dynamic Memory Managers (DMMs), a composition of one or more memory allocators. This novel approach allows programmers to simulate custom and general DMMs, which can be composed without incurring any additional runtime overhead or additional programming cost. We show that this infrastructure simplifies DMM construction, mainly because the target application does not need to be compiled every time a new DMM must be evaluated. Within a search procedure, the system designer can choose the """"best"""" allocator by simulation for a particular target application. In our evaluation, we show that our scheme will deliver better performance, less memory usage and less energy consumption than single memory allocators.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5615664,no
A Scenario of Service-Oriented Principles Adaptation to the Telecom Providers Service Delivery Platform,"Telecom service providers face a challenge how to increase average revenue per user by new-generation services. In view of the fact that it is extremely difficult to predict the success of the certain kind of service(s), as a result the providers are in need for a dynamic architecture that has to be capable to deliver new services promptly, add resources for successful services as demand increases, or remove unsuccessful services effortlessly. Such architecture has to be a modular standards-based service platform that supports different protocols and interfaces as well as QoS-based transformations and gateways. The potential candidate for this delivery platform is Service Oriented Architecture (SOA). Thus, the aim of our work is to develop a SOA implementation methodology considering telecom service providers existing enterprise network architecture and potential future growth.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5615724,no
Developing Fault Tolerant Distributed Systems by Refinement,"Distributed systems are usually large and complex systems composed of various components. System components are subject to various errors. These failures often require error recovery to be conducted at architectural-level. However, due to complexity of distributed systems, specifying fault tolerance mechanisms at architectural level is complex and error prone. In this paper, we propose a formal approach to specifying components and architectures of fault tolerant distributed and reactive systems. Our approach is based on refinement in the action system formalism - a framework for formal model-driven development of distributed systems. We demonstrate how to specify and refine fault tolerant components and complex distributed systems composed of them. The proposed approach provides designers with a systematic method for developing distributed fault tolerant systems.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5615742,no
Software Fault Prediction Models for Web Applications,"Our daily life increasingly relies on Web applications. Web applications provide us with abundant services to support our everyday activities. As a result, quality assurance for Web applications is becoming important and has gained much attention from software engineering community. In recent years, in order to enhance software quality, many software fault prediction models have been constructed to predict which software modules are likely to be faulty during operations. Such models can be utilized to raise the effectiveness of software testing activities and reduce project risks. Although current fault prediction models can be applied to predict faulty modules of Web applications, one limitation of them is that they do not consider particular characteristics of Web applications. In this paper, we try to build fault prediction models aiming for Web applications after analyzing major characteristics which may impact on their quality. The experimental study shows that our approach achieves very promising results.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5615749,no
The Level of Decomposition Impact on Component Fault Tolerance,"In fault tolerant software systems, the Level of Decomposition (LoD) where design diversity is applied has a major impact on software system reliability. By disregarding this impact, current fault tolerance techniques are prone to reliability decrease due to the inappropriate application level of design diversity. In this paper, we quantify the effect of the LoD on system reliability during software recomposition when the functionalities of the system are redistributed across its components. We discuss the LoD in fault tolerant software architectures according to three component failure transitions: component failure occurrence, component failure propagation, and component failure impact. We illustrate the component aspects that relate the LoD to each of these failure transitions. Finally, we quantify the effect of the LoD on system reliability according to a series of decomposition and/or merge operations that may occur during software recomposition.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5615751,no
The Right Tool for the Right Job: Assessing Model Transformation Quality,"Model-Driven Engineering (MDE) is a software engineering discipline in which models play a central role. One of the key concepts of MDE is model transformations. Because of the crucial role of model transformations in MDE, they have to be treated in a similar way as traditional software artifacts. They have to be used by multiple developers, they have to be maintained according to changing requirements and they should preferably be reused. It is therefore necessary to define and assess their quality. In this paper, we give two definitions for two different views on the quality of model transformations. We will also give some examples of quality assessment techniques for model transformations. The paper concludes with an argument about which type of quality assessment technique is most suitable for either of the views on model transformation quality.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5615754,no
Optimizing Software Quality Assurance,"A major concern for managers of software projects are the triple constraints of cost, schedule and quality due to the difficulties to quantify accurately the trade-offs between them. Project managers working for accredited companies with a high maturity will typically use software cost estimation models like COCOMO II and predict software quality by the estimated number of defects the product is likely to contain at release. However, most of these models are used separately and the interplay between cost/effort estimation, project scheduling and the resultant quality of the software product is not well understood. In this paper, we propose a regression-based model that allows project managers to estimate the trade-off between the quality, cost and development time of a software product, based on previously collected data.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5615758,no
Using Coverage Information to Guide Test Case Selection in Adaptive Random Testing,"Random Testing (RT) is a fundamental software testing technique. Adaptive Random Testing (ART) improves the fault-detection capability of RT by employing the location information of previously executed test cases. Compared with RT, test cases generated in ART are more evenly spread across the input domain. ART has conventionally been applied to programs that have only numerical input types, because the distance between numerical inputs is readily measurable. The vast majority of computer programs, however, involve non-numerical inputs. To apply ART to these programs requires the development of effective new distance measures. Different from those measures that focus on the concrete values of program inputs, in this paper we propose a method to measure the distance using coverage information. The proposed method enables ART to be applied to all kinds of programs regardless of their input types. Empirical studies are further conducted for the branch coverage Manhattan distance measure using the replace and space programs. Experimental results show that, compared with RT, the proposed method significantly reduces the number of test cases required to detect the first failure. This method can be directly applied to prioritize regression test cases, and can also be incorporated into code-based and model-based test case generation tools.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5615787,no
Natural Language Processing Based Detection of Duplicate Defect Patterns,"A Defect pattern repository collects different kinds of defect patterns, which are general descriptions of the characteristics of commonly occurring software code defects. Defect patterns can be widely used by programmers, static defect analysis tools, and even runtime verification. Following the idea of web 2.0, defect pattern repositories allow these users to submit defect patterns they found. However, submission of duplicate patterns would lead to a redundancy in the repository. This paper introduces an approach to suggest potential duplicates based on natural language processing. Our approach first computes field similarities based on Vector Space Model, and then employs Information Entropy to determine the field importance, and next combines the field similarities to form the final defect pattern similarity. Two strategies are introduced to make our approach adaptive to special situations. Finally, groups of duplicates are obtained by adopting Hierarchical Clustering. Evaluation indicates that our approach could detect most of the actual duplicates (72% in our experiment) in the repository.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5615790,no
A Knowledge Discovery Case Study of Software Quality Prediction: ISBSG Database,"Software becomes more and more important in modern society. However, the quality of software is influenced by many un-trustworthy factors. This paper applies MCLP model on ISBSG database to predict the quality of software and reveal the relation between the quality and development attributes. The experimental result shows that the quality level of software can be well predicted by MCLP Model. Besides, several useful conclusions have been drawn from the experimental result.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5615795,no
Similarity-Based Bayesian Learning from Semi-structured Log Files for Fault Diagnosis of Web Services,"With the rapid development of XML language which has good flexibility and interoperability, more and more log files of software running information are represented in XML format, especially for Web services. Fault diagnosis by analyzing semi-structured and XML like log files is becoming an important issue in this area. For most related learning methods, there is a basic assumption that training data should be in identical structure, which does not hold in many situations in practice. In order to learn from training data in different structures, we propose a similarity-based Bayesian learning approach for fault diagnosis in this paper. Our method is to first estimate similarity degrees of structural elements from different log files. Then the basic structure of combined Bayesian network (CBN) is constructed, and the similarity-based learning algorithm is used to compute probabilities in CBN. Finally, test log data can be classified into possible fault categories based on the generated CBN. Experimental results show our approach outperforms other learning approaches on those training datasets which have different structures.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5616413,no
A Hybrid Approach for Model-Based Random Testing,"Random testing is a valuable supplement to systematic test methods because it discovers defects that are very hard to detect with systematic test strategies. We propose a novel approach for random test generation that combines the benefits of model-based testing, constraint satisfaction, and pure random testing. The proposed method has been incorporated into the IDATG (Integrating Design and Automated Test case Generation) tool-set and validated in a number of case studies. Their results indicate that using the new approach it is indeed possible to generate effective test cases in acceptable time.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5617162,no
The SQALE Analysis Model: An Analysis Model Compliant with the Representation Condition for Assessing the Quality of Software Source Code,This paper presents the analysis model of the assessment method of software source code SQALE (Software Quality Assessment Based on Lifecycle Expectations). We explain what brought us to develop consolidation rules based in remediation indices. We describe how the analysis model can be implemented in practice.,2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5617180,no
Software life cycle-based defects prediction and diagnosis technique research,"A model based on Bayesian network is put forward to predict and diagnose software defects before project. In the model causes and effects inference is used to predict defects, and the Bayesian formula is introduced to analyze the prediction result that helps to find the root reason of bring defects. The model considers every phase of software life cycle, such as requirement, design, development and testing, maintenance. The model computes predict result through variational weight of affect-factor. The computation results of specifically affect-factor by model compared with practical defect in same condition which to indict that the model is validity. The model can predict and find defects early and effectively, it can control the software quality and the cost of development.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5619350,no
A framework to discover potential deviation between program and requirement through mining object graph,"Software is expected to be derived from requirements whose properties have been established perfectly. However, requirements are often inaccurate, incomplete or inconsistent as it is a very difficult task to define and analyze requirements. On the other hand, programs most likely deviates from requirements during implementation as the result of misunderstanding or/and neglecting requirements of software engineers. Deviations between programs and requirements are error prone, or cause software to act in unpredictable or unexpected ways. In this paper, we propose a novel framework that uses graph-based mining techniques to discover software execution patterns from object graph firstly, and then searches and matches within a pattern repository to determine whether the discovered software execution patterns are potential deviations from requirements corresponding to neglected requirements or not. After that, the new discovered software execution patterns are labeled and saved back into pattern repository. Hence, the framework is evolutionary and its ability will be more powerful. We give a case study to show how the framework works. The work indicates that the framework is effective and reasonably efficient for improving software quality.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5619394,no
Computational Modeling of Electrical Contact Crimping and Mechanical Strength Analysis,"Several thousands electrical connections are necessary in airplanes. Electrical cables are joined to contacts using manual crimping devices in industrial plants. When mechanical defects appear, the replacement of the defective connections has to be made directly on the airplane. This operation is difficult and time-consuming due to reduced accessibility, making it very costly. The aim of this paper is to simulate the crimping operation and then evaluate the mechanical strength of the obtained connection in order to identify, understand and eliminate defects due to unsatisfactory crimping operations. Material behavior data is accessed through mechanical testing performed on cables and contact; accurate material data is mandatory to improve the accuracy of the simulation results. The Forge software is used to perform the numerical simulation to predict the stress and strain distribution as well as the crimping force during crimping operations. Resulting mechanical strength of the joint is also analysed using numerical simulation. Experimental and numerical results are then compared.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5619564,no
A short-term prediction for QoS of Web Service based on RBF neural networks including an improved K-means algorithm,"The structure of RBF neural networks and an improved K-means algorithm will be introduced in the paper. Based on this, RBF neural networks is applied to predict the QoS of Web Service and the functions of the MATLAB toolbox are adopted to create a network model for QoS prediction. Finally the simulation experiments will prove that using RBF neural networks based on the improved K-means algorithm to predict the QoS of Web Service is effective and efficient.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5620138,no
An improved tone mapping algorithm for High Dynamic Range images,"Real world scenes contain a large range of light intensities. To adapt to display device, High Dynamic Range (HDR) image should be converted into Low Dynamic Range (LDR) image. A common task of tone mapping algorithms is to reproduce high dynamic range images on low dynamic range display devices. In this paper, a new tone mapping algorithm is proposed for high dynamic range images. Based on the probabilistic model is proposed for high dynamic image's tone reproduction, the proposed method uses a logarithmic normal distribution instead of normal distribution. Therefore, the algorithm can preserve visibility and contrast impression of high dynamic range scenes in the common display devices. Experimental results show the superior performance of the app roach in terms of visual quality.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5620562,no
Comparison research of two typical UML-class-diagram metrics: Experimental software engineering,"Measuring UML class diagram complexity can help developers select one with lowest complexity from a variety of different designs with the same functionality; also provide guidance for developing high quality class diagrams. This paper compared the advantages and disadvantages of two typical class-diagram complexity metrics based on statistics and entropy-distance respectively from the view of newly experimental software engineering. 27 class diagrams related to the banking system were classified and predicted their understandability, analyzability and maintainability by means of algorithm C5.0 in well-known software SPSS Clementine. Results showed that UML class diagrams complexity metric based on statistics has higher classification accuracy than that based on entropy-distance.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5622152,no
Rate control scheme for H.264/AVC video encoding,"Rate control is a critical part of video compression systems, while the introduction of Rate-Distortion Optimization makes rate control in H.264/AVC more complex than previous standards. The frame coding complexity MAD is predicted in H.264/AVC rate control. In this paper, we present a novel estimation method for frame coding complexity in H.264/AVC. Meanwhile, a rate control scheme is proposed to improve the coding efficiency. Experimental results demonstrate that our rate control scheme gains better visual quality than that of joint model in H.264/AVC reference software.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5622222,no
Development of a real-time machine vision system for detecting defeats of cord fabrics,"Automatic detection techniques based on machine vision can be used in fabric industry for quality control, which constantly pursues intelligent methods to replace human inspections of product. This work introduces the principle components of a real-time machine vision system for defeat detection of cord fabrics, which is usually a challenging task in practice. The work aims at solving some difficulties usually incurring in such kind of tasks. The design and implementation of the algorithm, software and hardware are introduced. Based on the Gabor wavelet techniques, the system can automatically detect regular texture defects. Our experiments show the proposed algorithm is favorably suited for detecting several types of cord fabric defects. The system testing has been carried in both on-line and off-line situations. The corresponding results show the system has good performance with high detection accuracy, quick response and strong robustness.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5622393,no
Research on formal description of data flow software faults,"Software plays an important part in our society. The occurrence of software fault may lead to serious disaster. Data flow software fault is a kind of important software fault. In this paper, the properties of data dependency relationship are studied, the formal definitions of some data flow software faults, such as using undefined variable, nonused variable since definition, and redefining nonused variable since definition are given, the corresponding detecting methods are proposed, and some sample data flow software faults are given to demonstrate the effectiveness of the proposed methods.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5622594,no
Practical Aspects in Analyzing and Sharing the Results of Experimental Evaluation,"Dependability evaluation techniques such as the ones based on testing, or on the analysis of field data on computer faults, are a fundamental process in assessing complex and critical systems. Recently a new approach has been proposed consisting in collecting the row data produced in the experimental evaluation and store it in a multidimensional data structure. This paper reports the work in progress activities of the entire process of collecting, storing and analyzing the experimental data in order to perform a sound experimental evaluation. This is done through describing the various steps on a running example.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5623408,no
Quantifying Resiliency of IaaS Cloud,"Cloud based services may experience changes - internal, external, large, small - at any time. Predicting and quantifying the effects on the quality-of-service during and after a change are important in the resiliency assessment of a cloud based service. In this paper, we quantify the resiliency of infrastructure-as-a-service (IaaS) cloud when subject to changes in demand and available capacity. Using a stochastic reward net based model for provisioning and servicing requests in a IaaS cloud, we quantify the resiliency of IaaS cloud w.r.t. two key performance measures - job rejection rate and provisioning response delay.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5623413,no
CCDA: Correcting control-flow and data errors automatically,"This paper presents an efficient software technique to detect and correct control-flow errors through addition of redundant codes in a given program. The key innovation performed in the proposed technique is detection and correction of the control-flow errors using both control-flow graph and data-flow graph. Using this technique, most of control-flow errors in the program are detected first, and next corrected, automatically; so, both errors in the control-flow and program data which is caused by control-flow errors can be corrected. In order to evaluate the proposed technique, a post compiler is used, so that the technique can be applied to every 8086 binaries, transparently. Three benchmarks quick sort, matrix multiplication and linked list are used, and a total of 5000 transient faults are injected on several executable points in each program. The experimental results demonstrate that at least 93% of the control-flow errors can be detected and corrected by the proposed technique automatically without any data error generation. Moreover, the performance and memory overheads of the technique are noticeably less than traditional techniques.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5623537,no
An approach for mining web service composition patterns from execution logs,"A service-oriented application is composed of multiple web services to fulfill complex functionality that cannot be provided by individual web service. The combination of services is not random. In many cases, a set of services are repetitively used together in various applications. We treat such a set of services as a service composition pattern. The quality of the patterns is desirable due to the extensive uses and testing in the large number of applications. Therefore, the service composition patterns record the best practices in designing and developing reliable service-oriented applications. The execution log tracks the execution of services in a service-oriented application. To document the service composition patterns, we propose an approach that automatically identifies service composition patterns from various applications using execution logs. We locate a set of associated services using Apriori algorithm and recover the control flows among the services by analyzing the order of service invocation events in the execution log. A case study shows that our approach can effectively detect service composition patterns.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5623568,no
Self-adaptive management of Web processes,"Nowadays, we are assisting to a paradigmatic shift for the development of web applications due to the pervasive distribution of their components among a lot of servers, which are dynamically interconnected by web links. As a consequence, the application logic is often defined by exploiting workflow languages since they are more suitable to address the complexity of these new running environments. Moreover, in many business environments, the behaviour of a large-scale distributed web application is significantly influenced by context events, whose handling could require run-time adaptations of the application logic to properly react to the changing conditions of the execution context. This paper addresses the need for adaptation in large-scale web applications by proposing a programming paradigm based on autonomic workflows, i.e. workflows that are able to self-change their structure in order to allow for the continuation of the execution towards the termination, even if unexpected anomalies occur during the execution. The proposed approach exploits semantic languages for service description, autonomic managers driven by policies specified using a dedicated language, and a knowledge base containing information collected during processes execution. Autonomic actions are performed using Event Condition Action (ECA) rules for assessing system and process conditions, and a set of operations that allow for dynamic adaptation of the running processes. Furthermore, the correctness of workflow adaptation is checked before the modifications are performed, by using both syntactical and semantic constraints.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5623573,no
Measuring web service interfaces,"The following short paper describes a tool supported method for measuring web service interfaces. The goal is to assess the complexity and quality of these interfaces as well as to determine their size for estimating evolution and testing effort. Besides the metrics for quantity, quality and complexity, rules are defined for ensuring maintainability. In the end a tool - WSDAudit - is described which the author has developed for the static analysis of web service definitions. The WSDL schemas are automatically audited and measured for quality assurance and cost estimation. Work is underway to verify them against the BPEL procedures from which they are invoked.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5623580,no
A Rigorous Method for Inspection of Model-Based Formal Specifications,"Writing formal specifications can help developers understand users' requirements, and build a solid foundation for implementation. But like other activities in software development, it is error-prone, especially for large-scale systems. In practice, effective detection of specification errors still remains a challenge. In this paper, we put forward a rigorous, systematic method for the inspection of model-based formal specifications. The method makes good use of the well-defined consistency properties of a specification to provide precise rules and guidelines for inspection. The inspection process utilizes both well-defined expressions derived from the specification and human inspectors' judgments to find errors. We present a case study of the method by describing how it is applied to inspect an Automated Teller Machine (ATM) software specification to investigate the method's feasibility, and explore potential challenges in using it. We also describe a prototype software tool including its functions and distinct features to demonstrate the tool supportability of the method.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5624629,no
Resonance verification of Tehran-Karaj electrical railway,"In this paper analyses results of harmonic and resonance behavior of Tehran-Karaj electric railway are presented. This special electric traction system supplied by 225 kV Autotransformers (ATs) in the west side and a simple mode (without any return feeder) in the east side is considered here where the east side will be equipped with ATs in future development. Also, some parameters which can change quantity and extremity of resonance are explored. For harmonic analysis, Total Harmonic Distortion (THD) factor and Total Demand Distortion (TDD) factor are assessed. To detect the resonance points of the traction power system, harmonic frequency scans are applied. Harmonic analysis of the model is simulated by PSCAD/EMTDC software.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5624707,no
Resonance verification of Tehran-Karaj electrical railway,"In this paper analyses results of harmonic and resonance behavior of Tehran-Karaj electric railway are presented. This special electric traction system supplied by 2  25 kV Autotransformers (ATs) in the west side and a simple mode (without any return feeder) in the east side is considered here where the east side will be equipped with ATs in future development. Also, some parameters which can change quantity and extremity of resonance are explored. For harmonic analysis, Total Harmonic Distortion (THD) factor and Total Demand Distortion (TDD) factor are assessed. To detect the resonance points of the traction power system, harmonic frequency scans are applied. Harmonic analysis of the model is simulated by PSCAD/EMTDC software.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5624738,no
Automatic detection of schwalbe's line in the anterior chamber angle of the eye using HD-OCT images,"Angle-closure glaucoma is a major cause of blindness in Asia and could be detected by measuring the anterior chamber angle (ACA) using gonioscopy, ultrasound biomicroscopy or anterior segment (AS) optical coherence tomography (OCT). The current software in the VisanteTM OCT system by Zeiss is based on manual labeling of the scleral spur, cornea and iris and is a tedious process for ophthalmologists. Furthermore, the scleral spur can not be identified in about 20% to 30% of OCT images and thus measurements of the ACA are not reliable. However, high definition (HD) OCT has identified a more consistent landmark: Schwalbe's line. This paper presents a novel algorithm which automatically detects Schwalbe's line in HD-OCT scans. The average deviation between the values detected using our algorithm and those labeled by the ophthalmologist is less than 0.5% and 0.35% in the horizontal and vertical image dimension, respectively. Furthermore, we propose a new measurement to quantify ACA which is defined as Schwalbe's line bounded area (SLBA).",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5626167,no
An expert system for hydrocephalus patient feedback,"Diagnosis of hydrocephalus symptoms and shunting system faults currently are based on clinical observation, monitoring of cranial growth, transfontanelle pressure, imaging techniques and, on occasion, studies of cerebrospinal fluid (CSF) dynamics. Up to date, the patient has to visit the hospital or meet consultant to diagnose the symptoms that occur due to rising of intracranial pressure or any shunt complications, which cause suffering for the patient and his family. This work presents the design and implementation of an expert system based on real-time patient feedback that aims to provide a suitable decision for hydrocephalus management and shunt diagnosis. Such decision would help in personalising the management as well as detecting and identifying of any shunt malfunctions without the need to contact or visit the hospital. In this paper, the development of patient feedback expert system is described. The outcome of such system would help satisfy the patient's needs regarding his/her shunt.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5626255,no
Automatic code generation for solvers of cardiac cellular membrane dynamics in GPUs,"The modeling of the electrical activity of the heart is of great medical and scientific interest, as it provides a way to get a better understanding of the related biophysical phenomena, allows the development of new techniques for diagnoses and serves as a platform for drug tests. However, due to the multi-scale nature of the underlying processes, the simulations of the cardiac bioelectric activity are still a computational challenge. In addition to that, the implementation of these computer models is a time consuming and error prone process. In this work we present a tool for prototyping ordinary differential equations (ODEs) in the area of cardiac modeling that aim to provide the automatic generation of high performance solvers tailored to the new hardware architecture of the graphic processing units (GPUs). The performance of these automatic solvers was evaluated using four different cardiac myocyte models. The GPU version of the solvers were between 75 and 290 times faster than the CPU versions.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5626620,no
Analysis of image quality parameter of conventional and dental radiographic digital images,"The image quality obtained by a radiographic equipment is very useful to characterize the physical properties of the image radiographic chain, in a quality control of the radiographic equipment. In the radiographic technique it is necessary that the evaluation of the image can guarantee the constancy of its quality to carry out a suitable diagnosis. In this work we have designed some radiographic phantoms for different radiographic digital devices, as dental, conventional, equipments with computed radiography (phosphor plate) and direct radiography (sensor) technology. Additionally, we have developed a software to analyse the image obtained by the radiographic equipment with digital processing techniques, as edge detector, morphological operators, statistical test for the detected combinations.. The design of these phantoms let the evaluation of a wide range of operating conditions of voltage, current and time of the digital equipments. Moreover, the image quality analysis by the automatic software, let study it with objective parameters.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5627182,no
iWander: An Android application for dementia patients,"Non-pharmacological management of dementia puts a burden on those who are taking care of a patient that suffer from this chronic condition. Caregivers frequently need to assist their patients with activities of daily living. However, they are also encouraged to promote functional independence. With the use of a discrete monitoring device, functional independence is increased among dementia patients while decreasing the stress put on caregivers. This paper describes a tool which improves the quality of treatment for dementia patients using mobile applications. Our application, iWander, runs on several Android based devices with GPS and communication capabilities. This allows for caregivers to cost effectively monitor their patients remotely. The data uncollected from the device is evaluated using Bayesian network techniques which estimate the probability of wandering behavior. Upon evaluation several courses of action can be taken based on the situation's severity, dynamic settings and probability. These actions include issuing audible prompts to the patient, offering directions to navigate them home, sending notifications to the caregiver containing the location of the patient, establishing a line of communication between the patient-caregiver and performing a party call between the caregiver-patient and patient's local 911. As patients use this monitoring system more, it will better learn and identify normal behavioral patterns which increases the accuracy of the Bayesian network for all patients. Normal behavior classifications are also used to alert the caregiver or help patients navigate home if they begin to wander while driving allowing for functional independence.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5627669,no
SeyeS - support system for preventing the development of ocular disabilities in leprosy,"Leprosy is an infectious disease caused by Mycobacterium Leprae, and generally compromises neural fibers, leading to the development of disabilities. These limit daily activities or social life. In leprosy, the study of disability considered functional (physical) and activity limitations; and social participation. These are measured respectively by EHF and SALSA scales; by and PARTICIPATION SCALE. The objective of this work was to propose a support system, SeyeS, to eyes disabilities development and progression identification, applying Bayesians network - BN's. It is expected that the proposed system be applied in monitoring the patient during treatment and after therapeutic cure of leprosy. SeyeS presented specificity 1 and sensitivity 0.6 in the identification of ocular disabilities development. With Seyes was discovered that the presence of trichiasis and lagophthalmos, tend to increase the probability of developing more disabilities. Otherwise, characteristics as cataracts tend to decrease development of other disabilities, considering that medical interventions could reduce it. The more import of this system is to indicate what should be monitored, and which elements needs interventions to not increasing patient's ocular disabilities.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5627769,no
A Hierarchical Formal Framework for Adaptive N-variant Programs in Multi-core Systems,"We propose a formal framework for designing and developing adaptive N-variant programs. The framework supports multiple levels of fault detection, masking, and recovery through reconfiguration. Our approach is two-fold: we introduce an Adaptive Functional Capability Model (AFCM) to define levels of functional capabilities for each service provided by the system. The AFCM specifies how, once a fault is detected, a system shall scale back its functional capabilities while still maintaining essential services. Next, we propose a Multilayered Assured Architecture Design (MAAD) to implement reconfiguration requirements specified by AFCMs. The layered design improves system resilience in two dimensions: (1) unlike traditional fault-tolerant architectures that treat functional requirements uniformly, each layer of the assured architecture implements a level of functional capability defined in AFCM. The architecture design uses lower-layer functionalities (which are simpler and more reliable) as reference to monitor high-layer functionalities. The layered design also facilitates an orderly system reconfiguration (resulting in graceful degradation) while maintaining essential system services. (2) Each layer of the assured architecture uses N-variant techniques to improve fault detection. The degree of redundancy introduced by Nvariant implementation determines the mix of faults that can be tolerated at each layer. Our hybrid fault model allows us to consider fault types ranging from benign faults to Byzantine faults. Last but not least, multi-layers combined with N-variant implementations are especially suitable for multi-core systems.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5628722,no
Hybrid Probabilistic Relational Models for System Quality Analysis,"The formalism Probabilistic Relational Models (PRM) couples discrete Bayesian Networks with a modeling formalism similar to UML class diagrams and has been used for architecture analysis. PRMs are well-suited to perform architecture analysis with respect to system qualities since they support both modeling and analysis within the same formalism. A particular strength of PRMs is the ability to perform meaningful analysis of domains where there is a high level of uncertainty, as is often the case when performing system quality analysis. However, the use of discrete Bayesian networks in PRMs complicates the analysis of continuous phenomena. The main contribution of this paper is the Hybrid Probabilistic Relational Models (HPRM) formalism which extends PRMs to enable continuous analysis thus extending the applicability for architecture analysis and especially for trade-off analysis of system qualities. HPRMs use hybrid Bayesian networks which allow combinations of discrete and continuous variables. In addition to presenting the HPRM formalism, the paper contains an example which details the use of HPRMs for architecture trade-off analysis.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5630235,no
A Multi-Agent Approach for Self-Diagnosis of a Hydrocephalus Shunting System,"The human brain is immersed in cerebrospinal fluid, which protects it from mechanical stresses and helps support its weight through buoyancy. A constant overproduction, blockage or reabsorption difficulty can result in a build-up of fluid in the skull (Hydrocephalus), which can lead to brain damage or even death. Existing treatments rely on passive implantable shunts that drain the excess fluid out of the skull cavity, thus keeping intracranial pressure in equilibrium. Shunt malfunction is one of the most common clinical problems in pediatric neurosurgery. Unfortunately, symptoms of various shunt complications can be very similar thus complicating the diagnosing process. It is proposed to complement the existing implanted valve with an intracranial pressure sensor, flowmeter and transceiver to be able to make a self-diagnosis. By using such method, all current shunt malfunctions should be detected early and the types of these malfunctions would be predicted. Currently, a mechatronic valve with control software is under investigation and it will be a future solution for most of current shunt problems. This paper describes the design of a multi-agent system for self-diagnosis of hydrocephalus shunting system. An intelligent concept for intelligent agents is proposed that would deal with any shunt malfunctions in an independent and efficient way, with different agents cooperating and communicating through message exchange, each agent specialised in specific tasks of the diagnosis process. Six types of agents have been proposed to detect any faults in hierarchical way. This paper proposed one of the most promising methods for the self-diagnosis and monitoring of hydrocephalus shunting system based on a novel multi-agent approach.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5633648,no
Predicting Faults in High Assurance Software,"Reducing the number of latent software defects is a development goal that is particularly applicable to high assurance software systems. For such systems, the software measurement and defect data is highly skewed toward the not-fault-prone program modules, i.e., the number of fault-prone modules is relatively very small. The skewed data problem, also known as class imbalance, poses a unique challenge when training a software quality estimation model. However, practitioners and researchers often build defect prediction models without regard to the skewed data problem. In high assurance systems, the class imbalance problem must be addressed when building defect predictors. This study investigates the roughly balanced bagging (RBBag) algorithm for building software quality models with data sets that suffer from class imbalance. The algorithm combines bagging and data sampling into one technique. A case study of 15 software measurement data sets from different real-world high assurance systems is used in our investigation of the RBBag algorithm. Two commonly used classification algorithms in the software engineering domain, Naive Bayes and C4.5 decision tree, are combined with RBBag for building the software quality models. The results demonstrate that defect prediction models based on the RBBag algorithm significantly outperform models built without any bagging or data sampling. The RBBag algorithm provides the analyst with a tool for effectively addressing class imbalance when training defect predictors during high assurance software development.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5634306,no
Proved Metamodels as Backbone for Software Adaptation,"In this paper we demonstrate the error-prone status of the UML 2.3 metamodel relating to state machines. We consequently provide a corrected version based on formal proofs written and processed with the help of the Coq system prover. The purpose of the proposed research is to support dynamical adaptation by means of models at runtime. Software components are internally endowed with complex state machines (models) realizing their behavior. Adaptation amounts to dynamically changing the state machines' structure (for instance, adding a new state). This occurs via SimUML, a state machine execution engine that is constructed on the top of a metamodel resulting from correctness proofs.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5634317,no
A Framework for Qualitative and Quantitative Formal Model-Based Safety Analysis,"In model-based safety analysis both qualitative aspects i.e. what must go wrong for a system failure) and quantitative aspects (i.e. how probable is a system failure) are very important. For both aspects methods and tools are available. However, until now for each aspect new and independent models must be built for analysis. This paper proposes the SAML framework as a formal foundation for both qualitative and quantitative formal model-based safety analysis. The main advantage of SAML is the combination of qualitative and quantitative formal semantics which allows different analyses on the same model. This increases the confidence in the analysis results, simplifies modeling and is less error-prone. The SAML framework is tool-independent. As proof-of-concept, we present sound transformation of the formalism into two state of the art model-checking notations. Prototypical tool support for the sound transformation of SAML into PRISM and MRMC for probabilistic analysis as well as different variants of the SMV model checker for qualitative analysis is currently being developed.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5634319,no
Request Path Driven Model for Performance Fault Diagnoses,"Locating and diagnosing performance faults in distributed systems is crucial but challenging. Distributed systems are increasingly complex, full of various correlation and dependency, and exhibit dramatic dynamics. All these made traditional approaches prone to high false alarms. In this paper, we propose a novel system modeling technique, which encodes component's dynamic dependencies and behavior characteristics into system's meta-model and takes it as a unifying framework to deploy component's sub-models. We propose an automatic analyze approach to distill, from request travel paths, request path signatures, the essential information of component's dynamic behaviors, and use it to induce metamodel with Bayesian network, and then use the model to make fault location and diagnoses. We take up fault-injection experiments with RUBiS, a TPCW alike benchmark, simulating eBay.com. The results indicate that our model approach provides effective problem diagnosis, i.e., Bayesian network technique is effective for fault detecting and pinpointing, in terms of request tracing context. Moreover, meta-model induced with request paths, provides an effective guidance for learning statistical correlations among metrics across the system, which effectively avoid 'false alarms' in fault pinpointing. As a case study, we construct a proactive recovery framework, which integrate our system modeling technique with software rejuvenation technique to guarantee system's quality of services.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5634347,no
Earliest Start Time Estimation for Advance Reservation-Based Resource Brokering within Computational Grids,"The ability to conduct advance reservations within grid environments is crucial for applications that want to utilize distributed resources in a predictable and efficient way. Advance reservations are essential for supporting deadline driven applications and the co-allocation of distributed resources. Further, advance reservations can significantly enhance the capabilities of resource brokers. Many Local Resource Management Systems (LRMSs), e.g. Torque/Maui, PBSPro and LSF, provide support for advance reservations. However, these capabilities are usually not accessible via the grid middleware. This paper presents and evaluates a job broker architecture that addresses this deficit and provides support for advance reservation on grid-level as well as a reservation-aware grid resource broker. A core component of the presented job broker architecture is the Earliest Start Time Estimator (ESE), which predicts queuing delays at grid resources. The job broker service is part of the Migol system - a grid middleware that addresses the fault tolerance of applications by offering capabilities such as automatic monitoring and recoveries.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5634403,no
A Method of Detecting Vulnerability Defects Based on Static Analysis,"This paper proposes a method for detecting vulnerability defects caused by tainted data based on state machine. It first uses state machine to define various defect patterns. If the states of state machine is considered as the value propagated in dataflow analysis and the union operation of the state sets as the aggregation operation of dataflow analysis, the defect detection can be treated as a forward dataflow analysis problem. To reduce the false positives caused by intraprocedural analysis, the dynamic information of program was represented approximately by abstract value of variables, and then infeasible path can be identified when some variable's abstract value is empty in the state condition. A function summary method is proposed to get the information needed for performing interprocedural defect detection. The method proposed has been implemented in a defect testing tools.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5634685,no
Hypervisor-Based Virtual Hardware for Fault Tolerance in COTS Processors Targeting Space Applications,"Commercial off the shelf processors are becoming mandatory in space applications to satisfy the ever-growing demand for on-board computing power. As a result, architecture able to withstand the harshness of the space environment are needed to cope with the errors that may affect such processors, which are not specifically designed for being used in space. Beside design and implementation costs, validation of the obtained architecture is a very cost- and time-consuming operation. In this paper we propose an architecture to quickly develop dependable embedded systems using time redundancy. The main novelty of the approach lies in the usage of a hyper visor for implementing seamlessly time redundancy, consistency checking, temporal and spatial segregation of programs that are needed to guarantee a safe execution of the application software. The proposed architecture needs to be validated only once then, provided that the same hyper visor is available for different hardware platforms, it can be deployed without the need for re-validation. We describe a prototypical implementation of the approach and we provide experimental data that assess the effectiveness of the approach.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5634983,no
Characterizing Failures in Mobile OSes: A Case Study with Android and Symbian,"As smart phones grow in popularity, manufacturers are in a race to pack an increasingly rich set of features into these tiny devices. This brings additional complexity in the system software that has to fit within the constraints of the devices (chiefly memory, stable storage, and power consumption) and hence, new bugs are revealed. How this evolution of smartphones impacts their reliability is a question that has been largely unexplored till now. With the release of open source OSes for hand-held devices, such as, Android (open sourced in October 2008) and Symbian (open sourced in February 2010), we are now in a position to explore the above question. In this paper, we analyze the reported cases of failures of Android and Symbian based on bug reports posted by third-party developers and end users and documentation of bug fixes from Android developers. First, based on 628 developer reports, our study looks into the manifestation of failures in different modules of Android and their characteristics, such as, their persistence and dependence on environment. Next, we analyze similar properties of Symbian bugs based on 153 failure reports. Our study indicates that Development Tools, Web Browsers, and Multimedia applications are most error-prone in both these systems. We further analyze 233 bug fixes for Android and categorized the different types of code modifications required for the fixes. The analysis shows that 77% of errors required minor code changes, with the largest share of these coming from modifications to attribute values and conditions. Our final analysis focuses on the relation between customizability, code complexity, and reliability in Android and Symbian. We find that despite high cyclomatic complexity, the bug densities in Android and Symbian are surprisingly low. However, the support for customizability does impact the reliability of mobile OSes and there are cautionary tales for their further development.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635045,no
Comparing SQL Injection Detection Tools Using Attack Injection: An Experimental Study,"System administrators frequently rely on intrusion detection tools to protect their systems against SQL Injection, one of the most dangerous security threats in database-centric web applications. However, the real effectiveness of those tools is usually unknown, which may lead administrators to put an unjustifiable level of trust in the tools they use. In this paper we present an experimental evaluation of the effectiveness of five SQL Injection detection tools that operate at different system levels: Application, Database and Network. To test the tools in a realistic scenario, Vulnerability and Attack Injection is applied in a setup based on three web applications of different sizes and complexities. Results show that the assessed tools have a very low effectiveness and only perform well under specific circumstances, which highlight the limitations of current intrusion detection tools in detecting SQL Injection attacks. Based on experimental observations we underline the strengths and weaknesses of the tools assessed.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635053,no
Change Bursts as Defect Predictors,"In software development, every change induces a risk. What happens if code changes again and again in some period of time? In an empirical study on Windows Vista, we found that the features of such change bursts have the highest predictive power for defect-prone components. With precision and recall values well above 90%, change bursts significantly improve upon earlier predictors such as complexity metrics, code churn, or organizational structure. As they only rely on version history and a controlled change process, change bursts are straight-forward to detect and deploy.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635057,no
The Impact of Coupling on the Fault-Proneness of Aspect-Oriented Programs: An Empirical Study,"Coupling in software applications is often used as an indicator of external quality attributes such as fault-proneness. In fact, the correlation of coupling metrics and faults in object oriented programs has been widely studied. However, there is very limited knowledge about which coupling properties in aspect-oriented programming (AOP) are effective indicators of faults in modules. Existing coupling metrics do not take into account the specificities of AOP mechanisms. As a result, these metrics are unlikely to provide optimal predictions of pivotal quality attributes such as fault-proneness. This impacts further by restraining the assessments of AOP empirical studies. To address these issues, this paper presents an empirical study to evaluate the impact of coupling sourced from AOP-specific mechanisms. We utilise a novel set of coupling metrics to predict fault occurrences in aspect-oriented programs. We also compare these new metrics against previously proposed metrics for AOP. More specifically, we analyse faults from several releases of three AspectJ applications and perform statistical analyses to reveal the effectiveness of these metrics when predicting faults. Our study shows that a particular set of fine-grained directed coupling metrics have the potential to help create better fault prediction models for AO programs.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635061,no
Prioritizing Mutation Operators Based on Importance Sampling,"Mutation testing is a fault-based testing technique for measuring the adequacy of a test suite. Test suites are assigned scores based on their ability to expose synthetic faults (i.e., mutants) generated by a range of well-defined mathematical operators. The test suites can then be augmented to expose the mutants that remain undetected and are not semantically equivalent to the original code. However, the mutation score can be increased superfluously by mutants that are easy to expose. In addition, it is infeasible to examine all the mutants generated by a large set of mutation operators. Existing approaches have therefore focused on determining the sufficient set of mutation operators and the set of equivalent mutants. Instead, this paper proposes a novel Bayesian approach that prioritizes operators whose mutants are likely to remain unexposed by the existing test suites. Probabilistic sampling methods are adapted to iteratively examine a subset of the available mutants and direct focus towards the more informative operators. Experimental results show that the proposed approach identifies more than 90% of the important operators by examining ? 20% of the available mutants, and causes a 6% increase in the importance measure of the selected mutants.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635074,no
Improving the Precision of Dependence-Based Defect Mining by Supervised Learning of Rule and Violation Graphs,"Previous work has shown that application of graph mining techniques to system dependence graphs improves the precision of automatic defect discovery by revealing subgraphs corresponding to implicit programming rules and to rule violations. However, developers must still confirm, edit, or discard reported rules and violations, which is both costly and error-prone. In order to reduce developer effort and further improve precision, we investigate the use of supervised learning models for classifying and ranking rule and violation subgraphs. In particular, we present and evaluate logistic regression models for rules and violations, respectively, which are based on general dependence-graph features. Our empirical results indicate that (i) use of these models can significantly improve the precision and recall of defect discovery, and (ii) our approach is superior to existing heuristic approaches to rule and violation ranking and to an existing static-warning classifier, and (iii) accurate models can be learned using only a few labeled examples.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635105,no
Assessing Asymmetric Fault-Tolerant Software,"The most popular forms of fault tolerance against design faults use """"asymmetric"""" architectures in which a """"primary"""" part performs the computation and a """"secondary"""" part is in charge of detecting errors and performing some kind of error processing and recovery. In contrast, the most studied forms of software fault tolerance are """"symmetric"""" ones, e.g. N-version programming. The latter are often controversial, the former are not. We discuss how to assess the dependability gains achieved by these methods. Substantial difficulties have been shown to exist for symmetric schemes, but we show that the same difficulties affect asymmetric schemes. Indeed, the latter present somewhat subtler problems. In both cases, to predict the dependability of the fault-tolerant system it is not enough to know the dependability of the individual components. We extend to asymmetric architectures the style of probabilistic modeling that has been useful for describing the dependability of """"symmetric"""" architectures, to highlight factors that complicate the assessment. In the light of these models, we finally discuss fault injection approaches to estimating coverage factors. We highlight the limits of what can be predicted and some useful research directions towards clarifying and extending the range of situations in which estimates of coverage of fault tolerance mechanisms can be trusted.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635113,no
A Multi-factor Software Reliability Model Based on Logistic Regression,"This paper proposes a multi-factor software reliability model based on logistic regression and its effective statistical parameter estimation method. The proposed parameter estimation algorithm is composed of the algorithm used in the logistic regression and the EM (expectation-maximization) algorithm for discrete-time software reliability models. The multi-factor model deals with the metrics observed in testing phase (testing environmental factors), such as test coverage and the number of test workers, to predict the number of residual faults and other reliability measures. In general, the multi-factor model outperforms the traditional software reliability growth model like discrete-time non-homogeneous models in terms of data-fitting and prediction abilities. However, since it has a number of parameters, there is the problem in estimating model parameters. Our modeling framework and its estimation method are quite simpler than the existing methods, and are promising for expanding the applicability of multi-factor software reliability model. In numerical experiments, we examine data-fitting ability of the proposed model by comparing with the existing multi-factor models. The proposed method provides the similar fitting ability to existing multi-factor models, although the computation effort of parameter estimation is low.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635114,no
"Flexible, Any-Time Fault Tree Analysis with Component Logic Models","This article presents a novel approach to facilitating fault tree analysis during the development of software-controlled systems. Based on a component-oriented system model, it combines second-order probabilistic analysis and automatically generated default failure models with a level-of-detail concept to ensure early and continuous analysability of system failure behaviour with optimal effort, even in the presence of incomplete information and dissimilar levels of detail in different parts of an evolving system model. The viability and validity of the method are demonstrated by means of an experiment.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635118,no
Towards a Bayesian Approach in Modeling the Disclosure of Unique Security Faults in Open Source Projects,"Software security has both an objective and a subjective component. A lot of the information available about that today is focused on the security vulnerabilities and their disclosure. It is less frequent that security breaches and failures rates are reported, even in open source projects. Disclosure of security problems can take several forms. A disclosure can be accompanied by a release of the fix for the problem, or not. The latter category can be further divided into voluntary and involuntary security issues. In widely used software there is also considerable variability in the operational profile under which the software is used. This profile is further modified by attacks on the software that may be triggered by security disclosures. Therefore a comprehensive model of software security qualities of a product needs to incorporate both objective measures, such as security problem disclosure, repair and, failure rates, as well as less objective metrics such as implied variability in the operational profile, influence of attacks, and subjective impressions of exposure and severity of the problems, etc. We show how a classical Bayesian model can be adapted for use in the security context. The model is discussed and assessed using data from three open source software projects. Our results show that the model is suitable for use with a certain subset of disclosed security faults, but that additional work will be needed to identify appropriate shape and scaling functions that would accurately reflect end-user perceptions associated with security problems.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635125,no
A Consistency Check Algorithm for Component-Based Refinements of Fault Trees,"The number of embedded systems in our daily lives that are distributed, hidden, and ubiquitous continues to increase. Many of them are safety-critical. To provide additional or better functionalities, they are becoming more and more complex, which makes it difficult to guarantee safety. It is undisputed that safety must be considered before the start of development, continue until decommissioning, and is particularly important during the design of the system and software architecture. An architecture must be able to avoid, detect, or mitigate all dangerous failures to a sufficient degree. For this purpose, the architectural design must be guided and verified by safety analyses. However, state-of-the-art component-oriented or model-based architectural design approaches use different levels of abstraction to handle complexity. So, safety analyses must also be applied on different levels of abstraction, and it must be checked and guaranteed that they are consistent with each other, which is not supported by standard safety analyses. In this paper, we present a consistency check for CFTs that automatically detects commonalities and inconsistencies between fault trees of different levels of abstraction. This facilitates the application of safety analyses in top-down architectural designs and reduces effort.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635142,no
DoDOM: Leveraging DOM Invariants for Web 2.0 Application Robustness Testing,"Web 2.0 applications are increasing in popularity. However, they are also prone to errors because of their dynamic nature. This paper presents DoDOM, an automated system for testing the robustness of Web 2.0 applications based on their Document Object Models (DOMs). DoDOM repeatedly executes the application under a trace of recorded user actions and observes the client-side behavior of the application in terms of its DOM structure. Based on the observations, DoDOM extracts a set of invariants on the web application's DOM structure. We show that invariants exist for real applications and can be learned within a reasonable number of executions. We further use fault-injection experiments to demonstrate the uses of the invariants in detecting errors in web applications. The invariants are found to provide high coverage in detecting errors that impact the DOM, with a low rate of false positives.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635146,no
Using Search Methods for Selecting and Combining Software Sensors to Improve Fault Detection in Autonomic Systems,"Fault-detection approaches in autonomic systems typically rely on runtime software sensors to compute metrics for CPU utilization, memory usage, network throughput, and so on. One detection approach uses data collected by the runtime sensors to construct a convex-hull geometric object whose interior represents the normal execution of the monitored application. The approach detects faults by classifying the current application state as being either inside or outside of the convex hull. However, due to the computational complexity of creating a convex hull in multi-dimensional space, the convex-hull approach is limited to a few metrics. Therefore, not all sensors can be used to detect faults and so some must be dropped or combined with others. This paper compares the effectiveness of genetic-programming, genetic-algorithm, and random-search approaches in solving the problem of selecting sensors and combining them into metrics. These techniques are used to find 8 metrics that are derived from a set of 21 available sensors. The metrics are used to detect faults during the execution of a Java-based HTTP web server. The results of the search techniques are compared to two hand-crafted solutions specified by experts.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635154,no
A Quantitative Approach to Software Maintainability Prediction,"Software maintainability is one important aspect in the evaluation of software evolution of a software product. Due to the complexity of tracking maintenance behaviors, it is difficult to accurately predict the cost and risk of maintenance after delivery of software products. In an attempt to address this issue quantitatively, software maintainability is viewed as an inevitable evolution process driven by maintenance behaviors, given a health index at the time when a software product are delivered. A Hidden Markov Model (HMM) is used to simulate the maintenance behaviors shown as their possible occurrence probabilities. And software metrics is the measurement of the quality of a software product and its measurement results of a product being delivered are combined to form the health index of the product. The health index works as a weight on the process of maintenance behavior over time. When the occurrence probabilities of maintenance behaviors reach certain number which is reckoned as the indication of the deterioration status of a software product, the product can be regarded as being obsolete. Longer the time, better the maintainability would be.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635174,no
Search-based Prediction of Fault-slip-through in Large Software Projects,"A large percentage of the cost of rework can be avoided by finding more faults earlier in a software testing process. Therefore, determination of which software testing phases to focus improvements work on, has considerable industrial interest. This paper evaluates the use of five different techniques, namely particle swarm optimization based artificial neural networks (PSO-ANN), artificial immune recognition systems (AIRS), gene expression programming (GEP), genetic programming (GP) and multiple regression (MR), for predicting the number of faults slipping through unit, function, integration and system testing phases. The objective is to quantify improvement potential in different testing phases by striving towards finding the right faults in the right phase. We have conducted an empirical study of two large projects from a telecommunication company developing mobile platforms and wireless semiconductors. The results are compared using simple residuals, goodness of fit and absolute relative error measures. They indicate that the four search-based techniques (PSO-ANN, AIRS, GEP, GP) perform better than multiple regression for predicting the fault-slip-through for each of the four testing phases. At the unit and function testing phases, AIRS and PSO-ANN performed better while GP performed better at integration and system testing phases. The study concludes that a variety of search-based techniques are applicable for predicting the improvement potential in different testing phases with GP showing more consistent performance across two of the four test phases.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635180,no
Differential Histogram Modification-Based Reversible Watermarking with Predicted Error Compensation,"Reversible watermarking inserts watermark into digital media in such a way that visual transparency is preserved and the original media can be restored from the marked one without any loss of media quality. High capacity and high visual quality are a major requirement for reversible watermarking. In this paper, we present a novel reversible watermarking scheme that embeds message bits by modifying the differential histogram of adjacent pixels. Also, overflow and underflow problem is prevented with the proposed predicted error compensation scheme. Through experiments on various images, we prove that the presented scheme achieves 100% reversibility, high capacity, and high visual quality over other methods, while maintaining the induced-distortion low.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635977,no
Design and Implementation of Movie Camera Recording on Worker's Motion Tracing System by Terrestrial Magnetism Sensors,"A basis of quality control on industrial products is to confirm that materials parts, assembly and processing and so on satisfy regulation. There are some cases depending on types of assembly processes that the result of certain process cannot be confirmed whether it satisfies regulation. For example, order to fasten some screws to fix a part is determined to guaranty accuracy to fox the part. However, in case of screwing order is not correct, the part is fixed. In this case, the accuracy is not guarantied. If accuracy is enough high accidentally, inspection on samples of assembled products cannot find the violation of screwing order. Therefore, we are now developing a system that monitors routine works using terrestrial magnetism sensors. In this system, a terrestrial magnetism sensor is attached into a worker to a tool, and the system judges whether certain routine work is correctly done by using output of the sensor. We have realized almost precise detection of wrong works, however, we could not identify which kind of wrong work is detected by our system without seeing worker's motion. So grasping which kind of wrong works happened is depends on worker's report on his/her work. Therefore, we added a video recording feature in our system. In this paper, we describe its design and implementation and simple evaluation result.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5636215,no
Classification of power quality disturbances using Wavelet and Artificial Neural Networks,"An automated classification system based on Wavelet transform as a feature extraction tool in combination with Artificial Neural Network as algorithm classifier is presented. Perturbed signals generated according to mathematical models have been used to obtain experimental results in two stages, first, with a data set with simple disturbances and, later, including complex disturbances, more usual in real electrical system. In both cases noise is added to the signals from 40dB to 20dB. Two different neural networks have been used as classifier algorithm, a backpropagation and probabilistic. A data set with several disturbances, simple and complex, has been generated by simulation software based on electrical models, to test the implemented system. Evaluation results verifying the accuracy of the proposed method are presented.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5636343,no
Effort and Quality of Recovering Requirements-to-Code Traces: Two Exploratory Experiments,"Trace links between requirements and code are essential for many software development and maintenance activities. Despite significant advances in traceability research, creating links remains a human-intensive activity and surprisingly little is known about how humans perform basic tracing tasks. We investigate fundamental research questions regarding the effort and quality of recovering traces between requirements and code. Our paper presents two exploratory experiments conducted with 100 subjects who recovered trace links for two open source software systems in a controlled environment. In the first experiment, subjects recovered trace links between the two systems' requirements and classes of the implementation. In the second experiment, trace links were established between requirements and individual methods of the implementation. In order to assess the validity of the trace links cast by subjects, key developers of the two software systems participated in our research and provided benchmarks. Our study yields surprising observations: trace capture is surprisingly fast and can be done within minutes even for larger classes; the quality of the captured trace links, while good, does not improve with higher trace effort; and it is not harder though slightly more expensive to recover the trace links for larger, more complex classes.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5636536,no
Automated Requirements Traceability: The Study of Human Analysts,"The requirements traceability matrix (RTM) supports many software engineering and software verification and validation (V&V) activities such as change impact analysis, reverse engineering, reuse, and regression testing. The generation of RTMs is tedious and error-prone, though, thus RTMs are often not generated or maintained. Automated techniques have been developed to generate candidate RTMs with some success. When using RTMs to support the V&V of mission-or safety-critical systems, however, a human analyst must vet the candidate RTMs. The focus thus becomes the quality of the final RTM. This paper investigate show human analysts perform when vetting candidate RTMs. Specifically, a study was undertaken at two universities and had 26 participants analyze RTMs of varying accuracy for a Java code formatter program. The study found that humans tend to move their candidate RTM toward the line that represents recall = precision. Participants who examined RTMs with low recall and low precision drastically improved both.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5636537,no
An Experimental Comparison Regarding the Completeness of Functional Requirements Specifications,"Providing high-quality software within budget is a goal pursued by most software companies. Incomplete requirements specifications can have an adverse effect on this goal and thus on a company's competitiveness. Several empirical studies have investigated the effects of requirements engineering methods on the completeness of a specification. In order to increase this body of knowledge, we suggest using an objective evaluation scheme for assessing the completeness of specification documents, as objectifying the term completeness facilitates the interpretation of evaluations and hence comparison among different studies. This paper reports experience from applying the scheme to a student experiment comparing a use case with a textual approach common in industry. The statistical analysis of the specification's completeness indicates that use case descriptions lead to more complete requirements specifications. We further experienced that the scheme is applicable to experiments and delivers meaningful results.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5636630,no
Hiding traces of the blurring effect in digital forgeries,"The confliction between image tampering and digital forensic has persisted for more than a decade. With powerful photo editing software, everyone can easily forge images by Copy-Paste operation. To eliminate the visual edge introduced by tampering, they may employ edge and region smoothing after the contents are manipulated or altered. This process often introduces disharmony between authentic regions and tampering regions. The traces of digital tampering can be detected by estimating the sharpness of the image regions. To remove the fragility of Copy-Paste operation, we proposed a genetic based algorithm to eliminate the blurring in the forgery image. After the processing, the sharpness of the blurred region is recovered and the image quality is preserved.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5636969,no
Assessing the Quality of B Models,"This paper proposes to define and assess the notion of quality of B models aiming at providing an automated feedback on a model by performing systematic checks on its content. We define and classify classes of automatic verification steps that help the modeller in knowing whether his model is well-written or not. This technique is defined in the context of ``behavioral models'' that describe the behavior of a system using the generalized substitutions mechanism. From these models, verification conditions are automatically computed and discharged using a dedicated tool. This technique has been adapted to the B notation, especially on B abstract machines, and implemented within a tool interfaced with a constraint solver that is able to find counter-examples to unvalid verification conditions.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5637412,no
Back-annotation of Simulation Traces with Change-Driven Model Transformations,"Model-driven analysis aims at detecting design flaws early in high-level design models by automatically deriving mathematical models. These analysis models are subsequently investigated by formal verification and validation (V&V) tools, which may retrieve traces violating a certain requirement. Back-annotation aims at mapping back the results of V&V tools to the design model in order to highlight the real source of the fault, to ease making necessary amendments. Here we propose a technique for the back-annotation of simulation traces based on change-driven model transformations. Simulation traces of analysis models will be persisted as a change model with high-level change commands representing macro steps of a trace. This trace is back-annotated to the design model using change-driven transformation rules, which bridge the conceptual differences between macro steps in the analysis and design traces. Our concepts will be demonstrated on the back-annotation problem for analyzing BPEL processes using a Petri net simulator.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5637422,no
A self-hosting configuration management system to mitigate the impact of Radiation-Induced Multi-Bit Upsets in SRAM-based FPGAs,"This paper presents an efficient circuit to mitigate the impact of Radiation-Induced Multi-Bit Upsets in Xilinx FPGAs from Virtex-II on. The proposed internal scrubber detects and corrects single bit upsets and double, triple and quadruple multi bit upsets by efficiently exploiting permuted and compressed Hamming check codes. When implemented using a Xilinx XC2V1000 Virtex-II device, it occupies just 1488 slices and dissipates less than 30 mW at a 50MHz running frequency, taking just 18us to complete the error checking over a single frame, and 18.76us to repair the corrupted frame.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5637493,no
Rapid design optimisation of microwave structures through automated tuning space mapping,"Tuning space mapping (TSM) is one of the latest developments in space mapping technology. TSM algorithms offer a remarkably fast design optimisation with satisfactory results obtained after one or two iterations, which amounts to just a few electromagnetic simulations of the optimised microwave structure. The TSM algorithms (as exemplified by `Type-1` tuning) could be simply implemented manually. The approach may require interaction between various electromagnetic-based and circuit models, as well as handling different sets of design variables and control parameters. As a result, certain TSM algorithms (especially so-called `Type-0` tuning) may be tedious, thus, error-prone to implement. Here, we present a fully automated tuning space mapping implementation that exploits the functionality of our user-friendly space mapping software, the SMF system. The operation and performance of our new implementation is illustrated through the design of a box-section Chebyshev bandpass filter and a capacitively coupled dual-behaviour resonator filter.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5639189,no
Identification of major responding proteins of Abnormal Leaf and Flower in soybean with an integrative omics strategy,"Proteomics has been utilized as an effective approach to bridge the gap between phenotype and genome sequence, however, more effective strategies need to be explored to find target gene(s) from many differentially expressed proteins (DEPs). Here, we utilized an interdisciplinary approach employing a range of methodologies and software tools of genomics, proteomics and metabolomics to identify important responding proteins of the Abnormal Leaf and Flower (ALF) gene involved in soybean leaf and flower development, then to get a global insight into the relevant regulating networks underpinning the alf phenotype. The main results were as follows: (1) A pair of soybean near-isogenic lines (NILs), i.e. NJS-10H-W and NJS-10H-M, differing from ALF locus, was developed with highly consistent genetic background verified by 167 simple sequence repeats (SSR) molecular markers, and an optimized 2-DE procedure was established to separate the whole proteins of leaves of the NILs. Among more than 1000 visualized protein spots, 58 spots presented expression difference, of which 41 proteins were successfully identified by mass spectrometry. The DEPs distributed on all the twenty soybean chromosomes, indicating a complicated regulation network involved in the development of leaf and flower in soybean. (2) The ALF gene was located at the end of the short arm of linkage group C1 (Chromosome 4) by gene mapping method using an F2 population. Three DEPs were also detected in the same region. (3) Ten proteins/genes of DEPs were located in the metabolism pathway by Kyoto Encyclopedia of Genes and Genomes Application Programming Interface (KEGG API), and most of the defects occurred at intersections among carbohydrate, amino acid, energy and cofactors and vitamins metabolism. The Gene Ontology (GO) annotation results of DEPs demonstrated considerable part of proteins as DNA-binding factors, metalloproteases and oxidoreduction enzymes. The GSA (Glutamate-1-Semialdehyde2,1-Aminomutase) and PIN - - (Peptidyl-prolyl cis-trans isomerase) genes were selected as potential candidate genes for ALF locus based on the affluent information from different omics analyses, and the possible regulating profile underpinning the phenome of the mutant was also inferred. In conclusion, some important responding proteins as upstream regulated factors within ALF expression network were identified and marked to the involved pathways for further analysis of the target gene. It showed that combination of omics methods could accelerate the process to isolate new gene(s) and provide potential information for further study on genes and proteins regulatory network.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5639832,no
Effects of Internal Hole-Defects Location on the Propagation Parameters of Acoustic Wave in the Maple Wood,"To detect the effects of internal hole-defects location on the propagation parameters of acoustic wave in the wood, forty maple wood samples used as the study objects are tested by using PLG (Portable Lumber Grader) instrument in this paper. The propagation velocity and vibration frequency of acoustic wave in intact wood and defective wood were compared, and then the correlation between the propagation velocity or vibration frequency and the elastic modulus were discussed respectively. The analysis results showed that: (1) there were significant positive correlations between the propagation velocity or vibration frequency of acoustic wave and the elastic modulus of the intact and defective maple wood samples; (2) the propagation velocity and vibration frequency of acoustic wave in defective wood samples were lower than those of intact wood samples; and (3) the changes of acoustic wave propagation parameters were different when the location of internal hole-defects of wood samples were different.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5640257,no
A clustering algorithm for software fault prediction,"Software metrics are used for predicting whether modules of software project are faulty or fault free. Timely prediction of faults especially accuracy or computation faults improve software quality and hence its reliability. As we can apply various distance measures on traditional K-means clustering algorithm to predict faulty or fault free modules. Here, in this paper we have proposed K-Sorensen-means clustering that uses Sorensen distance for calculating cluster distance to predict faults in software projects. Proposed algorithm is then trained and tested using three datasets namely, JM1, PCI and CM1 collected from NASA MDP. From these three datasets requirement metrics, static code metrics and alliance metrics (combining both requirement metrics and static code metrics) have been built and then K-Sorensen-means applied on all datasets to predict results. Alliance metric model is found to be the best prediction model among three models. Results of K-Sorensen-means clustering shown and corresponding ROC curve has been drawn. Results of K-Sorensen-means are then compared with K-Canberra-means clustering that uses other distance measure for evaluating cluster distance.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5640474,no
Model-based diagnosis of induction motor failure modes,"Induction motor failure modes can be instantaneous or progressive. This paper proposes a model-based methodology employed to attempt to identify the root causes of the main fault modes of progressive failures. Utilising indicators from the monitoring of three-phase motor currents, this paper explains how leading-edge and traditional theories could be combined and optimised to provide an integrated software algorithm set that could accurately predict the root causes of the five main failure modes of standard cage type Induction Motors. Utilising mathematical modelling and simulation of each specific fault mode, comparisons can be made between the known symptoms of specific faults and Induction Motor signals obtained from devices operating at full load in the field. Comparison of variances between model states and the state of the operating machine could enable the specific root cause diagnosis to be made.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5641297,no
A stochastic model for performance evaluation and bottleneck discovering on SOA-based systems,"The Service-Oriented Architecture (SOA) has become a unifying technical architecture that may be embodied through Web Service technologies. Predicting the variable behavior of SOA systems can mean a way to improve the quality of the business transactions. This paper proposes a simulation modeling approach based on stochastic Petri nets to estimate the performance of SOA-applications. Using the proposed model is possible to predict resource consumption and service levels degradation in scenarios with different compositions and workloads, even before developing the application. A case study was conducted in order to validate our approach, comparing its accuracy with the results from an analytical model existent in the literature.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5641802,no
Software reliability analysis with optimal release problems based on hazard rate model for an embedded OSS,"An OSS (open source software) system is frequently applied as server use, instead of client use. Especially, embedded OSS systems have been gaining a lot of attention in the embedded system area, i.e., Android, BusyBox, TRON, etc. However, the poor handling of quality problem and customer support prohibit the progress of embedded OSS. Also, it is difficult for developers to assess the reliability and portability of embedded OSS on a single-board computer. We focus on software quality/reliability problems that can prohibit the progress of embedded OSS. In this paper, we propose a method of software reliability assessment based on a hazard rate model for the embedded OSS. In particular, we derive several assessment measures from the model. Also, we analyze actual software failure-occurrence time-interval data to show numerical examples of software reliability assessment for the embedded OSS. Moreover, we discuss the optimal software release problem for the porting-phase based on the total expected software maintenance cost.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5641839,no
Dynamic parameter control of interactive local search in UML software design,"User-centered Interactive Evolutionary Computation (IEC) has been applied to a wide variety of areas, including UML software design. The performance of evolutionary search is important as user interaction fatigue remains an on-going challenge in IEC. However, to obtain optimal search performance, it is usually necessary to tune evolutionary control parameters manually, although tuning control parameters can be time-consuming and error-prone. To address this issue in other fields of evolutionary computation, dynamic parameter control including deterministic, adaptive and self-adaptive mechanisms have been applied extensively to real-valued representations. This paper postulates that dynamic parameter control may be highly beneficial to IEC in general, and UML software design in particular, wherein a novel object-based solution representation is used. Three software design problems from differing design domains and of differing scale have been investigated with mutation probabilities modified by simulated annealing, the Rechenberg 1/5 success rule and self-adaptation within local search. Results indicate that self-adaptation appears to be the most robust and scalable mutation probability modification mechanism. The use of self-adaption with an object-based representation is novel, and results indicate that dynamic parameter control offers great potential within IEC.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5642479,no
An integrated approach to Design for Quality (DfQ) in the high value added printed circuit assembly (PCA) manufacturing: A pilot tool,"High value added electronics manufacturing is a challenging sector that requires compliance with demanding quality standards. Tools and methods to support Design for Quality (DfQ) are limited within the domain. In this paper a software toolkit to support (DfQ) is proposed based on the underlying concept of integrated modelling. Based on this principle a simulation module to predict quality in terms of manufacturing defects and a Root Cause Analysis (RCA) module to support their elimination have been developed. The focus of this paper is on the latter RCA module. After a description of the integrated modelling concept and the software toolkit, a case study is presented to demonstrate the value of the software. The results illustrate possible roles of the quality support toolkit in solving real quality problems in printed circuit assemblies (PCA's) manufacturing. The results include measures of product quality improvements and savings in terms of time and cost.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5642990,no
Reverse Engineering Utility Functions Using Genetic Programming to Detect Anomalous Behavior in Software,"Recent studies have shown the promise of using utility functions to detect anomalous behavior in software systems at runtime. However, it remains a challenge for software engineers to hand-craft a utility function that achieves both a high precision (i.e., few false alarms) and a high recall (i.e., few undetected faults). This paper describes a technique that uses genetic programming to automatically evolve a utility function for a specific system, set of resource usage metrics, and precision/recall preference. These metrics are computed using sensor values that monitor a variety of system resources (e.g., memory usage, processor usage, thread count). The technique allows users to specify the relative importance of precision and recall, and builds a utility function to meet those requirements. We evaluated the technique on the open source Jigsaw web server using ten resource usage metrics and five anomalous behaviors in the form of injected faults in the Jigsaw code and a security attack. To assess the effectiveness of the technique, the precision and recall of the evolved utility function was compared to that of a hand-crafted utility function that uses a simple thresholding scheme. The results show that the evolved function outperformed the hand-crafted function by 10 percent.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5645446,no
Reverse Engineering Self-Modifying Code: Unpacker Extraction,"An important application of binary-level reverse engineering is in reconstructing the internal logic of computer malware. Most malware code is distributed in encrypted (or """"packed"""") form, at runtime, an unpacker routine transforms this to the original executable form of the code, which is then executed. Most of the existing work on analysis of such programs focuses on detecting unpacking and extracting the unpacked code. However, this does not shed any light on the functionality of different portions of the code so obtained, and in particular does not distinguish between code that performs unpacking and code that does not, identifying such functionality can be helpful for reverse engineering the code. This paper describes a technique for identifying and extracting the unpacker code in a self-modifying program. Our algorithm uses offline analysis of a dynamic instruction trace both to identify the point(s) where unpacking occurs and to identify and extract the corresponding unpacker code.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5645447,no
"Design, Modeling, and Evaluation of a Scalable Multi-level Checkpointing System","High-performance computing (HPC) systems are growing more powerful by utilizing more hardware components. As the system mean-time-before-failure correspondingly drops, applications must checkpoint more frequently to make progress. However, as the system memory sizes grow faster than the bandwidth to the parallel file system, the cost of checkpointing begins to dominate application run times. Multi-level checkpointing potentially solves this problem through multiple types of checkpoints with different costs and different levels of resiliency in a single run. This solution employs lightweight checkpoints to handle the most common failure modes and relies on more expensive checkpoints for less common, but more severe failures. This theoretically promising approach has not been fully evaluated in a large- scale, production system context. We have designed the Scalable Checkpoint/Restart (SCR) library, a multi-level checkpoint system that writes checkpoints to RAM, Flash, or disk on the compute nodes in addition to the parallel file system. We present the performance and reliability properties of SCR as well as a probabilistic Markov model that predicts its performance on current and future systems. We show that multi-level checkpointing improves efficiency on existing large-scale systems and that this benefit increases as the system size grows. In particular, we developed low-cost checkpoint schemes that are 100x-1000x faster than the parallel file system and effective against 85% of our system failures. This leads to a gain in machine efficiency of up to 35%, and it reduces the the load on the parallel file system by a factor of two on current and future systems.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5645453,no
Linguistic Driven Refactoring of Source Code Identifiers,"Identifiers are an important source of information during program understanding and maintenance. Programmers often use identifiers to build their mental models of the software artifacts. We have performed a preliminary study to examine the relation between the terms in identifiers, their spread in entities, and fault proneness. We introduced term entropy and context-coverage to measure how scattered terms are across program entities and how unrelated are the methods and attributes containing these terms. Our results showed that methods and attributes containing terms with high entropy and context-coverage are more fault-prone. We plan to build on this study by extracting linguistic information form methods and classes. Using this information, we plan to establish traceability link from domain concepts to source code, and to propose linguistic based refactoring.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5645489,no
Predicting Re-opened Bugs: A Case Study on the Eclipse Project,"Bug fixing accounts for a large amount of the software maintenance resources. Generally, bugs are reported, fixed, verified and closed. However, in some cases bugs have to be re-opened. Re-opened bugs increase maintenance costs, degrade the overall user-perceived quality of the software and lead to unnecessary rework by busy practitioners. In this paper, we study and predict re-opened bugs through a case study on the Eclipse project. We structure our study along 4 dimensions: (1) the work habits dimension (e.g., the weekday on which the bug was initially closed on), (2) the bug report dimension (e.g., the component in which the bug was found) (3) the bug fix dimension (e.g., the amount of time it took to perform the initial fix) and (4) the team dimension (e.g., the experience of the bug fixer). Our case study on the Eclipse Platform 3.0 project shows that the comment and description text, the time it took to fix the bug, and the component the bug was found in are the most important factors in determining whether a bug will be re-opened. Based on these dimensions we create decision trees that predict whether a bug will be re-opened after its closure. Using a combination of our dimensions, we can build explainable prediction models that can achieve 62.9% precision and 84.5% recall when predicting whether a bug will be re-opened.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5645566,no
Accuracy of automatic speaker recognition for telephone speech signal quality,"This paper was performed by examining the accuracy of speaker identification on telephone quality voice signals. Speaker recognizer was implemented using HTK. Influence of the considered telephone channels on transmitted voice signal is seen through its basic characteristics, types of the applied codecs and the effects caused by the condition of the transmission channel. These effects were observed by a factor of transmission error probability, while the VoIP telephone channels were analyzed and the appearance of echo. Simulation of the appropriate codecs and the probability of various errors made during transmission by using publicly available library of software tools, ITU-T STL2005, while the echo phenomenon was simulated using effect Delay / Echo-Simple suite Sony Sound Forge 9.0.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5647155,no
Bearing fault detection based on order bispectrum,"In order to process the non-stationary vibration signals such as speed up or speed down vibration signals effectively, the order bispectrum analysis technique is presented. This new method combines computed order tracking technique with bispectrum analysis. Firstly, the vibration signal is sampled at constant time increments and then uses software to resample the data at constant angle increments. Therefore, the time domain transient signal is converted into angle domain stationary one. In the end, the resampled signals are processed by bispectrum analysis technology. The experimental results show that order bispectrum analysis can effectively detect the bearing fault.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5647398,no
Energy optimal on-line Self-Test of microprocessors in WSN nodes,"Wireless Sensor Network (WSN) applications often need to be deployed in harsh environments, where the possibility of faults due to environmental hazards is significantly increased, while silicon aging and wearout effects are also exacerbated. For such applications, periodic on-line testing of the WSN nodes is an important step towards correctness of operation. However, on-line testing of processors integrated in WSN nodes has to address the additional challenge of minimum energy consumption, because these devices operate on battery, which usually cannot be replaced and in the absence of catastrophic failures determines the lifetime of the system. In this paper initially we derive analytically the optimal way for executing on-line periodic test with adjustable period, taking into account the degrading behavior of the system due to silicon aging effects but also the limited energy budget of WSN applications. The test is applied in the form of Software-Based Self-Test (SBST) routines, thus we proceed to the power optimized development of SBST routines targeting the transition delay fault model that is well suited for detecting timing violations due to silicon aging. Simulation results show that energy savings for the final SBST routine at processor level are up to 35.4% and the impact of test in the battery life of the system is negligible.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5647693,no
A simple hybrid image segmentation method for embedded visual display devices,"Image segmentation plays a major role in computer vision. It is a fundamental task for feature extraction and pattern matching applications. This paper proposes a simple hybrid Image segmentation method which is mainly based on mathematical morphological operations and filtering techniques. The main aim of the proposed hybrid segmentation method is to segment the foreground object in the given image and mark the segmented region with precision. The purpose of developing this method is to identify a prominent single object based photographs automatically in real time. Also the algorithm must work for worst cases (fog, mist, blur, noise etc). This requirement needs a precise segmentation approach which must be computationally less costly and easy to implement with better quality of segmenting the object as Region of interest. The images are at first subjected to Gaussian filtering to make the image smooth for segmentation. Later, applying Sobel edge detection algorithm to detect the edges properly and then applying morphological operations logically arranged in a novel way for morphological image cleaning purposes. In the final stage, the object of interest is segmented and marked which proves the efficiency of the proposed hybrid image segmentation algorithm. Furthermore, the proposed hybrid image segmentation algorithm is implemented in MATLAB (version 7.0 on an Intel P-4 dual core) and evaluated on 350 jpeg images with satisfactory results. The images sizes which were tested are 384 * 288, 480 * 320, 640 *480, 720 * 480, 800 * 600, 912 *608, 912 * 684, 1024 *768 and 1600 *1200.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5647697,no
Inter-frame error concealment using graph cut technique for video transmission,"Due to channel noise and congestion, video data packets can be lost during transmission in error-prone networks, which severely affects the quality of received video sequences. The conventional inter-frame error concealment (EC) methods estimate a motion vector (MV) for a corrupted block or reconstruct the corrupted pixel values using spatial and temporal weighted interpolation, which may result in boundary discontinuity and blurring artifacts of the reconstructed region. In this paper, we reconstruct corrupted macroblock (MB) by predicting sub-partitions and synthesizing the corrupted MB to reduce boundary discontinuity and avoid blurring artifacts. First, we select the optimal MV for each neighboring boundary using minimum side match distortion from a candidate MV set, and then we calculate the optimal cut path between the overlapping regions to synthesize the corrupted MB. The simulation results show that our proposed method is able to achieve significantly higher PSNR as well as better visual quality than using the H.264/AVC reference software.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5648292,no
Lightning overvoltages on an overhead transmission line during backflashover and shielding failure,Analysis of induced voltages on transmission tower and conductors has been performed when a high voltage line is subjected to propagation of lightning transient. PSCAD/EMTDC software program is used to carry out the modelling and simulation works. Lightning strikes on the tower top or conductors result in large overvoltages appearing between the tower and the conductors. Two cases considered for these effects are: (i) direct strike to a shield wire or tower top; (ii) shielding failure. The probability of a lightning strike terminating on a shield wire or tower top is higher than that of a phase conductor. Voltages produced during shielding failure on conductors are more significant than during back flashovers. The severity of the induced voltages from single stroke and multiple stroke lightning is illustrated using the simulation results. The results demonstrate high magnitude of induced voltages by the multiple stroke lightning compared to those by single strokes. Analytical studies were performed to verify the results obtained from the simulation. Analysis of the performance of the line using IEEE Flash version 1.81 computer programme was also carried out.,2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5648866,no
Implementation of finite mutual impedances and its influence on earth potential rise estimation along transmission lines,"As the proximity of high fault current power lines to residential areas increases, the need for accurate prediction of earth potential rise (EPR) is of crucial importance for both safety and equipment protection. To date, the most accurate methods for predicting EPR are power system modelling software tools, such as EMTP, or recursive methods that use a span by span approach to model a transmission line. These techniques are generally used in conjunction with impedances and admittances that are derived from the assumption of infinite line length. In this paper a span by span model was created to predict the EPR along a dual circuit transmission line in EMTP-RV, where the mutual impedances were considered to be between finite length conductors. A series of current injection tests were also performed on the system under study in order to establish the accuracy of both the finite and infinite methods.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5650125,no
"Disturbance detection, identification, and recovery by gait transition in legged robots","We present a framework for detecting, identifying, and recovering within stride from faults and other leg contact disturbances encountered by a walking hexapedal robot. Detection is achieved by means of a software contact-event sensor with no additional sensing hardware beyond the commercial actuators' standard shaft encoders. A simple finite state machine identifies disturbances as due either to an expected ground contact, a missing ground contact indicating leg fault, or an unexpected wall contact. Recovery proceeds as necessary by means of a recently developed topological gait transition coordinator. We demonstrate the efficacy of this system by presenting preliminary data arising from two reactive behaviors - wall avoidance and leg-break recovery. We believe that extensions of this framework will enable reactive behaviors allowing the robot to function with guarded autonomy under widely varying terrain and self-health conditions.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5651061,no
Wooded hedgerows characterization in rural landscape using very high spatial resolution satellite images,"The objective of this study is to evaluate the very high spatial resolution satellite images capacity to detect and characterize the hedgerows network. The significant qualitative attributes to characterize hedgerows are composition, morphology and spatial arrangement of small elements. Remote sensing images Spot 5 and Kompsat, respectively 5m and 1m spatial resolution, were used. We applied an object-based image analysis method. The first step consists on a multi-scale segmentation, and the second step consists on a multi-criterion classification. Then, the characterization consists on a combination of shape values on the fields boundaries. Results shows that both Spot 5 and Kompsat image allows to detect automatically with a good precision the hedgerow network (84.5% and 97% respectively). Only the Kompsat image is enable to detect the finest elements of hedgerows. The very high spatial resolution image, less than 1m, allows to characterize the hedgerow cover quality, the continuity and discontinuity. This study highlights an efficient, reliable and generic method. Moreover, this characterization of landscape structures elements allows to affine the knowledge of ecological elements.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5651636,no
Submerged aquatic vegetation habitat product development: On-screen digitizing and spatial analysis of Core Sound,"A hydrophyte of high relevance, submerged aquatic vegetation (SAV) is of great importance to estuarine environments. SAV helps improve water quality, provides food and shelter for waterfowl, fish, and shellfish, as well as protects shorelines from erosion. In coastal bays most SAV was eliminated by disease in the 1930's. In the late 1960's and 1970's a dramatic decline of all SAV species was correlated with increasing nutrient and sediment inputs from development of surrounding watersheds (MDNP et. al 2004). Currently state programs work to protect and restore existing wetlands, however, increasing development and population pressure continue to degrade and destroy both tidal and non-tidal wetlands and hinder overall development of SAV growth. The focus of this research was to utilize spatial referencing software in the mapping of healthy submerged aquatic vegetation (SAV) habitats. In cooperation with the United States Fish and Wildlife Service (USFWS), and the National Oceanic and Atmospheric Administration (NOAA), students from Elizabeth City State University (ECSU) developed and applied Geographic Information Systems (GIS) skills to evaluate the distribution and abundance of SAV in North Carolina's estuarine environments. Utilizing ESRI ArcView, which includes ArcMap, ArcCatalog and ArcToolbox, and the applications of on-screen digitizing, an assessment of vegetation cover was made through the delineation of observable SAV beds in Core Sound, North Carolina. Aerial photography of the identified coastal water bodies was taken at 12,000 feet above mean terrain (AMT) scale 1:24,000. The georeferenced aerial photographs were assessed for obscurities and the SAV beds were digitized. Through the adoption of NOAA guidelines and criteria for benthic habitat mapping using aerial photography for image acquisition and analysis, students delineated SAV beds and developed a GIS spatial database relevant to desired results. This newly created database yielded products in - - the form of usable shapefiles of SAV polygons as well as attribute information with location information, area in hectares, and percent coverage of SAV.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5651718,no
Evaluation of the VIIRS Land algorithms at Land PEATE,"The Land Product Evaluation and Algorithm Testing Element (Land PEATE), a component of the Science Data Segment of the National Polar-orbiting Operational Environmental Satellite System (NPOESS) Preparatory Project (NPP), is being developed at the NASA Goddard Space Flight Center (GSFC). The primary task of the Land PEATE is to assess the quality of the Visible Infrared Imaging Radiometer Suite (VIIRS) Land data products made by the Interface Data Processing System (IDPS) using the Operational (OPS) Code during the NPP era and to recommend improvements to the algorithms in the IDPS OPS code. The Land PEATE uses a version of the MODIS Adaptive Processing System (MODAPS), NPPDAPS, that has been modified to produce products from the IDPS OPS code and software provided by the VIIRS Science Team, and uses the MODIS Land Data Operational Product Evaluation (LDOPE) team for evaluation of the data records generated by the NPPDAPS. Land PEATE evaluates the algorithms by comparing data products generated using different versions of the algorithm and also by comparing to heritage products generated from different instrument such as MODIS using various quality assessment tools developed at LDOPE. This paper describes the Land PEATE system and some of the approaches used by the Land PEATE for evaluating the VIIRS Land algorithms during the pre-launch period of the NPP mission and the proposed plan for long term monitoring of the quality of the VIIRS Land products post-launch.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5652831,no
Application-Aware diagnosis of runtime hardware faults,"Extreme technology scaling in silicon devices drastically affects reliability, particularly because of runtime failures induced by transistor wearout. Current online testing mechanisms focus on testing all components in a microprocessor, including hardware that has not been exercised, and thus have high performance penalties. We propose a hybrid hardware/software online testing solution where components that are heavily utilized by the software application are tested more thoroughly and frequently. Thus, our online testing approach focuses on the processor units that affect application correctness the most, and it achieves high coverage while incurring minimal performance overhead. We also introduce a new metric, Application-Aware Fault Coverage, measuring a test's capability to detect faults that might have corrupted the state or the output of an application. Test coverage is further improved through the insertion of observation points that augment the coverage of the testing system. By evaluating our technique on a Sun OpenSPARC T1, we show that our solution maintains high Application-Aware Fault Coverage while reducing the performance overhead of online testing by more than a factor of 2 when compared to solutions oblivious to application's behavior. Specifically, we found that our solution can achieve 95% fault coverage while maintaining a minimal performance overhead (1.3%) and area impact (0.4%).",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5653788,no
SETS: Stochastic execution time scheduling for multicore systems by joint state space and Monte Carlo,"The advent of multicore platforms has renewed the interest in scheduling techniques for real-time systems. Historically, `scheduling decisions' are implemented considering fixed task execution times, as for the case of Worst Case Execution Time (WCET). The limitations of scheduling considering WCET manifest in terms of under-utilization of resources for large application classes. In the realm of multicore systems, the notion of WCET is hardly meaningful due to the large set of factors influencing it. Within soft real-time systems, a more realistic modeling approach would be to consider tasks featuring varying execution times (i.e. stochastic). This paper addresses the problem of stochastic task execution time scheduling that is agnostic to statistical properties of the execution time. Our proposed method is orthogonal to any number of linear acyclic task graphs and their underlying architecture. The joint estimation of execution time and the associated parameters, relying on the interdependence of parallel tasks, help build a `nonlinear Non-Gaussian state space' model. To obtain nearly Bayesian estimates, irrespective of the execution time characteristics, a recursive solution of the state space model is found by means of the Monte Carlo method. The recursive solution reduces the computational and memory overhead and adapts statistical properties of execution times at run time. Finally, the variable laxity EDF scheduler schedules the tasks considering the predicted execution times. We show that variable execution time scheduling improves the utilization of resources and ensures the quality of service. Our proposed new solution does not require any a priori knowledge of any kind and eliminates the fundamental constraints associated with the estimation of execution times. Results clearly show the advantage of the proposed method as it achieves 76% better task utilization, 68% more task scheduling and deadline miss reduction by 53% compared to current state-of-the-ar- - t methods.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5654114,no
Definition and Validation of Metrics for ITSM Process Models,"Process metrics can be used to establish baselines, to predict the effort required to go from an as-is to a to-be scenario or to pinpoint problematic ITSM process models. Several metrics proposed in the literature for business process models can be used for ITSM process models as well. This paper formalizes some of those metrics and proposes some new ones, using the Metamodel-Driven Measurement (M2DM) approach that provides precision, objectiveness and automatic collection. According to that approach, metrics were specified with the Object Constraint Language (OCL), upon a lightweight BPMN metamodel that is briefly described. That metamodel was instantiated with a case study consisting of two ITSM processes with two scenarios (as-is and to-be) each. Values collected automatically by executing the OCL metrics definitions, upon the instantiated metamodel, are presented. Using a larger sample with several thousand meta-instances, we analyzed the collinearity of the formalized metrics and were able to identify a smaller set, which will be used to perform further research work on the complexity of ITSM processes.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5654787,no
Requirements Certification for Offshoring Using LSPCM,"Requirements hand-over is a common practice in software development off shoring. Cultural and geographical distance between the outsourcer and supplier, and the differences in development practices hinder the communication and lead to the misinterpretation of the original set of requirements. In this article we advocate requirements quality certification using LSPCM as a prerequisite for requirements hand-over. LSPCM stands for LaQuSo Software Product Certification Model that can be applied by non-experienced IT assessors to verify software artifacts in order to contribute to the successfulness of the project. To support our claim we have analyzed requirements of three off shoring projects using LSPCM. Application of LSPCM revealed severe flaws in one of the projects. The responsible project leader confirmed later that the development significantly exceeded time and budget. In the other project no major flaws were detected by LSPCM and it was confirmed that the implementation was delivered within time and budget. Application of LSPCM to the projects above also allowed us to refine the model for requirements hand-over in software development off shoring.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5655261,no
Managing Risk in Decision to Outsource IT Projects,"Organizations all around the world are increasingly adopting the activity of outsourcing their IT function to service providers. However, the decision to outsource IT project is not an easy task. The risk will mostly affect the organizations as opposed to service provider. Therefore, it is important for the managers to manage this activity. This paper presents how organizations manage their decision to outsource IT projects. Mixed method was used to gather the information regarding current practices. The analysis revealed that some organizations did not have structured process to come up with the right decision to outsource. In the other hand, some of them assessed the risk associated with the decision to outsource. The analysis of the findings was then used as a basis of the proposed framework of Risk Management in Decision to Outsource IT Project. The proposed framework can act as a guideline to help organizations in making the decision to outsource as well as assessing and managing risk associated with the decision to outsource IT project.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5655266,no
Model-driven development of ARINC 653 configuration tables,"Model-driven development (MDD) has become a key technique in systems and software engineering, including the aeronautic domain. It facilitates on systematic use of models from a very early phase of the design process and through various model transformation steps (semi-)automatically generates source code and documentation. However, on one hand, the use of model-driven approaches for the development of configuration data is not as widely used as for source code synthesis. On the other hand, we believe that, particular systems that make heavy use of configuration tables like the ARINC 653 standard can benefit from model-driven design by (i) automating error-prone configuration file editing and (ii) using model based validation for early error detection. In this paper, we will present the results of the European project DIANA that investigated the use of MDD in the context of Integrated Modular Avionics (IMA) and the ARINC 653 standard. In the scope of the project, a tool chain was implemented that generates ARINC 653 configuration tables from high-level architecture models. The tool chain was integrated with different target systems (VxWorks 653, SIMA) and evaluated during case studies with real-world and real-sized avionics applications.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5655322,no
Conveying Conceptions of Quality through Instruction,"Building up an understanding of aspects of quality, and how to critically assess them, is a complex problem. This paper provides an overview of research on student conceptions of what constitutes quality in different programming domains. These conceptions are linked to tertiary education and computing education research results. Using this literature as a background we discuss how to develop and use instructional approaches that might assist students in developing a better understanding of software quality.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5655415,no
Model-driven development of ARINC 653 configuration tables,"Model-driven development (MDD) has become a key technique in systems and software engineering, including the aeronautic domain. It facilitates on systematic use of models from a very early phase of the design process and through various model transformation steps (semi-)automatically generates source code and documentation. However, on one hand, the use of model-driven approaches for the development of configuration data is not as widely used as for source code synthesis. On the other hand, we believe that, particular systems that make heavy use of configuration tables like the ARINC 653 standard can benefit from model-driven design by (i) automating error-prone configuration file editing and (ii) using model based validation for early error detection. In this paper, we will present the results of the European project DIANA that investigated the use of MDD in the context of Integrated Modular Avionics (IMA) and the ARINC 653 standard. In the scope of the project, a tool chain was implemented that generates ARINC 653 configuration tables from high-level architecture models. The tool chain was integrated with different target systems (VxWorks 653, SIMA) and evaluated during case studies with real-world and real-sized avionics applications.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5655451,no
Do Testers' Preferences Have an Impact on Effectiveness?,"Both verification and validation aim to improve the quality of software products during the development process. They use techniques like formal methods, symbolic execution, formal reviews, testing techniques, etc. Technique effectiveness depends not only on project size and complexity but also on the experience of the subject responsible for testing. We have looked at whether the opinions and preferences of subjects match the number of detected defects. Opinions and preferences can influence the decisions that testers have to make. In this paper, we present a piece of research that has explored this aspect by comparing the opinions of subjects (qualitative aspects) with the quantitative results. To do this, we use qualitative methods applied to a quantitative study of code evaluation technique effectiveness.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5655565,no
A Tool for Automatic Defect Detection in Models Used in Model-Driven Engineering,"In the Model-Driven Engineering (MDE) field, the quality assurance of the involved models is fundamental for performing correct model transformations and generating final software applications. To evaluate the quality of models, defect detection is usually performed by means of reading techniques that are manually applied. Thus, new approaches to automate the defect detection in models are needed. To fulfill this need, this paper presents a tool that implements a novel approach for automatic defect detection, which is based on a model-based functional size measurement procedure. This tool detects defects related to the correctness and the consistency of the models. Thus, our contribution lays in the new approach presented and its automation for the detection of defects in MDE environments.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5655578,no
Analyzing the Similarity among Software Projects to Improve Software Project Monitoring Processes,"Software project monitoring and control is crucial to detect deviation considering the project plan and to take appropriate actions, when needed. However, to determine which action should be taken is not an easy task, since project managers have to analyze the context of the deviation event and search for actions that were successfully taken on previous similar contexts, trying to repeat the effectiveness of these actions. To do so, usually managers use previous projects data or their own experience, and frequently there is no measure or similarity criteria formally established. Thus, in this paper we present the results of a survey that aimed to identify characteristics that can determine the similarity among software projects and also a measure to indicate the level of similarity among them. A recommendation system to support the execution of corrective actions based on previous projects is also described. We believe these results can support the improvement of software project monitoring process, providing important knowledge to project managers in order to improve monitoring and control activities on the projects.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5655652,no
A Gap Analysis Methodology for the Team Software Process,"Over the years software quality is becoming more and more important in software engineering. Like in other engineering disciplines where quality is already a commodity, software engineering is moving into these stages. The Team Software Process (TSP) was created by the Software Engineering Institute (SEI) with the main objective of helping software engineers and teams to ensure high-quality software products and improve process management in the organization. This paper presents a methodology for assessing an organization against the TSP practices so that it is possible to assess the future gains and needs an organization will have during and after the implementation of TSP. The gap analysis methodology has two pillars in terms of data collection: interviews and documentation analysis. Questionnaires have been developed to guide the assessment team on the task of conducting interviews and further guidance has been developed in what and where to look for information in an organization. A model for the rating has also been developed based on the knowledge and experience of working in several organizations on software quality. A report template was also created for documenting the analysis conclusions. The methodology developed was successfully applied in one well known Portuguese organization with the support and validation of SEI, and several refinements were introduced based on the lessons learnt. It is based on the most know reference models and standards for software process assessment - Capability Maturity Model Integration (CMMI) and ISO/IEC 15504. The objective of this methodology is to be fast and inexpensive when compared with those models and standards or with the SEI TSP assessment pilot.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5655653,no
Classification and Comparison of Agile Methods,"This manuscript describes a technique and its tool support to perform comparisons on agile methods, based on a set of relevant features and attributes. This set includes attributes related to four IEEE's Software Engineering Body of Knowledge (SWEBOK) Knowledge Areas (KAs) and to the agile principles defined in the Agile Manifesto. With this set of attributes, by analysing the practices proposed by each method, we are able to assess (1) the coverage degree for the considered KAs and (2) the agility degree. In this manuscript, the application of the technique is exemplified in comparing extreme Programming (XP) and Scrum.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5655659,no
A Method for Continuous Code Quality Management Using Static Analysis,The quality of source code is a key factor for any software product and its continuous monitoring is an indispensable task for a software development project. We have developed a method for systematic assessing and improving the code quality of ongoing projects by using the results of various static code analysis tools. With different approaches for monitoring the quality (a trend-based one and a benchmarking-based one) and an according tool support we are able to manage the large amount of data that is generated by these static analyses. First experiences when applying the method with software projects in practice have shown the feasibility of our method.,2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5655663,no
Towards Automated Quality Models for Software Development Communities: The QualOSS and FLOSSMetrics Case,"Quality models for software products and processes help both to developers and users to better understand their characteristics. In the specific case of libre (free, open source) software, the availability of a mature and reliable development community is an important factor to be considered, since in most cases both the evolvability and future fitness of the product depends on it. Up to now, most of the quality models for communities have been based on the manual examination by experts, which is time-consuming, generally inconsistent and often error-prone. In this paper, we propose a methodology, and some examples of how it works in practice, of how a quality model for development communities can be automated. The quality model used is a part of the QualOSS quality model, while the metrics are those collected by the FLOSS Metrics project.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5655666,no
Reducing Subjectivity in Code Smells Detection: Experimenting with the Long Method,"Guidelines for refactoring are meant to improve software systems internal quality and are widely acknowledged as among software's best practices. However, such guidelines remain mostly qualitative in nature. As a result, judgments on how to conduct refactoring processes remain mostly subjective and therefore non-automatable, prone to errors and unrepeatable. The detection of the Long Method code smell is an example. To address this problem, this paper proposes a technique to detect Long Method objectively and automatically, using a Binary Logistic Regression model calibrated by expert's knowledge. The results of an experiment illustrating the use of this technique are reported.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5655669,no
IDS: An Immune-Inspired Approach for the Detection of Software Design Smells,"We propose a parallel between object-oriented system designs and living creatures. We suggest that, like any living creature, system designs are subject to diseases, which are design smells (code smells and anti patterns). Design smells are conjectured in the literature to impact the quality and life of systems and, therefore, their detection has drawn the attention of both researchers and practitioners with various approaches. With our parallel, we propose a novel approach built on models of the immune system responses to pathogenic material. We show that our approach can detect more than one smell at a time. We build and test our approach on Gantt Project v1.10.2 and Xerces v2.7.0, for which manually-validated and publicly available smells exist. The results show a significant improvement in detection time, precision, and recall, in comparison to the state-of-the-art approaches.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5655670,no
Study of LEO satellite constellation systems based on quantum communications networks,"Quantum cryptography, or more specifically quantum key distribution (QKD), is the first offspring of quantum information that has reached the stage of real-world application. Its security is based on the fact that a possible spy (Eve, the eavesdropper) cannot obtain information about the bits that Alice sends to Bob, without introducing perturbations. Therefore authorized partners can detect the spy by estimating the amount of error in their lists. The central objective of this paper is to implement and improve practical systems for quantum cryptography. The essential work carried in our research laboratory concerns the software development to implement of Quantum Key Distribution (QKD) Network based on LEO orbit number and reduce the telecommunication interruption risks and this will provide indeed a better communication quality, and investigations into the causes of losses in the system and attempts to minimize the quantum bit error rate (QBER).",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5656276,no
Rapid prototyping and compact testing of CPU emulators,"In this paper, we propose a novel rapid prototyping technique to produce a high quality CPU emulator at reduced development cost. Specification mining from published CPU manuals, automated code generation of both the emulator and its test vectors from the mined CPU specifications, and a hardware-oracle based test strategy all work together to close the gaps between specification analysis, development and testing. The hardware-oracle is a program which allows controlled execution of one or more instructions on the CPU, so that its outputs can be compared to that of the emulator. The hardware-oracle eliminates any guesswork about the true behavior of an actual CPU, and it helps in the identification of several discrepancies between the published specifications vs. the actual processor behavior, which would be very hard to detect otherwise.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5656339,no
A Metrics-Based Approach to Technical Documentation Quality,"Technical documentation is now fully taking the step from stale printed booklets (or electronic versions of these) to interactive and online versions. This provides opportunities to reconsider how we define and assess the quality of technical documentation. This paper suggests an approach based on the Goal-Question-Metric paradigm: predefined quality goals are continuously assessed and visualized by the use of metrics. To test this approach, we perform two experiments. We adopt well known software analysis techniques, e.g., clone detection and test coverage analysis, and assess the quality of two real world documentations, that of a mobile phone and of (parts of) a warship. The experiments show that quality issues can be identified and that the approach is promising.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5656407,no
Algorithm for QOS-aware web service composition based on flow path tree with probabilities,"This paper first presents a novel model for QoS-aware web services composition based on workflow patterns, and an executing path tree with probability. Then a discrete PSO algorithm is proposed to fit our model, which also requires some other preliminary algorithms, such as the generation of all reachable paths and executing path tree. The experiments show the performance of that algorithm and its advantages, compared with genetic algorithm. Finally, we suggest some drawbacks of it and future works.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5657934,no
An Enhanced Prediction Method for All-Zero Block in H.264,"During the process of H.264 encoding, after the operation of quantization, it will generate a lot of all-zero blocks. In this paper we analyses the feature of the coefficients of the 44 residual block after transformation, on that basis a new algorithm for predicting all-zero block is proposed. And then, we utilize the sum of absolute residual(SAD)of the block and obtain thresholds to predict the all-zero block. So we can avoid doing transformation, quantization, and other operations on them, thus reduce the computing time in encoding process. Finally we use the H.264 reference software on VC platform to simulate the method, and compare it with other methods which are involved in the paper. The simulation results show that our method have the prediction rate up to 95.1%, which is more effective than other methods. And at the same time, the loss of the image quality is less than 0.2dB, which can be ignored.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5659165,no
Software Defect Prediction Using Dissimilarity Measures,"In order to improve the accuracy of software defect prediction, a novel method based on dissimilarity measures is proposed. Different from traditional predicting methods based on feature space, we solve the problem in dissimilarity space. First the new unit features in dissimilarity space are obtained by measuring the dissimilarity between the initial units and prototypes. Then proper classifier is chosen to complete prediction. By prototype selecting, we can reduce the dimension of units' features and the computational complexity of prediction. The empirical results in the NASA database KC2 and CM1 show that the prediction accuracies of KNN, Bayes, and SVM classifier in dissimilarity space are higher than that of feature space from 1.86% to 9.39%. Also the computational complexities reduce from 18% to 67%.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5659217,no
The Development about a Prediction System for Coal and Gas Outbursts Based on GIS,"Based on the differences of coal and gas outburst sensitivity index, importance and others in various mining, the author puts forward a system integration strategy about coal and gas outburst prediction. Applying for fault tree analysis, the influence factors about regional outburst and working outburst was analyzed, combined with comprehensive evaluation method, an evaluation index system about coal and gas outburst was established. Based on C # language for the development of tools, GIS software is nested in this system, the coal and gas outburst prediction system was developed. Preliminary application shows that the system can exact predict coal and gas outburst in mines, it is of good application prospects.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5660457,no
Avoiding extended failures in large interconnected power systems  Theoretical and practical issues,"This paper is treating the problem of correct and complete analysis of a large interconnected power system by evaluating the extended failures occurrence probability. Both theoretical and practical issues are treated from the perspective of complete periodically computations that have to be performed in order to have a clear image about the security margins available in a power system for a given stage of time. Main aspects related with steady-state stability limits and transient stability limits are presented from a practical point of view. The key aspects of the well known approaches related with the steady-state stability limits (SSSL) computation is reviewed and an algorithm for SSSL identification, developed by the authors in order to be fitted for power systems planning and operation is also described. The benefits of using a practical methodology to assess the transient stability margins in a real power system are going to be also presented. In order to cover this, the main theoretical aspects related with transient stability are described from a practical point of view. The paper highlights a methodology to assess the risk of a blackout in a power system by means of static and dynamic simulations using a dedicated software tool. Proposed methodology covers both SSSL and transient stability limits (TSL). Original aspects of the paper consists in a new approach to compute the SSSL on a specific constrained area in a power system as well as in a practical method to assess TSL for a given maximum loading of the analyzed power system.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5661962,no
From Use Case Model to Service Model: An Environment Ontology Based Approach,"One fundamental problem in services computing is how to bridge the gap between business requirements and various heterogeneous IT services. This involves eliciting business requirements and building a solution accordingly by reusing available services. While the business requirements are commonly elicited through use cases and scenarios, it is not straightforward to transform the use case model into a service model, and the existing manual approach is cumbersome and error-prone. In this paper, the environment ontology, which is used to model the problem space, is utilized to facilitate the model transformation process. The environment ontology provides a common understanding between business analysts and software engineers. The required software functionalities as well as the available services' capabilities are described using this ontology. By semi-automatically matching the required capability of each use case to the available capabilities provides by services, a use case is realized by that set of services. At the end of this paper, a fictitious case study was used to illustrate how this approach works.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5662486,no
Analysis and Design of the System for Detecting Fuel Economy,"The technology of testing fuel economy is not only a comprehensive parameter for evaluating the level of cars' technology and maintenance, but also an important reference for diagnosing and analyzing the troubles of cars. Using the method of Ultrasonic Flow Detection to measure the fuel consumption, this essay is based on a comprehensive comparison of several traditional fuel economy detection systems and designs a system for detecting fuel economy, that system is based on ESB software. That system can work real-timely, and detect automatically.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5663085,no
An enterprise business intelligence maturity model (EBIMM): Conceptual framework,"Business Intelligence (BI) market trend is hot for the past few years. According to the results from the Gartner Research 2009, Business Intelligence's market is ranked top of ranking in Business and Technology Priorities in 2009. CMM can be applied into various disciplines such as software field, engineering field or IT field. However, there is limited research of CMM that applied in Enterprise Business Intelligence (EBI) domain. This is because BI market is a quite new area. Based on the literature in BI and CMM, a multi-dimensional set of critical factors for characterizing the levels of EBI maturity has been proposed. Specifically, there are three key dimensions of capability influencing the EBI effort: data warehousing, information quality and knowledge process. From a practical standpoint, the proposed model provides useful basis to firms aspiring to elevate their business intelligence endeavour to higher levels of maturity. The research also serves as a foundation for initiating other relevant studies and for assessing EBI maturity.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5664244,no
A feedback-based method for adaptive ROI protection in H.264/AVC,"The bit error and packet loss in unreliable channels such as Internet and wireless networks lead the quality of video data unacceptable, which force us to take the network adaptive transmission into account. In this paper, we firstly propose a novel Greedy Spread (GS) ROI extraction method to extract ROI effectively, and then the network state changing is quickly perceived without occupying additional network overhead. According to the feedback about current network state from the receiver, Gilbert model is used to predict network packet loss, which enables the source encoder to dynamically adjust the ROI protecting schemes. Experimental results shows that the GS method extracts the ROI at high accuracy, also the adaptive method is of low computational complexity and high protection capability, suitable for real-time applications, so make a notable improvement in visual quality.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5664732,no
Exterior quality inspection of rice based on computer vision,"To develop an online inspection system of rice exterior quality (head rice rate, chalk rice, crackle rice) based on computer vision. The system was developed after analyzing the optic characteristics of rice kernel in the platform of VC + + 6.0 software. The five varieties of rice kernel Jinyou974, Gangyou182, Zhongyou205, Jiahe212, and Changnonggeng-2 were selected as experimental samples. The methods, such as gray transformation, automatic threshold segmentation, area labeling, were applied to extract single rice kernel image from collected mass rice kernel images. The chalk rice and crackle rice were inspected by the above methods. To inspect the head rice rate, the ten characteristic parameters, such as the area and perimeter of rice kernel, were selected as the inspection characteristic of head rice, and the method of principal component analysis was carried out to process substantive data. The optimal threshold of distinguishing head rice was made sure. The results showed that the accurate ratio of detecting crackle rice was 96.41%, the correct ratio of detecting chalk rice was 94.79%, and the accurate ratio of detecting head rice was 96.20%. The analysis indicated efficient discrimination from different rice exterior quality by computer vision.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5665507,no
Hand-held multi-parameter water quality recorder,"Water resources monitoring has become an important research topic because an unreasonable use of water resources results in the deterioration of water environment and affects the development of human beings and nature. The scheme of the hand-held multi-parameter water quality recorder based on the sensor of the YSI6600 was designed. Through the multi-parameter sensor of the YSI6600, the recorder can detect and analyze the water quality of 17 parameters, such as turbidity, PH value etc. The core controller by using the low-power single-chip MSP430F149 makes the entire system with battery-powered. The use of multi-parameter sensor YSI6600 makes collection of data-processing speed and high precision, small error. The advantages of the recorder are the small size, the low cost, the low consumption, the large-screen LCD display, and easily carrying. It can communicate with the computer by RS232, position water quality data of a water area by circumscribed GPS, and provide a favorable data protection for the water sector and environmental sector by the water quality data. It has been played a significant role in environmental monitoring and water quality monitoring. Comparing with the traditional sampling from the river and instrument in the past, the recorder has greatly improved the efficiency of detection.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5665537,no
Wavelet-based one-terminal fault location algorithm for aged cables without using cable parameters applying fault clearing voltage transients,"This paper presents a novel fault location algorithm, which in spite of using only voltage samples taken from one terminal, is capable to calculate precise fault location in aged power cables without any need to line parameters. Voltage transients generated after circuit breaker opening action are sampled and using wavelet and traveling wave theorem, first and second inceptions of voltage traveling wave signals are detected. Then wave speed is determined independent of cable parameters and finally precise location of fault is calculated. Because of using one terminal data, algorithm does not need to communication equipments and global positioning system (GPS). Accuracy of algorithm is not affected by aging, climate and temperature variations, which change the wave speed. In addition, fault resistance, fault inception angle and fault distance does not affect accuracy of algorithm. Extent simulations carried out with SimPowerSystem toolbox of MATLAB software, confirm capability and high accuracy of proposed algorithm to calculate fault location in the different faults and system conditions.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5666457,no
Static Detection Unsafe Use of variables In Java,"Exception handling has been introduced into object oriented programming languages to help developing robust software. At the same time, it makes programming more difficult and it is not easy to write high quality exception handling codes. Careless exception handling code will introduce bugs and it usually forms certain kind of bug pattern. In this paper we propose a new bug pattern unsafe use of variables due to exception occurrences. It may cause the dependency safety property violation in a program. We also develop a static approach to automatically detect unsafe use of variables that may introduce potential bugs in Java program. This approach can be integrated into current bug finding tools to help developer improve the quality of Java program.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5667112,no
Energy-Efficient Clustering Rumor Routing Protocol for Wireless Sensor Networks,"To develop an energy-efficient routing protocol becomes one of the most important and difficult key tasks for wireless sensor networks. Traditional Rumor Routing is effective in random path building but winding or even looped path is prone to be formed, leading to enormous energy wasting. Clustering Rumor Routing proposed in this paper has advantages in energy saving by making full use of three features-clustering structure, selective next-hop scheme and double variable energy thresholds for rotation of cluster-heads. Thus CRR can effectively avoid the problems that occur in RR and a more energy-efficient routing path from event agents to queries can be built. The performance between CRR and traditional RR are evaluated by simulations. The results indicate that compared with traditional RR, the CRR can save more energy consumption, provide better path quality, and improve the delivery rate as well.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5667201,no
Fault Localization in Constraint Programs,"Constraint programs such as those written in high level modeling languages (e.g., OPL, ZINC, or COMET) must be thoroughly verified before being used in applications. Detecting and localizing faults is therefore of great importance to lower the cost of the development of these constraint programs. In a previous work, we introduced a testing framework called CPTEST enabling automated test case generation for detecting non-conformities. In this paper, we enhance this framework to introduce automatic fault localization in constraint programs. Our approach is based on constraint relaxation to identify the constraint that is responsible of a given fault. CPTEST is henceforth able to automatically localize faults in optimized OPL programs. We provide empirical evidence of the effectiveness of this approach on classical benchmark problems, namely Golomb rulers, n-queens, social golfer and car sequencing.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5670017,no
A study on quality improvement of railway software,"The digital system performs more varying and highly complex functions efficiently compared to the existing analog system because software can be flexibly designed and implemented. The flexible design makes it difficult to predict the software failures. Even though the main characteristic of railway system is to ensure safety, nowadays software is widely used in the safety critical railway system just after evaluation of system function itself. The railway system is also safety critical system and the software is widely used in the railway system. For this reason, the safety criteria are suggested to secure the software safety for the field of railway system. The software used in the safety critical system has to be examined whether it is properly developed according to the safety criteria and certification process. This paper also suggests a development methodology for the railway company to easily apply the criteria to the railway system.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5670254,no
Requirement based test case prioritization,"Test case prioritization involves scheduling test cases in an order that increases the effectiveness in achieving some performance goals. One of the most important performance goals is the rate of fault detection. Test cases should run in an order that increases the possibility of fault detection and also that detects faults at the earliest in its testing life cycle. In this paper, an algorithm is proposed for system level test case prioritization (TCP) from software requirement specification to improve user satisfaction with quality software and also to improve the rate of severe fault detection. The proposed model prioritizes the system test cases based on the three factors: customer priority, changes in requirement, implementation complexity. The proposed prioritization technique is validated with two different sets of industrial projects and the results show that the proposed prioritization technique improves the rate of severe fault detection.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5670728,no
A Case Study of Software Security Test Based on Defects Threat Tree Modeling,"Due to the increasing complexity of software applications, traditional function security testing ways, which only test and validate software security mechanisms, are becoming ineffective to detect latent software security defects (SSD). However, the most of vulnerabilities result from some typical SSD. According to CERT/CC, ten defects known are responsible for 75% of security breaches in today software applications. On the base of threat tree modeling, we use it in the integrated software security test model. For introducing the usefulness of the method, we use the test model in M<sup>3</sup>TR software security test.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5670961,no
Interpreting the out-of-control signals of the Generalized Variance |S| control chart,"Multivariate quality control charts have some advantages for monitoring more than one variable. Nevertheless, there are some disadvantages when multivariate schemes are employed. The main problem is how to interpret the out-of-control signal. For example, in the case of control charts designed to monitor the mean vector, the chart signals show that there is a shift in the vector, but no indication is given about the variables that have shifted. Generalized Variance |S| quality control chart is a very powerful way to detect small shifts in the mean vector. Unfortunately, there are no previous works about the interpretation of the out-of-control signal of this chart. In this paper neural networks are used to interpret the out-of-control signal of the Generalized Variance |S| Chart. The utilization of this neural network in the industry is very easy, thanks to the developed software.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5674273,no
A study of applying the bounded Generalized Pareto distribution to the analysis of software fault distribution,"Software is currently a key part of many safety-critical applications. But the main problem facing the computer industry is how to develop a software with (ultra) high reliability on time, and assure the quality of software. In the past, some researchers reported that the Pareto distribution (PD) and the Weibull distribution (WD) models can be used for software reliability estimation and fault distribution modeling. In this paper we propose a modified PD model to predict and assess the software fault distribution. That is, we suggest using a special form of the Generalized Pareto distribution (GPD) model, named the bounded Generalized Pareto distribution (BGPD) model. We will show that the BGPD model eliminates several modeling issues that arise in the PD model, and perform detailed comparisons based on real software fault data. Experimental result shows that the proposed BGPD model presents very high fitness to the actual fault data. In the end, we conclude that the distribution of faults in a large software system can be well described by the Pareto principle.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5674517,no
Multimedia system verification through a usage model and a black test box,"This paper presents an automated verification methodology aimed at detecting failures in multimedia systems based on a black box testing approach. Moreover, the verification is performed using a black test box as part of a test harness. The quality of a system is examined against functional failures using a model-based testing approach for generating test scenarios. System under test (specifically, the software of the system) is modeled to represent the most probable system usage. In this way, failures that occur most frequently during system exploitation are detected through the testing. Test case execution is fully automated and test oracle is based on image quality analysis. The proposed framework is primarily intended for detecting software-related failures, but will also detect the failures that result from system hardware defects.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5674849,no
Improving of STS algorithm to detecting voltage unbalance in low voltage distribution networks,"Thyristor-based static transfer switches (STS's) are feeding sensitive loads with two independent sources by monitoring voltage quality. STS is used in distribution networks to provide connection to alternate sources of ac power for critical loads when the main source fails. In distribution system sensitive loads are endangered by either voltage sag or voltage unbalance. Simulation results show that, the STS-2 of the IEEE benchmark in voltage sag and transient conditions operates properly; but do not detect voltage un-balance. In this paper, operating of the STS-2 under unbalance distribution network - as a scenario to improve the power quality for a three-phase critical load in industrial areas - is discussed. This paper presents an improved algorithm to detecting voltage unbalance with the STS-2 based on the ratio of the negative and the positive sequences. Simulation is presented in the well known software ATP/EMTP. Also load and network parameters are based on the industrial network of the Naghadeh - Iran.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5674951,no
Analysis of integrated storage and grid interfaced photovoltaic system via nine-switch three-level inverter,"This paper focuses on the modeling, controller design, simulation and analysis of a two-string photovoltaic (PV) and storage system connected to grid via a centralized nine-switch three-level inverter. A circuit-based PV array model is presented for use. A boost converter is employed to enable PV to operate at its maximum power point. The storage system consists of a capacitor bank connected at the DC bus via a full-bridge DC-DC converter. The centralized inverter is controlled by a decoupled current control technique and interfaced with grid via a transformer and double transmission lines. And three-level PWM is generated by applying two symmetrical triangular carriers. Three-phase voltage source and RL load are used for simulation. Eventually, the system stability is assessed with respect to fault conditions and solar irradiation change in simulation software PSCAD.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5675035,no
Visual Indicator Component Software to Show Component Design Quality and Characteristic,"Good design is one of the prerequisites of high quality product. To measure the quality of software design, software metrics are used. Unfortunately in software development practice, there are a lot of software developers who are not concerned with the component's quality and characteristic. Software metrics does not interest them, because to understand the measurement of the metrics, a deep understanding about degree, dimension, and capacity of some attribute of the software product is needed. This event triggers them to build software's whose quality is below the standard. What is more dangerous is that these developers are not aware of quality and do not care with their work product. Of course these occurrences is concerning and a solution needed to be found. Through this paper the researcher is trying to formulate an indicator of component software that shows component design quality and characteristic visually. This indicator can help software developers to make design decision and refactoring decision, detect the design problem more quickly, able to decide which area to apply refactoring, and enable us to do early or final detection of design defects.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5675845,no
Intelligent monitoring and fault tolerance in large-scale distributed systems,"Summary form only. Electronic devices are starting to become widely available for monitoring and controlling large-scale distributed systems. These devices may include sensing capabilities for online measurement, actuators for controlling certain variables, microprocessors for processing information and making realtime decisions based on designed algorithms, and telecommunication units for exchanging information with other electronic devices or possibly with human operators. A collection of such devices may be referred to as a networked intelligent agent system. Such systems have the capability to generate a huge volume of spatial-temporal data that can be used for monitoring and control applications of large-scale distributed systems. One of the most important research challenges in the years ahead is the development of information processing methodologies that can be used to extract meaning and knowledge out of the ever-increasing electronic information that will become available. Even more important is the capability to utilize the information that is being produced to design software and devices that operate seamlessly, autonomously and reliably in some intelligent manner. The ultimate objective is to design networked intelligent agent systems that can make appropriate real-time decisions in the management of large-scale distributed systems, while also providing useful high-level information to human operators. One of the most important classes of large-scale distributed systems deals with the reliable operation and intelligent management of critical infrastructures, such as electric power systems, telecommunication networks, water systems, and transportation systems. The design, control and fault monitoring of critical infrastructure systems is becoming increasingly more challenging as their size, complexity and interactions are steadily growing. Moreover, these critical infrastructures are susceptible to natural disasters, frequent failures, as well as malicio- - us attacks. There is a need to develop a common system-theoretic fault diagnostic framework for critical infrastructure systems and to design architectures and algorithms for intelligent monitoring, control and security of such systems. The goal of this presentation is to motivate the need for health monitoring, fault diagnosis and security of critical infrastructure systems and to provide a fault diagnosis methodology for detecting, isolating and accommodating both abrupt and incipient faults in a class of complex nonlinear dynamic systems. A detection and approximation estimator based on computational intelligence techniques is used for online health monitoring. Various adaptive approximation techniques and learning algorithms will be presented and illustrated, and directions for future research will be discussed.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5675940,no
Control of airship in case of unpredictable environment conditions,"The article demonstrates how to generate the trajectory, taking into account the concept of the tunnel of error that ensures route trace with an error no greater than assumed, even in difficult to predict environmental conditions. The mathematical model of kinematics and dynamics using spatial vectors is presented in short. The theoretical assumptions are tested by simulation. The model used in the simulations takes into account the structure of the drives in the form of two engines placed symmetrically on the sides of the object.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5675976,no
Reliable online water quality monitoring as basis for fault tolerant control,"Clean data are essential for any kind of alarm or control system. To achieve the required level of data quality in online water quality monitoring, a system for fault tolerant control was developed. A modular approach was used, in which a sensor and station management module is combined with a data validation and an event detection module. The station management module assures that all relevant data, including operational data, is available and the state of the monitoring devices is fully documented. The data validation module assures that unreliable data is detected, marked as such, and that the need for sensor maintenance is timely indicated. Finally, the event detection module marks unusual system states and triggers measures and notifications. All these modules were combined into a new software package to be used on water quality monitoring stations.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5675985,no
Representing and Reasoning about Web Access Control Policies,"The advent of emerging technologies such as Web services, service-oriented architecture, and cloud computing has enabled us to perform business services more efficiently and effectively. However, we still suffer from unintended security leakages by unauthorized services while providing more convenient services to Internet users through such a cutting-edge technological growth. Furthermore, designing and managing Web access control policies are often error-prone due to the lack of logical and formal foundation. In this paper, we attempt to introduce a logic-based policy management approach for Web access control policies especially focusing on XACML (eXtensible Access Control Markup Language) policies, which have become the de facto standard for specifying and enforcing access control policies for various applications and services in current Web-based computing technologies. Our approach adopts Answer Set Programming (ASP) to formulate XACML that allows us to leverage the features of ASP solvers in performing various logical reasoning and analysis tasks such as policy verification, comparison and querying. In addition, we propose a policy analysis method that helps identify policy violations in XACML policies accommodating the notion of constraints in role-based access control (RBAC). We also discuss a proof-of-concept implementation of our method called XACMLl2ASP with the evaluation of several XACML policies from real-world software systems.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676253,no
Target Setting for Technical Requirements in Time-Stamped Software Quality Function,"Technical requirements are hard to determine in software development. They are often specified subjectively in practice. Poorly determined technical requirements often lead to poor customer satisfaction, cost overrun, and delay in schedule, and poor quality. Quality Function Deployment (QFD) is one of major engineering methods used to elicit customer's needs and transforms them into technical requirements in industry. It has been applied to develop numerous products, including software systems, to improve their quality. However, target setting for technical software requirements is a complicated and challenging task in product development in Software Quality Function Deployment (SQFD). Current methods for target setting for technical software requirements do not consider their technical trends for a given timeframe. As a result, by the time of completion of a project the target values of technical requirements may not be competitive any more. In this paper, we first discuss benchmarking, primitive linear regression and target setting based on impact analysis to set targets for technical requirements in Software Quality Function Deployment (SQFD). We then develop a method of target setting for technical requirements by incorporating timeframe and the technical trend. It can help us to assess impact of both under-achieved and over-achieved targets. By incorporating the technical trend and the time of delivery of the product into target setting process, we can set targets for technical requirements that provide a competitive edge for our product over the competitor's products and a high level of customer satisfaction.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676258,no
An Empirical Comparison of Fault-Prone Module Detection Approaches: Complexity Metrics and Text Feature Metrics,"In order to assure the quality of software product, early detection of fault-prone products is necessary. Fault-prone module detection is one of the major and traditional area of software engineering. However, comparative study using the fair environment rarely conducted so far because there is little data publicly available. This paper tries to conduct a comparative study of fault-prone module detection approaches.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676267,no
"Managing Consistency between Textual Requirements, Abstract Interactions and Essential Use Cases","Consistency checking needs to be done from the earliest phase of requirements capture as requirements captured by requirement engineers are often vague, error-prone and inconsistent with users' needs. To improve such consistency checking we have applied a traceability approach with visualization capability. We have embedded this into a light-weight automated tracing tool in order to allow users to capture their requirements and generate Essential Use Case models of these requirements automatically. Our tool supports inconsistency checking between textual requirements, abstract interactions that derive from the text and Essential Use Case models. A preliminary evaluation has been conducted with target end users and the tool usefulness and ease of use are evaluated. We describe our motivation for this research, our prototype tool and results of our evaluation.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676276,no
An Optimized Checkpointing Based Learning Algorithm for Single Event Upsets,"With the arrival of the CMOS technology, the sizes of the transistors are anything but increasing. Due to the current transistor sizes single event upsets, which were over looked for the previous generation are not so anymore. With memories and other peripherals well protected from single event upsets, processors are in a critical state. Hard errors too have a higher probability of occurrence. This work is aimed at detection of soft errors (SEUs) and making programs more resilient to them and to detect hard errors and eliminate them. SEUs are transient errors and hard errors are permanent in nature. The idea is to use the concept of CFGs, DFGs and data dependency graphs with Integer Linear Programming to improve the program and testing it on fault induced architectures.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676284,no
Design and Analysis of Cost-Cognizant Test Case Prioritization Using Genetic Algorithm with Test History,"During software development, regression testing is usually used to assure the quality of modified software. The techniques of test case prioritization schedule the test cases for regression testing in an order that attempts to increase the effectiveness in accordance with some performance goal. The most general goal is the rate of fault detection. It assumes all test case costs and fault severities are uniform. However, those factors usually vary. In order to produce a more satisfactory order, the cost-cognizant metric that incorporates varying test case costs and fault severities is proposed. In this paper, we propose a cost-cognizant test case prioritization technique based on the use of historical records and a genetic algorithm. We run a controlled experiment to evaluate the proposed technique's effectiveness. Experimental results indicate that our proposed technique frequently yields a higher Average Percentage of Faults Detected per Cost (APFDc). The results also show that our proposed technique is also useful in terms of APFDc when all test case costs and fault severities are uniform.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676289,no
A Study on the Applicability of Modified Genetic Algorithms for the Parameter Estimation of Software Reliability Modeling,"In order to assure software quality and assess software reliability, many software reliability growth models (SRGMs) have been proposed for estimation of reliability growth of products in the past three decades. In principle, two widely used methods for the parameter estimation of SRGMs are the maximum likelihood estimation (MLE) and the least squares estimation (LSE). However, the approach of these two estimations may impose some restrictions on SRGMs, such as the existence of derivatives from formulated models or the needs for complex calculation. Thus in this paper, we propose a modified genetic algorithm (MGA) to estimate the parameters of SRGMs. Experiments based on real software failure data are performed, and the results show that the proposed genetic algorithm is more effective and faster than traditional genetic algorithms.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676305,no
A Method for Detecting Defects in Source Codes Using Model Checking Techniques,This paper proposes a method of detecting troublesome defects in the Java source codes for enterprise systems using a model checking technique. A supporting tool also provides a function to automatically translate source code into a model which is simulated by UPPAAL model checker.,2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676307,no
Using Load Tests to Automatically Compare the Subsystems of a Large Enterprise System,"Enterprise systems are load tested for every added feature, software updates and periodic maintenance to ensure that the performance demands on system quality, availability and responsiveness are met. In current practice, performance analysts manually analyze load test data to identify the components that are responsible for performance deviations. This process is time consuming and error prone due to the large volume of performance counter data collected during monitoring, the limited operational knowledge of analyst about all the subsystem involved and their complex interactions and the unavailability of up-to-date documentation in the rapidly evolving enterprise. In this paper, we present an automated approach based on a robust statistical technique, Principal Component Analysis (PCA) to identify subsystems that show performance deviations in load tests. A case study on load test data of a large enterprise application shows that our approach do not require any instrumentation or domain knowledge to operate, scales well to large industrial system, generate few false positives (89% average precision) and detects performance deviations among subsystems in limited time.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676342,no
Study on the Evaluation Model of Student Satisfaction Based on Factor Analysis,"The paper first determined index system of the student satisfaction by the systematic analysis approach, established the mathematical evaluation model of student satisfaction based on factor analysis, then assessed and modified the model through empirical study. It turned out that perceived quality had the greatest impact on satisfaction rating. Student expectations and perceived value had significant effects, too. So the primary factor to improve the student satisfaction is to increase the input of teaching equipment and teaching materials, especially network and e-learning resources. Guiding the learning method by teachers should also be stressed.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676748,no
A Benchmarking Framework for Domain Specific Software,"With the development of the software engineering, finding the """"best-in-class"""" practice of a specific domain software and locating its position is an urgent task. This paper proposes a systematic, practical and simplified benchmarking framework for domain specific software. The methodology is useful to assess characteristics, sub-characteristics and attributes that influence the domain specific software product quality qualitative and quantitative. It is helpful for the business managers and the software designers to obtain the competitiveness analysis results of software. Using the domain specific benchmarking framework, the software designers and the business managers can easily recognize the positive and negative gap of their product and locate its position in the specific domain.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676799,no
Importance Sampling Based Safety-Critical Software Statistical Testing Acceleration,"It is necessary to assess the reliability of safety-critical software to a high degree of confidence before they are deployed in the field. However, safety-critical software often includes some rarely executed critical operations that are often inadequately tested in statistical testing based reliability estimation. This paper discusses software statistical testing acceleration based on importance sampling technique. When both the critical operations and the entire software are adequately tested, the method can still get the unbiased software reliability from the test results with much less test cases. Thus, the statistical testing cost of safety-critical software can be reduced effectively The simulated annealing algorithm for calculating optimum transition probabilities of the Markov chain usage model for software statistical testing accelerating is also presented.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676800,no
A Novel Evaluation Method for Defect Prediction in Software Systems,"In this paper, we propose a novel evaluation method for defect prediction in object-oriented software systems. For each metric to evaluate, we start by applying it to the dependency graph extracted from the target software system, and obtain a list of classes ordered by their predicted degree of defect under that metric. By utilizing the actual defect data mined from the subversion database, we evaluate the quality of each metric through means of a weighted reciprocal ranking mechanism. Our method can tell not only the overall quality of each evaluated metric, but also the quality of the prediction result for each class, especially those costly ones. Evaluation results and analysis show the efficiency and rationality of our method.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676810,no
The Application of Ant Colony Algorithm in Web Service Selection,"As an intelligent algorithm with the mechanism of positive feedback, the ant colony algorithm is useful in solving the optimal problem. Web service selection is the foundation of the Web service composition which is one of the most important ways to satisfy the users' personalized requirements. Firstly, analyzed the problem of Web service selection based on expounding the basic principle of ant colony algorithm and transformed the problem of Web service selection driven by QoS into the problem of finding the shortest path. Secondly, given the steps for solving the problem of Web Service selection based on the ant colony algorithm and contrasted the results under different parameters. Lastly, verified the validity of ant colony algorithm in Web Service selection.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676825,no
A Recommendation Method of BPEL Activity Based on Association Rules Mining,"As the de facto standard, Business Process Execution Language (BPEL) is widely used in the composition and orchestration of web services; however, it is tiring and error-prone if we just rely on the developers to create every activity icon and assign it an atomic service. On the basis of the author's early work, this paper creatively applies association rules mining to the analysis of BPEL process, and put forwards a recommendation method of BPEL activity based on Labeled Activity Tree (LAT) and sub-activity sequence (subASeq). Finally, the experiment result of the BPEL processes in a multimedia conference system indicates this method's validity.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676862,no
Automated Verification of Goal Net Models,"Multi-agent systems are increasingly complex and the problem of their verification is acquiring increasing importance. Most of the existing approaches are based on model checking techniques which require system specification in a formal language. This is an error-prone task and can only be performed by experts who have considerable experiences in logic reasoning. We proposed in this paper an easy-to-use approach to the verification of multi-agent systems based on the Goal Net modeling methodology. Goal Net is a goal-oriented framework which aspires to simplify the process of designing and implementing multi-agent systems. In our approach, a multi-agent system is modeled as a goal net. The goal net and its properties will be automatically converted into FSP (Finite State Processes) specification, which can be automatically verified by the model checker LTSA (Labeled Transition System Analyzer).",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676884,no
Improved Intra-Prediction Mode Selection Algorithm for H.264,"H.264 introduces a technology of intra-prediction to improve the coding efficiency of I frames, but also greatly increased the computational complexity of encoding. This paper proposes a fast intra- prediction mode selection algorithm. The algorithm predicted macroblock boundaries and the fast intra- prediction mode selection to speed up encoding rate. The result of experiments show that the proposed algorithm improve the coding rate effectively based on the guarantee of the image quality.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5677217,no
Research of Real-Time Algorithm for Chestnut's Size Based on Computer Vision,"A grading system was developed to classify chestnut automatically into various grades of quality in terms of size. The chestnut is scanned with a color charge-coupled-device camera and then the size is extracted by image processing. In the image processing there are two kinds of algorithm, one is the minimum enclosing rectangle (MER), and the other is the distance between centroid and border (DCB). The algorithm finds out the chestnut's major axis and minor axis which are used to predict the size of the chestnut. By regression analysis, it's found that the relative error of the MER is 0.7465%, and the relative error of the DBCB is 1.83%. The MER is better to predict the chestnut's size than the DCB.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5677220,no
Seamless high speed simulation of VHDL components in the context of comprehensive computing systems using the virtual machine faumachine,"Testing the interaction between hard- and software is only possible once prototype implementations of the hardware exist. HDL simulations of hardware models can help to find defects in the hardware design. To predict the behavior of entire software stacks in the environment of a complete system, virtual machines can be used. Combining a virtual machine with HDL-simulation enables to project the interaction between hard- and software implementations, even if no prototype was created yet. Hence it allows for software development to begin at an earlier stage of the manufacturing process and helps to decrease the time to market. In this paper we present the virtual machine FAUmachine that offers high speed emulation. It can co-simulate VHDL components in a transparent manner while still offering good overall performance. As an example application, a PCI sound card was simulated using the presented environment.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5679102,no
Effectiveness of the cumulative vs. normal mode of operation for combinatorial testing,"This paper discusses the state of the art of applying combinatorial interaction testing (CIT) in conjunction with mutation testing for hardware testing. In addition, the paper discusses the art of the practice of applying CIT in normal and cumulative mode in order to derive an optimal test suite that can be used for hardware testing in a production line. Our previous study based on applying CIT in cumulative mode; described the systematic application of the strategy for testing 4-bit Magnitude Comparator Integrated Circuits in a production line. Complementing our previous work, this paper compares the effectiveness of cumulative mode versus normal mode of operation. Our result demonstrates that the use of CIT in cumulative mode is more practical than normal mode of operation as far as detecting faults introduced by mutation.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5679441,no
Preventing insider malware threats using program analysis techniques,"Current malware detection tools focus largely on malicious code that is injected into target programs by outsiders by exploiting inadvertent vulnerabilities such as failing to guard against a buffer overflow or failure to properly validate a user input in those programs. Hardly any attention is paid to threats arising from software developers, who, with their intimate knowledge of the inner workings of those programs, can easily sneak logic bombs, Trojan horses, and backdoors in those programs. Traditional software validation techniques such as testing based on user requirements are unlikely to detect such malware, because normal use cases will not trigger them and thus will fail to expose them. The state-of-the-art in preventing such malware involves manual inspection of the target program, which is a highly tedious, time consuming, and error prone process. We propose a dynamic, test driven approach that automatically steers program analysts towards examining and discovering such insider malware threats. It uses program analysis techniques to identify program parts whose execution automatically guarantees execution of a large number of previously unexplored parts of the program. It effectively leads analysts into creating test cases which may trigger, in a protected test environment, any malware code hidden in that application as early as possible, so it can be removed from the application before it is deployed in the field. We also present a tool that helps translate this approach into practice.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5679584,no
An efficient negotiation based algorithm for resources advanced reservation using hill climbing in grid computing system,"Ensuring quality of services and reducing the blocking probability is one of the important cases in the environment of grid computing. In this paper, we present a deadline aware algorithm to give solution to ensuring the end-to-end QoS and improvement of the efficiency of grid resources. Investigating requests as a group in the start of time slot and trying to accept the highest number of them. Totally can cause to increase the possibility of acceptance of requests and also increase efficiency of grid resources. Deadline aware algorithm reaches us to this goal. Simulations show that the deadline aware algorithm improves efficiency of advance reservation resources in both case. Possibility of acceptance of requests and optimizing resources in short time slot and also in rate of high entrance of requests in each time.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5679601,no
Beautiful picture of an ugly place. Exploring photo collections using opinion and sentiment analysis of user comments,"User generated content in the form of customer reviews, feedbacks and comments plays an important role in all types of Internet services and activities like news, shopping, forums and blogs. Therefore, the analysis of user opinions is potentially beneficial for the understanding of user attitudes or the improvement of various Internet services. In this paper, we propose a practical unsupervised approach to improve user experience when exploring photo collections by using opinions and sentiments expressed in user comments on the uploaded photos. While most existing techniques concentrate on binary (negative or positive) opinion orientation, we use a real-valued scale for modeling opinion and sentiment strengths. We extract two types of sentiments: opinions that relate to the photo quality and general sentiments targeted towards objects depicted on the photo. Our approach combines linguistic features for part of speech tagging, traditional statistical methods for modeling word importance in the photo comment corpora (in a real-valued scale), and a predefined sentiment lexicon for detecting negative and positive opinion orientation. In addition, a semi-automatic photo feature detection method is applied and a set of syntactic patterns is introduced to resolve opinion references. We implemented a prototype system that incorporates the proposed approach and evaluates it on several regions in the World using real data extracted from Flickr.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5679726,no
A robust and fault-tolerant distributed intrusion detection system,"Since it is impossible to predict and identify all the vulnerabilities of a network, and penetration into a system by malicious intruders cannot always be prevented, intrusion detection systems (IDSs) are essential entities for ensuring the security of a networked system. To be effective in carrying out their functions, the IDSs need to be accurate, adaptive, and extensible. Given these stringent requirements and the high level of vulnerabilities of the current days' networks, the design of an IDS has become a very challenging task. Although, an extensive research has been done on intrusion detection in a distributed environment, distributed IDSs suffer from a number of drawbacks e.g., high rates of false positives, low detection efficiency etc. In this paper, the design of a distributed IDS is proposed that consists of a group of autonomous and cooperating agents. In addition to its ability to detect attacks, the system is capable of identifying and isolating compromised nodes in the network thereby introducing fault-tolerance in its operations. The experiments conducted on the system have shown that it has high detection efficiency and low false positives compared to some of the currently existing systems.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5679879,no
An Upper Bound on the Probability of Instability of a DVB-T/H Repeater with a Digital Echo Canceller,"The architecture of a digital On-Channel Repeater (OCR) for DVB-T/H signals is described in this paper. The presence of a coupling channel between the transmitting and the receiving antennas gives origin to one or more echoes, having detrimental effects on the quality of the repeated signal and critically affecting the overall system stability. A low-complexity echo canceller unit is then proposed, performing a coupling channel estimation based on the local transmission of low-power training signals. In particular, in this paper we focus on the stability issues which arise due to the non perfect echo cancellation. An upper bound on the probability of instability of the system is analytically found, providing useful guidelines for conservative OCR design, and some performance figures concerning different propagation scenarios are provided.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5683631,no
A Distributed Multi-Target Software Vulnerability Discovery and Analysis Infrastructure for Smart Phones,"Smart phones of today have increasingly sophisticated software. As the feature set grows further, the probability of system security related defects is likely to increase as well. Today, the security of mobile platforms and applications comes under great scrutiny as they are getting widely adopted. It is therefore crucial that code for mobile devices gets well tested and security bugs eliminated where possible. A popular and effective testing technique to identify severe security bugs in source code is fuzz testing. However, it is extremely time consuming to generate randomized input and test them on each version of the mobile phone and its software. This paper presents, MAFIA - Multi-target Automated Fuzzing Infrastructure and Arsenal, a composite, distributed client-server fuzz testing infrastructure for software applications and libraries in virtually any smartphone platform. The set of tools in MAFIA is file-format agnostic and can be used across various applications & libraries. With MAFIA, we conducted a large number of tests against image-handling libraries and logged more than 13,000 mutated inputs that successfully crash several Symbian OS retail phones models. The system is scalable and can be easily extended to be used on new devices and operating systems.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5684011,no
Perverse UML for generating web applications using YAMDAT,"In the current environment of accelerating technological change, software development continues to be difficult, unpredictable, expensive, and error-prone. Model Driven Architecture (MDA), sometimes known as Executable UML, offers a possible solution. MDA provides design notations with precisely defined semantics. Using these notations, developers can create a design model that is detailed and complete enough that the model can be verified and tested via simulation (""""execution""""). Design faults, omissions, and inconsistencies can be detected without writing any code. Furthermore, implementation code can be generated directly from the model. In fact, implementations in different languages or for different platforms can be generated from the same model. The YAMDATTMproject (Yet Another Model Driven Architecture Tool) will provide a convenient suite of tools for design, collaboration, model verification, and code generation. Parts of the system (notably, the code generation for state machines) are already sufficiently well-developed to support development of complex C++ and Java systems. This paper describes an experiment in repurposing a standard UML component, the class diagram, to support the design and automatic code generation for a domain not usually considered for MDA: the page interactions of a web application.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5686432,no
Regression test cases prioritization using Failure Pursuit Sampling,"The necessity of lowering the execution of system tests' cost is a consensual point in the software development community. The present study presents an optimization of the regression tests' activity, by adapting a test cases prioritization technique called Failure Pursuit Sampling-previously used and validated for the prioritization of tests in general-improving its efficiency for the exclusive execution of regression test. For this purpose, the clustering and sampling phases of the original technique were modified, so that it becomes capable of receive information from tests made on the previous version of a program, and can use this information to drive de efficiency of the new developed technique, for tests made on a present version. The adapted technique was implemented and executed using the Schedule program, of the Siemens suit. By using Average of the Percentage of Faults Detected charts, the modified Failure Pursuit Sampling technique presented a high level of efficiency improvement.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5687069,no
Event driven multi-context trust model,"Agent reasoning in large scale multi-agent systems requires techniques which often work with uncertainty and probability. In our research, we use trust and reputation principles to support agent reasoning and decisioning. Information about agents past behaviour and their qualities are transformed to multi-context trust. It allows to view a single agent from different point of views, because agents are judged in different aspects - contexts. In this paper we describe event driven multi-context trust model as extension of Hierarchical Model of Trust in Contexts (HMTC), when different types of events causes trust updates. This extension of HMTC also provides some solutions for avoiding conflicts which may appear in previous HMTC.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5687071,no
Fault Evaluator: A tool for experimental investigation of effectiveness in software testing,"The specifications for many software systems, including safety-critical control systems, are often described using complex logical expressions. It is important to find effective methods to test implementations of such expressions. Analyzing the effectiveness of the testing of logical expressions manually is a tedious and error prone endeavor, thus requiring special software tools for this purpose. This paper presents Fault Evaluator, which is a new tool for experimental investigation of testing logical expressions in software. The goal of this tool is to evaluate logical expressions with various test sets that have been created according to a specific testing method and to estimate the effectiveness of the testing method for detecting specific faulty variations of the original expressions. The main functions of the tool are the generation of complete sets of faults in logical expressions for several specific types of faults; gaining expected (Oracle) values of logical expressions; testing faulty expressions and detecting whether a test set reveals a specific fault; and evaluating the effectiveness of a testing approach.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5688000,no
A comprehensive evaluation methodology for domain specific software benchmarking,"With the wide using of information system, software users are demanding higher quality software and the software developers are pursue the best-in-class practice in a specific domain. But till now it is difficult to get an evaluation result due to lack of benchmark method. This paper proposes a systematic, practical and simplified evaluation methodology for domain specific software benchmarking. In this methodology, an evaluation model with software test characteristics (elementary set) and domain characteristics (extended set) is described; in order to weaken the uncertainty of subjective and objective, the rough set theory is applied to gain the attributes weights, and the gray analysis method is introduced to remedy the incomplete and inadequate information. The method is useful to assess characteristics, sub-characteristics and attribute that influence the domain specific software product quality qualitative and quantitative. The evaluation and benchmarking results prove that the model is practical and effective.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5688006,no
Detecting faults in technical indicator computations for financial market analysis,"Many financial trading and charting software packages provide users with technical indicators to analyze and predict price movements in financial markets. Any computation fault in technical indicator may lead to wrong trading decisions and cause substantial financial losses. Testing is a major software engineering activity to detect computation faults in software. However, there are two problems in testing technical indicators in these software packages. Firstly, the indicator values are updated with real-time market data that cannot be generated arbitrarily. Secondly, technical indicators are computed based on a large amount of market data. Thus, it is extremely difficult, if not impossible, to derive the expected indicator values to check the correctness of the computed indicator values. In this paper, we address the above problems by proposing a new testing technique to detect faults in computation of technical indicators. We show that the proposed technique is effective in detecting computation faults in faulty technical indicators on the MetaTrader 4 Client Terminal.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5689221,no
"Surface water quality forecasting based on ANN and GIS for the Chanzhi Reservoir, China","Artificial neural network (ANN) based on Arc Engine (AE) of GIS is used to predict surface water quality in the Chanzhi Reservoir, Qingdao, China. The results can reflect the water quality change trends with less than 10% average relative error. Using MS SQL Server database technology combined with Geodatabase, the system achieves the fundamental geographic information and hydrological data management, and water quality prediction. It uses GIS component technology to build an efficient and stable platform which can apply to general surface water quality prediction. This software has good information extraction and query functions to help decision-makers to manage water resource better.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5689328,no
An improved analytic hierarchy process model on Software Quality Evaluation,"With new global demands for better quality of software products, effective and efficient SQE (Software Quality Evaluation) becomes necessary and indispensible. Existing methods of SQE are as it is, however, subjective, non-quantitative and uncompleted to some extent. In this paper, we introduce a novel method, an improved AHPM (Analytic Hierarchy Process Model) incorporated traditional AHPM and some quality characteristics from ISO/IEC 9126, which attempts to propose a better method of software quality evaluation that is relatively objective, quantitative and completed. On this basis, current software can be well assessed as well as future products can be well predicted.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5690372,no
Dependency-aware fault diagnosis with metric-correlation models in enterprise software systems,"The normal operation of enterprise software systems can be modeled by stable correlations between various system metrics; errors are detected when some of these correlations fail to hold. The typical approach to diagnosis (i.e., pinpoint the faulty component) based on the correlation models is to use the Jaccard coefficient or some variant thereof, without reference to system structure, dependency data, or prior fault data. In this paper we demonstrate the intrinsic limitations of this approach, and propose a solution that mitigates these limitations. We assume knowledge of dependencies between components in the system, and take this information into account when analyzing the correlation models. We also propose the use of the Tanimoto coefficient instead of the Jaccard coefficient to assign anomaly scores to components. We evaluate our new algorithm with a Trade6-based test-bed. We show that we can find the faulty components within top-3 components with the highest anomaly score in four out of nine cases, while the prior method can only find one.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5691319,no
Congestion control research of underwater acoustic networks,"Congestion control is the key technology to ensure quality of network service. In underwater acoustic communication networks an important research area is how to control network congestion. Once detect the network congestion, we should take measures to solve congestion and make network return to normal data transmission. In this paper, we use congestion controller to solve network congestion problems. The simulation is done by software platform of OPNET and the results show that the mechanism we studied is feasible. It can resolve the network congestion effectively, achieve higher network throughput and less data packet delay.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5691726,no
A Quasi-best Random Testing,"Random testing, having been employed in both hardware and software for a long time, is well known for its simplicity and straightforwardness, in which each test is selected randomly regardless of the tests previously generated. However, traditionally, it seems to be inefficient for its random selection of test patterns. Therefore, a new concept of quasi-best distance random testing is proposed in the paper to make it more effective in testing. The new idea is based on the fact that the distance between two adjacent selected test vectors in a test sequence would greatly influence the efficiency of fault testing. Procedures of constructing such a testing sequence are presented and discussed in detail. The new approach has shown its remarkable advantage of fitting in most circuits. Experimental results and mathematical analysis of efficiency are also given to assess the performances of the proposed approach.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5692214,no
A New Approach to Generating High Quality Test Cases,"High quality test cases can effectively detect software errors and ensure software quality. However, except the regular expression-based test generation method, test cases generated from other model-based test generation methods have not contain the whole information of the model, resulting in test inadequacy. And test cases derived from regular expression have the prohibited lengths that cause the sustainable increase of test cost. To obtain high quality test cases, we suggest a new method for test generation by way of regular expression decomposition. Unlike the previous model decomposition techniques, our method lays emphasis on information completeness after regular expression is decomposed. Based on two empirical assumptions, we propose two processes of regular expression decomposition and three decomposition rules. Then we perform a case study to demonstrate our approach. The results show that our approach generates high quality test cases as well as avoids the problem of test complexity.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5692225,no
A Taxonomy for the Analysis of Scientific Workflow Faults,"Scientific workflows generally involve the distribution of tasks to distributed resources, which may exist in different administrative domains. The use of distributed resources in this way may lead to faults, and detecting them, identifying them and subsequently correcting them remains an important research challenge. We introduce a fault taxonomy for scientific workflows that may help in conducting a systematic analysis of faults, so that the potential faults that may arise at execution time can be corrected (recovered from). The presented taxonomy is motivated by previous work [4], but has a particular focus on workflow environments (compared to previous work which focused on Grid-based resource management) and demonstrated through its use in Weka4WS.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5692507,no
SQ^(2)E: An Approach to Requirements Validation with Scenario Question,"Adequate requirements validation could prevent errors from propagating into later development phase, and eventually improve the quality of software systems. However, often validating textual requirements is difficult and error prone. We develop a feedback-based requirements validation methodology that provides an interactive and systematic way to validate a requirements model. Our approach is based on the notion of querying a model, which is built from a requirements specification, with scenario questions, in order to determine whether the model's behavior satisfies the given requirements. To investigate feasibility of our approach, we implemented a Scenario Question Query Engine (SQ<sup>2</sup>E), which uses scenario questions to query a model, and performed a preliminary case study using a real-world application. The results show that the approach we proposed was effective in detecting both expected and unexpected behaviors in a model. We believe that our approach could improve the quality of requirements and ultimately the quality of software systems.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5693178,no
Quality Attributes Assessment for Feature-Based Product Configuration in Software Product Line,"Product configuration based on a feature model in software product lines is the process of selecting the desired features based on customers' requirements. In most cases, application engineers focus on the functionalities of the target product during product configuration process whereas the quality attributes are handled until the final product is produced. However, it is costly to fix the problem if the quality attributes have not been considered in the product configuration stage. The key issue of assessing a quality attribute of a product configuration is to measure the impact on a quality attribute made by the set of functional variable features selected in a configuration. Current existing approaches have several limitations, such as no quantitative measurements provided or requiring existing valid products and heavy human effort for the assessment. To overcome theses limitations, we propose an Analytic Hierarchical Process (AHP) based approach to estimate the relative importance of each functional variable feature on a quality attribute. Based on the relative importance value of each functional variable feature on a quality attribute, the level of quality attributes of a product configuration in software product lines can be assessed. An illustrative example based on the Computer Aided Dispatch (CAD) software product line is presented to demonstrate how the proposed approach works.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5693189,no
An Interaction-Pattern-Based Approach to Prevent Performance Degradation of Fault Detection in Service Robot Software,"In component-based robot software, it is crucial to monitor software faults and deal with them on time before they lead to critical failures. The main causes of software failures include limited resources, component-interoperation mismatches, and internal errors of components. Message-sniffing is one of the popular methods to monitor black-box components and handle these types of faults during runtime. However, this method normally causes some performance problems of the target software system because the fault monitoring and detection process consumes a significant amount of resources of the target system. There are three types of overheads that cause the performance degradation problems: frequent monitoring, transmission of a large amount of monitoring-data, and the processing time for fault analysis. In this paper, we propose an interaction-pattern-based approach to reduce the performance degradation caused by fault monitoring and detection in component-based service robot software. The core idea of this approach is to minimize the number of messages to monitor and analyze in detecting faults. Message exchanges are formalized as interaction patterns which are commonly observed in robot software. In addition, important messages that need to be monitored are identified in each of the interaction patterns. An automatic interaction pattern-identification method is also developed. To prove the effectiveness of our approach, we have conducted a performance simulation. We are also currently applying our approach to silver-care robot systems.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5693201,no
Testing Inter-layer and Inter-task Interactions in RTES Applications,"Real-time embedded systems (RTESs) are becoming increasingly ubiquitous, controlling a wide variety of popular and safety-critical devices. Effective testing techniques could improve the dependability of these systems. In this paper we present an approach for testing RTESs, intended specifically to help RTES application developers detect faults related to functional correctness. Our approach consists of two techniques that focus on exercising the interactions between system layers and between the multiple user tasks that enact application behaviors. We present results of an empirical study that shows that our techniques are effective at detecting faults.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5693202,no
An Automatic Testing Approach for Compiler Based on Metamorphic Testing Technique,"Compilers play an important role in software development, and it is quite necessary to perform abundant testing to ensure the correctness of compilers. A critical task in compiler testing is to validate the semantic-soundness property which requires consistence between semantics of source programs and behavior of target executables. For validating this property, one main challenging issue is generation of a test oracle. Most existing approaches fall into two main categories when dealing with this issue: reference-based approaches and assertion-based approaches. All these approaches have their weakness when new programming languages are involved or test automation is required. To overcome the weakness in the existing approaches, we propose a new automatic approach for testing compiler. Our approach is based on the technique of metamorphic testing, which validates software systems via so-called metamorphic relations. We select the equivalence-preservation relation as the metamorphic relation and propose an automatic metamorphic testing framework for compiler. We also propose three different techniques for automatically generating equivalent source programs as test inputs. Based on our approach, we developed a tool called Mettoc. Our mutation experiments show that Mettoc is effective to reveal compilers' errors in terms of the semantic-soundness property. Moreover, the empirical results also reveal that simple approaches for constructing test inputs are not weaker than complicated ones in terms of fault-detection capability. We also applied Mettoc in testing a number of open source compilers, and two real errors in GCC-4.4.3 and UCC-1.6 respectively have been detected by Mettoc.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5693203,no
Combinatorial Testing with Shielding Parameters,"Combinatorial testing is an important approach to detecting interaction errors for a system with several parameters. Existing research in this area assumes that all parameters of the system under test are always effective. However, in many realistic applications, there may exist some parameters that can disable other parameters in certain conditions. These parameters are called shielding parameters. Shielding parameters make test cases generated by the existing test model, which uses the Mixed Covering Array (MCA), fail in exposing some potential errors that should be detected. In this paper, the Mixed Covering Array with Shielding parameters (MCAS) is proposed to describe such problems. Then test cases can be generated by constructing MCAS's in three different approaches. According to the experimental results, our test model can generate satisfactory test cases for combinatorial testing with shielding parameters.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5693204,no
Evaluating Mutation Testing Alternatives: A Collateral Experiment,"Mutation testing while being a successful fault revealing technique for unit testing, it is a rather expensive one for practical use. To bridge these two aspects there is a need to establish approximation techniques able to reduce its expenses while maintaining its effectiveness. In this paper several second order mutation testing strategies are introduced, assessed and compared along with weak mutation against strong. The experimental results suggest that they both constitute viable alternatives for mutation as they establish considerable effort reductions without greatly affecting the test effectiveness. The experimental assessment of weak mutation suggests that it reduces significantly the number of the produced equivalent mutants on the one hand and that the test criterion it provides is not as weak as is thought to be on the other. Finally, an approximation of the number of first order mutants needed to be killed in order to also kill the original mutant set is presented. The findings indicate that only a small portion of a set of mutants needs to be targeted in order to be killed while the rest can be killed collaterally.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5693206,no
Using Faults-Slip-Through Metric as a Predictor of Fault-Proneness,"Background: The majority of software faults are present in small number of modules, therefore accurate prediction of fault-prone modules helps improve software quality by focusing testing efforts on a subset of modules. Aims: This paper evaluates the use of the faults-slip-through (FST) metric as a potential predictor of fault-prone modules. Rather than predicting the fault-prone modules for the complete test phase, the prediction is done at the specific test levels of integration and system test. Method: We applied eight classification techniques, to the task of identifying fault prone modules, representing a variety of approaches, including a standard statistical technique for classification (logistic regression), tree-structured classifiers (C4.5 and random forests), a Bayesian technique (Naive Bayes), machine-learning techniques (support vector machines and back-propagation artificial neural networks) and search-based techniques (genetic programming and artificial immune recognition systems) on FST data collected from two large industrial projects from the telecommunication domain. Results: Using area under the receiver operating characteristic (ROC) curve and the location of (PF, PD) pairs in the ROC space, the faults slip-through metric showed impressive results with the majority of the techniques for predicting fault-prone modules at both integration and system test levels. There were, however, no statistically significant differences between the performance of different techniques based on AUC, even though certain techniques were more consistent in the classification performance at the two test levels. Conclusions: We can conclude that the faults-slip-through metric is a potentially strong predictor of fault-proneness at integration and system test levels. The faults-slip-through measurements interact in ways that is conveniently accounted for by majority of the data mining techniques.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5693218,no
Clustering Performance on Evolving Data Streams: Assessing Algorithms and Evaluation Measures within MOA,"In today's applications, evolving data streams are ubiquitous. Stream clustering algorithms were introduced to gain useful knowledge from these streams in real-time. The quality of the obtained clusterings, i.e. how good they reflect the data, can be assessed by evaluation measures. A multitude of stream clustering algorithms and evaluation measures for clusterings were introduced in the literature, however, until now there is no general tool for a direct comparison of the different algorithms or the evaluation measures. In our demo, we present a novel experimental framework for both tasks. It offers the means for extensive evaluation and visualization and is an extension of the Massive Online Analysis (MOA) software environment released under the GNU GPL License.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5693462,no
Online Dynamic Control of Secondary Cooling for the Continuous Casting Process,"Aiming to reduce the occurrence of the surface and internal defects in the products, a dynamic secondary cooling control system was developed. The purpose of the system was to keep the surface temperature of the strand constant regardless of changes in casting speed. To accurately predict and control temperature in real time during the continuous casting process, A fast, accurate transient solidification and heat transfer model that serves as a software sensor was developed, which provide feedback to a control system. The control methodology and software suitable for online control of continuous casting cooling process were also designed. In order to maintain the target temperature profile throughout the steel, the new software system continuously read the operating conditions and adjusts the spray-water flow rates in the secondary cooling zone of the caster. The control system is demonstrated by simulation and the results show that the control system is capable of running in real time on billet caster.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5693731,no
Design of Control System for Intelligent Coater of Sealant,"The paper introduces a control system design of a new type of coater of sealant, and details the function and principle of the system, and describes the software architecture and protection system of the system. The coater of sealant is one of the most important part in the gearbox assembly line . Besides the functions of automatic glue and detect automatically to give an alarm, it also can communicate with assembly supervise system by internet, thereby you can manage and control the system using network. through using Trio motion coordinator, servo motor etc, it provides quality guarantee on the safe side for gearbox glue. Design of the protection system for various possible malfunction, can avoid damaging the coating mouth, and prevent to destroy the motor, and increase system stability.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5693740,no
Design of a Image Acquisition System with High Dynamic Range,"This paper presents a multi-sensor image acquisition platform which combines with hardware and software and can adjusts the dynamic range of sensor according to changes in light source. The hardware mainly consiste of image sensor MT9V032 and FPGA, and use the DMA in PCIE bus to obtain high-speed data transmission. Based on the High Dynamic Range characteristic of MT9V032, we propose a algorithm for determining image quality to track changes in light and consequently adjust the dynamic range and photoelectric conversion rate of image sensor. It can enhance the contrast of interested area and improve the image quality.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5694186,no
LavA: An Open Platform for Rapid Prototyping of MPSoCs,"Configurable hardware is becoming increasingly powerful and less expensive. This allows embedded system developers to exploit hardware parallelism in order to improve real time properties and energy efficiency. However, hardware design, even if performed using high-level hardware description languages, is error-prone and time consuming, especially when designing complex heterogeneous multiprocessor systems. To reduce the time to market for such systems, it is necessary to support the designer with a flexible workflow and methods for efficient reuse of existing components. In software engineering, this is enabled by using model-driven design flows and tools for configuration. In this paper, we describe LavA, a system which adapts these concepts to hardware design. By providing a streamlined toolchain and workflow to rapidly prototype complex, heterogeneous multiprocessor systems-on-chip based on a model-driven approach, developers can reduce turnaround times in design as well as design space exploration.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5694293,no
VirtCFT: A Transparent VM-Level Fault-Tolerant System for Virtual Clusters,"A virtual cluster consists of a multitude of virtual machines and software components that are doomed to fail eventually. In many environments, such failures can result in unanticipated, potentially devastating failure behavior and in service unavailability. The ability of failover is essential to the virtual cluster's availability, reliability, and manageability. Most of the existing methods have several common disadvantages: requiring modifications to the target processes or their OSes, which is usually error prone and sometimes impractical; only targeting at taking checkpoints of processes, not whole entire OS images, which limits the areas to be applied. In this paper we present VirtCFT, an innovative and practical system of fault tolerance for virtual cluster. VirtCFT is a system-level, coordinated distributed checkpointing fault tolerant system. It coordinates the distributed VMs to periodically reach the globally consistent state and take the checkpoint of the whole virtual cluster including states of CPU, memory, disk of each VM as well as the network communications. When faults occur, VirtCFT will automatically recover the entire virtual cluster to the correct state within a few seconds and keep it running. Superior to all the existing fault tolerance mechanisms, VirtCFT provides a simpler and totally transparent fault tolerant platform that allows existing, unmodified software and operating system (version unawareness) to be protected from the failure of the physical machine on which it runs. We have implemented this system based on the Xen virtualization platform. Our experiments with real-world benchmarks demonstrate the effectiveness and correctness of VirtCFT.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5695597,no
Cross-Layer Design to Merge Structured P2P Networks over MANET,"Peer-to-peer (P2P) network is an alternative of client/server system for sharing resources, e.g. files. P2P network is a robust, distributed and fault tolerant architecture. There are basic two types of P2P networks, structured P2P network and unstructured P2P network. Each of them has its own applications and advantages. Due recent advances in wireless and mobile technology, the P2P network can be deployed over mobile ad hoc network (MANET). We consider the scenarios of P2P network over MANET where all nodes are not the members of P2P network. Due to limited radio range and the mobility of nodes in MANET, there can occur network partition and merging of networks in the physical network. This can also lead to P2P network partition and merging at overlay layer. When two physical networks merge by coming into communication range of each other then their P2P networks would not be connected at overlay layer. Because P2P network operates at application layer as an overlay network. That is their P2P networks are connected in physical network but these P2P networks are disconnected at overlay layer. To detect this situation and merge these P2P networks at overlay layer, we extend the ODACP, an address auto-configuration protocol. Then we propose an approach to efficiently merge P2P networks such that routing traffic is minimized. Considering limited radio range and mobility of nodes, the simulation results shows that CAN over MANET performs better as compared to Chord over MANET in term of routing traffic and false-negative ratio.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5695695,no
Development on surface defect holes inspection based on image recognition,"A novel method of surface defect holes inspection was proposed based on both 2D and 3D image processing & recognition. In this method, the first step is to detect the holes in the binary image converted by the 3D image which is scanned by a 3D laser scanner, and the second step is to confirm the defect holes by dimension calculation using the data of the scanned 3D image. The software is developed in MATLAB.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5695748,no
A very fast unblocking scheme for distance protection to detect symmetrical faults during power swings,"Power swing blocking function in distance relays is necessary to distinguish between a power swing and a fault. However the distance relay should be fast and reliably unblocked if any fault occurs during a power swing. Although unblocking the relay under asymmetrical fault conditions is straightforward based on detecting the zero- or negative-sequence component of current but symmetrical fault detection during a power swing presents a challenge since there is no unbalancing. This paper presents a very fast detection method used to detect symmetrical faults occurring during a power swing. Based on a 50 Hz component getting on three-phase active power after symmetrical fault inception and using Fast Fourier Transform (FFT), the proposed detection method can reliably and quickly detect symmetrical faults occurring during power swing in one power cycle, i.e. 0.02 second. This detection method is easy to set and immune to the fault inception time and fault location. Power swing and fault conditions are simulated by using software PSCAD/EMTDC<sup></sup>. FFT is performed by using On-Line Frequency Scanner block included in the software.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5697162,no
Power quality problem classification based on Wavelet Transform and a Rule-Based method,"This paper describes a Wavelet Transform and Rule-Based method for detection and classification of various events of power quality disturbances. In this model, wavelet Multi-Resolution Analysis (MRA) technique was used to decompose the signal into its various details and approximation signals, and unique features from the 1<sup>st</sup>, 4<sup>th</sup>, 7<sup>th</sup> and 8<sup>th</sup> level detail are obtained as criteria for classifying the type of disturbance occurred. These features and together with the duration of disturbance of occurrence obtained from 1<sup>st</sup> level of detail, they form the criteria for a Rule-Based software algorithm for detecting different kinds of power quality disturbances effectively. It is presented in this paper that the choice of sampling frequency is important since it affects the average energy profile of the details and eventually may cause error in detection of power quality disturbances. The model is tested by using MATLAB toolbox. The simulation produces satisfactory result in identifying the disturbance and proof that it is possible to use this model for power disturbance classification. Since the method can reduce the number of parameters needed in classification, less memory space and computing time are required for its implementation. Thus it stands up to be a suitable model to be used in real time implementation through a dsPIC-based embedded system.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5697666,no
A proposed Genetic Algorithm to optimize service restoration in electrical networks with respect to the probability of transformers failure,"Power system reliability, stability and efficiency are the most important issues to insure continuously feeding of customers. However in process of time, system will be age and the probability of failures will increase and faults inevitably will occur. When a fault occurs, the first reaction is isolation of the faulty area, then with aid of software and/or skillful person quick restoration is essentially needed. To minimize the out-of-service area and activity time of restoration many methods are suggested depend on objectives and constraints of restoration strategy. In many researches a Genetic Algorithm is employed as a powerful tool to solve this multi-objective, multi-constraint optimization problem. Out-of-service area minimization, reduce the number of switching operation and minimizing the minimum electrical power loss in restored system are the prior objectives of restoration plan. In this paper, as transformers are the most expensive and more effective equipments in the electrical network, failure probability increasing is introduced as a new constraint in genetic algorithm by authors. Expected results of this new algorithm should lead to a new plan of restoration in permissible ranges of transformer loading in respect of their age, previous experienced faults and condition monitoring.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5697701,no
How dynamic is the Grid? Towards a quality metric for Grid information systems,"Grid information systems play a core role in today's production Grid Infrastructures. They provide a coherent view of the Grid services in the infrastructure while addressing the performance, robustness and scalability issues that occur in dynamic, large-scale, distributed systems. Quality metrics for Grid information systems are required in order to compare different implementations and to evaluate suggested improvements. This paper proposes the adoption of a quality metric, first used in the domain of Web search, to measure the quality of Grid information systems with respect to their information content. The application of this metric requires an understanding of the dynamic nature of Grid information. An empirical study based on information from the EGEE Grid infrastructure is carried out to estimate the frequency of change for different types of Grid information. Using this data, the proposed metric is assessed with regards to its applicability to measuring the quality of Grid information systems.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5697957,no
Analysis and modeling of time-correlated failures in large-scale distributed systems,"The analysis and modeling of the failures bound to occur in today's large-scale production systems is invaluable in providing the understanding needed to make these systems fault-tolerant yet efficient. Many previous studies have modeled failures without taking into account the time-varying behavior of failures, under the assumption that failures are identically, but independently distributed. However, the presence of time correlations between failures (such as peak periods with increased failure rate) refutes this assumption and can have a significant impact on the effectiveness of fault-tolerance mechanisms. For example, the performance of a proactive fault-tolerance mechanism is more effective if the failures are periodic or predictable; similarly, the performance of checkpointing, redundancy, and scheduling solutions depends on the frequency of failures. In this study we analyze and model the time-varying behavior of failures in large-scale distributed systems. Our study is based on nineteen failure traces obtained from (mostly) production large-scale distributed systems, including grids, P2P systems, DNS servers, web servers, and desktop grids. We first investigate the time correlation of failures, and find that many of the studied traces exhibit strong daily patterns and high autocorrelation. Then, we derive a model that focuses on the peak failure periods occurring in real large-scale distributed systems. Our model characterizes the duration of peaks, the peak inter-arrival time, the inter-arrival time of failures during the peaks, and the duration of failures during peaks; we determine for each the best-fitting probability distribution from a set of several candidate distributions, and present the parameters of the (best) fit. Last, we validate our model against the nineteen real failure traces, and find that the failures it characterizes are responsible on average for over 50% and up to 95% of the downtime of these systems.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5697961,no
Adaptively detecting changes in Autonomic Grid Computing,"Detecting the changes is the common issue in many application fields due to the non-stationary distribution of the applicative data, e.g., sensor network signals, web logs and grid-running logs. Toward Autonomic Grid Computing, adaptively detecting the changes in a grid system can help to alarm the anomalies, clean the noises, and report the new patterns. In this paper, we proposed an approach of self-adaptive change detection based on the Page-Hinkley statistic test. It handles the non-stationary distribution without the assumption of data distribution and the empirical setting of parameters. We validate the approach on the EGEE streaming jobs, and report its better performance on achieving higher accuracy comparing to the other change detection methods. Meanwhile this change detection process could help to discover the device fault which was not claimed in the system logs.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5698017,no
Validation of CK Metrics for Object Oriented Design Measurement,"Since object oriented system is becoming more pervasive, it is necessary that software engineers have quantitative measurements for accessing the quality of designs at both the architectural and components level. These measures allow the designer to access the software early in the process, making changes that will reduce complexity and improve the continuing capability of the product. Object oriented design metrics is an essential part of software engineering. This paper presents a case study of applying design measures to assess software quality. Six Java based open source software systems are analyzed using CK metrics suite to find out quality of the system and possible design faults that will reversely affect different quality parameters such as reusability, understandability, testability, maintainability. This paper also presents general guidelines for interpretation of reusability, understandability, testability, maintainability in the context of selected projects.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5698406,no
Finite Element Simulation for the Surface Flaw in Tube Open-die Cold Extrusion Process,"Numerical simulated for tube open-die cold extrusion process using finite element software Deform-3D. Analyzed the influence rule of the surface crack defects on the tube under the processing parameters which is half die angle, friction coefficient in open-die cold extrusion process. The simulation result shows that the closer the central of the tube roughcast the bigger the inner and outer of the maximum speed difference. So it can determine that the surface crack defects are prone at the central of the tube roughcast. It would be bring surface crack defects when the half die angle and the friction coefficient increase. So some necessary measures are taken to effectively avoid the surface crack defects appearance. Advisably decrease the half die angle, ensure favorable superficial treatment and lubrication are the effective measures.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5701266,no
The Equipment Development of Detecting the Beer Bottles' Thickness in Real Time,"In recent years, the requirement of beer bottle quality becomes ever-strict, so the annual detection can't satisfy the requirement of a modern industry, in this paper, we design a equipment which can detect the beer bottles' thickness in real time. The measurement system takes the chip computer W77E58 as the core, and takes the theory of ultrasonic measurement as the basis, we can observe directly the thickness of the detected beer bottle and judge the bottle is qualified or not by the visualization software. Through the analysis of experimental data, ultrasonic testing is relatively stable, it can meet the requirement of modern industry.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5701499,no
A Short-Term Prediction for QoS of Web Service Based on Wavelet Neural Networks,"The prediction will play a positive role when people need choose the best Web Service from numerous services, so a study on predicting QoS of Web Service will be shown in this paper. Concretely, the structure of wavelet neural networks (WNN) and a related algorithm will be introduced in the paper. Based on this, WNN is applied to predict the QoS of Web Service and the functions of the MATLAB toolbox are adopted to create a network model for QoS prediction. Finally the simulation experiments will prove that using WNN to predict the QoS of Web Service is more effective than common back propagation (BP) neural networks.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5701974,no
A Bayesian Based Method for Agile Software Development Release Planning and Project Health Monitoring,"Agile software development (ASD) techniques are iteration based powerful methodologies to deliver high quality software. To ensure on time high quality software, the impact of factors affecting the development cycle should be evaluated constantly. Quick and precise factor evaluation results in better risk assessment, on time delivery and optimal use of resources. Such an assessment is easy to carry out for a small number of factors. However, with the increase of factors, it becomes extremely difficult to assess in short time periods. We have designed and developed a project health measurement model to evaluate the factors affecting software development of the project. We used Bayesian networks (BNs) as an approach that gives such an estimation. We present a quantitative model for project health evaluation that helps decision makers make the right decision early to amend any discrepancy that may hinder on time and high quality software delivery.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5702095,no
A Decentralized Approach for Monitoring Timing Constraints of Event Flows,"This paper presents a run-time monitoring framework to detect end-to-end timing constraint violations of event flows in distributed real-time systems. The framework analyzes every event on possible event flow paths and automatically inserts timing fault checks for run-time detection. When the framework detects a timing violation, it provides users with the event flow's run-time path and the time consumption of each participating software module. In addition, it invokes a timing fault handler according to the timing fault specification, which allows our approach to aid the monitoring and management of the deployed systems. The experimental results show that the framework correctly detects timing constraint with insignificant overhead and provides related diagnostic information.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5702242,no
Analysis of in-loop denoising in lossy transform coding,"When compressing noisy image sequences, the compression efficiency is limited by the noise amount within these image sequences as the noise part cannot be predicted. In this paper, we investigate the influence of noise within the reference frame on lossy video coding of noisy image sequences. We estimate how much noise is left within a lossy coded reference frame. Therefore we analyze the transform and quantization step inside a hybrid video coder, specifically H.264/AVC. The noise power after transform, quantization, and inverse transform is calculated analytically. We use knowledge of the noise power within the reference frame in order to improve the inter frame prediction. For noise filtering of the reference frame, we implemented a simple denoising algorithm inside the H.264/AVC reference software JM15.1. We show that the bitrate can be decreased by up to 8.1% compared to the H.264/AVC standard for high resolution noisy image sequences.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5702584,no
Towards Identifying the Best Variables for Failure Prediction Using Injection of Realistic Software Faults,"Predicting failures at runtime is one of the most promising techniques to increase the availability of computer systems. However, failure prediction algorithms are still far from providing satisfactory results. In particular, the identification of the variables that show symptoms of incoming failures is a difficult problem. In this paper we propose an approach for identifying the most adequate variables for failure prediction. Realistic software faults are injected to accelerate the occurrence of system failures and thus generate a large amount of failure related data that is used to select, among hundreds of system variables, a small set that exhibits a clear correlation with failures. The proposed approach was experimentally evaluated using two configurations based on Windows XP. Results show that the proposed approach is quite effective and easy to use and that the injection of software faults is a powerful tool for improving the state of the art on failure prediction.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5703221,no
A Software Accelerated Life Testing Model,"Software system developed for a specific user under contract undergoes a period of testing by the user before acceptance. This is known as user acceptance testing and is useful to debug the software in the user's operational circumstance. In this paper we first present a simple non-homogeneous Poisson process (NHPP)-based software reliability model to assess the quantitative software reliability under the user acceptance test, where the idea of an accelerated life testing model is introduced to represent the user's operational phase and to investigate the impact of user's acceptance test. This idea is applied to the reliability assessment of web applications in a different testing environment, where two stress tests with normal and higher workload conditions are executed in parallel. Through numerical examples with real software fault data observed in actual user acceptance and stress tests, we show the applicability of the software accelerated life testing model to two different software testing schemes.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5703231,no
Two Efficient Software Techniques to Detect and Correct Control-Flow Errors,"This paper proposes two efficient software techniques, Control-flow and Data Errors Correction using Data-flow Graph Consideration (CDCC) and Miniaturized Check-Pointing (MCP), to detect and correct control-flow errors. These techniques have been implemented based on addition of redundant codes in a given program. The creativity applied in the methods for online detection and correction of the control-flow errors is using data-flow graph alongside of using control-flow graph. These techniques can detect most of the control-flow errors in the program firstly, and next can correct them, automatically. Therefore, both errors in the control-flow and program data which is caused by control-flow errors can be corrected, efficiently. In order to evaluate the proposed techniques, a post compiler is used, so that the techniques can be applied to every 8086 binaries, transparently. Three benchmarks quick sort, matrix multiplication and linked list are used, and a total of 5000 transient faults are injected on several executable points in each program. The experimental results demonstrate that at least 93% and 89% of the control-flow errors can be detected and corrected without any data error generation by the CDCC and MCP, respectively. Moreover, the strength of these techniques is significant reduction in the performance and memory overheads in compare to traditional methods, for as much as remarkable correction abilities.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5703238,no
Automatic Static Fault Tree Analysis from System Models,"The manual development of system reliability models such as fault trees could be costly and error prone in practice. In this paper, we focus on the problems of some traditional dynamic fault trees and present our static solutions to represent dynamic relations such as functional and sequential dependencies. The implementation of a tool for the automatic synthesis of our static fault trees from SysML system models is introduced.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5703257,no
PBTrust: A Priority-Based Trust Model for Service Selection in General Service-Oriented Environments,"How to choose the best service provider (agent), which a service consumer can trust in terms of the quality and success rate of the service in an open and dynamic environment, is a challenging problem in many service-oriented applications such as Internet-based grid systems, e-trading systems, as well as service-oriented computing systems. This paper presents a Priority-Based Trust (PBTrust) model for service selection in general service-oriented environments. The PBTrust is robust and novel from several perspectives. (1) The reputation of a service provider is derived from referees who are third parties and had interactions with the provider in a rich context format, including attributes of the service, the priority distribution on attributes and a rating value for each attribute from a third party, (2) The concept of 'Similarity' is introduced to measure the difference in terms of distributions of priorities on attributes between requested service and a refereed service in order to precisely predict the performance of a potential provider on the requested service, (3) The concept of general performance of a service provider on a service in history is also introduced to improve the success rate on the requested service. The experimental results can prove that PBtrust has a better performance than that of the CR model in a service-oriented environment.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5703618,no
SProt - from local to global protein structure similarity,"Similarity search in protein databases is one of the most essential issues in proteomics. With the growing number of experimentally solved protein structures, the focus shifted from sequence to structure. The area of structure similarity forms a big challenge since even no standard definition of optimal similarity exists in the field. In this paper, we propose a protein structure similarity method called SProt. SProt concentrates on high-quality modeling of local similarity in the process of feature extraction. SProt's features are based on spherical spatial neighborhood where similarity can be well defined. On top of the partial local similarities, global measure assessing similarity to a pair of protein structures is built. SProt outperforms other methods in classification accuracy, while it is at least comparable to the best existing solutions in terms of precision-recall or quality of alignment.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5703785,no
Joint H.264/SVC-MIMO rate control for wireless video transmission,"In this research, we propose a novel joint H.264/SVC-MIMO rate control (RC) algorithm for video compression and transmission over Multiple Input Multiple Output (MIMO) wireless systems. We first present system architecture for H.264/SVC compression and transmission over MIMO systems. Then, we use a packet-level two-state Markov model to estimate MIMO channel states and predict the number of retransmitted bits in the presence of automatic repeat request (ARQ). Finally, an efficient joint rate controller is proposed to regulate the output bit rate of each layer. Our extensive simulation results demonstrate that, our algorithm can respond to the sudden bandwidth fluctuation of MIMO channels, outperform JVT-W043 rate control algorithm, adopted in the H.264/SVC reference software, by providing more accurate output bit rate, reducing buffer overflow, depressing quality fluctuations, and finally, improves the overall coding quality.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5704637,no
Reliability models and open source software: An empirical study,"Open source communities have successfully developed a great deal of software. Due to its free availability and highly secure operating system environment it is promoted by most of the countries all over the world. India also started contributing in this field. Government of India is also promoting usage of Open Source softwares due to its economical feasibility and security. Due to its huge demand in software field a major concern is of its reliability, which is defined as the probability of failure-free software operation for a specified period of time in a given environment. In this paper on the basis of literature survey of open source and reliability, facts regarding Open Source softwares as well as different reliability concepts are elaborated. Important Models to estimate reliability are studied and then main factors are explained here. This paper will be helpful for research scholars doing their research in software reliability.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5705779,no
Sequence-based techniques for black-box test case prioritization for composite service testing,"Web service may evolve autonomously, making peer web services in the same service composition uncertain as to whether the evolved behaviors may still be compatible to its originally collaborative agreement. Although peer services may wish to conduct regression testing to verify the original collaboration, the source code of the former service can be inaccessible to them. Traditional code-based regression testing strategies are inapplicable to web services. In this paper, we formulate new test case prioritization strategies using sequences in XML messages to reorder regression test cases for composite web services, against the tag based techniques given in and reveal how the test cases use the interface specifications of the composite services. The results were evaluated experimentally and the results show that the new techniques can have a high probability of outperforming random ordering and the techniques given in.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5705784,no
Towards adaptive web services QoS prediction,"Quality of Service (QoS) has been widely used to support dynamic Web Service (WS) selection and composition. Due to the volatile nature of QoS parameters, QoS prediction has been put forward to understand the trend of QoS data volatility and estimate QoS values in dynamic environments. In order to provide adaptive and effective QoS prediction, we propose a WS QoS prediction approach, named WS-QoSP, based on the technique of forecast combination. Different from the existing QoS prediction approaches that choose a most feasible forecasting model and predict relying on this best model, WS-QoSP selects multiple potential forecasting models, combines the results of the selected models to optimize the overall forecast accuracy. Results of real data experiments demonstrate the diversified forecast accuracy gains by using WS-QoSP.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5707146,no
Detection of spoofed GPS signals at code and carrier tracking level,"Due to the large amount of different new applications based on GNSS systems, the issue of interference monitoring is becoming an increasing concern in the satellite navigation community. Threats for GNSS can be classified as unintentional interference, jamming and spoofing. Among them, spoofing is more deceitful because the target receiver might not be able to detect the attack and consequently generate misleading position solutions. Different kind of spoofing attacks can be implemented depending on their complexity. The paper analyzes what is known as intermediate spoofing attack, by means of a spoofer device developed at the Navigation Signal Analysis and Simulation (NavSAS) laboratory. The work focuses on the spoofing detection, performed by implementing proper signal quality monitoring techniques at code and carrier tracking level.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5708016,no
A Hybrid Collaborative Filtering Algorithm Based on User-Item,"Collaborative filtering is one of the most important technologies in e-commerce recommendation system. Traditional similarity measure methods work poorly when the user rating data are extremely sparse. Aiming at this issue a hybrid collaborative filtering is proposed. This method used a novel similarity measure method to predict the target item rating and it fused the advantages of the user-based algorithm and item-based algorithm with the control factor . The experimental results show that this improved algorithm obviously enhances the recommended accuracy, and provide better recommendation quality.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5709077,no
Chest X-ray analysis for an active distributed E-health system with computer-aided diagnosis,"The quality of life of a country's citizens is much depended on its healthcare system. People have the right to know the status of their health. Healthcare providers need to know the medical histories of patients to offer better treatment. Therefore, demand for improvement in the access of healthcare information has been increased. This paper presents the design of an active distributed E-health system, which is scalable, and more advanced softwares can be easily added. Some works on chest X-ray analysis are presented to demonstrate the capabilities of the system as CAD tools for some chest diseases like congestive heart failure, lung collapse, etc. Experimental result obtained with an algorithm to detect early nodules for lung cancer and TP is very encouraging. Data mining and other artificial intelligent techniques may be used to make the system becoming more active and powerful expert system.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5711149,no
Wide area measurement based out-of-step detection technique,"The electrical power systems function as a huge interconnected network dispersed over a large area. A balance exist between generated and consumed power, any disturbance to this balance in the system caused due to change in load as well as faults and their clearance often results in electromechanical oscillations. As a result there is variation in power flow between two areas. This phenomenon is referred as Power Swing. This paper uses PMU data to measure the currents and voltages of the three phases of two buses connected to a 400 kV line. The measured data is then used for differentiating between a swing or fault condition, and if a swing is detected, to predict whether the swing is a stable or an unstable one. The performance of the method has been tested on a simulated system using PSCAD and MATLAB software.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5712569,no
The prediction of software aging trend based on user intention,"Owing to the limitation of traditional software aging trend prediction method that based on time and based on measurement in dealing with sudden large scale concurrent questions, this paper proposes a new software aging trend prediction method which is based on user intention. This method predicts the trend of software aging according to the quantity of user requests for each components during the moment of system operation, and the software aging damage with each component is requested once.The experiment indicates, compared with the measurement method, this method has highter accuracy in dealing with sudden large scale concurrent questions.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5713081,no
Diagnosing the root-causes of failures from cluster log files,"System event logs are often the primary source of information for diagnosing (and predicting) the causes of failures for cluster systems. Due to interactions among the system hardware and software components, the system event logs for large cluster systems are comprised of streams of interleaved events, and only a small fraction of the events over a small time span are relevant to the diagnosis of a given failure. Furthermore, the process of troubleshooting the causes of failures is largely manual and ad-hoc. In this paper, we present a systematic methodology for reconstructing event order and establishing correlations among events which indicate the root-causes of a given failure from very large syslogs. We developed a diagnostics tool, FDiag, to extract the log entries as structured message templates and uses statistical correlation analysis to establish probable cause and effect relationships for the fault being analyzed. We applied FDiag to analyze failures due to breakdowns in interactions between the Lustre file system and its clients on the Ranger supercomputer at the Texas Advanced Computing Center (TACC). The results are positive. FDiag is able to identify the dates and the time periods that contain the significant events which eventually led to the occurrence of compute node soft lockups.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5713159,no
Automating Coverage Metrics for Dynamic Web Applications,"Building comprehensive test suites for web applications poses new challenges in software testing. Coverage criteria used for traditional systems to assess the quality of test cases are simply not sufficient for complex dynamic applications. As a result, faults in web applications can often be traced to insufficient testing coverage of the complex interactions between the components. This paper presents a new set of coverage criteria for web applications, based on page access, use of server variables, and interactions with the database. Following an instrumentation transformation to insert dynamic tracking of these aspects, a static analysis is used to automatically create a coverage database by extracting and executing only the instrumentation statements of the program. The database is then updated dynamically during execution by the instrumentation calls themselves. We demonstrate the usefulness of our coverage criteria and the precision of our approach on the analysis of the popular internet bulletin board system PhpBB 2.0.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5714417,no
Effort-Aware Defect Prediction Models,"Defect Prediction Models aim at identifying error-prone modules of a software system to guide quality assurance activities such as tests or code reviews. Such models have been actively researched for more than a decade, with more than 100 published research papers. However, most of the models proposed so far have assumed that the cost of applying quality assurance activities is the same for each module. In a recent paper, we have shown that this fact can be exploited by a trivial classifier ordering files just by their size: such a classifier performs surprisingly good, at least when effort is ignored during the evaluation. When effort is considered, many classifiers perform not significantly better than a random selection of modules. In this paper, we compare two different strategies to include treatment effort into the prediction process, and evaluate the predictive power of such models. Both models perform significantly better when the evaluation measure takes the effort into account.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5714425,no
SQM 2010: Fourth International Workshop on System Quality and Maintainability,"Software is playing a crucial role in modern societies. Not only do people rely on it for their daily operations or business, but for their lives as well. For this reason, correct and consistent behaviour of software systems is a fundamental part of end user expectations. Additionally, businesses require cost-effective production, maintenance, and operation of their systems. Thus, the demand for good quality software is increasing and is setting it as a differentiator for the success or failure of a software product. In fact, high quality software is becoming not just a competitive advantage but a necessary factor for companies to be successful. The main question that arises now is how quality is measured. What, where and when we assess and assure quality, are still open issues. Many views have been expressed about software quality attributes, including maintainability, evolvability, portability, robustness, reliability, usability, and efficiency. These have been formulated in standards such as ISO/IEC-9126 and CMMI. However, the debate about quality and maintainability between software producers, vendors and users is ongoing, while organizations need the ability to evaluate the software systems that they use or develop from multiple angles.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5714449,no
InCode: Continuous Quality Assessment and Improvement,"While significant progress has been made over the last ten years in the research field of quality assessment, developers still can't take full advantage of the benefits of these new tools and technique. We believe that there at least two main causes for this lack of adoption: (i) the lack of integration in mainstream IDEs and (ii) the lack of support for a continuous (daily) usage of QA tools. In this context we created INCODE as an Eclipe plug in that would transform quality assessment and code inspections from a standalone activity, into a continuous, agile process, fully integrated in the development life-cycle. But INCODE not only assesses continuously the quality of Java systems, it also assists developers in taking restructuring decisions, and even supports them in triggering refactorings.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5714452,no
Predicting grade of prostate cancer using image analysis software,"The prognosis of prostate cancer is determined by using the Gleason grading system. This grading is done based upon the tissue pattern obtained from the tumor, after staining the biopsy with Heamatoxylin and Eosin (H&E). Presently, experienced pathologists manually grade on prostate cancers subjectively. The grading therefore depends upon the experience of the pathologists, quality of the staining and various other factors. To overcome this, an image analysis system is developed using MATLAB that can examine the biopsy image and grade it objectively. Size distribution of the sample image is utilized to recognize the pattern. The prediction is done based on the pattern of lumen, nuclei and the glandular organization in the representative areas of biopsy-image taken from a microscope. The results obtained show a remarkable accuracy and is closer to the manual grading scores. This Computer-Adaptive-Diagnosis (CAD) system may be used as a powerful adjunct for effectively diagnosing the prostate cancers and grading them.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5714621,no
An End-to-End Framework for Business Compliance in Process-Driven SOAs,"It is significant for companies to ensure their businesses conforming to relevant policies, laws, and regulations as the consequences of infringement can be serious. Unfortunately, the divergence and frequent changes of different compliance sources make it hard to systematically and quickly accommodate new compliance requirements due to the lack of an adequate methodology for system and compliance engineering. In addition, the difference of perception and expertise of multiple stakeholders involving in system and compliance engineering further complicates the analyzing, implementing, and assessing of compliance. For these reasons, in many cases, business compliance today is reached on aper-case basis by using ad hoc, hand-crafted solutions for specific rules to which they must comply. This leads in the long run to problems regarding complexity, understandability, and maintainability of compliance concerns in a SOA. To address the aforementioned challenges, we present in this invited paper a comprehensive SOA business compliance software framework that enables a business to express, implement, monitor, and govern compliance concerns.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5715316,no
The Q-ImPrESS Method -- An Overview,"The difficulty in evolving service-oriented architectures with extra-functional requirements seriously hinders the spread of this paradigm in critical application domains. The Q-ImPrESS method offsets this disadvantage by introducing a quality impact prediction, which allows software engineers to predict the consequences of alternative design decisions on the quality of software services and select the optimal architecture without having to resort to costly prototyping. The method takes a wider perspective on software quality by explicitly considering multiple quality attributes (performance, reliability and maintainability), and the typical trade-offs between these attributes. The benefit of using this approach is that it enables the creation of service-oriented systems with predictable end-to-end quality.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5715317,no
Classification of Software Defect Detected by Black-Box Testing: An Empirical Study,"Software defects which are detected by black box testing (called black-box defect) are very large due to the wide use of black-box testing, but we could not find a defect classification which is specifically applicable to them in existing defect classifications. In this paper, we present a new defect classification scheme named ODC-BD (Orthogonal Defect Classification for Black-box Defect), and we list the detailed values of every attribute in ODC-BD, especially the 300 detailed black-box defect type. We aim to help black-box defect analyzers and black-box testers improve their analysis and testing efficiency. The classification study is based on 1860 black-box defects collected from 39 industry projects and 2 open source projects. Furthermore, two empirical studies are included to validate the use of our ODC-BD. The results show that our ODC-BD can improve the efficiency of black-box testing and black-box defect analysis.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5718384,no
SXMTool A Tool for Stream X-Machine Testing,"One of the great benefits of using a Stream X-machine (SXM) to specify a system is its associated testing method. Under certain test conditions, this method produces a test suite that can determine the correctness of the implementation under test (IUT). However, the size of the test suite is generally very large, the manual test suite generation is very complex and error-prone. With the more and more application for SXM in test area, developing the automatic support tool is urgent. The paper introduces the algorithm of obtaining the key values and sets, and develops the tool SXMtool which supports the editing of SXM models, automatic generation of SXM test suite. An example of using the SXMtool is then given to present its function.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5718390,no
Application of Fuzzy FMECA in Gas Network Safety Assessment,"In order to provide a basis for the natural gas pipeline system to effectively perform a safe management, dangerous sources in this system are analyzed firstly, and a Fault Tree of serious accidents frequently occurring is then established by assessing the system with the fuzzy FMECA method, and some appropriate preventive measures may be finally taken in accordance with the Fault Tree and a FMECA table.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5718403,no
Development and application of data analysis software for transformers PD UWB RF location,"Ultra-Wideband (UWB) RF partial discharge (PD) location technique is a new method for PD sources location in power transformers, which is based on multiple sensors array detection and Huygens-Fresnel principle of PD electromagnetic radiation signal. In this paper, a classification algorithm of multi-PD sources has been proposed, which is based on dynamic search in the TDOA (time difference of arrival) sample space, it is able to distinguish multiple PD sources and complete the TDOA classification of every PD source automatically when there are multiple PD sources has been detected simultaneously. Based on the classification algorithm of multi-PD sources and location algorithm based on Huygens-Fresnel principle, a set of data analysis software used for PD UWB RF location system has been developed based on Labview8.5. It manages multiple test projects using Access Database, and establishes the TDOA sample space for every testing program, finishes the multiple PD sources classification and location calculation for every PD source finally. Meanwhile, multiple functions have been integrated into the software, such as signal processing, report generation, human interface design and so on. Finally, an illustrative example has proved that the software's validity.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5724026,no
Estimating design quality of digital systems via machine learning,"Although the term design quality of digital systems can be assessed from many aspects, the distribution and density of bugs are two decisive factors. This paper presents the application of machine learning techniques to model the relationship between specified metrics of high-level design and its associated bug information. By employing the project repository (i.e., high level design and bug repository), the resultant models can be used to estimate the quality of associated designs, which is very beneficial for design, verification and even maintenance processes of digital systems. A real industrial microprocessor is employed to validate our approach. We hope that our work can shed some light on the application of software techniques to help improve the reliability of various digital designs.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5724589,no
A Cognitive QoS Method Based on Parameter Sensitivity,"Different applications in the network have different sensitivity for the certain QoS parameters. The existing mechanisms cannot modify the packet loss policy exactly according to the needs of QoS parameters. Therefore, the network will not maximize its overall efficiency. This paper proposes a novel cognitive approach for QoS. It classifies the applications by various combinations of the QoS parameters. And it uses a cognitive layer to modify the loss probability of the packet. Simulation results prove that this approach can reduce the account of useless packets sent when the congestion occurs in the network. The overall efficiency of the useful data transmission can be improved as well. The method can be used by Internet of Things to improve its message processing and sensor data fusion to reduce energy consumption.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5724876,no
Using pattern detection techniques and refactoring to improve the performance of ASMOV,"One of the most important challenges in semantic Web is ontology matching. Ontology matching is a technology that enables semantic interoperability between structurally and semantically heterogeneous resources on the Web. Despite serious research efforts on ontology matching, matchers still suffers from severe problems with respect to the quality of matching results. Furthermore, Most of them take a lot of time for finding the correspondences. The aim of this paper is improving ontology matching results by adding the preprocessing phase for analyzing the input ontologies. This phase is added in order to solve problems caused by ontology diversity. We select one of the best matchers of Ontology Alignment Evaluation Initiative (OAEI) which is Automated Semantic Matching of Ontologies with Verification, called ASMOV. In preprocessing phase, some new patterns of ontologies are detected and then refactoring operations are used for reaching assimilated ontologies. Afterward, we applied ASMOV for testing our approach on both the original ontologies and their refactored counterparts. Experimental results show that these refactored ontologies are more efficient than the original unrepaired ones with respect to the standard evaluation measures i.e. Precision, Recall, and F-Measure.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5734164,no
Measuring testability of aspect oriented programs,"Testability design is an effective way to realize the fault detection and isolation. It becomes crucial in the case of Aspect Oriented designs where control flows are generally not hierarchical, but are diffuse and distributed over the whole architecture. In this paper, we concentrate on detecting, pinpointing and suppressing potential testability weaknesses of a UML Aspect-class diagram. The attribute significant from design testability is called class interaction: it appears when potentially concurrent client/supplier relationships between classes exist in the system. These interactions point out parts of the design that need to be improved, driving structural modifications or constraints specifications, to reduce the final testing effort. This paper does an extensive review on testability of aspect oriented software, and put forth some relevant information about class-level testability.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5735101,no
The research and development of comprehensive evaluation and management system for harmonic and negative-sequence,"Power quality interference sources generate a large number of harmonics and negative-sequence current into power grid in the process of using electricity. Harmonics and negative-sequence current not only affect power grid's safety and economical operation also cause interference to other normal users, which brings large threat and danger to the power grid and users. As an effective method to prevent and control power quality interference source, it is important to evaluate the harmonic and negative-sequence current at the installation part of power quality interference source, guide the user taking measures to control power quality in the design of current electricity-consummation. This paper describes the research and development of comprehensive evaluation and management system for harmonic and negative-sequence current. This software has the following functions: harmonic computation, comprehensive assessment of user harmonic and negative-sequence, filter design and checking, SVC measurement, harmonic source database management. This software models the system components, load and typical harmonic source under the CIGRE standard with graphical interface. It models the power supply network by the means of one-line diagram, completes the harmonic calculation by Monte-Carlo or Laguerre polynomial, and simulates the distribution of power system harmonics by statistical moment with harmonics power flow techniques. The evaluation of user's harmonic and negative-sequence based mainly on the GB, combined with the access point short-circuit capacity, power capacity and protocol capacity to calculate the limit value of harmonic and negative-sequence current that assigned to user. According to the typical user's harmonic emission level or measured value, we can calculate the value of harmonic current generated by user and execute assessment by comparing them. One way to design filter and carry on SVC evaluation is calculating by custom method. Another way is to cut the ex- - isting data into calculation and checking, and then the bus voltage waveform before and after inputting filter or SVC by graphical virtual operation can be obtained, and a rich and intuitive results reports and graphics can be generated at the same time. This software is easy to operate, and its calculation results are accurate. It is an effective tool to assess and manage power quality.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5736040,no
A hierarchical fault detection and recovery in a computational grid using watchdog timers,"Grid computing basically means applying the resources of individual computers in a network to focus on a single problem/task at the same time. But the disadvantage of this feature is that the computers which are actually performing the calculations might not be always trustworthy and may fail periodically. Hence larger the number of nodes in the grid, greater is the probability that a node fails. Hence in order to execute the workflows in a fault tolerant manner we go for fault tolerance and recovery strategies. This paper proposes a method in which the instantaneous snapshot of the local state of processes within each node is recorded. An efficient algorithm is introduced for the detection of the node failures using watch dog timers. For recovery we make use of divide and conquer algorithm that avoids redoing of already completed jobs, enabling faster recovery.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5738775,no
Inspection system for detecting defects in a transistor using Artificial neural network (ANN),"A machine vision system based on ANN for identification of defects occurred in transistor fabrication is presented in this paper. The developed intelligent system can identify commonly occurring errors in the transistor fabrication. The developed machine vision and ANN module is compared with the commercial MATLAB<sup></sup> software and found results were satisfactory. This work is broadly divided into four stages, namely intelligent inspection system, machine vision module, ANN module and Inspection expert system. In the first a system with a camera is developed to capture the various segments of the transistor. The second stage is the image processing stage, in this the captured bitmap format image of the transistor is filtered and its size is altered to an acceptable size for the developed ANN using Set Partitioning Hierarchical Tree (SPIHT). These modified data are given as input to the ANN in the third stage. Generalized ANN with Back propagation algorithm is used to inspect the transistor. The ANN is trained and the weight values are updated in such a way that the error in identification is the least possible. The output of ANN is the inspected report. The developed system is explained with a real time industrial application. Thus, the developed algorithms will solve most of the problems in identifying defects in a transistor.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5738806,no
Influence of voltage stability in power quality,"Simulations and analysis of voltage stability and its influence on power quality are presented in this article. The voltage stability concern to the capacity of the power system to maintain an appropriate voltage profile, both in normal operation and in the event of severe disruption. Methods of assessing stability used here are based on the use of algebraic equations obtained from the model of power flow (static methods). From the continuation power flow, PV curves are used to carry out stability analysis. Such analysis allows evaluating how close the system is of a voltage collapse. Softwares produced by CEPEL: ANAREDE<sup></sup> and PlotCEPEL<sup></sup> were used for simulation and analysis of results.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5739972,no
Design and development of a software for fault diagnosis in radial distribution networks,"This paper presents an on-line fault diagnosis software in primary distribution feeders. The software is written in DELPHI and C++ languages and its interaction with the operator is made in a very friendly environment. The input data are the currents of the feeder per phase, monitored only in the substation. An artificial immune system was developed using the negative selection algorithm to detect and classify the faults. The fault location was identified by a genetic algorithm which is triggered by negative selection algorithm. The main application of the software is to assist in the operation during a fault, and supervise the protection system. A 103-bus non-transposed real feeder is used to evaluate the proposed software. The results show that the software is effective for diagnosing all types of faults involving short-circuits and it has great potential for online applications.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5740045,no
Logical method for detecting faults by fault detection table,"Algebro-logic vector method for diagnosing faults of systems and their components based on the use a fault detection table and transactional graph, is proposed. The method allows decreasing the verification time of software model.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5742148,no
Color detection for vision machine defect inspection on electronic devices,"This paper presents a recent innovation introduced by Ismeca in our novel vision platform, NativeNET, for the detection of surface defects in electronic device packages due to decoloration and which could not detected before. Up to now, mainly due to cost and processing-time constraints, most of inspection vision systems were working with monochrome images. Moreover, there is a need from semiconductor packaging industry to be able to provide new smart inspection which can detect more defects.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5746681,no
Yield model for estimation of yield impact of semiconductor manufacturing equipment,"A yield model was developed allowing the calculation of yield using defect density data of manufacturing equipment. The approach allows studying impact of semiconductor manufacturing equipment on yield, to calculate and monitor yield during semiconductor manufacturing and predicting yield based on real time input of semiconductor manufacturing equipment regarding failures, defect density etc. The yield model bases on generic flows of manufacturing processes. The model assigns each functional layer a yield loss during the sequence of manufacturing steps. The yield model was implemented in a software code. The software was used to study yield impact of specific equipment for different technologies and products.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5750257,no
The alice data quality monitoring system,"ALICE (A Large Ion Collider Experiment) is a heavy-ion detector designed to study the physics of strongly interacting matter and the quark-gluon plasma at the CERN Large Hadron Collider (LHC). Due to the complexity of ALICE in terms of number of detectors and performance requirements, data quality monitoring (DQM) plays an essential role in providing an online feedback on the data being recorded. It intends to provide operators with precise and complete information to quickly identify and overcome problems, and, as a consequence, to ensure acquisition of high quality data. DQM typically involves the online gathering of data samples, their analysis by user-defined algorithms and the visualization of the monitoring results. In this paper, we illustrate the final design of the DQM software framework of ALICE, AMORE (Automatic Monitoring Environment), and its latest features and developments. We describe how this system is used to monitor the event data coming from the ALICE detectors allowing operators and experts to access a view of monitoring elements and to detect potential problems. Important features include the integration with the offline analysis and reconstruction framework and the interface with the electronic logbook that makes the monitoring results available everywhere through a web browser. Furthermore, we show the advantage of using multi-core processors through a parallel images/results production and the flexibility of the graphic user interface that gives to the user the possibility to apply filters and customize the visualization. We finally review the wide range of usage people make of this framework, from the basic monitoring of a single sub-detector to the most complex ones within the High Level Trigger farm or using the Prompt Reconstruction. We also describe the various ways of accessing the monitoring results. We conclude with our experience, after the LHC restart, when monitoring the data quality in a realworld and challenging environment.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5750364,no
Quality Assurance and Data Quality Monitoring for the ALICE Silicon Drift Detectors,"In this paper, the Quality Assurance (QA) and Data Quality Monitoring (DQM) of the ALICE Silicon Drift Detectors will be discussed. The Quality Assurance functionality has been implemented as part of AliRoot, the ALICE software framework, so as to use the same code in offline and online mode. The QA system manages the three sub-detectors of the Inner Tracking System (ITS) in a modular way, in order to run the Quality Assurance of all or just one of them, creating the QA distributions for the selected detector(s). The ITS-QA framework can also be used for the online monitoring of the sub-detectors thanks to its interface to the AMORE (Automatic MonitoRing Environment) Data Quality Monitoring (DQM) framework, in turn interfaced to the Data Acquisition System. The online mode is steered by a specific subdetector agent that makes use of functionality provided by AMORE to connect and receive events from the Data Acquisition System, invokes AliRoot code for their reconstruction and analysis and for filling the QA distributions and then handles these distributions to AMORE that publishes them to its Database. A dedicated GUI allows the operators to retrieve and display the subdetector QA distributions from the AMORE Database. The SDD QA and DQM are fully operational since the beginning of the ALICE data taking and are important tools to assess the data quality both in real time and in the offline analysis.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5750480,no
A complete data recording and reporting system for the EU commercial fishing fleets,"Fisheries authorities and scientists agree that the management of fisheries is dependent on good quality data, and that such data is of critical importance in the light of declining fish stocks, worldwide. Historically, however, reliable data has been largely unavailable, due to a culture of protecting catch data amongst fishers, fishing companies and even formal state-run offices, and also because data collection was paper-based, which is unreliable, prone to error, and lacking in suitable controls for ensuring data quality. Electronic, real-time reporting of vessel activity is expected to overcome many of these deficiencies and the EU has published regulations requiring fishing vessels to electronically record and report fishing activities. Olfish Dynamic Data Logger is a software solution that has been developed by the South African company, OLRAC, in order to meet EU regulations and to provide the EU fishing fleet with an EU-compliant electronic logbook. The development project sought to meet EU requirements as well as to improve the likelihood of improved data quality through a number of innovative features that offer significant value to the fishing community. This paper discusses the project and the solution developed, as well as the lessons learnt in the development process.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5756552,no
Step Up Transformer online monitoring experience in Tucurui Power Plant,"The Step Up Transformers at the Tucurui Hydroelectric Power Plant are very important for the National Interconnected System (SIN). Due to that, and due to the severe work conditions, Eletrobras Eletronorte has always kept a rigorous preventive maintenance program for these equipments. However, transformer failure history in the first powerhouse (older ones) led to the implantation of the online monitoring system, in order to detect the defects when they start, and mitigate the risks even more. System installation started in 2006, with sensors and software. Four transformers which were already operating began to be monitored and implantation for three more was in progress, taking advantage of the modularity and expandability features of the decentralized architecture used. The Architecture and the solutions applied in system implantation, as well as the results obtained, will be described in this paper. Some of the goals successfully attained were easier insurance negotiation for some equipment and more safety for the personnel, the equipment and the facility.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5762910,no
Supporting evidence-based Software Engineering with collaborative information retrieval,"The number of scientific publications is constantly increasing, and the results published on Empirical Software Engineering are growing even faster. Some software engineering publishers have began to collaborate with research groups to make available repositories of software engineering empirical data. However, these initiatives are limited due to issues related to the available search tools. As a result, many researchers in the area have adopted a semi-automated approach for performing searches for systematic reviews as a mean to extract empirical evidence from published material. This makes this activity labor intensive and error prone. In this paper, we argue that the use of techniques from information retrieval, as well as text mining, can support systematic reviews and improve the creation of repositories of SE empirical evidence.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5767051,no
On trust guided collaboration among cloud service providers,"Cloud computing has emerged as a popular paradigm that offers computing resources (e.g. CPU, storage, bandwidth, software) as scalable and on-demand services over the Internet. As more players enter this emerging market, a heterogeneous cloud computing market is expected to evolve, where individual players will have different volumes of resources, and will provide specialized services, and with different levels of quality of services. It is expected that service providers will thus, besides competing, also collaborate to complement their resources in order to improve resource utilization and combine individual services to offer more complex value chains and end-to-end solutions required by the customers. It is challenging to select suitable partners in a decentralized setting due to various factors such as lack of global coordination or information, as well as diversity and scale. Trust is known to play an important role in promoting cooperation in many decentralized settings including the society at large, as well as on the Internet, e.g., in e-commerce, etc. In this paper, we explore how trust can promote collaboration among service providers. The novelty of our approach is a framework to combine disparate trust information - from direct interactions and from (indirect) references among service providers, as well as from customer feedbacks, depending on availability of these different kinds of information. Doing so provides decision making guidance to service providers to initialize collaborations by selecting trustworthy partners. Simulation results demonstrate the promise of our approach by showing that compared to random selection, our proposal can help effectively select trustworthy collaborators to achieve better quality of services.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5767057,no
PINCETTE  Validating changes and upgrades in networked software,"Summary form only given. PINCETTE is a STREP project under the European Community's 7th Framework Programme [FP7/2007-2013]. The project focuses on detecting failures resulting from software changes, thus improving the reliability of networked software systems. The goal of the project is to produce technology for efficient and scalable verification of complex evolving networked software systems, based on integration of static and dynamic analysis and verification algorithms, and the accompanying methodology. The resulting technology will also provide quality metrics to measure the thoroughness of verification. The PINCETTE consortium is composed of the following partners: IBM Israel, University of Oxford, Universita della Svizzera Italiana (USI), Universita degli Studi di Milano-Bicocca (UniMiB), Technical Research Center of Finland (VTT), ABB, and Israeli Aerospace Industries (IAI).",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770962,no
Analyzing personality types to predict team performance,"This paper presents an approach in analyzing personality types, temperament and team diversity to determine software engineering (SE) teams performance. The benefits of understanding personality types and its relationships amongst team members are crucial for project success. Rough set analysis was used to analyze Myers-Briggs Type Indicator (MBTI) personality types, Keirsey temperament, team diversity, and team performance. The result shows positive relationships between these attributes.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5773856,no
Test effort optimization by prediction and ranking of fault-prone software modules,"Identification of fault-prone or not fault-prone modules is very essential to improve the reliability and quality of a software system. Once modules are categorized as fault-prone or not fault-prone, test effort are allocated accordingly. Testing effort and efficiency are primary concern and can be optimized by prediction and ranking of fault-prone modules. This paper discusses a new model for prediction and ranking of fault-prone software modules for test effort optimization. Model utilizes the classification capability of data mining techniques and knowledge stored in software metrics to classify the software module as fault-prone or not fault-prone. A decision tree is constructed using ID3 algorithm for the existing project data. Rules are derived form the decision tree and integrated with fuzzy inference system to classify the modules as either fault-prone or not fault-prone for the target data. The model is also able to rank the fault-prone module on the basis of its degree of fault-proneness. The model accuracy are validated and compared with some other models by using the NASA projects data set of PROMOSE repository.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5779531,no
Effect of Class-IV power supply failure frequency on Core Damage Frequency (CDF),"In India, grid disturbance is a major cause of plant transients in nuclear power plants, and thus having an impact on plant safety. With better regulation and load management, the frequency of grid disturbance has come down substantially with time. Nevertheless the plant transients initiated by Class-IV power supply failure are experienced regularly. Further the incapability of emergency diesel generators' to restore the power supply following Class-IV failure raises several safety issues. Similarly, the maintenance down time of diesel generators either planned or unplanned contributes to the system unavailability. Realizing the significance of Class-IV power supply for nuclear power plants, due weightage is given to the event sequence progression initiated due to Class-IV failure by incorporating a dedicated event tree while estimating Core Damage Frequency (CDF) in Probabilistic Safety Assessment (PSA) Level-I. At MAPS, in the recently carried out PSA Level-I analysis, Class-IV failure was observed to be the 6<sup>th</sup> most significant contributor to the CDF. On a closure scrutiny it was observed that the Class-IV failure frequencies were high till nineties due to poor grid conditions which came down subsequently through the concerted efforts of grid authorities. In this paper, we have presented a parametric study to assess the sensitivity of CDF to Class-IV failure by using the PSA software package RISKSPECTRUM with the observed Class-IV related failure data for the entire operating year of MAPS station. It has been observed that with time the contribution of Class-IV power supply failure to CDF has got reduced. The Class-IV power supply failure frequency has hardly any effect on CDF, inferring that any external cause like this has no significant impact on CDF.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5779549,no
Improving operational reliability of Indus accelerators by implementation of EPICS based Control System for Microtron injector,"The Experimental Physics and Industrial Control System (EPICS) is a comprehensive set of software tools for creating control applications. The home-grown VME infrastructure at RRCAT, along with LabVIEW was used for the control of common injector of Indus rings i.e. Microtron. Increasing demands and continuous evolution of the system entailed upgrade of the control system. An EPICS based control system is recently commissioned and deployed for Microtron that renders enhanced SCADA functionalities. The SoftIOC running on Linux, talks to a VME station, an oscilloscope, a digital teslameter, a temperature scanner and an RF synthesizer on RS232 and TCP/IP. The OPI runs EDM on Linux. This paper discusses the operational improvements achieved in the control system by upgrading to EPICS. The reliability of the system is further enhanced by modules like Fault Diagnostics and Cathode Emission Auto-correction. The fault diagnostics module predicts anomalies in the system behavior and eases fault troubleshooting. The cathode emission auto-correction is a closed loop control for electron emission from the cathode. This paper also presents a system optimization perspective on hardware and software aspects chosen for the new system, and the design & implementation constraints on Windows and Linux.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5779582,no
Reliability comparison of computer based core temperature monitoring system with two and three thermocouples per sub-assembly for Fast Breeder Reactors,"Prototype Fast Breeder Reactor (PFBR) is a mixed oxide fuelled, sodium cooled, 500 MWe, pool type fast breeder reactor under construction at Kalpakkam, India. The reactor core consists of fuel pins assembled in a number of hexagonal shaped, vertically stacked SubAssemblies (SA). Sodium flows from the bottom of the SAs, takes heat from the fission reaction, comes out through the top. Reactor protection systems are provided to trip the reactor in case of design basis events which may cause the safety parameters (like clad, fuel and coolant temperature) to cross their limits. Computer based Core temperature monitoring system (CTMS) is one of the protection systems. CTMS for PFBR has two thermocouples (TC) at the outlet of each SA(other than central SA) to measure coolant outlet temperature, three TC at central SA outlet and six thermocouples to measure coolant inlet temperature. Each thermocouple at SA outlet is electronically triplicated and fed to three computer systems for further processing and generate reactor trip signal whenever necessary. Since the system has two sensors per SA and three processing units the redundancy provided is not independent. A study is done to analyze the reliability implications of providing three thermocouples at the outlet of each SA and thereby feed independent thermocouple signals to three computer systems. Failure data derived from fast reactor experiences and from reliability prediction methods provided by handbooks are used. Fault trees are built for the existing CTMS system with two TC per SA and for the proposed system with three TC per SA. Failure probability upon demand and spurious trip rates are estimated as reliability indicators. Since the computer systems have software intelligence to sense invalid field inputs, not all sensor failures would directly affect the system probability to fail upon a demand. For instance, the coolant outlet temperature cannot be lower than the coolant inlet temperature. This intelligence is ta- en into account by assuming different fault coverage percentage and comparing the results. A 100% fault coverage means the software algorithm could detect all of the possible thermocouple faults. It was found that the system probability to fail upon demand is reduced in the new independent system but the spurious trip rate is slightly worse. The diagnostic capability is marginally affected due to complete independence. The paper highlights how an intelligent computer based safety system poses difficulties in modeling and the checks and balances between an interlinked and independent redundancy.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5779593,no
Regulatory review of computer based systems: Indian perspectives,"The use of state of art digital instrumentation and control (I&C) in safety and safety related systems in nuclear power plants has become prevalent due to the performance in terms of accuracy, computational capabilities and data archiving capability for future diagnosis. Added advantages in computer based systems are fault tolerance, self-testing, signal validation capability and process system diagnostics. But, uncertainty exists about the quality, reliability and performance of such software based nuclear instrumentation which poses new challenges for the industry and regulators in using them for safety and safety related systems. To obtain adequate confidence in licensing them for use in NPPs, CBS were deployed gradually from monitoring system to control system (i.e, non-safety, safety related & lastly safety systems). Based upon the experience over a decade, AERB safety guide AERB/SGID-25 was prepared to prescribe the criteria and requirements to assess the qualitative reliability of such software based nuclear instrumentation. This paper describes the regulatory review and audit process as required by the above guide. Further, Software Configuration Management (SCM) is an important item during life cycle of CBS, whether it is design phase or operating phase. Configuration control becomes necessary due to operation feedback, introduction of additional features and due to obsolescence. Therefore configuration control during operating phase for CBS becomes all the more important. This paper elaborates on the regulatory approach adopted by AERB for regulatory review and control of design modifications in operating phase of NPPs. This paper also covers a case study of AERB audit on verification & validation activities for software based safety and safety related systems used in an Indian plant.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5779596,no
Software reliability estimation through black box and white box testing at prototype level,"Software reliability refers to the probability of failure-free operation of a system. It is related to many aspects of software, including the testing process. Directly estimating software reliability by quantifying its related factors can be difficult. Testing is an effective sampling method to measure software reliability. Guided by the operational profile, software testing (usually black-box testing) can be used to obtain failure data, and an estimation model can be further used to analyze the data to estimate the present reliability and predict future reliability. White box testing is based on inter-component interactions which deal with probabilistic software behavior. It uses an internal perspective of the system to design test cases based on internal structure at requirements and design phases. This paper has been applied for evolution of effective reliability quantification analysis at prototype level of a financial application case study with both failure data test data of software Development Life cycle (SDLC) phases captured from defect consolidation table in the form orthogonal defect classification as well functional requirements at requirement and design phases captured through software architectural modeling paradigms.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5779604,no
"Clone detection: Why, what and how?","Excessive code duplication is a bane of modern software development. Several experimental studies show that on average 15 percent of a software system can contain source code clones - repeatedly reused fragments of similar code. While code duplication may increase the speed of initial software development, it undoubtedly leads to problems during software maintenance and support. That is why many developers agree that software clones should be detected and dealt with at every stage of software development life cycle. This paper is a brief survey of current state-of-the-art in clone detection. First, we highlight main sources of code cloning such as copy-and-paste programming, mental code patterns and performance optimizations. We discuss reasons behind the use of these techniques from the developer's point of view and possible alternatives to them. Second, we outline major negative effects that clones have on software development. The most serious drawback duplicated code have on software maintenance is increasing the cost of modifications - any modification that changes cloned code must be propagated to every clone instance in the program. Software clones may also create new software bugs when a programmer makes some mistakes during code copying and modification. Increase of source code size due to duplication leads to additional difficulty of code comprehension. Third, we review existing clone detection techniques. Classification based on used source code representation model is given in this work. We also describe and analyze some concrete examples of clone detection techniques highlighting main distinctive features and problems that are present in practical clone detection. Finally, we point out some open problems in the area of clone detection. Currently questions like """"What is a code clone?"""", """"Can we predict the impact clones have on software quality"""" and """"How can we increase both clone detection precision and recall at the same time? """" stay open to further re- - search. We list the most important questions in modern clone detection and explain why they continue to remain unanswered despite all the progress in clone detection research.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5783148,no
Defect detection for multithreaded programs with semaphore-based synchronization,"The solution to the problem of automatic defects detection in multithreaded programs is covered in this paper. General approaches for defect detection are considered. Static analysis is chosen because of its full automation and soundness properties. Overview of papers about static analysis usage for defect detection in parallel programs is presented. The approach for expansion of static analysis algorithms to multithreaded programs is suggested. This approach is based on Thread Analysis Algorithm. Thread analysis algorithm provides analysis of threads creation and thread-executed functions. This algorithm uses static analysis algorithm results in particular to identify semaphore objects. Thread analysis algorithm and static analysis algorithms are processing jointly. Thread analysis algorithm interprets thread control functions calls (create, join, etc.) and synchronization functions calls (wait, post, etc.). The algorithm determines program blocks which may execute in parallel and interaction pairs of synchronization functions calls. This information is taking into consideration to analyze threads cooperation and detect synchronization errors. To analyze threads cooperation this algorithm uses join of shared objects values in -functions. Basic rules of thread analysis algorithm are considered in the paper. Application of these rules to multithreaded program example is presented. The suggested approach allows us to detect all single-threaded program defect types and some synchronization errors such as Race condition or Deadlock. This approach gives sound results. It obtains analysis of programs with any number of semaphores and threads. It is possible to analyze dynamically created threads. The approach can be extended to other classes of parallel programs and other types of synchronization objects.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5783153,no
Header-driven generation of sanity API tests for shared libraries,"There are thousands of various software libraries being developed in the modern world - completely new libraries emerge as well as new versions of existing ones regularly appear. Unfortunately, developers of many libraries focus on developing functionality of the library itself but neglect ensuring high quality and backward compatibility of application programming interfaces (APIs) provided by their libraries. The best practice to address these aspects is having an automated regression test suite that can be regularly (e.g., nightly) run against the current development version of the library. Such a test suite would ensure early detection of any regressions in the quality or compatibility of the library. But developing a good test suite can cost significant amount of efforts, which becomes an inhibiting factor for library developers when deciding QA policy. That is why many libraries do not have a test suite at all. This paper discusses an approach for low cost automatic generation of basic tests for shared libraries based on the information automatically extracted from the library header files and additional information about semantics of some library data types. Such tests can call APIs of target libraries with some correct parameters and can detect typical problems like crashes out-of-the-box. Using this method significantly lowers the barrier for developing an initial version of library tests, which can be then gradually improved with a more powerful test development framework as resources appear. The method is based on analyzing API signatures and type definitions obtained from the library header files and creating parameter initialization sequences through comparison of target function parameter types with other functions' return values or out-parameters (usually, it is necessary to call some function to get a correct parameter value for another function and the initialization sequence of the necessary function calls can be quite long). The - - paper also describes the structure of a tool that implements the proposed method for automatic generation of basic tests for Linux shared libraries (for C and C++ languages). Results of practical usage of the tool are also presented.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5783158,no
Implementation by capture with executable UML,"Despite all the progress and raise of abstraction involved in the software development, the main building block of each software system is still the traditional, mostly manually written, """"line of code"""". This paper is about the solution to low-level model behavior """"coding"""", applied by my team in the context of development of the executable UML tool """"Enterprise Analyst"""". Writing computer programs requires knowledge and skill. Programmers must learn and memorize a large knowledge base of rules and must be able to """"foresee """" the execution of code, while writing it. In addition, manual codification is highly error prone process. Any improvement in the realization of this essential software activity will naturally bring improvement in resulting software system as a whole. We have designed and implemented a variation of the technique known as """"programming by example"""", with the basic idea in """"teaching """" the machine how to perform sequence of tasks instead of """"programming"""" it.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5783159,no
Practical review of software requirements,"Quality of the requirements is more important than quality of any other work document of the software lifecycle. On the other hand, typical requirements quality assurance methods, such as peer review are always costly and often detect only formal and cosmetic defects. According to Luxoft experience, review is more effective when it is combined with practical validation of the requirements. The reviewers should not go through a checklist with abstract non-ambiguity, verifiability, or feasibility,.. criteria but should generate draft implementations of the requirements instead, to see if they can be really put into design, test cases, and user documentation. The approach improves quality and non-volatility of the requirements, decreases rework rate on the subsequent phases, and yet does not affect project budget.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5783173,no
Source code modification technology based on parameterized code patterns,"Source code modification is one of the most frequent operations which developers perform in software life cycle. Such operation can be performed in order to add new functionality, fix bugs or bad code style, optimize performance, increase readability, etc. During the modification of existing source code developer needs to find parts of code, which meet to some conditions, and change it according to some rules. Usually developers perform such operations repeatedly by hand using primitive search/replace mechanisms and """"copy and paste programming"""", and that is why manual modification of large-scale software systems is a very error-prone and time-consuming process. Automating source code modifications is one of the possible ways of coping with this problem because it can considerably decrease both the amount of errors and the time needed in the modification process. Automated source code modification technique based on parameterized source code patterns is considered in this article. Intuitive modification description that does not require any knowledge of complex transformation description languages is the main advantage of our technique. We achieve this feature by using a special source code pattern description language which is closely tied with the programming language we're modifying. This allows developers to express the modification at hand as simple """"before """"/""""after"""" source code patterns very similar to source code. Regexp-like elements are added to the language to increase its expressionalpower. The source code modification is carried out using difference abstract syntax trees. We build a set of transformation operations based on """"before""""/""""after"""" patterns (using algorithm for change detection in hierarchically structured information) and apply them to those parts of source code that match with the search """"before """" pattern. After abstract syntax tree transformation is completed we pretty-print them back to source code. A prototype of source code modification sy- - stem based on this technique has been implemented for the Java programming language. Experimental results show that this technique in some cases can increase the speed of source code modifications by several orders of magnitude, at the same time completely avoiding """"copy-and paste """" errors. In future we are planning to integrate prototype with existing development environments such as Eclipse and NetBeans.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5783177,no
Diagnostic systems and resource utilization of the ATLAS high level trigger,"With the first year of successful data taking of proton-proton collisions at LHC, the full chain of the ATLAS Trigger and Data Acquisition System could be tested under real conditions. The trigger monitoring and data quality infrastructure was essential to this success. We describe the software tools used to monitor the trigger system performance, assess the overall quality of the trigger selection and analyze the resource utilization during collision runs. Monitoring the performance and operation of these systems required smooth and parallel running of many complex software tools depending on each other. These are the basis for rate measurements, data quality determination of selected objects and supervision of the system during the data taking. Based on the data taking experience with first collisions, we describe the ATLAS trigger monitoring and operations performance.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5873867,no
Detector response function of the NanoPETTM/CT system,"Even with the huge advance of computers and computing power, full three dimensional reconstructions of PET and CT scans belong to the future as far as commercially available scanners and software are concerned. Recent investigations have shown that with the aid of Graphical Processing Units (GPUs) extremely high computational speed might be achieved, which lends itself to the implementation of iterative 3D reconstruction techniques. Moreover, these techniques make it possible to make use of off-line and on-the-fly Monte Carlo calculations. A consortium of several Hungarian institutions has been working on the development and optimization of a Monte Carlo supported 3D iterative reconstruction program. The in-body scatter processes are modeled by real-time Monte Carlo, however, the detector response is calculated off-line. Therefore, the effective implementation of this MC code requires the calculation of a detector response function in advance. The paper describes our analysis of the response function characteristics of the NanoPETTM (Mediso) detector system. Using MCNPX we constructed a data base consisting of 300 simulations (different incoming photon angles and energies). We studied the sensitivity of the system to several parameters. It was found that the spatial dependence is stronger than the energy dependence. At right angle (at 511 keV) the side-neighbors have an order of magnitude less probability compared to the central pixel, while for photons reaching the central pixel with 350 keV or 511 keV there is only 40% difference in the probabilities. We studied different techniques to include the response function into the MC code mentioned in order to find the optimal strategy. With the approach described in the paper a significant improvement in image quality can be obtained.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5874491,no
Design considerations for a high voltage compact power transformer,"This paper presents a new topology for a high voltage 50kV, high frequency (HF) 20kHz, multi-cored transformer. The transformer is suitable for use in pulsed power application systems. The main requirements are: high voltage capability, small size and weight. The HV, HF transformer is a main critical block of a high frequency power converter system. The transformer must have high electrical efficiency and in the proposed approach has to be optimized by the number of the cores. The transformer concept has been investigated analytically and through software simulations and experiments. This paper introduces the transformer topology and discusses the design procedure. Experimental measurements to predict core losses are also presented.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5958305,no
Maturity model for process of academic management,"The segment of education in Brazil, especially in higher education, has undergone major changes. The search for professionalization, for cost reduction and process standardization has led many institutions to associate through partnerships or acquisitions. On the other hand, maturity models have been used very successfully in several areas of knowledge especially in software development, as an approach for quality models. This paper presents a methodology for assessing the maturity of academic process management in private institutions of higher education that, initially, aims the Brazilian market, but its idea can be applied to a global maturity model of academic process management.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6018745,no
"Measuring complexity, effectiveness and efficiency in software course projects","This paper discusses results achieved in measuring complexity, effectiveness and efficiency, in a series of related software course projects, spanning a period of seven years. We focus on how the complexity of those projects was measured, and how the success of the students in effectively and efficiently taming that complexity was assessed. This required defining, collecting, validating and analyzing several indicators of size, effort and quality; their rationales, advantages and limitations are discussed. The resulting findings helped to improve the process itself.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062058,no
A discriminative model approach for accurate duplicate bug report retrieval,"Bug repositories are usually maintained in software projects. Testers or users submit bug reports to identify various issues with systems. Sometimes two or more bug reports correspond to the same defect. To address the problem with duplicate bug reports, a person called a triager needs to manually label these bug reports as duplicates, and link them to their """"master"""" reports for subsequent maintenance work. However, in practice there are considerable duplicate bug reports sent daily; requesting triagers to manually label these bugs could be highly time consuming. To address this issue, recently, several techniques have be proposed using various similarity based metrics to detect candidate duplicate bug reports for manual verification. Automating triaging has been proved challenging as two reports of the same bug could be written in various ways. There is still much room for improvement in terms of accuracy of duplicate detection process. In this paper, we leverage recent advances on using discriminative models for information retrieval to detect duplicate bug reports more accurately. We have validated our approach on three large software bug repositories from Firefox, Eclipse, and OpenOffice. We show that our technique could result in 17-31%, 22-26%, and 35-43% relative improvement over state-of-the-art techniques in OpenOffice, Firefox, and Eclipse datasets respectively using commonly available natural language information only.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062072,no
Has the bug really been fixed?,"Software has bugs, and fixing those bugs pervades the software engineering process. It is folklore that bug fixes are often buggy themselves, resulting in bad fixes, either failing to fix a bug or creating new bugs. To confirm this folklore, we explored bug databases of the Ant, AspectJ, and Rhino projects, and found that bad fixes comprise as much as 9% of all bugs. Thus, detecting and correcting bad fixes is important for improving the quality and reliability of software. However, no prior work has systematically considered this bad fix problem, which this paper introduces and formalizes. In particular, the paper formalizes two criteria to determine whether a fix resolves a bug: coverage and disruption. The coverage of a fix measures the extent to which the fix correctly handles all inputs that may trigger a bug, while disruption measures the deviations from the program's intended behavior after the application of a fix. This paper also introduces a novel notion of distance-bounded weakest precondition as the basis for the developed practical techniques to compute the coverage and disruption of a fix. To validate our approach, we implemented Fixation, a prototype that automatically detects bad fixes for Java programs. When it detects a bad fix, Fixation returns an input that still triggers the bug or reports a newly introduced bug. Programmers can then use that bug-triggering input to refine or reformulate their fix. We manually extracted fixes drawn from real-world projects and evaluated Fixation against them: Fixation successfully detected the extracted bad fixes.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062073,no
Mining API mapping for language migration,"To address business requirements and to survive in competing markets, companies or open source organizations often have to release different versions of their projects in different languages. Manually migrating projects from one language to another (such as from Java to C#) is a tedious and error-prone task. To reduce manual effort or human errors, tools can be developed for automatic migration of projects from one language to another. However, these tools require the knowledge of how Application Programming Interfaces (APIs) of one language are mapped to APIs of the other language, referred to as API mapping relations. In this paper, we propose a novel approach, called MAM (Mining API Mapping), that mines API mapping relations from one language to another using API client code. MAM accepts a set of projects each with two versions in two languages and mines API mapping relations between those two languages based on how APIs are used by the two versions. These mined API mapping relations assist in migration of projects from one language to another. We implemented a tool and conducted two evaluations to show the effectiveness of MAM. The results show that our tool mines 25,805 unique mapping relations of APIs between Java and C# with more than 80% accuracy. The results also show that mined API mapping relations help reduce 54.4% compilation errors and 43.0% defects during migration of projects with an existing migration tool, called Java2CSharp. The reduction in compilation errors and defects is due to our new mined mapping relations that are not available with the existing migration tool.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062087,no
Detecting atomic-set serializability violations in multithreaded programs through active randomized testing,"Concurrency bugs are notoriously difficult to detect because there can be vast combinations of interleavings among concurrent threads, yet only a small fraction can reveal them. Atomic-set serializability characterizes a wide range of concurrency bugs, including data races and atomicity violations. In this paper, we propose a two-phase testing technique that can effectively detect atomic-set serializability violations. In Phase I, our technique infers potential violations that do not appear in a concrete execution and prunes those interleavings that are violation-free. In Phase II, our technique actively controls a thread scheduler to enumerate these potential scenarios identified in Phase I to look for real violations. We have implemented our technique as a prototype system AssetFuzzer and applied it to a number of subject programs for evaluating concurrency defect analysis techniques. The experimental results show that AssetFuzzer can identify more concurrency bugs than two recent testing tools RaceFuzzer and AtomFuzzer.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062091,no
Falcon: fault localization in concurrent programs,"Concurrency fault are difficult to find because they usually occur under specific thread interleavings. Fault-detection tools in this area find data-access patterns among thread interleavings, but they report benign patterns as well as actual faulty patterns. Traditional fault-localization techniques have been successful in identifying faults in sequential, deterministic programs, but they cannot detect faulty data-access patterns among threads. This paper presents a new dynamic fault-localization technique that can pinpoint faulty data-access patterns in multi-threaded concurrent programs. The technique monitors memory-access sequences among threads, detects data-access patterns associated with a program's pass/fail results, and reports dataaccess patterns with suspiciousness scores. The paper also presents the description of a prototype implementation of the technique in Java, and the results of an empirical study we performed with the prototype on several Java benchmarks. The empirical study shows that the technique can effectively and efficiently localize the faults for our subjects.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062092,no
Predicting build outcome with developer interaction in Jazz,Investigating the human aspect of software development is becoming prominent in current research. Studies found that the misalignment between the social and technical dimensions of software work leads to losses in developer productivity and defects. We use the technical and social dependencies among pairs of developers to predict the success of a software build. Using the IBM JazzTM data we found information about developers and their social and technical relation can build a powerful predictor for the success of a software build. Investigating human aspects of software development is becoming prominent in current research. High misalignment between the social and technical dimensions of software work lowers productivity and quality.,2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062122,no
Integrating legacy systems with MDE,"Integrating several legacy software systems together is commonly performed with multiple applications of the Adapter Design Pattern in OO languages such as Java. The integration is based on specifying bi-directional translations between pairs of APIs from different systems. Yet, manual development of wrappers to implement these translations is tedious, expensive and error-prone. In this paper, we explore how models, aspects and generative techniques can be used in conjunction to alleviate the implementation of multiple wrappers. Briefly the steps are, (1) the automatic reverse engineering of relevant concepts in APIs to high-level models; (2) the manual definition of mapping relationships between concepts in different models of APIs using an ad-hoc DSL; (3) the automatic generation of wrappers from these mapping specifications using AOP. This approach is weighted against manual development of wrappers using an industrial case study. Criteria are the relative code length and the increase of automation.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062140,no
Can clone detection support quality assessments of requirements specifications?,"Due to their pivotal role in software engineering, considerable effort is spent on the quality assurance of software requirements specifications. As they are mainly described in natural language, relatively few means of automated quality assessment exist. However, we found that clone detection, a technique widely applied to source code, is promising to assess one important quality aspect in an automated way, namely redundancy that stems from copy & paste operations. This paper describes a large-scale case study that applied clone detection to 28 requirements specifications with a total of 8,667 pages. We report on the amount of redundancy found in real-world specifications, discuss its nature as well as its consequences and evaluate in how far existing code clone detection approaches can be applied to assess the quality of requirements specifications in practice.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062141,no
Flexible architecture conformance assessment with ConQAT,"The architecture of software systems is known to decay if no counter-measures are taken. In order to prevent this architectural erosion, the conformance of the actual system architecture to its intended architecture needs to be assessed and controlled; ideally in a continuous manner. To support this, we present the architecture conformance assessment capabilities of our quality analysis framework ConQAT. In contrast to other tools, ConQAT is not limited to the assessment of use-dependencies between software components. Its generic architectural model allows the assessment of various types of dependencies found between different kinds of artifacts. It thereby provides the necessary tool-support for flexible architecture conformance assessment in diverse contexts.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062171,no
JDF: detecting duplicate bug reports in Jazz,"Both developers and users submit bug reports to a bug repository. These reports can help reveal defects and improve software quality. As the number of bug reports in a bug repository increases, the number of the potential duplicate bug reports increases. Detecting duplicate bug reports helps reduce development efforts in fixing defects. However, it is challenging to manually detect all potential duplicates because of the large number of existing bug reports. This paper presents JDF (representing Jazz Duplicate Finder), a tool that helps users to find potential duplicates of bug reports on Jazz, which is a team collaboration platform for software development and process management. JDF finds potential duplicates for a given bug report using natural language and execution information.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062192,no
An incremental methodology for quantitative software architecture evaluation with probabilistic models,Probabilistic models are crucial in the quantification of non-functional attributes in safety-and mission-critical software systems. These models are often re-evaluated in assessing the design decisions. Evaluation of such models is computationally expensive and exhibits exponential complexity with the problem size. This research aims at constructing an incremental quality evaluation framework and delta evaluation scheme to address this issue. The proposed technique will provide a computational advantage for the probabilistic quality evaluations enabling their use in automated design space exploration by architecture optimization algorithms. The expected research outcomes are to be validated with a range of realistic architectures and case studies from automotive industry.,2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062204,no
Exploratory study of a UML metric for fault prediction,"This paper describes the use of a UML metric, an approximation of the CK-RFC metric, for predicting faulty classes before their implementation. We built a code-based prediction model of faulty classes using Logistic Regression. Then, we tested it in different projects, using on the one hand their UML metrics, and on the other hand their code metrics. To decrease the difference of values between UML and code measures, we normalized them using Linear Scaling to Unit Variance. Our results indicate that the proposed UML RFC metric can predict faulty code as well as its corresponding code metric does. Moreover, the normalization procedure used was of great utility, not just for enabling our UML metric to predict faulty code, using a code-based prediction model, but also for improving the prediction results across different packages and projects, using the same model.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062214,no
Capturing the long-term impact of changes,"Developers change source code to add new functionality, fix bugs, or refactor their code. Many of these changes have immediate impact on quality or stability. However, some impact of changes may become evident only in the long term. The goal of this thesis is to explore the long-term impact of changes by detecting dependencies between code changes and by measuring their influence on software quality, software maintainability, and development effort. Being able to identify the changes with the greatest long-term impact will strengthen our understanding of a project's history and thus shape future code changes and decisions.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062222,no
Failure preventing recommendations,"Software becomes more and more integral to our lives thus software failures affect more people than ever. Failures are not only responsible for billions of dollars lost to industry but can cause lethal accidents. Although there has been much research into predicting such failures, those predictions usually concentrate either on the technical or the social level of software development. With the ever growing size of software teams we think that coordination among developers is becoming increasingly more important. Therefore, we propose to leverage the combination of both social and technical dimensions to create recommendation upon which developers can act to prevent software failures.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062223,no
The Demand Side: Assessing Trade-offs and Making Choices,"This chapter contains sections titled: The User Survey Methodology, Investing in Cost Assessment for Open Source and Proprietary Software, Identifying the Cost Trade-offs for Open Source and Proprietary Software, Quality, Mixing by Consumers: The Cohabitation of Open Source and Proprietary Software, Appendix 5.1: An Economic Model of the Decision to Invest in TCO Analysis, Appendix 5.2: Who Does TCO Analysis: Econometric Estimates, Appendix 5.3: Cost Structure of Open Source and Proprietary Software: Econometric Estimates",2010,http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6277818.pdf&bkn=6267477&pdfType=chapter,no
Communication and Agreement Abstractions for Fault-Tolerant Asynchronous Distributed Systems,"Understanding distributed computing is not an easy task. This is due to the many facets of uncertainty one has to cope with and master in order to produce correct distributed software. Considering the uncertainty created by asynchrony and process crash failures in the context of message-passing systems, the book focuses on the main abstractions that one has to understand and master in order to be able to produce software with guaranteed properties. These fundamental abstractions are communication abstractions that allow the processes to communicate consistently (namely the register abstraction and the reliable broadcast abstraction), and the consensus agreement abstractions that allows them to cooperate despite failures. As they give a precise meaning to the words """"communicate"""" and """"agree"""" despite asynchrony and failures, these abstractions allow distributed programs to be designed with properties that can be stated and proved. Impossibility results are associated with these abstracti ns. Hence, in order to circumvent these impossibilities, the book relies on the failure detector approach, and, consequently, that approach to fault-tolerance is central to the book. Table of Contents: List of Figures / The Atomic Register Abstraction / Implementing an Atomic Register in a Crash-Prone Asynchronous System / The Uniform Reliable Broadcast Abstraction / Uniform Reliable Broadcast Abstraction Despite Unreliable Channels / The Consensus Abstraction / Consensus Algorithms for Asynchronous Systems Enriched with Various Failure Detectors / Constructing Failure Detectors",2010,http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6812883.pdf&bkn=6812882&pdfType=book,no
Cost-sensitive boosting neural networks for software defect prediction,"Software defect predictors which classify the software modules into defect-prone and not-defect-prone classes are effective tools to maintain the high quality of software products. The early prediction of defect-proneness of the modules can allow software developers to allocate the limited resources on those defect-prone modules such that high quality software can be produced on time and within budget. In the process of software defect prediction, the misclassification of defect-prone modules generally incurs much higher cost than the misclassification of not-defect-prone ones. Most of the previously developed predication models do not consider this cost issue. In this paper, three cost-sensitive boosting algorithms are studied to boost neural networks for software defect prediction. The first algorithm based on threshold-moving tries to move the classification threshold towards the not-fault-prone modules such that more fault-prone modules can be classified correctly. The other two weight-updating based algorithms incorporate the misclassification costs into the weight-update rule of boosting procedure such that the algorithms boost more weights on the samples associated with misclassified defect-prone modules. The performances of the three algorithms are evaluated by using four datasets from NASA projects in terms of a singular measure, the Normalized Expected Cost of Misclassification (NECM). The experimental results suggest that threshold-moving is the best choice to build cost-sensitive software defect prediction models with boosted neural networks among the three algorithms studied, especially for the datasets from projects developed by object-oriented language.",2010,https://pdfs.semanticscholar.org/3c0c/776156d537fe438af2fb25623fdc8816cda1.pdf,yes
"Defect prediction from static code features: current results, limitations, new approaches","Building quality software is expensive and software quality assurance (QA) budgets are limited. Data miners can learn defect predictors from static code features which can be used to control QA resources; e.g. to focus on the parts of the code predicted to be more defective. Recent results show that better data mining technology is not leading to better defect predictors. We hypothesize that we have reached the limits of the standard learning goal of maximizing area under the curve (AUC) of the probability of false alarms and probability of detection ""AUC(pd, pf)""; i.e. the area under the curve of a probability of false alarm versus probability of detection. Accordingly, we explore changing the standard goal. Learners that maximize ""AUC(effort, pd)"" find the smallest set of modules that contain the most errors. WHICH is a meta-learner framework that can be quickly customized to different goals. When customized to AUC(effort, pd), WHICH out-performs all the data mining methods studied here. More importantly, measured in terms of this new goal, certain widely used learners perform much worse than simple manual methods. Hence, we advise against the indiscriminate use of learners. Learners must be chosen and customized to the goal at hand. With the right architecture (e.g. WHICH), tuning a learner to specific local business goals can be a simple task.",2010,https://www.researchgate.net/profile/Tim_Menzies/publication/220136028_Defect_prediction_from_static_code_features_Current_results_limitations_new_approaches/links/0912f50bfb7775ec16000000.pdf,yes
A systematic and comprehensive investigation of methods to build and evaluate fault prediction models,"This paper describes a study performed in an industrial setting that attempts to build predictive models to identify parts of a Java system with a high fault probability. The system under consideration is constantly evolving as several releases a year are shipped to customers. Developers usually have limited resources for their testing and would like to devote extra resources to faulty system parts. The main research focus of this paper is to systematically assess three aspects on how to build and evaluate fault-proneness models in the context of this large Java legacy system development project: (1) compare many data mining and machine learning techniques to build fault-proneness models, (2) assess the impact of using different metric sets such as source code structural measures and change/fault history (process measures), and (3) compare several alternative ways of assessing the performance of the models, in terms of (i) confusion matrix criteria such as accuracy and precision/recall, (ii) ranking ability, using the receiver operating characteristic area (ROC), and (iii) our proposed cost-effectiveness measure (CE). The results of the study indicate that the choice of fault-proneness modeling technique has limited impact on the resulting classification accuracy or cost-effectiveness. There is however large differences between the individual metric sets in terms of cost-effectiveness, and although the process measures are among the most expensive ones to collect, including them as candidate measures significantly improves the prediction models compared with models that only include structural measures and/or their deltas between releases - both in terms of ROC area and in terms of CE. Further, we observe that what is considered the best model is highly dependent on the criteria that are used to evaluate and compare the models. And the regular confusion matrix criteria, although popular, are not clearly related to the problem at hand, namely the cost-effectiveness of using fault-proneness prediction models to focus verification efforts to deliver software with less faults at less cost.",2010,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.164.375&rep=rep1&type=pdf,yes
A Comparative Study of Ensemble Feature Selection Techniques for Software Defect Prediction,"Feature selection has become the essential step in many data mining applications. Using a single feature subset selection method may generate local optima. Ensembles of feature selection methods attempt to combine multiple feature selection methods instead of using a single one. We present a comprehensive empirical study examining 17 different ensembles of feature ranking techniques (rankers) including six commonly-used feature ranking techniques, the signal-to-noise filter technique, and 11 threshold-based feature ranking techniques. This study utilized 16 real-world software measurement data sets of different sizes and built 13,600 classification models. Experimental results indicate that ensembles of very few rankers are very effective and even better than ensembles of many or all rankers.",2010,http://ieeexplore.ieee.org/document/5708824/?arnumber=5708824,yes
Automated Derivation of Application-Aware Error Detectors Using Static Analysis: The Trusted Illiac Approach,"This paper presents a technique to derive and implement error detectors to protect an application from data errors. The error detectors are derived automatically using compiler-based static analysis from the backward program slice of critical variables in the program. Critical variables are defined as those that are highly sensitive to errors, and deriving error detectors for these variables provides high coverage for errors in any data value used in the program. The error detectors take the form of checking expressions and are optimized for each control-flow path followed at runtime. The derived detectors are implemented using a combination of hardware and software and continuously monitor the application at runtime. If an error is detected at runtime, the application is stopped so as to prevent error propagation and enable a clean recovery. Experiments show that the derived detectors achieve low-overhead error detection while providing high coverage for errors that matter to the application.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5089331,no
"Assessing, Comparing, and Combining State Machine-Based Testing and Structural Testing: A Series of Experiments","A large number of research works have addressed the importance of models in software engineering. However, the adoption of model-based techniques in software organizations is limited since these models are perceived to be expensive and not necessarily cost-effective. Focusing on model-based testing, this paper reports on a series of controlled experiments. It investigates the impact of state machine testing on fault detection in class clusters and its cost when compared with structural testing. Based on previous work showing this is a good compromise in terms of cost and effectiveness, this paper focuses on a specific state-based technique: the round-trip paths coverage criterion. Round-trip paths testing is compared to structural testing, and it is investigated whether they are complementary. Results show that even when a state machine models the behavior of the cluster under test as accurately as possible, no significant difference between the fault detection effectiveness of the two test strategies is observed, while the two test strategies are significantly more effective when combined by augmenting state machine testing with structural testing. A qualitative analysis also investigates the reasons why test techniques do not detect certain faults and how the cost of state machine testing can be brought down.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5416729,no
Self-Supervising BPEL Processes,"Service compositions suffer changes in their partner services. Even if the composition does not change, its behavior may evolve over time and become incorrect. Such changes cannot be fully foreseen through prerelease validation, but impose a shift in the quality assessment activities. Provided functionality and quality of service must be continuously probed while the application executes, and the application itself must be able to take corrective actions to preserve its dependability and robustness. We propose the idea of self-supervising BPEL processes, that is, special-purpose compositions that assess their behavior and react through user-defined rules. Supervision consists of monitoring and recovery. The former checks the system's execution to see whether everything is proceeding as planned, while the latter attempts to fix any anomalies. The paper introduces two languages for defining monitoring and recovery and explains how to use them to enrich BPEL processes with self-supervision capabilities. Supervision is treated as a cross-cutting concern that is only blended at runtime, allowing different stakeholders to adopt different strategies with no impact on the actual business logic. The paper also presents a supervision-aware runtime framework for executing the enriched processes, and briefly discusses the results of in-lab experiments and of a first evaluation with industrial partners.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5432226,no
GUI Interaction Testing: Incorporating Event Context,"Graphical user interfaces (GUIs), due to their event-driven nature, present an enormous and potentially unbounded way for users to interact with software. During testing, it is important to adequately cover this interaction space. In this paper, we develop a new family of coverage criteria for GUI testing grounded in combinatorial interaction testing. The key motivation of using combinatorial techniques is that they enable us to incorporate context into the criteria in terms of event combinations, sequence length, and by including all possible positions for each event. Our new criteria range in both efficiency (measured by the size of the test suite) and effectiveness (the ability of the test suites to detect faults). In a case study on eight applications, we automatically generate test cases and systematically explore the impact of context, as captured by our new criteria. Our study shows that by increasing the event combinations tested and by controlling the relative positions of events defined by the new criteria, we can detect a large number of faults that were undetectable by earlier techniques.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5444885,no
Recovery Device for Real-Time Dual-Redundant Computer Systems,"This paper proposes the design of specialized hardware, called Recovery Device, for a dual-redundant computer system that operates in real-time. Recovery Device executes all fault-tolerant services including fault detection, fault type determination, fault localization, recovery of system after temporary (transient) fault, and reconfiguration of system after permanent fault. The paper also proposes the algorithms for determination of fault type (whether the fault is temporary or permanent) and localization of faulty computer without using self-testing techniques and diagnosis routines. Determination of fault type allows us to eliminate only the computer with a permanent fault. In other words, the determination of fault type prevents the elimination of nonfaulty computer because of short temporary fault. On the other hand, localization of faulty computer without using self-testing techniques and diagnosis routines shortens the recovery point time period and reduces the probability that a fault will occur during the execution of fault-tolerant procedure. This is very important for real-time fault-tolerant systems. These contributions bring both an increase in system performance and an increase in the degree of system reliability.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5444888,no
hiCUDA: High-Level GPGPU Programming,"Graphics Processing Units (GPUs) have become a competitive accelerator for applications outside the graphics domain, mainly driven by the improvements in GPU programmability. Although the Compute Unified Device Architecture (CUDA) is a simple C-like interface for programming NVIDIA GPUs, porting applications to CUDA remains a challenge to average programmers. In particular, CUDA places on the programmer the burden of packaging GPU code in separate functions, of explicitly managing data transfer between the host and GPU memories, and of manually optimizing the utilization of the GPU memory. Practical experience shows that the programmer needs to make significant code changes, often tedious and error-prone, before getting an optimized program. We have designed hiCUDA}, a high-level directive-based language for CUDA programming. It allows programmers to perform these tedious tasks in a simpler manner and directly to the sequential code, thus speeding up the porting process. In this paper, we describe the hiCUDA} directives as well as the design and implementation of a prototype compiler that translates a hiCUDA} program to a CUDA program. Our compiler is able to support real-world applications that span multiple procedures and use dynamically allocated arrays. Experiments using nine CUDA benchmarks show that the simplicity hiCUDA} provides comes at no expense to performance.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5445082,no
Certifying the Floating-Point Implementation of an Elementary Function Using Gappa,"High confidence in floating-point programs requires proving numerical properties of final and intermediate values. One may need to guarantee that a value stays within some range, or that the error relative to some ideal value is well bounded. This certification may require a time-consuming proof for each line of code, and it is usually broken by the smallest change to the code, e.g., for maintenance or optimization purpose. Certifying floating-point programs by hand is, therefore, very tedious and error-prone. The Gappa proof assistant is designed to make this task both easier and more secure, due to the following novel features: It automates the evaluation and propagation of rounding errors using interval arithmetic. Its input format is very close to the actual code to validate. It can be used incrementally to prove complex mathematical properties pertaining to the code. It generates a formal proof of the results, which can be checked independently by a lower level proof assistant like Coq. Yet it does not require any specific knowledge about automatic theorem proving, and thus, is accessible to a wide community. This paper demonstrates the practical use of this tool for a widely used class of floating-point programs: implementations of elementary functions in a mathematical library.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5483294,no
Toward a Formalism for Conservative Claims about the Dependability of Software-Based Systems,"In recent work, we have argued for a formal treatment of confidence about the claims made in dependability cases for software-based systems. The key idea underlying this work is """"the inevitability of uncertainty"""": It is rarely possible to assert that a claim about safety or reliability is true with certainty. Much of this uncertainty is epistemic in nature, so it seems inevitable that expert judgment will continue to play an important role in dependability cases. Here, we consider a simple case where an expert makes a claim about the probability of failure on demand (pfd) of a subsystem of a wider system and is able to express his confidence about that claim probabilistically. An important, but difficult, problem then is how such subsystem (claim, confidence) pairs can be propagated through a dependability case for a wider system, of which the subsystems are components. An informal way forward is to justify, at high confidence, a strong claim, and then, conservatively, only claim something much weaker: """"I'm 99 percent confident that the pfd is less than 10<sup>-5</sup>, so it's reasonable to be 100 percent confident that it is less than 10<sup>-3</sup>."""" These conservative pfds of subsystems can then be propagated simply through the dependability case of the wider system. In this paper, we provide formal support for such reasoning.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5492693,no
Dynamic Programming and Graph Algorithms in Computer Vision,"Optimization is a powerful paradigm for expressing and solving problems in a wide range of areas, and has been successfully applied to many vision problems. Discrete optimization techniques are especially interesting since, by carefully exploiting problem structure, they often provide nontrivial guarantees concerning solution quality. In this paper, we review dynamic programming and graph algorithms, and discuss representative examples of how these discrete optimization techniques have been applied to some classical vision problems. We focus on the low-level vision problem of stereo, the mid-level problem of interactive object segmentation, and the high-level problem of model-based recognition.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5518769,no
Implementation Details and Safety Analysis of a Microcontroller-based SIL-4 Software Voter,"This paper presents a microcontroller-based software voting process that complies with Safety Integrity Level-4 (SIL-4) requirements. The selected system architecture consists of a 2 out of 2 schema, in which one channel acts as Master and the other as Slave. Each redundant channel uses a microcontroller as central element. The present analysis demonstrates that this system fulfills SIL-4 requirements. Once the system architecture is detailed, the system overall functionality and the data flow are presented. Then, the microcontroller's internal architecture is explained, and the software voting process flow-diagram is discussed. Afterward, the resources of the microcontroller architecture that are used for the execution of each task involved in the software voting process (hardware-software interaction) are determined. Finally, a fault analysis is elaborated to demonstrate that the cases in which the safety requirements are compromised have a very small occurrence probability, i.e., the hazard rate of proposed voting is below 1E-9.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5535164,no
Cause-Effect Modeling and Spatial-Temporal Simulation of Power Distribution Fault Events,"Modeling and simulation are important tools in the study of power distribution faults due to the limited amount of actual data and the high cost of experimentation. Although a number of software packages are available to simulate the electrical signals, approaches for simulating fault events in different environments have not been well developed. In this paper, we propose a framework for modeling and simulating fault events in power distribution systems based on environmental factors and the cause-effect relationships among them. The spatial and temporal aspects of significant environmental factors leading to various faults are modeled as raster maps and probability functions, respectively. The cause-effect relationships are expressed as fuzzy rules and a hierarchical fuzzy inference system is built to infer the probability of faults in the simulated environments. A test case simulating a part of a typical city's power distribution systems demonstrates the effectiveness of the framework in generating realistic distribution faults. This work is helpful in fault diagnosis for different local systems and provides a configurable data source to other researchers and engineers in similar areas as well.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5545499,no
"Evaluating Complexity, Code Churn, and Developer Activity Metrics as Indicators of Software Vulnerabilities","Security inspection and testing require experts in security who think like an attacker. Security experts need to know code locations on which to focus their testing and inspection efforts. Since vulnerabilities are rare occurrences, locating vulnerable code locations can be a challenging task. We investigated whether software metrics obtained from source code and development history are discriminative and predictive of vulnerable code locations. If so, security experts can use this prediction to prioritize security inspection and testing efforts. The metrics we investigated fall into three categories: complexity, code churn, and developer activity metrics. We performed two empirical case studies on large, widely used open-source projects: the Mozilla Firefox web browser and the Red Hat Enterprise Linux kernel. The results indicate that 24 of the 28 metrics collected are discriminative of vulnerabilities for both projects. The models using all three types of metrics together predicted over 80 percent of the known vulnerable files with less than 25 percent false positives for both projects. Compared to a random selection of files for inspection and testing, these models would have reduced the number of files and the number of lines of code to inspect or test by over 71 and 28 percent, respectively, for both projects.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5560680,no
Simple Fault Diagnosis Based on Operating Characteristic of Brushless Direct-Current Motor Drives,"In this paper, a simple fault diagnosis scheme for brushless direct-current motor drives is proposed to maintain control performance under an open-circuit fault. The proposed scheme consists of a simple algorithm using the measured phase current information and detects open-circuit faults based on the operating characteristic of motors. It requires no additional sensors or electrical devices to detect open-circuit faults and can be embedded into the existing drive software as a subroutine without excessive computation effort. The feasibility of the proposed fault diagnosis algorithm is proven by simulation and experimental results.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5560811,no
Recognition of Fault Transients Using a Probabilistic Neural-Network Classifier,"This paper investigates the applicability of decision tree, hidden Markov model, and probabilistic neural-network (PNN) classification techniques to distinguish the transients originating from the faults from those originating from normal switching events. Current waveforms due to different types of events, such as faults, load switching, and capacitor bank switching were generated using a high-voltage transmission system simulated in PSCAD/EMTDC simulation software. Simulated transients were used to train and test the classifiers offline. The wavelet energies calculated using three-phase currents were used as input features for the classifiers. The results of the study showed the potential for developing a highly reliable transient classification system using the PNN technique. An online classification model for PNN was fully implemented in PSCAD/EMTDC. This model was extensively tested under different scenarios. The effects of the fault impedance, signal noise, current-transformer saturation, and arcing faults were investigated. Finally, the operation of the classifier was verified using actual recorded waveforms obtained from a high-voltage transmission system.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5565524,no
A Numerical Method for the Evaluation of the Distribution of Cumulative Reward till Exit of a Subset of Transient States of a Markov Reward Model,"Markov reward models have interesting modeling applications, particularly those addressing fault-tolerant hardware/software systems. In this paper, we consider a Markov reward model with a reward structure including only reward rates associated with states, in which both positive and negative reward rates are present and null reward rates are allowed, and develop a numerical method to compute the distribution function of the cumulative reward till exit of a subset of transient states of the model. The method combines a model transformation step with the solution of the transformed model using a randomization construction with two randomization rates. The method introduces a truncation error, but that error is strictly bounded from above by a user-specified error control parameter. Further, the method is numerically stable and takes advantage of the sparsity of the infinitesimal generator of the transformed model. Using a Markov reward model of a fault-tolerant hardware/software system, we illustrate the application of the method and analyze its computational cost. Also, we compare the computational cost of the method with that of the (only) previously available method for the problem. Our numerical experiments seem to indicate that the new method can be efficient and that for medium size and large models can be substantially faster than the previously available method.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5590256,no
Robust Execution of Service Workflows Using Redundancy and Advance Reservations,"In this paper, we develop a novel algorithm that allows service consumers to execute business processes (or workflows) of interdependent services in a dependable manner within tight time-constraints. In particular, we consider large interorganizational service-oriented systems, where services are offered by external organizations that demand financial remuneration and where their use has to be negotiated in advance using explicit service-level agreements (as is common in Grids and cloud computing). Here, different providers often offer the same type of service at varying levels of quality and price. Furthermore, some providers may be less trustworthy than others, possibly failing to meet their agreements. To control this unreliability and ensure end-to-end dependability while maximizing the profit obtained from completing a business process, our algorithm automatically selects the most suitable providers. Moreover, unlike existing work, it reasons about the dependability properties of a workflow, and it controls these by using service redundancy for critical tasks and by planning for contingencies. Finally, our algorithm reserves services for only parts of its workflow at any time, in order to retain flexibility when failures occur. We show empirically that our algorithm consistently outperforms existing approaches, achieving up to a 35-fold increase in profit and successfully completing most workflows, even when the majority of providers fail.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5611480,no
A General Software Defect-Proneness Prediction Framework,"BACKGROUND - Predicting defect-prone software components is an economically important activity and so has received a good deal of attention. However, making sense of the many, and sometimes seemingly inconsistent, results is difficult. OBJECTIVE - We propose and evaluate a general framework for software defect prediction that supports 1) unbiased and 2) comprehensive comparison between competing prediction systems. METHOD - The framework is comprised of 1) scheme evaluation and 2) defect prediction components. The scheme evaluation analyzes the prediction performance of competing learning schemes for given historical data sets. The defect predictor builds models according to the evaluated learning scheme and predicts software defects with new data according to the constructed model. In order to demonstrate the performance of the proposed framework, we use both simulation and publicly available software defect data sets. RESULTS - The results show that we should choose different learning schemes for different data sets (i.e., no scheme dominates), that small details in conducting how evaluations are conducted can completely reverse findings, and last, that our proposed framework is more effective and less prone to bias than previous approaches. CONCLUSIONS - Failure to properly or fully evaluate a learning scheme can be misleading; however, these problems may be overcome by our proposed framework.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5611551,yes
Dynamic Analysis for Diagnosing Integration Faults,"Many software components are provided with incomplete specifications and little access to the source code. Reusing such gray-box components can result in integration faults that can be difficult to diagnose and locate. In this paper, we present Behavior Capture and Test (BCT), a technique that uses dynamic analysis to automatically identify the causes of failures and locate the related faults. BCT augments dynamic analysis techniques with model-based monitoring. In this way, BCT identifies a structured set of interactions and data values that are likely related to failures (failure causes), and indicates the components and the operations that are likely responsible for failures (fault locations). BCT advances scientific knowledge in several ways. It combines classic dynamic analysis with incremental finite state generation techniques to produce dynamic models that capture complementary aspects of component interactions. It uses an effective technique to filter false positives to reduce the effort of the analysis of the produced data. It defines a strategy to extract information about likely causes of failures by automatically ranking and relating the detected anomalies so that developers can focus their attention on the faults. The effectiveness of BCT depends on the quality of the dynamic models extracted from the program. BCT is particularly effective when the test cases sample the execution space well. In this paper, we present a set of case studies that illustrate the adequacy of BCT to analyze both regression testing failures and rare field failures. The results show that BCT automatically filters out most of the false alarms and provides useful information to understand the causes of failures in 69 percent of the case studies.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5611554,no
Evaluation of the Impact of Superconducting Fault Current Limiters on Power System Network Protections Using a RTS-PHIL Methodology,"Planning the integration of a Superconducting Fault Current Limiter (SFCL) in an electric power network mainly consists in predicting the current limiting characteristics in any fault condition, in order to set the protection relays accordingly. Due to the very non linear behavior of the SFCL, modifications to the settings of existing protection relays are expected. To explore the potential changes, we used a Real-Time Simulation (RTS) methodology with Power-Hardware-In-the-Loop (PHIL) capabilities (i.e. circuit simulator coupled with power amplifiers for driving external physical power devices). The RTS-PHIL is a powerful approach that makes it possible to incorporate the actual transient reaction of the hardware under study without the need for developing a complicated numerical model, while the power system circuit, generally simpler in nature, can be purely simulated. In this project, the response of a commercial protection relay in the presence of a SFCL was investigated. Both the relay and a small scale shielded-core inductive limiter were coupled to the real time simulator (HYPERSIM) through single-phase linear power amplifiers and a variety of faults were applied. So far, this setup has allowed us to evaluate the impact of inserting a SFCL on overcurrent relays (OCR), in a simple radial distribution network. The results show that coordination has indeed to be slightly revised.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5621851,no
Protector: A Probabilistic Failure Detector for Cost-Effective Peer-to-Peer Storage,"Maintaining a given level of data redundancy is a fundamental requirement of peer-to-peer (P2P) storage systems-to ensure desired data availability, additional replicas must be created when peers fail. Since the majority of failures in P2P networks are transient (i.e., peers return with data intact), an intelligent system can reduce significant replication costs by not replicating data following transient failures. Reliably distinguishing permanent and transient failures, however, is a challenging task, because peers are unresponsive to probes in both cases. In this paper, we propose Protector, an algorithm that enables efficient replication policies by estimating the number of remaining replicas for each object, including those temporarily unavailable due to transient failures. Protector dramatically improves detection accuracy by exploiting two opportunities. First, it leverages failure patterns to predict the likelihood that a peer (and the data it hosts) has permanently failed given its current downtime. Second, it detects replication level across groups of replicas (or fragments), thereby balancing false positives for some peers against false negatives for others. Extensive simulations based on both synthetic and real traces show that Protector closely approximates the performance of a perfect oracle failure detector, and significantly outperforms time-out-based detectors using a wide range of parameters. Finally, we design, implement and deploy an efficient P2P storage system called AmazingStore by combining Protector with structured P2P overlays. Our experience proves that Protector enables efficient long-term data maintenance in P2P storage systems.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5639009,no
Preventing Temporal Violations in Scientific Workflows: Where and How,"Due to the dynamic nature of the underlying high-performance infrastructures for scientific workflows such as grid and cloud computing, failures of timely completion of important scientific activities, namely, temporal violations, often take place. Unlike conventional exception handling on functional failures, nonfunctional QoS failures such as temporal violations cannot be passively recovered. They need to be proactively prevented through dynamically monitoring and adjusting the temporal consistency states of scientific workflows at runtime. However, current research on workflow temporal verification mainly focuses on runtime monitoring, while the adjusting strategy for temporal consistency states, namely, temporal adjustment, has so far not been thoroughly investigated. For this issue, two fundamental problems of temporal adjustment, namely, where and how, are systematically analyzed and addressed in this paper. Specifically, a novel minimum probability time redundancy-based necessary and sufficient adjustment point selection strategy is proposed to address the problem of where and an innovative genetic-algorithm-based effective and efficient local rescheduling strategy is proposed to tackle the problem of how. The results of large-scale simulation experiments with generic workflows and specific real-world applications demonstrate that our temporal adjustment strategy can remarkably prevent the violations of both local and global temporal constraints in scientific workflows.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5645643,no
A Refactoring Approach to Parallelism,"In the multicore era, a major programming task will be to make programs more parallel. This is tedious because it requires changing many lines of code; it's also error-prone and nontrivial because programmers need to ensure noninterference of parallel operations. Fortunately, interactive refactoring tools can help reduce the analysis and transformation burden. The author describes how refactoring tools can improve programmer productivity, program performance, and program portability. The article also describes a toolset that supports several refactorings for making programs thread-safe, threading sequential programs for throughput, and improving scalability of parallel programs.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5672516,no
Lower Upper Bound Estimation Method for Construction of Neural Network-Based Prediction Intervals,"Prediction intervals (PIs) have been proposed in the literature to provide more information by quantifying the level of uncertainty associated to the point forecasts. Traditional methods for construction of neural network (NN) based PIs suffer from restrictive assumptions about data distribution and massive computational loads. In this paper, we propose a new, fast, yet reliable method for the construction of PIs for NN predictions. The proposed lower upper bound estimation (LUBE) method constructs an NN with two outputs for estimating the prediction interval bounds. NN training is achieved through the minimization of a proposed PI-based objective function, which covers both interval width and coverage probability. The method does not require any information about the upper and lower bounds of PIs for training the NN. The simulated annealing method is applied for minimization of the cost function and adjustment of NN parameters. The demonstrated results for 10 benchmark regression case studies clearly show the LUBE method to be capable of generating high-quality PIs in a short time. Also, the quantitative comparison with three traditional techniques for prediction interval construction reveals that the LUBE method is simpler, faster, and more reliable.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5672788,no
Assessment of the Impact of SFCL on Voltage Sags in Power Distribution System,"This paper assesses and analyses the effects of superconducting fault current limiter (SFCL) installed in power distribution system on voltage sags. First of all, resistor-type SFCL is modeled using PSCAD/EMTDC to represent the quench and recovery characteristics based on the experimental results. Next, typical power distribution system of Korea is modeled. When the SFCL is installed in various locations from the starting point to end point of feeders, improvement of voltage sag is evaluated using the Information of Technology Industry Council (ITIC) curve of customer's loads when a fault occur. Finally, future studies needing to apply SFCL to power distribution system are presented.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5675721,no
Contract Specification for Hardware Interoperability Testing and Fault Analysis,"Hardware failures occur especially due to external influences, component aging, or faulty interoperability. By testing, faulty components can be localized, allowing for fault isolation or repair. The contract testing strategy from software specifies component interoperability conditions, and systematically creates correspondent tests ensuring the operability of the system. We adapt contract testing to hardware, providing component specification, and monitoring thereof. Contract specification has to be specialized with requirements on the physical environment and component input signals. Contract testing is then executed through the monitoring of the contract parameters. Furthermore, to reason about the external cause of errors, signal faults are categorized. As a case study, we present an communication system. For this system, a contract is defined, and circuits implemented to perform contract testing and fault categorization. Communication faults, related to hardware errors and to sporadic environment disturbance, are injected in the developed system. These faults are completely detected, but can be only partially categorized by the monitoring approach.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5701675,no
Managing Security: The Security Content Automation Protocol,"Managing information systems security is an expensive and challenging task. Many different and complex software components- including firmware, operating systems, and applications-must be configured securely, patched when needed, and continuously monitored for security. Most organizations have an extensive set of security requirements. For commercial firms, such requirements are established through complex interactions of business goals, government regulations, and insurance requirements; for government organizations, security requirements are mandated. Meeting these requirements has been time consuming and error prone, because organizations have lacked standardized, automated ways of performing the tasks and reporting on results. To overcome these deficiencies and reduce security administration costs, the National Institute of Standards and Technology developed the security content automation protocol using community supported security resources. SCAP (pronounced """"ess-cap"""") is a suite of specifications that standardizes the format and nomenclature by which security software products communicate information about software identification, software flaws, and security configurations.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5708279,no
Which Crashes Should I Fix First?: Predicting Top Crashes at an Early Stage to Prioritize Debugging Efforts,"Many popular software systems automatically report failures back to the vendors, allowing developers to focus on the most pressing problems. However, it takes a certain period of time to assess which failures occur most frequently. In an empirical investigation of the Firefox and Thunderbird crash report databases, we found that only 10 to 20 crashes account for the large majority of crash reports; predicting these top crashes thus could dramatically increase software quality. By training a machine learner on the features of top crashes of past releases, we can effectively predict the top crashes well before a new release. This allows for quick resolution of the most important crashes, leading to improved user experience and better allocation of maintenance efforts.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5711013,no
A Game Platform for Treatment of Amblyopia,"We have developed a prototype device for take-home use that can be used in the treatment of amblyopia. The therapeutic scenario we envision involves patients first visiting a clinic, where their vision parameters are assessed and suitable parameters are determined for therapy. Patients then proceed with the actual therapeutic treatment on their own, using our device, which consists of an Apple iPod Touch running a specially modified game application. Our rationale for choosing to develop the prototype around a game stems from multiple requirements that such an application satisfies. First, system operation must be sufficiently straight-forward that ease-of-use is not an obstacle. Second, the application itself should be compelling and motivate use more so than a traditional therapeutic task if it is to be used regularly outside of the clinic. This is particularly relevant for children, as compliance is a major issue for current treatments of childhood amblyopia. However, despite the traditional opinion that treatment of amblyopia is only effective in children, our initial results add to the growing body of evidence that improvements in visual function can be achieved in adults with amblyopia.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5713843,no
Efficient Fault Detection and Diagnosis in Complex Software Systems with Information-Theoretic Monitoring,"Management metrics of complex software systems exhibit stable correlations which can enable fault detection and diagnosis. Current approaches use specific analytic forms, typically linear, for modeling correlations. In practice, more complex nonlinear relationships exist between metrics. Moreover, most intermetric correlations form clusters rather than simple pairwise correlations. These clusters provide additional information and offer the possibility for optimization. In this paper, we address these issues by using Normalized Mutual Information (NMI) as a similarity measure to identify clusters of correlated metrics, without assuming any specific form for the metric relationships. We show how to apply the Wilcoxon Rank-Sum test on the entropy measures to detect errors in the system. We also present three diagnosis algorithms to locate faulty components: RatioScore, based on the Jaccard coefficient, SigScore, which incorporates knowledge of component dependencies, and BayesianScore, which uses Bayesian inference to assign a fault probability to each component. We evaluate our approach in the context of a complex enterprise application, and show that 1) stable, nonlinear correlations exist and can be captured with our approach; 2) we can detect a large fraction of faults with a low false positive rate (we detect up to 18 of the 22 faults we injected); and 3) we improve the diagnosis with our new diagnosis algorithms.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5714701,no
Hardware/Software Codesign Architecture for Online Testing in Chip Multiprocessors,"As the semiconductor industry continues its relentless push for nano-CMOS technologies, long-term device reliability and occurrence of hard errors have emerged as a major concern. Long-term device reliability includes parametric degradation that results in loss of performance as well as hard failures that result in loss of functionality. It has been reported in the ITRS roadmap that effectiveness of traditional burn-in test in product life acceleration is eroding. Thus, to assure sufficient product reliability, fault detection and system reconfiguration must be performed in the field at runtime. Although regular memory structures are protected against hard errors using error-correcting codes, many structures within cores are left unprotected. Several proposed online testing techniques either rely on concurrent testing or periodically check for correctness. These techniques are attractive, but limited due to significant design effort and hardware cost. Furthermore, lack of observability and controllability of microarchitectural states result in long latency, long test sequences, and large storage of golden patterns. In this paper, we propose a low-cost scheme for detecting and debugging hard errors with a fine granularity within cores and keeping the faulty cores functional, with potentially reduced capability and performance. The solution includes both hardware and runtime software based on codesigned virtual machine concept. It has the ability to detect, debug, and isolate hard errors in small noncache array structures, execution units, and combinational logic within cores. Hardware signature registers are used to capture the footprint of execution at the output of functional modules within the cores. A runtime layer of software (microvisor) initiates functional tests concurrently on multiple cores to capture the signature footprints across cores to detect, debug, and isolate hard errors. Results show that using targeted set of functional test sequences, faults ca- be debugged to a fine-granular level within cores. The hardware cost of the scheme is less than three percent, while the software tasks are performed at a high-level, resulting in a relatively low design effort and cost.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5714704,no
Experimental Validation of Channel State Prediction Considering Delays in Practical Cognitive Radio,"As part of the effort toward building a cognitive radio (CR) network testbed, we have demonstrated real-time spectrum sensing. Spectrum sensing is the cornerstone of CR. However, current hardware platforms for CR introduce time delays that undermine the accuracy of spectrum sensing. The time delay named response delay incurred by hardware and software can be measured at two antennas colocated at a secondary user (SU), the receiving antenna, and the transmitting antenna. In this paper, minimum response delays are experimentally quantified and reported based on two hardware platforms, i.e., the universal software radio peripheral 2 (USRP2) and the small-form-factor software-defined-radio development platform (SFF SDR DP). The response delay has a negative impact on the accuracy of spectrum sensing. A modified hidden Markov model (HMM)-based single-secondary-user (single-SU) prediction is proposed and examined. When multiple SUs exist and their channel qualities are diverse, cooperative prediction can benefit the SUs as a whole. A prediction scheme with two stages is proposed, where the first stage includes individual predictions conducted by all the involved SUs, and the second stage further performs cooperative prediction using individual single-SU prediction results obtained at the first stage. In addition, a soft-combining decision rule for cooperative prediction is proposed. To have convincing performance evaluation results, real-world Wi-Fi signals are used to test the proposed approaches, where the Wi-Fi signals are simultaneously recorded at four different locations. Experimental results show that the proposed single-SU prediction outperforms the 1-nearest neighbor (1-NN) prediction, which uses current detected state as an estimate of future states. Moreover, even with just a few SUs, cooperative prediction leads to overall performance improvement.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5714762,no
Using autonomous components to improve runtime qualities of software,"In the development of software systems, quality properties should be considered along with the development process so that the qualities of software systems can be inferred and predicted at the specification and design stages and be evaluated and verified at the deployment and execution stages. However, distributed autonomous software entities are developed and maintained independently by third parties and their executions and qualities are beyond the control of system developers. In this study, the notion of an autonomous component is used to model an independent autonomous software entity. An autonomous component encapsulates data types, associated operations and quality properties into a uniform syntactical unit, which provides a way to reason about the functional and non-functional properties of software systems and meanwhile offers a means of evaluating and assuring the qualities of software systems at runtime. This study also describes the implementation and running support of autonomous components and studies a case application system to demonstrate how autonomous components can be used to improve the qualities of the application system.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5718209,no
Systems engineering and safety - a framework,"This study provides a definition of safety and assesses currently available systems engineering approaches for managing safety in systems development. While most work in relation to safety is of a `safety critical` nature, the authors concentrate on wider issues associated with safety. The outcomes of the assessment lead to a proposal for a framework providing the opportunity to develop a model incorporating the safety requirements of a system. The concept of a framework facilitates an approach of combining a number of disparate methods and at the same time utilising only the beneficial features of each. Such a safety framework when combined with an approach which addresses the management of safety will enhance system effectiveness thus ensuring the non-functional requirements of stakeholders are met.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5718212,no
Functional testing of feature model analysis tools: a test suite,"A feature model is a compact representation of all the products of a software product line. Automated analysis of feature models is rapidly gaining importance: new operations of analysis have been proposed, new tools have been developed to support those operations and different logical paradigms and algorithms have been proposed to perform them. Implementing operations is a complex task that easily leads to errors in analysis solutions. In this context, the lack of specific testing mechanisms is becoming a major obstacle hindering the development of tools and affecting their quality and reliability. In this article, the authors present FaMa test suite, a set of implementation-independent test cases to validate the functionality of feature model analysis tools. This is an efficient and handy mechanism to assist in the development of tools, detecting faults and improving their quality. In order to show the effectiveness of their proposal, the authors evaluated the suite using mutation testing as well as real faults and tools. Their results are promising and directly applicable in the testing of analysis solutions. The authors intend this work to be a first step towards the development of a widely accepted test suite to support functional testing in the community of automated analysis of feature models.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5718214,no
Nomenclature unification of software product measures,"A large number of software quality prediction models are based on software product measures (SPdMs). There are different interpretations and representations of these measures which generate inconsistencies in their naming conventions. These inconsistencies affect the efforts to develop a generic approach to predict software quality. This study identifies two types of such inconsistencies and categorises them into Type I and Type II. Type I inconsistency emerges when different labels are suggested for the same software product measure. Type II inconsistency appears when same label is used for different measures. This study suggests a unification and categorisation framework to remove Type I and Type II inconsistencies. The proposed framework categorises SPdMs with respect to three dimensions: usage frequency, software development paradigm and software lifecycle phase. The framework is applied to 140 SPdMs and a searchable unified measures database (UMD) is developed. Overall, 48.5% of the measures are found inconsistent. Out of the total measures studied 34.28% measures are frequently used. It has been found that 30.71% measures are used in object oriented paradigm and 31.43% measures are used in conventional paradigm. There is an overlap of 37.86% measures between the two paradigms. The UMD reveals that the percentages of measures used in design and implementation phases are 52.86 and 35%, respectively.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5718215,no
Measuring the Effectiveness of the Defect-Fixing Process in Open Source Software Projects,"The defect-fixing process is a key process in which an open source software (OSS) project team responds to customer needs in terms of detecting and resolving software defects, hence the dimension of defect-fixing effectiveness corresponds nicely to adopters' concerns regarding OSS products. Although researchers have been studying the defect fixing process in OSS projects for almost a decade, the literature still lacks rigorous ways to measure the effectiveness of this process. Thus, this paper aims to create a valid and reliable instrument to measure the defect-fixing effectiveness construct in an open source environment through the scale development methodology proposed by Churchill [4]. This paper examines the validity and reliability of an initial list of indicators through two rounds of data collection and analysis. Finally four indicators are suggested to measure defect-fixing effectiveness. The implication for practitioners is explained through a hypothetical example followed by implications for the research community.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5718854,no
Quality Market: Design and Field Study of Prediction Market for Software Quality Control,"Given the increasing competition in the software industry and the critical consequences of software errors, it has become important for companies to achieve high levels of software quality. Generating early forecasts of potential quality problems can have significant benefits to quality improvement. In our research, we utilized a novel approach, called prediction markets, for generating early forecasts of confidence in software quality for an ongoing project in a firm. Analogous to financial market, in a quality market, a security was defined that represented the quality requirement to be predicted. Participants traded on the security to provide their predictions. The market equilibrium price represented the probability of occurrence of the quality being measured. The results suggest that forecasts generated using the prediction markets are closer to the actual project outcomes than polls. We suggest that a suitably designed prediction market may have a useful role in software development domain.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5718861,no
A Systemic Approach for Assessing Software Supply-Chain Risk,"In today's business environment, multiple organizations must routinely work together in software supply chains when acquiring, developing, operating, and maintaining software products. The programmatic and product complexity inherent in software supply chains increases the risk that defects, vulnerabilities, and malicious code will be inserted into a delivered software product. As a result, effective risk management is essential for establishing and maintaining software supply-chain assurance over time. The Software Engineering Institute (SEI) is developing a systemic approach for assessing and managing software supply-chain risks. This paper highlights the basic approach being implemented by SEI researchers and provides a summary of the status of this work.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5718996,no
Text Mining Support for Software Requirements: Traceability Assurance,"Requirements assurance aims to increase confidence in the quality of requirements through independent audit and review. One important and effort intensive activity is assurance of the traceability matrix (TM). In this, determining the correctness and completeness of the many-to-many relationships between functional and non-functional requirements (NFRs) is a particularly tedious and error prone activity for assurance personnel to peform manually. We introduce a practical to use method that applies well-established text-mining and statistical methods to reduce this effort and increase TM assurance. The method is novel in that it utilizes both requirements similarity (likelihood that requirements trace to each other) and dissimilarity (or anti-trace, likelihood that requirements do not trace to each other) to generate investigation sets that significantly reduce the complexity of the traceability assurance task and help personnel focus on likely problem areas. The method automatically adjusts to the quality of the requirements specification and TM. Requirements assurance experiences from the SQA group at NASA's Jet Propulsion Laboratory provide motivation for the need and practicality of the method. Results of using the method are verifiably promising based on an extensive evaluation of the NFR data set from the publicly accessible PROMISE repository.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5719000,no
A Rule-Based Natural Language Technique for Requirements Discovery and Classification in Open-Source Software Development Projects,"Open source projects do have requirements; they are, however, mostly informal, text descriptions found in requests, forums, and other correspondence. Understanding of such requirements can provide insight into the nature of open source projects. Unfortunately, manual analysis of natural language (NL) requirements is time-consuming, and for large projects, error-prone. Automated analysis of NL requirements, even partial, will be of great benefit. Towards that end, we describe the design and validation of an automated NL requirements classifier for open source projects. Initial results suggest that it can reduce the effort required to analyze requirements of open source projects.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5719011,no
Facilitating Performance Predictions Using Software Components,"Component-based software engineering (CBSE) poses challenges for predicting and evaluating software performance but also offers several advantages. Software performance engineering can benefit from CBSE ideas and concepts. The MediaStore, a fictional system, demonstrates how to achieve compositional reasoning about software performance.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5719590,no
Detecting SEEs in Microprocessors Through a Non-Intrusive Hybrid Technique,"This paper presents a hybrid technique based on software signatures and a hardware module with watchdog and decoder characteristics to detect SEU and SET faults in microprocessors. These types of faults have a major influence in the microprocessor's control-flow, affecting the basic blocks and the transitions between them. In order to protect the transitions between basic blocks a light hardware module is implemented in order to spoof the data exchanged between the microprocessor and its memory. Since the hardware alone is not capable of detecting errors inside the basic blocks, it is enhanced to support the new technique and then provide full control-flow protection. A fault injection campaign is performed using a MIPS microprocessor. Simulation results show high detection rates with a small amount of performance degradation and area overhead.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5720529,no
QoS and Admission Probability Study for a SIP-Based Central Managed IP Telephony System,"This work presents a study of a SIP-based IP telephony system in terms of Quality of Service (QoS) and admission probability. The system is designed for an enterprise with offices in different countries. A software IP PBX maintains the dial plan, and SIP proxies are used in order to implement Call Admission Control (CAC) and to allow the sharing of the gateways' lines. The system is implemented in a testbed where QoS parameters like One Way Delay (OWD), packet loss, jitter and ITU's R-factor are measured. Simulation is also used in order to build a scenario with a big number of offices in which establishment delays and admission probability are measured.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5720600,no
Defects Detection of Cold-roll Steel Surface Based on MATLAB,"The measure of detecting the surface edge information of cold-roll steel sheets has been investigated rely on the digital image processing toolbox of the mathematic software MATLAB, and the edge detecting experiment of surface grayscale image has been conducted on the computer. The method of detecting defects such as black flecks and scratching has been realized in the research, the different effect of operators with the disturbance of noise has been compared, the performance of the LOG's edge detection ability various due to the change of the parameter Sigma. Conclusions have been made that LOG operator maintains the satisfying performance with the disturbance of noise, smaller Sigma is, less satisfying the smoothing ability is, which maintains more details, whereas the smoothing ability is better with loss of more details, defects detection of cold-roll steel surface based on MATLAB has a quite satisfying performance.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5720911,no
Fault Diagnosis of Rolling Element Bearing Based on Vibration Frequency Analysis,"Element bearing is one of the widely used universal parts in machine. Its running condition and failure rate influence the performance, service life and efficiency of the equipment directly. So it is significant to research on bearing condition monitoring and fault diagnosis techniques. The paper describes the use of vibration measurements by a periodic monitoring system for monitoring the condition of rolling element bearing of the centrifugal machine. Vibration data is collected using accelerometers, which are placed at the 12 o'clock position at both the drive end and the driven end of the centrifugal machine. Vibration signals are collected using a 16 channel DAT recorder, and are post processed by vibration signals analysis software in personal computer. Simple diagnosis by vibration is based on frequency analysis. Each element of rolling bearing is of its fault characteristic frequency. This paper introduces frequency analysis method of using low and high frequency bands in conjunction with time domain waveform. Fault position of drive end bearing in the centrifugal machine is detected successfully by using this method.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5721155,no
Online PD Monitoring and Analysis for Step-up Transformers of Hydropower Plant,"As we known, to evaluate and diagnosis the insulation faults of large-size power transformers, partial discharges (PDs) can be detected via ultra high frequency (UHF) technique. In this paper, an UHF PD online monitoring system was developed for large-size step-up transformers. The principle of UHF PD monitoring method was introduced., and the hardware structure and software key techniques in the system were described. In order to achieve the integrated PD monitoring and ensure the accuracy of PD analysis, the operating environments and operating conditions of the transformers have been acquired synchronously. Meanwhile, the correlative analysis of PDs with respect to operating conditions can be performed and the characteristics of PD activities with respect to relevant states can be studied. The association analysis of PDs is performed as follows: (a) periodical PDs caused by power frequency voltage under stable operating conditions, (b) stochastic PDs caused by transient over-voltages under unstable operating conditions. At present, the system has been applied into several large-size step-up transforms and achieved large mount of on-site monitoring results, and the reliability of PD analyzing results is verified by the analysis of the onsite data.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5721292,no
Online UHF PD Monitoring for Step-up Transformers of Hydropower Plant,"As we known, to evaluate and diagnosis the insulation faults of large-size power transformers, partial discharges (PDs) can be detected via ultra high frequency (UHF) technique. In this paper, an UHF PD online monitoring system was developed for large-size step-up transformers. The principle of UHF PD monitoring method was introduced., and the hardware structure and software key techniques in the system were described. In order to achieve the integrated PD monitoring and ensure the accuracy of PD analysis, the operating environments and operating conditions of the transformers have been acquired synchronously. Meanwhile, the correlative analysis of PDs with respect to operating conditions can be performed and the characteristics of PD activities with respect to relevant states can be studied. The association analysis of PDs is performed as follows: (a) periodical PDs caused by power frequency voltage under stable operating conditions, (b) stochastic PDs caused by transient over-voltages under unstable operating conditions. At present, the system has been applied into several large-size step-up transforms and achieved large mount of on-site monitoring results, and the reliability of PD analyzing results is verified by the analysis of the onsite data.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5721302,no
Research on the Virtual Maintenance Training and Testing System of a Command and Control Equipment,"The maintenance teaching, training and testing of a certain type of control and command system fails to achieve satisfied results for the limitation in quantity and complexity. This paper discusses the development of a virtual maintenance training and testing system. Based on fault cases and B/S mode, the system realizes online virtual maintenance training and testing, greatly improving the training efficiency and effectiveness.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5721443,no
Smart Scarecrow,"Thailand is an agricultural country, where is located in Southeast Asia. We can produce various kinds of food in not only a good quality but also a huge quantity. One problem of both quality and quantity control of our food products are the food harmful pests such as bird, ant, weevil, aphid, grasshopper etc. Therefore, this project intends to develop the computer system that can be chased birds from a farm. The smart scarecrow is developed by using an image processing technique. Overall works are software development. The system is designed to detect pest birds from a real time video frame after it detects the birds then it generates a loudly sound to chase them. The system consists of four major components: 1) image acquisition 2) image preprocessing, 3) bird recognition and 4) generating sound. The experiment has been conducted in order to access the following qualities: 1) usability, to prove that the system can detect and scare pest birds and 2) efficiency, to show that the system can work with a high accuracy.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5721481,no
Selecting Oligonucleotide Probes for Whole-Genome Tiling Arrays with a Cross-Hybridization Potential,"For designing oligonucleotide tiling arrays popular, current methods still rely on simple criteria like Hamming distance or longest common factors, neglecting base stacking effects which strongly contribute to binding energies. Consequently, probes are often prone to cross-hybridization which reduces the signal-to-noise ratio and complicates downstream analysis. We propose the first computationally efficient method using hybridization energy to identify specific oligonucleotide probes. Our Cross-Hybridization Potential (CHP) is computed with a Nearest Neighbor Alignment, which efficiently estimates a lower bound for the Gibbs free energy of the duplex formed by two DNA sequences of bounded length. It is derived from our simplified reformulation of t-gap insertion-deletion-like metrics. The computations are accelerated by a filter using weighted ungapped q-grams to arrive at seeds. The computation of the CHP is implemented in our software OSProbes, available under the GPL, which computes sets of viable probe candidates. The user can choose a trade-off between running time and quality of probes selected. We obtain very favorable results in comparison with prior approaches with respect to specificity and sensitivity for cross-hybridization and genome coverage with high-specificity probes. The combination of OSProbes and our Tileomatic method, which computes optimal tiling paths from candidate sets, yields globally optimal tiling arrays, balancing probe distance, hybridization conditions, and uniqueness of hybridization.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5722951,no
Does Socio-Technical Congruence Have an Effect on Software Build Success? A Study of Coordination in a Software Project,"Socio-technical congruence is an approach that measures coordination by examining the alignment between the technical dependencies and the social coordination in the project. We conduct a case study of coordination in the IBM Rational Team Concert project, which consists of 151 developers over seven geographically distributed sites, and expect that high congruence leads to a high probability of successful builds. We examine this relationship by applying two congruence measurements: an unweighted congruence measure from previous literature, and a weighted measure that overcomes limitations of the existing measure. We discover that there is a relationship between socio-technical congruence and build success probability, but only for certain build types, and observe that in some situations, higher congruence actually leads to lower build success rates. We also observe that a large proportion of zero-congruence builds are successful, and that socio-technical gaps in successful builds are larger than gaps in failed builds. Analysis of the social and technical aspects in IBM Rational Team Concert allows us to discuss the effects of congruence on build success. Our findings provide implications with respect to the limits of applicability of socio-technical congruence and suggest further improvements of socio-technical congruence to study coordination.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5740929,no
Sub-graph Mining: Identifying Micro-architectures in Evolving Object-Oriented Software,"Developers introduce novel and undocumented micro-architectures when performing evolution tasks on object-oriented applications. We are interested in understanding whether those organizations of classes and relations can bear, much like cataloged design and anti-patterns, potential harm or benefit to an object-oriented application. We present SGFinder, a sub-graph mining approach and tool based on an efficient enumeration technique to identify recurring micro-architectures in object-oriented class diagrams. Once SGFinder has detected instances of micro-architectures, we exploit these instances to identify their desirable properties, such as stability, or unwanted properties, such as change or fault proneness. We perform a feasibility study of our approach by applying SGFinder on the reverse-engineered class diagrams of several releases of two Java applications: ArgoUML and Rhino. We characterize and highlight some of the most interesting micro-architectures, e.g., the most fault prone and the most stable, and conclude that SGFinder opens the way to further interesting studies.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5741259,no
Revealing Mistakes in Concern Mapping Tasks: An Experimental Evaluation,"Concern mapping is the activity of assigning a stakeholder's concern to its corresponding elements in the source code. This activity is primordial to guide software maintainers in several tasks, such as understanding and restructuring the implementation of existing concerns. Even though different techniques are emerging to facilitate the concern mapping process, they are still manual and error-prone according to recent studies. Existing work does not provide any guidance to developers to review and correct concern mappings. In this context, this paper presents the characterization and classification of eight concern mapping mistakes commonly made by developers. These mistakes were found to be associated with various properties of concerns and modules in the source code. The mistake categories were derived from actual mappings of 10 concerns in 12 versions of industry systems. In order to further evaluate to what extent these mistakes also occur in wider contexts, we ran two experiments where 26 subjects mapped 10 concerns in two systems. Our experimental results confirmed the mapping mistakes that often occur when developers need to interact with the source code.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5741266,no
Assistance System for OCL Constraints Adaptation during Metamodel Evolution,"Metamodels evolve over time, as well as other artifacts. In most cases, this evolution is performed manually by stepwise adaptation. In most cases, metamodels are described using the MOF language. Often OCL constraints are added to metamodels in order to ensure consistency of their instances (models). However, during metamodel evolution these constraints are omitted or manually rewritten, which is time consuming and error prone. We propose a tool to help the designer to make a decision on the constraints attached to a metamodel during its evolution. Thus, the tool highlights the constraints that should disappear after evolution and makes suggestions for those which need adaptation to remain consistent. For the latter case, we formally describe how the OCL constraints have to be transformed to preserve their syntactical correctness. Our adaptation rules are defined using QVT which is the OMG standard language for specifying model-to-model transformations.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5741271,no
Using Multivariate Split Analysis for an Improved Maintenance of Automotive Diagnosis Functions,"The amount of automotive software functions is continuously growing. With their interactions and dependencies increasing, the diagnosis' task of differencing between symptoms indicating a fault, the fault cause itself and uncorrelated data gets enormously difficult and complex. For instance, up to 40% of automotive software functions are contributable to diagnostic functions, resulting in approximately three million lines of diagnostic code. The diagnosis' complexity is additionally increased by legal requirements forcing automotive manufacturers maintaining the diagnosis of their cars for 15 years after the end of the car's series production. Clearly, maintaining these complex functions over such an extend time span is a difficult and tedious task. Since data from diagnosis incidents has been transferred back to the OEMs for some years, analysing this data with statistic techniques promises a huge facilitation of the diagnosis' maintenance. In this paper we use multivariate split analysis to filter diagnosis data for symptoms having real impact on faults and their repair measures, thus detecting diagnosis functions which have to be updated as they contain irrelevant or erroneous observations and/or repair measurements. A key factor for performing an unbiased split analysis is to determine an ideally representative control data set for a given test data set showing some property whose influence is to be studied. In this paper, we present a performant algorithm for creating such a representative control data set out of a very large initial data collection. This approach facilitates the analysis and maintenance of diagnosis functions. It has been successfully evaluated on case studies and is part of BMW's continuous improvement process for automotive diagnosis.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5741278,no
Safety Monitoring for ETCS with 4-valued LTL,"When verifying the safety of ETCS, testing and formal methods have limitations to some degree. Runtime verification is effective to detect deviation between the current and the expected system behaviors. To improve the accuracy of runtime monitoring, 4-valued LTL (Linear Time Logic) semantics and formula rewriting based algorithm are proposed. Furthermore, approximation technique is presented for 4-valued LTL formulae to make the verification procedure high efficient. Finally, the method is applied to the European Train Control System (ETCS) by monitoring several scenario traces. The experimental results show that the 4-valued LTL semantics are able to generate the most accurate verification outcomes. It can also be found that the approximation technique improves the verification efficiency apparently in some cases.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5741284,no
On the Utility of a Defect Prediction Model during HW/SW Integration Testing: A Retrospective Case Study,"Testing is an important and cost-intensive part of the software development life cycle. Defect prediction models try to identify error-prone components, so that these can be tested earlier or more in-depth, and thus improve the cost-effectiveness during testing. Such models have been researched extensively, but whether and when they are applicable in practice is still debated. The applicability depends on many factors, and we argue that it cannot be analyzed without a specific scenario in mind. In this paper, we therefore present an analysis of the utility for one case study, based on data collected during the hardware/software integration test of a system from the avionic domain. An analysis of all defects found during this phase reveals that more than half of them are not identifiable by a code-based defect prediction model. We then investigate the predictive performance of different prediction models for the remaining defects. The small ratio of defective instances results in relatively poor performance. Our analysis of the cost-effectiveness then shows that the prediction model is not able to outperform simple models, which order files either randomly or by lines of code. Hence, in our setup, the application of defect prediction models does not offer any advantage in practice.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5741333,no
Prioritizing Requirements-Based Regression Test Cases: A Goal-Driven Practice,"Any changes for maintenance or evolution purposes may break existing working features, or may violate the requirements established in the previous software releases. Regression testing is essential to avoid these problems, but it may be ended up with executing many time-consuming test cases. This paper tries to address prioritizing requirements-based regression test cases. To this end, system-level testing is focused on two practical issues in industrial environments: i) addressing multiple goals regarding quality, cost and effort in a project, and ii) using non-code metrics due to the lack of detailed code metrics in some situations. This paper reports a goal-driven practice at Research In Motion (RIM) towards prioritizing requirements-based test cases regarding these issues. Goal-Question-Metric (GQM) is adopted in identifying metrics for prioritization. Two sample goals are discussed to demonstrate the approach: detecting bugs earlier and maintaining testing effort. We use two releases of a prototype Web-based email client to conduct a set of experiments based on the two mentioned goals. Finally, we discuss lessons learned from applying the goal-driven approach and experiments, and we propose few directions for future research.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5741354,no
On demand check pointing for grid application reliability using communicating process model,The objective of the work is to propose an on-demand asynchronous check pointing technique for the fault recovery of a grid application in communicating process approach. The formal modelling of processes using LOTOS is done wherein the process features are declared in terms of possibilities of rollback and replicas permitted to accept the assigned tasks as decided by the scheduler. If any process is tending to be faulty in run time that will be detected by check pointing mechanism through the Task Dependency Graph (TDG) and their respective worst case execution time and dead line parameters are used to decide the schedulability. The Asynchronous Check Pointing On Demand (ACP-OD) approach is used to enhance the grid application reliability through the needed fault tolerant services. The scheduling of concurrent tasks can be done using the proposed Concurrent Task Scheduling Algorithm (CTSA) algorithm to recover from the faulty states using replication or rollback techniques. The check pointing and replication mechanisms have been used in which the synchronization between communicating processes is needed to enhance the efficiency of check pointing mechanism. The model is tested with a number of rollback variables treating the application as a Stochastic Activity Network (SAN) using Mobius.,2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5745839,no
Blind Image Quality Assessment Using a General Regression Neural Network,"We develop a no-reference image quality assessment (QA) algorithm that deploys a general regression neural network (GRNN). The new algorithm is trained on and successfully assesses image quality, relative to human subjectivity, across a range of distortion types. The features deployed for QA include the mean value of phase congruency image, the entropy of phase congruency image, the entropy of the distorted image, and the gradient of the distorted image. Image quality estimation is accomplished by approximating the functional relationship between these features and subjective mean opinion scores using a GRNN. Our experimental results show that the new method accords closely with human subjective judgment.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5746647,no
SCIPS: An emulation methodology for fault injection in processor caches,"Due to the high level of radiation endured by space systems, fault-tolerant verification is a critical design step for these systems. Space-system designers use fault-injection tools to introduce system faults and observe the system's response to these faults. Since a processor's cache accounts for a large percentage of total chip area and is thus more likely to be affected by radiation, the cache represents a key system component for fault-tolerant verification. Unfortunately, processor architectures limit cache accessibility, making direct fault injection into cache blocks impossible. Therefore, cache faults can be emulated by injecting faults into data accessed by load instructions. In this paper, we introduce SPFI-TILE, a software-based fault-injection tool for many-core devices. SPFI-TILE emulates cache fault injections by randomly injecting faults into load instructions. In order to provide unbiased fault injections, we present the cache fault-injection methodology SCIPS (Smooth Cache Injection Per Skipping). Results from MATLAB simulation and integration with SPFI-TILE reveal that SCIPS successfully distributes fault-injection probabilities across load instructions, providing an unbiased evaluation and thus more accurate verification of fault tolerance in cache memories.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5747450,no
Fault tolerance in ZigBee wireless sensor networks,"Wireless sensor networks (WSN) based on the IEEE 802.15.4 Personal Area Network standard are finding increasing use in the home automation and emerging smart energy markets. The network and application layers, based on the ZigBee 2007 PRO Standard, provide a convenient framework for component-based software that supports customer solutions from multiple vendors. This technology is supported by System-on-a-Chip solutions, resulting in extremely small and low-power nodes. The Wireless Connections in Space Project addresses the aerospace flight domain for both flight-critical and non-critical avionics. WSNs provide the inherent fault tolerance required for aerospace applications utilizing such technology. The team from Ames Research Center has developed techniques for assessing the fault tolerance of ZigBee WSNs challenged by radio frequency (RF) interference or WSN node failure.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5747474,no
Data-driven framework for detecting anomalies in field failure data,"This paper discusses the design of a data-driven framework for detecting anomalies in the automotive field failure and repair data. The anomaly detection framework detects anomalies at two levels: 1) It detects anomalies in repair data using system-level fault model (or fault dependency-matrix) and diagnostic reasoner; 2) It detects anomalies in diagnostic trouble code (DTC) data using operating sensory parameter identifiers (PIDs) data mining. The system-level fault model provides a way to capture causal relationships between failures and symptoms of a given system. A repair is declared as anomalous if it does not match the repair recommended by the fault model and diagnostic reasoner. The PIDs data mining detects anomalies in DTC data by detecting patterns in the associated PIDs using various statistical techniques such as scatter plots, clustering and decision trees. The DTC anomalies could be either due to errors in the preconditions under which the DTCs are designed to set or errors while implementing them in the software. The PIDs data mining module provides a focused feedback to engineers for detecting the errors in DTC software algorithms and enhancing the diagnostic design of DTCs during the early stages of vehicle production. We demonstrate the data-driven framework on automobile fuel vapor pressure sensor problem.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5747580,no
Automated generation of test cases from output domain and critical regions of embedded systems using genetic algorithms,"A primary issue in black-box testing is how to generate adequate test cases from input domain of the system under test on the basis of user's requirement specification. However, for some types of systems including embedded systems, developing test cases from output domain is more suitable than developing from input domain, especially, when the output domain is smaller. Exhaustive testing of the embedded systems in the critical regions is important as the embedded systems must be basically fail safe systems. The Critical regions of the input space of the embedded systems can be pre-identified and supplied as seeds. In this paper, the authors presents an Automated Test Case Generator (ATCG) that uses Genetic algorithms (GAs) to automate the generation of test cases from output domain and the criticality regions of an embedded System. The approach is applied to a pilot project `Temperature monitoring and controlling of Nuclear Reactor System' (TMCNRS) which is an embedded system developed using modified Cleanroom Software Engineering methodology. The ATCG generates test cases which are useful to conduct pseudo-exhaustive testing to detect single, double and several multimode faults in the system. The generator considers most of the combinations of outputs, and finds the corresponding inputs while optimizing the number of test cases generated.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5751411,no
Predicting the software performance during feasibility study,"Performance is an important non-functional attribute to be considered for producing quality software. Software performance engineering (SPE) is a methodology having significant role in software engineering to assess the performance of software systems early in the lifecycle. Gathering performance data is an essential aspect of SPE approach. The authors have proposed a methodology to gather data during feasibility study by exploiting the use case point approach, gearing factor and COCOMO model. The proposed methodology is used to estimate the performance data required for performance assessment in the integrated performance prediction process (IP<sup>3</sup>) model. The gathered data is used as the input for solving the two models, (i) use case performance model and (ii) system model. The methodology is illustrated with a case study of airline reservation application. A regression analysis is carried out to validate the response time obtained in the use case performance model. The analysis shows the proposed estimation can be used along with performance walkthrough in data gathering. The performance metrics are obtained by solving the system model, and the behaviour of the hardware resources is observed. Bottleneck resources are identified and the performance parameters are optimised using sensitivity analysis.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5751770,no
Effort estimation of component-based software development  a survey,"Effort estimation of software development is an important sub-discipline in software engineering. It has been the focus of much research mostly over the last couple of decades. In recent years, software development turned into engineering through the introduction of component-based software development (CBSD). The industry has reported significant advantages in using CBSD over traditional software development paradigms. However, the introduction of CBSD has also brought a host of unique challenges to software effort estimation which are quite different from those associated with traditional software development. Owing to the increasing tendency to use the CBSD approach in recent years, it is clear that effort estimation of CBSD is particularly an important area of research with a direct relevance to industry. In this study, the authors survey the most up-to-date research work published on predicting the effort of CBSD. The authors analyse the surveyed approaches in terms of modelling technique, the type of data required for their use, the type of estimation provided, lifecycle activities covered and their level of acceptability with regard to any validation. The aim of this survey is to provide a better understanding of the cost and schedule estimation approaches for CBSD.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5751771,no
Using lightweight virtual machines to achieve resource adaptation in middleware,"Current middleware does not offer enough support to cover the demands of emerging application domains, such as embedded systems or those featuring distributed multimedia services. These kinds of applications often have timeliness constraints and yet are highly susceptible to dynamic and unexpected changes in their environment. There is then a clear need to introduce adaptation in order for these applications to deal with such unpredictable changes. Resource adaptation can be achieved by using scheduling or allocation algorithms, for large-scale applications, but such a task can be complex and error-prone. Virtual machines (VMs) represent a higher-level approach, whereby resources can be managed without dealing with lower-level details, such as scheduling algorithms, scheduling parameters and so on. However, the overhead penalty imposed by traditional VMs is unsuitable for real-time applications. On the other hand, virtualisation has not been previously exploited as a means to achieve resource adaptation. This study presents a lightweight VM framework that exploits application-level virtualisation to achieve resource adaptation in middleware for soft real-time applications. Experimental results are presented to validate the approach.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5751772,no
Investigation of automatic prediction of software quality,"The subjective nature of software code quality makes it a complex topic. Most software managers and companies rely on the subjective evaluations of experts to determine software code quality. Software companies can save time and money by utilizing a model that could accurately predict different code quality factors during and after the production of software. Previous research builds a model predicting the difference between bad and excellent software. This paper expands this to a larger range of bad, poor, fair, good, and excellent, and builds a model predicting these classes. This research investigates decision trees and ensemble learning from the machine learning tool Weka as primary classifier models predicting reusability, flexibility, understandability, functionality, extendibility, effectiveness, and total quality of software code.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5751946,no
Towards near-real time data property specification and verification for Arctic hyperspectral sensor data,"Environmental scientists, especially those conducting studies in remote areas such as the Arctic, can benefit from assessing data quality from autonomous sensors in near-real time. The Data Assessment Run-Time (DART) framework was developed to allow environmental scientists to specify and verify data properties associated with autonomous sensors. Data properties are logical statements about data values associated with sensors and their relationship with other sensor output or properties derived from historical data. The properties can be verified at near-real time, i.e., as the data are being collected in the field, or through post-processing routines after the data has been collected. This paper describes a case study that evaluates the specification of data properties associated with hyperspectral sensor data and how the DART framework was used to verify these data in both near-real time and through post-processing.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5752047,no
An End-to-End Virtual Path Construction System for Stable Live Video Streaming over Heterogeneous Wireless Networks,"In this paper, we propose an effective end-to-end virtual path construction system, which exploits path diversity over heterogeneous wireless networks. The goal of the proposed system is to provide a high quality live video streaming service over heterogeneous wireless networks. First, we propose a packetization-aware fountain code to integrate multiple physical paths efficiently and increase the fountain decoding probability over wireless packet switching networks. Second, we present a simple but effective physical path selection algorithm to maximize the effective video encoding rate while satisfying delay and fountain decoding failure rate constraints. The proposed system is fully implemented in software and examined over real WLAN and HSDPA networks.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5753567,no
Fault Management of Robot Software Components Based on OPRoS,"Component-based robot development has been a vibrant research topic in robotics due to its reusability and interoperability benefits. However, robot application developers using robot components must invest non-trivial amount of time and effort applying fault tolerance techniques into their robot applications. Despite the need for a common, framework-level fault management, the majority of existing robot software frameworks has failed to provide systematic fault management features. In this paper, we propose a fault management method to detect, diagnose, isolate and recover faults based on the OPRoS software framework. The proposed method provides a collective, framework-level management for commonly encountered robot software faults, thereby reducing the application developers' efforts while enhancing the robot system reliability. To verify the effectiveness of the proposed approach, we have implemented a prototype reconnaissance robot using OPRoS components and injected different types of faults. The results of the experiments have shown that our approach effectively detects, diagnoses, and recovers component faults using the software framework.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5753614,no
Intelligent Trend Indices in Detecting Changes of Operating Conditions,"Temporal reasoning is a very valuable tool to diagnose and control slow processes. Identified trends are also used in data compression and fault diagnosis. Although humans are very good at visually detecting such patterns, for control system software it is a difficult problem including trend extraction and similarity analysis. In this paper, an intelligent trend index is developed from scaled measurements. The scaling is based on monotonously increasing, nonlinear functions, which are generated with generalised norms and moments. The monotonous increase is ensured with constraint handling. Triangular episodes are classified with the trend index and the derivative of it. Severity of the situations is evaluated by a deviation index which takes into account the scaled values of the measurements.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5754208,no
Software reliability model with bathtub-shaped fault detection rate,"This paper proposes a software reliability model with a bathtub-shaped fault detection rate. We discuss how the inherent characteristics of the software testing process support the three phases of the bathtub; the first phase with a decreasing fault detection rate arises from the removal of simple, yet frequent faults like syntax errors and typos; the second phase possesses a constant fault detection rate marking the beginning of functional requirements testing; the third and final code comprehension stage exhibits an increasing fault detection rate because testers are now familiar with the system and can focus their attention on the outstanding and as yet untested portions of code. We also discuss how eliminating one of the testing phases gives rise to the burn-in model, which is a special case of the bathtub model. We compare the performance of the bathtub and burn-in models with the three classical software reliability models using the Predictive Mean Square Error and Akaike Information Criterion, by applying these models to a data set in the literature. Our results suggest that the bathtub model best describes the observed data and also most precisely predicts the future data points compared to the other popular software reliability models. The bathtub model can thus be used to provide accurate predictions during the testing process and guide optimal release time decisions.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5754490,no
Consequence Oriented Self-Healing and Autonomous Diagnosis for Highly Reliable Systems and Software,"Computing software and systems have become increasingly large and complex. As their dependability and autonomy are of great concern, self-healing is an ongoing challenge. This paper presents an innovative model and technology to realize the self-healing function under the real-time requirement. The proposed approach, different from existing technologies, is based on a new concept defined as consequence-oriented diagnosis and healing. Derived from the new concept, a prototype model for proactive self-healing actions is presented. Then, a hybrid diagnosis tool is proposed that takes advantages from the Multivariate Decision Diagram, Fuzzy Logic, and Neural Networks, achieving an efficient, effective, accurate, and intelligent result. The consequence-oriented diagnosis and self-healing function is also implemented. The experimental results exhibit that the innovative system is very effective and precise in predicting the consequence, and in preventing resulting software and system failures.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5755138,no
Blind Image Quality Assessment: From Natural Scene Statistics to Perceptual Quality,"Our approach to blind image quality assessment (IQA) is based on the hypothesis that natural scenes possess certain statistical properties which are altered in the presence of distortion, rendering them un-natural; and that by characterizing this un-naturalness using scene statistics, one can identify the distortion afflicting the image and perform no-reference (NR) IQA. Based on this theory, we propose an (NR)/blind algorithm-the Distortion Identification-based Image Verity and INtegrity Evaluation (DIIVINE) index-that assesses the quality of a distorted image without need for a reference image. DIIVINE is based on a 2-stage framework involving distortion identification followed by distortion-specific quality assessment. DIIVINE is capable of assessing the quality of a distorted image across multiple distortion categories, as against most NR IQA algorithms that are distortion-specific in nature. DIIVINE is based on natural scene statistics which govern the behavior of natural images. In this paper, we detail the principles underlying DIIVINE, the statistical features extracted and their relevance to perception and thoroughly evaluate the algorithm on the popular LIVE IQA database. Further, we compare the performance of DIIVINE against leading full-reference (FR) IQA algorithms and demonstrate that DIIVINE is statistically superior to the often used measure of peak signal-to-noise ratio (PSNR) and statistically equivalent to the popular structural similarity index (SSIM). A software release of DIIVINE has been made available online: http://live.ece.utexas.edu/research/quality/DIIVINE_release.zip for public use and evaluation.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5756237,no
Transmission line fault detection and classification,"Transmission line protection is an important issue in power system engineering because 85-87% of power system faults are occurring in transmission lines. This paper presents a technique to detect and classify the different shunt faults on a transmission lines for quick and reliable operation of protection schemes. Discrimination among different types of faults on the transmission lines is achieved by application of evolutionary programming tools. PSCAD/EMTDC software is used to simulate different operating and fault conditions on high voltage transmission line, namely single phase to ground fault, line to line fault, double line to ground and three phase short circuit. The discrete wavelet transform (DWT) is applied for decomposition of fault transients, because of its ability to extract information from the transient signal, simultaneously both in time and frequency domain. The data sets which are obtained from the DWT are used for training and testing the SVM architecture. After extracting useful features from the measured signals, a decision of fault or no fault on any phase or multiple phases of a transmission line is carried out using three SVM classifiers. The ground detection task is carried out by a proposed ground index. Gaussian radial basis kernel function (RBF) has been used, and performances of classifiers have been evaluated based on fault classification accuracy. In order to determine the optimal parametric settings of an SVM classifier (such as the type of kernel function, its associated parameter, and the regularization parameter c), fivefold cross-validation has been applied to the training set. It is observed that an SVM with an RBF kernel provides better fault classification accuracy than that of an SVM with polynomial kernel. It has been found that the proposed scheme is very fast and accurate and it proved to be a robust classifier for digital distance protection.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5760084,no
Experimental study report on Opto-electronic sensor based gaze tracker system,The paper presents a smart assistive technology to improve the life quality of the people with severe mobility disorders by giving them independence of motion despite the difficulties in moving their limbs. The objective of this paper is to develop an Opto-electronic sensor based gaze tracker apparatus to detect the eye gaze direction based on movement of iris. Eventually this detection helps the user to control and steer the wheel chair by themselves through their eye gaze. The principle of operation of this system is based on the reflection of incidental light in the iris and sclera regions of eye. The user's gaze tracker is in the form of eye-goggle embedded with infrared source and detectors. The performance of various optical sources and detectors are tested and the results are graphically represented. A template database and also an algorithm is generated to provide real time computational analysis.,2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5760182,no
Network architecture for smart grids,"Smart grid is designed to make the existing power grid system function spontaneously and independently without human intervention. Sensors are deployed to detect faults in the flow of power. In the proposed work, smart monitoring and controls are done by intelligent electronic devices. IED's (Intelligent electronic devices) monitors and records the value of power generated, and its corresponding voltage and frequency, which in turn is fed in to the demand-supply chain. Network architecture is designed to determine the flow of power from the generation end to the consumers. Demand-supply curve is embedded in architecture to map the power generated with the supply. If the demand at a particular instant of time is higher than the supply, then tariff is fixed and warning is given to the users regarding the rate of payment, to meet the higher tariff. Trust based authencity is provided for security.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5762480,no
A single-specification principle for functional-to-timing simulator interface design,"Microarchitectural simulators are often partitioned into separate, but interacting, functional and timing simulators. These simulators interact through some interface whose level of detail depends upon the needs of the timing simulator. The level of detail supported by the interface profoundly affects the speed of the functional simulator, therefore, it is desirable to provide only the detail that is actually required. However, as the microarchitectural design space is explored, these needs may change, requiring corresponding time-consuming and error-prone changes to the interface. Thus simulator developers are tempted to include extra detail in the interface """"just in case"""" it is needed later, trading off simulator speed for development time. We show that this tradeoff is unnecessary if a single-specification design principle is practiced: write the simulator once with an extremely detailed interface and then derive less-detailed interfaces from this detailed simulator. We further show that the use of an Architectural Description Language (ADL) with constructs for interface specification makes it possible to synthesize simulators with less-detailed interfaces from a highly-detailed specification with only a few lines of code and minimal effort. The speed of the resulting low-detail simulators is up to 14.4 times the speed of high-detail simulators.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5762735,no
Architectures for online error detection and recovery in multicore processors,"The huge investment in the design and production of multicore processors may be put at risk because the emerging highly miniaturized but unreliable fabrication technologies will impose significant barriers to the life-long reliable operation of future chips. Extremely complex, massively parallel, multi-core processor chips fabricated in these technologies will become more vulnerable to: (a) environmental disturbances that produce transient (or soft) errors, (b) latent manufacturing defects as well as aging/wearout phenomena that produce permanent (or hard) errors, and (c) verification inefficiencies that allow important design bugs to escape in the system. In an effort to cope with these reliability threats, several research teams have recently proposed multicore processor architectures that provide low-cost dependability guarantees against hardware errors and design bugs. This paper focuses on dependable multicore processor architectures that integrate solutions for online error detection, diagnosis, recovery, and repair during field operation. It discusses taxonomy of representative approaches and presents a qualitative comparison based on: hardware cost, performance overhead, types of faults detected, and detection latency. It also describes in more detail three recently proposed effective architectural approaches: a software-anomaly detection technique (SWAT), a dynamic verification technique (Argus), and a core salvaging methodology.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5763096,no
Modeling manufacturing process variation for design and test,"For process nodes 22nm and below, a multitude of new manufacturing solutions have been proposed to improve the yield of devices being manufactured. With these new solutions come an increasing number of defect mechanisms. There is a need to model and characterize these new defect mechanisms so that (i) ATPG patterns can be properly targeted, (ii) defects can be properly diagnosed and addressed at design or manufacturing level. This presentation reviews currently available defect modeling and test solutions and summarizes open issues faced by the industry today. It also explores the topic of creating special test structures to expose manufacturing process parameters which can be used as input to software defect models to predict die specific defect locations for better targeting of test.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5763192,no
Multi-level attacks: An emerging security concern for cryptographic hardware,"Modern hardware and software implementations of cryptographic algorithms are subject to multiple sophisticated attacks, such as differential power analysis (DPA) and fault-based attacks. In addition, modern integrated circuit (IC) design and manufacturing follows a horizontal business model where different third-party vendors provide hardware, software and manufacturing services, thus making it difficult to ensure the trustworthiness of the entire process. Such business practices make the designs vulnerable to hard-to-detect malicious modifications by an adversary, termed as Hardware Trojans. In this paper, we show that malicious nexus between multiple parties at different stages of the design, manufacturing and deployment makes the attacks on cryptographic hardware more potent. We describe the general model of such an attack, which we refer to as Multi-level Attack, and provide an example of it on the hardware implementation of the Advanced Encryption Standard (AES) algorithm, where a hardware Trojan is embedded in the design. We then analytically show that the resultant attack poses a significantly stronger threat than that from a Trojan attack by a single adversary. We validate our theoretical analysis using power simulation results as well as hardware measurement and emulation on a FPGA platform.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5763307,no
Markov Chain Based Monitoring Service for Fault Tolerance in Mobile Cloud Computing,"Mobile cloud computing is a combination of mobile computing and cloud computing, and provides cloud computing environment through various mobile devices. Recently, due to rapid expansion of smart phone market and wireless communication environment, mobile devices are considered as resource for large scale distributed processing. But mobile devices have several problems, such as unstable wireless connection, limitation of power capacity, low communication bandwidth and frequent location changes. As resource providers, mobile devices can join and leave the distributed computing environment unpredictably. This interrupts the undergoing operation, and the delay or failure of completing the operation may cause a system failure. Because of low reliability and no-guarantee of completing an operation, it is difficult to use a mobile device as a resource. That means that mobile devices are volatile. Therefore, we should consider volatility, one of dynamic characteristics of mobile devices, for stable resource provision. In this paper, we propose a monitoring technique based on the Markov Chain model, which analyzes and predicts resource states. With the proposed monitoring technique and state prediction, a cloud system will get more resistant to the fault problem caused by the volatility of mobile devices. The proposed technique diminishes the volatility of a mobile device through modeling the patterns of past states and making a prediction of future state of a mobile device.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5763554,no
Adaptive Genetic Algorithm for QoS-aware Service Selection,"An adaptive Genetic Algorithm is presented to select optimal web service composite plan from a lot of composite plans on the basis of global Quality-of-Service (QoS) constraints. In this Genetic Algorithm, a population diversity measurement and an adaptive crossover strategy are proposed to further improve the efficiency and convergence of Genetic Algorithm. The probability value of the crossover operation can be set according to the combination of population diversity and individual fitness. The algorithm can get more excellent composite service plan because it accords with the characteristic of web service selection very well. Some simulation results on web service selection with global QoS constraints have shown that the adaptive Genetic Algorithm can gain quickly better composition service plan that satisfies the global QoS requirements.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5763674,no
Software faults prediction using multiple classifiers,"In recent years, the use of machine learning algorithms (classifiers) has proven to be of great value in solving a variety of problems in software engineering including software faults prediction. This paper extends the idea of predicting software faults by using an ensemble of classifiers which has been shown to improve classification performance in other research fields. Benchmarking results on two NASA public datasets show all the ensembles achieving higher accuracy rates compared with individual classifiers. In addition, boosting with AR and DT as components of an ensemble is more robust for predicting software faults.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5763845,no
A systematic approach to assemble sequence diagrams from use case scenarios,"The critical task of developing executable system-level sequence diagrams to represent those scenarios remains a manual task that has to be entirely performed by the tester. This obviously is time consuming, error prone and very costly, since even the smallest systems can potentially have a large number of scenarios. In this paper, we propose an approach to semi-automate the construction of system-level sequence diagrams. The approach is based on a traceability framework, which allows its users to efficiently specify scenarios at a high level using use case descriptions, while systematically building the corresponding sequence diagrams. An ATM case study is presented to demonstrate the feasibility of the proposed approach.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5763878,no
Phase-based tuning for better utilization of performance-asymmetric multicore processors,"The latest trend towards performance asymmetry among cores on a single chip of a multicore processor is posing new challenges. For effective utilization of these performance-asymmetric multicore processors, code sections of a program must be assigned to cores such that the resource needs of code sections closely matches resource availability at the assigned core. Determining this assignment manually is tedious, error prone, and significantly complicates software development. To solve this problem, we contribute a transparent and fully-automatic process that we call phase-based tuning which adapts an application to effectively utilize performance-asymmetric multicores. Compared to the stock Linux scheduler we see a 36% average process speedup, while maintaining fairness and with negligible overheads.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5764670,no
A comparison study of automatic speech quality assessors sensitive to packet loss burstiness,"The paper delves the behavior rating of new emerging automatic quality assessors of VoIP calls subject to bursty packet loss process. The examined speech quality assessment (SQA) algorithms are able to estimate speech quality of live VoIP calls at run-time using control information extracted from header content of received packets. They are especially designed to be sensitive to packet loss burstiness. The performance evaluation study is performed using a dedicated set-up software-based SQA framework. It offers a personalized packet killer and includes implementation of four SQA algorithms. A speech quality database, which covers a wide range of bursty packet loss conditions, has been created then thoroughly analyzed. Our important findings are the following: (1) all examined automatic bursty-loss aware speech quality assessors achieve a satisfactory correlation under upper (>;20%) and lower (<;10%) ranges of packet loss process (2) They exhibit a clear weakness to assess speech quality under a moderated packet loss process (3) The accuracy of sequence-by-sequence basis of examined SQA algorithms should be addressed in details for further precision.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5766503,no
Predicting upgrade failures using dependency analysis,"Upgrades in component based systems can disrupt other components. Being able to predict the possible consequence of an upgrade just by analysing inter-component dependencies can avoid errors and downtime. In this paper we precisely identify in a repository the components p whose upgrades force a large set of others components to be upgraded. We are also able to discriminate whether all the future versions of p have the same impact, or whether there are different classes of future versions that have different impacts. We perform our analysis on Debian, one of the largest FOSS distributions.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5767626,no
Software reliability prediction model based on PSO and SVM,"Software reliability prediction classifies software modules as fault-prone modules and less fault-prone modules at the early age of software development. As to a difficult problem of choosing parameters for Support Vector Machine (SVM), this paper introduces Particle Swarm Optimization (PSO) to automatically optimize the parameters of SVM, and constructs a software reliability prediction model based on PSO and SVM. Finally, the paper introduces Principal Component Analysis (PCA) method to reduce the dimension of experimental data, and inputs these reduced data into software reliability prediction model to implement a simulation. The results show that the proposed prediction model surpasses the traditional SVM in prediction performance.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5768285,no
Numerical simulation of FLD based on leaner strain path for fracture on auto panel surface,"In order to emerge the traditional measurement's shortage of auto body panel, we proposed the corrected FLD based on the linear path method and calculation methods. According to the above mentioned programs and the fracture defect diagnosis, we developed a CAE module for the fracture defect analysis based on VC++ environment, and solved the problems which the traditional sheet metal forming CAE software can not accurately predict. And then a forming process of a hood is simulated by applying the proposed method and AUTOFORM. Comparison between the two simulation results is done which shows the method we proposed is better, and then we introduced some methods to optimize the adjustment amount of metal flow and the stamping dies for fracture. Some suggestions are given by investigating the adjustment amount and modification of the stamping die.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5768473,no
MoldFlow analysis and parameters optimization of button seat injecting,"The effect of the gate number and location, packing pressure and time on the filling cavity pressure, distribution of welding lines, warpage distortion quantity and shrinkage of the plastics part were analyzed by means of moldflow software for plastic electric button seat. Some possible defects in the products were predicted based on the numerical simulation. The optimal runner gate, the optimal technology scheme and the injecting process parameters were obtained. The research shows that the analysis results can provide the effective references for the injecting mold design, Practice has proved that Moldflow can be used to ensure the reasonable and efficiency of mold design.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5768662,no
Modeling of shift hydraulic system for automatic transmission,"The main functions of the shift hydraulic system for stepped automatic transmission are to generate and maintain desired clutch pressures for shifting operation, as well as to initiate gear shifts and control shift quality. It consists of supply line pressure regulation system, solenoid valve, pressure control valve (PCV), and wet clutch. This paper presents a dynamic model of the shift control system and conducts simulation based on AMESim software. The simulation model is then validated against experimental data. Because the model derived is complex, highly nonlinear and high order, which is not suitable for use in controller, the model simplification is carried out based on an energy-based model order reduction method. The results confirm that simulation analysis with AMESim can predict hydraulic system dynamic response accurately and the model simplification is instructive for controller design.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5768674,no
Assessing Oracle Quality with Checked Coverage,"A known problem of traditional coverage metrics is that they do not assess oracle quality - that is, whether the computation result is actually checked against expectations. In this paper, we introduce the concept of checked coverage - the dynamic slice of covered statements that actually influence an oracle. Our experiments on seven open-source projects show that checked coverage is a sure indicator for oracle quality - and even more sensitive than mutation testing, its much more demanding alternative.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770598,no
An Empirical Evaluation of Assertions as Oracles,"In software testing, an oracle determines whether a test case passes or fails by comparing output from the program under test with the expected output. Since the identification of faults through testing requires that the bug is both exercised and the resulting failure is recognized, it follows that oracles are critical to the efficacy of the testing process. Despite this, there are few rigorous empirical studies of the impact of oracles on effectiveness. In this paper, we report the results of one such experiment in which we exercise seven core Java classes and two sample programs with branch-adequate, input only(i.e., no oracle) test suites and collect the failures observed by different oracles. For faults, we use synthetic bugs created by the muJava mutation testing tool. In this study we evaluate two oracles: (1) the implicit oracle (or """"null oracle"""") provided by the runtime system, and (2) runtime assertions embedded in the implementation (by others) using the Java Modeling Language. The null oracle establishes a baseline measurement of the potential benefit of rigorous oracles, while the assertions represent a more rigorous approach that is sometimes used in practice. The results of our experiments are interesting. First, on a per-method basis, we observe that the null oracle catches less than 11% of the faults, leaving more than 89% uncaught. Second, we observe that the runtime assertions in our subjects are effective at catching about 53% of the faults not caught by null oracle. Finally, by analyzing the data using data mining techniques, we observe that simple, code-based metrics can be used to predict which methods are amenable to the use of assertion-based oracles with a high degree of accuracy.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770600,no
EFindBugs: Effective Error Ranking for FindBugs,"Static analysis tools have been widely used to detect potential defects without executing programs. It helps programmers raise the awareness about subtle correctness issues in the early stage. However, static defect detection tools face the high false positive rate problem. Therefore, programmers have to spend a considerable amount of time on screening out real bugs from a large number of reported warnings, which is time-consuming and inefficient. To alleviate the above problem during the report inspection process, we present EFindBugs to employ an effective two-stage error ranking strategy that suppresses the false positives and ranks the true error reports on top, so that real bugs existing in the programs could be more easily found and fixed by the programmers. In the first stage, EFindBugs initializes the ranking by assigning predefined defect likelihood for each bug pattern and sorting the error reports by the defect likelihood in descending order. In the second stage, EFindbugs optimizes the initial ranking self-adaptively through the feedback from users. This optimization process is executed automatically and based on the correlations among error reports with the same bug pattern. Our experiment on three widely-used Java projects (AspectJ, Tomcat, and Axis) shows that our ranking strategy outperforms the original ranking in Find Bugs in terms of precision, recall and F1-score.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770619,no
Finding Software Vulnerabilities by Smart Fuzzing,"Nowadays, one of the most effective ways to identify software vulnerabilities by testing is the use of fuzzing, whereby the robustness of software is tested against invalid inputs that play on implementation limits or data boundaries. A high number of random combinations of such inputs are sent to the system through its interfaces. Although fuzzing is a fast technique which detects real errors, its efficiency should be improved. Indeed, the main drawbacks of fuzz testing are its poor coverage which involves missing many errors, and the quality of tests. Enhancing fuzzing with advanced approaches such as: data tainting and coverage analysis would improve its efficiency and make it smarter. This paper will present an idea on how these techniques when combined give better error detection by iteratively guiding executions and generating the most pertinent test cases able to trigger potential vulnerabilities and maximize the coverage of testing.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770635,no
Cost Optimizations in Runtime Testing and Diagnosis of Systems of Systems,"In practically all development processes tests are used to detect the presence of faults. This is not an exception for critical and high-availability systems. However, these systems cannot be taken offline or duplicated for testing in some cases. This makes runtime testing necessary. This paper presents work aimed at optimizing the three main sources of testing cost: preparation, execution and diagnosis. First, preparation cost is optimized by defining a metric of the runtime testability of the system, used to elaborate an implementation plan of preparative work for runtime testing. Second, the interrelated nature of test execution cost and diagnostic cost is highlighted and a new diagnostic test prioritization is introduced.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770638,no
Constraint generation for software-based post-silicon bug masking with scalable resynthesis technique for constraint optimization,"Due to the dramatic increase in design complexity, verifying the functional correctness of a circuit is becoming more difficult. Therefore, bugs may escape all verification efforts and be detected after tape-out. While most existing solutions focus on fixing the problem on the hardware, in this work we propose a different methodology that tries to generate constraints which can be used to mask the bugs using software. This is achieved by utilizing formal reachability analysis to extract the conditions that can trigger the bugs. By synthesizing the bug conditions, we can derive input constraints for the software so that the hardware bugs will never be exposed. In addition, we observe that such constraints have special characteristics: they have small onset terms and flexible minterms. To facilitate the use of our methodology, we also propose a novel resynthesis technique to reduce the complexity of the constraints. In this way, software can be modified to run correctly on the buggy hardware, which can improve system quality without the high cost of respin.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770722,no
Occurrence probability analysis of a path at the architectural level,"In this paper, we propose an algorithm to compute the occurrence probability for a given path precisely in an acyclic synthesizable VHDL or software code. This can be useful for the ranking of critical paths and in a variety of problems that include compiler-level architectural optimization and static timing analysis for improved performance. Functions that represent condition statements at the basic blocks are manipulated using Binary Decision Diagrams (BDDs). Experimental results show that the proposed method outperforms the traditional Monte Carlo simulation approach. The later is shown to be non-scalable as the number of inputs increases.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770768,no
Improving the control-design process in naval applications using CHIL,"Different steps are necessary to develop power-electronic systems (PES): After the concept is chosen, the controller is designed using software simulations. The control is implemented and tested on the dedicated PES. However, a successful final test requires that the controller hardware is interacting with the PES properly. For this, a real-time simulator suits best which allows to verify the function of the controller hard- and software in real-time. This is realized by simulating the entire controlled system - including the power electronics - by means of state-space equations. Furthermore, it is possible to verify the generation of the switching-signals independent of the simulation. The proposed real-time simulator offers the possibility to test the full control hardware. On the one hand side, the control algorithms can be assessed regarding quality and time expense. On the other hand, the proper operation of the switching-signal generation can be tested without endangering the costly PES.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770897,no
CT Saturation Detection Based on Waveform Analysis Using a Variable-Length Window,"Saturation of current transformers (CTs) can lead to maloperation of protective relays. Using the waveshape differences between the distorted and undistorted sections of fault current, this paper introduces a novel method to quickly detect CT saturation. First, a symmetrical variable-length window is defined for the current waveform. The least error squares technique is employed to process the current inside this window and make two estimations for the current samples exactly before and after the window. CT saturation can be identified based on the difference between these two estimations. The accurate performance of this method is independent of the CT parameters, such as CT remanence and its magnetization curve. Moreover, the proposed method is not influenced by the fault current characteristics, noise, etc., since it is based on the significant differences between the distorted and undistorted fault currents. Extensive simulation studies were performed using PSCAD/EMTDC software and the fast and reliable response of the proposed method for various conditions, including very fast and mild saturation events, was demonstrated.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5771582,no
Fast Inter-Mode Decision Algorithm Based on Contextual Mode and Priority Information for H.264/AVC Video Encoding System,"The recentH.264/AVCvideo coding standard provides a higher coding efficiency than previous standards. H.264/AVCachieves a bit rate saving of more than 50 % with many new technologies, but it shows very heavy computational complexity. In this paper, a fast mode decision scheme for inter-frame coding is proposed to reduce the computational complexity for H.264/AVC video encoding system. To reduce the block mode decision complexity in inter-frame coding, we use the contextual information based on the co-located and neighboring macroblocks (MBs) to detect a proper MB that can be early stopped. Then for the current MB, a priority information of the context is suggested for adding more mode types adaptively. The proposed algorithm shows the average speedup factors of 59.11 ~ 77.41% for various sequences with a negligible bit increment and a minimal loss of image quality, in JM 11.0 reference software.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5772336,no
Simulation Based Functional and Performance Evaluation of Robot Components and Modules,"This paper presents a simulation based test method for functional and performance evaluation of robotic components and modules. In the proposed test method, function test procedure consists of unit, state, and interface tests which assess if the functional specifications of robot component 1 or module 2 are met. As for performance test, simulation environment provides a down scaled virtual work space for performance test of robot module accommodating virtual devices in conformity with the detailed performance specifications of real robot components. The proposed method can be used for verification of reliability of robot modules and components which prevents faults of them before their usage in real applications. In addition, the developed test system can be extended to support various test conditions implying possible cost saving for additional tests.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5772438,no
Modeling Variability from Requirements to Runtime,"In software product line (SPL) engineering, a software configuration can be obtained through a valid selection of features represented in a feature model (FM). With a strong separation between requirements and reusable components and a deep impact of high level choices on technical parts, determining and configuring an well-adapted software configuration is a long, cumbersome and error-prone activity. This paper presents a modeling process in which variability sources are separated in different FMs and inter-related by propositional constraints while consistency checking and propagation of variability choices are automated. We show how the variability requirements can be expressed and then refined at design time so that the set of valid software configurations to be considered at run time may be highly reduced. Software tools support the approach and some experimentations on a video surveillance SPL are also reported.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5773382,no
Towards a MDE Transformation Workflow for Dependability Analysis,"In the last ten years, Model Driven Engineering (MDE) approaches have been extensively used for the analysis of extra-functional properties of complex systems, like safety, dependability, security, predictability, quality of service. To this purpose, engineering languages (like UML and AADL) have been extended with additional features to model the required non-functional attributes, and transformations have been used to automatically generate the analysis models to be solved by appropriate analysis tools. In most of the available works, however, the transformations are not inte grated into a more general development process, aimed to support both domain-specific design analysis and verification of extra-functional properties. In this paper we explore this research direction presenting a transformation work flow for dependability analysis that is part of an industrial-quality infrastructure for the specification, analysis and verification of extra-functional properties, currently under development within the ARTEMIS-JU CHESS project. Specifically, the paper provides the following major contributions: i) definition of the required transformation steps to automatically assess the system dependability properties starting from the CHESS Modeling Language, ii) definition of a new Intermediate Dependability Model (IDM) acting as a bridge between the CHESS Modeling Language and the low-level analysis models, iii) definition of transformations from the CHESS Modeling Language to IDM models.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5773390,no
Qualification and Selection of Off-the-Shelf Components for Safety Critical Systems: A Systematic Approach,"Mission critical systems are increasingly been developed by means of Off-The-Shelf (OTS) items since this allows reducing development costs. Crucial issues to be properly treated are (i) to assess the quality of each potential OTSitem to be used and (ii) to select the one that better fits the system requirements. Despite the importance of these issues, the current literature lacks a systematic approach to perform the previous two operations. The aim of this paper is to present a framework that can overcome this lack. Reasoning from the available product assurance standards for certifying mission critical systems, the proposed approach is based on the customized quality model that describes the quality attributes. Such quality model will guide a proper evaluation of OTS products, and the choice of which product to use is based on the outcomes of such an evaluation process. This framework represents a key solution to have a dominant role in the market of mission critical systems due to the demanding request by manufactures of such systems for an efficient qualification/certification process.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5773415,no
Preserving the Exception Handling Design Rules in Software Product Line Context: A Practical Approach,"Checking the conformance between implementation and design rules is an important activity to guarantee quality on architecture and source code. To address the current needs of dependable systems it is also important to define design rules related to the exception handling behavior. The current approaches to automatically check design rules, however, do not provide suitable ways to define design rules related to the exception handling policy of a system. This paper proposes a practical approach to preserve the exception policy of a system or a family of systems along with its evolution, based on the definition and automatic checking of exception handling design rules, that regulates how exceptions flow inside the system -- which exceptions should flow and which elements are responsible for signaling and handling them. This approach automatically generates the partial code of JUnit tests to check such rules, and use the aspect-oriented technique to support such tests. The proposed approach was applied to define and check the exception handling rules of a software product line. Four different versions were evaluated (in both object-oriented and aspect-oriented implementations) in order to evaluate whether the exception handling policy was preserved during SPL evolution. Our experience shows that the proposed approach can be used to effectively detect violations on the exception handling policy of a software product line during its evolution.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5773432,no
Discussion on questions about using artificial neural network for predicting of concrete property,"Several questions about predicting concrete property using BP artificial neural network have been discussed, including the selection of network structure, the determination of sample capacity and grouping method, the protection from over-fitting, and the comparison on precision of prediction. For the network-structure, it has been found that directly apply the consumption of raw-material and other crucial quality indices as the units of input can bring about a satisfactory result of prediction, in which a single hide layer holds 10 units and the workability along with the strength and durability formed two sub-networks simultaneously. For the sample capacity and grouping method, at least 100 sets of samples are necessary to find the intrinsic regularity, among them 1/3-1/4 should be taken as test samples. A new tactics for error-tracking has been proposed which is verified effective to avoid the over-fitting. The comparison of effectiveness and feasibility between BP neural networks and linear regression algorithm showed that BP neural networks have better performance in accuracy of prediction. Finally, an applicable software has been developed and used as examples to predict 163 sets of mixes for a ready-mixed concrete plant, to show its application in detail.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5775143,no
Formal methods and automation for system verification,"Software and hardware systems are growing fast in both functionality and complexity and consequently, the probability of delicate faults existence in these systems is also increasing. Some of these faults may result in disastrous loss in both money and time. One main goal of designing those systems is to construct better and more reliable systems, regardless of the level of their complexity. Formal methods can be used to specify such systems and be automated to verify them. In this paper, we introduce and show how we can use some of those formal methods, Propositional Logic (PL) and First Order Logic (FOL), in specifying and verifying the correctness of related system aspects.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5775479,no
A car steering rod fatigue analysis based on user conditions,"Fatigue one of the major mechanical failure, has caught more and more attention in vehicle reliability study. Fatigue test results of vehicles are usually discrete, which is caused mostly by the user's conditions. It is considered the user's real purpose in the test, which can help the users to develop or design a suitable testing program, improve the test run quality and shorten the product development cycle from concept to bulk production. This article describes the ways and methods of investigating the user's conditions, deals with the strain gauge data which is obtained from the test of the right steering link by using the professional signal processing software according to the reference of the survey results, and assesses the fatigue life of the steering link according to the rain-flow counting method and local strain-life fatigue analysis effecting on the strain spectrum finally.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5777344,no
Simulation of sensing field for electromagnetic tomography system,"Electromagnetic tomography has potential value in process measurement. The frontier of electromagnetic tomography system is the sensor array. Owing to the excited signal acting on the sensor array directly, the excited strategy and the frequency and amplitude of the signal affect the quality of the information that the detected coil acquired from the object space. Furthermore, it would affect the accuracy of the information post extracted from the object field. To improve the sensitivity of the sensor array on the changes of the object field distribution, upgrade the sensitivity and accuracy of the system and guarantee high precision and high stability of the experimental data, use the finite element simulation software COMSOL Multiphysics to analyze the excited strategy and the characteristic of the excitation frequency of electromagnetic tomography. Establish the foundation in optimal using of electromagnetic tomography system.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5777631,no
A distributed cable harness tester based on CAN bus,"This paper discusses a distributed cable harness tester based on CAN bus, which has a few functions such as connection detecting of a wire, diode orientation testing and resistor's impedance testing. In this article, application layer protocal design is researched on in particular in order to improve the tester's performance, and the software design of upper computer and the framework of handware of tester nodes are introduced in details. The tester is designed to ensure quality and reliability of cable harness. Through detecting , early failure products such as breakage circuit, short circuit and wrong conductor arrangement can be rejected. So it improves the efficiency of detection.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5777720,no
The design of circular saw blade dynamic performance detection of high-speed data acquisition system,"The dynamic properties of the circular saw blade when cutting at a high speed influence the cutting quality at a large extent. This paper gives a brief introduction about the employment of advanced sensors and detection methods together with self-detection software system detect circular saw blade vibration quickly and accurately, non-destructively at the condition of high-speed rotating of the circular saw blade. The results of the actual measurement of the different carbide circular saw blades show that this detection system has the quality of high accuracy and high reliability. It is fitful for the production process of circular saw blade body for quality control.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5777815,no
Phase Asymmetry: A New Parameter for Detecting Single-Phase Earth Faults in Compensated MV Networks,"Traditionally, the detection of high-resistance earth faults has been a difficult task in compensated medium-voltage (MV) distribution networks, mainly due to its very low-current fault. To date, several techniques have been proposed to detect them: using current injection in the neutral, superposing voltage signals, varying the value of the arc suppression coil, etc. These techniques use different detection parameters, such as fault resistance to earth, line asymmetries, or partial residual neutral voltages. In this paper, phase asymmetry is defined as a new parameter that can be used, together with the aforementioned techniques, in order to improve the reliability and efficiency of the detection process in single-phase earth faults, especially for compensated networks. The use of this parameter has been validated through extensive simulations of resistive faults up to 15 k , with the use of RESFAL software, which is based on Matlab/Simulink.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5778962,no
A Visualization Quality Evaluation Method for Multiple Sequence Alignments,"Multiple sequence alignments (MSA) method is basic way for the analysis of biology sequences. As dozens of MSA algorithms appears, reasonable and effective quality evaluation method is necessary. Gap-insertion is common phenomenon after MSA, and should be an important factor to evaluate whether MSA is successful. A new MSA score evaluation method was introduced in this paper, which was based on the combination of both row distance of letters and column consensus of sequences. It can be used to evaluate the quality of MSA in the global and local justly and effectively. At same time, two formulas for assessing the distance of different MSA algorithms were derived.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5779972,no
On the Reliability and Availability of Systems Tolerant to Stealth Intrusion,"This paper considers the estimation of reliability and availability of intrusion-tolerant systems subject to non-detectable intrusions. Our motivation comes from the observation that typical techniques of intrusion tolerance may in certain circumstances worsen the non-functional properties they were meant to improve (e.g., dependability). We start by modeling attacks as adversarial efforts capable of affecting the intrusion rate probability of components of the system. Then, we analyze several configurations of intrusion-tolerant replication and pro-active rejuvenation, to find which ones lead to security enhancements. We analyze several parameterizations, considering different attack and rejuvenation models and taking into account the mission time of the overall system and the expected time to intrusion of its components. In doing so, we identify thresholds that distinguish between improvement and degradation. We compare the effects of replication and rejuvenation and highlight their complementarity, showing improvements of resilience not attainable with any of the techniques alone, but possible only as a synergy of their combination. We advocate the need for thorougher system models, by showing fundamental vulnerabilities arising from incomplete specifications.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5783402,no
Prediction of compression bound and optimization of compression architecture for linear decompression-based schemes,"On-chip linear decompression-based schemes have been widely adopted by industrial circuits nowadays to effectively reduce the ever increasing test data volume and test time. Though they can easily achieve relatively high compression ratio, there is a bound of effective compression ratio for these compression schemes. Prior work tried to address this problem by trying different compression architectures to identify this compression bound. However, they can not predict this compression bound efficiently. In this paper, we will first analyze the correlation between the effective compression ratio and the compression architecture, thus to predict that compression bound efficiently. In addition, this paper will also propose how to design the compression architecture for target effective compression ratio with one-pass calculation, which was usually done by a time-consuming try-and-error process as well in the current DFT flow. Experimental results show the accuracy of the prediction and the effectiveness of the compression architecture design.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5783737,no
A new methodology for realistic open defect detection probability evaluation under process variations,"CMOS IC scaling has provided significant improvements in electronic circuit performance. Advances in test methodologies to deal with new failure mechanisms and nanometer issues are required. Interconnect opens are an important defect mechanism that requires detailed knowledge of its physical properties. In nanometer process, variability is predominant and considering only nominal value of parameters is not realistic. In this work, a model for computing a realistic coverage of via open defect that takes into account the process variability is proposed. Correlation between parameters of the affected gates is considered. Furthermore, spatial correlation of the parameters for those gates tied to the defective floating node can also influence the detectability of the defect. The proposed methodology is implemented in a software tool to determine the probability of detection of via opens for some ISCAS benchmark circuits. The proposed detection probability evaluation together with a test methodology to generate favorable logic conditions at the coupling lines can allow a better test quality leading to higher product reliability.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5783781,no
Ultra-high-speed protection of parallel transmission lines using current travelling waves,"This study presents a protective algorithm for discrimination and classification of short-circuit faults in parallel circuit transmission lines. The proposed algorithm is based on the initial fault-induced current travelling waves (TWs) detected by the relay using the wavelet transform. In the case of internal faults on any of the parallel circuits, the detected current TWs in the corresponding phases of the parallel circuits are different, whereas, for external faults, the initial current TWs are almost similar. This feature is used to discriminate between internal and external faults. A fault-type classification algorithm is also proposed to identify the faulted phases. The proposed algorithm only uses the initial current TWs caused by the fault and provides an ultra-high-speed protective technique. It also covers inter-circuit faults, in which phases of both parallel circuits get involved in the fault. The obtained simulation results using the PSCAD/EMTDC software show that the proposed algorithm is able to discriminate the internal faults and to select the faulted phases very rapidly and reliably. It is able to identify faults in just ~1~ms.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5783871,no
Adding code generation to develop a simulation platform,"Efficient and accurate control technologies require extensive simulation capabilities to validate the control software and demonstrate the impact on the business and equipment. To create a platform for rapid development and simulation of complex dynamic models, the authors and their colleagues have designed an object-oriented architecture. A portion of the architecture framework is constructed using code generation based on XML component definitions. This paper describes the key aspects of the architecture of the control simulator platform and its code generation capabilities. The simulation platform consists of defining a collection of components represented by differential equations, the capability to select, configure and interconnect components, and the ability to solve the coupled set of equations. The code generation is custom-built, however, the generated code results in more consistency and improved reliability by eliminating error prone steps and allowing the simulation engineer to focus on component mathematical description. As the number of components that are generated increases, the investment in a custom-built code generation is quickly realized by significantly reducing the amount of time required to create or update the code template for each component. The paper also discusses the importance of handling updates as well as the initial creation of code files. The techniques leveraged within this project have been learned through the use of other code generation tools, including GUI development tools. In particular, care has been taken to minimize the accidental loss of manually introduced code and handle version updates of the code generator. Our team is using this framework to develop simulation tests for power plant optimizations and have plans to add custom code generation to other areas of the platform.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5784231,no
CEDA: Control-Flow Error Detection Using Assertions,"This paper presents an efficient software technique, control-flow error detection through assertions (CEDA), for online detection of control-flow errors. Extra instructions are automatically embedded into the program at compile time to continuously update runtime signatures and to compare them against preassigned values. The novel method of computing runtime signatures results in a huge reduction in the performance overhead, as well as the ability to deal with complex programs and the capability to detect subtle control-flow errors. The widely used C compiler, GCC, has been modified to implement CEDA, and the SPEC benchmark programs were used as the target to compare with earlier techniques. Fault injection experiments were used to demonstrate the effect of control-flow errors on software and to evaluate the fault detection capabilities of CEDA. Based on a new comparison metric, method efficiency, which takes into account both error coverage and performance overhead, CEDA is found to be much better than previously proposed methods.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5871589,no
A Hybrid Multiagent Framework With Q-Learning for Power Grid Systems Restoration,"This paper presents a hybrid multiagent framework with a Q-learning algorithm to support rapid restoration of power grid systems following catastrophic disturbances involving loss of generators. This framework integrates the advantages of both centralized and decentralized architectures to achieve accurate decision making and quick responses when potential cascading failures are detected in power systems. By using this hybrid framework, which does not rely on a centralized controller, the single point of failure in power grid systems can be avoided. Further, the use of the Q-learning algorithm developed in conjunction with the restorative framework can help the agents to make accurate decisions to protect against cascading failures in a timely manner without requiring a global reward signal. Simulation results demonstrate the effectiveness of the proposed approach in comparison with the typical centralized and decentralized approaches based on several evaluation attributes.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5871714,no
When to stop testing: A study from the perspective of software reliability models,"The important question often asked in the software industry is: when to stop testing and to release a software product? Unfortunately, in industrial practices, it is not easy for project manager and developers to be able to answer this question confidently. Software release time is a trade-off between capturing the benefits of an earlier market introduction and the deferral of a product release in order to enhance functionalities or to improve quality. The question has a lot to do with the time required to detect and correct faults in order to ensure a specified reliability goal. During testing, reliability measure is an important criterion in deciding when to release a software product. This study helps answer this question by presenting the perspectives from a study of software reliability models, with focuses on reliability paradigm, efficient management of resources and decision making under uncertainty.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5871780,no
Towards a software quality assessment model based on open-source statical code analyzers,"In the context of software engineering, quality assessment is not straightforward. Generally, quality assessment is important since it can cut costs in the product life-cycle. A software quality assessment model based on open-source analyzers and quality factors facilitates quality measurement. Our quality assessment model is based on three principles: mapping rules to quality factors, computing scores based on the percentage of rule violating entities (classes, methods, instructions), assessing quality as a weighted mean of the rule attached scores.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5873026,no
Product defect prediction model,"The prediction of software reliability can determine the current reliability of a product, using statistical techniques based on the failures data, obtained during testing or system usability. Software reliability growth models attempt to predict the number of defect using a correlation between exponential function and defect data. The purpose of this paper is to study the evolution of a real-life product over three releases, using the Rayleigh function in order to predict the number of defects. Our paper offers two possibilities for computing the model parameters, and then we should be able to decide which is better and what can be improved. Results from this study will be used to determine which approach is best to be used.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5873055,no
A new method for High Impedance Faults detection in power system connected to the wind farm equipped with DFIGs,"This paper, investigates the effect of High Impedance Fault (HIF) on wind turbine equipped with Doubly Fed Induction Generator (DFIG) operation. Consequently, a new proposed method is used to HIF detecting in power system connected to wind farm equipped with DFIGs means of harmonic component analyzing of DFIG rotor current. The simulation has been done with PSCAD/EMTDC software.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5874569,no
Employing S-transform for fault location in three terminal lines,"This paper has proposed a new fault location method for three terminal transmission lines. Once a fault occurs in a transmission line, high frequency transient components through travelling of the waves are appeared in all terminals. In this paper, S-transform is employed to detect the arrival time of these waves to terminals. For simulating various conditions, ATP-EMTP software and to process the proposed fault location method MATLAB software are used. The results have shown the good performance of the algorithm in different conditions.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5874769,no
A quantitative assessment method for simulation-based e-learnings,"For several years the software industry has been focused on improving its product's quality by implementing different frameworks, models and standards like CMMI and ISO. It has been discovered that training team members is a must within these quality frameworks. Given the vast technologies differentiations and new methodologies for developing software, it is imminent that alternative faster, effective and more customized ways of training people are needed. One alternative way in training people is using simulation-based e-learning technologies. Due to the vast e-learnings market's availability, evaluations on educational software must be done to verify the quality of the training that is been produced or acquired. This paper presents a method that provides a quantitative assessment of the training quality. The proposed method presents an approach towards assessing educational software through the quantitative evaluation of predefined attribute. A pilot experience is presented in this paper along with the method description and explanation.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5876083,no
An improved software reliability model incorporating detection process and the total number of faults,"Software reliability growth models base on the NHPP are quite successful tools that have been proposed to assess the software reliability. Various NHPP-SRGMs have been built upon various assumptions such as the number of remaining faults, software failure rate, and software reliability. But in realistic, the number of faults and the detection rate are not constants. They are time functions. In this paper, we aim to incorportate total number function of faults and detection rate function into conditional SRGMs. Experimental results indicate that the new model which proposed in this paper has a fairly accurate prediction capability.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5876677,no
An AMI based measurement and control system in smart distribution grid,"To realize some of the smart grid goals, for the distribution system of the rural area, with GPRS communication network, a feeder automation based on AMI is proposed. The three parts of the system are introduced. Integrated with the advanced communication and measurement technology, the proposed system can monitor the operating situation and status of breakers, detect and locate the fault of the feeders. The information from the system will help realizing the advanced distribution operation, such as improve power quality, loss detection, state estimation and so on. The application case in Qingdao utilities in Shandong Province, PR China shows the effectiveness of the proposed system.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5890876,no
Detecting matrix multiplication faults in many-core systems,"Many-core systems are characterized by a large number of components based on ever-shrinking circuit geometries. System reliability becomes an issue because of the system complexity, the large number of components and nanoscale issues due to soft errors. While information redundancy techniques can be used for fault tolerance, they occupy too much memory space and increase the memory and network bandwidth. Moreover, in many-cores, resources are plentiful encouraging the design of simple cores without hardware fault tolerance. Thus in the absence of information redundancy, software fault detection techniques become necessary to detect errors. Herein, we present fault detection techniques for 22 matrix multiplication which we extend to nxn matrix multiplication. These tests can detect transient and some intermittent and permanent hardware faults. These tests are also suitable to computing grids and distributed heterogeneous systems where the result-forming node may run tests in software to validate the sub-results submitted by the grid nodes.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5893843,no
Empirical study of an intelligent argumentation system in MCDM,"Intelligent argumentation based collaborative decision making system assists stakeholders in a decision making group to assess various alternatives under different criterion based on the argumentation. A performance score of each alternative under every criterion in Multi-Criteria Decision Making (MCDM) is represented in a decision matrix and it denotes satisfaction of the criteria by that alternative. The process of determining the performance scores of alternatives in a decision matrix for criterion could be controversial sometimes because of the subjective nature of criterion. We developed a framework for acquiring performance scores in a decision matrix for multi-criteria decision making using an intelligent argumentation and collaborative decision support system we developed in the past [1]. To validate the framework empirically, we have conducted a study in a group of stakeholders by providing them an access to use the intelligent argumentation based collaborative decision making tool over the Web. The objectives of the study are: 1) to validate the intelligent argumentation system for deriving performance scores in multi-criteria decision making, and 2) to validate the overall effectiveness of the intelligent argumentation system in capturing rationale of stakeholders. The results of the empirical study are analyzed in depth and they show that the system is effective in terms of collaborative decision support and rationale capturing. In this paper, we present how the study was carried out and its empirical results.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5928674,no
Software and communications architecture for Prognosis and Health Monitoring of ocean-based power generator,"This paper presents a communications and software architecture in support of Prognosis and Health Monitoring (PHM) applications for renewable ocean-based power generation. The generator/turbine platform is instrumented with various sensors (e.g. vibration, temperature) that generate periodic measurements used to assess the current system health and to project its future performance. The power generator platform is anchored miles offshore and uses a pair of wireless data links for monitoring and control. Since the link is expected to be variable and unreliable, being subject to challenging environmental conditions, the main functions of the PHM system are performed on a computing system located on the surface platform. The PHM system architecture is implemented using web services technologies following MIMOSA OSA-CBM standards. To provide sufficient Quality of Service for mission-critical traffic, the communications system employs application-level queue management with semantic-based filtering for the XML PHM messages, combined with IP packet traffic control and link quality monitoring at the network layer.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5929092,no
The possibility of application the optical wavelength division multiplexing network for streaming multimedia distribution,"In this paper, simulation is used to investigate the possibility of streaming multimedia distribution over optical wavelength division multiplexing (WDM) network with optical burst switching (OBS) nodes. Simulation model is developed using software tool Delsi for Delfi 4.0. Pareto generator is used to model real-time multimedia stream. The wavelength allocation (WA) method and the deflection routing are implemented in OBS nodes. Performance measures, packet loss and delay, are estimated in order to investigate the quality of multimedia service. Statistical analysis of simulation output is performed by estimating the confidence intervals for a given degree according to the Student distribution. Obtained results indicate that optical WDM network manages multimedia contents distribution in efficient way to give a high quality of service.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5929183,no
Applying source code analysis techniques: A case study for a large mission-critical software system,"Source code analysis has been and still is extensively researched topic with various applications to the modern software industry. In this paper we share our experience in applying various source code analysis techniques for assessing the quality of and detecting potential defects in a large mission-critical software system. The case study is about the maintenance of a software system of a Bulgarian government agency. The system has been developed by a third-party software vendor over a period of four years. The development produced over 4 million LOC using more than 20 technologies. Musala Soft won a tender for maintaining this system in 2008. Although the system was operational, there were various issues that were known to its users. So, a decision was made to assess the system's quality with various source code analysis tools. The expectation was that the findings will reveal some of the problems' cause, allowing us to correct the issues and thus improve the quality and focus on functional enhancements. Musala Soft had already established a special unit - Applied Research and Development Center - dealing with research and advancements in the area of software system analysis. Thus, a natural next step was for this unit to use the know-how and in-house developed tools to do the assessment. The team used various techniques that had been subject to intense research, more precisely: software metrics, code clone detection, defect and code smells detection through flow-sensitive and points-to analysis, software visualization and graph drawing. In addition to the open-source and free commercial tools, the team used internally developed ones that complement or improve what was available. The internally developed Smart Source Analyzer platform that was used is focused on several analysis areas: source code modeling, allowing easy navigation through the code elements and relations for different programming languages; quality audit through software metri- s by aggregating various metrics into a more meaningful quality characteristic (e.g. maintainability); source code pattern recognition - to detect various security issues and code smells. The produced results presented information about both the structure of the system and its quality. As the analysis was executed in the beginning of the maintenance tenure, it was vital for the team members to quickly grasp the architecture and the business logic. On the other hand, it was important to review the detected quality problems as this guided the team to quick solutions for the existing issues and also highlighted areas that would impede future improvements. The tool IPlasma and its System Complexity View (Fig. 1) revealed where the business logic is concentrated, which are the most important and which are the most complex elements of the system. The analysis with our internal metrics framework (Fig. 2) pointed out places that need refactoring because the code is hard to modify on request or testing is practically impossible. The code clone detection tools showed places where copy and paste programming has been applied. PMD, Find Bugs and Klockwork Solo tools were used to detect various code smells (Fig. 3). There were a number of occurrences that were indeed bugs in the system. Although these results were productive for the successful execution of the project, there were some challenges that should be addressed in the future through more extensive research. The two aspects we consider the most important are usability and integration. As most of the tools require very deep understanding of the underlying analysis, the whole process requires tight cooperation between the analysis team and the maintenance team. For example, most of the metrics tools available provide specific values for a given metric without any indication what the value means and what is the threshold. Our internal metrics framework aggregates the met",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5929241,no
Automatic generation of system-level virtual prototypes from streaming application models,"Virtual prototyping is a more and more accepted technology to enable early software development in the design flow of embedded systems. Since virtual prototypes are typically constructed manually, their value during design space exploration is limited. On the other hand, system synthesis approaches often start from abstract and executable models, allowing for fast design space exploration, considering only predefined design decisions. Usually, the output of these approaches is an """"ad hoc"""" implementation, which is hard to reuse in further refinement steps. In this paper, we propose a methodology for automatic generation of heterogeneous MPSoC virtual prototypes starting with models for streaming applications. The advantage of the proposed approach lies in the fact that it is open to subsequent design steps. The applicability of the proposed approach to real-world applications is demonstrated using a Motion JPEG decoder application that is automatically refined into several virtual prototypes within seconds, which are correct by construction, instead of using error-prone manual refinement, which typically requires several days.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5929986,no
A novel approach for spam detection using boosting pages,"Link spam techniques are widely used in commercial web pages to achieve higher ranking in search results which may reduce the quality of the results. Most of techniques require some boosting pages to increase score of the target pages. In this paper, we aim to detect boosting pages and then spam pages to increase the quality of results. Our approach consists of two steps: (1) find boosting pages from spam seed set, and (2) detect web spam from the discovered boosting pages. An experimental result shows that our approach can detect 93.20 % of web spam pages with 79.96% precision.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5930100,no
Pulse inversion linear bandpass filter for detecting subharmonic from microbubbles,"Subharmonic imaging promises to improve ultrasound imaging quality due to an increasing contrast-to-tissue ratio (CTR). However, to improve image quality, signal processing techniques for maximizing subharmonic signal are needed. In this work, we present the pulse inversion linear bandpass filter (PILBF) method for detection of sub harmonic components in order to separate signals from microbubble contrast agent echoes. This method is based on the combination between pulse inversion (PI) and linear bandpass filter (LBF) methods. While PI without LBF produces CTR 54 dB, the PILBF produces 82 dB enhancement. The high CTR value confirms the generation of high quality image.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5930110,no
An efficient approach for data-duplication detection based on RDBMS,"Data-duplication is one of the most important issues in the context of information system management. Instead of storing a single real-world object as an entity in an information system, the duplication, storing more than one entity representing a single object, can be occurred. This problem can decrease the quality of service of information systems. In this paper, we propose an efficient approach to detect the duplication based on the RDBMS foundation. Our approach is based on the assumption that the data to be processed have been stored in the RDBMS at the first place. Thus, the proposed approach does not require the data to be imported/exported from the storage. Also, such approach will benefit from the query optimizer of the RDBMS. The experiment results on the TPC-H dataset have been presented to validate such proposed work.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5930142,no
Bad-smell prediction from software design model using machine learning techniques,Bad-smell prediction significantly impacts on software quality. It is beneficial if bad-smell prediction can be performed as early as possible in the development life cycle. We present methodology for predicting bad-smells from software design model. We collect 7 data sets from the previous literatures which offer 27 design model metrics and 7 bad-smells. They are learnt and tested to predict bad-smells using seven machine learning algorithms. We use cross-validation for assessing the performance and for preventing over-fitting. Statistical significance tests are used to evaluate and compare the prediction performance. We conclude that our methodology have proximity to actual values.,2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5930143,no
Towards a Rapid-Alert System for Security Incidents,"Predicting security incidents and forecasting risk are two essential duties when designing an enterprise security system. Based on a quantitative risk assessment technique arising from an an attacker-defender model, we propose a Bayesian learning strategy to continuously update the quality of protection and forecast the decision-theoretic risk. Evidence for or against the security of particular system components can be obtained from various sources, including security patches, software updates, scientific or industrial research result notifications retrieved through RSS feeds. Using appropriate stochastic distribution models, we obtain closed-form expressions (formulas) for the times when to expect the next security incident and when a re-consideration of a security system or component becomes advisable.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5931117,no
Apad: A QoS Guarantee System for Virtualized Enterprise Servers,"Today's data center often employs virtualization to allow multiple enterprise applications to share a common hosting platform in order to improve resource and space utilization. It could be a greater challenge when the shared server becomes overloaded or enforces priority of the common resource among applications in such an environment. In this paper, we propose Apad, a feedback-based resource allocation system which can dynamically adjust the resource shares to multiple virtual machine nodes in order to meet performance target on shared virtualized infrastructure. To evaluate our system design, we built a test bed hosting several virtual machines which employed Xen Virtual Machine Monitor (VMM), and using Apache server along with its workload-generated tool. Our experiment results indicate that our system is able to detect and adapt to resource requirement that changes over time and allocate virtualized resources accordingly to achieve application-level Quality of Service (QoS).",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5931129,no
Identity attack and anonymity protection for P2P-VoD systems,"As P2P multimedia streaming service is becoming more popular, it is important for P2P-VoD content providers to protect their servers identity. In this paper, we first show that it is possible to launch an """"identity attack"""": exposing and identifying servers of peer-to-peer video-on-demand (P2P-VoD) systems. The conventional wisdom of the P2P-VoD providers is that identity attack is very difficult because peers cannot distinguish between regular peers and servers in the P2P streaming process. We are the first to show that it is otherwise, and present an efficient and systematic methodology to perform P2P-VoD servers detection. Furthermore, we present an analytical framework to quantify the probability that an endhost is indeed a P2P-VoD server. In the second part of this paper, we present a novel architecture that can hide the identity and provide anonymity protection for servers in P2P-VoD systems. To quantify the protective capability of this architecture, we use the """"fundamental matrix theory"""" to show the high complexity of discovering all protective nodes so as to disrupt the P2P-VoD service. We not only validate the model via extensive simulation, but also implement this protective architecture on PlanetLab and carry out measurements to reveal its robustness against identity attack.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5931313,no
Online condition monitoring network for critical equipment at Holcim's STE. genevieve plant,"This paper describes a network architecture for remote monitoring and fault detection of critical rotating equipment. Holcim's Sphinx Monitoring System (SMS) is intended for equipment protection and early prediction of machine defects. SMS is based on a server-client model, where the server maintains a database of equipment configurations and evaluates each target with a unique set of rules that monitor the equipment's function and its integrated components' failure modes. Adaptive software algorithms monitor process and operational changes to tune the analysis criteria for evaluating vibration signals and detecting common machine faults. Network structure, software algorithms and other aspects of the system are further discussed and evaluated against samples of collected data, generated analysis results and physical inspections' outcomes.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5934563,no
Formal Verification of Distributed Transaction Management in a SOA Based Control System,"In large scale, heavy workload systems, managing distributed transactions on multiple datasets becomes challenging and error prone task. Software systems based on service oriented architecture principles that manage critical infrastructures are typical environments where robust transaction management is one of the essential goals to achieve. The aim of this paper is to provide a formal description of the solution for transaction management and individual service component behavior in a SOA-based control system, and prove the correctness of the proposed design with the SMV formal verification tool. Atomic commitment protocol is used as a basis for solving distributed transaction management problem. SMV language and verification tool are utilized for formal description of the problem and verification of the necessary properties. The case study describes an application of the proposed approach in commercial software system for electrical power distribution management. Verification of given model properties has shown that suggested solution is suitable for the described class of SOA-based systems.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5934821,no
Method of safety critical requirements flow in product life cycle processes,"The safety-related requirements are a part of the system requirements which are inputs to the software life cycle processes. These system requirements are developed from systems architecture. The system requirements are developed on each functional area applications. The safety requirements are assessed at the individual functional areas. This white paper proposes a solution to develop safety requirements for functional areas interfaces and method to flowdown software requirements throughout product lifecycle. While developing requirements from system requirements to high level and low level requirements, the requirements would be completely analyzed for safety perspective. The advantage of this proposal is that the functional modules interface requirements are analyzed and captured from safety perspective. Reuse of the product extracts specific safety related requirements at each functional level as the requirements are developed at each interface modules. This leads to extensive verification of interface requirements along with system requirements which would leads to a safer product.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5935349,no
Component level risk assessment in Grids: A probablistic risk model and experimentation,"The current approaches to manage risk in Grid computing are a major step towards the provision of Quality of Service (QoS) to the end-user. However these approaches are based on node or machine level Assessment. As a node may contain CPU(s), storage devices, connections for communication and software resource, a node failure may actually be a failure of any of these components. This paper proposes a probabilistic risk model at the component level; the probabilistic risk model encompasses series and parallel model(s). Our approach towards risk assessment is aimed at a granularity level of individual components as compared to previous efforts at node level. The benefits of this probabilistic approach is the provision of a detailed risk assessment to the Grid resource provider leading to risk aware scheduling and an efficient usage of resources. Grid failure data was analyzed and experimentation was conducted based the proposed risk model. The results of the experiments provide detailed risk information at component level for the nodes required in the SLA (Service Level Agreement).",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5936600,no
Test case generation for use case dependency fault detection,Testing is an important phase of quality control in Software development. The use case diagram present in UML 2.0 plays a vital role in describing the behavior of a system and it is widely used for generating test cases. But to identify the dependency faults that occur between use cases is a challenge for the test engineers in a Model Based Testing (MBT) environment. This paper presents a novel approach for generating test cases to detect use case dependency faults in UML use case diagrams using mulitway trees. Our approach includes transforming the UML use case diagrams into a tree representation called as Use Case Dependency Tree (UCDT). This is followed by a thorough traversal of the tree to generate the test cases so as to detect any existing intra and inter use case dependency faults among the various use cases being invoked by the different actors interacting with the software system.,2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5941585,no
A model based prioritization technique for component based software retesting using uml state chart diagram,"Regression testing is the process of testing a modified system using the old test suite. As the test suite size is large, system retesting consumes large amount of time and computing resources. This issue of retesting of software systems can be handled using a good test case prioritization technique. A prioritization technique schedules the test cases for execution so that the test cases with higher priority executed before lower priority. The objective of test case prioritization is to detect fault as early as possible so that the debuggers can begin their work earlier. In this paper we propose a new prioritization technique to prioritize the test cases to perform regression testing for Component Based Software System (CBSS). The components and the state changes for a component based software systems are being represented by UML state chart diagrams which are then converted into Component Interaction Graph (CIG) to describe the interrelation among components. Our prioritization algorithm takes this CIG as input along with the old test cases and generates a prioritized test suit taking into account total number of state changes and total number of database access, both direct and indirect, encountered due to each test case. Our algorithm is found to be very effective in maximizing the objective function and minimizing the cost of system retesting when applied to few JAVA projects.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5941719,no
Prediction of software project effort using fuzzy logic,"Software development effort estimation is a branch of forecasting that has received increased interest in academia as well as in the field of research and development. Predicting software effort with any acceptable degree remains challenging. In this paper we have developed 2 different linear regression models using fuzzy function point (FFP) and non fuzzy function point in order to predict the software project effort and further we have also considered that the entire projects are organic in nature i.e. the project size lies between 2 to 50 KLOC. After obtaining the software effort, project manager can control the cost and ensures the quality more accurately.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5941919,no
Test case prioritization for regression testing based on fault dependency,"Test case prioritization techniques involve scheduling test cases for regression testing in an order that increases their effectiveness at meeting some performance goal. This is inefficient to re execute all the test cases in regression testing following the software modifications. Using information obtained from previous test case execution, prioritization techniques order the test cases for regression testing so that most beneficial are executed first thus allows an improved effectiveness of testing. One performance goal, rate of dependency detected among faults, measures how quickly dependency among faults are detected within the regression testing process. An improved rate of fault dependency can provide faster feedback on software and let developers start debugging on the severe faults that cause other faults to appear later. This paper presents the new metric for assessing rate of fault dependency detection and an algorithm to prioritize test cases. Using the new metric the effectiveness of this prioritization is shown comparing it with non-prioritized test case. Analysis proves that prioritized test cases are more effective in detecting dependency among faults.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5941954,no
Automated generation of test cases from output domain of an embedded system using Genetic algorithms,"A primary issue in black-box testing is how to generate adequate test cases from input domain of the system under test on the basis of user's requirement specification. However, for some types of systems including embedded systems, developing test cases from output domain is more suitable than developing from input domain, especially, when the output domain is smaller. This approach ensures better reliability of the system under test. In this paper, the authors present a new approach to automate the generation of test cases from output domain of a pilot project Temperature Monitoring and Controlling of Nuclear Reactor System (TMCNRS) which is an embedded system developed using modified Cleanroom Software Engineering methodology. An Automated Test Case Generator (ATCG) that uses Genetic algorithms (GAs) extensively and generates test cases from output domain is proposed. The ATCG generates test cases which are useful to conduct pseudo - exhaustive testing to detect single, double and several multimode faults in the system. The generator considers most of the combinations of outputs, and finds the corresponding inputs while optimizing the number of test cases generated. In order to investigate the effectiveness of this approach, test cases were generated by ATCG and the tests were conducted on the target embedded system at a minimum cost and time. Experimental results show that this approach is very promising.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5941989,no
MngRisk  A decisional framework to measure managerial dimensions of legacy application for rejuvenation through reengineering,"Nowadays legacy system reengineering has emerged as a well-known system evolution technique. The goal of reengineering is to increase productivity and quality of legacy system through fundamental rethinking and radical redesigning of system. A broad range of risk issues and concerns must be addressed to understand and model reengineering process. Overall success of reengineering effort requires to considering three distinctive but connected areas of interest i.e. system domain, managerial domain and technical domain. We present a hierarchical managerial domain risk framework MngRisk to analyze managerial dimensions of legacy system. The fundamental premise of framework is to observe, extract and categories the contextual perspective models and risk clusters of managerial domain. This work contributes for a decision driven framework to identify and assess risk components of managerial domain. Proposed framework provides guidance on interpreting the results obtained from assessment to take decision about when evolution of a legacy system through reengineering is successful.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5942048,no
Predicting the Reliability of Software Systems Using Fuzzy Logic,"Software industry suffer many challenges in developing a high quality reliable software. Many factors affect their development such as the schedule, limited resources, uncertainty in the developing environment and inaccurate requirement specification. Software Reliability Growth Models (SRGM)were significantly used to help in solving these problems by accurately predicting the number of faults in the software during both development and testing processes. The issue of building growth models was the subject of many research work. In this paper, we explore the use of fuzzy logic to build a SRGM. The proposed fuzzy model consists of a collection of linear sub-models joined together smoothly using fuzzy membership functions to represent the fuzzy model. Results and analysis based data set developed by John Musa of Bell Telephone Laboratories are provided to show the potential advantages of using fuzzy logic in solving this problem.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5945204,no
Bayesian Estimation of the Normal Mean from Censored Samples,"The normal distribution is often used a s a model for reliability, and censored samples naturally arise in software reliability applications. Bayesian estimation methods have an advantage over the frequentist approach as they provide the user a framework for incorporating important factors such as software complexity, operating system, level and quality of verification and validation in the software reliability estimation process. Our goal in this paper is to compute the Bayes estimate of the mean of a normal population when the data set is censored . The proposed method is illustrated via several examples.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5945236,no
An Empirical Study on the Impacts of Autonomy of Components on Qualities of Software Systems,"More and more autonomous computing entities are implemented and deployed on the Internet and they are supposed to be able to adapt to the instable connection, decentralized control, dynamism and openness of the networking environment. Prior to implementing these autonomous computing entities, software engineers should decide a range of acceptable autonomy of components to ensure qualities of both individual components and the whole system. In order to qualitatively investigate how the autonomy of components impact on the qualities of the whole system and what other factors impact their relations, we conducted an experimental study based on the stochastic process. First, we give the definition of autonomy and an approach for measuring autonomy degree of a component based on the general recognition of the academia. Next, we build up a mathematical model for the relationship between autonomy degree and quality by using the stochastic process. At last, we construct an intelligent traffic control simulation system composed of Autonomous Components to concrete the mathematical model and to draw some generic conclusions from the experimental system. By recording these qualities under different autonomy degrees and different environment complexities, we work out the probability density distribution of quality movement. Combining the mathematical model, we give out some guides for autonomous components to adjust their autonomy degrees automatically under different contexts.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5946188,no
Portable electronic nose for beverage quality assessment,"A portable electronic nose (e-nose) was appropriately designed for investigating quality of beverages such as juice or wine, etc. The e-nose system comprises of sample and reference containers, air flow unit, sensing unit and data acquisition unit. All of the hardware units were controlled by in-house software under LABVIEW program via USB port of a DAQ card. The sensing unit includes eight different metal oxide gas sensors from Figaro Engineering Inc. Principal component analysis (PCA) was used as a statistical method in order to discriminate and assess the experimental data as defined by the percentage change in sensor resistances that correlates directly to difference in the aroma characteristics. Drift compensation model was applied to the raw data that sometimes suffer from the effects of sensor drift. Constructed portable e-nose has been tested on-field in a winery to evaluate wine aroma during process of wine bottling. The e-nose using PCA algorithm can distinguish the wine bottling under nitrogen from the bottling under partial vacuum. We also demonstrated that e-nose can be used to help wine maker to design the appropriate process of wine bottling achieving a high quality of wine product.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5947797,no
Evaluation of the functionality of a traditional setting policy applied on directional earth fault function,"High-impedance faults (HIFs) cannot be detected by the protective zones of distance relays. To overcome to this problem, a protective function called directional earth fault (DEF) function is included in numerical distance relays. To set the function some extensive calculations and probably some exhaustive simulations should be done. However, protection engineers usually utilize a traditional setting policy to make the setting easy. In this paper, we will have an evaluation on this setting policy to find out whether it can result in a proper operation of DEF function in detecting HIFs or not. To do this, using software MATLAB<sup></sup>, we will simulate different HIFs on line Dogonbadan-Behbahan (a line of Iran transmission grid). Then the functionality of DEF function of the line relays which is set on the basis of the traditional setting policy will be evaluated in the presence of each HIF case. Eventually, for all simulated HIFs the impedance measured by the distance function of the line relays will be presented to show how HIFs affect the impedance location with respect to the protective zones.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5947989,no
Failure Avoidance through Fault Prediction Based on Synthetic Transactions,"System logs are an important tool in studying the conditions (e.g., environment misconfigurations, resource status, erroneous user input) that cause failures. However, production system logs are complex, verbose, and lack structural stability over time. These traits make them hard to use, and make solutions that rely on them susceptible to high maintenance costs. Additionally, logs record failures after they occur: by the time logs are investigated, users have already experienced the failures' consequences. To detect the environment conditions that are correlated with failures without dealing with the complexities associated with processing production logs, and to prevent failure-causing conditions from occurring before the system goes live, this research suggests a three step methodology: (i) using synthetic transactions, i.e., simplified workloads, in pre-production environments that emulate user behavior, (ii) recording the result of executing these transactions in logs that are compact, simple to analyze, stable over time, and specifically tailored to the fault metrics of interest, and (iii) mining these specialized logs to understand the conditions that correlate to failures. This allows system administrators to configure the system to prevent these conditions from happening. We evaluate the effectiveness of this approach by replicating the behavior of a service used in production at Microsoft, and testing the ability to predict failures using a synthetic workload on a 650 million events production trace. The synthetic prediction system is able to predict 91% of real production failures using 50-fold fewer transactions and logs that are 10,000-fold more compact than their production counterparts.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5948623,no
Fault Tree Analysis Based on Logic Grey Gate in Power System,"Because there exists the ambiguity in complex systems, and there is the limitation in traditional fault tree, a novel fault tree analysis theory is introduced. In the theory, the probability grey number, which can express the event's subjective ambiguity and objective ambiguity, is introduced to express the degree and probability that the components go wrong, dynamic envelope is applied to score the relation among components, and a new logic gate, Grey-gate, is advanced for expressing the effect of system reliability when the components go wrong. Finally, the theory of fault diagnosis is applied to analyze the fault effect of the system with software and hardware.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5948736,no
A systems engineering approach for crown jewels estimation and mission assurance decision making,"Understanding the context of how IT contributes to making missions more or less successful is a cornerstone of mission assurance. This paper describes a continuation of our previous work that used process modeling to allow us to estimate the impact of cyber incidents on missions. In our previous work we focused on developing a capability that could work as an online process to estimate the impacts of incidents that are discovered and reported. In this paper we focus instead on how our techniques and approach to mission modeling and computing assessments with the model can be used offline to help support mission assurance engineering. The heart of our approach involves using a process model of the system that can be run as an executable simulation to estimate mission outcomes. These models not only contain information about the mission activities, but also contain attributes of the process itself and the context in which the system operates. They serve as a probabilistic model and stochastic simulation of the system itself. Our contributions to this process modeling approach have been the addition of IT activity models that document in the model how various mission activities depend on IT supported processes and the ability to relate how the capabilities of the IT can affect the mission outcomes. Here we demonstrate how it is possible to evaluate the mission model offline and compute characteristics of the system that reflect its mission assurance properties. Using the models it is possible to identify the crown jewels, to expose the systems susceptibility to different attack effects, and evaluate how different mitigation techniques would likely work. Being based on an executable model of the system itself, our approach is much more powerful than a static assessment. Being based on business process modeling, and since business process analysis is becoming popular as a systems engineering tool, we also hope our approach will push mission assurance analysis tasks into - - a framework that allows them to become a standard systems engineering practice rather than the off to the side activity it currently is.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5949403,no
An evolutionary multiobjective optimization approach to component-based software architecture design,"The design of software architecture is one of the difficult tasks in the modern component-based software development which is based on the idea that develop software systems by assembling appropriate off-the-shelf components with a well-defined software architecture. Component-based software development has achieved great success and been extensively applied to a large range of application domains from realtime embedded systems to online web-based applications. In contrast to traditional approaches, it requires software architects to address a large number of non-functional requirements that can be used to quantify the operation of system. Moreover, these quality attributes can be in conflict with each other. In practice, software designers try to come up with a set of different architectural designs and then identify good architectures among them. With the increasing scale of architecture, this process becomes time-consuming and error-prone. Consequently architects could easily end up with some suboptimal designs because of large and combinatorial search space. In this paper, we introduce AQOSA (Automated Quality-driven Optimization of Software Architecture) toolkit, which integrates modeling technologies, performance analysis techniques, and advanced evolutionary multiobjective optimization algorithms (i.e. NSGA-II, SPEA2, and SMS-EMOA) to improve non-functional properties of systems in an automated manner.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5949650,no
Definition of Test Criteria Based on the Scene Graph for VR Applications,"Virtual Reality applications are becoming more popular. In general, the development of these applications does not include a testing phase, or, at best, the evaluation is conducted only with the users. The activity of software testing has received considerable attention from researchers and software engineers who recognize its usefulness in creating quality products. However, the tests are expensive and prone to errors, which imposes the need to systematize and hence the definition of techniques to increase quality and productivity in their driving. Several testing techniques have been developed and have been used, each with its own characteristics in terms of effectiveness, cost, implementation stages, etc. Moreover, these techniques can also be adapted. In this paper, testing criteria based on scene graph are studied in order to ensure the quality of the Virtual Reality applications implementation. In addition, a proof of concept is presented, by using the defined criteria applied to a VR framework built to generate applications in the medical training area.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5951835,no
A Configurable Approach to Tolerate Soft Errors via Partial Software Protection,"Compared with hardware-based methods, software-based methods which need not additional hardware costs are regarded as efficient methods to tolerate soft errors. Software-based methods which are implemented by software protection have performance sacrifice. This paper proposes a new configurable approach whose purpose is to balance system reliability and performance, to tolerate soft errors via partial software protection. Those unprotected software regions which are motivated by soft error mask on software level are related to statically dead codes, those codes whose probabilities to be executed are low and some partially dead codes. For those protected codes, we copy every data and operate every operation twice to ensure those data stored into memory are right. Additionally, we ensure every branch instruction can jump to the right address by checking condition and destination address. Finally, our approach is implemented by modification of compiler. System reliability and performance are evaluated with different configurations. Experimental results demonstrate our purpose to balance system reliability and performance.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5951985,no
Combined methods for solving inductive coupling problems,"The problem of induced AC voltages on pipelines has always been with us, and the interference caused by power transmission lines to buried gas pipelines is under investigation for many years. Situations where a pipeline is influenced by power lines in a right-of-way are more frequent nowadays. Even under normal operating conditions, voltages and currents are induced on the pipeline that may pose danger to working personnel or may accelerate the corrosion of the pipeline's metal. The aim of the paper is to evaluate the induced voltages and currents in case of an underground gas pipeline, which shares a same right of way with a high voltage transmission line, in order to detect the possibility of the AC corrosion in occurring of the pipeline. We choose to combine the electromagnetic field method and the conventional circuit method. Because the electrical equivalent circuit involves a high number of circuit elements that must be defined, a software code that generates this automatically was created. The considered complex problem is studied for different operating conditions of the power transmission line and different values of the coupling coefficient.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5952251,no
Automatic generation of test data for path testing by adaptive genetic simulated annealing algorithm,"Software testing has become an important stage of the software developing process in recent years, and it is crucial element of software quality assurance. Path testing has become one of the most important unit test methods, and it is a typical white box test. The generation of testing data is one of the key steps which have a great effect on the automation of software testing. GA is adaptive heuristic search algorithm premised on the evolutionary ideas of natural selection and genetic. Because it is a robust search method requiring little information to search effectively in a large or poorly-understood search space, it is widely used to search and optimize, and also can be used to generate test data. In this article we put the anneal mechanism of the Simulated Anneal Algorithm into the genetic algorithm to decide to accept the new individuals or not, and we import dynamic selections to adaptive select individuals which can be copied to next generation. Adaptive crossover probability, adaptive mutation probability and elitist preservation ensure that the best individuals can not be destroyed. The experiment results show that adaptive genetic simulated annealing algorithm is superior to genetic algorithm in effectiveness and efficiency.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5952418,no
Guaranteed Seamless Transmission Technique for NGEO Satellite Networks,"Non-geostationary (NGEO) satellite communication systems are able to provide global communication with reasonable latency and low terminal power requirements. However the highly topological dynamics, large delay and error prone links have been a matter of fact in the satellite network studies. This paper proposes a novel Guaranteed Seamless Transmission Technique(GST), which is a Hop-by-Hop scheme enhanced with the End-to-End scheme and associated with a link algorithm, which updates the link load explicitly and sends it back to the sources that use the link. We analyze GST theoretically by adopting a simple fluid model. The good performance of GST, in terms of bandwidth utilization, effective transmission ratio and fairness, is verified via a set of simulations.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5952753,no
A middleware for reliable soft real-time communication over IEEE 802.11 WLANs,"This paper describes a middleware layer for soft real-time communication in wireless networks devised and realized using standard WLAN hardware and software. The proposed middleware relies on a simple network architecture comprising a number of stations that generate real-time traffic and a particular station, called Scheduler, that coordinates the transmission of the real-time packets using a polling mechanism. The middleware combines EDF scheduling with a dynamic adjustment of the maximum number of transmission attempts, so as to adapt the performance to fluctuations of the link quality, thus increasing the communication reliability, while taking deadlines into account. After describing the basic communication paradigm and the underlying concepts of the proposed middleware, the paper describes the target network configuration and the software architecture. Finally, the paper assesses the effectiveness of the proposed middleware in terms of PER, On-Time Throughput, and Deadline Miss Rate, presenting the results of measurements performed on real testbeds.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5953653,no
Improving model-based verification of embedded systems by analyzing component dependences,"Embedded systems in automobiles become increasingly complex as they are intended to make vehicles even more safe, comfortable, and efficient. International norms like ISO 26262 and IEC 61165 postulate methods for the development and verification of safety critical systems. These standards should ensure that the dependability and quality of the embedded systems is maintained while their complexity and interdependence increases. Yet, the standards do not contain concrete methods or tools for their fulfillment. As concerns classic techniques for dependability analysis they either base on system analysis by means of Markov analysis or on reliability estimation from a usage perspective. Treating the system only from one perspective, however, is a drawback as the system analysis neglects functional or non-functional dependences of the system. These dependences can directly influence the reliability in the field usage. In this paper we present our approach to combine component dependency models with usage models to overcome these deficiencies. It is possible to identify usage scenarios which aim for critical dependences and to analyze the interaction of components inside the system. On the other hand usage scenarios can be assessed whether they meet the desired verification purpose. The component dependency models reveal dependences that were not identified before, because it allows the extraction of implications across functional and non functional dependences like memory, timing and processor utilization.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5953678,no
Towards runtime testing in automotive embedded systems,"Runtime testing is a common way to detect faults during normal system operation. To achieve a specific diagnostic coverage runtime testing is also used in safety critical, automotive embedded systems. In this paper we propose a test architecture to consolidate the hardware resource consumption and timing needs of runtime tests and of application and system tasks in a hard real-time embedded system as applied to the automotive domain. Special emphasis is put to timing requirements of embedded systems with respect to hard real-time and concurrent hardware resource accesses of runtime tests and tasks running on the target system.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5953679,no
Control-flow error detection using combining basic and program-level checking in commodity multi-core architectures,"This paper presents a software-based technique to detect control-flow errors using basic level control-flow checking and inherent redundancy in commodity multi-core processors. The proposed detection technique is composed of two phases of basic and program-level control-flow checking. Basic-level control-flow error detection is achieved through inserting additional instructions into program at design time regarding to control-flow graph. Previous research shows that modern superscalar microprocessors already contain significant amounts of redundancy. Program-level control-flow checking can detect CFEs by leveraging existing microprocessors redundancy. Therefore, the cost of adding extra redundancy for fault tolerance is eliminated. In order to evaluate the proposed technique, three workloads quick sort, matrix multiplication and linked list utilized to run on a multi-core processor, and a total of 6000 transient faults have been injected on the processor. The advantage of the proposed technique in terms of performance and memory overheads and detection capability compared with conventional control-flow error detection techniques.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5953691,no
Regression Test Selection Techniques for Test-Driven Development,"Test-Driven Development (TDD) is characterized by repeated execution of a test suite, enabling developers to change code with confidence. However, running an entire test suite after every small code change is not always cost effective. Therefore, regression test selection (RTS) techniques are important for TDD. Particularly challenging for TDD is the task of selecting a small subset of tests that are most likely to detect a regression fault in a given small and localized code change. We present cost-bounded RTS techniques based on both dynamic program analysis and natural-language analysis. We implemented our techniques in a tool called Test Rank, and evaluated its effectiveness on two open-source projects. We show that using these techniques, developers can accelerate their development cycle, while maintaining a high bug detection rate, whether actually following TDD, or in any methodology that combines testing during development.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954400,no
A Principled Evaluation of the Effect of Directed Mutation on Search-Based Statistical Testing,"Statistical testing generates test inputs by sampling from a probability distribution that is carefully chosen so that the inputs exercise all parts of the software being tested. Sets of such inputs have been shown to detect more faults than test sets generated using traditional random and structural testing techniques. Search-based statistical testing employs a metaheuristic search algorithm to automate the otherwise labour-intensive process of deriving the probability distribution. This paper proposes an enhancement to this search algorithm: information obtained during fitness evaluation is used to direct the mutation operator to those parts of the representation where changes may be most beneficial. A principled empirical evaluation demonstrates that this enhancement leads to a significant improvement in algorithm performance, and so increases both the cost-effectiveness and scalability of search-based statistical testing. As part of the empirical approach, we demonstrate the use of response surface methodology as an effective and objective method of tuning algorithm parameters, and suggest innovative refinements to this methodology.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954408,no
Identifying Infeasible GUI Test Cases Using Support Vector Machines and Induced Grammars,"Model-based GUI software testing is an emerging paradigm for automatically generating test suites. In the context of GUIs, a test case is a sequence of events to be executed which may detect faults in the application. However, a test case may be infeasible if one or more of the events in the event sequence are disabled or made inaccessible by a previously executed event (e.g., a button may be disabled until another GUI widget enables it). These infeasible test cases terminate prematurely and waste resources, so software testers would like to modify the test suite execution to run only feasible test cases. Current techniques focus on repairing the test cases to make them feasible, but this relies on executing all test cases, attempting to repair the test cases, and then repeating this process until a stopping condition has been met. We propose avoiding infeasible test cases altogether by predicting which test cases are infeasible using two supervised machine learning methods: support vector machines (SVMs) and grammar induction. We experiment with three feature extraction techniques and demonstrate the success of the machine learning algorithms for classifying infeasible GUI test cases in several subject applications. We further demonstrate a level of robustness in the algorithms when training and classifying test cases of different lengths.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954411,no
Event-Based GUI Testing and Reliability Assessment Techniques -- An Experimental Insight and Preliminary Results,"It is widely accepted that graphical user interfaces (GUIs) highly affect - positive or negative - the quality and reliability of human-machine systems. In spite of this fact, quantitative assessment of the reliability of GUIs is a relatively young research field. Existing software reliability assessment techniques attempt to statistically describe the software testing process and to determine and thus predict the reliability of the system under consideration (SUC). These techniques model the reliability of the SUC based on particular assumptions and preconditions on probability distribution of cumulative number of failures, failure data observed, and form of the failure intensity function, etc. We expect that the methods used for modeling a GUI and related frameworks used for testing it also affect the factors mentioned above, especially failure data to be observed and prerequisites to be met. Thus, the quality of the reliability assessment process, and ultimately also the reliability of the GUI, depends on the methods used for modeling and testing the SUC. This paper attempts to gain some experimental insight into this problem. GUI testing frameworks based on event sequence graphs and event flow graphs were chosen as examples. A case study drawn from a large commercial web-based system is used to carry out the experiments and discuss the results.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954412,no
Change Sensitivity Based Prioritization for Audit Testing of Webservice Compositions,"Modern software systems have often the form of Web service compositions. They take advantage of the availability of a variety of external Web services to provide rich and complex functionalities, obtained as the integration of external services. However, Web services change at a fast pace and while syntactic changes are easily detected as interface incompatibilities, other more subtle changes are harder to detect and may give raise to faults. They occur when the interface is compatible with the composition, but the semantics of the service response has changed. This typically involves undocumented or implicit aspects of the service interface. Audit testing of services is the process by which the service integrator makes sure that the service composition continues to work properly with the new versions of the integrated services. Audit testing of services is conducted under strict (sometimes extreme) time and budget constraints. Hence, prioritizing the audit test cases so as to execute the most important ones first becomes of fundamental importance. We propose a test case prioritization method specifically tailored for audit testing of services. Our method is based on the idea that the most important test cases are those that have the highest sensitivity to changes injected into the service responses (mutations). In particular, we consider only changes that do not violate the explicit contract with the service (i.e., the WSDL), but may violate the implicit assumptions made by the service integrator.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954434,no
An Evaluation of Mutation and Data-Flow Testing: A Meta-analysis,"Mutation testing is a fault-based testing technique for assessing the adequacy of test cases in detecting synthetic faulty versions injected to the original program. The empirical studies report the effectiveness of mutation testing. However, the inefficiency of mutation testing has been the major drawback of this testing technique. Though a number of studies compare mutation to data flow testing, the summary statistics for measuring the magnitude order of effectiveness and efficiency of these two testing techniques has not been discussed in literature. In addition, the validity of each individual study is subject to external threats making it hard to draw any general conclusion based solely on a single study. This paper introduces a novel meta-analytical approach to quantify and compare mutation and data flow testing techniques based on findings reported in research articles. We report the results of two statistical meta-analyses performed on comparing and measuring the effectiveness as well as efficiency of mutation and data-flow testing based on relevant empirical studies. We focus on the results of three empirical research articles selected from the premier venues with their focus on comparing these two testing techniques. The results show that mutation is at least two times more effective than data-flow testing, i.e., odds ratio= 2.27. However, mutation is three times less efficient than data-flow testing, i.e., odds ratio= 2.94.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954435,no
Test Case Generation from Mutants Using Model Checking Techniques,"Mutation testing is a powerful testing technique: a program is seeded with artificial faults and tested. Undetected faults can be used to improve the test bench. The problem of automatically generating test cases from undetected faults is typically not addressed by existing mutation testing systems. We propose a symbolic procedure, namely Sym BMC, for the generation of test cases from a given program using Bounded Model Checking (BMC) techniques. The Sym BMC procedure determines a test bench, that detects all seeded faults affecting the semantics of the program, with respect to a given unrolling bound. We have built a prototype tool that uses a Satisfiability Modulo Theories (SMT) solver to generate test cases and we show initial results for ANSI-C benchmark programs.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954438,no
An Experience Report on Using Code Smells Detection Tools,"Detecting code smells in the code and consequently applying the right refactoring steps when necessary is very important to improve the quality of the code. Different tools have been proposed for code smell detection, each one characterized by particular features. The aim of this paper is to describe our experience on using different tools for code smell detection. We outline the main differences among them and the different results we obtained.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954446,no
Prioritising Refactoring Using Code Bad Smells,"We investigated the relationship between six of Fowler et al.'s Code Bad Smells (Duplicated Code, Data Clumps, Switch Statements, Speculative Generality, Message Chains, and Middle Man) and software faults. In this paper we discuss how our results can be used by software developers to prioritise refactoring. In particular we suggest that source code containing Duplicated Code is likely to be associated with more faults than source code containing the other five Code Bad Smells. As a consequence, Duplicated Code should be prioritised for refactoring. Source code containing Message Chains seems to be associated with a high number of faults in some situations. Consequently it is another Code Bad Smell which should be prioritised for refactoring. Source code containing only one of the Data Clumps, Switch Statements, Speculative Generality, or Middle Man Bad Smell is not likely to be fault-prone. As a result these Code Bad Smells could be put into a lower refactoring priority.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954447,no
On Investigating Code Smells Correlations,"Code smells are characteristics of the software that may indicate a code or design problem that can make software hard to evolve and maintain. Detecting and removing code smells, when necessary, improves the quality and maintainability of a system. Usually detection techniques are based on the computation of a particular set of combined metrics, or standard object-oriented metrics or metrics defined ad hoc for the smell detection. The paper investigates the direct and indirect correlations existing between smells. If one code smell exists, this can imply the existence of another code smell, or if one smell exists, another one cannot be there, or perhaps it could observe that some code smells tend to go together.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954451,no
"Automatic Validation and Correction of Formalized, Textual Requirements","Nowadays requirements are mostly specified in unrestricted natural language so that each stakeholder understands them. To ensure high quality and to avoid misunderstandings, the requirements have to be validated. Because of the ambiguity of natural language and the resulting absence of an automatic mechanism, this has to be done manually. Such manual validation techniques are time-consuming, error-prone, and repetitive because hundreds or thousands of requirements must be checked. With an automatic validation the requirements engineering process can be faster and can produce requirements of higher quality. To realize an automatism, we propose a controlled natural language (CNL) for the documentation of requirements. On basis of the CNL, a concept for an automatic requirements validation is developed for the identification of inconsistencies and incomplete requirements. Additionally, automated correction operations for such defective requirements are presented. The approach improves the quality of the requirements and therefore the quality of the whole development process.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954453,no
Model Based Test Specifications: Developing of Test Specifications in a Semi Automatic Model Based Way,Developing and implementing conformity tests is a time-consuming and fault-prone task. To reduce these efforts a new route must be tackled. The current way of specifying tests and implementing them includes too many manual parts. Based on the experience of testing electronic smart cards in ID documents like passports or ID cards the author describes a new way of saving time to write new test specifications and to get test cases based on these specifications. With new technologies like model based testing (MBT) and domain specific languages (DSL) it is possible to improve the specification and implementation of tests significantly. The author describes his experience in using a DSL to define a new language for testing smart cards and to use this language to generate both documents and test cases that can be run in several test tools.,2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954454,no
"Scanstud: A Methodology for Systematic, Fine-Grained Evaluation of Static Analysis Tools","Static analysis of source code is considered to be a powerful tool for detecting potential security vulnerabilities. However, only limited information regarding the current quality of static analysis tools exist. A public assessment of the capabilities of the competing approaches and products is not available. Also, neither a common benchmark nor a standard evaluation procedure has yet been defined. In this paper, we propose a general methodology for systematically evaluating static analysis tools. We document the design of an automatic execution and evaluation framework to support iterative test case design and reliable result analysis. Furthermore, we propose a methodology for creating test cases which can assess the specific capabilities of static analysis tools on a very fine level of detail. We conclude the paper with a brief discussion of our experiences which we collected through a practical evaluation study of six commercial static analysis products.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954458,no
Introducing Test Case Derivation Techniques into Traditional Software Development: Obstacles and Potentialities,"In traditional development, extracting test cases manually is an effort-consuming and error-prone process. To examine whether automation techniques can be integrated into such traditional development, we implemented our previously proposed method to """"TesMa"""", a test case generation tool. We had a case study to evaluate the effectiveness and the cost.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954464,no
Assessing the Impact of Using Fault Prediction in Industry,"Software developers and testers need realistic ways to measure the practical effects of using fault prediction models to guide software quality improvement methods such as testing, code reviews, and refactoring. Will the availability of fault predictions lead to discovery of different faults, or to more efficient means of finding the same faults? Or do fault predictions have no practical impact at all? In this challenge paper we describe the difficulties of answering these questions, and the issues involved in devising meaningful ways to assess the impact of using prediction models. We present several experimental design options and discuss the pros and cons of each.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954465,no
An Empirical Study on Object-Oriented Metrics and Software Evolution in Order to Reduce Testing Costs by Predicting Change-Prone Classes,"Software maintenance cost is typically more than fifty percent of the cost of the total software life cycle and software testing plays a critical role in reducing it. Determining the critical parts of a software system is an important issue, because they are the best place to start testing in order to reduce cost and duration of tests. Software quality is an important key factor to determine critical parts since high quality parts of software are less error prone and easy to maintain. As object oriented software metrics give important evidence about design quality, they can help software engineers to choose critical parts, which should be tested firstly and intensely. In this paper, we present an empirical study about the relation between object oriented metrics and changes in software. In order to obtain the results, we analyze modifications in software across the historical sequence of open source projects. Empirical results of the study indicate that the low level quality parts of a software change frequently during the development and management process. Using this relation we propose a method that can be used to estimate change-prone classes and to determine parts which should be tested first and more deeply.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954466,no
Probabilistic Error Propagation Modeling in Logic Circuits,"Recent study has shown that accurate knowledge of the false negative rate (FNR) of tests can significantly improve the diagnostic accuracy of spectrum-based fault localization. To understand the principles behind FNR modeling in this paper we study three error propagation probability (EPP) modeling approaches applied to a number of logic circuits from the 74XXX/ISCAS-85 benchmark suite. Monte Carlo simulations for random injected faults show that a deterministic approach that models gate behavior provides high accuracy (O(1%)), while probabilistic approaches that abstract from gate modeling generate higher prediction errors (O(10%)), which increase with the number of injected faults.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954474,no
Modeling the Diagnostic Efficiency of Regression Test Suites,"Diagnostic performance, measured in terms of the manual effort developers have to spend after faults are detected, is not the only important quality of a diagnosis. Efficiency, i.e., the number of tests and the rate of convergence to the final diagnosis is a very important quality of a diagnosis as well. In this paper we present an analytical model and a simulation model to predict the diagnostic efficiency of test suites when prioritized with the information gain algorithm. We show that, besides the size of the system itself, an optimal coverage density and uniform coverage distribution are needed to achieve an efficient diagnosis. Our models allow us to decide whether using IG with our current test suite will provide a good diagnostic efficiency, and enable us to define criteria for the generation or improvement of test suites.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954476,no
Compression Strategies for Passive Testing,"Testing is one of the most widely used techniques to increase the confidence on the correctness of complex software systems. In this paper we extend our previous work on passive testing with invariants, this technique checks the logs, collected from the system under test, in order to detect faults. This new proposal is focused on how the collected logs can be compressed without loosing information and how the invariants must be adapted with respect to the selected compression strategy, in a correct way. We show the soundness of this new methodology.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954477,no
A Diagnostic Point of View for the Optimization of Preparation Costs in Runtime Testing,"Runtime testing is emerging as the solution for the validation and acceptance testing of service-oriented systems, where many services are external to the organization, and duplicating the system's components and their context is too complex, if possible at all. In order to perform runtime tests, an additional expense in the test preparation phase is required, both in software development and in hardware. Preparation cost prioritization methods have been based on runtime testability (i.e, coverage) and do not consider whether a good runtime testability is sufficient for a good runtime diagnosis quality in case faults are detected, and whether this diagnosis will be obtained efficiently (i.e., with a low number of test cases). In this paper we show (1) the direct relationship between testability and diagnosis quality, that (2) these two properties do not guarantee an efficient diagnosis, and (3) a measurement that ensures better prediction of efficiency.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954478,no
Detection of Sleeping Cells in LTE Networks Using Diffusion Maps,"In mobile networks emergence of failures is caused by various breakdowns of hardware and software elements. One of the serious failures in radio networks is a Sleeping Cell. In our work one of the possible root causes for appearance of this network failure is simulated in a dynamic network simulator. The main aim of the research is to detect the presence of a Sleeping Cell in the network and to define its location. For this purpose Diffusion Maps data mining technique is employed. The developed fault identification framework is using the performance characteristics of the network, collected during its regular operation, and for that reason it can be implemented in real Long Term Evolution (LTE) networks within the Self-Organizing Networks (SON) concept.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5956626,no
Design Time Validation of Service Orientation Principles Using Design Diagrams,"Design principles of Services ensure reliability, scalability and reusability of software components. Services that follow the design principles are robust to changes and are largely reusable in multiple scenarios but in similar domains. To-date there is no systematic approach to apply these design principles to Services design that will ensure Service quality. Errors in Service design stage often pass through multiple levels of amplification to subsequent stages of development and maintenance. Early detection of Service design faults reduces the cumulative effect on succeeding stages of service development. It is important to validate that the Services follows the principles at design time. In this paper, we present a formal rigorous approach to check the adherence of the Services designed for an enterprise solution to the Service orientation principles using design diagrams. We introduce a set of """"mapping rules"""" by which relevant aspects of design diagrams can be used for validating the Services' adherence to design principles. We also present the results of an empirical study to assess the feasibility of our new approach.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5958162,no
Boundless memory allocations for memory safety and high availability,"Spatial memory errors (like buffer overflows) are still a major threat for applications written in C. Most recent work focuses on memory safety - when a memory error is detected at runtime, the application is aborted. Our goal is not only to increase the memory safety of applications but also to increase the application's availability. Therefore, we need to tolerate spatial memory errors at runtime. We have implemented a compiler extension, Boundless, that automatically adds the tolerance feature to C applications at compile time. We show that this can increase the availability of applications. Our measurements also indicate that Boundless has a lower performance overhead than SoftBound, a state-of-the-art approach to detect spatial memory errors. Our performance gains result from a novel way to represent pointers. Nevertheless, Boundless is compatible with existing C code. Additionally, Boundless provides a trade-off to reduce the runtime overhead even further: We introduce vulnerability specific patching for spatial memory errors to tolerate only known vulnerabilities. Vulnerability specific patching has an even lower runtime overhead than full tolerance.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5958203,no
A combinatorial approach to detecting buffer overflow vulnerabilities,"Buffer overflow vulnerabilities are program defects that can cause a buffer to overflow at runtime. Many security attacks exploit buffer overflow vulnerabilities to compromise critical data structures. In this paper, we present a black-box testing approach to detecting buffer overflow vulnerabilities. Our approach is motivated by a reflection on how buffer overflow vulnerabilities are exploited in practice. In most cases the attacker can influence the behavior of a target system only by controlling its external parameters. Therefore, launching a successful attack often amounts to a clever way of tweaking the values of external parameters. We simulate the process performed by the attacker, but in a more systematic manner. A novel aspect of our approach is that it adapts a general software testing technique called combinatorial testing to the domain of security testing. In particular, our approach exploits the fact that combinatorial testing often achieves a high level of code coverage. We have implemented our approach in a prototype tool called Tance. The results of applying Tance to five open-source programs show that our approach can be very effective in detecting buffer overflow vulnerabilities.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5958225,no
Fault injection-based assessment of aspect-oriented implementation of fault tolerance,"Aspect-oriented programming provides an interesting approach for implementing software-based fault tolerance as it allows the core functionality of a program and its fault tolerance features to be coded separately. This paper presents a comprehensive fault injection study that estimates the fault coverage of two software implemented fault tolerance mechanisms designed to detect or mask transient and intermittent hardware faults. We compare their fault coverage for two target programs and for three implementation techniques: manual programming in C and two variants of aspect-oriented programming. We also compare the impact of different compiler optimization levels on the fault coverage. The software-implemented fault tolerance mechanisms investigated are: i) triple time-redundant execution with voting and forward recovery, and ii) a novel dual signature control flow checking mechanism. The study shows that the variations in fault coverage among the implementation techniques generally are small, while some variations for different compiler optimization levels are significant.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5958244,no
Aaron: An adaptable execution environment,"Software bugs and hardware errors are the largest contributors to downtime, and can be permanent (e.g. deterministic memory violations, broken memory modules) or transient (e.g. race conditions, bitflips). Although a large variety of dependability mechanisms exist, only few are used in practice. The existing techniques do not prevail for several reasons: (1) the introduced performance overhead is often not negligible, (2) the gained coverage is not sufficient, and (3) users cannot control and adapt the mechanism. Aaron tackles these challenges by detecting hardware and software errors using automatically diversified software components. It uses these software variants only if CPU spare cycles are present in the system. In this way, Aaron increases fault coverage without incurring a perceivable performance penalty. Our evaluation shows that Aaron provides the same throughput as an execution of the original application while checking a large percentage of requests - whenever load permits.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5958254,no
A new framework for call admission control in wireless cellular network,"Managing the limited amount of the radio spectrum is an important issue with increasing demand of the same. In recent work, we have introduced MAS (Multi-agent System) for channel assignment problem in wireless cellular networks. Instead of using a base station directly for negotiation, a multi-agent system comprising of software agents was designed to work at base station. The system consists of a collection of layers to take care of local and global scenario. In this paper we propose the combination of MAS with a new call admission control (CAC) mechanism based on fuzzy control. This paper aims to provide improvement in QoS parameters using fuzzy control at call admission level. From the simulation studies it is observed that combined approach of Multi-agent system and fuzzy control at initial level improves channel allocation and other QoS factors in an effective and efficient manner. The simulation results are presented on a benchmark 49 cell environment with 70 channels that validate the performance of this approach.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5958510,no
Automated vulnerability discovery in distributed systems,"In this paper we present a technique for automatically assessing the amount of damage a small number of participant nodes can inflict on the overall performance of a large distributed system. We propose a feedback-driven tool that synthesizes malicious nodes in distributed systems, aiming to maximize the performance impact on the overall behavior of the distributed system. Our approach focuses on the interface of interaction between correct and faulty nodes, clearly differentiating the two categories. We build and evaluate a prototype of our approach and show that it is able to discover vulnerabilities in real systems, such as PBFT, a Byzantine Fault Tolerant system. We describe a scenario generated by our tool, where even a single malicious client can bring a BFT system of over 250 nodes down to zero throughput.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5958811,no
DynaPlan: Resource placement for application-level clustering,"Creating a reliable computing environment from an unreliable infrastructure is a common challenge. Application-Level High Availability (HA) clustering addresses this problem by relocating and restarting applications when failures are detected. Current methods of determining the relocation target(s) of an application are rudimentary in that they do not take into account the myriad factors that influence an optimal placement. This paper presents DynaPlan, a method that improves the quality of failover planning by allowing the expression of a wide and extensible range of considerations, such as multidimensional resource consumption and availability, architectural compatibility, security constraints, location constraints, and policy considerations, such as energy-favoring versus performance-favoring. DynaPlan has been implemented by extending the IBM PowerHA clustering solution running on a group of IBM System P servers. In this paper, we describe the design, implementation, and preliminary performance evaluation of DynaPlan.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5958825,no
Verification of embedded system by a method for detecting defects in source codes using model checking,We have proposed a method based on model checking for detecting hard-to-discover defects in enterprise systems. We apply our method to embedded system development to easily discover some defects caused by input/output data of the hardware which are influenced by the external environment before the software is integrated into the hardware. This paper discuss the effectiveness of our method using a case study to develop a line tracing robot.,2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5958972,no
On sequence based interaction testing,"T-way strategies aim to generate effective test data for detecting fault due to interaction. Different levels of interaction possibilities have been considered as part of existing t-way strategies including that of uniform strength interaction, variable strength interaction as well as input-output based relations. Many t-way strategies have been developed as a result (e.g. GTWay, TConfig, AETG, Jenny and GA for uniform strength interaction; PICT, IPOG and ACS for variable strength interaction; and TVG, Density and ParaOrder for input-output based relations). Although useful, all aforementioned t-way strategies have assumed  sequence-less interactions amongst input parameters. In the case of reactive system, such an assumption is invalid as some parameter operations (or events) occur in sequence and hence, creating a possibility of bugs or faults triggered by the order (or sequence) of input parameters. If t-way strategies are to be adopted in such a system, there is also a need to support test data generation based on sequence of interactions. In line with such a need, this paper discusses the sequence based t-way testing (termed sequence covering array) as an extension of existing t-way strategies. Additionally, this paper also highlights the current progress and achievements.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5958995,no
An Architectural Approach to Support Online Updates of Software Product Lines,"Despite the successes of software product lines (SPL), managing the evolution of a SPL remains difficult and error-prone. Our focus of evolution is on the concrete tasks integrators have to perform to update deployed SPL products, in particular products that require runtime updates with minimal interruption. The complexity of updating a deployed SPL product is caused by multiple interdependent concerns, including variability, traceability, versioning, availability, and correctness. Existing approaches typically focus on particular concerns while making abstraction of others, thus offering only partial solutions. An integrated approach that takes into account the different stakeholder concerns is lacking. In this paper, we present an architectural approach for updating SPL products that supports multiple concerns. The approach comprises of two complementary parts: (1) an update viewpoint that defines the conventions for constructing and using architecture views to deal with multiple update concerns, and (2) a supporting framework that provides an extensible infrastructure supporting integrators of a SPL. We evaluated the approach for an industrial SPL for logistic systems providing empirical evidence for its benefits and recommendations.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5959692,no
Industrial Architectural Assessment Using TARA,"Scenario based architectural assessment is a well established approach for assessing architectural designs. However scenario-based methods are not always usable in an industrial context, where they can be perceived as complicated and expensive to use. In this paper we explore why this may be the case and define a simpler technique called TARA which has been designed for use in situations where scenario based methods are unlikely to be successful. The method is illustrated through a case study that explains how it was applied to the assessment of two quantitative analysis systems.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5959699,no
SOFAS: A Lightweight Architecture for Software Analysis as a Service,"Access to data stored in software repositories by systems such as version control, bug and issue tracking, or mailing lists is essential for assessing the quality of a software system. A myriad of analyses exploiting that data have been proposed throughout the years: source code analysis, code duplication analysis, co-change analysis, bug prediction, or detection of bug fixing patterns. However, easy and straight forward synergies between these analyses rarely exist. To tackle this problem we have developed SOFAS, a distributed and collaborative software analysis platform to enable a seamless interoperation of such analyses. In particular, software analyses are offered as Restful web services that can be accessed and composed over the Internet. SOFAS services are accessible through a software analysis catalog where any project stakeholder can, depending on the needs or interests, pick specific analyses, combine them, let them run remotely and then fetch the final results. That way, software developers, testers, architects, or quality assurance experts are given access to quality analysis services. They are shielded from many peculiarities of tool installations and configurations, but SOFAS offers them sophisticated and easy-to-use analyses. This paper describes in detail our SOFAS architecture, its considerations and implementation aspects, and the current set of implemented and offered Restful analysis services.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5959723,no
Assessing Suitability of Cloud Oriented Platforms for Application Development,"The enterprise data centers and software development teams are increasingly embracing the cloud oriented and virtualized computing platforms and technologies. As a result it is no longer straight forward to choose the most suitable platform which may satisfy a given set of Non-Functional Quality Attributes (NFQA) criteria that is significant for an application. Existing methods such as Serial Evaluation and Consequential Choice etc. are inadequate as they fail to capture the objective measurement of various criteria that are important for evaluating the platform alternatives. In practice, these methods are applied in an ad-hoc fashion. In this paper we introduce three application development platforms: 1) Traditional non-cloud 2) Virtualized and 3) Cloud Aware. We propose a systematic method that allows the stakeholders to evaluate these platforms so as to select the optimal one by considering important criteria. We apply our evaluation method to these platforms by considering a certain (non-business) set of NFQAs. We show that the pure cloud oriented platforms fare no better than the traditional non-cloud and vanilla virtualized platforms in case of most NFQAs.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5959767,no
A Low-Power High-Performance Concurrent Fault Detection Approach for the Composite Field S-Box and Inverse S-Box,"The high level of security and the fast hardware and software implementations of the Advanced Encryption Standard have made it the first choice for many critical applications. Nevertheless, the transient and permanent internal faults or malicious faults aiming at revealing the secret key may reduce its reliability. In this paper, we present a concurrent fault detection scheme for the S-box and the inverse S-box as the only two nonlinear operations within the Advanced Encryption Standard. The proposed parity-based fault detection approach is based on the low-cost composite field implementations of the S-box and the inverse S-box. We divide the structures of these operations into three blocks and find the predicted parities of these blocks. Our simulations show that except for the redundant units approach which has the hardware and time overheads of close to 100 percent, the fault detection capabilities of the proposed scheme for the burst and random multiple faults are higher than the previously reported ones. Finally, through ASIC implementations, it is shown that for the maximum target frequency, the proposed fault detection S-box and inverse S-box in this paper have the least areas, critical path delays, and power consumptions compared to their counterparts with similar fault detection capabilities.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5962403,no
10-Gbps IP Network Measurement System Based on Application-Generated Packets Using Hardware Assistance and Off-the-Shelf PC,"Targeting high-bandwidth applications such as video streaming services, we discuss advanced measurement systems for high-speed 10-Gbps networks. To verify service stability in such high-speed networks, we need to detect network quality under real environmental conditions. For example, test traffic injected into networks under-test for measurements should have the same complex characteristics as the video streaming traffic. For such measurements, we have built Internet protocol (IP) stream measurement systems by using our 10-Gbps network interface card with hardware-assisted active/passive monitor extensions based on low-cost off-the-shelf personal computers (PCs). After showing hardware requirements and our implementation of each hardware-assisted extensions, we report how we build pre-service and in-service network measurement systems to verify the feasibility of our hardware architecture. A traffic-playback system captures packets and stores traffic characteristics data without sampling any packets and then sends them precisely emulating the complex characteristics of the original traffic by using our hardware assistance. The generated traffic is useful as test traffic in pre-service measurement. A distributed in-service network monitoring system collects traffic characteristics at multiple sites by utilizing synchronized precise timestamps embedded in video streaming traffic. The results are presented on the operator's display. We report on their effectiveness by measuring 1.5-Gbps uncompressed high-definition television traffic flowing in the high-speed testbed IP network in Japan.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5962735,no
Automatic Correction of Registration Errors in Surgical Navigation Systems,"Surgical navigation systems are used widely among all fields of modern medicine, including, but not limited to ENT- and maxillofacial surgery. As a fundamental prerequisite for image-guided surgery, intraoperative registration, which maps image to patient coordinates, has been subject to many studies and developments. While registration methods have evolved from invasive procedures like fixed stereotactic frames and implanted fiducial markers toward surface-based registration and noninvasive markers fixed to the patient's skin, even the most sophisticated registration techniques produce an imperfect result. Due to errors introduced during the registration process, the projection of navigated instruments into image data deviates up to several millimeter from the actual position, depending on the applied registration method and the distance between the instrument and the fiducial markers. We propose a method that allows to automatically and continually improve registration accuracy during intraoperative navigation after the actual registration process has been completed. The projections of navigated instruments into image data are inspected and validated by the navigation software. Errors in image-to-patient registration are identified by calculating intersections between the virtual instruments' axes and surfaces of hard bone tissue extracted from the patient's image data. The information gained from the identification of such registration errors is then used to improve registration accuracy by adding an additional pair of registration points at every location where an error has been detected. The proposed method was integrated into a surgical navigation system based on paired points registration with anatomical landmarks. Experiments were conducted, where registrations with deliberately misplaced point pairs were corrected with automatic error correction. Results showed an improvement in registration quality in all cases.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5963707,no
Earthquake response of an arch bridge under near-fault ground motions,"During the Wenchuan earthquake in 2008, some of the arch bridges in the seismic zone were damaged. To assess the performance of arch bridge under near-fault ground motions, both experimental and numerical studies are conducted in this paper. Firstly, an ambient vibration test of a real typical arch bridge is made to measure the dynamic characteristics of the structure and calibrate the finite-element model. Then the earthquake response of the bridge is made using FE software MIDAS based on recorded ground motion in the Wenchuan earthquake. The study shows that the joint between deck and tie at 1/4 span and 3/4 span, the joint between beam and arch and middle of the beam are weak links under near-fault ground motions.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5964478,no
Software component quality-finite mixture component model using Weibull and other mathematical distributions,"Software component quality has a major influence in software development project performances such as lead-time, time to market and cost. It also affects the other projects within the organization, the people assigned into the projects and the organization in general. In this study a finite mixture of several mathematical distributions is used to describe the fault occurrence in the system based on individual software component contribution. Several examples are selected to demonstrate model fitting and comparison between the models. Four case studies are presented and evaluated for modeling software quality in very large development projects within the AXE platform, BICC as a call control protocol in the Ericsson Nikola Tesla R&D.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5967122,no
Interconnectable gadgets and web services usage in supervisory control of Unmanned Underwater vehicles,"Unmanned Underwater vehicles (UUVs) are routinely used for data collection during underwater research missions. UUV operators which perform advanced data collection are usually not qualified for data interpretation. On the fly adaptation of data collection methods based on interpreted data can increase data quality and lower the operator's effort. However, this requires the presence of an expert on site. In order to avoid this, a system of remote monitoring and control over the Internet is proposed. Closing the vehicle control loop over the Internet is problematic due to latency issues, therefore a supervisory control approach is used. This requires only high-level commands to be sent over the Internet while closing the control loop locally. Service oriented architecture (SOA) is used as an API for vehicle monitoring and mission control, while software gadgets are used to display collected data and to send commands for mission adaptation. Gadgets provide support for modifying and displaying data as well as defining and detecting logical conditions. Usage of connectible gadgets as building blocks eliminates the need for expertise in programming languages while increasing the scalability and flexibility of the system.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5967143,no
Complex systems and risk management,"The risk management process, and in particular, risk assessment is a very tedious and error prone process with no exact measure of how it progresses, or even the justification that it reflects the real situation. This is because the whole process heavily depends on the experience of the people doing it. Furthermore, simplifications are done that run just contrary to what the real systems are, complex systems! In this paper we argue that all this process has to be done with complexity in mind, as it is complex system, and we outline a novel risk management method based on those premises. It is possible to automate the risk assessment process presented in this paper to a high degree. Also, the risk method has better justifications and is less dependent on the skills of the people doing risk assessment. Finally, progress can be measured by measuring the complexity of the model.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5967302,no
Design of Three Phase Network Parameter Monitoring System based on 71M6513,"In this paper, the design process of Three Phase Network Parameter Monitoring System is analyzed and the overall research scheme and design idea of the system are expounded. This paper mainly studies how to improve the accuracy of input voltage and current. The front-end voltage decreasing, current dropping circuit and remote data transmission circuit are designed. The programs of the data acquisition and calculation are made, real-time detection of power network parameters, energy calculation and the remote data transmission are achieved. Based on .Net platform, the host computer software is designed to receive the data and store them in the database, the mean value of power and energy parameters are calculated by the day, the week and the month, which are available to the user for inquiry. Many parameters such as three-phase voltage, three-phase current, frequency, active power, reactive power and voltage harmonics, can be detected by the system, which will provide a reliable basis for power quality analysis.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5968842,no
Online monitoring of transformer winding axial displacement and its extent using scattering parameters and k-nearest neighbour method,"The online monitoring of the transformer winding using scattering parameters fingerprint is presented. As a test object, a simplified model of transformer is used. The winding axial displacement is modelled on this test object. The scattering parameters of the test object are calculated using the high-frequency simulation software and measured using a network analyser. Two indices are defined based on the magnitude and phase of scattering parameters for the detection of the axial displacement. A new algorithm for the estimation of the axial displacement extent is presented using the proposed indices and high-frequency modelling of the transformer. To detect this mechanical defect and its extent, the <i>k</i>-nearest neighbour (<i>k</i>-NN) regression is suggested.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5969513,no
Reasoning about Faults in Aspect-Oriented Programs: A Metrics-Based Evaluation,"Aspect-oriented programming (AOP) aims at facilitating program comprehension and maintenance in the presence of crosscutting concerns. Aspect code is often introduced and extended as the software projects evolve. Unfortunately, we still lack a good understanding of how faults are introduced in evolving aspect-oriented programs. More importantly, there is little knowledge whether existing metrics are related to typical fault introduction processes in evolving aspect-oriented code. This paper presents an exploratory study focused on the analysis of how faults are introduced during maintenance tasks involving aspects. The results indicate a recurring set of fault patterns in this context, which can better inform the design of future metrics for AOP. We also pinpoint AOP-specific fault categories which are difficult to detect with popular metrics for fault-proneness, such as coupling and code churn.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5970147,no
Adding Process Metrics to Enhance Modification Complexity Prediction,"Software estimation is used in various contexts including cost, maintainability or defect prediction. To make the estimate, different models are usually applied based on attributes of the development process and the product itself. However, often only one type of attributes is used, like historical process data or product metrics, and rarely their combination is employed. In this report, we present a project in which we started to develop a framework for such complex measurement of software projects, which can be used to build combined models for different estimations related to software maintenance and comprehension. First, we performed an experiment to predict modification complexity (cost of a unity change) based on a combination of process and product metrics. We observed promising results that confirm the hypothesis that a combined model performs significantly better than any of the individual measurements.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5970157,no
Design Defects Detection and Correction by Example,"Detecting and fixing defects make programs easier to understand by developers. We propose an automated approach for the detection and correction of various types of design defects in source code. Our approach allows to automatically find detection rules, thus relieving the designer from doing so manually. Rules are defined as combinations of metrics/thresholds that better conform to known instances of design defects (defect examples). The correction solutions, a combination of refactoring operations, should minimize, as much as possible, the number of defects detected using the detection rules. In our setting, we use genetic programming for rule extraction. For the correction step, we use genetic algorithm. We evaluate our approach by finding and fixing potential defects in four open-source systems. For all these systems, we found, in average, more than 80% of known defects, a better result when compared to a state-of-the-art approach, where the detection rules are manually or semi-automatically specified. The proposed corrections fix, in average, more than 78%of detected defects.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5970166,no
Satisfying Programmers' Information Needs in API-Based Programming,"Programmers encounter many difficulties in using an API to solve a programming task. To cope with these difficulties, they browse the Internet for code samples, tutorials, and API documentation. In general, it is time-consuming to find relevant help from the plethora of information on the web. While programmers can use search-based tools to help locate code snippets or applications that may be relevant to the APIs they are using, they still face the significant challenge of understanding and assessing the quality of the search results. We propose to investigate a proactive help system that is integrated in a development environment to provide contextual suggestions to the programmers as the code is being read and edited in the editor.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5970174,no
Context and Vision: Studying Two Factors Impacting Program Comprehension,"Linguistic information derived from identifiers and comments has a paramount role in program comprehension. Indeed, very often, program documentation is scarce and when available, it is almost always outdated. Previous research works showed that program comprehension is often solely grounded on identifiers and comments and that, ultimately, it is the quality of comments and identifiers that impact the accuracy and efficiency of program comprehension. Previous works also investigated the factors influencing program comprehension. However, they are limited by the available tools used to establish relations between cognitive processes and program comprehension. The goal of our research work is to foster our understanding of program comprehension by better understanding its implied underlying cognitive processes. We plan to study vision as the fundamental mean used by developers to understand a code in the context of a given program. Vision is indeed the trigger mechanism starting any cognitive process, in particular in program comprehension. We want to provide supporting evidence that context guides the cognitive process toward program comprehension. Therefore, we will perform a series of empirical studies to collect observations related to the use of context and vision in program comprehension. Then, we will propose laws and then derive a theory to explain the observable facts and predict new facts. The theory could be used in future empirical studies and will provide the relation between program comprehension and cognitive processes.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5970176,no
Capturing Expert Knowledge for Automated Configuration Fault Diagnosis,"The process of manually diagnosing a software misconfiguration problem is time consuming. Manually writing and updating rules to detect future problems is still the state of the practice. Consequently, there is a need for increased automation. In this paper, we propose a three-phase framework using machine learning techniques for automated configuration faults diagnosis. This system can also help in capturing expert knowledge of configuration troubleshooting. Our experiments on Apache web server configurations are generally encouraging and non-experts can use this system to diagnose misconfigurations effectively.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5970183,no
Towards model-driven safety analysis,"Model-based safety analysis allows very high quality analysis of safety requirements. Both qualitative (i.e. what must go wrong for a system failure) and quantitative aspects (i.e. how probable is a system failure) are of great interest for safety analysis. Traditionally, the analysis of these aspects requires separate, tool-dependent formal models. However, building adequate models for each analysis requires a lot of effort and expertise. Model-driven approaches support this by automating the generation of analysis models. SAML is a tool-independent modeling framework that allows for the construction of models with both non-deterministic and probabilistic behavior. SAML models can automatically be transformed into the input language of different state of the art formal analysis tools - while preserving the semantics - to analyze different aspects of safety. As a consequence both - qualitative and quantitative - model-based safety analysis can be done without any additional generation of models and with transferable results. This approach makes SAML an ideal intermediate language for a model-driven safety analysis approach. Every higher-level language that can be transformed into SAML can be analyzed with all targeted formal analysis tools. New analysis tools can be added and the user benefits from every advancement of the analysis tools.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5970318,no
Power swing detection for correct distance relay operation using S-transform and neural networks,"This paper presents an advanced technique for detecting power swings for distance relay operation. It uses a derivative of voltage and signal processing technique called as S-transform for feature extraction. Then an artificial neural network (ANN) is deployed in detecting the unstable swing in power systems. This approach overcomes the traditional relay scheme drawback, by distinguishing a fault, stable swing and unstable swing. To illustrate the effectiveness of the proposed approach, simulations were carried out on the IEEE 39 bus test system using the PSS/E software. Test results show that the proposed approach can effectively differentiate the fault, stable swing and unstable swing with a good accuracy.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5970431,no
A cross platform intrusion detection system using inter server communication technique,"In recent years, web applications have become tremendously popular. However, vulnerabilities are pervasive resulting in exposure of organizations and firms to a wide array of risks. SQL injection attacks, which has been ranked at the top in web application attack mechanisms used by hackers can potentially result in unauthorized access to confidential information stored in a backend database and the hackers can take advantages due to flawed design, improper coding practices, improper validations of user input, configuration errors, or other weaknesses in the infrastructure. Whereas using cross-site scripting techniques, miscreants can hijack Web sessions, and craft credible phishing sites. In this paper we have made a survey on different techniques to prevent SQLi and XSS attacks and we proposed a solution to detect and prevent against the malicious attacks over the developer's Web Application written in programming languages like PHP, ASP.NET and JSP also we have created an API (Application Programming Interface) in native language through which transactions and interactions are sent to IDS Server through Inter Server Communication Mechanism. This IDS Server which is developed from PHPIDS, a purely PHP based intrusion detection system and has a system architecture meant only for PHP application detects and prevents attacks like SQLi (SQL Injection) and XSS(Cross-site scripting), LFI(Local File Inclusion), and RFE(Remote File Execution) and returns back the result to the Web Application and logs the intrusions. In addition to this behavioural pattern of Web Logs is analysed using WAPT algorithm (Web Access Pattern Tree), which helps in recording the activity of the web application and examines any suspicious behaviour, uncommon patterns of behaviour over a period of time, and it also monitors the increased activity and known attack variants. Based on this an report is generated dynamically using P-Chart which can help the Website owner to increase the security measu- - res, and also used to improve the quality of the Web Application.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5972284,no
Black box test case prioritization techniques for semantic based composite web services using OWL-S,"Web services are the basic building blocks for the business which is different from web applications. Testing of web services is difficult and increases the cost due to the unavailability of source code. Researchers have, web services are tested based on the syntactic structure using Web Service Description Language (WSDL) for atomic web services. This paper proposes an automated testing framework for composite web services based on semantics where the domain knowledge of the web services is described using protege tool and the behaviour of the entire business operation flow for the composite web service is described by Ontology Web Language for services (OWL-S). Prioritization of test cases is performed based on various coverage criteria for composite web services. Series of experiments were conducted to assess the effectiveness of prioritization and empirical results shown that prioritization techniques perform well in detecting faults compared to traditional techniques.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5972354,no
An integrated apparatus to measure Mallampati score for the characterization of Obstructive Sleep Apnea (OSA),"Obstructive Sleep Apnea (OSA) affects as many as 1 in every 5 adults and has the potential to cause serious long-term health complications such as, cardiovascular disease, stroke, hypertension and the consequent reduced quality of life. Studies have shown that the probability of having OSA increases with a higher BMI irrespective of gender and that there is a definite link concerning the race of the patient and having OSA. This paper describes the design of an integrated apparatus to collect Mallampati scores with little human intervention and perform automatic processing of various parameters. The system permits life-cycle studies on patients with OSA and other sleep disorders.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5972398,no
Data mining: An application to the semiconductor industry,"The development project consists of the study and use of data mining techniques to analyze a dataset of high dimensionality and large volume generated during the manufacture of memory modules in a semiconductor industry. In this paper, we propose the use of a neural network self-organizing map as a technique for knowledge extraction, we discuss the analysis phase and preparing the database for the mining process and presents the following steps research. It is expected that the results at the end of the project to identify relationships and / or patterns that may help in predicting possible factors causing failures during the production process, thus contributing to improving the quality of industrial processes, particularly in semiconductor industry.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5974165,no
Comparative analysis of virtual worlds,"This paper presents a comparative analysis between a set of virtual worlds in order to facilitate the process of selecting a virtual world to serve as a platform for application development. Based on exhaustive research in the area, we selected a set of criteria, based on the work of Mannien in 2004 and Robbins in 2009. After this identification we applied the Quantitative Evaluation Framework (QEF) developed by Squire in 2007 with the aim of assessing quantitatively the platforms under consideration. The results showed that Second Life, OpenSim and Active Worlds are platforms that offer more services and tools for developing applications with quality.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5974203,no
"Classification of defect types in requirements specifications: Literature review, proposal and assessment","Requirements defects have a major impact throughout the whole software lifecycle. Having a specific defects classification for requirements is important to analyse the root causes of problems, build checklists that support requirements reviews and to reduce risks associated with requirements problems. In our research we analyse several defects classifiers; select the ones applicable to requirements specifications, following rules to build defects taxonomies; and assess the classification validity in an experiment of requirements defects classification performed by graduate and undergraduate students. Not all subjects used the same type of defect to classify the same defect, which suggests that defects classification is not consensual. Considering our results we give recommendations to industry and other researchers on the design of classification schemes and treatment of classification results.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5974237,no
A programming tool to ease modular programming with C++,"Module management support is very rough in the C and C++ programming languages. Modules must be separated in interface and implementation files, which will store declarations and definitions, respectively. Ultimately, only text substitution tools are available, by means of the C/C++ preprocessor, which is able to insert an interface file in a given point of a translation unit. This way of managing modules does not take into account aspects like duplicated inclusions, or proper separation of declarations and definitions, just to name a few. While the seasoned programmer will find this characteristic of the language annoying and error-prone, students will find it not less than challenging. In this document, a tool specially designed for improving the support of modules in C++ is presented. Its main advantage is that it makes it easier to manage large, module-based projects, while still allowing to use classic translation units. This tool is designed for students who have to learn modular programming; not only those in the computer science discipline, but also those in other engineerings in which programming is part of the curriculum.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5974276,no
Building the pillars for the definition of a data quality model to be applied to the artifacts used in the Planning Process of a Software Development Project,"The success of a software development project is mainly dependent on the quality of the used artifacts along the project; this quality is reliant on the contents of the artifacts, as well as the level of quality of the data values corresponding to the metadata that describe the artifacts. In order to assess both kind of qualities, it should be therefore taken into account the artifacts' structure and metadata. This paper proposes a DQ model that can be used as a reference by project managers to assess and, if necessary, improve the quality of the data values corresponding to the metadata describing the artifacts used in the process of planning a software development project. For our research, we have identified the corresponding artifacts from those described as part of the Planning Process defined in international standard ISO / IEC 12207:2008. We have aligned these found artifacts with those proposed by PMBOK, in order to better depict their structure; and finally, we are to build our data quality model upon the DQ dimensions proposed by Strong, D. M., Y. W. Lee and Wang, R. in Data Quality in Context. Comm. of the ACM 1997 40 (5): 103-110. We all of these elements, we intend to optimize the performance of the software development process by improving the project management process.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5974325,no
Real world oriented test functions,"The global optimal solutions of presently widely-used test functions are known or controllable, which leaves room for algorithm falsification. Moreover, the most algorithmic results to optimize test functions from papers are one-way conversation without peers' recognition, whose authenticity depends entirely on the authors' personal integrity. In this paper, in order to change the situation, two groups of real world oriented test functions and the resultative optimal solutions are given. The chief characteristics of these functions are its global optimal solution unknown to all people forever, which blocks off the loopholes of algorithmic cheating from the source. The comparison of the quality of the two algorithmic models depends on which can find out a better optimal solution coordinate. Therefore, a real world oriented test function can detect the actual level of algorithm.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5975041,no
Research of software tools for DO-254 projects,"Airborne Electronic Hardware (AEH) development relies deeply on the quality of tools that helps the hardware artifact implementation from requirement to entity. Electronic Data Automation (EDA) tools are made to test the logic, synthesize the circuits, place and route the electronic elements and their connections prior to final implementation. A critical issue for EDA tools is its adequate safety to use in avionic. The paper focuses on approaches, from EDA industry, to using the EDA tools in the DO-254 project recently, then explains the principle of assessment and qualification, finally introduces methods to assess some EDA tools. The discussed contents will provide a guideline for the tools certification process.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5975052,no
Dynamic fault tree analysis approach to Safety Analysis of Civil Aircraft,"As one of key projects of China at present, the safety of the civil aircraft is a major issue first considered in design and development. Fault-tolerant systems with redundancy are used more often for the design of modern advanced large aircraft. It is a new research area for the safety analysis technology which is not perfect in China. A novel approach to Safety Analysis of Civil Aircraft based on Dynamic Fault Tree Analysis has been proposed in this paper. This paper takes the mature typical design of large aircraft and general aircraft in developed countries as research object, performs safety analysis using dynamic fault tree, and optimizes the analysis of fault tree using modularization thinking. Then, the failure probability of civil aircraft can be calculated in the newly-developed simulation software.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5975816,no
An Evaluation of QoE in Cloud Gaming Based on Subjective Tests,"Cloud Gaming is a new kind of service, which combines the successful concepts of Cloud Computing and Online Gaming. It provides the entire game experience to the users remotely from a data center. The player is no longer dependent on a specific type or quality of gaming hardware, but is able to use common devices. The end device only needs a broadband internet connection and the ability to display High Definition (HD) video. While this may reduce hardware costs for users and increase the revenue for developers by leaving out the retail chain, it also raises new challenges for service quality in terms of bandwidth and latency for the underlying network. In this paper we present the results of a subjective user study we conducted into the user-perceived quality of experience (QoE) in Cloud Gaming. We design a measurement environment, that emulates this new type of service, define tests for users to assess the QoE, derive Key Influence Factors (KFI) and influences of content and perception from our results.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5976180,no
Reliability of consecutive k-out-of-n: F system under incomplete information,"Most studies of consecutive k-out-of-n: F system assume that the component lifetime distributions are known precisely, this implies the reliability of component is known. However it is difficult to expect this assumption is fulfilled, (such as software, human-machine systems). Therefore the paper considers the reliability of consecutive k-out-of-n: F system by using imprecise probability theory under the condition, that there is only some partial information about the component lifetime distribution. The exact formulas are obtained when k is 2; an algorithm is put forward. Finally an example is illustrated.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5976569,no
Software defect prediction based on stability test data,"Software defect prediction is an essential part of evaluating product readiness in terms of software quality prior to the software delivery. As a new software load with new features and bug fixes becomes available, stability tests are performed typically with a call load generator in a full configuration environment. Defect data from the stability test provides most accurate information required for the software quality assessment. This paper presents a software defect prediction model using defect data from stability test. We demonstrate that test run duration in hours is a better measure than calendar time in days for predicting the number of defects in a software release. An exponential reliability growth model is applied to the defect data with respect to test run duration. We then address how to identify whether estimates of the model parameters are stable enough for assuring the prediction accuracy.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5976636,no
Research on key techniques of simulation and detection for fire control systemon the basis of database,"Fire control system is the core component of the x-type artillery weapon system. Because of the position distribution, work control process and signal transfer relationship of the units in the fire control system, the traditional detective devices are limited when detecting it. This paper describe how to simulate the work environment of the fire control system on the basis of which the equipment units are detected. Realizing the management of sending and reception of serial port information by building the datagram management model and communication protocol management model adopting database techniques in simulation system. On the basis of the purpose that it is used to detect the units, it is hardly necessary to realize the ballistic curve solving and manipulation & aiming solving according to the equipment principle. Also, database technique is employed to build up the database of ballistic curve solving and manipulation & aiming solving in line with which the functions of ballistic curve solving and manipulation & aiming solving are completed in simulation system. Then the performance testing is realized through comparing the resolution results with the standard results in database by querying the database.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5976763,no
Study on the relevance of the warnings reported by Java bug-finding tools,"Several bug-finding tools have been proposed to detect software defects by means of static analysis techniques. However, there is still no consensus on the effective role that such tools should play in software development. Particularly, there is still no concluding answer to the following question usually formulated by software developers and software quality managers: how relevant are the warnings reported by bug finding tools? The authors first report an in-depth study involving the application of two bug-finding tools (FindBugs and PMD) in five stable versions of the Eclipse platform. Next, in order to check whether the initial conclusions are supported by other systems, the authors describe an extended case study with 12 systems. In the end, it has been concluded that rates of relevance superior to 50% can be achieved when FindBugs is configured in a proper way. On the other hand, in the best scenario considered in the research, only 10% of the warnings reported by PMD have been classified as relevant.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5977131,no
Suffix tree-based approach to detecting duplications in sequence diagrams,"Models are core artefacts in software development and maintenance. Consequently, quality of models, especially maintainability and extensibility, becomes a big concern for most non-trivial applications. For some reasons, software models usually contain some duplications. These duplications had better be detected and removed because the duplications may reduce maintainability, extensibility and reusability of models. As an initial attempt to address the issue, the author propose an approach in this study to detecting duplications in sequence diagrams. With special preprocessing, the author convert 2-dimensional (2-D) sequence diagrams into an 1-D array. Then the author construct a suffix tree for the array. With the suffix tree, duplications are detected and reported. To ensure that every duplication detected with the suffix tree can be extracted as a separate reusable sequence diagram, the author revise the traditional construction algorithm of suffix trees by proposing a special algorithm to detect the longest common prefixes of suffixes. The author also probe approaches to removing duplications. The proposed approach has been implemented in DuplicationDetector. With the implementation, the author evaluated the proposed approach on six industrial applications. Evaluation results suggest that the approach is effective in detecting duplications in sequence diagrams. The main contribution of the study is an approach to detecting duplications in sequence diagrams, a prototype implementation and an initial evaluation.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5977133,no
From village greens to volcanoes: Using wireless networks to monitor air quality,"Summary form only given. Air quality mapping is a rapidly growing need as people become aware of the acute and long term risks of poor air quality. Both spatial and temporal mapping is required to understand, predict and improve air quality. The collision of several technologies has led to an opportunity to create affordable wireless networks to monitor air quality: short range radio on a chip; GPS and internet maps; GSM integrated chips; improved batteries and solar power; Python and other internet languages; and low power, low cost gas sensors with ppb resolution. Bringing these technologies together requires collaboration between electronics engineers, mathematicians, software programmers, atmospheric chemists and sensor technology providers. And add in local politicians because wireless networks need permission. We will discuss implementation and results of successful trials and look at what is now underway: Newcastle, Cambridge and London trials (MESSAGE); Cambridge UK real time air quality mapping (2010); Heathrow airport air quality network (2011); volcanic ash mapping; landfill site monitoring networks; indoor air quality studies; and rural air measurements. Each application has its own requirements of power, wireless protocol, air monitoring needs, data analysis and presentation of results.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5978637,no
An approach of mission completion success probability prediction for circuits based on Saber simulation,"To solve problems existing in the static fault simulation method, the dynamic one is proposed in this paper, and based on it, an approach of predicting mission completion success probability for circuits is proposed. In this approach, the failure distributions of components are derived from Reliability Prediction Handbook for Electronic Equipment, and the sampling algorithm is introduced to simulate the randomness of failure modes and fault occurring time. Moreover, this approach introduces looping fault simulation and automatic fault judgment techniques to simulate large numbers of tests to predict mission completion success probability for circuits. For the realization of this approach in Saber simulation software, this paper presents automatic fault injection based on MAST models of Saber components and automatic fault judgment techniques in the circuit simulation process, and predicts the mission completion success probability for a representative example with Saber based on the approach.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5979282,no
Research on the definition and model of software testing quality,"Aim of software testing is to find out software defects and evaluate the software quality. In order to explain the software testing can really elevate the software quality, it is necessary to assess the quality of software testing itself. This paper discusses the definition of software testing quality, and further builds the framework model of software testing quality (STQFM) and the metrics model of software testing quality (SFTMM). The availability of the models is verified by example application.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5979352,no
Study of software reliability prediction based on GR neural network,"The failures of safety-critical software may result in the serious loss of property and life, thus software reliability has become very demanding. As an important quantitative approach for estimating and predicting software reliability, software reliability prediction technique is significantly useful for improving and ensuring software quality and testing efficiency. A novel software reliability prediction method based on general regression neural network (GRNN) is proposed, which makes it feasible that without constructing a statistical model like classic software reliability models and having difficulties of solving multivariate likelihood equations, this method can be used to predict software failures. It also incorporates test coverage which has increased prediction accuracy. By using probability plot technique and the least square fitting, the probability distribution functions of the original failure data can be determined. And large amount of data can be simulated to make the reliable prediction, which provides a way for solving the inaccuracy problem caused by small size sample of test failure data. A case study has also been done in a real failure data set. The results show that the proposed method can reflect the relationships among the time, test coverage and number of the faults. And it can improve the prediction accuracy.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5979353,no
The object-FMA based test case generation approach for GUI software exception testing,"The traditional exception testing usually utilizes the error-guessing approach or the equivalence class-partition method to generate test cases, which heavily depend on the experience of testers and easily make the exception testing omissive. In order to solve this problem, this paper introduces the SFMEA (Software Failure Mode Effect Analysis) to generate the exception testing cases for the GUI software by analyzing the failure modes of the controls and the control sets of the GUI software and then translating those failure modes directly into the exception test cases. In order to make the failure mode analysis sufficient, we first propose an object-based approach for the failure mode analysis (i.e. Object-FMA), and then utilize this approach to analyze the failure modes of the common controls in the Windows, and generate the database of the failure modes of the controls for guiding to design the test cases. In an actual GUI software-testing project, a case study is presented through comparing four diverse exception test suites. Three test groups with different experience use the error-guessing approach to generate three exception test suites respectively. Then one group is selected from these three groups to use this proposed Object-FMA approach to generate one exception test suite. The comparison results show that the exception testing cases generated by the Object-FMA approach not only are more sufficient than the ones generated by the error-guessing approach, but also detect more exceptions. This proposed Object-FMA approach can avoid to overreliance on the experience of testers during designing the exception testing cases. Moreover, this approach can ensure the quality of the exception testing cases from the methodological viewpoint. Thus, the feasibility and validity of this proposed Object-FMA approach are validated consequently.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5979358,no
A dynamic software binary fault injection system for real-time embedded software,"Dynamic fault injection is an efficient technique for reliability evaluation. In this paper, a new fault injection system called DDSFIS (debug-based dynamic software fault injection system) is presented. DDSFIS is designed to be adaptable to real-time embedded systems. Locations of injections and faults are detected and injected automatically without recompiling. The tool is highly portable between different platforms since it relies on the GNU tool chains. The effectiveness and performance of DDSFIS is validated by experiments.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5979375,no
The development of a software dependability case based on GSN,"The GSN method is used to develop a dependability case to study the software dependability on the basis of the extension of safety case. Given the scalability of GSN, the dependability of the anti-icy software system is analyzed as a case by developing the software dependability case based on extending GSN from the four aspects: the behaviors and results of the software can be predicted; the software behavior states can be monitored; the software behavior results can be assessed and the software abnormal behaviors can be controlled.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5979376,no
Design and implement of RS-485 bus fault injection,"Testability is a significant design feature for system, which has gong through five stages such as External Test, BIT (Build-in Test), Intelligent BIT, Comprehensive Diagnosis and PHM (Prognostics and Health Management), has greatly improved the maintainability, reliability and availability of the modern weapons especially in electronic systems. Meanwhile, in the process of the researches on systems and equipment, we need to inject faults to undertake the actual tests in the prototype, in order to verify the correctness of the testability analysis and design, identify design flaws and inspect whether the products satisfy the requirements of the testability design absolutely. Nowadays, for the complex electronic systems, the foreign researchers widely use the technology of fault injection to find the testability design flaws in the electronic systems exactly and assess the testability indexes of the systems. In this paper, we have mainly researched the external-bus faults injection system for the requirement of the PHM verification tests in the position of the external-bus, designed and implemented a fault injector based on the RS-485 bus and designed a simulating experimental environment to validate the authenticity and validity of the RS-485 bus fault injector.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5979407,no
Cassini spacecraft post-launch malfunction correction success,"After the launch of the Cassini Mission-to-Saturn Spacecraft, the volume of subsequent mission design modifications was expected to be minimal due to the rigorous testing and verification of the Flight Hardware and Flight Software. For known areas of risk where faults could potentially occur, component redundancy and/or autonomous Fault Protection (FP) routines were implemented to ensure that the integrity of the mission was maintained. The goal of Cassini's FP strategy is to ensure that no credible Single Point Failure (SPF) prevents attainment of mission objectives or results in a significantly degraded mission, with the exception of the class of faults which are exempted due to low probability of occurrence. In the case of Cassini's Propulsion Module Subsystem (PMS) design, a waiver was approved prior to launch for failure of the prime regulator to properly close; a potentially mission catastrophic single point failure. However, one month after Cassini's launch when the fuel and oxidizer tanks were pressurized for the first time, the prime regulator was determined to be leaking at a rate significant enough to require a considerable change in Main Engine (ME) burn strategy for the remainder of the mission. Crucial mission events such as the Saturn Orbit Insertion (SOI) bum task which required a characterization exercise for the PMS system 30 days before the maneuver were now impossible to achieve. This details the steps necessary to support the unexpected malfunction of the prime regulator, the introduction of new failure modes which required new FP design changes consisting of new/modified under-pressure and over-pressure algorithms; all which must be accomplished during the operation phase of the spacecraft, as a result of a presumed low probability waived failure which occurred after launch.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5980604,no
Elman network voting system for cyclic system,"It is important to improve voting system in current software fault tolerance research. In this paper, we propose an Elman network voting system. This is an application of Elman network (a form of recurrent neural network). In time sequential environment, Elman network can predict next state by referencing previous state. Thus, Elman network is especially suitable for cyclic system. Majority voting system is a classical voting system with inherent safety mechanism. Combination of majority voting system and Elman network earns predictive and secure characteristics. Experiment shows that Elman network voting system can give an appropriate advice for disagreement situation after training; this approach performs quite well in small and big turbulent.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5982232,no
Modified real-value negative selection algorithm and its application on fault diagnosis,"Analyze the drawbacks of common real-value negative selection algorithm applied on fault diagnosis, and the modified real-value negative selection algorithm is presented based on the corresponding innovations. Firstly, the fault detector set is partitioned into remember-detector set covering known-fault space and random-detector set covering unknown-fault space. Secondly, taking all known states including normal state as self set in training period, get the random-detector set through negative selection and distribution optimization. Lastly, in order to avoid `Fail to Alarm' event caused by the Hole, the two-time-matching method is presented in detecting period which takes the normal state as self set. A resistance circuit fault diagnosis experiment shows that compared with the common real-value negative selection algorithm, the modified real-value negative algorithm can effectively avoid `Fail to Alarm' event, and has higher diagnostic accuracy.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5982293,no
Software fault detection using program patterns,"Effective detection of software faults is an important activity of software development process. The main difficulty of detecting software fault is finding faults in a large and complex software system. In this paper, we propose an approach that applies program patterns to detect and locate software fault so that programmer can fix bug and increase software quality. The advantage of the proposed approach is that the defect-prone code segments can be detected. To facilitate the programmer to detect program bugs, this approach also includes a Graphic User Interface to locate the defect-prone code segments.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5982308,no
Research on quality of service in wireless sensor networks,"As a complex network consisting of sensing, processing and communication, wireless sensor network is driven by various applications and highly requires new Quality of Service guarantees. The application target of WSN is to monitor events, and it pays more attention to the QoS of colony data packets. So, we present an improved QoS mechanism based on weighted mapping table. Because of the limited computational capabilities of WSN, the proposed mechanism import mapping table. The improved QoS mechanism finds the weight from mapping table according to the level of events, and dynamically adjusts the transmit probability of each event. The mechanism considers the different event delay requirements, and adjusts the bandwidth and event delay reasonably for each event. The simulation proves that the improved mechanism algorithm is simple and effective; it can meet the application requirement of wireless sensor network.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5982316,no
An adaptive threshold segmentation method based on BP neural network for paper defect detection,"Threshold segmentation is the fastest method of defect detection in the modern defect inspection system based on computer vision. But in the real paper defect detection system, the segmentation thresholds usually change with the paper image luminance which is influenced by many factors. In order to resolve this problem, an adaptive threshold segmentation method based on BP neural network is proposed in this paper. For this method, BP neural network models are created and trained to obtain the segmentation thresholds according to the image luminance and the defects are segmented with these thresholds obtained by the network. This method is especially suitable for detecting three typical types of paper defects: dark spot, light spot and hole. The experiment results indicate that this method is efficient and can be applied to modern paper defect inspection system.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5982338,no
Strategy-based two-level fault handling mechanism for composite service,"Service composition becomes more and more popular in enterprise application, but it is prone to errors when the composite service is complex. So fault handling mechanism of composite service turns into an important challenge. However, most of current solutions only concentrate on service-level fault handling which mainly deal with service invoking failures. As process becomes more and more complex and business requirements demand to modify the running process to handle fault, we need to provide a more flexible and effective fault handling mechanism for composite service. So we propose a strategy-based two-level fault handling mechanism for composite service providing user-defined fault handling strategy. Besides the traditional fault handling such as service redundancy, we use composite service evolution for reference, present a process-level fault handling mechanism, so we can solve fault with dynamically evolution of process. Then based on this mechanism, we achieve a BPMN-based composite service execution engine SCENE BPMNEngine which sustain above-mentioned fault handling mechanism. Then we test and verify the effectiveness of this mechanism using specific cases.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5982361,no
Improving quality in a distributed IP telephony system by the use of multiplexing techniques,"Nowadays, many enterprises use Voice over Internet Protocol (VoIP) in their telephony systems. Companies with offices in different countries or geographical areas can build a central managed telephony system sharing the lines of their gateways in order to increase admission probability and to save costs on international calls. So it is convenient to introduce a system to ensure a minimum QoS (Quality of Service) for conferences, and one of these solutions is CAC (Call Admission Control). In this work we study the improvements in terms of admission probability and conversation quality (R-factor) which can be obtained when RTP multiplexing techniques are introduced, as in this scenario there will be multiple conferences with the same origin and destination offices. Simulations have been carried out in order to compare simple RTP and TCRTP (Tunneling Multiplexed Compressed RTP). The results show that these parameters can be improved while maintaining an acceptable quality for the conferences if multiplexing is used.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5984847,no
Influence of traffic management solutions on Quality of Experience for prevailing overlay applications,"Different sorts of peer-to-peer (P2P) applications emerge every day and they are becoming more and more popular. The performance of such applications may be measured by means of Quality of Experience (QoE) metrics. In this paper, the factors that influence these metrics are surveyed. Moreover, the impact of economic traffic management solutions (e.g., proposed by IETF) on perceived QoE for the dominant overlay applications is assessed. The possible regulatory issues regarding QoE for P2P applications are also mentioned.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5985866,no
Evaluating the efficiency of data-flow software-based techniques to detect SEEs in microprocessors,"There is a large set of software-based techniques that can be used to detect transient faults. This paper presents a detailed analysis of the efficiency of dataflow software-based techniques to detect SEU and SET in microprocessors. A set of well-known rules is presented and implemented automatically to transform an unprotected program into a hardened one. SEU and SET are injected in all sensitive areas of MIPS-based microprocessor architecture. The efficiency of each rule and a combination of them are tested. Experimental results are used to analyze the overhead of data-flow techniques allowing us to compare these techniques in the respect of time, resources and efficiency in detecting this type of faults. This analysis allows us to implement an efficient fault tolerance method that combines the presented techniques in such way to minimize memory area and performance overhead. The conclusions can lead designers in developing more efficient techniques to detect these types of faults.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5985914,no
Using an FPGA-based fault injection technique to evaluate software robustness under SEEs: A case study,"Microprocessor-based system's robustness under Single Event Effects is a very current concern. A widely adopted solution to make robust a microprocessor-based system consists in modifying the software application by adding redundancy and fault detection capabilities. The efficiency of the selected software-based solution must be assessed. This evaluation process allows the designers to choose the more suitable robustness technique and check if the hardened system achieves the expected dependability levels. Several approaches with this purpose can be found in the literature, but their efficiency is limited in terms of the number of faults that can be injected, as well as the level of accuracy of the fault injection process. In this paper, we propose FPGA-based fault injection techniques to evaluate software robustness methods under Single Event Upset (SEU) as well as Single Event Transient (SET). Experimental results illustrate the benefits of using the proposed fault injection method, which is able to evaluate a high amount of faults of both types of events.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5985918,no
Automated testing of embedded automotive systems from requirement specification models,"Embedded software for modern automotive and avionic systems is increasingly complex. In early design phases, even when there is still uncertainty about the feasibility of the requirements, valuable information can be gained from models that describe the expected usage and the desired system reaction. The generation of test cases from these models indicates the feasibility of the intended solution and helps to identify scenarios for which the realization is hardly feasible or the intended system behavior is not properly defined. In this paper we present the formalization of requirements by models to simulate the expected field usage of a system. These so called usage models can be enriched by information about the desired system reaction. Thus, they are the basis for all subsequent testing activities: First, they can be used to verify the first implementation models and design decisions w.r.t. the fulfillment of requirements and second, test cases can be derived in a random or statistic manner. The generation can be controlled with operational profiles that describe different classes of field usage. We have applied our approach at a large German car manufacturer in the early development phase of active safety functionalities. Test cases were generated from the usage models to assess the implementation models in MATLAB/Simulink. The parametrization of the systems could be optimized and a faulty transition in the implementation models was revealed. These design and implementation faults had not been discovered with the established test method.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5985928,no
A cache based algorithm to predict HDL modules faults,"Verification is the most challenging and time consuming stage in the integrated circuit development cycle. As designs complexities double every two years, novel verification methodologies are needed. We propose an algorithm that dynamically builds and updates an HDL module error proneness list. This list can be used to assist the development team to allocate resources during verification stage. The algorithm is build up using the idea that problematic modules usually hide many uncovered errors. Thus, our algorithm caches the most frequently modified and fixed modules. In an academic experiment composed by 17 modules, using a cache of size 3, we were able to correctly predict almost 80% of the faults occurrences.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5985931,no
Event-driven monitoring and scheduling in express and logistics industry,"Express delivery industry is growing rapidly as the result of an explosive growth in the e-commerce, telemarketing and TV home-shopping, especially in developing countries. On the other hand parcel delay, loss and damage become the major challenges of the excessive number of express companies. In this paper we introduce an event-driven shipment monitoring and scheduling system, which utilizes the information flow and material flow together to monitor the events during the shipment lifecycle and detect any exception event. A centralized scheduling engine will generate dynamic routing for the shipment in case of exception. The system can efficiently avoid parcel loss and guarantee parcel service quality.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5986575,no
Warpage simulation and optimization for the shell of color liquid crystal display monitor based on Moldflow,"The injection molding of thin-walled shell of color liquid crystal display (LCD) monitor is simulated by using Moldflow software and the warpage is predicted after molding. It is found that the warpage of plastic parts is mainly caused by uneven shrinkage. In order to reduce the warpage of the shell of LCD monitor, different cooling system of the mold are established, optimal molding conditions are found by optimizing process parameters. The results provide the basis for product design and mold manufacture. It is significantly important to improve the efficiency and quality of the plastic parts.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5987082,no
Research on indirect lighting protection for military power supply,"According to lightning protection characteristics and the existing problems of military power supply, the electromagnetic transient model of key part in lighting protection such as generator, control unit and cables based on PSCAD/EMTDC power simulation software is established. The dispersed-continuous separation modeling method of electronic power device in military power supply is suggested. The indirect lightning protection for military power system is presented. It is mainly through installing SPD on the ports of electric supply line, power supply line and signal line, shielding and filtering for protection of indirect lightning. Finally, the lighting protection module device that is in high probability and security is developed and indirect lighting protection and all weather flight ability of military power supply is improved.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5987244,no
Reasonable coal pillar size design in Xiaoming mine,"The pillar size has great influence not only on the stability of surrounding rock, but also on the recovery rate of coal resources, which directly affects the economic benefits of coal. According to XiaoMing tiefa coalmining group geological conditions and the use of coal pillar, it adopts field measurement, numerical simulation and theory calculation methods to get small coal pillar width and reasonable wide size of coal pillar. Use rock stratum detect to find out the fissures, faults, broken area; analyze the coal pillar stress and displacement distribution of the different pillar widths (3, 5, 8, 10,15and20m) by FLAC2D simulation of numerical simulation software, determine a reasonable size finally.lt provides a guarantee of security for the exploitation of the mine.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5987948,no
An Inquiry-based Ubiquitous Tour System,"An inquiry-based ubiquitous tour system is proposed in this paper. The main concept is to use the strategy of inquiry to achieve the purposes of guiding and learning by using PDAs in a ubiquitous environment. This system has the following characteristics: the inquiry learning theory is used to design the guide activities, the provisions of clues are used to guide the inquiry activities of learners in the process of inquiry, and to use interactive learning objects to enhance motivation and learning interest, the system also uses GPS and instant messaging functions for cooperative learning. After the system is completed, a questionnaire based on the technology acceptance model is designed to assess the learning effectiveness of learners. The questionnaire includes system quality, content quality, environment quality, system mobility, perceived usefulness, perceived ease of use, and behavioral intentions to learner. There are a total of 9 hypotheses in the study questions, and a total of 17 items. The experimental results show that the research scale is highly reliable and all of the assumptions are valid.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5989013,no
QoS Driven Web Services Evolution,"The loose coupling and on-demand integration are the fundamental characteristics of the Service Oriented Architecture (SOA), which have enforced rapid development of Web services. However, nonfunctional quality of service (QoS) attributes may evolve due to the changes of network conditions and locations of the service users. Some real services may update their QoS properties on-the-fly, others may turn to unavailable. Thus, addressing the problem of uninformed QoS evolution of Web services has become a significant research issue. This paper proposes a dynamic evolution framework of Web services, which uses the Collaborative Filtering (CF) to predict the QoS values and enables the evolution of Web services. In this framework, the QoS values of current users can be predicted using the past QoS data of similar users. There is no extra Web services invocation. About 1.5 millions real-world QoS data are used for evaluation and the experimental results show that it is a feasible and supplementary manner in dynamic evolution of the Web Services.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5989034,no
Engineering SLS Algorithms for Statistical Relational Models,"We present high performing SLS algorithms for learning and inference in Markov Logic Networks (MLNs). MLNs are a state-of-the-art representation formalism that integrates first-order logic and probability. Learning MLNs structure is hard due to the combinatorial space of candidates caused by the expressive power of first-order logic. We present current work on the development of algorithms for learning MLNs, based on the Iterated Local Search (ILS) metaheuristic. Experiments in real-world domains show that the proposed approach improves accuracy and learning time over the existing state-of-the-art algorithms. Moreover, MAP and conditional inference in MLNs are hard computational tasks too. This paper presents two algorithms for these tasks based on the Iterated Robust Tabu Search (IRoTS) schema. The first algorithm performs MAP inference by performing a RoTS search within a ILS iteration. Extensive experiments show that it improves over the state-of the-art algorithm in terms of solution quality and inference times. The second algorithm combines IRoTS with simulated annealing for conditional inference and we show through experiments that it is faster than the current state-of-the-art algorithm maintaining the same inference quality.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5989060,no
An Atomatic Fundus Image Analysis System for Clinical Diagnosis of Glaucoma,"Glaucoma is a serious ocular disease and leads blindness if it couldn't be detected and treated in proper way. The diagnostic criteria for glaucoma include intraocular pressure measurement, optic nerve head evaluation, retinal nerve fiber layer and visual field defect. The observation of optic nerve head, cup to disc ratio and neural rim configuration are important for early detecting glaucoma in clinical practice. However, the broad range of cup to disc ratio is difficult to identify early changes of optic nerve head, and different ethnic groups possess various features in optic nerve head structures. Hence, it is still important to develop various detection techniques to assist clinicians to diagnose glaucoma at early stages. In this study, we developed an automatic detection system which contains two major phases: the first phase performs a series modules of digital fundus retinal image analysis including vessel detection, vessel in painting, cup to disc ratio calculation, and neuro-retinal rim for ISNT rule, the second phase determines the abnormal status of retinal blood vessels from different aspect of view. In this study, the novel idea of integrating image in painting and active contour model techniques successfully assisted the correct identification of cup and disk regions. Several clinical fundus retinal images containing normal and glaucoma images were applied to the proposed system for demonstration.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5989070,no
Evolutionary Feature Construction for Ultrasound Image Processing and its Application to Automatic Liver Disease Diagnosis,"In this paper, the self organization properties of genetic algorithms are employed to tackle the problem of feature selection and extraction in ultrasound images, which can facilitate early disease detection and diagnosis. Accurately identifying the aberrant features at a particular location of clinical ultrasound images is important to find the possibly damaged tissues. Unfortunately, it is difficult to exactly detect the regions of interest (ROIs) from relatively low quality of clinical ultrasound images. The presented evolutionary optimization algorithm presents a novel approach to building features for automatic liver cirrhosis diagnosis using a genetic algorithm. The extracted features provide several advantages over other feature extraction techniques which include: automatically construct feature set and tune their parameters, ability to integrate multiple feature sets to improve the diagnosis accuracy, and ability to find local ROIs and integrate their local features into effective global features. As compared with past approaches, we span a new way to unify the processing steps in a clinical application using the evolutionary optimization algorithms for ultrasound images. Experimental results show the effectiveness of the proposed method.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5989071,no
Using Diversity to Design and Deploy Fault Tolerant Web Services,"Any software component for instance Web services is prone to unexpected failures. To guarantee business process continuity despite these failures, component replication is usually put forward as a solution to make these components fault tolerant. In this paper we illustrate the limitations of replication and suggests diversity as an alternative solution. In the context of Web services diversity stems from the semantic similarity of functionalities of Web services. Building upon this similarity, a diversity group consisting of semantically similar Web services is built and then controlled using different execution models. Each model defines how the Web services in the diversity group collaborate and step in when one of them fails to ensure operation continuity. An architecture showing the use of diversity to design and deploy fault tolerant Web services is presented in this paper.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5990004,no
Execution Constraint Verification of Exception Handling on UML Sequence Diagrams,"Exception handling alters the control flow of the program. As such, errors introduced in exception handling code may influence the overall program in undesired ways. To detect such errors early and thereby decrease the programming costs, it is worthwhile to consider exception handling at design level. Preferably, design models must be extended to incorporate exception handling behavior and the control flow must be verified accordingly. Common practices for verification require a formal model and semantics of the design. Defining semantics and manually converting design models to formal models are costly. We propose an approach for verifying exception handling in UML design models, where we extend UML with exception handling notations, define execution and exception handling semantics, and automatically transform UML models to a formal model. The formal model is used for generating execution paths. Constraints are specified (as temporal logic formulas) on execution paths and are verified.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5992001,no
Runtime Verification of Domain-Specific Models of Physical Characteristics in Control Software,"Control logic of embedded systems is nowadays largely implemented in software. Such control software implements, among others, models of physical characteristics, like heat exchange among system components. Due to evolution of system properties and increasing complexity, faults can be left undetected in these models. Therefore, their accuracy must be verified at runtime. Traditional runtime verification techniques that are based on states and/or events in software execution are inadequate in this case. The behavior suggested by models of physical characteristics cannot be mapped to behavioral properties of software. Moreover, implementation in a general-purpose programming language makes these models hard to locate and verify. This paper presents a novel approach to explicitly specify models of physical characteristics using a domain-specific language, to define monitors for inconsistencies by detecting and exploiting redundancy in these models, and to realize these monitors using an aspect-oriented approach. The approach is applied to two industrial case studies.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5992002,no
Automatic Synthesis of Static Fault Trees from System Models,"Fault tree analysis (FTA) is a traditional reliability analysis technique. In practice, the manual development of fault trees could be costly and error-prone, especially in the case of fault tolerant systems due to the inherent complexities such as various dependencies and interactions among components. Some dynamic fault tree gates, such as Functional Dependency (FDEP) and Priority AND (PAND), are proposed to model the functional and sequential dependencies, respectively. Unfortunately, the potential semantic troubles and limitations of these gates have not been well studied before. In this paper, we describe a framework to automatically generate static fault trees from system models specified with SysML. A reliability configuration model (RCM) and a static fault tree model (SFTM) are proposed to embed system configuration information needed for reliability analysis and error mechanism for fault tree generation, respectively. In the SFTM, the static representations of functional and sequential dependencies with standard Boolean AND and OR gates are proposed, which can avoid the problems of the dynamic FDEP and PAND gates and can reduce the cost of analysis based on a combinatorial model. A fault-tolerant parallel processor (FTTP) example is used to demonstrate our approach.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5992011,no
Evaluation of Experiences from Applying the PREDIQT Method in an Industrial Case Study,"We have developed a method called PREDIQT for model-based prediction of impacts of architectural design changes on system quality. A recent case study indicated feasibility of the PREDIQT method when applied on a real-life industrial system. This paper reports on the experiences from applying the PREDIQT method in a second and more recent case study - on an industrial ICT system from another domain and with a number of different system characteristics, compared with the previous case study. The analysis is performed in a fully realistic setting. The system analyzed is a critical and complex expert system used for management and support of numerous working processes. The system is subject to frequent changes of varying type and extent. The objective of the case study has been to perform an additional and more structured evaluation of the PREDIQT method and assess its performance with respect to a set of success criteria. The evaluation argues for feasibility and usefulness of the PREDIQT-based analysis. Moreover, the study has provided useful insights into the weaknesses of the method and suggested directions for future research and improvements.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5992012,no
Dynamic Service Replacement to Improve Composite Service Reliability,"Service-oriented architecture (SOA) provides an ability to satisfy the increasing demand of the customer for complicated services in business environments via the composition of service components scattered on the Internet. Service composition is a mechanism to create a new service by the integration of several services to meet complex business goals. Web services are frequently exposed to unexpected service faults in network environments, because most SOA has been recently realized in the area of web services. Thus, services participating in the service composition cannot always be free of service faults, thereby decreasing the reliability of service composition. It is necessary to improve the reliability of the service composition to provide a reliable service. In this paper, we focus on the availability of a web service and propose a technique to improve service composition reliability using the web service-business process execution language (WS-BPEL) to support successful service composition. The proposed technique performs dynamic service replacement with the WS-BPEL extension. This is combined as the concept of the aspect-oriented programming when a web service fault is detected. We can prevent the failures of composite web service from unexpected service faults using our technique.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5992017,no
Automatic Generation of Code for the Evaluation of Constant Expressions at Any Precision with a Guaranteed Error Bound,"The evaluation of special functions often involves the evaluation of numerical constants. When the precision of the evaluation is known in advance (e.g., when developing libms) these constants are simply precomputed once and for all. In contrast, when the precision is dynamically chosen by the user (e.g., in multiple precision libraries), the constants must be evaluated on the fly at the required precision and with a rigorous error bound. Often, such constants are easily obtained by means of formulas involving simple numbers and functions. In principle, it is not a difficult work to write multiple precision code for evaluating such formulas with a rigorous round off analysis: one only has to study how round off errors propagate through sub expressions. However, this work is painful and error-prone and it is difficult for a human being to be perfectly rigorous in this process. Moreover, the task quickly becomes impractical when the size of the formula grows. In this article, we present an algorithm that takes as input a constant formula and that automatically produces code for evaluating it in arbitrary precision with a rigorous error bound. It has been implemented in the Solly a free software tool and its behavior is illustrated on several examples.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5992130,no
MiriaPOD A distributed solution for virtual network topologies management,"Virtualization technologies have grown steadily in the testing and educational environments, owing to their low costs for deploying a wide variety of scenarios. Many applications that provide network node virtualization face scalability problems when larger topologies are created because of their high memory and processing requirements. Although such applications offer methods for load distribution on multiple machines, the distribution must be manually set up through a cumbersome and error prone process. The miriaPOD distributed virtualization application provides an easy to use graphical topology editor that relies on a broker service to deploy virtual network nodes on multiple machines. The extensibility and scalability of the project enables its use for assessing migration scenarios and running network technologies harmonization tests. We also propose integration possibilities with the Moodle platform, positioning the miriaPOD infrastructure as a vital tool for testing and teaching networking concepts.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5993714,no
An intellectual property core to detect task schedulling-related faults in RTOS-based embedded systems,"The use of Real-Time Operating Systems (RTOSs) became an attractive solution to simplify the design of safety-critical real-time embedded systems. Due to their stringent constraints such as battery-powered, high-speed and low-voltage operation, these systems are often subject to transient faults originated from a large spectrum of noisy sources, among them, the conducted and radiated Electromagnetic Interference (EMI). As the major consequence, the system's reliability degrades. In this paper, we present a hardware-based intellectual property (IP) core, namely RTOS-Guardian (RTOS-G) able to monitor the RTOS' execution in order to detect faults that corrupt the tasks' execution flow in embedded systems based on preemptive RTOS. Experimental results based on the Plasma microprocessor IP core running different test programs that exploit several RTOS resources have been developed. During test execution, the proposed system was exposed to conducted EMI according to the international standard IEC 61.000-4-29 for voltage dips, short interruptions and voltage transients on the power supply lines of electronic systems. The obtained results demonstrate that the proposed approach is able to provide higher fault coverage and reduced fault latency when compared to the native fault detection mechanisms embedded in the kernel of the RTOS.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5993805,no
RVC-based time-predictable faulty caches for safety-critical systems,"Technology and Vcc scaling lead to significant faulty bit rates in caches. Mechanisms based on disabling faulty parts show to be effective for average performance but are unacceptable in safety critical systems where worst-case execution time (WCET) estimations must be safe and tight. The Reliable Victim Cache (RVC) deals with this issue for a large fraction of the cache bits. However, replacement bits are not protected, thus keeping the probability of failure still high. This paper proposes two mechanisms to tolerate faulty bits in replacement bits and keep time-predictability by extending the RVC. Our solutions offer different tradeoffs between cost and complexity. In particular, the Extended RVC (ERVC) has low energy and area overheads while keeping complexity at a minimum. The Reliable Replacement Bits (RRB) solution has even lower overheads at the expense of some more wiring complexity.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5993806,no
Software-based control flow error detection and correction using branch triplication,"Ever Increasing use of commercial off-the-shelf (COTS) processors to reduce cost and time to market in embedded systems has brought significant challenges in error detection and recovery methods employing in such systems. This paper presents a software based control flow error detection and correction technique, so called branch TMR (BTMR), suitable for use in COTS-based embedded systems. In BTMR method, each branch instruction is triplicated and a software interrupt routine is invoked to check the correctness of the branch instruction. During the execution of a program, when a branch instruction is executed, it is compared with the second redundant branch in the interrupt routine. If a mismatch is detected, the third redundant branch instruction is considered as the error-free branch instruction; otherwise, no error has occurred. The main advantage of BTMR over previously proposed control flow checking (CFC) methods is its ability to correct CFEs as well as protection of indirect branch instructions. The BTMR method is evaluated on LEON2 embedded processor. The results show that, error correction coverage is about 96%, while memory overhead and performance overhead of BTMR is about 28% and 10%, respectively.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5993847,no
Application research on temperature WSN nodes in switchgear assemblies based on TinyOS and ZigBee,"Temperature detection can timely discover the potential faults in switchgear assemblies. Appling a Wireless Sensor Networks(WSN) in on-line monitoring system is an effective measure to realize Condition Based Maintenance(CBM) for intelligent switchgear assemblies. A design solution of a temperature wireless sensor network node with CC2430 chip based on ZigBee is presented. The thermocouple is used as main sensor unit to detect the temperature-rise of key points in switchgear assemblies in real time, and the digital temperature sensor is adopted as subsidiary sensor unit to detect the environment temperature, so that the temperature of the points at high voltage and high current can be measured effectively and accurately. TinyOS, the embedded operating system adopted widely for WSN node, is researched and transplanted into CC2430, therefore the node software can be programmed and updated remotely. On this basis, the temperature wireless sensor prototypes based on ZigBee and/or TinyOS with CC2430 are designed and developed, and tested via the WSN experimental platform which is built by us. The experimental result shows that the temperature wireless sensor node can meet the requirement of the system function, and has the features such as low power dissipation, small volume, stable performance and long lifespan etc., and can be broadly applied in on-line temperature measuring and monitoring of high-voltage and low-voltage switchgear assemblies.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5993950,no
Towards improved survivability in safety-critical systems,"Performance demand of Critical Real-Time Embedded (CRTE) systems implementing safety-related system features grows at an exponential rate. Only modern semiconductor technologies can satisfy CRTE systems performance needs efficiently. However, those technologies lead to high failure rates, thus lowering survivability of chips to unacceptable levels for CRTE systems. This paper presents SESACS architecture (Surviving Errors in SAfety-Critical Systems), a paradigm shift in the design of CRTE systems. SESACS is a new system design methodology consisting of three main components: (i) a multicore hardware/firmware platform capable of detecting and diagnosing hardware faults of any type with minimal impact on the worst-case execution time (WCET), recovering quickly from errors, and properly reconfiguring the system so that the resulting system exhibits a predictable and analyzable degradation in WCET; (ii) a set of analysis methods and tools to prove the timing correctness of the reconfigured system; and (iii) a white-box methodology and tools to prove the functional safety of the system and compliance with industry standards. This new design paradigm will deliver huge benefits to the embedded systems industry for several decades by enabling the use of more cost-effective multicore hardware platforms built on top of modern semiconductor technologies, thereby enabling higher performance, and reducing weight and power dissipation. This new paradigm will further extend the life of embedded systems, therefore, reducing warranty and early replacement costs.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5994536,no
"Right dose, right care, every time: A distributed system for quality use of drugs","In hospitals drug dosage calculation is a complex process which involves multiple steps when calculating a single dose. This multi-step process is error-prone, as it requires experience and the full attention of physicians or nurses involved. The potential for error is high and the consequences are potentially serious. Software technologies can offer much added value in ensuring that patients receive the correct dose of a drug based on their individual needs. This paper describes a distributed drug management system that resolves critical drug dosing problems.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5999030,no
A Parallel Vulnerability Detection Framework via MPI,"Open source applications have flourished recent years. Meanwhile, security vulnerabilities in such applications have grown. Since manual code auditing is error-prone, time-consuming and costly, it is necessary to find automatic solutions. To address this problem we propose an approach that combines constraint-based analysis and model checking together. Model checking technology as a constraint solver can be employed to solve the constraint-based system. CodeAuditor, the prototype implementation of our methods, is targeted at detecting vulnerabilities in C source code. With this tool, 9 previously unknown vulnerabilities in two open source applications were discovered and the observed false positive rate was at around 29%.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5999219,no
Software Testing and Verification in Climate Model Development,"Over the past 30 years, most climate models have grown from relatively simple representations of a few atmospheric processes to complex multidisciplinary systems. Computer infrastructure over that period has gone from punchcard mainframes to modern parallel clusters. Model implementations have become complex, brittle, and increasingly difficult to extend and maintain. Verification processes for model implementations rely almost exclusively on some combination of detailed analyses of output from full climate simulations and system-level regression tests. Besides being costly in terms of developer time and computing resources, these testing methodologies are limited in the types of defects they can detect, isolate, and diagnose. Mitigating these weaknesses of coarse-grained testing with finer-grained unit tests has been perceived as cumbersome and counterproductive. Recent advances in commercial software tools and methodologies have led to a renaissance of systematic fine-grained testing. This opens new possibilities for testing climate-modeling-software methodologies.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5999647,no
A dynamic workflow simulation platform,"In numeric optimization algorithms errors at application level considerably affect the performance of their execution on distributed infrastructures. Hours of execution can be lost only due to bad parameter configurations. Though current grid workflow systems have facilitated the deployment of complex scientific applications on distributed environments, the error handling mechanisms remain mostly those provided by the middleware. In this paper, we propose a collaborative platform for the execution of scientific experiments in which we integrate a new approach for treating application errors, using the dynamicity and exception handling mechanisms of the YAWL workflow management system. Thus, application errors are correctly detected and appropriate handling procedures are triggered in order to save as much as possible of the work already executed.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5999841,no
A dependable system based on adaptive monitoring and replication,"A multi agent system (MAS) has recently gained public attention as a method to solve competition and cooperation in distributed systems. However, MAS's vulnerability due to the propagation of failures prevents it from being applied to a large-scale system. This paper proposes a method to improve the reliability and efficiency of distributed systems. Specifically, the paper deals with the issue of fault tolerance. Distributed systems are characterized by a large number of agents, who interact according to complex patterns. The effects of a localized failure may spread across the whole network, depending on the structure of the interdependences between agents. The method monitors messages between agents to detect undesirable behaviors such as failures. Collecting the information, the method generates global information of interdependence between agents and expresses it in a graph. This interdependence graph enables us to detect or predict undesirable behaviors. This paper also shows that the method can optimize performance of a MAS and improve adoptively its reliability under complicated and dynamic environment by applying the global information acquired from the interdependence graph to a replication system. The advantages of the proposed method are illustrated through simulation experiments based on a virtual auction market.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5999843,no
A face super-resolution method based on illumination invariant feature,"Human faces in surveillance video images usually have low resolution and poor quality. They need to be reconstructed in super-resolution for identification. The traditional subspace-based face super-resolution algorithms are sensitive to light. For solving the problem, this paper proposes a face super-resolution method based on illumination invariant feature. The method firstly extracts the illumination invariant features of an input low resolution image by using adaptive L1-L2 total variation model and self-quotient image in logarithmic domain. Then it projects the feature onto non-negative basis obtained by Nonnegative Matrix Factorization (NMF) in face image database. Finally it reconstructs the high resolution face images under the framework of Maximum A Posteriori(MAP) probability. Experimental results demonstrate that the proposed method outperforms the compared methods both in subjective and objective quality under poor light conditions.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6001848,no
Sidescan sonar imagery processing software for underwater research,"Detailed submarine digital analysis of side scan sonar images significantly enhances the ability to assess seafloor features and artifacts digital images. These images are usually poor in their resolution if they are compared with optical images. There are commercial solutions that could solve this trouble, such as: the use of high resolution multibeam sidescan sonar, or the use of bathymetric sonar. Present work shows an economical solution to avoid this kind of problem by using digital image processing techniques under MATLAB environment. The application presented here is easy to use and has been developed under user friendly philosophy and could be operated for users at any level. Two types of sonar surveys, seafloor mapping and submerged target searches (buried or not), each require different processing methods for data analysis. Results are comparable in quality with commercial hardware solutions. Present work is the first step of a new general purpose tool that will be used in submerged objects recognition.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6003565,no
A Dynamic Fault Localization Technique with Noise Reduction for Java Programs,"Existing fault localization techniques combine various program features and similarity coefficients with the aim of precisely assessing the similarities among the dynamic spectra of these program features to predict the locations of faults. Many such techniques estimate the probability of a particular program feature causing the observed failures. They ignore the noise introduced by the other features on the same set of executions that may lead to the observed failures. In this paper, we propose both the use of chains of key basic blocks as program features and an innovative similarity coefficient that has noise reduction effect. We have implemented our proposal in a technique known as MKBC. We have empirically evaluated MKBC using three real-life medium-sized programs with real faults. The results show that MKBC outperforms Tarantula, Jaccard, SBI, and Ochiai significantly.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6004307,no
Context-Sensitive Interprocedural Defect Detection Based on a Unified Symbolic Procedure Summary Model,"Precise interprocedural analysis is crucial for defect detection faced with the problem of procedure call. Procedure summary is an effective and classical technique to handle this problem. However, there is no general recipe to construct and instantiate procedure summaries with context-sensitivity. This paper addresses the above challenge by introducing a unified symbolic procedure summary model (PSM), which consists of three aspects: (1) the post-condition briefly records the invocation side effects to calling context, (2) the feature means some inner attributes that might cause both the dataflow and control-flow transformation and (3) the pre-condition implies some potential dataflow safety properties that should not be violated at the call site, or there would exist defects. We represent each aspect of PSM in a three-valued logic: <;Conditional Constraints, Symbolic Expression, Abstract Value>;. Moreover, by comparing the concrete call site context (CSC) with the conditional constraints (CC), we achieve context-sensitivity while instantiating the summary. Furthermore, we proposed a summary transfer function for capturing the nesting call effect of a procedure, which transfers the procedure summary in a bottom-up manner. Algorithms are proposed to construct and instantiate the summary model at concrete call sites with context-sensitivity. Experimental results on 10 open source GCC benchmarks attest to the effectiveness of our technique on detecting null pointer dereference and out of boundary defects.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6004311,no
Static Detection of Bugs Caused by Incorrect Exception Handling in Java Programs,"Exception handling is a vital but often poorly tested part of a program. Static analysis can spot bugs on exceptional paths without actually making the exceptions happen. However, the traditional methods only focus on null dereferences on exceptional paths, but do not check the states of variables, which may be corrupted by exceptions. In this paper we propose a static analysis method that combines forward flow sensitive analysis and backward path feasibility analysis, to detect bugs caused by incorrect exception handling in Java programs. We found 8 bugs in three open source server applications, 6 of which cannot be found by Find Bugs. The experiments showed that our method is effective for finding bugs related to poorly handled exceptions.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6004312,no
Model-Driven Design of Performance Requirements,"Obtaining the expected performance of a workflow is much simpler if the requirements for each of its tasks are well defined. However, most of the time, not all tasks have well-defined requirements, and these must be derived by hand. This can be an error-prone and time consuming process for complex workflows. In this work, we present an algorithm which can derive a time limit for each task in a workflow, using the available task and workflow expectations. The algorithm assigns the minimum time required by each task and distributes the slack according to the weights set by the user, while checking that the task and workflow expectations are consistent with each other. The algorithm avoids having to evaluate every path in the workflow by building its results incrementally over each edge. We have implemented the algorithm in a model handling language and tested it against a naive exhaustive algorithm which evaluates all paths. Our incremental algorithm reports equivalent results in much less time than the exhaustive algorithm.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6004314,no
Towards Impact Analysis of Test Goal Prioritization on the Efficient Execution of Automatically Generated Test Suites Based on State Machines,"Test prioritization aims at reducing test execution costs. There are several approaches to prioritize test cases based on collected data of previous test runs, e.g., in regression testing. In this paper, we present a new approach to test prioritization for efficient test execution that is focused on the artifacts used in model-based test generation from state machines. We propose heuristics for test goal prioritizations and evaluate them using two different test models. Our finding is that the prioritizations can have a positive impacton the test execution efficiency. This impact, however, is hard to predict for a concrete situation. Thus, the question for the general gain of test goal prioritizations is still open.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6004322,no
Improving the Modifiability of the Architecture of Business Applications,"In the current rapidly changing business environment, organizations must keep on changing their business applications to maintain their competitive edges. Therefore, the modifiability of a business application is critical to the success of organizations. Software architecture plays an important role in ensuring a desired modifiability of business applications. However, few approaches exist to automatically assess and improve the modifiability of software architectures. Generally speaking, existing approaches rely on software architects to design software architecture based on their experience and knowledge. In this paper, we build on our prior work on automatic generation of software architectures from business processes and propose a collection of model transformation rules to automatically improve the modifiability of software architectures. We extend a set of existing product metrics to assess the modifiability impact of the proposed model transformation rules and guide the quality improvement process. Eventually, we can generate software architecture with desired modifiability from business processes. We conduct a case study to illustrate the effectiveness of our transformation rules.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6004325,no
A Hierarchical Security Assessment Model for Object-Oriented Programs,"We present a hierarchical model for assessing an object-oriented program's security. Security is quantified using structural properties of the program code to identify the ways in which `classified' data values may be transferred between objects. The model begins with a set of low-level security metrics based on traditional design characteristics of object-oriented classes, such as data encapsulation, cohesion and coupling. These metrics are then used to characterise higher-level properties concerning the overall readability and writ ability of classified data throughout the program. In turn, these metrics are then mapped to well-known security design principles such as `assigning the least privilege' and `reducing the size of the attack surface'. Finally, the entire program's security is summarised as a single security index value. These metrics allow different versions of the same program, or different programs intended to perform the same task, to be compared for their relative security at a number of different abstraction levels. The model is validated via an experiment involving five open source Java programs, using a static analysis tool we have developed to automatically extract the security metrics from compiled Java byte code.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6004330,no
DRiVeR: Diagnosing Runtime Property Violations Based on Dependency Rules,"To ensure the reliability of complex software systems, runtime software monitoring is widely accepted to monitor and check system execution against formal properties specification at runtime. Runtime software monitoring can detect property violations, however it can not explain why a violation has occurred. Diagnosing runtime property violations is still a challenge issue. In this paper, a novel diagnosis method based on dependency rules is constructed to diagnose runtime property violations in complex software systems. A set of rules is formally defined to isolate software fault from hardware fault, then software faults is localized by combining trace slicing and dicing. The method is implemented in the runtime software monitoring system SRMS, and experimental results demonstrate that the method can effectively isolate and locate the related faults with property violations.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6004476,no
Static Data Race Detection for Interrupt-Driven Embedded Software,"Interrupt mechanisms are widely used to process multiple concurrent tasks in the software without OS abstraction layer in various cyber physical systems (CPSs), such as space flight control systems. Data races caused by interrupt preemption frequently occur in those systems, leading to unexpected results or even severe system failures. In recent Chinese space projects, many software defects related to data races have been reported. How to detect interrupt based data races is an important issue in the quality assurance for aerospace software. In this paper, we propose a tool named Race Checker that can statically detect data races for interrupt-driven software. Given the source code or binary code of interrupt-driven software, the tool aggressively infers information such as interrupts priority states, interrupt enable states and memory accesses at each program point using our extended interprocedural data flow analysis. With the information above, it identifies the suspicious program points that may lead to data races. Race Checker is explicitly designed to find data race bugs in real-life aerospace software. Up to now, the tool has been applied in aerospace software V&V and found several severe data race bugs that may lead to system failures.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6004502,no
Multi-layered Adaptive Monitoring in Service Robots,"Service failure in a service robot is an event that occurs when the delivered service deviates from the correct original service specified by the developers. The cause of failures is due to faults in the robot system, which can be detected based on a model. However, the monitoring task that compares the model and system's behavior is overload. In this study, we propose a multi-layered adaptive monitoring method that complements model-based fault detection. When the target component can be monitored according to their priority adaptively, it results in keeping the efficiency of fault detection, while the overload is reduced.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6004506,no
A Reliability Model for Complex Systems,"A model of software complexity and reliability is developed. It uses an evolutionary process to transition from one software system to the next, while complexity metrics are used to predict the reliability for each system. Our approach is experimental, using data pertinent to the NASA satellite systems application environment. We do not use sophisticated mathematical models that may have little relevance for the application environment. Rather, we tailor our approach to the software characteristics of the software to yield important defect-related predictors of quality. Systems are tested until the software passes defect presence criteria and is released. Testing criteria are based on defect count, defect density, and testing efficiency predictions exceeding specified thresholds. In addition, another type of testing efficiency-a directed graph representing the complexity of the software and defects embedded in the code-is used to evaluate the efficiency of defect detection in NASA satellite system software. Complexity metrics were found to be good predictors of defects and testing efficiency in this evolutionary process.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6004508,no
A Methodology of Model-Based Testing for AADL Flow Latency in CPS,"AADL (Architecture Analysis and Design Language) is a kind of model-based real-time CPS (Cyber-Physical System) modeling language, which has been widely used in avionics and space areas. The current challenges have been raised up on how to test CPS model described in AADL dynamically and find design fault at the design phase to iterate and refine the model architecture. This paper mainly tests the flow latency in design model based on PDA (Push-Down Automata). It abstracts the properties of flow latency in CPS model, and translates them into PDA in order to assess the latency in simulation. Meanwhile, this paper presents a case study of pilotless aircraft cruise control system to prove the feasibility of dynamic model-based testing on model performances and achieve the architecture iteration and refining aim.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6004510,no
Integrating DSL-CBI and NuSMV for Modeling and Verifiying Interlocking Systems,"The Computer Based Interlocking System (CBI) is used to ensure safe train movements at a railway station. For a given station, all the train routes and the concrete safety rules associated with these are defined in the interlocking table. Currently, the development and verification of interlocking tables is entirely manual process, which is inefficient and error-prone due to the complexity of the CBI and the human interferences. Besides, the complexity and volume of the verification results tend to make users feel extremely non-understandable. In order to tackle these problems, we introduce a toolset based on Domain Specific Language for Computer Based Interlocking Systems (DSL-CBI) to automatically generate and verify the interlocking table, and then mark the conflicting routes in the railway station. In this paper, we also discuss the advantages of the toolset and the significant contribution in developing CBI based on the proposed toolset.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6004515,no
Towards Synthesizing Realistic Workload Traces for Studying the Hadoop Ecosystem,"Designing cloud computing setups is a challenging task. It involves understanding the impact of a plethora of parameters ranging from cluster configuration, partitioning, networking characteristics, and the targeted applications' behavior. The design space, and the scale of the clusters, make it cumbersome and error-prone to test different cluster configurations using real setups. Thus, the community is increasingly relying on simulations and models of cloud setups to infer system behavior and the impact of design choices. The accuracy of the results from such approaches depends on the accuracy and realistic nature of the workload traces employed. Unfortunately, few cloud workload traces are available (in the public domain). In this paper, we present the key steps towards analyzing the traces that have been made public, e.g., from Google, and inferring lessons that can be used to design realistic cloud workloads as well as enable thorough quantitative studies of Hadoop design. Moreover, we leverage the lessons learned from the traces to undertake two case studies: (i) Evaluating Hadoop job schedulers, and (ii) Quantifying the impact of shared storage on Hadoop system performance.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6005384,no
Enhancing the Effectiveness of Usability Evaluation by Automated Heuristic Evaluation System,"Usability defects test escapee can have a negative impact on the success of software. It is quite common for projects to have a tight timeline. For these projects, it is crucial to ensure there are effective processes in place. One way to ensure project success is to improve the manual processes of the usability inspection via automation. An automated usability tool will enable the evaluator to reduce manual processes and focus on capturing more defects in a shorter period of time. Thus improving the effectiveness of the usability inspection and minimizing defects escapee. There exist many usability testing and inspection methods. The scope of this paper is on the Heuristic Evaluation (HE) procedures automation. The Usability Management System (UMS) was developed to automate as many manual steps as possible throughout the software development life cycle (SDLC). It is important for the various teams within the organization to understand the benefits of automation. The results show that with the help of automation more usability defects can be detected. Hence, enhancing the effectiveness of usability evaluation by an automated Heuristic Evaluation System is feasible.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6005653,no
Transient Fault Representativenesses Comparison Analysis,"As semiconductor technology scales into deep submicron regime, transient fault vulnerability of both combinational logic and sequential logic increase rapidly. It is predicted that in 2011 transient fault rate of combinational logic will overtake that of sequential logic in processor. In this paper, particle radiation-induced multi-bit transient faults in decoder unit, representative combinational logic components in SPARC processor, are simulated under different fault injection method, namely simulation-based fault injection and compilation supported static fault injection respectively. Fault representativenesses observed in both fault injection experiments are analyzed, and the inaccuracy factors of the static error injection method are put forward.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6006140,no
Design patterns and fault-proneness a study of commercial C# software,"In this paper, we document a study of design patterns in commercial, proprietary software and determine whether design pattern participants (i.e. the constituent classes of a pattern) had a greater propensity for faults than non-participants. We studied a commercial software system for a 24 month period and identified design pattern participants by inspecting the design documentation and source code; we also extracted fault data for the same period to determine whether those participant classes were more fault-prone than non-participant classes. Results showed that design pattern participant classes were marginally more fault-prone than non-participant classes, The Adaptor, Method and Singleton patterns were found to be the most fault-prone of thirteen patterns explored. However, the primary reason for this fault-proneness was the propensity of design classes to be changed more often than non-design pattern classes.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6006827,no
The Case for Software Health Management,"Software Health Management (SWHM) is a new field that is concerned with the development of tools and technologies to enable automated detection, diagnosis, prediction, and mitigation of adverse events due to software anomalies. Significant effort has been expended in the last several decades in the development of verification and validation (V&V) methods for software intensive systems, but it is becoming increasingly more apparent that this is not enough to guarantee that a complex software system meets all safety and reliability requirements. Modern software systems can exhibit a variety of failure modes which can go undetected in a verification and validation process. While standard techniques for error handling, fault detection and isolation can have significant benefits for many systems, it is becoming increasingly evident that new technologies and methods are necessary for the development of techniques to detect, diagnose, predict, and then mitigate the adverse events due to software that has already undergone significant verification and validation procedures. These software faults often arise due to the interaction between the software and the operating environment. Unanticipated environmental changes lead to software anomalies that may have significant impact on the overall success of the mission. Because software is ubiquitous, it is not sufficient that errors are detected only after they occur. Rather, software must be instrumented and monitored for failures before they happen. This prognostic capability will yield safer and more dependable systems for the future. This paper addresses the motivation, needs, and requirements of software health management as a new discipline.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6007769,no
System-Software Co-Engineering: Dependability and Safety Perspective,"The need for an integrated system-software co-engineering framework to support the design of modern space systems is pressing. The current tools and formalisms tend to be tailored to specific analysis techniques and are not amenable for the full spectrum of required system aspects such as safety, dependability and performability. Additionally, they cannot handle the intertwining of hardware and software interaction. As such, the current practices lack integration and coherence. We recently developed a coherent and multidisciplinary approach towards developing space systems at architectural design level, linking all of the aforementioned aspects, and assessed it with several industrial evaluations. This paper reports on the approach, the evaluations and our perspective on current and future developments.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6007771,no
Integrated Software and Sensor Health Management for Small Spacecraft,"Despite their size, small spacecraft have highly complex architectures with many sensors and computer-controlled actuators. At the same time, size, weight, and budget constraints often dictate that small spacecraft are designed as single-string systems, which means that there are no or few redundant systems. Thus, all components, including software, must operate as reliably. Faults, if present, must be detected as early as possible to enable (usually limited) forms of mitigation. Telemetry bandwidth for such spacecraft is usually very limited. Therefore, fault detection and diagnosis must be performed on-board. Further restrictions include low computational power and small memory. In this paper, we discuss the use of Bayesian networks (BNs) to monitor the health of on-board software and sensor systems, and to perform advanced on-board diagnostic reasoning. Advanced compilation techniques are used to obtain a compact SSHM (Software and Sensor Health Management) system with a powerful reasoning engine, which can run in an embedded software environment and is amenable to V&V. We successfully demonstrate our approach using an OSEK-compliant operating system kernel, and discuss in detail several nominal and fault scenarios for a small satellite simulation with a simple bang-bang controller.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6007778,no
Designing of expert system for troubleshooting diagnosis on Gas Chromatography GC-2010 by means of inference method,"Gas Chromatography (GC) is used to analyze various products of volatile materials such natural gas, oils, pharmaceuticals, and foods. It is needed to control the quality of products till fulfill its requirements to be marketable. Therefore, maintenance is required to maintain the GC system running well. The expert system software for watering down the troubleshooting diagnosis on GC-2010 had been done by means of inference method. Based on the result of analysis, the probability of damage which often occurred are fluctuating pressure and high noise. The result of simulation show that the total time effectiveness using expert system software is 51.3% and the average step effectiveness to solve a problem in GC system is 21.5%.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6007820,no
Preparation for strustural path analysis from plain text input output matrix,"We are dealing with a matrix of the size 136 (formerly 79) known as financial social accounting matrix (FSAM). Inside a financial SAM is a square matrix of 107 rows times 107 columns sub set known as SAM. We are augmenting a command line based software for more functionality. The addition from us includes: data base for the output, bar chart output, table generation, graphical presentation of path. Users want more versatile input not just single proprietaty plain text format. We take over preparing tedious input such as : moving exogenous rows to bottom side of matrix, error prone command line typing, filling blanks with zeroes, serial numbering of rows, separation of numbers and names. This is presented graphically with a model called use case.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6007846,no
Modeling and simulation of quality analysis of internal cracks based on FTA in continuos casting billet,"To improve the internal quality of the continuous casting billet, one new approach for quality analysis and optimum control is proposed based on fault tree analysis (FTA). The interrelation between different hypotheses of the formation mechanism of internal cracks is studied to find out the root causes. According to the features of FTA and expertise, a FTA model of internal cracks is developed, then the software VC++6.0 is introduced to form the visual-FTA system, which is available to generate and analyze the fault tree automatically. Applying the analysis system to the steel Q235 supplied by a steel-maker, the qualitative analysis/quantitative results basically conform to the on-line statistical ones, and then an optimum control scheme is proposed to improve the billet quality.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6008033,no
Test control&communication of hierarchical design-for-testability for testing dynamically reconfigurable computer,"The dynamically reconfigurable computer is a novel type of computers. Test control&communication of hierarchical Design-For-Testability for testing dynamically reconfigurable computer is presented in this paper. It can be used to detect both faults in the circuits of test targets and the test controller itself. Using this method, the fault in the test target can be detected in time, and the test result can be reported to the user immediately. Adopting hierarchical Design-For-Testability technique, the safety and stability of the system are enhanced, and the test result becomes more dependable.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6008224,no
SkelCL - A Portable Skeleton Library for High-Level GPU Programming,"While CUDA and OpenCL made general-purpose programming for Graphics Processing Units (GPU) popular, using these programming approaches remains complex and error-prone because they lack high-level abstractions. The especially challenging systems with multiple GPU are not addressed at all by these low-level programming models. We propose SkelCL - a library providing so-called algorithmic skeletons that capture recurring patterns of parallel computation and communication, together with an abstract vector data type and constructs for specifying data distribution. We demonstrate that SkelCL greatly simplifies programming GPU systems. We report the competitive performance results of SkelCL using both a simple Mandelbrot set computation and an industrial-strength medical imaging application. Because the library is implemented using OpenCL, it is portable across GPU hardware of different vendors.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6008967,no
Detection and Correction of Silent Data Corruption for Large-Scale High-Performance Computing,"Faults have become the norm rather than the exception for high-end computing on clusters with 10s/100s of thousands of cores, and this situation will only become more dire as we reach exascale computing. Exacerbating this situation, some of these faults will not be detected, manifesting themselves as silent errors that will corrupt memory while applications continue to operate but report incorrect results. This paper introduces RedMPI, an MPI library residing in the profiling layer of any standards-compliant MPI implementation. RedMPI is capable of both online detection and correction of soft errors that occur in MPI applications without requiring code changes to application source code. By providing redundancy, RedMPI is capable of transparently detecting corrupt messages from MPI processes that become faulted during execution. Furthermore, with triple redundancy RedMPI """"votes'' out MPI messages of a faulted process by replacing corrupted results with corrected results from unfaulted processes. We present an evaluation of RedMPI on an assortment of applications to demonstrate the effectiveness and assess associated overheads. Fault injection experiments establish that RedMPI is not only capable of successfully detecting injected faults, but can also correct these faults while carrying a corrupted application to successful completion without propagating invalid data.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6009019,no
Capability Diagnostics of Enterprise Service Architectures Using a Dedicated Software Architecture Reference Model,"The SOA Innovation Lab - an innovation network of industry leaders in Germany and Europe - investigates the practical use of vendor platforms in a service-oriented context. For this purpose an original service-oriented ESA-Enterprise-Software-Architecture-Reference-Model, an associated ESA-Architecture-Maturity-Framework and an ESA-Pattern-Language for supporting architecture evaluation and optimization has been researched, leveraging and extending CMMI and TOGAF, as well as other service-oriented state-of-the art frameworks and methods. Current approaches for assessing architecture quality and maturity of service-oriented enterprise software architectures are rarely validated and were intuitively developed, having sparse reference model, metamodel or pattern foundations. This is a real problem because enterprise and software architects should know how advanced architecture quality concepts can successfully be used and how a stable foundation for introducing service-oriented enterprise architectures for adaptive systems looks like. Our idea and contribution is to extend existing enterprise and software architecture reference models and maturity frameworks to accord with a sound metamodel approach. We have developed and are presenting the idea of a pattern language for assessing the architecture quality of adaptable service-oriented enterprise systems. Our approach is based on ESARC -- an Enterprise Software Architecture Reference Model we have developed.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6009311,no
Predicting Software Service Availability: Towards a Runtime Monitoring Approach,"This paper presents a prediction model for software services availability measured by the mean-time-to-repair (MTTR) and mean-time-to-failure (MTTF) of a service. The prediction model is based on the experimental identification of probabilistic prediction for variables that affect MTTR/MTTF, based on monitoring service data collected at runtime.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6009418,no
Progressive Reliability Forecasting of Service-Oriented Software,"Reliability is an essential quality requirement for service-oriented systems. A number of models have been developed for predicting reliability of traditional software, in which code-based defects are the main concern for the causes of failures. Service-oriented software, however, shares many common characteristics with distributed systems and web applications. In addition to residual defects, the reliabilities of these types of systems can be affected by their execution context, message transmission media, and their usages. We present a case study to demonstrate that the reliability of a service varies on an hourly basis, and reliability forecasts should be recalibrated accordingly. In this study, the failure behavior of a required external service, used by a provided service, was monitored for two months to compute the initial estimates, which then continuously re-computed based on the learning of the new failure patterns. These reliabilities are integrated with the reliability of the component in the provided service. The results show that with this progressive re-calibration we provide more accurate reliability forecasts for the service.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6009434,no
Reputation-Driven Web Service Selection Based on Collaboration Network,"Most of trustworthy web service selection simply focus on individual reputation and ignore the collaboration reputation between services. To enhance the collaboration trust during web service selection, a reputation model called collaboration reputation is proposed. The reputation model is built on web service collaboration network(WSCN), which is constructed in terms of the composite service execution log. Thus, the WSCN aims to maintain the trustworthy collaboration alliance among web services, In WSCN, the collaboration reputation can be assessed by two metrics, one called invoking reputation is computed by recommendation, which is selected from the community structure hiding in WSCN, the other is assessed by the invoked web service. In addition, the web service selection based on WSCN is designed.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6009461,no
Measuring robustness of Feature Selection techniques on software engineering datasets,"Feature Selection is a process which identifies irrelevant and redundant features from a high-dimensional dataset (that is, a dataset with many features), and removes these before further analysis is performed. Recently, the robustness (e.g., stability) of feature selection techniques has been studied, to examine the sensitivity of these techniques to changes in their input data. In this study, we investigate the robustness of six commonly used feature selection techniques as the magnitude of change to the datasets and the size of the selected feature subsets are varied. All experiments were conducted on 16 datasets from three real-world software projects. The experimental results demonstrate that Gain Ratio shows the least stability on average while two different versions of ReliefF show the most stability. Results also show that making smaller changes to the datasets has less impact on the stability of feature ranking techniques applied to those datasets.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6009565,no
Application of quantitative fault tree analysis based on fuzzy synthetic evaluation to software development for space camera,"Software reliability analysis is a significant measure to guide design of software reliability. Now software fault tree method to carry on software reliability analysis is used more and more. But it can only qualitative carry on the analysis because of many reasons. For example, the scale of software is enlarging, the software architecture is getting more and more complex, the reason of software fault is very difficult to analyze, the software is difference with hardware, and it is very hard to obtain the expiration the quantitative data and so on. The space camera is extremely high to the reliable request. So the qualitative analysis is unable to satisfy the analysis request. In view of space camera characteristics, a method of fault tree analysis based on fuzzy synthetic evaluation is proposed. The results show that the new method makes software quantitative fault tree analysis possible and optimizes. It can also solve the problem of software component which is unable to quantize precisely.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6010596,no
Fast mode decision algorithm for enhancement layer of spatial and CGS scalable video coding,"The scalable video coding is an effective solution to fulfil the different requirements in modern video transmission system. Though the coding efficiency is high, the computational complexity is expensive. This paper presents a proposed algorithm to reduce the encoding time of enhancement layer through probability analysis. Firstly two classes are defined, and the Bayes Classifier decides whether the current macroblock belongs to Big Block class or Small Block class. Then an early termination strategy is checked for ignoring some inter modes. When inter-layer residual prediction is used, the optimal inter mode is set as the best inter mode of co-located macroblock in base layer. For the intra modes, the INTRA_1616 is skipped based on statistical data. Experiment results show that the proposed algorithm reduces computational complexity with negligible video quality loss and bit rate increment when compared with reference software and other methods.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6011846,no
Negotiation towards Service Level Agreements: A Life Cycle Based Approach,"Service Based Systems (SBSs) are composed of loosely coupled services. Different stakeholders in these systems, e.g. service providers, service consumers, and business decision makers, have different types of concerns which may be dissimilar or inconsistent. Service Level Agreements (SLAs) play a major role in ensuring the quality of SBSs. They stipulate the availability, reliability, and quality levels required for an effective interaction between service providers and consumers. It has been noticed that because of having conflicting priorities and concerns, conflicts arise between service providers and service consumers while negotiating over the functionality of potential services. Since these stakeholders are involved with different phases the life cycle, it is really important to take into consideration these life cycle phases for proposing any kind of SLA negotiation methodology. In this research, we propose a stakeholder negotiation strategy for Service Level Agreements, which is based on prioritizing stakeholder concerns based on their frequency at each phase of the SBS development life cycle. We make use of a Collaxa BPEL Orchestration Server Loan service example to demonstrate the applicability of the proposed approach. In addition, we simulate the negotiation priority values to predict their potential impact on the cost of the SLA negotiation.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6012679,no
Automatic Library Generation for BLAS3 on GPUs,"High-performance libraries, the performance-critical building blocks for high-level applications, will assume greater importance on modern processors as they become more complex and diverse. However, automatic library generators are still immature, forcing library developers to manually tune library to meet their performance objectives. We are developing a new script-controlled compilation framework to help domain experts reduce much of the tedious and error-prone nature of manual tuning, by enabling them to leverage their expertise and reuse past optimization experiences. We focus on demonstrating improved performance and productivity obtained through using our framework to tune BLAS3 routines on three GPU platforms: up to 5.4x speedups over the CUBLAS achieved on NVIDIA GeForce 9800, 2.8x on GTX285, and 3.4x on Fermi Tesla C2050. Our results highlight the potential benefits of exploiting domain expertise and the relations between different routines (in terms of their algorithms and data structures).",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6012842,no
Research on available resource management model of cognitive networks based on intelligence agent,"A research on available resources management model (ARMM) of cognitive networks based on intelligence agent (IA) proposed in this paper. In cognitive networks (CNs) environment, it is important to master various resources information of network accurately, in the guarantee resources information completeness, the uniformity, the stability, above the reliable foundation, overall evaluation user quality of service (QoS) and the network condition, monitor the use of resources have been allocated, check the amount of resource allocation, calculate the remaining resources, then carry on the independent management to the network resource. It not only can record the system from the use of current resources, but can predict the future network according to the remaining resources and redistribute resources to address inefficient allocation of network resources defects and improve resource utilization, enable the centralization of the network resources, intelligent, visual management, also meet the cognitive networks requirements of the various functions of resources. The ARMM of cognitive networks based on intelligence agent can be deployed in the context of CNs as well as current network environment. It could enhance the QoS of network and users because of its cognitive functions.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6013575,no
RSSI vector attack detection method for wireless sensor networks,"In contrast to traditional networks, Wireless Sensor networks (WSN) are more vulnerable to attacks such as DOS, eavesdropping, tampering, node compromise, wormhole and Sibyl. To give security protection for WSN, many attack prevention and detection methods were proposed. As the second line of defence, Intrusion Detection can resist attacks under cryptography technologies are unavailable, and provide strategies for network recovery. This paper proposed a novel attack detection based on RSSI technology. It collects multi-observed RSSI values to form vectors, and uses mean vector confidential interval examination of multivariate normal population to detect malicious packets. Real experiments showed that vector-based detection approach have a better detection sensitivity and fault tolerance than single value detection approach.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6013581,no
Efficient testing model based on Pareto distribution,"Regression testing is a very important part of the program development cycle as it should be executed after feature is integrated or bugs are fixed each time. There are hundreds of test cases in the case library for a little complex product, it is impossible to perform all of the test cases in regression testing every time. Test engineers always select the regression cases by their experience only, most of the cases are executed for several times but cannot find any issues, and many of the defects are escaped from test engineers because of the existing cases have not been selected to run. Pareto distribution model is introduced to assist test engineers in selecting efficient regression cases those are expected to find most of the defects at the incipient stage of testing according to quantitative efficiency of the test cases. And this model helps test engineers to forecast the progress of fault finding also.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6013601,no
The application of artificial neural network in wastewater treatment,"In this paper, the applications of artificial neural network(ANN) in wastewater treatment quality control are reviewed. The main applications include three fields: to predict the effluent quality, such as the concentration of COD, N and P; to soft sensing some parameters hard to measure on site, such as BOD; to measure some parameters more accurately, such as heavy metals. In combination with the developing trend of the researches, the future developing direction of ANN is analyzed.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6013606,no
PAD: Policy Aware Data center network,"Middle Boxes serve for the security in Data Center Networks (DCNs). Together with the growth of the services and applications in DCNs, flexible and scalable middle box deployment is highly required. The current middle box deployment methods are error prone. In this paper we propose Policy Aware Data center network (PAD), a flexible and scalable middle box provisioning architecture. PAD supports traditional Ethernet DCNs based on IP protocol. It allows DCN users freely define their traversal sequences of middle boxes without any complex configurations. PAD also makes DCN topology changing more easily, so VM migration, network expanding and so on can be easily implemented in PAD. PAD uses Policy Routing Information Injection (PRII) to control the middle boxes traversal, and simulation shows that PRII will bring no more than 6% throughput loss in practical utilization.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6013623,no
A public cryptosystem from R-LWE,"Recently Vadim Lyubashevsky etc. built LWE problem on ring and proposed a public cryptosystem based on R-LWE, which, to a certain extent, solved the defect of large public key of this kind, but it didn't offer parameter selections and performance analysis in detail. In this paper an improved scheme is proposed by sharing a ring polynomial vector that makes public key as small as 1/m of the original scheme in multi-user environments. In additions, we introduce a parameter r to control both the private key space size and decryption errors probability, which greatly enhances the flexibly and practicality. The correctness, security and efficiency are analyzed in detail and choice of parameters is studied, at last concrete parameters are recommended for the new scheme.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6013644,no
Verification and validation of UML 2.0 sequence diagrams using colored Petri nets,"One of the major challenges in the software development process is the improvement of the error detection in the early phases of the software life cycle. If the software error is detected at the design phase before of the implementation, the software quality will acceptably be increased. For this purpose, the Verification and Validation of UML diagrams play a very important role in detecting flaws at the design phase. This paper presents a Verification and Validation technique for one of the most popular UML diagrams: sequence diagrams. The proposed approach creates an executable model from UML interactions expressed in sequence diagrams using colored petri nets and uses CPN Tools to simulate the execution and to verify properties written in standard ML. In The proposed approach, we have used the sequence diagram elements including massages, send/receive events and source/destination of messages and have written properties in terms of boolean expression over the elements. The main contribution of this work is to provide an efficient mechanism to be able to track the execution state of an interaction in sequence diagram. The obtained results show that The proposed approach reduces impressively the probability of errors appearance at the software implementation phase. therefore, sofware can be more reliable at the end of the software development process.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6013675,no
Online course quality maturity model based on evening university and correspondence education(OCQMM),"The contradictions between the working times and the learning time of the adult in evening university and correspondence education have become more and more serious. Anecdotal evidence suggests that the traditional educations (There are no online courses) are not able to ease the contradiction. It has been found that the online course can solve the conflict between working and learning of the adult students in evening university and correspondence education. Therefore, many academies which engaged in adult education brought in online courses model. However, how to ensure implementary quality of online course that has become a vital problem, in the paper, Online Course Quality Maturity Model Based on Evening University and Correspondence Education(OCQMM) was built. This model is not only for assessing the implementary quality of online courses in Evening University and Correspondence Education, more importantly, it can guide the institutions that engaged in adult education to meliorate the implementary process, so that the implementation quality of online course will be improved.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6013763,no
Application of virtual instrument techonoly in electric courses teaching,"This paper analyzed the problems existed in practice teaching process of electric courses and put forward a new view which is to apply virtual instrument (VI) technology in theses courses. On the basis of a simple instruction for labVIEW's VI design function, an instance which designed by labVIEW to emulate and detect harmonic signals was described in detail. The instance has been used in the course of power quality and obtained a good result. More and more instances were designed based VI and used in electric courses, it can promote the combination between theory and practice and improve teaching level of these courses.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6013887,no
Localized approach to distributed QoS routing with a bandwidth guarantee,"Localized Quality of Service (QoS) routing has been recently proposed as a promising alternative to the currently deployed global routing schemes. In localized routing schemes, routing decisions are taken solely based on locally collected statistical information rather than global state information. This approach significantly reduces the overheads associated with maintaining global state information at each node, which in turn improves the overall routing performance. In this paper we introduce a Localized Distributed QoS Routing algorithm (LDR), which integrates the localized routing scheme into distributed routing. We compare the performance of our algorithm against other existing localized routing algorithm, namely Credit Based Routing (CBR); and against the contemporary global routing algorithm, the Widest Shortest Path (WSP). With the aid of simulations, we show that the proposed algorithm outperforms the others in terms of the overall network blocking probability.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014049,no
Performance comparison of Multiple Description Coding and Scalable video coding,"For a video server, providing a good quality of service to highly diversified users is a challenging task because different users have different link conditions and different requirement of quality and demand. Multiple Description Coding (MDC) and Scalable video coding (SVC) are the two technical methods for quality adaptation to operate over a wide range of quality of service in heterogeneous requirements. Both are techniques of coding a video sequence in a way that multiple levels of quality can be obtained depending on the parts of the video bit stream that are received. For Scalable video coding special protection is mad for the base layer using Forward Error Protection while the streams (descriptions) in multiple description coding has been tested to simulate the advantages of diversity systems where each sub-stream has an equal probability of being correctly decoded. In this paper, the performance comparison of the two coding approaches is made by using DCT coefficients in generating the base layer and enhancement Layers for SVC and Descriptions for MDC with respect to their perspective achievements in image quality and Compression Ratio. Simulation results show that MDC out performs SVC in both cases.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014090,no
Statistical prediction modeling for software development process performance,"With the advent of the information age and more intense competitions among IT companies, it is more important to assess the quality of software development processes and products by not only measuring outcomes but also predicting outcomes. On the basis of analysis and experiments about software development processes and products, a study of the process performance modeling has been conducted with statistical analyzing past and current process performance data. In this paper, we present a statistical prediction modeling for Software Development Process Performance. For predicting delivered defects effectively, a simple case study is illustrated, and several suggestions on planning and controlling management brought from this model are analyzed in detail. At the end, the conclusions with a discussion of future research consideration are pointed out in this paper.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014187,no
Applications of MODFLOW in quantitative analysis coal seam floor water inrush condition,"In north of China, the main coal mine underlying the thin Taiyuan group limestone aquifers and the thick ordovician limestone aquifers, ordovician limestone aquifers as the main supply aquifer which with high pressure, rich of dynamic and static water storage, and the vertical water-filled geological structure(such as fault zone, karst column, etc.) as the main supply channel. and take an application case, using simulation software Visual MODFLOW conducted a three-dimensional(3D) numerical model to simulate the limestone aquifers and vertical geological structures. after calibration the model, obtained some quantitative cognition about the mining hydro-geological conditions in the perspective of groundwater systems theory. finally, and predicted the pumping rate in the different mining levels which be controlled by the vertical water-filled geological structure, for example, in  0m level: the pumping rate is only 145.83 m3/h when there were no vertical water-filled geological structure (scenario 1), and the pumping rate should reach to 416 m3/h when the vertical geological structure conducting limestone aquifers water to the mining levels (scenario 2). in -150 level: the scenario 2 pumping rate more than 1200 m3/h.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014189,no
Automatic testing framework for Internet applications,"The methods and systems for testing Internet applications are extremely hot areas in current mobile device development. However, the current test methods for Internet application (mainly by manual) are inefficient and error-prone. An automatic testing framework at the presentation layer is introduced for the Internet applications, as well as the implementation of it for the internet application Data Synchronization. By separating the presentation layer, the general testing framework could thoroughly test the internet application's presentation layer automatically and exactly reconstruct the scene to activate the bug of the applications. Using the testing framework, the cycle of the presentation stack development is dramatically accelerated.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014212,no
High-precision detection device of motor speed sensor based image recognition research,"A device is developed in this paper, which is used to detect whether the motor speed sensor meets the technical specifications, and has the ability of real-time displaying the current actual speed and automatic centering. In order to achieve the high-precision automatic centering of the device, image recognition technology is used in the device, and using ARM and servo control system to detect, recognize, high precision position control and high speed completion. It provides a strong protection for the sensor of speed to enter the market, gives a strong technical basis for the motor manufacturing and motor efficiency, and offers a reliable technical support and quality supervision for the measurement of industry and quality inspection departments in China.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014273,no
Underground drainage system based on artificial intelligence control,"By analyzing common artificial intelligence technologies, the paper introduced design of software and structure of a drainage system of coal mine underground based on artificial intelligence control. By detecting water level in sump and other parameters, the system controls pumps to work by turns and standby pump to start at duly time, so as to schedule dumps run reasonably, and it has function of fault alarming, which decreases labor intensity of workers greatly and improves utilization rate of devices. The system also has good expansibility and is fit for different fields.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014364,no
An improved credibility method based on matrix weight,"While in many fields the credibility method has obtained great successful applications, but there are still many shortcomings. This paper presents a credibility method based on matrix weights analysis, an improvement focusing on credibility method based on Euclidean distance analysis within the normal distribution. It not only solves the contradictory conclusion due to the too close numerical value between the prior probability and posterior probability, but also reduces the negative effects from non-important evidence, which better reflects the positive effect of important evidence, eliminating the defects of traditional credibility methods in these two aspects.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014762,no
Set diagnosis on the power station's devices by fuzzy neural network,"Current situation for energy system which is consist from thermo power station faced on diagnosis of generator fault set without break down maintenance. In ageing thermo power plants, large turbo generator's retrofits or main drive to upgrade was poor reliability associated with increase maintenance cost. Introduction of higher competition in the power market and subsequent introduction of new environmental constraints in our energy system, installed plants life time extension with overall performance improvement through upgrade or retrofit of main components is today a valuable. Also, set fault diagnosis during life time and predictive maintenance can be defined as collecting information from machines as they operate to aid in making decisions about their health, repair and possible improvements in order to reach maximum reliability, before any unplanned break down. As the turbo-generator fault set occurs when sensors should be put on bearing of these to detect vibration signal for extracting fault symptoms, but the relationships between faults and fault symptoms are too complex to get enough accuracy for industry application. In this paper, a new diagnosis method based on fuzzy neural network is proposed and a fuzzy neural network system is structured by associating the fuzzy set theory with neural network technology.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014767,no
R-largest order statistics for the prediction of bursts and serious deteriorations in network traffic,"Predicting bursts and serious deteriorations in Internet traffic is important. It enables service providers and users to define robust quality of service metrics to be negotiated in service level agreements (SLA). Traffic exhibits the heavy tail property for which extreme value theory is the perfect setting for the analysis and modeling. Traditionally, methods from EVT, such as block maxima and peaks over threshold were applied, each treating a different aspect of the prediction problem. In this work, the r-largest order statistics method is applied to the problem. This method is an improvement over the block maxima method and makes more efficient use of the available data by selecting the r largest values from each block to model. As expected, the quality of estimation increased with the use of this method; however, the fit diagnostics cast some doubt about the applicability of the model, possibly due to the dependence structure in the data.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014960,no
Project Management Methodologies: Are they sufficient to develop quality software,"This paper considers whether the use of a project management methodology PRINCE is sufficient to achieve quality information systems. Without the additional use of effort estimation methods and the user of measure that can predict and help control quality during system development. How to make sure the management of software quality to be effective is a critical issue that software development organizations have to face. Software quality management is a series of activities to direct and control the software quality, includes establishment of the quality policy and quality goals, quality planning, quality control, quality assurance and quality improvement. Professional bodies have paid more attention to software standards. Meanwhile, many countries are participating in significant consolidation and coordination effort.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6015648,no
Automated generation of FRU devices inventory records for xTCA devices,"The Advanced Telecommunications Computing Architecture (ATCA) and Micro Telecommunications Computing Architecture (TCA) standards, intended for high-performance applications, offer an array of features that are compelling from the industry use perspective, like high reliability (99,999%) or hot-swap support. The standards incorporate the Intelligent Platform Management Interface (IPMI) for the purpose o advanced diagnostics and operation control. This standard imposes support for non-volatile Field Replaceable Unit (FRU) information for specific components of an ATCA/TCA-based system, which would typically include description of a given component. The Electronic Keying (EK) mechanism is capable of using this information for ensuring more reliable cooperation of the components. The FRU Information for the ATCA/TCA implementation elements may be of sophisticated structure. This paper focuses on a software tool facilitating the process of assembling this information, the goal of which is to make it more effective and less error-prone.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6016055,no
"Virtual instrument based online monitoring, real-time detecting and automatic diagnosis management system for multi-fiber lines","Along with the popularity of the optical fiber communication, virtual instrument based online monitoring real-time detecting and automatic diagnosis management system on multi-fiber is proposed in this paper. To manage fiber lines and landmarks, simplified landmark map are presented based on the landmark list. Multi-fiber lines are online monitored, which may have the different or same parameters. When the optical power of any monitored line exceeds the set threshold, the subsystem gives a low power alarm, detecting subsystem will be started automatically to detect alarmed line. After data processing and analysis of detection results, then draws the result curve in the virtual instrument panel. User can locate plot of the curve, and zoom in event curve based on the event list. It utilizes detection results, event list, landmark list and simplified landmarks map synthetically, and presents comprehensive diagnosis conclusion of detection based on the map. To avoid fault, the system predicts fault in future through the analysis of a period of the historical detection result. This system has the advantages of excellent stability, powerful analysis, friendly interface, and convenient operation.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6016391,no
Strategic management system for academic world: Expert system based on composition of cloud computing and case based reasoning system,"The presented work in this paper explores the relationship between Information Technology (IT) and Process Redesign (PR) in academic world. Existing processes for student's and faculties' vital data collection require a great deal of labor work to collect, input and analyze the information. These processes are usually slow and error prone, introducing a latency that prevents real-time data accessibility. We propose a solution to automate this process by using documents attached to existing faculty/ student datasheet that are inter-connected to exchange service. The proposal is based on the concepts of utility computing and cloud computing networks. The information becomes available in the cloud from where it can be processed by expert systems and/or distributed to administrative staff. The proof-of-concept design applies commodity computing integrated to legacy education devices, ensuring cost effectiveness and simple integration. In the presented paper, author suggests that IT also have a stronger role in software realization of an expert system. Author would like to assemble experts experience in our dumb box (Personal Computer) as knowledge base.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6016624,no
Strategic e-commerce model driven-architecture for e-Learning: TQM & e-ERP Perspective,"Innovation and value proposition in distance-learning-programs (e-learning programs) is a multifaceted-activity with enormous dimensionality. The implementation of information communication technology (ICT) and enhanced enterprise-resource-planning (eERP) for smart-campuses has emerged as inevitability for industrial competitiveness in smart-factories as per the dictates of total quality management (TQM). The philosophy of productivity-management and concurrent engineering stipulates a competitive framework for e-learning for dissemination and absorption of knowledge by the intelligentsia, technologists and students. Conversely, competitiveness in today's global village demands an innovative and systems-approach for improving the learning-curves in a virtual environment. The e-learning-programs have numerous inherent perplexities since in every virtual classroom there is a much higher probability to interact with orthogonal-cultures that too with multiple-intelligences. This paper proposes a conceptual-strategic-planning-framework for diffusion of innovative in a distance learning program (Course-Technovation). The framework is based on literature review and field visits encompassing integrated E-commerce-model-driven architecture embedded with business intelligence and coupled with eERP-design functionalities.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6017800,no
Detecting outliers in sliding window over categorical data streams,"Outlier mining is an important and active research issue in anomaly detection. However, it is a difficult problem since categorical data arrive at a fast rate, some data may be outdated and the outliers identified are likely to change. In this paper, we propose an efficient algorithm for mining outliers from categorical data streams, which discover closed frequent patterns in sliding window first. Then WCFPOF (Weighted Closed Frequent Pattern Outlier Factor) is introduced to measure the complete categorical data, and the corresponding candidate outliers are stored in QIS (Query Indexed Structure). By employing the decayed function, the outdated outliers are faded to generate the final outliers. Experimental results show that our algorithm has higher detection precision than FindFPOF. Otherwise, our algorithm has better scalability with different data sizes.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6019780,no
An evaluation of source code mining techniques,"This paper reviews the tools and techniques which rely only on data mining methods to determine patterns from source code such as programming rules, copy paste code segments, and API usage. The work provides comparison and evaluation of the current state-of-the-art in source code mining techniques. Furthermore it identifies the essential strengths and weaknesses of individual tools and techniques to make an evaluation indicative of future potential. The pervious related works only focus on one specific pattern being mined such as special kind of bug detection. Thus, there is a need of multiple tools to test and find potential information from software which increase cost and time of development. Hence there is a strong need of tool which helps in developing quality software by automatically detecting different kind of bugs in one pass and also provides code reusability for the developers.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6019877,no
A fault-tolerant permanent magnet synchronous motor drive with integrated voltage source inverter open-circuit faults diagnosis,"This paper presents a variable speed ac drive based on a permanent magnet synchronous motor, supplied by a three-phase fault-tolerant power converter. In order to achieve this, beyond the main routines, the control system integrates a reliable and simple algorithm for real-time diagnostics of inverter open-circuit faults. This algorithm performs an important role since it is able to detect an inverter malfunction and gives the information about its faulty phase. Then, the control system acts in order to first isolate the fault and then to proceed to a hardware and software reconfiguration. By doing this, a fully automated fault-tolerant variable speed drive can be achieved. Simulation and experimental results are presented showing the effectiveness of the proposed system under several operating conditions.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6020230,no
An activity-based genetic algorithm approach to multiprocessor scheduling,"In parallel and distributed computing, development of an efficient static task scheduling algorithm for directed acyclic graph (DAG) applications is an important problem. The static task scheduling problem is NP-complete in its general form. The complexity of the problem increase when task scheduling is to be done in a heterogeneous environment, where the processors in the network may not be identical and take different amounts of time to execute the same task. This paper presents an activity-based genetic task scheduling algorithm for the tasks run on the network of heterogeneous systems and represented by Directed Acyclic Graphs (DAGs). First, a list scheduling algorithm is incorporated in the generation of the initial population of a GA to represent feasible operation sequences and diminish coding space when compared to permutation representation. Second, the algorithm assigns an activity to each task which is assigned on the processor, and then the quality of the solution will be improved by adding the activity and the random probability in the crossover and mutation operator. The performance of the algorithm is illustrated by comparing with the existing effectively scheduling algorithms.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6022236,no
Evolutionary generation of test data for path coverage with faults detection,"The aim of software testing is to find faults in the program under test. Previous methods of path-oriented test data generation can generate test data traversing target paths, but they may not guarantee to find faults in the program. We present a method of evolutionary generation of test data for path coverage with faults detection in this paper. First, we establish a mathematical model of the problem considered in this paper, in which the number of faults detected in the path traversed by test data, and the risk level of faults are optimization objectives, and the approach level of the traversed path from the target one is a constraint. Then, we generate test data using a multi-objective evolutionary optimization algorithm with constraints. Finally, we apply the proposed method in a benchmark program bubble sort and an industrial program totinfo, and compare it with the traditional method. The experimental results conform that our method can generate test data that not only traverse the target path but also detect faults in it. Our achievement provides a novel way to generate test data for path coverage with faults detection.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6022397,no
Review on integrated health management for aerospace plane,"The references at home and abroad are summarized in this paper. This paper introduces the basic concept and the application significance of aerospace plane, the main differences between the aerospace plane and general aircraft, integrated health management system and its compositions of aerospace plane, the development status of health management for aerospace plane in domestic and foreign countries. Especially it recommends special technical experiments in America, Japan and Italy, and analyzes the health management approaches of them. Finally, it also describes the main fault diagnosis methods. Integrated health management of aerospace plane generally involves a series of activities, including signal processing, monitoring, health assessment, failure prediction, decision support, human-computer interaction, restoring, and so on. In addition, this paper predicts development direction of health management for the aerospace plane.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6023505,no
Improving system health monitoring with better error processing,"To help identify unexpected software events and impending hardware failures, developers typically incorporate error-checking code in their software to detect and report them. Unfortunately, implementing checks with reporting capabilities that give the most useful results comes at a price. Such capabilities should report the exact nature of impending failures and additionally limit reporting to only the first occurrence of an error to prevent flooding the error log with the same message. They must report when an existing error or fault is replaced by another error of a different nature or value. They must recognize what makes occasional faults allowable and they must reset themselves upon recovery from a reported failure so the checking process can begin anew. They must also report recovery from previously reported failures that appear to have healed themselves. Since the price associated with providing all these features is limited by budget and schedule, system reliability and health monitoring often suffer. However, there are practical techniques that can simplify the effort associated with incorporating such error detection and reporting. When done properly, they can greatly improve system reliability and health monitoring by finding potentially hidden problems during development and can also greatly improve system maintainability by providing concise running descriptions of problems when things go wrong particularly when minor errors might otherwise go unnoticed. In addition, preventative maintenance can be greatly aided by applying error detection techniques to performance monitoring in the absence of errors. Many of the techniques described in this paper take advantage of simple classes to do bookkeeping tasks such as updating and tracking statistical analysis of errors and error reporting. The paper highlights several of these classes and gives examples from actual applications.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6024322,no
Automatic loading control circuit fault diagnosis system,"Automatic loading control circuit is as an important component of automatic loading system, its work's reliability directly affects the system's operational effectiveness and its safety, its fault diagnosis is important to improve the reliability of the automatic loading system. Based on describing the fault diagnosis system's structure and function of the automatic loading control circuit, CPU's reset circuit of automatically loading control circuit is as example, the fault tree method is introduced, the fault dictionary is established, and the corresponding software diagnostic process is given. Finally, according to the end of event probability, CPU's failure probability and the importance of bottom events are identified. Experiments show that the method is correct and effective.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6024510,no
Numerical simulation of springback base on formability index for auto panel surface,"In order to emerge the traditional measurement's shortage of auto body panel, we proposed the method based on formability index. According to the above mentioned programs and the springback defect diagnosis, we developed a CAE module for the springback defect analysis based on VC++ environment, and solved the problems which the traditional sheet metal forming CAE software can not accurately predict. And then a forming process of U-beam is simulated by applying the proposed method which shows the method we proposed is accurate, and then we introduced some methods to optimize the adjustment amount of metal flow and the stamping dies for springback. Some suggestions are given by investigating the adjustment amount and modification of the stamping die.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6026028,no
cPLC  A cryptographic programming language and compiler,"Cryptographic two-party protocols are used ubiquitously in everyday life. While some of these protocols are easy to understand and implement (e.g., key exchange or transmission of encrypted data), many of them are much more complex (e.g., e-banking and e-voting applications, or anonymous authentication and credential systems). For a software engineer without appropriate cryptographic skills the implementation of such protocols is often difficult, time consuming and error-prone. For this reason, a number of compilers supporting programmers have been published in recent years. However, they are either designed for very specific cryptographic primitives (e.g., zero-knowledge proofs of knowledge), or they only offer a very low level of abstraction and thus again demand substantial mathematical and cryptographic skills from the programmer. Finally, some of the existing compilers do not produce executable code, but only metacode which has to be instantiated with mathematical libraries, encryption routines, etc. before it can actually be used. In this paper we present a cryptographically aware compiler which is equally useful to cryptographers who want to benchmark protocols designed on paper, and to programmers who want to implement complex security sensitive protocols without having to understand all subtleties. Our tool offers a high level of abstraction and outputs well-structured and documented Java code. We believe that our compiler can contribute to shortening the development cycles of cryptographic applications and to reducing their error-proneness.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6027533,no
A property based security risk analysis through weighted simulation,"The estimation of security risks in complex information and communication technology systems is an essential part of risk management processes. A proper computation of risks requires a good knowledge about the probability distributions of different upcoming events or behaviours. Usually, technical risk assessment in Information Technology (IT) systems is concerned with threats to specific assets. However, for many scenarios it can be useful to consider the risk of the violation of particular security properties. The set of suitable qualities comprises authenticity of messages or non-repudiability of actions within the system but also more general security properties like confidentiality of data. Furthermore, as current automatic security analysis tools are mostly confined to a technical point of view and thereby missing implications on an application or process level, it is of value to facilitate a broader view including the relation between actions within the IT system and their external influence. The property based approach aims to help assessing risks in a process-oriented or service level view of a system and also to derive a more detailed estimation on a technical level. Moreover, as systems' complexities are growing, it becomes less feasible to calculate the probability of all patterns of a system's behaviour. Thus, a model based simulation of the system is advantageous in combination with a focus on precisely defined security properties. This paper introduces the first results supporting a simulation based risk analysis tool that enables a security property oriented view of risk. The developed tool is based on an existing formal validation, verification and simulation tool, the Simple Homomorphism Verification Tool (SHVT). The new simulation software provides a graphical interface for a monitor automaton which facilitates the explicit definition of security properties to be investigated during the simulation cycles. Furthermore, in order to model different- - likelihoods of actions in a system, weighting factors can be used to sway the behaviour where the occurrence of events is not evenly distributed. These factors provide a scheme for weighting classes of transitions. Therefore, the tool facilitates probabilistic simulation, providing information about the probability distribution of satisfaction or violation of specified properties.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6027534,no
Research on UAV health evaluation based on PNN,"In view of UAV system development and the need of the UAV avionics system health evaluation, this paper establishes UAV state evaluate model of avionics system based on PNN(Probabilistic Neural Networks), designs and realizes UAV avionics system evaluation software. Finally, the simulation result indicates that this algorithm is reasonable and effective. This method can help people to design the avionics system, evaluate the UAV avionics system online and locate avionics system fault.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6028614,no
Statistical method based computer fault prediction model for large computer centers,"Computer laboratories in large computer centers play an important role for the computing education in higher education institutions. Due to heavy usage, there are often hardware or software problems on these computers. Thus computer laboratories management and maintenance is a very important and long-term task. For a computer laboratory, the computers' monthly used time follows the normal distribution, which is verified by the chi-square goodness-of-fit hypothesis test. According to probability distributions based on the normal distribution, a statistical method based computer fault prediction model for public computer laboratories is presented. Computers' health state is divided into four classes: the Normal state, the Concerning state, the Warning state, and the Fault state. For different levels of computer fault, different methods are used to maintain it. The experiment and application was conducted in a university computer center with hundreds of computers in 9 computer laboratories. The experiment and application results show that it is an effective way for finding fault computers. It can be used to guide computer maintenance. Computers maintenance workload is greatly reduced by using this fault prediction model.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6028795,no
A Fast and Effective Control Scheme for the Dynamic Voltage Restorer,"A novel control scheme for the dynamic voltage restorer (DVR) is proposed to achieve fast response and effective sag compensation capabilities. The proposed method controls the magnitude and phase angle of the injected voltage for each phase separately. Fast least error squares digital filters are used to estimate the magnitude and phase of the measured voltages. The utilized least error squares estimated filters considerably reduce the effects of noise, harmonics, and disturbances on the estimated phasor parameters. This enables the DVR to detect and compensate voltage sags accurately, under linear and nonlinear load conditions. The proposed control system does not need any phase-locked loops. It also effectively limits the magnitudes of the modulating signals to prevent overmodulation. Besides, separately controlling the injected voltage in each phase enables the DVR to regulate the negative- and zero-sequence components of the load voltage as well as the positive-sequence component. Results of the simulation studies in the PSCAD/EMTDC software environment indicate that the proposed control scheme 1) compensates balanced and unbalanced voltage sags in a very short time period, without phase jump and 2) performs satisfactorily under linear and nonlinear load conditions.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6029403,no
Automatic Generation of Efficient Predictable Memory Patterns,"Verifying firm real-time requirements gets increasingly complex, as the number of applications in embedded systems grows. Predictable systems reduce the complexity by enabling formal verification. However, these systems require predictable software and hardware components, which is problematic for resources with highly variable execution times, such as SDRAM controllers. A predictable SDRAM controller has been proposed that addresses this problem using predictable memory patterns, which are precomputed sequences of SDRAM commands. However, the memory patterns are derived manually, which is a time-consuming and error-prone process that must be repeated for every memory device, and may result in inefficient use of scarce and expensive bandwidth. This paper addresses this issue by proposing three algorithms for automatic generation of efficient memory patterns that provide different trade-offs between run-time of the algorithm and the bandwidth guaranteed by the controller. We experimentally evaluate the algorithms for a number of DDR2/DDR3 memories and show that an appropriate choice of algorithm reduces run-time to less than a second and increases the guaranteed bandwidth by up to 10.2%.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6029846,no
Optimizing the Product Derivation Process,"Feature modeling is widely used in software product-line engineering to capture the commonalities and variabilities within an application domain. As feature models evolve, they can become very complex with respect to the number of features and the dependencies among them, which can cause the product derivation based on feature selection to become quite time consuming and error prone. We address this problem by presenting techniques to find good feature selection sequences that are based on the number of products that contain a particular feature and the impact of a selected feature on the selection of other features. Specifically, we identify a feature selection strategy, which brings up highly selective features early for selection. By prioritizing feature selection based on the selectivity of features our technique makes the feature selection process more efficient. Moreover, our approach helps with the problem of unexpected side effects of feature selection in later stages of the selection process, which is commonly considered a difficult problem. We have run our algorithm on the e-Shop and Berkeley DB feature models and also on some automatically generated feature models. The evaluation results demonstrate that our techniques can shorten the product derivation processes significantly.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6030044,no
Finding software fault relevant subgraphs a new graph mining approach for software debugging,"In this paper, a new approach for analyzing program behavioral graphs to detect fault suspicious subgraphs is presented. The existing graph mining approaches for bug localization merely detect discriminative subgraphs between failing and passing runs, which are not applicable when the context of a failure is not appeared in a discriminative pattern. In our proposed method, the suspicious transitions are identified by contrasting nearest neighbor failing and passing dynamic behavioral graphs. The technique takes advantage of null hypothesis testing and a new formula for ranking edges is presented. To construct the most bug relevant subgraph, the high ranked edges are applied and presented to the debugger. The experimental results on Siemens test suite and Space program reveal effectiveness of the proposed method on weighted dynamic graphs for locating bugs in comparison with other methods.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6030590,no
Unit test case design metrics in test driven development,"Testing is a validation process that determines the conformance of the software's implementation to its specification. It is an important phase in unit test case design and is even more important in object-oriented systems. We want to develop test case designing criteria that give confidence in unit testing of object-oriented system. The main aim testing can be viewed as a means of assessing the existing quality of the software to probe the software for defect and fix them. We also want to develop and executing our test case automatically since this decreases the effort (and cost) of software development cycle (maintenance), and provide re-usability in Test Driven Development framework. We believe such approach is necessary for reaching the levels of confidence required in unit testing. The main goal of this paper is to assist the developers/testers to improve the quality of the ATCUT by accurately design the test case for unit testing of object-oriented software based on the test results. A blend of unit testing assisted by the domain knowledge of the test case designer is used in this paper to improve the design of test case. This paper outlines a solution strategy for deriving Automated Test Case for Unit Testing (ATCUT) metrics from object-oriented metrics via TDD concept.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6031205,no
Unified framework for developing Two Dimensional software reliability growth models with change point,"In order to assure software quality and assess software reliability, many software reliability growth models (SRGMs) have been proposed. In One-Dimension Software Reliability Growth Models researcher used one factor such as Testing-Time, Testing-Effort or Coverage, etc for designing the model but in Two-Dimensional software reliability growth model, process depends on two-types of reliability growth factors like: Testing-time and Testing-effort or Testing-time and Testing-Coverage or any combination between factors. Alsozin more realistic situations, the failure distribution can be affected by many factors, such as the running environment, testing strategy and resource allocation. Once these factors are changed during testing phase, it could result in failure intensity function that increases or decreases non-monotonically and the time point corresponding to abrupt fluctuations is called change point. In this paper, we discuss generalized framework for Two-Dimensional SRGM with change-point for software reliability assessment. The models developed have been validated on real data set.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6031604,no
Systemic assessment of risks for projects: A systems and Cybernetics approach,"The current and past success rate of software projects is poor. This is mainly due to the reductionist/ analytic methods used in project risk assessments. The risk assessment practices adhered today based on standards or otherwise are non-systemic. These assessments cannot provide or deal with the systemic view of Project risks. They handle the project context and complex issues in Projects inadequately. The issue with reductionist thinking is that it leads to reductionist approach to problem solving and history has shown us Project failures continue to happen. This paper explains the Systemic Assessment of Risks (SAR) methodology that is proposed for assessment of project risks by considering Project as a system. This methodology uses the Cybernetics Risk Influence Diagramming (CRID) technique for identification of probable interconnected, interrelated and emergent risks. SAR's application in a software development project in a telecommunications enterprise demonstrates the methodology with the project risks assessed systemically.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6031745,no
A System for Nuclear Fuel Inspection Based on Ultrasonic Pulse-Echo Technique,"Nuclear Pressurized Water Reactor (PWR) technology has been widely used for electric energy generation. The follow-up of the plant operation has pointed out the most important items to optimize the safety and operational conditions. The identification of nuclear fuel failures is in this context. The adoption of this operational policy is due to recognition of the detrimental impact that fuel failures have on operating cost, plant availability, and radiation exposure. In this scenario, the defect detection in rods, before fuel reloading, has become an important issue. This paper describes a prototype of an ultrasonic pulse-echo system designed to inspect failed rods (with water inside) from PWR. This system combines development of hardware (ultrasonic transducer, mechanical scanner and pulser-receiver instrumentation) as well as of software (data acquisition control, signal processing and data classification). The ultrasonic system operates at center frequency of 25 MHz and failed rod detection is based on the envelope amplitude decay of successive echoes reverberating inside the clad wall. The echoes are classified by three different methods. Two of them (Linear Fisher Discriminant and Neural Network) have presented 93% of probability to identify failed rods, which is above the current accepted level of 90%. These results suggest that a combination of a reliable data acquisition system with powerful classification methods can improve the overall performance of the ultrasonic method for failed rod detection.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6031787,no
Test-Driving Static Analysis Tools in Search of C Code Vulnerabilities,"Recently, a number of tools for automated code scanning came in the limelight. Due to the significant costs associated with incorporating such a tool in the software lifecycle, it is important to know what defects are detected and how accurate and efficient the analysis is. We focus specifically on popular static analysis tools for C code defects. Existing benchmarks include the actual defects in open source programs, but they lack systematic coverage of possible code defects and the coding complexities in which they arise. We introduce a test suite implementing the discussed requirements for frequent defects selected from public catalogues. Four open source and two commercial tools are compared in terms of their effectiveness and efficiency of their detection capability. A wide range of C constructs is taken into account and appropriate metrics are computed, which show how the tools balance inherent analysis tradeoffs and efficiency. The results are useful for identifying the appropriate tool, in terms of cost-effectiveness, while the proposed methodology and test suite may be reused.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032220,no
Error-Based Software Testing and Analysis,"An approach to error-based testing is described that uses simple programmer error models and focus-directed methods for detecting the effects of errors. Errors are associated with forgetting, ignorance, bandwidth and perversity. The focus-directed approach was motivated by the observation that focus is more important than methodology in detecting such errors. The strengths and weaknesses of error-based versus more methodological methods are compared using three underlying assumptions called the faith, coincidence and hindsight effects. The weaknesses of error-based testing are compensated for by establishment of an expertise-based foundation that uses research from the study of natural decision making. Examples of the application of error-based methods are given from projects in which the author had access to the programmers, making it possible to track failure back to both defect and error. The relationship of error-based testing to contemporary methods, such as context-driven and exploratory testing, is described.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032231,no
Towards Rapid Creation of Test Adaptation in On-line Model-Based Testing,"Model-based Testing (MBT) is an approach for generating test cases automatically from abstract models of the system under test (SUT). The resulting test cases are also abstract and they have to be concretized before being applied to the SUT. This task is typically delegated to the test adaptation layer. The test adaptation is usually created manually which is tedious and error prone. In this paper, we present an approach in which we take advantage of an existing test execution framework for implementing the test adaptation between an on-line MBT tool and the SUT. The approach allows the reuse of message libraries and automatic concretization/abstraction of tests in on-line testing. In addition, we also discuss ways to automate the building of the test adaptation from other artifacts of the testing process. We exemplify our approach with excerpts from a telecom case study.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032233,no
Statistical Evaluation of Test Sets Using Mutation Analysis,"Evaluation of the ability of test sets for fault detection, and indirectly also evaluation of the quality of test techniques that generate those test sets, have become more of an issue in software testing. Based on mutation analysis, this paper evaluates and compares fault detection ability of test sets using statistical techniques. In this process also different mutant types (and indirectly different fault types) are considered. A case study, drawn from a large commercial web-based system, validates the approach and analyzes its characteristics.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032234,no
Predicting Timing Performance of Advanced Mechatronics Control Systems,"Embedded control is a key product technology differentiator for many high-tech industries, including ASML. The strong increase in complexity of embedded control systems, combined with the occurrence of late changes in control requirements, results in many timing performance problems showing up only during the integration phase. The fallout of this is extremely costly design iterations, severely threatening the time-to-market and time-to-quality constraints. This paper reports on the industrial application at ASML of the Y-chart method to attack this problem. Through the largely automated construction of executable models of a wafer scanner's mechatronics control application and platform, ASML was able to obtain high-level overview early on in the development process. The system wide insight in timing bottlenecks gained this way resulted in more than a dozen improvement proposals yielding significant performance gains. These insights also led to a new development roadmap of the mechatronics control execution platform.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032239,no
On the Consensus-Based Application of Fault Localization Techniques,"A vast number of software fault localization techniques have been proposed recently with the growing realization that manual debugging is time-consuming, tedious and error-prone, and fault localization is one of the most expensive debugging activities. While some of these techniques perform better than one another on a large number of data sets, they do not do so on all data sets and therefore, the actual quality of fault localization can vary considerably by using just one technique. This paper proposes the use of a consensus-based strategy that combines the results of multiple fault localization techniques, to consistently provide high quality performance, irrespective of data set. Empirical evidence based on case studies conducted on three sets of programs (the seven programs of the Siemens suite, and the gzip and make programs) and three different fault localization techniques suggests that the consensus-based strategy holds merit and generally provides close to the best, if not the best, results. Additionally the consensus-based strategy makes use of techniques that all operate on the same set of input data, minimizing the overhead. It is also simple to include or exclude techniques from consensus, making it an easily extensible, or alternatively, tractable strategy.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032290,no
Ontology-Based Reliability Evaluation for Web Service,"Reliability has become a major quality metric for Web service. However, current reliability evaluation approaches lack a formal semantic representation and the support of incomplete or uncertain information. We propose a Web service reliability ontology (WSRO) serving as a basis to characterize the knowledge of Web service. And based on WSRO, a mapping to the probability graphical model is constructed. The Web service reliability evaluation results are obtained by the causality reasoning. Some evaluation results reveal that our approach is applicable and effective.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032363,no
Reliability and Accuracy of the Estimation Process - Wideband Delphi vs. Wisdom of Crowds,This research paper addresses the reliability of estimation techniques based on technical knowledge possessed by software engineers. The goal was to identify weaknesses and limitations in the estimation practices based on Wideband Delphi method and to propose an alternative solution. The initial experiment highlights the results of employing a different estimation method based on Wisdom of Crowds approach and compares them to standard Wideband Delphi approach. The estimation was focused on the expected quality of the software release -- predicting the distribution pattern of defects among the system components which would be found throughout the development and test phases (before reaching the customer). The final results for both estimation techniques were evaluated against real system data (the software product release development in the public safety domain).,2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032364,no
Quality Model Driven Dynamic Analysis,"Release managers often face a dilemma about the quality of software under delivery before a release. The presence of run-time errors such as memory leaks, buffer overflows, and deadlocks affects quality attributes such as efficiency, security, and reliability. Such errors are detected using dynamic analysis methods in practice. However, the dynamic analysis methods employed in practice are by and large ad hoc. It is essential to use dynamic analysis focusing on finding the right set of run-time errors in a software component that have the maximum impact on quality. There exists a need to identify quality attributes such as reliability, efficiency, and security that are important for a software component, or, a system. In this paper, a quality model driven dynamic analysis methodology is proposed. Various run-time errors that can arise during the execution of programs written in a language such as C++ are mapped to the respective quality attributes, thereby forming a basis for run-time error classification. Our experiences in the application of dynamic analysis on real projects are reported. The methodology reports the error findings mapped to the quality attributes along with their distributions. The reported findings help management understand quality problems and take appropriate corrective action.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032365,no
Software Reliability Prediction for Open Source Software Adoption Systems Based on Early Lifecycle Measurements,"Various OSS(Open Source Software)s are being modified and adopted into software products with their own quality level. However, it is difficult to measure the quality of an OSS before use and to select the proper one. These difficulties come from OSS features such as a lack of bug information, unknown development schedules, and variable documentations. Conventional software reliability models are not adequate to assess the reliability of a software system in which an OSS is being adopted as a new add-on feature because the OSS can be modified while Commercial Off-The-Shelf (COTS) software cannot. This paper provides an approach to assessing the software reliability of OSS adopted software system in the early stage of a software life cycle. We identify the software factors that affect the reliability of software system using the COCOMOII modeling methodology and define the module usage as a module coupling measure. We build the fault count models using the multivariate linear regression and performed the model evaluation. Early software reliability assessment in OSS adoption helps to make an effective development and testing strategies for improving the reliability of the whole system.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032366,no
Software-based analysis of the effects of electrostatic discharge on embedded systems,"This paper illustrates the use of software for monitoring and recording the effects of electrostatic discharge (ESD) on the operation of embedded systems, with the goal of facilitating root-cause analysis of resulting failures. Hardware -- based scanning techniques are typically used for analyzing the effect of ESD on systems by identifying physical coupling paths. This paper proposes software techniques that monitor registers and flags associated with peripherals of embedded systems to detect faults associated with the effects of ESD. A lightweight, cost-effective, and non-intrusive software tool has been developed that monitors and records the status of all registers associated with a designated peripheral under test, identifying the fault propagation caused by ESD in the system, and visually presenting the resulting errors. The tool has been used to detect and visually summarize ESD-induced errors on the SD card peripheral of the S3C2440 development board, using local injection and system-level scanning. Root-cause analysis of these faults can potentially assist in identification of coupling paths of electromagnetic interference, as well as determination of areas of the hardware that are more vulnerable to ESD.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032376,no
Security Monitoring of Components Using Aspects and Contracts in Wrappers,"The re-usability and modularity of components reduce the cost and complexity of the software design. It is difficult to predict run-time scenarios covering all possible circumstances to ensure that the components are fully compatible with the system. Given that, monitoring run-time behaviours of components presents a close view of the component qualities. The existing monitoring approaches either implement applications with built-in monitoring features, or observe the external resources and events to predict the status of the components. In this paper, we propose an approach to monitor the runtime behaviours of components using aspect-oriented wrappers and contracts. We design monitoring wrappers to encapsulate the monitored components. We use contracts to define the mutual obligations of two interacting components. The policies implemented in contracts are woven into component wrappers as separate aspect modules. If the component contains any flaws or vulnerabilities, the wrappers can monitor some behaviours and prevent failures propagating into the wrapped components and the rest of the system. This approach assures that the system is running in a safe environment with the erroneous behaviours detected appropriately. We conducted experiments on the run-time monitoring of SQL Injection, Cross Site Scripting attacks, and access control policies. The results show that the framework is very flexible to impose separate policies as aspects on component wrappers without the modifications of the underlying components.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032399,no
Usage-Based Online Testing for Proactive Adaptation of Service-Based Applications,"Increasingly, service-based applications (SBAs) are composed of third-party services available over the Internet. Even if third-party services have shown to work during design-time, they might fail during the operation of the SBA due to changes in their implementation, provisioning, or the communication infrastructure. As a consequence, SBAs need to dynamically adapt to such failures during run-time to ensure that they maintain their expected functionality and quality. Ideally the need for an adaptation is proactively identified, i.e., failures are predicted before they can lead to consequences such as costly compensation and roll-back activities. Currently, approaches to predict failures are based on monitoring. Due to its passive nature, however, monitoring might not cover all relevant service executions, which can diminish the ability to correctly predict failures. In this paper we demonstrate how online testing, as an active approach, can improve failure prediction by considering a broader range of service executions. Specifically, we introduce a framework and prototypical implementation that exploits synergies between monitoring, online testing and quality prediction. For online test selection and assessment we adapt usage-based testing strategies. We experimentally evaluate the strengths of our approach in predicting the need for an adaptation of an SBA.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032401,no
Characterizing the Implementation of Software Non-functional Requirements from Probabilistic Perspective,"Non-functional requirements are quality concerns of a software envisioned. As an effective treatment, goal-oriented method can capture NFR-related knowledge so that an evaluation for a specific implementation strategy can be provided. This paper makes a meaningful attempt to observe the implementation strategies of non-functional requirements in a probabilistic way, and obtain the probabilistic result for each satisficing status. The contribution of our work is to give a clear justification about whether there exists a proper implementation strategy for multiple non-functional requirements so that they can be guaranteed of the specific satisficing statuses, and if so how big the possibility is.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032406,no
Evaluating an Interactive-Predictive Paradigm on Handwriting Transcription: A Case Study and Lessons Learned,"Transcribing handwritten text is a laborious task which currently is carried out manually. As the accuracy of automatic handwritten text recognizers improves, post-editing the output of these recognizers could be foreseen as a possible alternative. Alas, the state-of-the-art technology is not suitable to perform this kind of work, since current approaches are not accurate enough and the process is usually both inefficient and uncomfortable for the user. As alternative, an interactive-predictive paradigm has gained recently an increasing popularity, mainly due to promising empirical results that estimate considerable reductions of user effort. In order to assess whether these empirical results can lead indeed to actual benefits, we developed a working prototype and conducted a field study remotely. Thirteen regular computer users tested two different transcription engines through the above-mentioned prototype. We observed that the interactive-predictive version allowed to transcribe better (less errors and fewer iterations to achieve a high-quality output) in comparison to the manual engine. Additionally, participants ranked higher such an interactive-predictive system in a usability questionnaire. We describe the evaluation methodology and discuss our preliminary results. While acknowledging the known limitations of our experimentation, we conclude that the interactive-predictive paradigm is an efficient approach for transcribing handwritten text.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032407,no
Quantifying Usability and Security in Authentication,"Substantial research has been conducted in developing sophisticated security methods with authentication mechanisms placed in the front line of defense. Since these mechanisms are based on user conduct, they may not accomplish the intended objectives with improper use. Despite the influence of usability, little research has been focused on the balance between usability and security in authentication mechanisms when evaluating the effectiveness of these systems. In this paper we present a quantification approach for assessing usable security in authentication mechanisms. The purpose of this approach is to guide the evaluation process of authentication mechanisms in a given environment by balancing usability and security and defining quantifiable quality criteria.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032409,no
"Does """"Depth"""" Really Matter? On the Role of Model Refinement for Testing and Reliability","Model-based testing attempts to generate test cases from a model focusing on relevant aspects of a given system under consideration (SUC). When SUC becomes too large to be modeled in a single step, existing design techniques usually require a modularization of the modeling process. Thereby, the refinement process results in a decomposition of the model into several hierarchical layers. Conventional testing requires the refined components be completely replaced by these subcomponents for test case generation. Mostly, this resolution of components leads to an oversized, large model where test case generation becomes very costly, and the generated test case set is very large leading to infeasible long test execution time. To solve these problems, we present a new strategy to reduce (i) the number of test cases, and (ii) the costs of test case generation and test execution. For determining the trade-off due to this cost reduction, the reliability achieved by the new approach is compared with the reliability of the conventional approach. A case study based on a large web-based commercial system validates the approach and discusses its characteristics. We found out that the new approach could detect about 80% of the faults for about 20% of the test effort compared with the conventional approach.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032410,no
Semantic-Based Test Oracles,"Test oracle is one of the most difficult parts for test automation. For software with a large number of test cases, it is always both expensive and error prone to develop and maintain test oracles. The research is motivated by industry needs of automated testing on software with standard interfaces in an open system architecture. In counter to test oracle challenges, it proposes an innovative method to represent and calculate test oracles based on the semantic model of standard interface service specification of the software under test (SUT). Semantic model provides well-defined domain knowledge of service data, functionalities and constraints. Rules are created to model the expected SUT behavior in terms of antecedents and consequents. For each service, it captures both direct input-output relations and service interactions, that is, how the execution of a service may be affected by (pre-condition) or impact (post-condition) the SUT system state. As rule languages are neutral to programming languages, oracles specified in this way are independent of SUT implementations and can be reused across different systems conforming to the same interface standards. With the support of semantic techniques and tools like ontology modeler and rule engine, the proposed approach can enhance test oracle automation based on sophisticated defined domain model. Experiments and analysis show promising improvements in test productivity and quality.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032411,no
SoftWare IMmunization (SWIM) - A Combination of Static Analysis and Automatic Testing,"Static program analysis uses many checkers to discover a very large number of programming issues, but with a high false alarm rate. With the aid of dynamic automatic testing, the actual severe defects can be confirmed by failures of test cases. After defects are fixed, similar types of defects tend to reoccur again. In this paper, we propose a SoftWare IMmunization (SWIM) method to combine static analysis and automatic testing results for detecting severe defects and preventing similar defects from reoccurring, i.e. to have the software immunized from the same type of defects. Three industrial trials of the technology demonstrated the feasibility and defect detection accuracy of the SWIM technology.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032413,no
The impact of fault models on software robustness evaluations,"Following the design and in-lab testing of software, the evaluation of its resilience to actual operational perturbations in the field is a key validation need. Software-implemented fault injection (SWIFI) is a widely used approach for evaluating the robustness of software components. Recent research [24, 18] indicates that the selection of the applied fault model has considerable influence on the results of SWIFI-based evaluations, thereby raising the question how to select appropriate fault models (i.e. that provide justified robustness evidence). This paper proposes several metrics for comparatively evaluating fault models's abilities to reveal robustness vulnerabilities. It demonstrates their application in the context of OS device drivers by investigating the influence (and relative utility) of four commonly used fault models, i.e. bit flips (in function parameters and in binaries), data type dependent parameter corruptions, and parameter fuzzing. We assess the efficiency of these models at detecting robustness vulnerabilities during the SWIFI evaluation of a real embedded operating system kernel and discuss application guidelines for our metrics alongside.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032444,no
Assessing programming language impact on development and maintenance: a study on c and c++,"Billions of dollars are spent every year for building and maintaining software. To reduce these costs we must identify the key factors that lead to better software and more productive development. One such key factor, and the focus of our paper, is the choice of programming language. Existing studies that analyze the impact of choice of programming language suffer from several deficiencies with respect to methodology and the applications they consider. For example, they consider applications built by different teams in different languages, hence fail to control for developer competence, or they consider small-sized, infrequently-used, short-lived projects. We propose a novel methodology which controls for development process and developer competence, and quantifies how the choice of programming language impacts software quality and developer productivity. We conduct a study and statistical analysis on a set of long-lived, widely-used, open source projects - Firefox, Blender, VLC, and MySQL. The key novelties of our study are: (1) we only consider projects which have considerable portions of development in two languages, C and C++, and (2) a majority of developers in these projects contribute to both C and C++ code bases. We found that using C++ instead of C results in improved software quality and reduced maintenance effort, and that code bases are shifting from C to C++. Our methodology lays a solid foundation for future studies on comparative advantages of particular programming languages.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032456,no
Socio-technical developer networks: should we trust our measurements?,"Software development teams must be properly structured to provide effectiv collaboration to produce quality software. Over the last several years, social network analysis (SNA) has emerged as a popular method for studying the collaboration and organization of people working in large software development teams. Researchers have been modeling networks of developers based on socio-technical connections found in software development artifacts. Using these developer networks, researchers have proposed several SNA metrics that can predict software quality factors and describe the team structure. But do SNA metrics measure what they purport to measure? The objective of this research is to investigate if SNA metrics represent socio-technical relationships by examining if developer networks can be corroborated with developer perceptions. To measure developer perceptions, we developed an online survey that is personalized to each developer of a development team based on that developer's SNA metrics. Developers answered questions about other members of the team, such as identifying their collaborators and the project experts. A total of 124 developers responded to our survey from three popular open source projects: the Linux kernel, the PHP programming language, and the Wireshark network protocol analyzer. Our results indicate that connections in the developer network are statistically associated with the collaborators whom the developers named. Our results substantiate that SNA metrics represent socio-technical relationships in open source development projects, while also clarifying how the developer network can be interpreted by researchers and practitioners.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032467,no
Run-time efficient probabilistic model checking,"Unpredictable changes continuously affect software systems and may have a severe impact on their quality of service, potentially jeopardizing the system's ability to meet the desired requirements. Changes may occur in critical components of the system, clients' operational profiles, requirements, or deployment environments. The adoption of software models and model checking techniques at run time may support automatic reasoning about such changes, detect harmful configurations, and potentially enable appropriate (self-)reactions. However, traditional model checking techniques and tools may not be simply applied as they are at run time, since they hardly meet the constraints imposed by on-the-fly analysis, in terms of execution time and memory occupation. This paper precisely addresses this issue and focuses on reliability models, given in terms of Discrete Time Markov Chains, and probabilistic model checking. It develops a mathematical framework for run-time probabilistic model checking that, given a reliability model and a set of requirements, statically generates a set of expressions, which can be efficiently used at run-time to verify system requirements. An experimental comparison of our approach with existing probabilistic model checkers shows its practical applicability in run-time verification.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032473,no
Detecting software modularity violations,"This paper presents Clio, an approach that detects modularity violations, which can cause software defects, modularity decay, or expensive refactorings. Clio computes the discrepancies between how components should change together based on the modular structure, and how components actually change together as revealed in version history. We evaluated Clio using 15 releases of Hadoop Common and 10 releases of Eclipse JDT. The results show that hundreds of violations identified using Clio were indeed recognized as design problems or refactored by the developers in later versions. The identified violations exhibit multiple symptoms of poor design, some of which are not easily detectable using existing approaches.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032480,no
Bringing domain-specific languages to digital forensics,"Digital forensics investigations often consist of analyzing large quantities of data. The software tools used for analyzing such data are constantly evolving to cope with a multiplicity of versions and variants of data formats. This process of customization is time consuming and error prone. To improve this situation we present DERRIC, a domain-specific language (DSL) for declaratively specifying data structures. This way, the specification of structure is separated from data processing. The resulting architecture encourages customization and facilitates reuse. It enables faster development through a division of labour between investigators and software engineers. We have performed an initial evaluation of DERRIC by constructing a data recovery tool. This so-called carver has been automatically derived from a declarative description of the structure of JPEG files. We compare it to existing carvers, and show it to be in the same league both with respect to recovered evidence, and runtime performance.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032508,no
Building and using pluggable type-checkers,"This paper describes practical experience building and using pluggable type-checkers. A pluggable type-checker refines (strengthens) the built-in type system of a programming language. This permits programmers to detect and prevent, at compile time, defects that would otherwise have been manifested as run-time errors. The prevented defects may be generally applicable to all programs, such as null pointer dereferences. Or, an application-specific pluggable type system may be designed for a single application. We built a series of pluggable type checkers using the Checker Framework, and evaluated them on 2 million lines of code, finding hundreds of bugs in the process. We also observed 28 first-year computer science students use a checker to eliminate null pointer errors in their course projects. Along with describing the checkers and characterizing the bugs we found, we report the insights we had throughout the process. Overall, we found that the type checkers were easy to write, easy for novices to productively use, and effective in finding real bugs and verifying program properties, even for widely tested and used open source projects.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032509,no
Characterizing the differences between pre- and post- release versions of software,"Many software producers utilize beta programs to predict post-release quality and to ensure that their products meet quality expectations of users. Prior work indicates that software producers need to adjust predictions to account for usage environments and usage scenarios differences between beta populations and post-release populations. However, little is known about how usage characteristics relate to field quality and how usage characteristics differ between beta and post-release. In this study, we examine application crash, application hang, system crash, and usage information from millions of Windows users to 1) examine the effects of usage characteristics differences on field quality (e.g. which usage characteristics impact quality), 2) examine usage characteristics differences between beta and post-release (e.g. do impactful usage characteristics differ), and 3) report experiences adjusting field quality predictions for Windows. Among the 18 usage characteristics that we examined, the five most important were: the number of application executed, whether the machines was pre-installed by the original equipment manufacturer, two sub-populations (two language/geographic locales), and whether Windows was 64-bit (not 32-bit). We found each of these usage characteristics to differ between beta and post-release, and by adjusting for the differences, accuracy of field quality predictions for Windows improved by ~59%.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032513,no
An industrial case study on quality impact prediction for evolving service-oriented software,"Systematic decision support for architectural design decisions is a major concern for software architects of evolving service-oriented systems. In practice, architects often analyse the expected performance and reliability of design alternatives based on prototypes or former experience. Model-driven prediction methods claim to uncover the tradeoffs between different alternatives quantitatively while being more cost-effective and less error-prone. However, they often suffer from weak tool support and focus on single quality attributes. Furthermore, there is limited evidence on their effectiveness based on documented industrial case studies. Thus, we have applied a novel, model-driven prediction method called Q-ImPrESS on a large-scale process control system consisting of several million lines of code from the automation domain to evaluate its evolution scenarios. This paper reports our experiences with the method and lessons learned. Benefits of Q-ImPrESS are the good architectural decision support and comprehensive tool framework, while one drawback is the time-consuming data collection.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032519,no
Positive effects of utilizing relationships between inconsistencies for more effective inconsistency resolution: NIER track,"State-of-the-art modeling tools can help detect inconsistencies in software models. Some can even generate fixing actions for these inconsistencies. However such approaches handle inconsistencies individually, assuming that each single inconsistency is a manifestation of an individual defect. We believe that inconsistencies are merely expressions of defects. That is, inconsistencies highlight situations under which defects are observable. However, a single defect in a software model may result in many inconsistencies and a single inconsistency may be the result of multiple defects. Inconsistencies may thus be related to other inconsistencies and we believe that during fixing, one should consider clusters of such related inconsistencies. This paper provides first evidence and emerging results that several inconsistencies can be linked to a single defect and show that with such knowledge only a subset of fixes need to be considered during inconsistency resolution.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032538,no
Automated usability evaluation of parallel programming constructs: nier track,"Multicore computers are ubiquitous, and proposals to extend existing languages with parallel constructs mushroom. While everyone claims to make parallel programming easier and less error-prone, empirical language usability evaluations are rarely done in-the-field with many users and real programs. Key obstacles are costs and a lack of appropriate environments to gather enough data for representative conclusions. This paper discusses the idea of automating the usability evaluation of parallel language constructs by gathering subjective and objective data directly in every software engineer's IDE. The paper presents an Eclipse prototype suite that can aggregate such data from potentially hundreds of thousands of programmers. Mismatch detection in subjective and objective feedback as well as construct usage mining can improve language design at an early stage, thus reducing the risk of developing and maintaining inappropriate constructs. New research directions arising from this idea are outlined for software repository mining, debugging, and software economics.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032556,no
DyTa: dynamic symbolic execution guided with static verification results,"Software-defect detection is an increasingly important research topic in software engineering. To detect defects in a program, static verification and dynamic test generation are two important proposed techniques. However, both of these techniques face their respective issues. Static verification produces false positives, and on the other hand, dynamic test generation is often time consuming. To address the limitations of static verification and dynamic test generation, we present an automated defect-detection tool, called DyTa, that combines both static verification and dynamic test generation. DyTa consists of a static phase and a dynamic phase. The static phase detects potential defects with a static checker; the dynamic phase generates test inputs through dynamic symbolic execution to confirm these potential defects. DyTa reduces the number of false positives compared to static verification and performs more efficiently compared to dynamic test generation.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032571,no
The quamoco tool chain for quality modeling and assessment,"Continuous quality assessment is crucial for the long-term success of evolving software. On the one hand, code analysis tools automatically supply quality indicators, but do not provide a complete overview of software quality. On the other hand, quality models define abstract characteristics that influence quality, but are not operationalized. Currently, no tool chain exists that integrates code analysis tools with quality models. To alleviate this, the Quamoco project provides a tool chain to both define and assess software quality. The tool chain consists of a quality model editor and an integration with the quality assessment toolkit ConQAT. Using the editor, we can define quality models ranging from abstract characteristics down to operationalized measures. From the quality model, a ConQAT configuration can be generated that can be used to automatically assess the quality of a software system.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032576,no
Using software evolution history to facilitate development and maintenance,"Much research in software engineering have been focused on improving software quality and automating the maintenance process to reduce software costs and mitigating complications associated with the evolution process. Despite all these efforts, there are still high cost and effort associated with software bugs and software maintenance, software still continues to be unreliable, and software bugs can wreak havoc on software producers and consumers alike. My dissertation aims to advance the state-of-art in software evolution research by designing tools that can measure and predict software quality and to create integrated frameworks that helps in improving software maintenance and research that involves mining software repositories.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032605,no
Test blueprint: an effective visual support for test coverage,"Test coverage is about assessing the relevance of unit tests against the tested application. It is widely acknowledged that a software with a """"good"""" test coverage is more robust against unanticipated execution, thus lowering the maintenance cost. However, insuring a coverage of a good quality is challenging, especially since most of the available test coverage tools do not discriminate software components that require a """"strong"""" coverage from the components that require less attention from the unit tests. HAPAO is an innovative test covepage tool, implemented in the Pharo Smalltalk programming language. It employs an effective and intuitive graphical representation to visually assess the quality of the coverage. A combination of appropriate metrics and relations visually shapes methods and classes, which indicates to the programmer whether more effort on testing is required. This paper presents the essence of HAPAO using a real world case study.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032614,no
Programming safety requirements in the REFLECT design flow,"The common approach to include non-functional requirements in tool chains for hardware/software embedded systems requires developers to manually change the software code and/or the hardware, in an error-prone and tedious process. In the REFLECT research project we explore a novel approach where safety requirements are described using an aspect- and strategy-oriented programming language, named LARA, currently under development. The approach considers that the weavers in the tool chain use those safety requirements specified as aspects and strategies to produce final implementations according to specific design patterns. This paper presents our approach including LARA-based examples using an avionics application targeting the FPGA-based embedded systems consisting of a general purpose processor (GPP) coupled to custom computing units.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6035002,no
Reputation measure approach of web service for service selection,"In choosing web services with quality of service (QoS), the reputation attribute of QoS is very important for users to obtain reliable services in service selection. However, existing approaches rely on feedback ratings, which usually lead to the subjectivity and unfairness of reputation measure. The authors propose a reputation measure approach for web services. The approach employs three phases (i.e. feedback checking, feedback adjustment and malicious feedback detection) to enhance the reputation measure accuracy. A user survey form was first established to check the feedback ratings from these users who are lacking in feedback ability. Then the feedback ratings are adjusted with different user feedback preferences by calculating feedback similarity. Finally, the authors detect malicious feedback ratings by adopting cumulative sum method. Simulation results show that the proposed approach is effective and can greatly improve service selection process in service-oriented business applications.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6036406,no
Reliability-Aware Design Optimization for Multiprocessor Embedded Systems,"This paper presents an approach for the reliability-aware design optimization of real-time systems on multi-processor platforms. The optimization is based on an extension of well accepted fault- and process-models. We combine utilization of hardware replication and software re-execution techniques to tolerate transient faults. A System Fault Tree (SFT) analysis is proposed, which computes the system-level reliability in presence of the hardware and software redundancy based on component failure probabilities. We integrate the SFT analysis with a Multi-Objective Evolutionary Algorithm (MOEA) based optimization process to perform efficient reliability-aware design space exploration. The solution resulting from our optimization contains the mapping of tasks to processing elements (PEs), the exact task and message schedule and the fault-tolerance policy assignment. The effectiveness of the approach is illustrated using several case studies.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6037415,no
Modular Fault Injector for Multiple Fault Dependability and Security Evaluations,"The increasing level of integration and decreasing size of circuit elements leads to greater probabilities of operational faults. More sensible electronic devices are also more prone to external influences by energizing radiation. Additionally not only natural causes of faults are a concern of today's chip designers. Especially smart cards are exposed to complex attacks through which an adversary tries to extract knowledge from a secured system by putting it into an undefined state. These problems make it increasingly necessary to test a new design for its fault robustness. Several previous publications propose the usage of single bit injection platforms, but the limited impact of these campaigns might not be the right choice to provide a wide fault attack coverage. This paper first introduces a new in-system fault injection strategy for automatic test pattern injection. Secondly, an approach is presented that provides an abstraction of the internal fault injection structures to a more generic high level view. Through this abstraction it is possible to support the task separation of design and test-engineers and to enable the emulation of physical attacks on circuit level. The controller's generalized interface provides the ability to use the developed controller on different systems using the same bus system. The high level of abstraction is combinable with the advantage of high performance autonomous emulations on high end FPGA-platforms.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6037460,no
Compatibility Study of Compile-Time Optimizations for Power and Reliability,"Historically compiler optimizations have been used mainly for improving embedded systems performance. However, for a wide range of today's power restricted, battery operated embedded devices, power consumption becomes a crucial problem that is addressed by modern compilers. Biomedical implants are one good example of such embedded systems. In addition to power, such devices need to also satisfy high reliability levels. Therefore, performance, power and reliability optimizations should all be considered while designing and programming implantable systems. Various software optimizations, e.g., during compilation, can provide the necessary means to achieve this goal. Additionally the system can be configured to trade-off between the above three factors based on the specific application requirements. In this paper we categorize previous works on compiler optimizations for low power and fault tolerance. Our study considers differences in instruction count and memory overhead, fault coverage and hardware modifications. Finally, the compatibility of different methods from both optimization classes is assessed. Five compatible pairs that can be combined with few or no limitations have been identified.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6037493,no
Developing Mobile Applications for Multiple Platforms,"Developing software for mobile devices requires special attention, and it still requires more large effort compared to software development for desktop computers and servers. With the introduction and popularity of wireless devices, the diversity of the platforms has also been increased. There are different platforms and tools from different vendors such as Microsoft, Sun, Nokia, SonyEricsson and many more. Because of the relatively low-level programming interface, software development (e.g. for Symbian) platform is a tiresome and error prone task, whereas Android and Windows Mobile contains higher level structures. This keynote introduces the problem of the software development for incompatible mobile platforms. Moreover, it provides a Model-Driven Architecture (MDA) and Domain Specific Modeling Language (DSML)-based solution. We will also discuss the relevance of the model-based approach that facilitates a more efficient software development because the reuse and the generative techniques are key characteistics of model-based computing. In the presented approach, the platform-independence lies in the model transformation. This keynote illustrates the creation of model compliers on a metamodeling basis by a software package called Visual Modeling and Transformation System (VMTS), which is a multipurpose modeling and metamodel-based transformation system. A case study is also presented on how model compilers can be used to generate user interface handler code for different mobile platforms from the same platform-independent input models.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6037507,no
Modeling Contextual Concerns in Enterprise Architecture,"Enterprise Architecture approaches are used to provide rigorous descriptions of the organization-wide environment, manage the alignment of deployed services to the organization's mission, end ensure a clear separation of the concerns addressed in an architecture. Thus, an effective Enterprise Architecture approach assists in the management of relations and dependencies of any components of the organization environment and supports the integration and evolution of the architecture. However, the quality of that approach is strongly influenced by the precision of the architecture context description, a fact which is not always recognized. This paper focuses on the architecture context description and addresses the gap between the stakeholders' concerns and the resulting architecture. Based on a combination of established references and standards, we show how an explicit integration of the architecture context into the architecture model improves the linking of concerns and key elements of the architecture vision. We apply our approach to a subject of increasing concern in the Information Systems area: longevity of information. Digital preservation is an interdisciplinary problem, but existent initiatives address it in a very domain-centric way, making it impossible to integrate documented knowledge into an overall organization architecture. We analyze several references and models and derive a description of the architecture context and a capability model that supports incremental development through an explicit distinction between systems and their capabilities. The presented approach allows not just any organization to assess their current digital preservation awareness and evolve their architectures to address this challenge, but in particular demonstrates the added value of an explicit architecture context model in an Enterprise Architecture approach.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6037595,no
Design of nuclear measuring instrument fault diagnosis system based on circuit characteristic test,"The circuits of nuclear measuring instruments are very complicated, and the testing and fault inspection of the nuclear measuring instrument system is difficult. To solve this problem, the fault diagnosis system was designed by combing circuit characteristic test technology with virtual instrument, virtual testing, test data analysis and data management technology. The method based on circuit characteristic test, which is comparing the characteristic curves of tested circuit with the one of the corresponding normal circuit, is applied to the fault diagnosis of nuclear measuring instrument. The fault diagnosis system consists of hardware part and software part. The hardware part includes computer, data acquisition module, arbitrary waveform generator (AWG) module, interface circuit module, electric relay module, etc; the software is made up of computer management software module, control software module and testing functional software module and so on. By the fault diagnosis system, it can test the circuit characteristic of any circuit module of the nuclear measuring instruments, diagnoses and finds out the faulted parts of the nuclear measuring instruments quickly and shows the diagnosing results on the display.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6037965,no
The study on stability and reliability of the nuclear detector,"Because of measurement environment and its own performance defect, the poor stability of nuclear radiation detector used on portable nuclear instrument leads to spectrum drift and decrease of energy resolution, which causes reduction of the detect efficiency. This results in lower measurement precision and accuracy of the system. Generally, the stabilization method based on hardware and software was applied. It is difficult to solve the problem about Spectrum drift caused by multiple factors. To slove the nonlinear problems caused by interference source, Multi-sensor Data Fusion Technique is adopt to design the intelligent nuclear detector with the Self-compensation Technique. It shows that the results of the project can improve the stability and reliability of the fieldwork measurement in system of the nuclear instruments. Except that it will improve the adaptive ability of the nuclear instruments to measuring environment.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6038005,no
Optimal Model-Based Policies for Component Migration of Mobile Cloud Services,"Two recent trends are major motivators for service component migration: the upcoming use of cloud-based services and the increasing number of mobile users accessing Internet-based services via wireless networks. While cloud-based services target the vision of Software as a Service, where services are ubiquitously available, mobile use leads to varying connectivity properties. In spite of temporary weak connections and even disconnections, services should remain operational. This paper investigates service component migration between the mobile client and the infrastructure-based cloud as a means to avoid service failures and improve service performance. Hereby, migration decisions are controlled by policies. To investigate component migration performance, an analytical Markov model is introduced. The proposed model uses a two-phased approach to compute the probability to finish within a deadline for a given reconfiguration policy. The model itself can be used to determine the optimal policy and to quantify the gain that is obtained via reconfiguration. Numerical results from the analytic model show the benefit of reconfigurations and the impact of different reconfigurations applied to three service types, as immediate reconfigurations are in many cases not optimal, a threshold on time before reconfiguration can take place is introduced to control reconfiguration.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6038602,no
The Study of OFDM ICI Cancellation Schemes in 2.4 GHz Frequency Band Using Software Defined Radio,"In Orthogonal Frequency Division Multiplexing (OFDM), frequency offset is a common problem that causes inter-carrier-interference (ICI) that degrades the quality of the transmitted signal. Many theoretical studies of the different ICI-cancellation schemes have been reported earlier by many authors. The need for experimental verification of the theoretically predicted results in 2.4 GHz frequency band is important. One of most widely used systems is Wi-Fi (IEE 802.11b) that makes use of this frequency band for short range wireless communication with throughput as high as 11 Mbps. In this work, several new ICI cancellation schemes have been tested in 2.4 GHz frequency using open source Software Defined Radio (SDR) namely GNU Radio. The GNU Radio system used in the experiment had two Universal Software Radio Peripheral (USRP N210) modules connected to a computer. Both the USRP units had one-daughterboard (XCVR2450) each for transmission and reception of radio signals. The input data to the USRP was prepared in compliance with IEEE-802.11b specification. The experimental results were compared with the theoretical results of the new Inter-Carrier Interference (ICI) cancellation schemes. The comparison of the results revealed that the new schemes are suitable for high performance transmission. The results of this paper open up new opportunities of using OFDM in heavily congested 2.4 GHz and 5 GHz bands (WiFi5: IEEE 802.a) for error free data transmission. The schemes also can be used in other frequencies where channels are heavily congested.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6040059,no
The Research and Implement of Air Quality Monitoring System Based on ZigBee,"Abstract- Health, Safety and Environment management system (HSE) is a general management system of international oil and gas industry. In order to comply with HSE management system, an air quality monitoring system is researched based on ZigBee wireless sensor technology, which is applied in industrial sites. The system includes: detecting terminal, wireless router, wireless gateway, software of field devices and monitoring equipment; the system can measure a variety of gas parameters, such as: CO<sub>2</sub> concentration, CO concentration, air quality level, temperature and humidity; Features of the system have high accuracy, quick sensitivity, wide monitoring range, etc..",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6040328,no
A New Face Detection Method with GA-BP Neural Network,"In this paper, the BP neural network improved by the genetic algorithm (GA) is applied to the problem of human face detection. GA is used to optimize the initial weights of the BP neural network to make full use of its global optimization and local accurate searching of the BP algorithm. Matlab Software and its neural network toolbox are used to simulate and compute. The experiment results show that the GA-BP neural network has a good performance for face detection. Furthermore, compared with the conventional BP algorithm, the GA-BP learning algorithm has more rapid convergence and better assessment accuracy of detecting quality.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6040617,no
A self-healing architecture for web services based on failure prediction and a multi agent system,"Failures during web service execution may depend on a wide variety of causes. One of those is loss of Quality of Service (QoS). Failures during web service execution impose heavy costs on services-oriented architecture (SOA). In this paper, we seek to achieve a self-healing architecture to reduce failures in web services. We believe that failure prediction prevents the occurrence of failures and enhances the performance of SOA. The proposed architecture consists of three agents: Monitoring, Diagnosis and Repair. Monitoring agent measures quality parameters in communication level and predicts future values of quality parameter by Time Series Forecasting (TSF) with the help of Neural Network (NN). Diagnosis agent analyzes current and future QoS parameters values for diagnose web service failures. Based on its algorithm, the Diagnosis agent detects failures and faults in web services executions. Repair agent manages repair actions by using Selection agent.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6041420,no
TMM Appraisal Assistant Tool,"Software testing is an important component in the software development life cycle, which leads to have the high quality of software product. Therefore, software industry has focused on improving the testing process for better performance. The Testing Maturity Model (TMM) is one choice to apply for improving the testing process. It guides organization about framework of software testing. The TMM Assessment Model (TMM-AM) is a test process assessment model following the TMM. The TMM-AM consists of processes to assess test capability of organizations. Currently, each organization has various limitations such as cost, effort, time, and know-how. The assessment process lacks of tools to be performed. How to help them to improve their testing process is our key point. This paper proposes a supporting tool based on the TMM-AM which each organization can assess its testing process by itself. The tool can identify test maturity level for an organization and suggest procedures to reach its goal.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6041564,no
A Design and Implementation of a Terrestrial Magnetism and Acceleration Sensor Device for Worker's Motion Tracing System,"Guarantee of quality on industrial products is based on confirm whether materials, parts assembly and processing satisfy regulated standards or not. In some assemble process, it is hard to confirm the regulations are satisfied after once a work for the process have completed. For example, to fix a part using some screws, order to fasten the screws is regulated as a standard procedure to warrant accuracy. However, if screws are fastened in wrong order, the part will be fixed, however, incorrect order cannot be detected once this work have completed. We have long term experiment in a fuel tank attaching process in an automobile assembly factory. In this experiment, reliability of a sensor device we used was not enough. Also, to apply our system to other processes, improvement of reliability of the sensor is required. So we designed and implemented new sensor hardware. In this paper, we describe our new developed terrestrial magnetism sensor.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6041971,no
Next-generation massively parallel short-read mapping on FPGAs,"The mapping of DNA sequences to huge genome databases is an essential analysis task in modern molecular biology. Having linearized reference genomes available, the alignment of short DNA reads obtained from the sequencing of an individual genome against such a database provides a powerful diagnostic and analysis tool. In essence, this task amounts to a simple string search tolerating a certain number of mismatches to account for the diversity of individuals. The complexity of this process arises from the sheer size of the reference genome. It is further amplified by current next-generation sequencing technologies, which produce a huge number of increasingly short reads. These short reads hurt established alignment heuristics like BLAST severely. This paper proposes an FPGA-based custom computation, which performs the alignment of short DNA reads in a timely manner by the use of tremendous concurrency for reasonable costs. The special measures to achieve an extremely efficient and compact mapping of the computation to a Xilinx FPGA architecture are described. The presented approach also surpasses all software heuristics in the quality of its results. It guarantees to find all alignment locations of a read in the database while also allowing a freely adjustable character mismatch threshold. On the contrary, advanced fast alignment heuristics like Bowtie and Maq can only tolerate small mismatch maximums with a quick deterioration of the probability to detect existing valid alignments. The performance comparison with these widely used software tools also demonstrates that the proposed FPGA computation achieves its guaranteed exact results in very competitive time.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6043268,no
Design of a high performance FPGA based fault injector for real-time safety-critical systems,"Fault injection methods have long been used to assess fault tolerance and safety. However, many conventional fault injection methods face significant shortcomings, which hinder their ability to execute fault injections on target real-time safety-critical systems. We demonstrate a novel fault injection system implemented on a commercial Field-Programmable Gate Array board. The fault injector is unobtrusive to the target system as it utilizes only standardized On-Chip-Debugger (OCD) interfaces present on most current processors. This effort resulted in faults being injected orders of magnitude faster than by utilizing a commercial OCD debugger, while incorporating novel features such as concurrent injection of faults into distinct target processors. The effectiveness of this high performance fault injector was successfully demonstrated on a tightly synchronized commercial real-time safety-critical system used in nuclear power applications.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6043278,no
Temporal aspects of scoring in the user based quality evaluation of HD video,The paper deals with the temporal properties of a scoring session when assessing the subjective quality of full HD video sequences using the continuous video quality tests. The performed experiment uses a modification of the standard test methodology described in ITU-R Rec. BT.500. It focuses on the reactive times and the time needed for the user ratings to stabilize at the beginning of a video sequence.,2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6043659,no
Networked fault detection of nonlinear systems,"This paper addresses Fault Detection (FD) problem of a class of nonlinear systems which are monitored via the communications networks. A sufficient condition is derived which guarantees exponential mean-square stability of the proposed nonlinear NFD systems in the presence of packet drop, quantization error and unwanted exogenous inputs such as disturbance and noise. A Linear Matrix Inequality (LMI) is obtained for the design of the fault detection filter parameters. Finally, the effectiveness of the proposed NFD technique is extensively assessed by using an experimental testbed that has been built for performance evaluation of such systems with the use of IEEE 802.15.4 Wireless Sensor Networks (WSNs) technology. An algorithm is presented to handle floating point calculus when connecting the WSNs to the engineering design softwares such as Matlab.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6044434,no
Detecting and diagnosing application misbehaviors in on-demand virtual computing infrastructures,"Numerous automated anomaly detection and application performance modeling and management tools are available to detect and diagnose faulty application behavior. However, these tools have limited utility in `on-demand' virtual computing infrastructures because of the increased tendencies for the applications in virtual machines to migrate across un-comparable hosts in virtualized environments and the unusually long latency associated with the training phase. The relocation of the application subsequent to the training phase renders the already collected data meaningless and the tools need to re-initiate the learning process on the new host afresh. Further, data on several metrics need to be correlated and analyzed in real time to infer application behavior. The multivariate nature of this problem makes detection and diagnosis of faults in real time all the more challenging as any suggested approach must be scalable. In this paper, we provide an overview of a system architecture for detecting and diagnosing anomalous application behaviors even as applications migrate from one host to another and discuss a scalable approach based on Hotelling's T<sup>2</sup> statistic and MYT decomposition. We show that unlike existing methods, the computations in the proposed fault detection and diagnosis method is parallelizable and hence scalable.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6045060,no
Interactive requirements validation for reactive systems through virtual requirements prototype,"Adequate requirements validation can prevent errors from propagating into later development phases, and eventually improve the quality of software systems. However, validating natural language requirements is often difficult and error-prone. An effective means of requirements validation for embedded software systems has been to build a working model of the requirements in the form of a physical prototype that stakeholders can interact with. However, physical prototyping can be costly, and time consuming, extending the time it takes to obtain and implement stakeholder feedback. We have developed a requirements validation technique, called Virtual Requirements Prototype (VRP), that reduces cost and stakeholder feedback time by allowing stakeholders to validate embedded software requirements through the interaction with a virtual prototype.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6045361,no
Streamlining scenario modeling with Model-Driven Development: A case study,"Scenario modeling can be realized through different perspectives. In UML, scenarios are often modeled with activity models, in an early stage of development. Later, sequence diagrams are used to detail object interactions. The migration from activity diagrams to sequence diagrams is a repetitive and error-prone task. Model-Driven Development (MDD) can help streamlining this process, through transformation rules. Since the information in the activity model is insufficient to generate the corresponding complete sequence model, manual refinements are required. Our goal is to compare the relative effort of building the sequence diagrams manually with that of building them semi-automatically. Our results show a decrease in the number of operations required to build and refine the sequence model of approximately 64% when using MDD, when compared to the manual approach.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6045367,no
Probabilistic fault detection and handling algorithm for testing stability control systems with a drive-by-wire vehicle,"This paper presents a probabilistic fault detection and handling algorithm (PFDH) for redundant and deterministic X-by-wire systems. The algorithm is specifically designed to guarantee safe operation of an experimental drive-by-wire vehicle used as test platform and development tool in research projects focusing on vehicle dynamics. The required flexibility of the overall system for use as a test bed influences significantly the redundancy structure of the onboard network. A black box approach to integrate newly developed user algorithms is combined with a hot-standby architecture controlled by PFDH. This way, functional redundancy for basic driving operations can be achieved despite unknown software components. PFDH is based on monitoring multiple criteria over time, including vehicle dynamics and relative error probabilities of hard- and software components provided by experts or statistical data.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6045395,no
Evaluating the use of model-based requirements verification method: A feasibility study,"Requirements engineering is one of the most important and critical phases in the software development life cycle, and should be carefully performed to build high quality and reliable software. However, requirements are typically gathered through various sources and represented in natural language (NL), making requirements engineering a difficult, fault prone, and a challenging task. To address this challenge, we propose a model-based requirements verification method called NLtoSTD, which transforms NL requirements into a state transition diagram (STD) that can be verified through automated reasoning. This paper analyzes the effect of NLtoSTD method in improving the quality of requirements. To do so, we conducted an empirical study at North Dakota State University in which the participants employed the NLtoSTD method during the inspection of requirement documents to identify the amibiguities and incompleteness of requirements. The experiment results show that the proposed method is capable of finding ambiguities and missing functionalities in a set of NL requirements, and provided us with insights and feedback to improve the method. The results are promising and have motivated the refinement of NLtoSTD method and future empirical evaluation.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6046248,no
Assessing think-pair-square in distributed modeling of use case diagrams,"In this paper, we propose a new method for the modeling of use case diagrams in the context of global software development. It is based on think-pair-square, a widely used cooperative method for active problem solving. The validity of the developed technology (i.e., the method and its supporting environment) has been assessed through two controlled experiments. In particular, the experiments have been conducted to compare the developed technology with a brainstorming session based on face-to-face interaction. The comparison has been performed with respect to the time needed to model use case diagrams and the quality of the produced models. The data analysis indicates a significant difference in favor of the brainstorming session for the time, with no significant impact on the requirements specification.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6046249,no
Assesing the understandability of collaborative systems requirements notations: An empirical study,"As for single user systems, a proper specification of software requirements is a very important issue to achieve the quality of the collaborative systems. Nevertheless, many of these requirements are from a non-functional nature because are related to the user's need of being aware of other users, that is, the workspace awareness. In order to model these special kind of requirements, CSRML, an extension of i* has been proposed. In this paper, we present a controlled experiment to assess the understandability of this notation compared to i*. The specification of two different systems was used as experimental material and undergraduate students of Computer Science with an average of two years experience in Requirements Engineering were the experimental subjects.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6046250,no
Precise is better than light a document analysis study about quality of business process models,"Business process modelling is often used in the initial phases of traditional software development to reduce faulty requirements and as starting point for building SOA based applications. Often, modellers produce business process models without following recognized guidelines and opt for light models where nodes representing the actions are simply decorated with natural language text. The potential consequence of this practice is that the quality of built business process models may be low. In this paper, we propose a method based on manual transformations to detect flaws in light business process models expressed as activity diagrams. Using that method we have executed a document analysis study with 14 business process models taken by books and websites. Preliminary results of this study show that almost all the analysed business process models contain errors and style violations (precisely 92% of them).",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6046257,no
A Framework to Manage Knowledge from Defect Resolution Process,"This paper presents a framework for the management, the processing and the reuse, of information relative to defects. This framework is based on the fact that each defect triggers a resolution process in which information about the detected incident (i.e. the problem) and about the applied protocol to resolve it (i.e. the solution) is collected. These different types of information are the cornerstone of the optimization of corrective and preventive processes for new defects. Experimentations show that our prototype provides a very satisfactory quality of results with good performances.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6046949,no
Critical-Path-Guided Interactive Parallelisation,"With the prevalence of multi-core processors, it is essential that legacy programs are parallelised effectively and efficiently. However, compilers have not been able to automatically extract sufficient parallelism in general programs. One of the major reasons, we argue, is that algorithms are often implemented sequentially in a way that unintentionally precludes efficient parallelisation. As manual parallelisation is usually tedious and error-prone, we propose a profiling-based interactive approach to program parallelisation, by presenting a tool-chain with two main components: Embla 2, a dependence-profiler that estimates the amount of task-level parallelism in programs, and Woolifier, a source-to-source transformer that uses Embla 2's output to parallelise programs using Wool, a Cilk-like API, to express parallelism. Based on profiled dependences, our tool-chain (i) performs an automatic best-effort parallelisation and (ii) presents remaining critical paths in a concise graphical form to the programmer, who can then quickly locate and refactor parallelism bottlenecks. Using case studies from the SPEC CPU 2000 benchmarks, we demonstrate how this tool-chain enables us to efficiently parallelise legacy sequential programs, achieving significant speed-ups on commodity multi-core processors.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6047052,no
Virtual Machine Provisioning Based on Analytical Performance and QoS in Cloud Computing Environments,"Cloud computing is the latest computing paradigm that delivers IT resources as services in which users are free from the burden of worrying about the low-level implementation or system administration details. However, there are significant problems that exist with regard to efficient provisioning and delivery of applications using Cloud-based IT resources. These barriers concern various levels such as workload modeling, virtualization, performance modeling, deployment, and monitoring of applications on virtualized IT resources. If these problems can be solved, then applications can operate more efficiently, with reduced financial and environmental costs, reduced under-utilization of resources, and better performance at times of peak load. In this paper, we present a provisioning technique that automatically adapts to workload changes related to applications for facilitating the adaptive management of system and offering end-users guaranteed Quality of Services (QoS) in large, autonomous, and highly dynamic environments. We model the behavior and performance of applications and Cloud-based IT resources to adaptively serve end-user requests. To improve the efficiency of the system, we use analytical performance (queueing network system model) and workload information to supply intelligent input about system requirements to an application provisioner with limited information about the physical infrastructure. Our simulation-based experimental results using production workload models indicate that the proposed provisioning technique detects changes in workload intensity (arrival pattern, resource demands) that occur over time and allocates multiple virtualized IT resources accordingly to achieve application QoS targets.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6047198,no
Intelligent agent based micro grid control,"Massive interconnection of power network has posed an operational challenge. The concept of intelligent control for regulating the power network variables has been realized. The intelligent agent based control can be a solution in today's power network to maintain the dynamics such as adequate power balance along with quality voltage under changing system conditions such as load and power injection. The technology with multi-agent intelligent control may be main module of Smart Grid architecture. This paper presents a concept of multi-agent intelligent grid control. A case study has been done to demonstrate the functionality in Matlab-Simulink environment. The multi-agent system is implemented by using an open source agent building toolkit Java Agent Development framework (JADE). Finally, both micro grid simulation and multi-agent system are connected together via MACSimJX toolbox. The simulation results indicate that proposed multi-agent system may facilitate the seamless transition from grid connected to an island mode when upstream outages are detected. This reveals the intelligence of multi-agent system for controlling the micro grid operation.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6049007,no
A declarative approach to hardening services against QoS vulnerabilities,"The Quality of Service (QoS) in a distributed service-oriented application can be negatively affected by a variety of factors. Network volatility, hostile exploits, poor service management, all can prevent a service-oriented application from delivering its functionality to the user. This paper puts forward a novel approach to improving the reliability, security, and availability of service-oriented applications. To counter service vulnerabilities, a special service detects vulnerabilities as they emerge at runtime, and then hardens the applications by dynamically deploying special components. The novelty of our approach lies in using a declarative framework to express both vulnerabilities and hardening strategies in a domain-specific language, independent of the service infrastructure in place. Thus, our approach will make it possible to harden service-oriented applications in a disciplined and systematic fashion.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6049034,no
Safe software processing by concurrent execution in a real-time operating system,"The requirements for safety-related software systems increases rapidly. To detect arbitrary hardware faults, there are applicable coding mechanism, that add redundancy to the software. In this way it is possible to replace conventional multi-channel hardware and so reduce costs. Arithmetic codes are one possibility of coded processing and are used in this approach. A further approach to increase fault tolerance is the multiple execution of certain critical parts of software. This kind of time redundancy is easily realized by the parallel processing in an operating system. Faults in the program flow can be monitored. No special compilers, that insert additional generated code into the existing program, are required. The usage of multi-core processors would further increase the performance of such multi-channel software systems. In this paper we present the approach of program flow monitoring combined with coded processing, which is encapsulated in a library of coded data types. The program flow monitoring is indirectly realized by means of an operating system.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6049061,no
On human analyst performance in assisted requirements tracing: Statistical analysis,"Assisted requirements tracing is a process in which a human analyst validates candidate traces produced by an automated requirements tracing method or tool. The assisted requirements tracing process splits the difference between the commonly applied time-consuming, tedious, and error-prone manual tracing and the automated requirements tracing procedures that are a focal point of academic studies. In fact, in software assurance scenarios, assisted requirements tracing is the only way in which tracing can be at least partially automated. In this paper, we present the results of an extensive 12 month study of assisted tracing, conducted using three different tracing processes at two different sites. We describe the information collected about each study participant and their work on the tracing task, and apply statistical analysis to study which factors have the largest effect on the quality of the final trace.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6051649,no
Simulating and optimising design decisions in quantitative goal models,Making decisions among a set of alternative system designs is an essential activity of requirements engineering. It involves evaluating how well each alternative satisfies the stakeholders' goals and selecting one alternative that achieves some optimal tradeoffs between possibly conflicting goals. Quantitative goal models support such activities by describing how alternative system designs - expressed as alternative goal refinements and responsibility assignments - impact on the levels of goal satisfaction specified in terms of measurable objective functions. Analyzing large numbers of alternative designs in such models is an expensive activity for which no dedicated tool support is currently available. This paper takes a first step towards providing such support by presenting automated techniques for (i) simulating quantitative goal models so as to estimate the levels of goal satisfaction contributed by alternative system designs and (ii) optimising the system design by applying a multi-objective optimisation algorithm to search through the design space. These techniques are presented and validated using a quantitative goal model for a well-known ambulance service system.,2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6051653,no
NSTX Power Supply configuration control upgrade,"The National Spherical Torus Experiment (NSTX) is in its second decade of operation at PPPL. NSTX has a total of 15 coil systems (which include the coils, their dedicated power supplies and associated auxiliary equipment) that create and control the plasma per the experimental objectives. Each coil system is individually controllable via the NSTX Power Supply Real Time Controller (PSRTC) software code written in C language. The NSTX has great flexibility in both the configuration of its coil system and in the operating envelope afforded by the connected power supplies. To ensure proper operation and to minimize the probability of lost runtime due to system faults, the project has developed a procedure that governs system configuration. The Integrated System Test Procedure (ISTP-001) documents the NSTX machine parameters, experiment configuration limits, machine protection settings and device settings. This paper will describe calculations for the ISTP 001 methodology and system protection settings; record keeping of the various configuration revisions and the upgrade in progress to improve readability and calculation capabilities.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6052335,no
On performance of combining methods for three-node half-duplex cooperative diversity network,"We analysis the performance of the ad-hoc network with a base station, a mobile and a third station acting as a relay. Three combining methods for the Amplify-and-Forward (AF) protocol and the Decode-and-Forward (DF) protocol are compared. Simulations indicate that the Amplifyand-Forward (AF) protocol beats the Decode-and-Forward (DF) protocol under all these three combining methods. To combine the incoming signals the channel quality should be estimated as well as possible, more estimation accuracy requires more resource. A very simple combining method can obtain the performance compared with that by optimal combining methods approximately. At the same time, all three combining methods for both diversity protocols can achieve the maximum diversity order.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6057427,no
Automatic measurement of electrical parameters of signal relays,"The manufacturing process of Metal to Carbon relays used in railway signaling systems for configuring various circuits of signals / points / track circuits etc. consists of seven phases from raw material to finished goods. To ensure in-process quality, the electrical parameters are measured manually after each stage. Manual measurement process is tedious, error prone and involves lot of time, effort and manpower. Besides, it is susceptible to manipulation and may lead to inferior quality products being passed, either due to deliberation or due to malefic intentions. Due to erroneous measurement of electrical parameters, the functional reliability of relays is adversely affected. To enhance the trustworthiness of measurement of electrical parameters & to make the process faster, an automated measurement system having proprietary application software and a testing jig attachment has been developed. When the relay is fixed on the testing jig, the software scans all the relay contacts and measures all the electrical parameters viz. operating voltage / current, contact resistance, release voltage / current, coil resistance etc. The results are displayed on the computer screen and stored in a database file.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6057433,no
An integrated Automatic Test Generation and executing system,"This paper presents an integrated Automatic Test Generation (ATG) and Automatic Test Executing/Equipment (ATE) system for complex boards. We developed an ATG technique called Behavior-Based Automatic Test Generation technique (namely BBATG). BBATG uses the device behavior fault model and represents a circuit board as interconnection of devices. A behavior of a device is a set of functions with timing relations on its in/out pins. When used for a digital circuit board test generation, BBATG utilizes device behavior libraries to drive behavior error signals and sensitize paths along one or multiple vectors so that a heavy and complicated iterating process can be avoided for sequential circuit test deductions. We have developed a complete set of test executing software and test supporting hardware for the ATE which can use the BBATG generated test data directly to detect behavior faults and diagnose faults at the device level for complex circuit boards. In addition, we have proposed and implemented useful technique, especially Design For Testability (DFT) [1][2] application technique on the integrated system, so the test generating/executing for complex boards with VLSI can be further simplified and optimized.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6058726,no
Comparing software design for testability to hardware DFT and BIST,"Software is replacing hardware whenever possible, and this trend is increasing. Software faults are every bit as pervasive and difficult to deal with as hardware faults. Debugging software faults is manual, time consuming, often elusive and since they affect all systems deployed, most often they are critical. Design for Debugging would ensure that a software package can be readily debugged for any software fault. A comprehensive software test, however, is intended to eliminate the need for ad hoc debugging and ideally all bugs (we call software faults) would be caught and identified by the software test. Thus, it is imperative that the software community adopt means to ensure that software components are designed in a way that will detect and isolate software faults. This requirement is familiar to designers of hardware systems. Could the discipline of hardware design for testability (DFT) and Built-In [Self] Test (BIST) apply to software design for testability? The purpose of this paper is to discuss how many of the testability requirements and techniques for hardware DFT can be applied to software.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6058776,no
Software tools: A key component in the successful implementation of the ATML standards,"This paper examines the IEEE Automatic Test Markup Language (ATML) family of standards and some of the impediments which must be overcome to successfully implement these standards. The paper specifically focuses on how software tools can help alleviate these issues and increase the benefits of using these new standards in Automatic Test System (ATS) related applications. The ATML standards provide a common exchange format for test data adhering to the Extensible Markup Language (XML) standard. ATML promises to provide interoperability between tools and multiple test platforms through the standardization of common test related data. The ATML standards have now been published through the IEEE Standards Coordinating Committee 20 (SCC20) committee and are beginning to exhibit considerable interest in the ATS community and are now a requirement on some new Department of Defense (DoD) ATS programs. Different aspects of ATML related tools shall be discussed such as ATML Development tools which assist in the generation of ATML compliant instance files, new ATS related tools which use ATML data in their applications and the modification of existing ATS tools to utilize ATML Data. This paper also examines the work in progress of a Small Business Innovative Research (SBIR) Naval Air Systems Command (NAVAIR) sponsored program to develop ATML and test diagram tools. Utilizing ATML standards without the benefit of tools can be a labor-intensive, error-prone process, and requires an intimate knowledge of the ATML and XML standards. Employing the ATML standards on ATS programs promises to significantly reduce costs and schedule; the use of software tools are a key component in the success of these implementations and will help promote the use of ATML throughout the test industry.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6058788,no
Risk minimization in modernization projects of plant automation  A knowledge-based approach by means of semantic web technologies,"In high-wage countries the number of Greenfield projects for plant automation is decreasing. In contrast to this, plant modernization becomes more and more important. The estimation of the costs for a re-engineering of the existing plant automation is an error-prone task which has to be done in the bidding phase of a modernization project. This article describes a knowledge-based approach to reduce the risk potential in the bidding phase of plant modernization projects. Based on a concept for rough plant modeling in CAEX and technologies of the semantic web a concept for a software assistance system is presented.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6058987,no
Resolving state inconsistency in distributed fault-tolerant real-time dynamic TDMA architectures,"State consistency in safety-critical distributed systems is mandatory for synchronizing distributed decisions as found in dynamic time division multiple access (TDMA) schedules in the presence of faults. A TDMA schedule that supports networked systems making decisions at run time is sensitive to transient faults, because stations can make incorrect local decisions at run time and cause state inconsistency and collisions. We refer to this type of TDMA schedule as a dynamic TDMA schedule. Faulty decisions are especially undesirable for safety-critical systems with hard real-time constraints. Hence, real-time communication schedules must have the capability of detecting state inconsistency within a fixed amount of time. In this paper, we show through experimentation that state inconsistency is a real problem, and we propose a solution for resolving state inconsistency in TDMA schedules.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6059022,no
Large-Scale Simulator for Global Data Infrastructure Optimization,"IT infrastructures in global corporations are appropriately compared with nervous systems, in which body parts (interconnected datacenters) exchange signals (request responses) in order to coordinate actions (data visualization and manipulation). A priori inoffensive perturbations in the operation of the system or the elements composing the infrastructure can lead to catastrophic consequences. Downtime disables the capability of clients reaching the latest versions of the data and/or propagating their individual contributions to other clients, potentially costing millions of dollars to the organization affected. The imperative need of guaranteeing the proper functioning of the system not only forces to pay particular attention to network outages, hot-objects or application defects, but also slows down the deployment of new capabilities, features and equipment upgrades. Under these circumstances, decision cycles for these modifications can be extremely conservative, and be prolonged for years, involving multiple authorities across departments of the organization. Frequently, the solutions adopted are years behind state-of-the art technologies or phased out compared to leading research on the IT infrastructure field. In this paper, the utilization of a large-scale data infrastructure simulator is proposed, in order to evaluate the impact of """" what if"""" scenarios on the performance, availability and reliability of the system. The goal is to provide data center operators a tool that allows understanding and predicting the consequences of the deployment of new network topologies, hardware configurations or software applications in a global data infrastructure, without affecting the service. The simulator was constructed using a multi-layered approach, providing a granularity down to the individual server component and client action, and was validated against a downscaled version of the data infrastructure of a Fortune 500 company.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6061065,no
The Fading Boundary between Development Time and Run Time,"Summary form only given. Modern software applications are often embedded in highly dynamic contexts. Changes may occur in the requirements, in the behavior of the environment in which the application is embedded, in the usage profiles that characterize interactive aspects. Changes are difficult to predict and anticipate, and are out of control of the application. Their occurrence, however, may be disruptive, and therefore the software must also change accordingly. In many cases, changes to the software cannot be handled off-line, but require the software to self react by adapting its behavior dynamically, in order to continue to ensure the required quality of service. The big challenge in front of us is how to achieve the necessary degrees of flexibility and dynamism required in this setting without compromising dependability of the applications. To achieve dependability, a software engineering paradigm shift is needed. The traditional focus on quality, verification, models, and model transformations must extend from development time to run time. Not only software development environments (SDEs) are important for the software engineer to develop better software. Feature-full Software Run-time Environments (SREs) are also key. SREs must be populated by a wealth of functionalities that support on-line monitoring of the environment, inferring significant changes through machine learning methods, keeping models alive and updating them accordingly, reasoning on models about requirements satisfaction after changes occur, and triggering model-driven self-adaptive reactions, if necessary. In essence, self adaptation must be grounded on the firm foundations provided by formal methods and tools in a seamless SDE SRE setting. The talk discusses these concepts by focusing on non-functional requirements-reliability and performance-that can be expressed in quantitative probabilistic requirements. In particular, it shows how probabilistic model checking can help reasoning about re- - quirements satisfaction and how it can be made run-time efficient. The talk reports on some results of research developed within the SMScom project, funded by the European Commission, Programme IDEAS-ERC, Project 227977 (http://www.erc-smscom.org/).",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6061095,no
A Novel Energy-Aware Fault Tolerance Mechanism for Wireless Sensor Networks,"Sensors in a Wireless Sensor Network (WSN) are prone to failure, due to energy depletion, hardware failures, etc. Fault tolerance is one of the critical issues in WSNs. The existing fault tolerant mechanisms either consume significant extra energy to detect and recover from the failures or need to use additional hardware and software resource. In this paper, we propose a novel energy-aware fault tolerance mechanism for WSN, called Informer Homed Routing (IHR). In our IHR, the non cluster head nodes limit and select the target of their data transmission. Therefore, it consumes less energy. Our experiments show that our proposed protocol can dramatically reduce energy consumption, compared to two existing protocols, LEACH and DHR.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6061298,no
"Digital microfluidic biochips: Functional diversity, More than Moore, and cyberphysical systems","Summary form only given. The 2010 International Technology Roadmap for Semiconductors (ITRS) predicted that bio-medical chips will soon revolutionize the healthcare market. These bio-medical chips should be able to sense and actuate, store and manipulate data, and transmit information. To realize such bio-medical chips, the integration of embedded systems and microfluidics inevitably leads to a new research dimension for More than Moore and beyond. This tutorial will introduce attendees to the emerging technology of digital microfluidics, which is poised to play a key role in the transformation of healthcare and the interplay between biochemistry and embedded systems. Advances in droplet-based digital microfluidics have led to the emergence of biochip devices for automating laboratory procedures in biochemistry and molecular biology. These devices enable the precise control of nanoliter-volume droplets of biochemical samples and reagents. Therefore, integrated circuit (IC) technology can be used to transport and transport chemical payload in the form of micro/nanofluidic droplets. As a result, non-traditional biomedical applications and markets (e.g., high-throughout DNA sequencing, portable and point-of-care clinical diagnostics, protein crystallization for drug discovery), and fundamentally new uses are opening up for ICs and systems. However, continued growth (and larger revenues resulting from technology adoption by pharmaceutical and healthcare companies) depends on advances in chip integration and design-automation tools. In particular, design-automation tools are needed to ensure that biochips are as versatile as the macro-labs that they are intended to replace. This is therefore an opportune time for the semiconductor industry and circuit/system designers to make an impact in this emerging field. This tutorial offers attendees an opportunity to bridge the semiconductor ICs/systems industry with the biomedical and pharmaceutic- l industries. The audience will see how a biochip compiler can translate protocol descriptions provided by an end user (e.g., a chemist or a nurse at a doctor's clinic) to a set of optimized and executable fluidic instructions that will run on the underlying digital microfluidic platform. Testing techniques will be described to detect faults after manufacture and during field operation. Sensor integration and close coupling between the underlying hardware and the control software in a cyberphysical framework will also be described. A number of case studies based on representative assays and laboratory procedures will be interspersed in appropriate places throughout the tutorial. Commercial devices and advanced prototypes from the major company in this market segment (Advanced Liquid Logic, Inc.) will be described, and ongoing activity on newborn screening using digital microfluidic biochips at several large hospitals in Illinois will be highlighted. The topics covered in the tutorial include the following: 1) Technology and application drivers: Motivation and background, actuation methods, electrowetting and digital microfluidics, review of micro-fabrication processes, applications to biochemistry, medicine, and laboratory procedures. 2) System-level design automation: Synthesis techniques: scheduling of fluidic operations, resource binding (mapping of operations to on-chip resources), module placement. 3) Physical-level design automation: droplet routing, defect tolerance, chip-level design, and design of pin-constrained biochips. 4) Testing and design-for-testability: Defects, fault modeling, test planning, reconfiguration techniques, sensor integration and cyberphysical system design.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062311,no
Spectrum-Based Health Monitoring for Self-Adaptive Systems,"An essential requirement for the operation of self-adaptive systems is information about their internal health state, i.e., the extent to which the constituent software and hardware components are still operating reliably. Accurate health information enables systems to recover automatically from (intermittent) failures in their components through selective restarting, or self-reconfiguration. This paper explores and assesses the utility of Spectrum-based Fault localisation (SFL) combined with automatic health monitoring for self-adaptive systems. Their applicability is evaluated through simulation of online diagnosis scenarios, and through implementation in an adaptive surveillance system inspired by our industrial partner. The results of the studies performed confirm that the combination of SFL with online monitoring can successfully provide health information and locate problematic components, so that adequate self-* techniques can be deployed.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6063492,no
A New Approach for a Fault Tolerant Mobile Agent System,"Improving the survivability of mobile agents in the presence of agent server failures with unreliable underlying networks is a challenging issue. In this paper, we address a fault tolerance approach of deploying cooperating agents to detect agent failures as well as to recover services in mobile agent systems. Three types of agents are involved, which are the actual agent, the supervisor agent and the replicas. We introduce a failure detection and recovery protocol by employing a message-passing mechanism among these three kinds of agents. Different failure scenarios and their corresponding recovery procedures are discussed. We choose Fatomas approach as a basic method. Message complexity of this approach is O (m<sup>2</sup>). Cooperative agents haven't been considered in this approach. We are going to improve this message complexity in a system of cooperative agents.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6063556,no
On-line detection of stator and rotor faults occurring in induction machine diagnosis by parameters estimation,"The authors propose a diagnosis method for on-line interturns short-circuit windings and broken bars detection by parameters estimation. For predictive detection, Kalman filtering algorithm has been adapted to take into account the on-line parameters deviations in faulty case. Experimental rig is used to validate the on-line identification of stator default. Within the framework of the rotor defects diagnosis, it is difficult to conduct experimental tests to validate the on-line identification of such default. For this reason, one propose an on-line technique to detect rotor broken bars. This technique was validated by using a finite element software (Flux2D). Estimation results show a good agreement and demonstrate the possibility of on-line stator and rotor faults detection.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6063609,no
Implicit SIP proxy overload detection mechanism based on response behavior,"Detecting overload in telecommunication networks is an important goal, because it allows to react on it and to reduce traffic smartly to ensure constant user experienced quality of service. This applies not only to media as voice and video, but also to signaling, which is responsible for setting up these media. An overloaded signaling network increases the response delay and reduces the successfully processed service requests and therefore the revenue. We propose an implicit overload detection mechanism for SIP networks that allows detecting overloaded components by their response behavior. This mechanism realizes maximum throughput with marginal response delay increase in the case of congestion without protocol modifications or extensions and therefore ensures proper operation of SIP networks in case of overload.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064383,no
SIP proxy high-load detection by continuous analysis of response delay values,"The 3GPP has chosen the Session Initiation Protocol as signalling protocol for the IP Multimedia Subsystem; therefore, it is expected that telecom operators will widely use it for their systems. SIP relies on an underlying transport protocol, like, e.g., TCP, UDP or SCTP. In the case of UDP, SIP has to ensure itself that messages will be reliably delivered. For this purpose, retransmission timers within the SIP transaction state machine are used. On the other hand, retransmissions can lead to congestion or even cause a congestion collapse if traffic load becomes too high, and services of the operator may become unavailable. It is therefore important to detect an imminent collapse and to act accordingly in order to keep the users' perceived quality high. We propose to continuously measure response delay values to detect high-load situations that can lead to a collapse in order to be able to reduce the traffic load early enough for avoiding congestion situations. We validate this approach by means of dedicated simulations.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064440,no
From Boolean to quantitative synthesis,"Motivated by improvements in constraint-solving technology and by the increase of routinely available computational power, partial-program synthesis is emerging as an effective approach for increasing programmer productivity. The goal of the approach is to allow the programmer to specify a part of her intent imperatively (that is, give a partial program) and a part of her intent declaratively, by specifying which conditions need to be achieved or maintained. The task of the synthesizer is to construct a program that satisfies the specification. As an example, consider a partial program where threads access shared data without using any synchronization mechanism, and a declarative specification that excludes data races and deadlocks. The task of the synthesizer is then to place locks into the program code in order for the program to meet the specification. In this paper, we argue that quantitative objectives are needed in partial-program synthesis in order to produce higher-quality programs, while enabling simpler specifications. Returning to the example, the synthesizer could construct a naive solution that uses one global lock for shared data. This can be prevented either by constraining the solution space further (which is error-prone and partly defeats the point of synthesis), or by optimizing a quantitative objective that models performance. Other quantitative notions useful in synthesis include fault tolerance, robustness, resource (memory, power) consumption, and information flow.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064521,no
A Method for Software Process Capability / Maturity Models Customization to Specific Domains,"Software Process Capability/Maturity Models (SPCMMs) are repositories of best practices for software processes suitable for assessing and/or improving processes in software intensive organizations. Each software development domain, however, presents particular needs, which has led to the tendency of SPCMMs customization for specific domains, which has often been undertaken in an unsystematic way. This paper presents a method for the customization of SPCMMs for specific domains, developed based on standards development, process modeling and knowledge engineering techniques as well as experiences reported in the literature. Formative evaluations of the method have taken place through case studies and summative evaluation has been conducted through an Expert Panel. The observed results reveal early evidence that the method is suitable for SPCMMs customization.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065127,no
Contributions and Perspectives in Architectures of Software Testing Environments,"Producing high quality software systems has been one of the most important software development concerns. In this perspective, Software Architecture and Software Testing are two important research areas that have contributed in that direction. The attention given to the software architecture has played a significant role in determining the success of software systems. Otherwise, software testing has been recognized as a fundamental activity for assuring the software quality; however, it is an expensive, error-prone, and time consuming activity. For this reason, a diversity of testing tools and environments has been developed; however, they have been almost always designed without an adequate attention to their evolution, maintenance, reuse, and mainly to their architectures. Thus, this paper presents our main contributions to systematize the development of testing tools and environments, aiming at improving their quality, reuse, and productivity. In particular, we have addressed architectures for software testing tools and environments and have also developed and made available testing tools. We also state perspectives of research in this area, including open research issues that must be treated, considering the unquestionable relevance of testing automation to the testing activity.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065147,no
On the Interplay between Structural and Logical Dependencies in Open-Source Software,"Structural dependencies have long been explored in the context of software quality. More recently, software evolution researchers have investigated logical dependencies between artifacts to assess failure-proneness, detect design issues, infer code decay, and predict likely changes. However, the interplay between these two kinds of dependencies is still obscure. By mining 150 thousand commits from the Apache Software Foundation repository and employing object-oriented metrics reference values, we concluded that 91% of all established logical dependencies involve non-structurally related artifacts. Furthermore, we found some evidence that structural dependencies do not lead to logical dependencies in most situations. These results suggest that dependency management methods and tools should rely on both kinds of dependencies, since they represent different dimensions of software evolvability.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065158,no
Analyzing Refactorings on Software Repositories,"Currently analysis of refactoring in software repositories is either manual or only syntactic, which is time-consuming, error-prone, and non-scalable. Such analysis is useful to understand the dynamics of refactoring throughout development, especially in multi-developer environments, such as open source projects. In this work, we propose a fully automatic technique to analyze refactoring frequency, granularity and scope in software repositories. It is based on SAFEREFACTOR, a tool that analyzes transformations by generating tests to detect behavioral changes - it has found a number of bugs in refactoring implementations within some IDEs, such as Eclipse and Netbeans. We use our technique to analyze five open source Java projects (JHotDraw, ArgoUML, SweetHome 3D, HSQLDB and jEdit). From more than 40,723 software versions, 39 years of software development, 80 developers and 1.5 TLOC, we have found that: 27% of changes are refactorings. Regarding the refactorings, 63,83% are Low level, and 71% have local scope. Our results indicate that refactorings are frequently applied before likely functionality changes, in order to better prepare design for accommodating additions.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065160,no
A Model for the Evaluation of Educational Games for Teaching Software Engineering,"Teaching software engineering through educational games is expected to have several benefits. Various games have already been developed in this context, yet there is still a lack of assessment models to measure the real benefits and quality of these educational resources. This article presents the development of a model for assessing the quality of educational games for teaching software engineering. The model has been systematically derived from literature and evaluated in terms of its applicability, usefulness, validity and reliability through a series of case studies, applying educational board games in software engineering courses. Early results indicate that the model can be used to assess the aspects of motivation, user experience and learning of educational SE games.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065163,no
Tuning Static Data Race Analysis for Automotive Control Software,"Implementation of concurrent software systems is difficult and error-prone. Race conditions can cause intermittent failures, which are rarely found during testing. In safety-critical applications, the absence of race conditions should be demonstrated before deployment of the system. Several static analysis techniques to show the absence of data races are known today. In this paper, we report on our experiences with a static data race detector. We define a basic analysis based on classical lockset analysis and present three enhancements to that algorithm. We evaluate and compare the effectiveness of the basic and enhanced analysis algorithms empirically for an automotive embedded system. We find that the number of warnings could be reduced by more than 40% and that the ratio of true positives per total number of warnings could be doubled.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065196,no
Are the Clients of Flawed Classes (Also) Defect Prone?,"Design flaws are those characteristics of design entities (e.g., methods, classes) which make them harder to maintain. Existing studies show that classes revealing particular design flaws are more change and defect prone than the other classes. Since various collaborations are found among the instances of classes, classes are not isolated within the source code of object-oriented systems. In this paper we investigate if classes using classes revealing design flaws are more defect prone than classes which do not use classes revealing design flaws. We detect four design flaws in three releases of Eclipse and investigate the relation between classes that use/do not use flawed classes and defects. The results show that classes that use flawed classes are defect prone and this does not depend on the number of the used flawed classes. This findings show a new type of correlation between design flaws and defects, bringing evidence related to an increased likelihood of exhibiting defects for classes that use classes revealing design flaws. Based on the provided evidence, practitioners are advised once again about the negative impact design flaws have at a source code level.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065198,no
Distortion Measurement for Automatic Document Verification,"Document forgery detection is important as techniques to generate forgeries are becoming widely available and easy to use even for untrained persons. In this work, two types of forgeries are considered: forgeries generated by re-engineering a document and forgeries that are generated using scanning and printing a genuine document. An unsupervised approach is presented to automatically detect forged documents of these types by detecting the geometric distortions introduced during the forgery process. Using the matching quality between all pairs of documents, outlier detection is performed on the summed matching quality to identify the tampered document. Quantitative evaluation is done on two public data sets, reporting a true positive rate from to 0.7 to 1.0.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065321,no
"Risk Management in Global Software Development Projects: Challenges, Solutions, and Experience","The benefits of using globally distributed sites for the development, maintenance, and operation of software-based systems and services are obvious. But global development also bears large risks. What seems at first to be economically reasonable often proves to be too expensive. Missing adjustment of communication and processes between different sites and insufficient knowledge of suitable management practices and organizational skills often lead to insufficient product quality. Global development and maintenance processes are difficult to control and often additional costs arise, especially for quality assurance and follow-up activities. Mastering global software projects requires, on the one hand, suitable tailoring of software development tasks and their distribution to different sites based on multiple criteria (not only cost!). On the other hand, appropriate process and management practices need to be established. Quantitative models can then be used to assess cost, schedule goals, and quality risks. I will introduce fundamental techniques for the establishment of well-understood and manageable distributed development processes and discuss different ways for managing risks. Based on a technique for splitting up development tasks and a multidisciplinary decision model for """"smart"""" task distribution to different sites, I will demonstrate how distributed development processes can be organized in a productive way. This will be done by using examples from industry projects. Additionally, I will present upcoming topics such as cloud-supported global software development or the software factory, a research and development infrastructure at the University of Helsinki that supports systematic testing of novel distributed development techniques. Finally I will show how, in order to avoid global development risks, the application of fundamental software engineering principles must be emphasized.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065574,no
Integrating Early V&V Support to a GSE Tool Integration Platform,"The ever-growing market pressure and complex products demand high quality work and effectiveness from software practitioners. This relates also for the methods and tools they use for the development of software-intensive systems. Validation and verification (V&V) are the cornerstones of the overall quality of a system. By performing efficient V&V activities to detect defects during the early phases of development, the developers are able to save time and effort required for fixing them. Tool support is available for all types of V&V activities, especially testing, model checking, syntactic verification, and inspection. In distributed development the role of tools is even more relevant than in single-site development, and tool integration is often imperative for ensuring the effectiveness of work. In this paper, we discuss how a tool integration framework was extended to support early V&V activities via continuous integrations. We find that integrating early V& V supporting tools is feasible and useful, and makes a tool integration framework even more beneficial.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065585,no
Requirement Development Life Cycle: The Industry Practices,"Requirements engineering activities act as a backbone of software development. The more efforts devoted during requirements engineering activities guarantee a better software product. Appropriate selection of requirements has been a challenge for software industry. This selection will increase the probability of success of the software product. Each year many cases are registered against companies for not fulfilling product requirements appropriately. The product failure mostly depends on, either by missing important requirements or capturing irrelevant requirements. SDLC consists of stages where software starts from scratch to a refined product. Requirements Development Life cycle (RDLC) consists of stages where requirements gets initiated, raised, refined, forcefully changed, implemented and validated. The processes to capture requirements vary industry to industry. This paper presents several requirements engineering processes used during the development of requirements, in industry. These processes will identify appropriate requirements and develop a quality product within budget on time. These practices are captured within the Pakistan software industry. This paper also explains the motivations for selecting particular methods, within company, during requirements development and the results associated with it. The processes captured in this paper, from different companies, can be an education for software industry.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065612,no
Wiring harness assembly detection system based on image processing technology,"Describes a image processing based wiring harness assembly technique detection system. Real-time collection on detected harness's color, location and other information by image processing, to determine whether the errors mounted harness by image processing software. First through the spatial filter method, using the median filter to eliminate noise and interference of the collected images in order to obtain a clear picture. Then using Otsu method for image thresholding, and Sobel operator to detect edge to determine the wiring harness of the exact location in the image. Finally, use color matching algorithm to match and compare the image to finalize the wiring harness eligibility. Testing accuracy of the system has greatly improved than the previous manual detection. Improving test efficiency, product quality and productivity.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6066493,no
Correct Implementation of Open Real-Time Systems,"Correct and efficient implementation of open real-time systems is still a costly and error-prone process. We present a rigorous model-based implementation method of such systems based on the use of two models: (i) an abstract model representing the interactions between the environment and the application and its timing behavior without considering any execution platform, (ii) a physical model representing the behavior of the abstract model running on a given platform by taking into account execution times. We define an Execution Engine that performs the online computation of schedules for a given application so as to meet its timing constraints. In contrast to standard even-driven programming techniques, our method allows static analysis and online checking of essential properties such as time-safety and time-robustness. We implemented the Execution Engine for BIP programs and validated our method for a module of an autonomous rover.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6068323,no
Autonomic Configuration Adaptation Based on Simulation-Generated State-Transition Models,"Configuration management is a complex task, even for experienced system administrators, which makes self-managing systems a particularly desirable solution. This paper describes a novel contribution to self-managing systems, including an autonomic configuration self-optimization methodology. Our solution involves a systematic simulation method that develops a state-transition model of the behavior of a service-oriented system in terms of its configuration and performance. At run time, the system's behavior is monitored and classified in one of the model states. If this state may lead to futures that violate service level agreements, the system configuration is changed toward a safer future state. Similarly, a satisfactory state that is over-provisioned may be transitioned to a more economical satisfactory state. Aside from the typical benefits of self-optimization, our approach includes an intuitive, explainable decision model, the ability to predict the future with some accuracy avoiding trial-and-error, offline training, and the ability to improve the model at run-time. We demonstrate this methodology in an experiment where Amazon EC2 instances are added and removed to handle changing request volumes to a real service-oriented application. We show that a knowledge base generated entirely in simulation can be used to make accurate changes to a real-world application.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6068341,no
E-Quality: A graph based object oriented software quality visualization tool,"Recently, with increasing maintenance costs, studies on software quality are becoming increasingly important and widespread because high quality software means more easily maintainable software. Measurement plays a key role in quality improvement activities and metrics are the quantitative measurement of software design quality. In this paper, we introduce a graph based object-oriented software quality visualization tool called """"E-Quality"""". E-Quality automatically extracts quality metrics and class relations from Java source code and visualizes them on a graph-based interactive visual environment. This visual environment effectively simplifies comprehension and refactoring of complex software systems. Our approach assists developers in understanding of software quality attributes by level categorization and intuitive visualization techniques. Experimental results show that the tool can be used to detect software design flaws and refactoring opportunities.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6069454,no
Turn-to-turn fault detection in transformers using negative sequence currents,"This paper presents a new, simple and efficient protection technique which is based on negative sequence currents. Using this protection technique, it is possible to detect minor internal turn-to-turn faults in power transformers. Also, it can differentiate between internal and external faults. The discrimination is achieved by comparing the phase shift between two phasors of total negative sequence current. The new protection technique has been studied via an extensive simulation study using PSCAD<sup></sup>/EMTDCTM software in a three-phase power system and also has been compared with a traditional differential algorithm. The results indicate that the new technique can provide a fast and sensitive approach for identifying minor internal turn-to-turn faults in power transformers.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6070187,no
Computing indicators of creativity,"Currently, the most common measurement of creativity is based on tests of divergence. These creativity tests include divergent thinking, divergent feeling, etc. In most cases the evaluation criteria is a subjective appraisal by a trained """"rater"""" to assess the amount of divergence from the """"norm"""" a particular submitted solution has to a presented or discovered task. The larger the divergence from the standard, the more creative the solution is. Although the quality and quantity of the solutions to the task must be considered, divergence from the accepted """"norm"""" is a significant indicator of creativity. Using the current model for showing creative divergence, a method for evaluating the divergence of programming solutions as compared to the standard tutorial solution, in order to indicate creativity should be in line with current creativity research. Instead of subjective """"rater evaluations"""" a method of calculating numerical divergence from programming solutions was devised. This method was employed on three separate class conditions and yielded three separate divergence patterns, indicating that the divergence calculation appears to demonstrate, not only that creativity can be shown to exist in programming solutions, but that the calculation is sensitive enough to differentiate between different class learning conditions of the same teacher. So based on the idea that creativity can be shown through divergence in thinking and feeling, it stands to reason that creativity in programming could be revealed through a similar divergence to a standard norm through calculating the divergence to that norm. Consequently, this divergence calculation method shows promising indicators to inform the measurement of creativity within programming and possibly other scientific areas.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6070407,no
Reliable State Retention-Based Embedded Processors Through Monitoring and Recovery,"State retention power gating and voltage-scaled state retention are two effective design techniques, commonly employed in embedded processors, for reducing idle circuit leakage power. This paper presents a methodology for improving the reliability of embedded processors in the presence of power supply noise and soft errors. A key feature of the method is low cost, which is achieved through reuse of the scan chain for state monitoring, and it is effective because it can correct single and multiple bit errors through hardware and software, respectively. To validate the methodology, ARM<sup></sup> CortexTM-M0 embedded microprocessor (provided by our industrial project partner) is implemented in field-programmable gate array and further synthesized using 65-nm technology to quantify the cost in terms of area, latency, and energy. It is shown that the proposed methodology has a small area overhead (8.6%) with less than 4% worst-case increase in critical path and is capable of detecting and correcting both single bit and multibit errors for a wide range of fault rates.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6071090,no
Detection and classification device for malaria parasites in thick-blood films,"In Thailand, malaria diagnosis still relies primarily on microscopic examination of Giemsa-stained thick and thin blood films. However, the method requires vigorously trained technicians to correctly identify the disease, and is known to be error-prone due to human fatigue. The limited number of such technicians further reduces the effectiveness of the attempt to control malaria. Thus, this project aims to develop an automated system to identify and analyze parasite species on thick blood films by image analysis techniques. The system comprises two main components: (1) Image acquisition unit and (2) Image analysis module. In our work, we have developed an image acquisition system that can be easily mounted on most conventional light microscopes. It automatically controls the movement of microscope stage in 3-directional planes. The vertical adjustment (focusing) can be made in a nanometer range (7-9 nm). Images are acquired with a digital camera that is installed at the top of microscope. The captured images are analyzed by our image analysis software which utilizes the state-of-the-art algorithms to detect and identify malaria parasites.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6072791,no
A novel approach to sentence alignment from comparable corpora,This paper introduces a new technique to select candidate sentences for alignment from bilingual comparable corpora. Tests were done utilizing Wikipedia as a source for bilingual data. Our test languages are English and Chinese. A high quality of sentence alignment is illustrated by a machine translation application.,2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6072842,no
Impact of attribute selection on defect proneness prediction in OO software,"Defect proneness prediction of software modules always attracts the developers because it can reduce the testing efforts as well as software development time. In the current context, with the piling up of constraints like requirement ambiguity and complex development process, developing fault free reliable software is a daunting task. To deliver reliable software, software engineers are required to execute exhaustive test cases which become tedious and costly for software enterprises. To ameliorate the testing process one can use a defect prediction model so that testers can focus their efforts on defect prone modules. Building a defect prediction model becomes very complex task when the number of attributes is very large and the attributes are correlated. It is not easy even for a simple classifier to cope with this problem. Therefore, while developing a defect proneness prediction model, one should always be careful about feature selection. This research analyzes the impact of attribute selection on Naive Bayes (NB) based prediction model. Our results are based on Eclipse and KC1 bug database. On the basis of experimental results, we show that careful combination of attribute selection and machine learning apparently useful and, on the Eclipse data set, yield reasonable good performance with 88% probability of detection and 49% false alarm rate.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6075151,no
A Mobile Camera Tracking System Using GbLN-PSO with an Adaptive Window,"The availability of high quality and inexpensive video camera, as well as the increasing need for automated video analysis is leading towards a great deal of interest in numerous applications. However the video tracking systems is still having many open problems. Thus, some of research activities in a video tracking system are still being explored. Generally, most of the researchers are used a static camera in order to track an object motion. However, the use of a static camera system for detecting and tracking the motion of an object is only capable for capturing a limited view. Therefore, to overcome the above mentioned problem in a large view space, researcher may use several cameras to capture images. Thus, the cost will increases with the number of cameras. To overcome the cost increment a mobile camera is employed with the ability to track the wide field of view in an environment. Conversely, mobile camera technologies for tracking applications have faced several problems; simultaneous motion (when an object and camera are concurrently movable), distinguishing objects in occlusion, and dynamic changes in the background during data capture. In this study we propose a new method of Global best Local Neighborhood Oriented Particle Swarm Optimization (GbLN-PSO) to address these problems. The advantages of tracking using GbLN-PSO are demonstrated in experiments for intelligent human and vehicle tracking systems in comparison to a conventional method. The comparative study of the method is provided to evaluate its capabilities at the end of this paper.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6076367,no
"Fault Injection, A Fast Moving Target in Evaluations","Differential Fault Analysis has been known since 1996 (Dan Boneh, Richard A. DeMillo and Richard J. Lipton, """"The Bellcore Attack"""") [1]. Before that, the implementa tions of cryptographic functions were developed without the awareness of fault analysis attacks. The first fault injection set-ups produced single voltage glitches or single light flashes at a single location on the silicon. A range of countermeasures has been developed and applied in cryptographic devices since. But while the countermeasures against perturbation attacks were being developed, attack techniques also evolved. The accuracy of the timing was improved, multiple light flashes were used to circumvent double checks, perturbation attacks were being combined with side channels such as power consumption and detection methods developed to prevent chips from blocking after they detected the perturbation attempt. Against all these second generation attack methods new countermeasures were developed. This raised the level of security of secure microcontroller chips to a high level, especially compared to products of ten years ago. The certification schemes are mandating more and more advanced tests to keep secure systems secure in the future. One of the latest requirements is light manipulation test using power consumption waveform based triggering with multiple light flashes at multiple locations on the silicon. If attack scenarios that are as complicated as this one are in scope where will it end? The equipment necessary for the attack is expensive and special software is required. The perturbation attacks that are performed outside security labs and universities are of a different level.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6076468,no
Piggy-Backing Link Quality Measurements to IEEE 802.15.4 Acknowledgements,"In this paper we present an approach to piggyback link quality measurements to IEEE 802.15.4 acknowledgement frames by generating acknowledgements in software instead of relying on hardware support. We show that the software-generated ACKs can be sent meeting the timing constraints of IEEE 802.15.4. This allows for a standard conforming, energy neutral dissemination of link quality related information in IEEE 802.15.4 networks. This information is available at no cost when transmitting data and can be used as input for various schemes for adaptive transmission power control and to assess the current channel quality.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6076690,no
Dangers and Joys of Stock Trading on the Web: Failure Characterization of a Three-Tier Web Service,"Characterizing latent software faults is crucial to address dependability issues of current three-tier systems. A client should not have a misconception that a transaction succeeded, when in reality, it failed due to a silent error. We present a fault injection-based evaluation to characterize silent and non-silent software failures in a representative three-tier web service, one that mimics a day trading application widely used for benchmarking application servers. For failure characterization, we quantify distribution of silent and non-silent failures, and recommend low cost application-generic and application-specific consistency checks, which improve the reliability of the application. We inject three variants of null-call, where a callee returns null to the caller without executing business logic. Additionally, we inject three types of unchecked exceptions and analyze the reaction of our application. Our results show that 49% of error injections from null-calls result in silent failures, while 34% of unchecked exceptions result in silent failures. Our generic-consistency check can detect silent failures in null-calls with an accuracy as high as 100%. Non-silent failures with unchecked exceptions can be detected with an accuracy of 42% with our application-specific checks.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6076773,no
Analyzing Performance of Lease-Based Schemes under Failures,"Leases have proved to be an effective concurrency control technique for distributed systems that are prone to failures. However, many benefits of leases are only realized when leases are granted for approximately the time of expected use. Correct assessment of lease duration has proven difficult for all but the simplest of resource allocation problems. In this paper, we present a model that captures a number of lease styles and semantics used in practice. We consider a few performance characteristics for lease-based systems and analytically derive how they are affected by lease duration. We confirm our analytical findings by running a set of experiments with the OO7 benchmark suite using a variety of workloads and fault loads.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6076777,no
FastFIX: An approach to self-healing,"The EU FP7 FastFIX project tackles issues related to remote software maintenance. In order to achieve this, the project considers approaches relying on context elicitation, event correlation, fault-replication and self-healing. Self-healing helps systems return to a normal state after the occurrence of a fault or vulnerability exploitation has been detected. The problem is intuitively appealing as a way to automate the different maintenance type processes (corrective, adaptive and perfective) and forms an interesting area of research that has inspired many research initiatives. In this paper, we propose a framework for automating corrective maintenance and present its early stage development, based on software control principles. Our approach automates the engineering of self-healing systems as it does not require the system to be designed in a specific way. Instead it can be applied to legacy systems and automatically equips them with observation and control points. Moreover, the proposed approach relies on a sound control theory developed for Discrete Event Systems. Finally, this paper contributes to the field by introducing challenges for effective application of this approach to relevant industrial systems.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6078234,no
Assessing risk for network resilience,"Communication networks and the Internet, in particular, have become a critical infrastructure for daily life, business and governance. Various challenging conditions can render networks or parts thereof unusable, with severe consequences. Protecting a network from all possible challenges is infeasible because of monetary, hardware and software constraints. Hence, a methodology to measure the risk imposed by the various challenges threatening the system is a necessity. In this paper, we present a risk assessment process to identify the challenges with the highest potential impact to a network and its users. The result of this process is a prioritised list of challenges and associated system faults, which can guide network engineers towards the mechanisms that have to be built into the network to ensure network resilience, whilst meeting cost constraints. Furthermore, we discuss how outcomes from the intermediate steps of our risk assessment process can be used to inform network resilience design. A better understanding of these aspects and a way to determine reliable measures are open issues, and represent important new research items in the context of resilient and survivable networks.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6078858,no
Measurement methods for QoS in VoIP review,"In the last years there is a merging trend towards unified communication systems over IP protocols from desktop to handheld devices. However this trend brings forth the limited QoS control existing in these types of networks. The lack of cost-effective QoS strategies is felt negatively directly by the end user in terms of both communication quality and increasing costs. Therefore, this paper analyses which are the main methods available for measuring the QoS in VoIP networks for both audio and video calls and how neural networks can be used to predict the quality as perceived from the end user perspective.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6078907,no
The design of a software fault prone application using evolutionary algorithm,"Most of the current project management software's are utilizing resources on developing areas in software projects. This is considerably essential in view of the meaningful impact towards time and cost-effective development. One of the major areas is the fault proneness prediction, which is used to find out the impact areas by using several approaches, techniques and applications. Software fault proneness application is an application based on computer aided approach to predict the probability that the software contains faults. The application will uses object oriented metrics and count metrics values from open source software as input values to the genetic algorithm for generation of the rules to classify the software modules in the categories of Faulty and Non Faulty modules. At the end of the process, the result will be visualized using genetic algorithm applet, bar and pie chart. This paper will discussed the detail design of software fault proneness application by using genetic algorithm based on the object oriented approach and will be presented using the Unified Modeling Language (UML). The aim of the proposed design is to develop an automated tool for software development group to discover the most likely software modules to be high problematic in the future.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6079246,no
Low-Complexity Encoding Method for H.264/AVC Based on Visual Perception,"H.264/AVC standard achieved excellent encoding performance of at the cost of increased computational complexity and falling encoding speed. In order to overcome poor real-time encoding performance of H.264/AVC, aiming at computing redundancy, based on the integration of visual selective attention mechanism and low complexity encoding of information analysis and visual perception, making use of the distribution of motion vector and the relationship between the mode decision probability and the human visual attention a low-complexity H.264/AVC video coding scheme is implemented in this paper. The simulation results show that the approach encoding effectively resolves the conflict between coding accuracy and speed, saving about 80% coding time on average, which effectively maintains good video quality and overall improves the encoding performance of H.264/AVC.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6079599,no
Got Issues? Do New Features and Code Improvements Affect Defects?,"There is a perception that when new features are added to a system that those added and modified parts of the source-code are more fault prone. Many have argued that new code and new features are defect prone due to immaturity, lack of testing, as well unstable requirements. Unfortunately most previous work does not investigate the link between a concrete requirement or new feature and the defects it causes, in particular the feature, the changed code and the subsequent defects are rarely investigated. In this paper we investigate the relationship between improvements, new features and defects recorded within an issue tracker. A manual case study is performed to validate the accuracy of these issue types. We combine defect issues and new feature issues with the code from version-control systems that introduces these features, we then explore the relationship of new features with the fault-proneness of their implementations. We describe properties and produce models of the relationship between new features and fault proneness, based on the analysis of issue trackers and version-control systems. We find, surprisingly, that neither improvements nor new features have any significant effect on later defect counts, when controlling for size and total number of changes.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6079844,no
An Empirical Validation of the Benefits of Adhering to the Law of Demeter,"The Law of Demeter formulates the rule-of-thumb that modules in object-oriented program code should """"only talk to their immediate friends"""". While it is said to foster information hiding for object-oriented software, solid empirical evidence confirming the positive effects of following the Law of Demeter is still lacking. In this paper, we conduct an empirical study to confirm that violating the Law of Demeter has a negative impact on software quality, in particular that it leads to more bugs. We implement an Eclipse plugin to calculate the amount of violations of both the strong and the weak form of the law in five Eclipse sub-projects. Then we discover the correlation between violations of the law and the bug-proneness and perform a logistic regression analysis of three sub-projects. We also combine the violations with other OO metrics to build up a model for predicting the bug-proneness for a given class. Empirical results show that violations of the Law of Demeter indeed highly correlate with the number of bugs and are early predictor of the software quality. Based on this evidence, we conclude that obeying the Law of Demeter is a straight-forward approach for developers to reduce the number of bugs in their software.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6079847,no
Assessing Software Quality by Program Clustering and Defect Prediction,"Many empirical studies have shown that defect prediction models built on product metrics can be used to assess the quality of software modules. So far, most methods proposed in this direction predict defects by class or file. In this paper, we propose a novel software defect prediction method based on functional clusters of programs to improve the performance, especially the effort-aware performance, of defect prediction. In the method, we use proper-grained and problem-oriented program clusters as the basic units of defect prediction. To evaluate the effectiveness of the method, we conducted an experimental study on Eclipse 3.0. We found that, comparing with class-based models, cluster-based prediction models can significantly improve the recall (from 31.6% to 99.2%) and precision (from 73.8% to 91.6%) of defect prediction. According to the effort-aware evaluation, the effort needed to review code to find half of the total defects can be reduced by 6% if using cluster-based prediction models.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6079848,no
Modularization Metrics: Assessing Package Organization in Legacy Large Object-Oriented Software,"There exist many large object-oriented software systems consisting of several thousands of classes that are organized into several hundreds of packages. In such software systems, classes cannot be considered as units for software modularization. In such context, packages are not simply classes containers, but they also play the role of modules: a package should focus to provide well identified services to the rest of the software system. Therefore, understanding and assessing package organization is primordial for software maintenance tasks. Although there exist a lot of works proposing metrics for the quality of a single class and/or the quality of inter-class relationships, there exist few works dealing with some aspects for the quality of package organization and relationship. We believe that additional investigations are required for assessing package modularity aspects. The goal of this paper is to provide a complementary set of metrics that assess some modularity principles for packages in large legacy object-oriented software: Information-Hiding, Changeability and Reusability principles. Our metrics are defined with respect to object-oriented dependencies that are caused by inheritance and method call. We validate our metrics theoretically through a careful study of the mathematical properties of each metric.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6079866,no
ImpactScale: Quantifying change impact to predict faults in large software systems,"In software maintenance, both product metrics and process metrics are required to predict faults effectively. However, process metrics cannot be always collected in practical situations. To enable accurate fault prediction without process metrics, we define a new metric, ImpactScale. ImpactScale is the quantified value of change impact, and the change propagation model for ImpactScale is characterized by probabilistic propagation and relation-sensitive propagation. To evaluate ImpactScale, we predicted faults in two large enterprise systems using the effort-aware models and Poisson regression. The results showed that adding ImpactScale to existing product metrics increased the number of detected faults at 10% effort (LOC) by over 50%. ImpactScale also improved the predicting model using existing product metrics and dependency network measures.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080771,no
Identifying distributed features in SOA by mining dynamic call trees,"Distributed nature of web service computing imposes new challenges on software maintenance community for localizing different software features and maintaining proper quality of service as the services change over time. In this paper, we propose a new approach for identifying the implementation of web service features in a service oriented architecture (SOA) by mining dynamic call trees that are collected from distributed execution traces. The proposed approach addresses the complexities of SOA-based systems that arise from: features whose locations may change due to changing of input parameters; execution traces that are scattered throughout different service provider platforms; and trace files that contain interleaving of execution traces related to different concurrent service users. In this approach, we execute different groups of feature-specific scenarios and mine the resulting dynamic call trees to spot paths in the code of a service feature, which correspond to a specific user input and system state. This allows us to focus on a the implementation of a specific feature in a distributed SOA-based system for different maintenance tasks such as bug localization, structure evaluation, and performance analysis. We define a set of metrics to assess structural properties of a SOA-based system. The effectiveness and applicability of our approach is demonstrated through a case study consisting of two service-oriented banking systems.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080774,no
Structural conformance checking with design tests: An evaluation of usability and scalability,"Verifying whether a software meets its functional requirements plays an important role in software development. However, this activity is necessary, but not sufficient to assure software quality. It is also important to check whether the code meets its design specification. Although there exists substantial tool support to assure that a software does what it is supposed to do, verifying whether it conforms to its design remains as an almost completely manual activity. In a previous work, we proposed design tests - test-like programs that automatically check implementations against design rules. Design test is an application of the concept of test to design conformance checking. To support design tests for Java projects, we developed DesignWizard, an API that allows developers to write and execute design tests using the popular JUnit testing framework. In this work, we present a study on the usability and scalability of DesignWizard to support structural conformance checking through design tests. We conducted a qualitative usability evaluation of DesignWizard using the Think Aloud Protocol for APIs. In the experiment, we challenged eleven developers to compose design tests for an open-source software project. We observed that the API meets most developers' expectations and that they had no difficulties to code design rules as design tests. To assess its scalability, we evaluated DesignWizard's use of CPU time and memory consumption. The study indicates that both are linear functions of the size of software under verification.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080781,no
Graph-based detection of library API imitations,"It has been a common practice nowadays to employ third-party libraries in software projects. Software libraries encapsulate a large number of useful, well-tested and robust functions, so that they can help improve programmers' productivity and program quality. To interact with libraries, programmers only need to invoke Application Programming Interfaces (APIs) exported from libraries. However, programmers do not always use libraries as effectively as expected in their application development. One commonly observed phenomenon is that some library behaviors are re-implemented by client code. Such re-implementation, or imitation, is not just a waste of resource and energy, but its failure to abstract away similar code also tends to make software error-prone. In this paper, we propose a novel approach based on trace subsumption relation of data dependency graphs to detect imitations of library APIs for achieving better software maintainability. Furthermore, we have implemented a prototype of this approach and applied it to ten large real-world open-source projects. The experiments show 313 imitations of explicitly imported libraries with high precision average of 82%, and 116 imitations of static libraries with precision average of 75%.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080785,no
A probabilistic software quality model,"In order to take the right decisions in estimating the costs and risks of a software change, it is crucial for the developers and managers to be aware of the quality attributes of their software. Maintainability is an important characteristic defined in the ISO/IEC 9126 standard, owing to its direct impact on development costs. Although the standard provides definitions for the quality characteristics, it does not define how they should be computed. Not being tangible notions, these characteristics are hardly expected to be representable by a single number. Existing quality models do not deal with ambiguity coming from subjective interpretations of characteristics, which depend on experience, knowledge, and even intuition of experts. This research aims at providing a probabilistic approach for computing high-level quality characteristics, which integrate expert knowledge, and deal with ambiguity at the same time. The presented method copes with goodness functions, which are continuous generalizations of threshold based approaches, i.e. instead of giving a number for the measure of goodness, it provides a continuous function. Two different systems were evaluated using this approach, and the results were compared to the opinions of experts involved in the development. The results show that the quality model values change in accordance with the maintenance activities, and they are in a good correlation with the experts' expectations.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080791,no
Predicting post-release defects using pre-release field testing results,"Field testing is commonly used to detect faults after the in-house (e.g., alpha) testing of an application is completed. In the field testing, the application is instrumented and used under normal conditions. The occurrences of failures are reported. Developers can analyze and fix the reported failures before the application is released to the market. In the current practice, the Mean Time Between Failures (MTBF) and the Average usage Time (AVT) are metrics that are frequently used to gauge the reliability of the application. However, MTBF and AVT cannot capture the whole pattern of failure occurrences in the field testing of an application. In this paper, we propose three metrics that capture three additional patterns of failure occurrences: the average length of usage time before the occurrence of the first failure, the spread of failures to the majority of users, and the daily rates of failures. In our case study, we use data derived from the pre-release field testing of 18 versions of a large enterprise software for mobile applications to predict the number of post-release defects for up to two years in advance. We demonstrate that the three metrics complement the traditional MTBF and AVT metrics. The proposed metrics can predict the number of post-release defects in a shorter time frame than MTBF and AVT.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080792,no
Using source code metrics to predict change-prone Java interfaces,"Recent empirical studies have investigated the use of source code metrics to predict the change- and defect-proneness of source code files and classes. While results showed strong correlations and good predictive power of these metrics, they do not distinguish between interface, abstract or concrete classes. In particular, interfaces declare contracts that are meant to remain stable during the evolution of a software system while the implementation in concrete classes is more likely to change. This paper aims at investigating to which extent the existing source code metrics can be used for predicting change-prone Java interfaces. We empirically investigate the correlation between metrics and the number of fine-grained source code changes in interfaces of ten Java open-source systems. Then, we evaluate the metrics to calculate models for predicting change-prone Java interfaces. Our results show that the external interface cohesion metric exhibits the strongest correlation with the number of source code changes. This metric also improves the performance of prediction models to classify Java interfaces into change-prone and not change-prone.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080797,no
Industrial experiences with automated regression testing of a legacy database application,"This paper presents a practical approach and tool (DART) for functional black-box regression testing of complex legacy database applications. Such applications are important to many organizations, but are often difficult to change and consequently prone to regression faults during maintenance. They also tend to be built without particular considerations for testability and can be hard to control and observe. We have therefore devised a practical solution for functional regression testing that captures the changes in database state (due to data manipulations) during the execution of a system under test. The differences in changed database states between consecutive executions of the system under test, on different system versions, can help identify potential regression faults. In order to make the regression test approach scalable for large, complex database applications, classification tree models are used to prioritize test cases. The test case prioritization can be applied to reduce test execution costs and analysis effort. We report on how DART was applied and evaluated on business critical batch jobs in a legacy database application in an industrial setting, namely the Norwegian Tax Accounting System (SOFIE) at the Norwegian Tax Department (NTD). DART has shown promising fault detection capabilities and cost-effectiveness and has contributed to identify many critical regression faults for the past eight releases of SOFIE.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080803,no
A clustering approach to improving test case prioritization: An industrial case study,"Regression testing is an important activity for controlling the quality of a software product, but it accounts for a large proportion of the costs of software. We believe that an understanding of the underlying relationships in data about software systems, including data correlations and patterns, could provide information that would help improve regression testing techniques. We conjecture that if test cases have common properties, then test cases within the same group may have similar fault detection ability. As an initial approach to investigating the relationships in massive data in software repositories, in this paper, we consider a clustering approach to help improve test case prioritization. We implemented new prioritization techniques that incorporate a clustering approach and utilize code coverage, code complexity, and history data on real faults. To assess our approach, we have designed and conducted empirical studies using an industrial software product, Microsoft Dynamics Ax, which contains real faults. Our results show that test case prioritization that utilizes a clustering approach can improve the effectiveness of test case prioritization techniques.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080805,no
Code Hot Spot: A tool for extraction and analysis of code change history,"Commercial software development teams have limited time available to focus on improvements to their software. These teams need a way to quickly identify areas of the source code that would benefit from improvement, as well as quantifiable data to defend the selected improvements to management. Past research has shown that mining configuration management systems for change information can be useful in determining faulty areas of the code. We present a tool named Code Hot Spot, which mines change records out of Microsoft's TFS configuration management system and creates a report of hot spots. Hot spots are contiguous areas of the code that have higher values of metrics that are indicators of faulty code. We present a study where we use this tool to study projects at ABB to determine areas that need improvement. The resulting data have been used to prioritize areas for additional code reviews and unit testing, as well as identifying change prone areas in need of refactoring.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080806,no
Relating developers' concepts and artefact vocabulary in a financial software module,"Developers working on unfamiliar systems are challenged to accurately identify where and how high-level concepts are implemented in the source code. Without additional help, concept location can become a tedious, time-consuming and error-prone task. In this paper we study an industrial financial application for which we had access to the user guide, the source code, and some change requests. We compared the relative importance of the domain concepts, as understood by developers, in the user manual and in the source code. We also searched the code for the concepts occurring in change requests, to see if they could point developers to code to be modified. We varied the searches (using exact and stem matching, discarding stop-words, etc.) and present the precision and recall. We discuss the implication of our results for maintenance.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080808,no
"Precise detection of un-initialized variables in large, real-life COBOL programs in presence of unrealizable paths","Using variables before assigning any values to them are known to result in critical failures in an application. Few compilers warn about the use of some, but not all uses of un-initialized variables. The problem persists, especially in COBOL systems, due to lack of reliable program analysis tools. A critical reason is the presence of large number of control flow paths due to the use of un-structured constructs of the language. We present the problems faced by one of our big clients in his large, COBOL based software system due to the use of un-initialized variables. Using static data and control-flow analysis to detect them, we observed large number of false positives (imprecision) introduced due to the unrealizable paths in the un-structured COBOL code. We propose a solution to address the realizability issue. The solution is based on the summary based function analysis, which is adapted for COBOL Paragraphs and Sections, to handle the perform-through and fall-through control-flow, and is significantly engineered to scale for large programs (single COBOL program extending to tens of thousands of lines). Using this technique, we noted very large reduction, 45% on an average, in the number of false positives for the un-initialized variables.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080812,no
Source code comprehension strategies and metrics to predict comprehension effort in software maintenance and evolution tasks - an empirical study with industry practitioners,"The goal of this research was to assess the consistency of source code comprehension strategies and comprehension effort estimation metrics, such as LOC, across different types of modification tasks in software maintenance and evolution. We conducted an empirical study with software development practitioners using source code from a small paint application written in Java, along with four semantics-preserving modification tasks (refactoring, defect correction) and four semantics-modifying modification tasks (enhancive and modification). Each task has a change specification and corresponding source code patch. The subjects were asked to comprehend the original source code and then judge whether each patch meets the corresponding change specification in the modification task. The subjects recorded the time to comprehend and described the comprehension strategies used and their reason for the patch judgments. The 24 subjects used similar comprehension strategies. The results show that the comprehension strategies and effort estimation metrics are not consistent across different types of modification tasks. The recorded descriptions indicate the subjects scanned through the original source code and the patches when trying to comprehend patches in the semantics-modifying tasks while the subjects only read the source code of the patches in semantics-preserving tasks. An important metric for estimating comprehension efforts of the semantics-modifying tasks is the Code Clone Subtracted from LOC(CCSLOC), while that of semantics-preserving tasks is the number of referred variables.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080814,no
Evidence-based software process recovery: A post-doctoral view,"Software development processes are often viewed as a panacea for software quality: prescribe a process and a quality project will emerge. Unfortunately this has not been the case, as practitioners are prone to push against processes that they do not perceive as helpful, often much to the dismay of stakeholders such as their managers. Yet practitioners still tend to follow some sort of software development processes regardless of the prescribed processes. Thus if a team wants to recover the software development processes of a project or if team is trying to achieve a certification such as ISO9000 or CMM, the team will be tasked with describing their development processes. Previous research has tended to focus on modifying existing projects in order to extract process related information. In contrast, our approach of software process recovery attempts to analyze software artifacts extracted from software repositories in order to infer the underlying software development processes visible within these software repositories.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080831,no
Practical combinatorial (t-way) methods for detecting complex faults in regression testing,"Regression testing can be among the most challenging of software assurance tasks because program changes often introduce faults, including unexpected interactions among different parts of the code. Unanticipated interactions may also occur when software is modified for a new platform. Techniques such as pairwise testing are not sufficient for detecting these faults, because empirical evidence shows that some errors are triggered only by the interaction of three, four, or more parameters. However, new algorithms and tools make it possible to generate tests that cover complex combinations of values (2-way to 6-way), or to analyze existing test suites and automatically generate tests that provide combinatorial coverage. The key advantage of this approach is that it produces better testing using a fraction of the tests required by other methods.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080840,no
A novel software-based defect-tolerance approach for application-specific embedded systems,"Traditional approaches for improving yield are based on the use of hardware redundancy (HR), and their benefits are limited for high defect densities due to increasing layout complexities and diminishing return effects. This research is based on an observation that completely correct operation of user programs can be guaranteed while using chips with one or more unrepairable memory modules if software-level techniques satisfy two condistions: (1) defects only affect a few memory cells rather than cause malfunction for the entire memory module, and (2) either we do not use any part of the memory affected by the un-repaired defect, or we do use the affected part, but only in a manner that does not excite the un repaired defect to cause errors. This paper proposes a software based defect-tolerance (SBDT) approach in combination with HR to utilize defective memory chips for application-specific systems. The proposed approach requires known and fixed program and information about defective locations for each memory module, hence this paper focuses on SoCs and other application-specific systems built around processors, such as DSP and graphics processors. We model an application program and defective memory copies as described next.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6081441,no
A case study on the application of an artefact-based requirements engineering approach,"[Background:] Nowadays, industries are facing the problem that the Requirements Engineering (RE) process is highly volatile, since it depends on project influences from the customer's domain or from process models used. Artefact-based approaches promise to provide guidance in the creation of consistent artefacts in volatile project environments, because these approaches concentrate on the artefacts and their dependencies, instead of prescribing processes. Yet missing, however, is empirical evidence on the advantages of applying artefact-based RE approaches in real projects. [Aim:] We developed a customisable artefact-based RE approach for the domain of business information systems. Our goal is to investigate the advantages and limitations of applying this customisable approach in an industrial context. [Method:] We conduct a case study with our artefact-based RE approach and its customisation procedure. For this, we apply it at a software development project at Siemens following the steps of the customisation procedure. We assess our approach in direct comparison with the previously used RE approach considering possible improvements in the process and in the quality of the produced artefacts. [Results:] We show that our approach is flexible enough to respond to the individual needs in the analysed project environment. Although the approach is not rated to be more productive, we find an improvement in the syntactic and the semantic quality of the created artefacts. [Conclusions:] We close a gap in the RE literature by giving empirical evidence on the advantages of artefact orientation in RE in an industrial setting.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6083168,no
An empirical validation of FindBugs issues related to defects,"Background: Effective use of bug finding tools promise to speed up the process of source code verification and to move a portion of discovered defects from testing to coding phase. However, many problems related to their usage, especially the large number of false positives, could easily hinder the potential benefits of such tools. Aims: Assess the percentage and type of issues of a popular bugfinding tool (FindBugs) that are actual defects. Method: We analyzed 301 Java Projects developed at a university with FindBugs, collecting the issues signalled on the source code. Afterwards, we checked the precision of issues with information on changes, we ranked and validated them using both manual inspection and validation with tests failures. Results: We observed that a limited set of issues have high precision and conversely we identified those issues characterized by low precision. We compared findings first with our previous experiment and then to related work: results are consistent with both of them. Conclusions: Since our and other empirical studies demonstrated that few issues are related to real defects with high precision, developers could enable only them (or prioritize), reducing the information overload of FindBugs and having the possibility to discover defects earlier. Furthermore, the technique presented in the paper can be adopted to other tools on a code base with tests to find issues with high precision that can be checked on code in production to find defects earlier.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6083173,no
Empirical data-based modeling of teaching material sharing network dynamics,"Teaching material sharing networks (TMSN) may enrich teachers teaching capacity and quality through sharing. To effectively manage a network and its evolution, managers have to characterize members' behaviors such as joining/leaving the network (membership) and uploading/downloading teaching materials (sharing) and network state dynamics of membership, teaching material (TM) quantity and quality. The challenge presented in this paper is to design a methodology for modeling individual behaviors and network dynamics to predict network evolution based on empirical data of the network. SCTNet, a TMSN among elementary school teachers, serves as an exemplary network for designing the modeling methodology. Novelty of the design has three folds. i) Probabilistic individual behaviors are modeled to capture the individual difference. In particular, the features of probabilities with respect to states from data are slow start, fast growth, and saturation, thus the Bass diffusion model is adopted to model how the probabilities are affected by network states ii) How network states evolve over time with respect to the current states and individual behavior probabilities is then described by a set of Bass-Model embedded difference equations. iii) Because of limited empirical data for modeling, a Quasi-bootstrap based nonlinear least square (NLS) method is used to estimate Bass model parameters of the behavior probabilities along with the network evolution. User behaviors and network dynamics thus obtained were validated via an agent-based simulation (ABS), and the results observe that the accuracy of membership evolution reproduced by ABS matches the empirical data of SCTNet by more than 95%. This proven modeling accuracy shed the light for a better TMSN network management.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6083658,no
Detecting emergent behavior in distributed systems using an ontology based methodology,"Lack of central control makes the design of distributed software systems a challenging task because of possible unwanted behavior at runtime, commonly known as emergent behavior. Developing methodologies to detect emergent behavior prior to the implementation stage of the system can lead to huge savings in time and cost. However manual review of requirements and design documents for real-life systems is inefficient and error prone; thus automation of analysis methodologies is considered greatly beneficial. This paper proposes the utilization of an ontology-based approach to analyze system requirements expressed by a set of message sequence charts (MSC). This methodology involves building a domain-specific ontology of the system, and examines the requirements based on this ontology. The advantages of this approach in comparison with other methodologies are its consistency and increased level of automation. The effectiveness of this approach is explained using a case study of an IntelliDrive system.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6084038,no
Predicting software defects: A cost-sensitive approach,"Find software defects is a complex and slow task which consumes most of the development budgets. In order to try reducing the cost of test activities, many researches have used machine learning to predict whether a module is defect-prone or not. Defect detection is a cost-sensitive task whereby a misclassification is more costly than a correct classification. Yet, most of the researches do not consider classification costs in the prediction models. This paper introduces an empirical method based in a COCOMO (COnstructive COst MOdel) that aims to assess the cost of each classifier decision. This method creates a cost matrix that is used in conjunction with a threshold-moving approach in a ROC (Receiver Operating Characteristic) curve to select the best operating point regarding cost. Public data sets from NASA (National Aeronautics and Space Administration) IV&V (Independent Verification & Validation) Facility Metrics Data Program (MDP) are used to train the classifiers and to provide some development effort information. The experiments are carried out through a methodology that complies with validation and reproducibility requirements. The experimental results have shown that the proposed method is efficient and allows the interpretation of the classifier performance in terms of tangible cost values.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6084055,no
Calculating the strength of ties of a social network in a semantic search system using hidden Markov models,"The Web of information has grown to millions of independently evolved decentralized information repositories. Decentralization of the web has advantages such as no single point of failure and improved scalability. Decentralization introduces challenges such as ontological, communication and negotiation complexity. This has given rise to research to enhance the infrastructure of the Web by adding semantic to the search systems. In this research we view semantic search as an enabling technique for the general Knowledge Management (KM) solutions. We argue that, semantic integration, semantic search and agent technology are fundamental components of an efficient KM solution. This research aims to deliver a proof-of-concept for semantic search. A prototype agent-based semantic search system supported by ontological concept learning and contents annotation is developed. In this prototype, software agents, deploy ontologies to organize contents in their corresponding repositories; improve their own search capability by finding relevant peers and learn new concepts from each other; conduct search on behalf of and deliver customized results to the users; and encapsulate complexity of search and concept learning process from the users. A unique feature of this system is that the semantic search agents form a social network. We use Hidden Markov Model (HMM) to calculate the tie strengths between agents and their corresponding ontologies. The query will be forwarded to those agents with stronger ties and relevant documents are returned. We have shown that this will improve the search quality. In this paper, we illustrate the factors that affect the strength of the ties and how these factors can be used by HMM to calculate the overall tie strength.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6084089,no
Design and creation of Dysarthric Speech Database for development of QoLT software technology,"In this paper we will introduce the work of creation of a speech database to develop speech technology for disabled persons, which has been done as part of a national program to help better life for Korean people. We will report about the creation of speech database of a total of 160 persons: prompting items, designs, etc. for the creation of a database which is needed to develop an embedded key-word spotting speech recognition system tailored for the persons disabled in articulation. The created database is being used by the technology development team in the national program to study the phonetic characteristics of the different types of disabled persons, develop the automatic method to assess degrees of disability, investigate the phonetic features of speech of the disabled, and design and implement the software prototype for personal embedded speech recognition systems adapted to the disabled persons.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6085978,no
Intelligent alarm management,"An ergonomic problem for the plant operators has appeared in the modern electronic control systems, in which configure an alarm is very easy. We present a methodology and an intelligent software tool to manage alarms and make early fault detection and diagnosis in industrial processes, integrating three techniques to detect and diagnose faults. The three techniques use available information in industrial environments: The alarms of the electronic control system; the fault knowledgebase of the process, formulated in terms of rules; and a simplified model used to detect disturbances in the process. A prototype in a Fluid Catalytic Cracking process is shown.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6086852,no
Towards identifying OS-level anomalies to detect application software failures,"The next generation of critical systems, namely complex Critical Infrastructures (LCCIs), require efficient runtime management, reconfiguration strategies, and the ability to take decisions on the basis of current and past behavior of the system. Anomaly-based detection, leveraging information gathered at Operating System (OS) level (e.g., number of system call errors, signals, and holding semaphores in the time unit), seems to be a promising approach to reveal online application faults. Recently an experimental campaign to evaluate the performance of two anomaly detection algorithms was performed on a case study from the Air Traffic Management (ATM) domain, deployed under the popular OS used in the production environment, i.e., Red Hat 5 EL. In this paper we investigate the impact of the OS and the monitored resources on the quality of the detection, by executing experiments on Windows Server 2008. Experimental results allow identifying which of the two operating systems provides monitoring facilities best suited to implement the anomaly detection algorithms that we have considered. Moreover numerical sensitivity analysis of the detector parameters is carried out to understand the impact of their setting on the performance.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6088494,no
Remote diagnostics and performance analysis for a wireless sensor network,"Wireless Sensor Networks (WSNs) comprise embedded sensor nodes that operate autonomously in a multi-hop topology. The challenges are unreliable wireless communications, harsh environment, and limited energy and computation resources. To ensure the desired level of service, it is essential to diagnose performance issues e.g. due to low quality links or energy depletion. This paper presents remote diagnostics and performance analysis that comprise self-diagnostics on embedded sensor nodes, the remote collection of diagnostics, and the design of a diagnostics analysis tool. Unlike the related proposals, our approach allows correcting detected problems by identifying the reasons for misbehavior. The diagnostics is verified with a practical WSN implementation. It has only a small overhead, less than 18 B/min per node in the implementation, allowing the use in bandwidth and energy constrained WSNs.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6088951,no
A multi agent system model for evaluating quality service of clinical engineering department,"Biomedical technology is strategically important to the operational effectiveness of healthcare facilities. As a consequence, clinical engineers have become an essential figure in hospital environment: their role in maintenance, support, evaluation, integration, assessment of new, advanced and complex technologies in point of view of patient safety and cost reduction is become inalienable. For this reason, nations have begun to establish Clinical Engineering Department, but, unfortunately, in a very diversified and fragmented way. So, a tool able to evaluate and improve the quality of current services is needed. Hence, this work builds a model that acts as a reference tool in order to assess the quality of an existing Clinical Engineering Department, underlining its defaulting aspects and suggesting improvements.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6090284,no
Mosaicing of optical microscope imagery based on visual information,"Tools for high-throughput high-content image analysis can simplify and expedite different stages of biological experiments, by processing and combining different information taken at different time and in different areas of the culture. Among the most important in this field, image mosaicing methods provide the researcher with a global view of the biological sample in a unique image. Current approaches rely on known motorized x-y stage offsets and work in batch mode, thus jeopardizing the interaction between the microscopic system and the researcher during the investigation of the cell culture. In this work we present an approach for mosaicing of optical microscope imagery, based on local image registration and exploiting visual information only. To our knowledge, this is the first approach suitable to work on-line with non-motorized microscopes. To assess our method, the quality of resulting mosaics is quantitatively evaluated through on-purpose image metrics. Experimental results show the importance of model selection issues and confirm the soundness of our approach.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6091522,no
SOC HW/SW co-verification technology for application of FPGA test and diagnosis,"Process of configuration and fault scan is required to be repeated many times before all resources of a FPGA-under-test are tested and diagnosed. Both FPGA test system and test schemes have been studied and presented in the keynote. Construction of the in-house developed FPGA test system is based on SOC HW/SW co-verification technology. Algorithms for FPGA test and diagnosis covering all FPGA resources such as, configurable logic blocks (CLBs), interconnect resources (IRs), input/output blocks (IOBs), wide edge decoder, et al with minimum configuration numbers are also discussed. Not only multiple faults in FPGA can be detected, but location and type of the multiple faults can also be determined by the FPGA test system and associated test schemes. Furthermore, 100% fault coverage can be achieved in experiment.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092258,no
Combating class imbalance problem in semi-supervised defect detection,"Detection of defect-prone software modules is an important topic in software quality research, and widely studied under enough defect data circumstance. An improved semi-supervised learning approach for defect detection involving class imbalanced and limited labeled data problem has been proposed. This approach employs random under-sampling technique to resample the original training set and updating training set in each round for co-train style algorithm. In comparison with conventional machine learning approaches, our method has significant superior performance in the aspect of AUC (area under the receiver operating characteristic) metric. Experimental results also show that with the proposed learning approach, it is possible to design better method to tackle the class imbalanced problem in semi-supervised learning.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092260,no
On the Effectiveness of Contracts as Test Oracles in the Detection and Diagnosis of Race Conditions and Deadlocks in Concurrent Object-Oriented Software,"The idea behind Design by Contract (DbC) is that a method defines a contract stating the requirements a client needs to fulfill to use it, the precondition, and the properties it ensures after its execution, the post condition. Though there exists ample support for DbC for sequential programs, applying DbC to concurrent programs presents several challenges. We have proposed a solution to these challenges in the context of Java as programming language and the Java Modeling language as specification language. This paper presents our findings when applying our DbC technique on an industrial case study to evaluate the ability of contract-based, runtime assertion checking code at detecting and diagnosing race conditions and deadlocks during system testing. The case study is a highly concurrent industrial system from the telecommunications domain, with actual faults. It is the first work to systematically investigate the impact of contract assertions for the detection of race conditions and deadlocks, along with functional properties, in an industrial system.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092549,no
A Qualitative Study of Open Source Software Development: The Open EMR Project,"Open Source software is competing successfully in many areas. The commercial sector is recognizing the benefits offered by Open Source development methods that lead to high quality software. Can these benefits be realized in specialized domains where expertise is rare? This study examined discussion forums of an Open Source project in a particular specialized application domain - electronic medical records - to see how development roles are carried out, and by whom. We found through a qualitative analysis that the core developers in this system include doctors and clinicians who also use the product. We also found that the size of the community associated with the project is an order of magnitude smaller than predicted, yet still maintains a high degree of responsiveness to issues raised by users. The implication is that a few experts and a small core of dedicated programmers can achieve success using an Open Source approach in a specialized domain.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092551,no
Exploring Software Measures to Assess Program Comprehension,"Software measures are often used to assess program comprehension, although their applicability is discussed controversially. Often, their application is based on plausibility arguments, which, however, is not sufficient to decide whether software measures are good predictors for program comprehension. Our goal is to evaluate whether and how software measures and program comprehension correlate. To this end, we carefully designed an experiment. We used four different measures that are often used to judge the quality of source code: complexity, lines of code, concern attributes, and concern operations. We measured how subjects understood two comparable software systems that differ in their implementation, such that one implementation promised considerable benefits in terms of better software measures. We did not observe a difference in program comprehension of our subjects as the software measures suggested it. To explore how software measures and program comprehension could correlate, we used several variants of computing the software measures. This brought them closer to our observed result, however, not as close as to confirm a relationship between software measures and program comprehension. Having failed to establish a relationship, we present our findings as an open issue to the community and initiate a discussion on the role of software measures as comprehensibility predictors.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092561,no
Network Versus Code Metrics to Predict Defects: A Replication Study,"Several defect prediction models have been proposed to identify which entities in a software system are likely to have defects before its release. This paper presents a replication of one such study conducted by Zimmermann and Nagappan on Windows Server 2003 where the authors leveraged dependency relationships between software entities captured using social network metrics to predict whether they are likely to have defects. They found that network metrics perform significantly better than source code metrics at predicting defects. In order to corroborate the generality of their findings, we replicate their study on three open source Java projects, viz., JRuby, ArgoUML, and Eclipse. Our results are in agreement with the original study by Zimmermann and Nagappan when using a similar experimental setup as them (random sampling). However, when we evaluated the metrics using setups more suited for industrial use -- forward-release and cross-project prediction -- we found network metrics to offer no vantage over code metrics. Moreover, code metrics may be preferable to network metrics considering the data is easier to collect and we used only 8 code metrics compared to approximately 58 network metrics.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092570,no
Measuring Architectural Change for Defect Estimation and Localization,"While there are many software metrics measuring the architecture of a system and its quality, few are able to assess architectural change qualitatively. Given the sheer size and complexity of current software systems, modifying the architecture of a system can have severe, unintended consequences. We present a method to measure architectural change by way of structural distance and show its strong relationship to defect incidence. We show the validity and potential of the approach in an exploratory analysis of the history and evolution of the Spring Framework. Using other, public datasets, we corroborate the results of our analysis.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092571,no
Handling Estimation Uncertainty with Bootstrapping: Empirical Evaluation in the Context of Hybrid Prediction Methods,"Reliable predictions are essential for managing software projects with respect to cost and quality. Several studies have shown that hybrid prediction models combining causal models with Monte Carlo simulation are especially successful in addressing the needs and constraints of today's software industry: They deal with limited measurement data and, additionally, make use of expert knowledge. Moreover, instead of providing merely point estimates, they support the handling of estimation uncertainty, e.g., estimating the probability of falling below or exceeding a specific threshold. Although existing methods do well in terms of handling uncertainty of information, we can show that they leave uncertainty coming from imperfect modeling largely unaddressed. One of the consequences is that they probably provide over-confident uncertainty estimates. This paper presents a possible solution by integrating bootstrapping into the existing methods. In order to evaluate whether this solution does not only theoretically improve the estimates but also has a practical impact on the quality of the results, we evaluated the solution in an empirical study using data from more than sixty projects and six estimation models from different domains and application areas. The results indicate that the uncertainty estimates of currently used models are not realistic and can be significantly improved by the proposed solution.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092573,no
Inferring Skill from Tests of Programming Performance: Combining Time and Quality,"The skills of software developers are important to the success of software projects. Also, when studying the general effect of a tool or method, it is important to control for individual differences in skill. However, the way skill is assessed is often ad hoc, or based on unvalidated methods. According to established test theory, validated tests of skill should infer skill levels from well-defined performance measures on multiple, small, representative tasks. In this respect, we show how time and quality, which are often analyzed separately, can be combined as task performance and subsequently be aggregated as an approximation of skill. Our results show significant positive correlations between our proposed measures of skill and other variables, such as seniority, lines of code written, and self-evaluated expertise. The method for combining time and quality is a promising first step to measuring programming skill in both industry and research settings.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092579,no
Modeling the Number of Active Software Users,"More and more software applications are developed within a software ecosystem (SECO), such as the Face book ecosystem and the iPhone AppStore. A core asset of a software ecosystem is its users, and the behavior of the users strongly affects the decisions of software vendors. The number of active users reflects user satisfaction and quality of the applications in a SECO. However, we can hardly find any literature about the number of active software users. Because software users are one of the most important assets of a software business, this information is very sensitive. In this paper, we analyzed the traces of software application users within a large scale software ecosystem with millions of active users. We identified useful patterns of user behavior, and proposed models that help to understand the number of active application users. The model we proposed better predicts the number of active users than just looking at the traditional retention rate. It also provides a fast way to monitor user satisfaction of online software applications. We have therefore provided an alternative way for SECO platform vendors to identify rising or falling applications, and for third party application vendors to identify risks and opportunity of their products.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092592,no
What are Problem Causes of Software Projects? Data of Root Cause Analysis at Four Software Companies,"Root cause analysis (RCA) is a structured investigation of a problem to detect the causes that need to be prevented. We applied ARCA, an RCA method, to target problems of four medium-sized software companies and collected 648 causes of software engineering problems. Thereafter, we applied grounded theory to the causes to study their types and related process areas. We detected 14 types of causes in 6 process areas. Our results indicate that development work and software testing are the most common process areas, whereas lack of instructions and experiences, insufficient work practices, low quality task output, task difficulty, and challenging existing product are the most common types of the causes. As the types of causes are evenly distributed between the cases, we hypothesize that the distributions could be generalizable. Finally, we found that only 2.5% of the causes are related to software development tools that are widely investigated in software engineering research.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092595,no
Obtaining Thresholds for the Effectiveness of Business Process Mining,"Business process mining is a powerful tool to retrieve the valuable business knowledge embedded in existing information systems. The effectiveness of this kind of proposal is usually evaluated using recall and precision, which respectively measure the completeness and exactness of the retrieved business processes. Since the effectiveness assessment of business process mining is a difficult and error-prone activity, the main hypothesis of this work studies the possibility of obtaining thresholds to determine when recall and precision values are appropriate. The business process mining technique under study is MARBLE, a model-driven framework to retrieve business processes from existing information systems. The Bender method was applied to obtain the thresholds of the recall and precision measures. The experimental data used as input were obtained from a set of 44 business processes retrieved with MARBLE through a family of case studies carried out over the last two years. The study provides thresholds for recall and precision measures, which facilitates the interpretation of their values by means of five linguistic labels that range from low to very high. As a result, recall must be high (with at least a medium precision above 0.56), and precision must also be high (with at least a low recall of 0.70) to ensure that business processes were recovered (by using MARBLE) with an effectiveness value above 0.65. The thresholds allowed us to ascertain with more confidence whether MARBLE can effectively mine business processes from existing information systems. In addition, the provided results can be used as reference values to compare MARBLE with other similar business process mining techniques.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092604,no
Predicting software black-box defects using stacked generalization,"Defect number prediction is essential to make a key decision on when to stop testing. For more applicable and accurate prediction, we propose an ensemble prediction model based on stacked generalization (PMoSG), and use it to predict the number of defects detected by third-party black-box testing. Taking the characteristics of black-box defects and causal relationships among factors which influence defect detection into account, Bayesian net and other numeric prediction models are employed in our ensemble models. Experimental results show that our PMoSG model achieves a significant improvement in accuracy of defect numeric prediction than any individual model, and achieves best prediction accuracy when using LWL(Locally Weighted Learning) method as level-1 model.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6093330,no
Automated extraction of architecture-level performance models of distributed component-based systems,"Modern enterprise applications have to satisfy increasingly stringent Quality-of-Service requirements. To ensure that a system meets its performance requirements, the ability to predict its performance under different configurations and workloads is essential. Architecture-level performance models describe performance-relevant aspects of software architectures and execution environments allowing to evaluate different usage profiles as well as system deployment and configuration options. However, building performance models manually requires a lot of time and effort. In this paper, we present a novel automated method for the extraction of architecture-level performance models of distributed component-based systems, based on monitoring data collected at run-time. The method is validated in a case study with the industry-standard SPECjEnterprise2010 Enterprise Java benchmark, a representative software system executed in a realistic environment. The obtained performance predictions match the measurements on the real system within an error margin of mostly 10-20 percent.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6100052,no
Iterative mining of resource-releasing specifications,"Software systems commonly use resources such as network connections or external file handles. Once finish using the resources, the software systems must release these resources by explicitly calling specific resource-releasing API methods. Failing to release resources properly could result in resource leaks or even outright system failures. Existing verification techniques could analyze software systems to detect defects related to failing to release resources. However, these techniques require resource-releasing specifications for specifying which API method acquires/releases certain resources, and such specifications are not well documented in practice, due to the large amount of manual effort required to document them. To address this issue, we propose an iterative mining approach, called RRFinder, to automatically mining resource-releasing specifications for API libraries in the form of (resource-acquiring, resource-releasing) API method pairs. RRFinder first identifies resource-releasing API methods, for which RRFinder then identifies the corresponding resource-acquiring API methods. To identify resource-releasing API methods, RRFinder performs an iterative process including three steps: model-based prediction, call-graph-based propagation, and class-hierarchy-based propagation. From heterogeneous information (e.g., source code, natural language), the model-based prediction employs a classification model to predict the likelihood that an API method is a resource-releasing method. The call-graph-based and class-hierarchy-based propagation propagates the likelihood information across methods. We evaluated RRFinder on eight open source libraries, and the results show that RRFinder achieved an average recall of 94.0% with precision of 86.6% in mining resource-releasing specifications, and the mined specifications are useful in detecting resource leak defects.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6100058,no
Towards dynamic backward slicing of model transformations,"Model transformations are frequently used means for automating software development in various domains to improve quality and reduce production costs. Debugging of model transformations often necessitates identifying parts of the transformation program and the transformed models that have causal dependence on a selected statement. In traditional programming environments, program slicing techniques are widely used to calculate control and data dependencies between the statements of the program. Here we introduce program slicing for model transformations where the main challenge is to simultaneously assess data and control dependencies over the transformation program and the underlying models of the transformation. In this paper, we present a dynamic backward slicing approach for both model transformation programs and their transformed models based on automatically generated execution trace models of transformations.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6100084,no
Observations on the connectedness between requirements-to-code traces and calling relationships for trace validation,"Traces between requirements and code reveal where requirements are implemented. Such traces are essential for code understanding and change management. Unfortunately, the handling of traces is highly error prone, in part due to the informal nature of requirements. This paper discusses observations on the connectedness between requirements-to-code traces and calling relationships within the source code. These observations are based on the empirical evaluation of four case study systems covering 150 KLOC and 59 sample requirements. We found that certain patterns of connectedness have high or low likelihoods of occurring. These patterns can thus be used to confirm or reject existing traceability - hence they are useful for validating requirements-to-code traces.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6100087,no
Stateful testing: Finding more errors in code and contracts,"Automated random testing has shown to be an effective approach to finding faults but still faces a major unsolved issue: how to generate test inputs diverse enough to find many faults and find them quickly. Stateful testing, the automated testing technique introduced in this article, generates new test cases that improve an existing test suite. The generated test cases are designed to violate the dynamically inferred contracts (invariants) characterizing the existing test suite. As a consequence, they are in a good position to detect new faults, and also to improve the accuracy of the inferred contracts by discovering those that are unsound. Experiments on 13 data structure classes totalling over 28,000 lines of code demonstrate the effectiveness of stateful testing in improving over the results of long sessions of random testing: stateful testing found 68.4% new faults and improved the accuracy of automatically inferred contracts to over 99%, with just a 7% time overhead.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6100094,no
Generating essential user interface prototypes to validate requirements,"Requirements need to be validated at an early stage of analysis to address inconsistency and incompleteness issues. Capturing requirements usually involves natural language analysis, which is often imprecise and error prone, or translation into formal models, which are difficult for non-technical stakeholders to understand and use. Users often best understand proposed software systems from the likely user interface they will present. To this end we describe novel automated tool support for capturing requirements as Essential Use Cases and translating these into Essential User Interface low-fidelity rapid prototypes. We describe our automated tool supporting requirements capture, lo-fi user interface prototype generation and consistency management.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6100126,no
CloneDifferentiator: Analyzing clones by differentiation,"Clone detection provides a scalable and efficient way to detect similar code fragments. But it offers limited explanation of differences of functions performed by clones and variations of control and data flows of clones. We refer to such differences as semantic differences of clones. Understanding these semantic differences is essential to correctly interpret cloning information and perform maintenance tasks on clones. Manual analysis of semantic differences of clones is complicated and error-prone. In the paper, we present our clone analysis tool, called Clone-Differentiator. Our tool automatically characterizes clones returned by a clone detector by differentiating Program Dependence Graphs (PDGs) of clones. CloneDifferentiator is able to provide a precise characterization of semantic differences of clones. It can provide an effective means of analyzing clones in a task oriented manner.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6100129,no
Automatically detecting the quality of the query and its implications in IR-based concept location,"Concept location is an essential task during software maintenance and in particular program comprehension activities. One of the approaches to this task is based on leveraging the lexical information found in the source code by means of Information Retrieval techniques. All IR-based approaches to concept location are highly dependent on the queries written by the users. An IR approach, even though good on average, might fail when the input query is poor. Currently there is no way to tell when a query leads to poor results for IR-based concept location, unless a considerable effort is put into analyzing the results after the fact. We propose an approach based on recent advances in the field of IR research, which aims at automatically determining the difficulty a query poses to an IR-based concept location technique. We plan to evaluate several models and relate them to IR performance metrics.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6100144,no
Generating program inputs for database application testing,"Testing is essential for quality assurance of database applications. Achieving high code coverage of the database application is important in testing. In practice, there may exist a copy of live databases that can be used for database application testing. Using an existing database state is desirable since it tends to be representative of real-world objects' characteristics, helping detect faults that could cause failures in real-world settings. However, to cover a specific program code portion (e.g., block), appropriate program inputs also need to be generated for the given existing database state. To address this issue, in this paper, we propose a novel approach that generates program inputs for achieving high code coverage of a database application, given an existing database state. Our approach uses symbolic execution to track how program inputs are transformed before appearing in the executed SQL queries and how the constraints on query results affect the application's execution. One significant challenge in our problem context is the gap between program-input constraints derived from the program and from the given existing database state; satisfying both types of constraints is needed to cover a specific program code portion. Our approach includes novel query formulation to bridge this gap. Our approach is loosely integrated into Pex, a state-of-the-art white-box testing tool for .NET from Microsoft Research. Empirical evaluations on two real database applications show that our approach assists Pex to generate program inputs that achieve higher code coverage than the program inputs generated by Pex without our approach's assistance.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6100152,no
Prioritizing tests for fault localization through ambiguity group reduction,"In practically all development processes, regression tests are used to detect the presence of faults after a modification. If faults are detected, a fault localization algorithm can be used to reduce the manual inspection cost. However, while using test case prioritization to enhance the rate of fault detection of the test suite (e.g., statement coverage), the diagnostic information gain per test is not optimal, which results in needless inspection cost during diagnosis. We present RAPTOR, a test prioritization algorithm for fault localization, based on reducing the similarity between statement execution patterns as the testing progresses. Unlike previous diagnostic prioritization algorithms, RAPTOR does not require false negative information, and is much less complex. Experimental results from the Software Infrastructure Repository's benchmarks show that RAPTOR is the best technique under realistic conditions, with average cost reductions of 40% with respect to the next best technique, with negligible impact on fault detection capability.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6100153,no
System design for PCB defects detection based on AOI technology,"A design of PCB automatic defects detection system based on AOI technology is presented. The hardware design is emphatically introduced including illumination module, image acquisition module, motion control unit, PC, graphic display device and operation unit. Simultaneously, the software design is briefly explained. This design is a non-contact PCB defects detection technology which can not only detect open circuit and short circuit defects, but also can detect wire gaps, voids, scratch defects etc. The highest resolution of the design is 15m and the detection success rate is over 95%.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6100553,no
New approach to determine the critical number of failure in software systems,"Software-Engineering is very important today. In industry (specifically by software critical system) it is important to produce high reliable software, i.e. software with low proportion of faults. To produce such reliable software, a long handling process is required, and because this process consumes a large amount of time and resources to achieve the desired reliability goals it is useful to use Software Reliability Stochastic Models to predict the required software testing time. In this paper a new approach to reflecting the residual number of critical failures in software-systems is introduced. There are currently very few processes enabling us to predict the reliability of the critical failures or the critical failure rate for critical systems. Furthermore, we will focus on distinguishing the critical failures in the software. We will thus distinguish both critical as well as non-critical failures in the Software. Therefore it is important to divide the process into two classes, detection- and correction class. To develop an approach it is necessary to determine corresponding distribution functions and model assumptions.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6102114,no
Texture feature based fingerprint recognition for low quality images,"Fingerprint-based identification is one of the most well-known and publicized biometrics for personal identification. Extracting features out of poor quality prints is the most challenging problem faced in this area. In this paper, the texture feature based approach for fingerprint recognition using Discrete Wavelet Transform (DWT) is developed to identify the low quality fingerprint from inked-printed images on paper. The fingerprint image from paper is very poor quality image and sometimes it is complex with fabric background. Firstly, a center point area of the fingerprint is detected and keeping the Core Point as center point, the image of size w x w is cropped. Gabor filtering is applied for fingerprint enhancement over the orientation image. Finally, the texture features are extracted by analyzing the fingerprint with Discrete Wavelet Transform (DWT) and Euclidean distance metric is used as similarity measure. The accuracy is improved up to 98.98%.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6102204,no
System Monitoring with Metric-Correlation Models,"Modern software systems expose management metrics to help track their health. Recently, it was demonstrated that correlations among these metrics allow errors to be detected and their causes localized. Prior research shows that linear models can capture many of these correlations. However, our research shows that several factors may prevent linear models from accurately describing correlations, even if the underlying relationship is linear. Common phenomena we have observed include relationships that evolve, relationships with missing variables, and heterogeneous residual variance of the correlated metrics. Usually these phenomena can be discovered by testing for heteroscedasticity of the underlying linear models. Such behaviour violates the assumptions of simple linear regression, which thus fail to describe system dynamics correctly. In this paper we address the above challenges by employing efficient variants of Ordinary Least Squares regression models. In addition, we automate the process of error detection by introducing the Wilcoxon Rank-Sum test after proper correlations modeling. We validate our models using a realistic Java-Enterprise-Edition application. Using fault-injection experiments we show that our improved models capture system behavior accurately.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6102277,no
Thermal analysis and experimental validation on cooling efficiency of thin film transistor liquid crystal display (TFT-LCD) panels,"This research explored the thermal analysis and modeling of a 32 thin film transistor liquid crystal display (TFT-LCD) panel, in the purpose of making possible improvements in cooling efficiencies. The illumination of the panel was insured by 180 light emitting diodes (LEDs) located at the top and bottom edges of the panels. These LEDs dissipate high heat flux at low thermal resistance. Hence, in order to insure good image quality in panels and long service life, an adequate thermal management is necessary. For this purpose, a commercially available computational fluid dynamics (CFD) simulation software FloEFD was used to predict the temperature distribution. This thermal prediction by computational method was validated by an experimental thermal analysis by attaching 10 thermocouples on the back cover of the panel and measuring the temperatures. Also, thermal camera images of the panel by FLIR Thermacam SC 2000 test device were also analyzed.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6102689,no
A Self Healing Action Composition Agent,"The establishment of a self-healing agent has received much interest in multiple domains such as : Web services, production supply chain, transport systems, etc. This agent has a set of actions. Its role is to respond to user request with a plan of composed actions, to on-line diagnose the status of the plan execution and to automatically repair the plan when a fault is detected during the plan's execution. To this end, three main areas are studied and modeled for the establishment of such an agent : composition, diagnosis and repair.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6103380,no
Machine-Learning Models for Software Quality: A Compromise between Performance and Intelligibility,"Building powerful machine-learning assessment models is an important achievement of empirical software engineering research, but it is not the only one. Intelligibility of such models is also needed, especially, in a domain, software engineering, where exploration and knowledge capture is still a challenge. Several algorithms, belonging to various machine-learning approaches, are selected and run on software data collected from medium size applications. Some of these approaches produce models with very high quantitative performances, others give interpretable, intelligible, and """"glass-box"""" models that are very complementary. We consider that the integration of both, in automated decision-making systems for assessing software product quality, is desirable to reach a compromise between performance and intelligibility.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6103446,no
Impact of Data Sampling on Stability of Feature Selection for Software Measurement Data,"Software defect prediction can be considered a binary classification problem. Generally, practitioners utilize historical software data, including metric and fault data collected during the software development process, to build a classification model and then employ this model to predict new program modules as either fault-prone (fp) or not-fault-prone (nfp). Limited project resources can then be allocated according to the prediction results by (for example) assigning more reviews and testing to the modules predicted to be potentially defective. Two challenges often come with the modeling process: (1) high-dimensionality of software measurement data and (2) skewed or imbalanced distributions between the two types of modules (fp and nfp) in those datasets. To overcome these problems, extensive studies have been dedicated towards improving the quality of training data. The commonly used techniques are feature selection and data sampling. Usually, researchers focus on evaluating classification performance after the training data is modified. The present study assesses a feature selection technique from a different perspective. We are more interested in studying the stability of a feature selection method, especially in understanding the impact of data sampling techniques on the stability of feature selection when using the sampled data. Some interesting findings are found based on two case studies performed on datasets from two real-world software projects.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6103463,no
Automatic Construction of Deployment Descriptors for Web Applications,"Web applications are a kind of component based distributed systems, and these components are deployed in various containers and engines. In web applications, runtime deployment descriptors are corresponding to vendor specific platforms. Due to the complexities of applications and environments, it is tedious and error-prone for deployers to create runtime deployment descriptors manually. In this paper, we propose a generalized approach to promote the automation degree in runtime deployment descriptors construction. We view deployment descriptor schemas as models and create transformation relations between schema elements by a comprehensive matching method. Transformation code, in form of XSLT, can be generated base on parameterized templates. We implement a prototype and evaluate the effects of this approach with some experiments finally.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6104626,no
Computing Properties of Large Scalable and Fault-Tolerant Logical Networks,"As the number of processors embedded in high performance computing platforms becomes higher and higher, it is vital to force the developers to enhance the scalability of their codes in order to exploit all the resources of the platforms. This often requires new algorithms, techniques and methods for code development that add to the application code new properties: the presence of faults is no more an occasional event but a challenge. Scalability and Fault-Tolerance issues are also present in hidden part of any platform: the overlay network that is necessary to build for controlling the application or in the runtime system support for messaging which is also required to be scalable and fault tolerant. In this paper, we focus on the computational challenges to experiment with large scale (many millions of nodes) logical topologies. We compute Fault-Tolerant properties of different variants of Binomial Graphs (BMG) that are generated at random. For instance, we exhibit interesting properties regarding the number of links regarding some desired Fault-Tolerant properties and we compare different metrics with the Binomial Graph structure as the reference structure. A software tool has been developed for this study and we show experimental results with topologies containing 21000 nodes. We also explain the computational challenge when we deal with such large scale topologies and we introduce various probabilistic algorithms to solve the problems of computing the conventional metrics.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6106018,no
Device register classes for embedded systems,"A device register is the view any peripheral device presents to the software world. Low-level routines in typical embedded systems, e.g., device drivers, communicate with devices by reading and writing device registers. Many processors use memory-mapped I/O, which assigns device registers to fixed addresses in conventional memory. To high level languages like C or C++, memory mapped devices behave like ordinary data objects to some extent. Programs use assignment operators to read values from or write values to memory mapped device registers. Unfortunately, traditional approaches for organizing and accessing memory-mapped devices are inconvenient and error prone. In this paper, a new way of writing and using C++ classes which encapsulate memory mapped device registers is described. The principle is extended to handle I/O mapped device registers for coprocessors. A Device Register Class description language is also described.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6106297,no
Design and development of the CO<inf>2</inf> enriched Seawater Distribution System,"The kinetics of the reaction that occurs when CO<sub>2</sub> and seawater are in contact is a complex function of temperature, alkalinity, final pH and TCO<sub>2</sub> which taken together determine the time required for complete equilibrium. This reaction is extremely important to the study of Ocean Acidification (OA) and is the critical technical driver in the Monterey Bay Aquarium Research Institute's (MBARI) Free Ocean CO<sub>2</sub> Enrichment (FOCE) experiments. The deep water FOCE science experiments are conducted at depths beyond scuba diver reach and demand that a valid perturbation experiment operate at a stable yet naturally fluctuating lower pH condition and avoid large or rapid pH variation as well as incomplete reactions, when we expose an experimental region or sample. Therefore, the technical requirement is to create a CO<sub>2</sub> source in situ that is stable and well controlled. After extensive research and experimentation MBARI has developed the ability to create an in situ source of CO<sub>2</sub> enriched seawater (ESW) for distribution and subsequent use in an ocean acidification experiment. The system mates with FOCE, but can be used in conjunction with other CO<sub>2</sub> experimental applications in deep water. The ESW system is completely standalone from FOCE. While the chemical changes induced by the addition of fossil fuel CO<sub>2</sub> on the ocean are well known and easily predicted, the biological consequences are less clear and the subject of considerable debate. Experiments have been successfully carried out on land to investigate the effects of elevated atmospheric CO<sub>2</sub> levels in various areas around the globe but only limited work on CO<sub>2</sub> impacts to ocean environmental systems have been carried out to date. With rising concern over the long-term reduction in ocean pH, there is a need for viable in situ techniques to carry out experiments on marine biological systems. Previous investigations have used aqua- ia that compromise these studies because of reduced ecological complexity and buffering capacity. Additionally, aquaria use tightly controlled experimental conditions such as temperature, artificial light, and water quality that do not represent the natural ocean variability. In order to study the future effects of ocean acidification, scientists and engineers at MBARI have developed a technique and apparatus for an in situ perturbation experiment, the FOCE experimental platform. At the time of this writing the FOCE system and associated ESW are attached to the Monterey Accelerated Research System (MARS) cabled observatory. Engineering validation and tuning experiments using remote control and real time experimental feedback are underway. Additionally, an extensive instrumentation suite provides all of the necessary data for pH calculation and experimental control. The ESW is a separately deployed system that stores and distributes CO<sub>2</sub> enriched seawater. It receives power and communications via an underwater mateable electrical tether. The CO<sub>2</sub> enriched seawater is pumped into the FOCE sections from the ESW. This paper describes the design, development, and testing of the underwater ESW Distribution System as well as the software control algorithms as applied to FOCE. The paper covers the initial prototype, lessons learned, and the final operational version.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6107095,no
A preliminary investigation towards test suite optimization approach for enhanced State-Sensitivity Partitioning,"Testing is crucial in software development. Continuous researches being done to discover effective approaches in testing that capable to detect faults despite of reducing cost. Previous work in State-Sensitivity Partitioning (SSP) technique, which based on all-transition coverage criterion, has been introduced to avoid exhaustively testing the entire data states of a module by partitioning it based on state's sensitivity towards events, conditions and action. The test data for that particular module testing is in form of event sequences (or test sequence) and sets of test sequences in test cases will perform SSP test suite. The problem occurs in SSP test suite is data state redundancy that leads towards suite growth. This paper aims to discuss an initial step of our ongoing research in enhancing prior SSP test suite. Our work will try to find out the best way in removing redundant data state in order to minimize the suite size but yet capable to detect faults introduced by five selective mutation operators effectively as the original suite.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6108592,no
3-dimensional analysis of Ground Penetrating Radar image for non-destructive road inspection,"Regular maintenance of highway is an important issue to ensure safety of the vehicles using the road. Most of existing method of highway inspections are destructive, which take much times, efforts, and costs. In this paper, we propose GPR (Ground Penetrating Radar) imaging to detect possible defect of the road. GPR scanning on a plane parallel to the road yields 3D images, so that slice-by-slice images can be generated for a comprehensive evaluation. First, we simulate the subsurface-scanning with GPR-Max software, by setting up the parameters similar to expected real-condition. Then, we set up the experiment in our GPR Test-Range, in which a Network Analyzer is employed as a GPR. We compare and analyze both of the simulation and Test-Range results, including slice analysis, to asses the quality of the method. Our results indicates implementability of such 3D GPR imaging for road inspection.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6108603,no
Input-input relationship constraints in T-way testing,"T-way testing is designed to detect faults due to interaction. In order to be effective, all t combinations of input parameters must be tested. While many t-way strategies can be used to generate the t-way test data (e.g. IPOG, AETG, GT-Way, Jenny, TVG and MIPOG), most do not ensure that all t combinations of input parameters can be practically tested. Addressing this issue, this paper highlights a new type of constraints that might prevent some t-way parameter interactions from being tested (and hence compromising the effectiveness of t-way testing), termed input-input relationship constraints. Apart from ensuring all t combinations are properly tested, input-input relationship constraints can further optimize the generated test data since all impossible combinations are completely ignored. In addition, this paper also introduces a new strategy that supports input-input relationship constraints and demonstrates the correctness of the strategy as well as the effectiveness of test data with input-input relationship.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6108767,no
ConfigChecker: A tool for comprehensive security configuration analytics,"Recent studies show that configurations of network access control is one of the most complex and error prone network management tasks. For this reason, network misconfiguration becomes the main source for network unreachablility and vulnerability problems. In this paper, we present a novel approach that models the global end-to-end behavior of access control configurations of the entire network including routers, IPSec, firewalls, and NAT for unicast and multicast packets. Our model represents the network as a state machine where the packet header and location determines the state. The transitions in this model are determined by packet header information, packet location, and policy semantics for the devices being modeled. We encode the semantics of access control policies with Boolean functions using binary decision diagrams (BDDs). We then use computation tree logic (CTL) and symbolic model checking to investigate all future and past states of this packet in the network and verify network reachability and security requirements. Thus, our contributions in this work is the global encoding for network configurations that allows for general reachability and security property-based verification using CTL model checking. We have implemented our approach in a tool called ConfigChecker. While evaluating ConfigChecker, we modeled and verified network configurations with thousands of devices and millions of configuration rules, thus demonstrating the scalability of this approach. We also present a SCAP-based tool on top of ConfigChecker that integrates host and network configuration compliance checking in one model and allows for executing comprehensive analysis queries in order to verify security and risk requirements across the end-to-end network as a single system.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6111667,no
Measuring firewall security,"In the recent years, more attention is given to firewalls as they are considered the corner stone in Cyber defense perimeters. The ability to measure the quality of protection of a firewall policy is a key step to assess the defense level for any network. To accomplish this task, it is important to define objective metrics that are formally provable and practically useful. In this work, we propose a set of metrics that can objectively evaluate and compare the hardness and similarities of access policies of single firewalls based on rules tightness, the distribution of the allowed traffic, and security requirements. In order to analyze firewall polices based on the policy semantic, we used a canonical representation of firewall rules using Binary Decision Diagrams (BDDs) regardless of the rules format and representation. The contribution of this work comes in measuring and comparing firewall security deterministically in term of security compliance and weakness in order to optimize security policy and engineering.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6111669,no
Vulnerability hierarchies in access control configurations,"This paper applies methods for analyzing fault hierarchies to the analysis of relationships among vulnerabilities in misconfigured access control rule structures. Hierarchies have been discovered previously for faults in arbitrary logic formulae [11,10,9,21], such that a test for one class of fault is guaranteed to detect other fault classes subsumed by the one tested, but access control policies reveal more interesting hierarchies. These policies are normally composed of a set of rules of the form if [conditions] then [decision], where [conditions] may include one or more terms or relational expressions connected by logic operators, and [decision] is often 2-valued (grant or deny), but may be n-valued. Rule sets configured for access control policies, while complex, often have regular structures or patterns that make it possible to identify generic vulnerability hierarchies for various rule structures such that an exploit for one class of configuration error is guaranteed to succeed for others downstream in the hierarchy. A taxonomy of rule structures is introduced and detection conditions computed for nine classes of vulnerability: added term, deleted term, replaced term, stuck-at-true condition, stuck-at-false condition, negated condition, deleted rule, replaced decision, negated decision. For each configuration rule structure, detection conditions were analyzed for the existence of logical implication relations between detection conditions. It is shown that hierarchies of detection conditions exist, and that hierarchies vary among rule structures in the taxonomy. Using these results, tests may be designed to detect configuration errors, and resulting vulnerabilities, using fewer tests than would be required without knowledge of the hierarchical relationship among common errors. In addition to practical applications, these results may help to improve the understanding of access control policy configurations.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6111679,no
Critiquing Rules and Quality Quantification of Development-Related Documents,"As the development of embedded systems grows in scale, it is becoming more important for engineers to share development documents such as requirements, design specifications and testing specifications, and to accurately circulate and understand the information necessary for development. Also, many defects that can be originated in the surface expression of the documents are reported through investigations of causes of defects in embedded systems development, In this paper, we highlight improper surface expressions of Japanese documents, and define quality criteria and critiquing rules to detect problems such as ambiguous expressions or omissions of information. We also carry out visual quality inspections and evaluate detection performance, correlations and working time. Then, we verify the validity of the critiquing rules we have defined and apply them to the document critiquing tool to evaluate the quality of the actual documents used in the development of embedded systems. And we quantify the quality of these documents by automatically detecting improper expression. We also apply supplemental critiquing rules to the document critiquing tool for use by non-native speakers of Japanese, and verify its efficacy at improving the quality of Japanese documents created by foreigners.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6113041,no
A Proposal of NHPP-Based Method for Predicting Code Change in Open Source Development,"This paper proposes a novel method for predicting the amount of source code changes (changed lines of code: changed-LOC) in the open source development (OSD). While the software evolution can be observed through the public code repository in OSD, it is not easy to understand and predict the state of the whole development because of the huge amount of less-organized information.The method proposed in the paper predicts the code changes by using only data freely available from the code repository the code-change time stamp and the changed-LOC.The method consists of two steps: 1) to predict the number of occurrences of code changes by using a non-homogeneous Poisson process (NHPP)-based model, and 2) to predict the amount of code changes by using the outcome of the step-1 and the previously changed-LOC.The empirical work shows that the proposed method has an ability to predict the changed-LOC in the next 12 months with less than 10% error.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6113042,no
An Empirical Study of Fault Prediction with Code Clone Metrics,"In this paper, we present a replicated study to predict fault-prone modules with code clone metrics to follow Baba's experiment. We empirically evaluated the performance of fault prediction models with clone metrics using 3 datasets from the Eclipse project and compared it to fault prediction without clone metrics. Contrary to the original Baba's experiment, we could not significantly support the effect of clone metrics, i.e., the result showed that F1-measure of fault prediction was not improved by adding clone metrics to the prediction model. To explain this result, this paper analyzed the relationship between clone metrics and fault density. The result suggested that clone metrics were effective in fault prediction for large modules but not for small modules.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6113044,no
Quantifying the Effectiveness of Testing Efforts on Software Fault Detection with a Logit Software Reliability Growth Model,"Quantifying the effects of software testing metrics such as the number of test runs on the fault detection ability is quite important to design and manage effective software testing. This paper focuses on the regression model which represents the causal relationship between the software testing metrics and the fault detection probability. In a numerical experiment, we perform the quantitative estimation of the causal relationship through the quantization of software testing metrics.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6113045,no
Using Efficient Machine-Learning Models to Assess Two Important Quality Factors: Maintainability and Reusability,"Building efficient machine-learning assessment models is an important achievement of empirical software engineering research. Their integration in automated decision-making systems is one of the objectives of this work. It aims at empirically verify the relationships between some software internal artifacts and two quality attributes: maintainability and reusability. Several algorithms, belonging to various machine-learning approaches, are selected and run on software data collected from medium size applications. Some of these approaches produce models with very high quantitative performances; others give interpretable and """"glass-box"""" models that are very complementary.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6113057,no
An Exploratory Study on the Impact of Usage of Screenshot in Software Inspection Recording Activity,"This paper describes an exploratory study on theuse of screenshots for recording software inspection activities such as defect reproduction and correction. Although detected defects are usually recorded in writing, using screenshots to record detected defects should ecrease the percentage of irreproducible defects and the time needed to reproduce defects during the defect orrection phase. An experiment was conducted to clarify the efficiency of using screenshots to record detected defects. One practitioner group and two student groups participated in the experiment. The recorder in each group used a prototype support tool for capturing screenshots during the experiment. Each group conducted two trials: one with a general spreadsheet application to support recording, the other with the prototype tool that supportsrecording inspection activities. After the inspection meeting, the recorder was asked to reproduce the recorded defects. The percentage of reproduce defects and time to reproduce defects was measured. The results of the experiment show that use of screenshots increases the percentage of reproduced defects and decreases the time needed to reproduce the defects. The results also indicate that use of the recording tool affected the types of defects.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6113068,no
Software Metrics Based on Coding Standards Violations,"Software metrics is one of promise technique to capture the size and quality of products, development process in order to assess a software development. Many software metrics based on various aspects of a product and/or a process have been proposed. There is some research which discuss the relation between software metrics and faults to use these metrics as the indicator of quality. Most of these software metrics are based on structural features of products or process information related to explicit fault. In this paper, we focus on latent faults detected by static analysis techniques. The coding checker is widely used to find coding standards violations which are strongly relating to latent faults. In this paper, we propose new software metrics based on coding standards violations to capture latent faults in a development. We analyze two open source projects by using proposed metrics and discuss the effectiveness.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6113072,no
FMEA-Based Control Mechanism for Embedded Control Software,"Current software FMEA analysis depends on the static model of the embedded real-time system which cannot fully assess the dynamics of control loops and changes in timing. Control block diagram is not only the static model but also the dynamic model of the system. Though the simulation of the control blocks diagram, the dynamics of control lops and changes in timing can result in the injection of the failure mode. For illustration, a small embedded control system is utilized. Empirical results show that more detailed information such as the dynamics of control loops and changes in timing will provide the comprehensive effect analysis for software FMEA. Tough FMEA and control mechanism analysis techniques which assess operation under normal conditions and simulation of dynamic timing and failure conditions, a complete validation of the safety characteristics of embedded real-time control systems can result.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6113368,no
CriticalFault: Amplifying Soft Error Effect Using Vulnerability-Driven Injection,"As future microprocessors will be prone to various types of errors, researchers have looked into cross-layer hardware-software reliability solutions to reduce overheads. These mechanisms are shown to be effective when evaluated with statistical fault injection (SFI). However, under SFI, a large number of injected faults can be derated, making the evaluation less rigorous. To handle this problem, we propose a biased fault injection framework called Ciritical Fault that leverages vulnerability analysis to identify faults that are more likely to stress test the underlying reliability solution. Our experimental results show that the injection space is reduced by 30% and a large portion of injected faults cause software aborts and silent data corruptions. Overall, Critical Fault allows us to amplify soft error effects on reliability mechanism-under-test, which can help improve current techniques or inspire other new fault-tolerant mechanisms.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6113801,no
OpenMDSP: Extending OpenMP to Program Multi-Core DSP,"Multi-core Digital Signal Processors (DSP) are widely used in wireless telecommunication, core network transcoding, industrial control, and audio/video processing etc. Comparing with general purpose multi-processors, the multi-core DSPs normally have more complex memory hierarchy, such as on-chip core-local memory and non-cache-coherent shared memory. As a result, it is very challenging to write efficient multi-core DSP applications. The current approach to program multi-core DSPs is based on proprietary vendor SDKs, which only provides low-level, non-portable primitives. While it is acceptable to write coarse-grained task level parallel code with these SDKs, it is very tedious and error prone to write fine-grained data parallel code with them. We believe it is desired to have a high-level and portable parallel programming model for multi-core DSPs. In this paper, we propose Open MDSP, an extension of Open MP designed for multi-core DSPs. The goal of Open MDSP is to fill the gap between Open MP memory model and the memory hierarchy of multi-core DSPs. We propose three class of directives in Open MDSP: (1) data placement directives allow programmers to control the placement of global variables conveniently, (2) distributed array directives divide whole array into sections and promote them into core-local memory to improve performance, and (3) stream access directives promote big array into core-local memory section by section during a parallel loop's processing. We implement the compiler and runtime system for Open MDSP on Free Scale MSC8156. Benchmarking result shows that seven out of nine benchmarks achieve a speedup of more than 5 with 6 threads.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6113837,no
An Evaluation of Vectorizing Compilers,"Most of today's processors include vector units that have been designed to speedup single threaded programs. Although vector instructions can deliver high performance, writing vector code in assembly language or using intrinsics in high level languages is a time consuming and error-prone task. The alternative is to automate the process of vectorization by using vectorizing compilers. This paper evaluates how well compilers vectorize a synthetic benchmark consisting of 151 loops, two application from Petascale Application Collaboration Teams (PACT), and eight applications from Media Bench II. We evaluated three compilers: GCC (version 4.7.0), ICC (version 12.0) and XLC (version 11.01). Our results show that despite all the work done in vectorization in the last 40 years 45-71% of the loops in the synthetic benchmark and only a few loops from the real applications are vectorized by the compilers we evaluated.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6113845,no
On High-Assurance Scientific Workflows,"Scientific Workflow Management Systems (S-WFMS), such as Kepler, have proven to be an important tools in scientific problem solving. Interestingly, S-WFMS fault-tolerance and failure recovery is still an open topic. It often involves classic fault-tolerance mechanisms, such as alternative versions and rollback with re-runs, reliance on the fault-tolerance capabilities provided by subcomponents and lower layers such as schedulers, Grid and cloud resources, or the underlying operating systems. When failures occur at the underlying layers, a workflow system sees this as failed steps in the process, but frequently without additional detail. This limits S-WFMS' ability to recover from failures. We describe a light weight end-to-end S-WFMS fault-tolerance framework, developed to handle failure patterns that occur in some real-life scientific workflows. Capabilities and limitations of the framework are discussed and assessed using simulations. The results show that the solution considerably increase workflow reliability and execution time stability.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6113876,no
An Availability Model of a Virtual TMR System with Applications in Cloud/Cluster Computing,"Three important factors in dependable computing are cost, error correction and high availability. In this paper we will focus on assessing a proposed model that encapsulates all three important factors and a virtual architecture that can be implemented in the IaaS layer of cloud computing. The proposed model will be assessed against a popular existing architecture (Triple Modular Redundant System TMR) and the availability analysis done with Fault-Trees combined with Markov Chains. These experiments will demonstrate that the virtualization of the TMR system using the architecture that we have proposed, will achieve almost the same level of availability/reliability and cost, along with the inherent advantages of virtual systems. Advantages include faster system restart, efficient use of resources and migration.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6113906,no
Using Automated Control Charts for the Runtime Evaluation of QoS Attributes,"As modern software systems operate in a highly dynamic context, they have to adapt their behaviour in response to changes in their operational environment or/and requirements. Triggering adaptation depends on detecting quality of service (QoS) violations by comparing observed QoS values to predefined thresholds. These threshold-based adaptation approaches result in late adaptations as they wait until violations have occurred. This may lead to undesired consequences such as late response to critical events. In this paper we introduce a statistical approach CREQA - Control Charts for the Runtime Evaluation of QoS Attributes. This approach estimates at runtime capability of a system, and then it monitors and provides early detection of any changes in QoS values allowing timely intervention in order to prevent undesired consequences. We validated our approach using a series of experiments and response time datasets from real world web services.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6113911,no
Software-Based Instrumentation for Localization of Faults Caused by Electrostatic Discharge,"Electrostatic discharge (ESD) is often the cause of system-level failure or malfunction of embedded systems. The underlying faults are difficult to localize, as the information gained from the hardware-based diagnostic methods typically in use lacks sufficient detail. The alternative proposed in this paper is software instrumentation that monitors key registers and flags to detect anomalies indicative of failure. In contrast to hardware-based techniques, which use invasive probes that can alter the very phenomena being studied, the proposed approach makes use of standard peripherals such as the serial or Ethernet port to monitor and record the effect of ESD. We illustrate the use of this software instrumentation technique in conjunction with a three-dimensional ESD injection system to produce a sensitivity map that visualizes the susceptibility of various segments of an embedded system to ESD.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6113916,no
Cardio: Adaptive CMPs for reliability through dynamic introspective operation,"Current technology scaling enables the integration of tens of processing elements into a single chip, and future technology nodes will soon allow the integration of hundreds of cores per device. While very powerful, many experts agree that these systems will be prone to a significant number of permanent and transient faults during their lifetime. If not properly handled, effects of runtime failures can be dramatic. In this work, we propose Cardio, a distributed architecture for reliable chip multiprocessors. Cardio, a novel approach for on-chip reliability is based on hardware detectors that spot failures and on software routines that reorganize the system to work around faulty components. Compared to previous online reliability solutions, Cardio provides failure reactivity comparable to hardware-only reliable solutions while requiring a much lower area overhead. Cardio operates a distributed resource manager to collect health information about components and leverages a robust distributed control mechanism to manage system-level recovery. Our architecture operational as long as at least one general purpose processor is still functional in the chip. We evaluated our design using a custom simulator and estimate its runtime impact on the SPECMPI benchmarks to be lower than 3%. We estimate its dynamic reconfiguration time to be comprised between 20 and 50 thousand cycles per failure.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6113983,no
Full-system analysis and characterization of interactive smartphone applications,"Smartphones have recently overtaken PCs as the primary consumer computing device in terms of annual unit shipments. Given this rapid market growth, it is important that mobile system designers and computer architects analyze the characteristics of the interactive applications users have come to expect on these platforms. With the introduction of high-performance, low-power, general purpose CPUs in the latest smartphone models, users now expect PC-like performance and a rich user experience, including high-definition audio and video, high-quality multimedia, dynamic web content, responsive user interfaces, and 3D graphics. In this paper, we characterize the microarchitectural behavior of representative smartphone applications on a current-generation mobile platform to identify trends that might impact future designs. To this end, we measure a suite of widely available mobile applications for audio, video, and interactive gaming. To complete this suite we developed BBench, a new fully-automated benchmark to assess a web-browser's performance when rendering some of the most popular and complex sites on the web. We contrast these applications' characteristics with those of the SPEC CPU2006 benchmark suite. We demonstrate that real-world interactive smartphone applications differ markedly from the SPEC suite. Specifically the instruction cache, instruction TLB, and branch predictor suffer from poor performance. We conjecture that this is due to the applications' reliance on numerous high level software abstractions (shared libraries and OS services). Similar trends have been observed for UI-intensive interactive applications on the desktop.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6114205,no
Evaluating the viability of process replication reliability for exascale systems,"As high-end computing machines continue to grow in size, issues such as fault tolerance and reliability limit application scalability. Current techniques to ensure progress across faults, like checkpoint-restart, are increasingly problematic at these scales due to excessive overheads predicted to more than double an application's time to solution. Replicated computing techniques, particularly state machine replication, long used in distributed and mission critical systems, have been suggested as an alternative to checkpoint-restart. In this paper, we evaluate the viability of using state machine replication as the primary fault tolerance mechanism for upcoming exascale systems. We use a combination of modeling, empirical analysis, and simulation to study the costs and benefits of this approach in comparison to check-point/restart on a wide range of system parameters. These results, which cover different failure distributions, hardware mean time to failures, and I/O bandwidths, show that state machine replication is a potentially useful technique for meeting the fault tolerance demands of HPC applications on future exascale platforms.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6114406,no
Large scale debugging of parallel tasks with AutomaDeD,"Developing correct HPC applications continues to be a challenge as the number of cores increases in today's largest systems. Most existing debugging techniques perform poorly at large scales and do not automatically locate the parts of the parallel application in which the error occurs. The over head of collecting large amounts of runtime information and an absence of scalable error detection algorithms generally cause poor scalability. In this work, we present novel, highly efficient techniques that facilitate the process of debugging large scale parallel applications. Our approach extends our previous work, AutomaDeD, in three major areas to isolate anomalous tasks in a scalable manner: (i) we efficiently compare elements of graph models (used in AutomaDeD to model parallel tasks) using pre-computed lookup-tables and by pointer comparison; (ii) we compress per-task graph models before the error detection analysis so that comparison between models involves many fewer elements; (iii) we use scalable sampling-based clustering and nearest-neighbor techniques to isolate abnormal tasks when bugs and performance anomalies are manifested. Our evaluation with fault injections shows that AutomaDeD scales well to thousands of tasks and that it can find anomalous tasks in under 5 seconds in an online manner.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6114453,no
Self-Awareness in Autonomous Nano-Technology Swarm Missions,"NASA is currently exploring swarm-based technologies, targeting the development of prospective exploration missions to explore regions of space, where single large spacecraft would be impractical. Such systems are envisioned to operate autonomously and their success factor depends highly on self-awareness capabilities. This research emphasizes the development of algorithms and prototyping models for self-awareness in swarm-based space-exploration systems. This article tackles the self-initiation and self-healing properties of swarm-based space-exploration systems.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6114817,no
Device Driver Generation and Checking Approach,"Optimizing time and effort in embedded systems design is essential nowadays. The increased productivity gap together with the reduced time to market make the design of some components of the system the main design bottleneck.Taking into account the natural complexity of HdS design, a software checking technique helps finding bugs. However the increasing complexity of HdS makes the development and use of checking techniques a challenge.Reducing the time spent to build the checking environment can be a solution for this kind of problem. This can be accomplished by automating the generation of the checking environment from a device specification. The use of virtual platforms also represents an advantage since it supports to start the HdS development in an initial design phase.This paper proposes an approach for checking errors during the development of a very error prone Hardware dependent Software, that is device drivers. The proposed checking mechanism can be generated from a device specification using a language called Temporal DevC. Taking a device description in TDevC, the proposed approach generates a driver checking mechanism based on state machines. Experiments show the efficiency and effectiveness of the proposed mechanism, enabling its use for the detection of unwanted flows in the device driver simulation as well.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6114832,no
Testing embedded software by metamorphic testing: A wireless metering system case study,"In this paper, we present our experience of testing wireless embedded software. We used a wireless metering system in operation, and its software as a case study to demonstrate how a property-based testing technique, called metamorphic testing, can be used in detecting software failures of this wireless embedded system. Our study shows that a careful design of test environments and selection of system properties will enable us to trace back the cause of failures and help in fault diagnosis and debugging.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6115306,no
An adaptive H.264 video protection scheme for video conferencing,"Real-time video communication such as Internet video conferencing is often afflicted by packet loss over the network. To improve the quality of video, error protection schemes have been introduced based on FMO in H.264 whose encoding efficiency is unacceptable. This paper presents a novel region of interest (ROI) protection scheme that can accurately extract ROI area using facial recognition and greatly speedup video encoding based on feedback using x264 codec implementation. In this scheme, the video receiver uses a packet loss prediction model to predict whether to send feedback to the video sender that dynamically adjust the ROI protecting scheme. Experiments prove that the quality of the ROI area can be effectively improved by the scheme whose encoding performance increases by 50 times compared with FMO based algorithms.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6115954,no
Pixel domain referenceless visual degradation detection and error concealment for mobile video,"In mobile video applications, where unreliable networks are commonplace, corrupted video packets can have a profound impact on the quality of the user experience. In this paper, we show that, in a wide range of operating conditions, selectively reusing data resulting from decodable errorneous packets leads to better results than frame copy. This selection is guided by a novel concept that combines motion estimation and a measure of blocking artifacts at block edges to predict visual degradation caused by the decoding of erroneous packets. Simulation results show that, by using the proposed solution, the H.264/AVC JM reference software decoder can select the best option between frame copy and the erroneous frame decoding in 82% of test cases. We also obtain an average gain of 1.95 dB for concealed frames (when they differ from those concealed by the JM decoder).",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6116080,no
Automated image quality assessment for camera-captured OCR,"Camera-captured optical character recognition (OCR) is a challenging area because of artifacts introduced during image acquisition with consumer-domain hand-held and Smart phone cameras. Critical information is lost if the user does not get immediate feedback on whether the acquired image meets the quality requirements for OCR. To avoid such information loss, we propose a novel automated image quality assessment method that predicts the degree of degradation on OCR. Unlike other image quality assessment algorithms which only deal with blurring, the proposed method quantifies image quality degradation across several artifacts and accurately predicts the impact on OCR error rate. We present evaluation results on a set of machine-printed document images which have been captured using digital cameras with different degradations.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6116204,no
"The evidential independent verification of software of information and control systems, critical to safety: Functional model of scenario","The results of development of the techniques which form the scenario of target technology Evidential independent verification of I&C Systems Software of critical application and utilities of the scenario support at information, analytical and organizational levels are presented in the article. The result of the scenario implementation is the quantitative definition of latent faults probability and completeness of test coverage for critical software. This technology can be used by I&C systems developers, certification and regulation bodies to carry out independent verification (or certification) during modernization and modification of critical software directly on client objects without intruding (interrupting) in technological processes.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6116420,no
Diagnosis infrastructure of software-hardware systems,"This article describes an infrastructure and technologies for diagnosis. A transactional graph model and method for diagnosis of digital system-on-chip are developed. They are focused to considerable decrease the time of fault detection and memory for storage of diagnosis matrix by means of forming ternary relations in the form of test, monitor, and functional component. The following problems are solved: creation of digital system model in the form of transaction graph and multitree of fault detection tables, as well as ternary matrices for activating functional components in tests, relative to the selected set of monitors; development of a method for analyzing the activation matrix to detect the faults with given depth and synthesizing logic functions for subsequent embedded hardware fault diagnosing.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6116425,no
Software testing of a simple network,"It is costly to have defective networks and nodes. There are many factors involved in the cost of defective design of networks. The size of development team, stage of development when the defect occurs, routing protocols and subtlety of the defect are only a few of the possibilities. Testing software, therefore has to be designed to detect the defect, and as early as possible in the design cycle. Otherwise the costs can be overwhelming. This is yet another compelling argument for QA engineers to justify up-front test costs similar to the electronics design programs of JTAG (Joint Test Action Group for boundary scan) or BIST (Built-in Self Test) circuitry.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6116429,no
A stochastic formulation of successive software releases with faults severity,"Software companies are coming with multiple add-ons to survive in the pure competitive environment. Each succeeding up-gradation offers some performance enhancement and distinguishing itself from the past release. If the size of the software system is large, the number of faults detected during the testing phase becomes large, and the number of faults, which are removed through each debugging, becomes small compared to initial fault content at the beginning of the testing phase. In such a situation, we can model the software fault detection process as a stochastic process with continuous state space. In this paper, we propose a multi-release software reliability growth model based on Ito's type of differential equation. The model categorizes Faults in two categories: simple and hard with respect to time which they take for isolation and removal after their observation. The model developed is validated on real data set.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6117894,no
A Connection-Based Signature Approach for Control Flow Error Detection,"Control Flow Errors (CFEs) are major impairments of software system correctness. These CFEs can be caused by operational faults with respect to the execution environment of a software system. Several techniques are proposed to monitor the control flow using signature-based approaches. These techniques partition a software program into branch-free blocks and assign a unique signature for each block. They detect CFEs by comparing the runtime signatures of these blocks with pre-computed signatures based on the program Control Flow Graph (CFG). Unfortunately, branch-free block partitioning does not completely include all the program connections. Consequently, these techniques may fail to detect some invalid transitions due to lack of signatures associated with those missing connections. In this paper, we propose a connection-based signature approach for CFE detection. We first describe our connection-based signature structure in which we partition the program components into Connection Implementation Blocks (CIBs). Each CIB is associated with a Connection-based CFG (CCFG) to represent the control structure of its code segment. We present our control flow monitor structure and CFE checking algorithm using these CCFGs. The error detection approach is evaluated using PostgreSQL open-source database. The results show that this technique is capable of detecting CFEs in different software versions with variable numbers of randomly injected faults.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6118363,no
Investigation on Safety-Related Standards for Critical Systems,"In each application domain for safety-critical systems, international organizations have issued regulations concerned with the development, implementation, validation and maintenance of safety-critical systems. In particular, each of them indicate a definition of what safety means, proper qualitative and quantitative properties for evaluating the quality of the system under development, and a set of methodologies to be used for assessing the fulfilment of the mentioned properties. These standards are today and essential tool for ensuring the required safety levels in many domains that require extremely high dependability. This paper summarizes the analysis on a set of well-known safety standards in different domains of critical systems with the intend of highlighting similarities and differences among them, pointing out common areas of interest and reporting on which features the newest (and upcoming) standards are focusing.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6118514,no
Assessing Measurements of QoS for Global Cloud Computing Services,"Many global distributed cloud computing applications and services running over the Internet, between globally dispersed clients and servers, will require certain levels of Quality of Service (QoS) in order to deliver and give a sufficiently smooth user experience. This would be essential for real-time streaming multimedia applications like online gaming and watching movies on a pay as you use basis hosted in a cloud computing environment. However, guaranteeing or even predicting QoS in global and diverse networks supporting complex hosting of application services is a very challenging issue that needs a stepwise refinement approach to be solved as the technology of cloud computing matures. In this paper, we investigate if latency in terms of simple Ping measurements can be used as an indicator for other QoS parameters such as jitter and throughput. The experiments were carried out on a global scale, between servers placed in universities in Denmark, Poland, Brazil and Malaysia. The results show some correlation between latency and throughput, and between latency and jitter, even though the results are not completely consistent. As a side result, we were able to monitor the changes in QoS parameters during a number of 24-hour periods. This is also a first step towards defining QoS parameters to be included in Service Level Agreements for cloud computing in the foreseeable future.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6118936,no
A Runtime Fault Detection Method for HPC Cluster,"As the number of nodes keeps increasing, faults have become commonplace for HPC cluster. For fast recovery from faults, the fault detection method is necessary. Based on the usage patterns of HPC cluster, a automatic runtime fault detection mechanism is proposed in this paper: First, the normal activities for nodes in HPC cluster are modeled using runtime state by clustering analysis, Second, the fault detection process is implemented by comparing the current runtime state of nodes with normal activity models. A fault alarm is made immediately when the current runtime state deviates from the normal activity models. In the experiments, the faults are simulated by fault injection methods and the experimental results show that the runtime fault detection method in this paper can detect faults with high accuracy.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6118961,no
Analysis of rotor fault detection in inverter fed induction machines at no load by means of finite element method,"This paper analyzes a new method for detecting defective rotor bars at zero load and standstill by means of modeling using the finite element method (FEM). The detection method uses voltage pulses generated by the switching of the inverter to excite the machine and measures the corresponding reaction of the machine phase currents, which can be used to identify a modulation of the transient leakage inductance caused by asymmetries within the machine. The presented 2D finite element model and the simulation procedure are oriented towards this approach and are developed by means of the FEM software ANSYS. The analysis shows how the transient flux linkage imposed by voltage pulses is influenced by a broken bar leading to very distinct rotor-fixed modulation, that can be clearly exploited for monitoring. Simulation results are presented to show the transient flux paths. These simulation results are supported by measurements on a specially manufactured induction machine.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6119572,no
Component-wise optimization for a commercial central cooling plant,"Thermal comfort and energy savings are two main goals of heating, ventilation and air conditioning (HVAC) systems. In this paper, the optimization-simulation approach is proposed for effective energy saving potential in a commercial central cooling plant by refining the model of optimal operation for system components and deriving optimal conditions for their operation subject to technical and human comfort constraints. To investigate the potential of energy savings and air quality, a real-world commercial building, located in a hot and dry climate region, together with its central cooling plant is used for experimentation and data collection. Both inputs and outputs of the existing central cooling plant are measured from the field monitoring in one typical week in the summer. Optimization is performed by using empirically-based models of the central cooling plant components. Optimization algorithms implemented on a transient simulation software package, are used to solve the minimization problem of energy consumption for each considered control strategies and predict the HVAC system optimized set-points under transient load. The integrated simulation tool was validated by comparing predicted and measured power consumption of the chiller during the first day of July. Results show that between 3.2% and 11.8% power savings can be obtained by this approach while maintaining the predicted mean vote (PMV) from -0.5 to +1 for most of the summer time.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6119750,no
System Failure Forewarning Based on Workload Density Cluster Analysis,"Each computer system contains design objectives for long-term usage, so the operator must conduct a continuous and accurate assessment of system performance in order to detect the potential factors that will degrade system performance. Condition indicators are the basic components of diagnosis. It is important to select feature vectors that meet the criteria in order to provide true accuracy and powerful diagnostic routines. Our goal is to indicate the actual system status according to the workload, and use clustering techniques to analyze the workload distribution density to build diagnostic templates. Such templates can be used for system failure forewarning. In the proposed system, we present an approach, based on workload density cluster analysis to automatically monitor the health of software systems and system failure forewarning. Our approach consists of tracking the workload density of metric clusters. We employ the statistical template model to automatically identify significant changes in cluster moving, therefore enabling robust fault detection. We observed two circumstances from the experiment results. First, under most normal status, the lowest accuracy value is approximate our theoretical minimum threshold of 84%. Such result implies a close correlation between our measured and real system status. Second, the command data used by the system could predict 90% of events announced, which reveals the prediction effectiveness of this proposed system. Although it is infeasible for the system to process the largest possible fault events in the deployment of resources, we could apply statistics to characterize the anomalous behaviors to understand the nature of emergencies and to test system service under such scenarios.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6120749,no
Variable Precision Rough Set-Based Fault Diagnosis for Web Services,"Web service is the emergent technology for constructing more complex and flexible software system for business applications. However, some new features of Web service-based software such as heterogeneity and loose coupling bring great trouble to the latter fault debugging and diagnosis. In the paper, variable precision rough set-based diagnosis framework is presented. In such debugging model, SOAP message monitoring and service invocation instrument are used to record service interface information. Meanwhile, factors of execution context are also viewed as conditional attributes of knowledge representation system. The final execution result is treated as the decision attribute, and failure ontology is utilized to classify system's failure behaviors. Based on this extended information system, variable precision rough set reasoning is performed to generate the probability association rules, which are the clues for locating the possible faulty services. In addition, the experiment on a real-world Web services system is performed to demonstrate the feasibility and effectiveness of our proposed method.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6121011,no
A hybrid method for constructing High Level Architecture of BBS user network,"It is useful to understand the High Level Architecture (HLA) of the user network of Bulletin Board Systems (BBS) for some applications. In this paper, we construct the HLA of a BBS user network through hybrid static and dynamic analysis of the quantitative temporal graphs that are extracted from the BBS entries. We detect the HLA framework first though the static structural analysis of the aggregation of the temporal graphs. Then, we identify the HLA components including communities, community cores, and hubs elaborately through the dynamic analysis of the quantitative temporal attributes of nodes. The hybrid method guarantees the HLA quality as it removes the false components from the HLA. It controls the computational cost at a low level also. A metric is proposed to evaluate the HLA efficiency in information transmission. The experiments show that the HLA constructed by the hybrid method outperforms that constructed by the comparative method.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6122657,no
Algorithm analyzer to check the efficiency of codes,Efficiency of codes developed is always an issue in software development. Software can be said to be of good quality if the measurable features of the software can be quantitatively checked for adoption of standards or following certain set rules. Software metrics can therefore come into play in the sense of helping to measure certain characteristics of software. The issue and factors pertaining to efficiency of a code will be addressed by software metrics. Existing tools that are used to analyze several software metrics have come a long way in helping to assess this very important part of software development. This paper described how software metrics can be used in analyzing efficiency of the developed code in early stage of development. A tool (algorithm analyzer) was developed to enable analyze a given code to check its efficiency level and produce efficiency reports based on the analysis. The system is able to help the code checking whilst maintaining the standards of coding for its users. With the reports that are generated it would be easy for users to determine the efficiency of their object oriented codes.,2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6122740,no
A study of process improvement best practices,"Software project success depends on various reasons including project control, software standards and procedures. Software development organizations realize the importance of using best practices to improve software development practices. An increasing number of literature have described about process improvement best practices and standards. Formal process improvement frameworks have emerged widely to promote the use of systematic processes for software engineering. These approaches identify best practices for managing software engineering quality. They provide methods for assessing an organization's process maturity level and capability. In this article, recent process improvement best practices and standards are presented. Its objective is to analyze the existing approaches towards software process improvement initiatives. Another objective is to determine the issues related to adoption of process improvement and standards. The research outcome is to obtain the significant process improvement issues and formulate a classification of generic steps for process improvements.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6122742,no
Design of the mechanical condition monitoring system for Molded Case Circuit Breakers,"In this paper, a mechanical condition monitoring system of Molded Case Circuit Breaker (MCCB) is designed and developed. The operation principle of the monitoring system, the hardware design and the software design are introduced in detail. The three-phase voltage, the voltage between auxiliary normally closed contacts and the motor current are detected during the opening operation, closing operation and reclosing operation of MCCB, wherein these operations are driven by the motor. The mechanical condition characteristic parameters, including closing time, opening time, reclosing time, three phases asynchronous time, closing speed, opening speed, reclosing speed and the force on the handle, can be calculated by analyzing the voltage signals and current signals. The test results show that the system has a good performance. Moreover the characteristic parameters of circuit breaker, obtained in the test, provide test data for the theory research on remaining life prediction of MCCB.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6122986,no
Wavelet analysis and application on the technology detecting single phase-to-ground fault line for distribution network,"The single phase-to-ground fault is the most frequent accident in distribution network. In order to achieve detecting fault line, the wavelet packet decomposition method was used in this paper. The first step is to analyze the transient characteristics of fault line and the wavelet packet decomposition of transient ground capacitive current. Then the energy method is used to extract the feature band, so judging fault line can be done by analyzing the relationship of amplitude and polarity with singularity and modulus maxima theory. In the end, the feasibility of this method is verified with MATLAB software.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6123055,no
SmartCM a smart card fault injection simulator,"Smart card are often the target of software or hardware attacks. The most recent attack is based on fault injection which modifies the behavior of the application. We propose an evaluation of the effect of the propagation and the generation of hostile application inside the card. We designed several countermeasures and models of smart cards. Then we evaluate the ability of these countermeasures to detect the faults, and the latency of the detection. In a second step we evaluate the mutant with respect to security properties in order to focus only on the dangerous mutants.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6123124,no
Exploiting Text-Related Features for Content-based Image Retrieval,"Distinctive visual cues are of central importance for image retrieval applications, in particular, in the context of visual location recognition. While in indoor environments typically only few distinctive features can be found, outdoors dynamic objects and clutter significantly impair the retrieval performance. We present an approach which exploits text, a major source of information for humans during orientation and navigation, without the need for error-prone optical character recognition. To this end, characters are detected and described using robust feature descriptors like SURF. By quantizing them into several hundred visual words we consider the distinctive appearance of the characters rather than reducing the set of possible features to an alphabet. Writings in images are transformed to strings of visual words termed visual phrases, which provide significantly improved distinctiveness when compared to individual features. An approximate string matching is performed using N-grams, which can be efficiently combined with an inverted file structure to cope with large datasets. An experimental evaluation on three different datasets shows significant improvement of the retrieval performance while reducing the size of the database by two orders of magnitude compared to state-of-the-art. Its low computational complexity makes the approach particularly suited for mobile image retrieval applications.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6123328,no
Efficient Clustering-based Algorithm for Predicting File Size and Structural Similarity of Transcoded JPEG Images,"The problem of adapting JPEG images to satisfy constraints such as file size and resolution arises in a number of applications, from universal media access to multimedia messaging services. Visually optimized adaptation, however, commands a non-negligible computational cost which we aim to minimize using predictors. In previous works, we presented predictors and systems to achieve low-cost near-optimal adaptation of JPEG images. In this work, we propose a new approach to file size and quality prediction resulting from the Transcoding of a JPEG image subject to changes in quality factor and resolution. We show that the new predictor significantly outperforms the previously proposed solutions in accuracy.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6123337,no
Towards Energy Consumption Measurement in a Cloud Computing Wireless Testbed,"The evolution of the Next Generation Networks, especially the wireless broadband access technologies such as Long Term Evolution (LTE) and Worldwide Interoperability for Microwave Access (WiMAX), have increased the number of """"all-IP"""" networks across the world. The enhanced capabilities of these access networks has spearheaded the cloud computing paradigm, where the end-users aim at having the services accessible anytime and anywhere. The services availability is also related with the end-user device, where one of the major constraints is the battery lifetime. Therefore, it is necessary to assess and minimize the energy consumed by the end-user devices, given its significance for the user perceived quality of the cloud computing services. In this paper, an empirical methodology to measure network interfaces energy consumption is proposed. By employing this methodology, an experimental evaluation of energy consumption in three different cloud computing access scenarios (including WiMAX) were performed. The empirical results obtained show the impact of accurate network interface states management and application network level design in the energy consumption. Additionally, the achieved outcomes can be used in further software-based models to optimized energy consumption, and increase the Quality of Experience (QoE) perceived by the end-users.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6123444,no
Implementation and Usability Evaluation of a Cloud Platform for Scientific Computing as a Service (SCaaS),"Scientific computing requires simulation and visualization involving large data sets among collaborating teams. Cloud platforms offer a promising solution via ScaaS. We report on the architecture, implementation and User Experience (UX) evaluation of one such SCaaS platform implementing TOUGH2V2.0, a numerical simulator for sub-surface fluid and heat flow, offered as a service. Results from example simulations, with virtualization of workloads in a multi-tenant, Virtual Machine (VM)-based cloud platform, are presented. These include fluid production from a geothermal reservoir, diffusive and advective spreading of contaminants, radial flow from a CO2 injection well and gas diffusion of a chemical through porous media. Prepackaged VM pools deployed autonomically ensure that sessions are provisioned elastically on demand. Users can access data-intensive visualizations via a web-browser. Authentication, user state and sessions are managed securely via an Access Gateway, to autonomically redirect and manage the workflows when multiple concurrent users are accessing their own sessions. Usability in the cloud and the traditional desktop are comparatively assessed, using several UX metrics. Simulated network conditions of different quality were imposed using a WAN emulator. Usability was found to be good for all the simulations under even moderately degraded network quality, as long as latency was not well above 100 ms. Hosting of a complex scientific computing application on an actual, global Enterprise cloud platform (as opposed to earlier remoting platforms) and its usability assessment, both presented for the first time, are the essential contributions of this work.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6123522,no
Development of an Online Energy Auditing Software Application with Remote SQL-Database Support,"Energy efficiency is a very current topic, both locally and internationally. It follows that methodologies to improve efficiency are also gaining importance. In the context of the local utility Eskom, Demand Side Management and improved Energy Efficiency in particular have been identified as major components of the campaign to reduce the negative impacts of the current constraints in generation and transmission capacities. Manual methodologies, however, are time-consuming, error-prone and require highly skilled manpower. This paper describes the development of an energy auditing tool to assist in improving the energy auditing process together with a methodology for calculating the usage-profiles of the various loads. The theoretical results are also presented.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6125558,no
Optimal Sizing of Combined Heat & Power (CHP) Generation in Urban Distribution Network (UDN),"The capacity of Combine Heat and Power (CHP) generation connected to Urban Distribution Network (UDN) will increase significantly as a result of EU government targets and initiatives. CHP generation can have significant impact on the power flow, voltage profile, fault current level and the power quality for customers and electricity suppliers. The connection of CHP plant at UDN creates a number of welldocumented impacts with voltage rise and fault current level being the dominant effects. A range of options have traditionally been used to mitigate adverse impacts but these generally revolve around network upgrades, the cost of which may be considerable. Connection of CHP generation can fundamentally alter the operation of UDN. Where CHP plant capacity is comparable to or larger than local demand there are likely to be observable impacts on network power flows, voltage regulation and fault current level. New connection of CHP schemes must be evaluated to identify and quantify any adverse impact on the security and quality of local electricity supplies. The impacts that arise from an individual CHP scheme are assessed in details when the developer makes an application for the connection of the CHP plant. The objective of this paper is to use static method to develop techniques that provide means of determining the optimum capacity of a CHP plant that may be accommodate within UDN. The main tool used in this paper is ERAC power analyzing software incorporating load flow and fault current level analysis. These analysis are demonstrated on 15 busbar network resembling part of typical UDN. In order to determine optimal placement and sizing of a CHP plant that could be connected at any particular busbar on UDN without causing a significant adverse impact on performance of the UDN, the multiple linear regression model is created and demonstrated using the data obtain by the analysis performed by ERAC power analyzing software.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6125628,no
AudioGene: Computer-based prediction of genetic factors involved in non-syndromic hearing impairment,"AudioGene is a software system developed at the University of Iowa to classify and predict gene mutations that indicate causal or increased risk factors of disease. We focus on a concise example - the most likely genetic causes of a particular form of inherited hearing loss - ADNSHL. Whereas the cost and throughput involved in the collection of genomic data have advanced dramatically during the past decade, gathering and interpreting clinical information regarding disease diagnosis remains slow, costly and error-prone. AudioGene employs machine-learning techniques in an iterative procedure to prioritize probable genetic risk factors of disease, which are then verified with a molecular (wet lab) assay. In our current implementation AudioGene achieves 67% first-choice accuracy (versus 23% using a majority classifier). When the top three choices are considered, accuracy increases to 83%. This has numerous implications for reducing the cost of genetic screening as well as increasing the power of novel gene discovery efforts. While AudioGene is focused on hearing loss, the design and underlying mechanisms are generalizable to many other diseases including heart disease, cancer and mental illness.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6126605,no
Automatic functionality detection in behavior-based IDS,"Detection of malicious functionalities presents an effective way to detect malware in behavior-based IDS. A technology including the utilization of Colored Petri Nets for the generalized description and consequent detection of specific malicious functionalities from system call data has been previously developed, verified and presented. A successful effort was made to neutralize possible attempts to obfuscate this approach. Nevertheless, the approach has two major drawbacks. First, target functionalities have to be initially specified by an expert, which is a time consuming, sometimes subjective and error prone process. Second, the identification of typical functionalities indicative of malicious programs is not generally straightforward and requires reverse engineering and careful study of many instances of malware. Our paper addresses these drawbacks, clearing the way for a full-scale practical application of this technology. We utilized graph mining and graph similarity assessment algorithms for processing system call data resulting in automatic extraction of functionalities from system call data. This enabled us to identify sets of functionalities suggesting software maliciousness and construct a general obfuscation-resilient malware detector. The paper presents the results of the implementation and testing of the described technologies on the computer network testbed.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6127482,no
Computational resiliency for distributed applications,"In recent years, computer network attacks have decreased overall reliability of computer systems and undermined confidence in mission-critical software. These robustness issues are magnified in distributed applications, which provide multiple points of failure and attack. The notion of resiliency is concerned with constructing applications that are able to operate through a wide variety of failures, errors, and malicious attacks. A number of approaches have been proposed in the literature based on fault tolerance achieved through replication of resources. In general, these approaches provide graceful degradation of performance to the point of failure but do not guarantee progress in the presence of multiple cascading and recurrent failures. Our approach is to dynamically replicate message-passing processes, detect inconsistencies in their behavior, and restore the level of fault tolerance as a computation proceeds. This paper describes a novel operating system technology for resilient message-passing applications that is automated, scalable, and transparent. The technology provides mechanisms for process replication, process migration, and adaptive failure detection. To quantify the performance overhead of the technology, we benchmark a distributed application exemplar to represent a broader class of applications.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6127514,no
Analysis and implementation of the virtual network system,"Messaging applications want to use different communication networks. But the unfortunate state of affairs is that applications need to use several different application programming interfaces (APIs) and to design protocols on how and when to use a specific communication network(s). This is troublesome, error-prone and APIs vary a lot; applications want to use just one API but get the benefit of several communications networks. In this paper we detail, implement and test in real field tests a virtual network system (VNS). We argue why messaging applications should use and benefit from VNS. The VNS is a middleware solution that enables seamless usage of different networks. Netlink Next Generation (NLNG) protocol is a practical implementation of VNS and we elaborate on it's features, design choices and problems faced. The NLNG protocol has its origins in the Linux Netlink and VNS concept is a middleware, thus we provide a comparison between these previous works and our work. The VNS system and NLNG protocol have already been tested with several applications, e.g., mail clients, tracking and command softwares, and network interfaces, e.g., IP, VHF, HF, GSM SMS and TETRA SDS.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6127528,no
Adaptive Failure Detection via Heartbeat under Hadoop,"Hadoop has become one popular framework to process massive data sets in a large scale cluster. However, it is observed that the detection of the failed worker is delayed, which may result in a significant increase in the completion time of jobs with different workload. To cope with it, we present two mechanisms: Adaptive interval and Reputation-based Detector that support Hadoop to detect the failed worker in the shortest time. The Adaptive interval is trying to dynamically configure the expiration time which is adaptive to the job size. The Reputation-based Detector is trying to evaluate the reputation of each worker. Once the reputation of a worker is lower than a threshold, then the worker will be considered as a failed worker. In our experiments, we demonstrate that both of these strategies have achieved great improvement in the detection of the failed worker. Specifically, the Adaptive interval has a relatively better performance with small jobs, while the Reputation-based Detector is more suitable for large jobs.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6127967,no
Software Fault Prediction Framework Based on aiNet Algorithm,"Software fault prediction techniques are helpful in developing dependable software. In this paper, we proposed a novel framework that integrates testing and prediction process for unit testing prediction. Because high fault prone metrical data are much scattered and multi-centers can represent the whole dataset better, we used artificial immune network (aiNet) algorithm to extract and simplify data from the modules that have been tested, then generated multi-centers for each network by Hierarchical Clustering. The proposed framework acquires information along with the testing process timely and adjusts the network generated by aiNet algorithm dynamically. Experimental results show that higher accuracy can be obtained by using the proposed framework.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6128133,no
A controlled switching methodology for transformer inrush current elimination: Theory and experimental validation,"Transformers are generally energized by closing the circuit breakers at random times. Consequently, this operation generates high transient inrush currents as a result of the asymmetrical magnetic flux produced in the windings. In light of these facts, this paper presents a strategy to control the switching phenomena which occurs during power transformer inrush. The general idea consists of calculating the pre-existing magnetic fluxes left on the core limbs as a function of operating voltage previously applied to the transformer, just prior to the moment in which de-energization has happened. By using these data and the equations to predict the most suitable closing moments, it is shown the proposal effectiveness at accomplishing the main target here pointed out. Experimental investigations are carried out in order to demonstrate the application method and its validation. The results show the feasibility of building hardware and software structures to drastically reduce the transformer inrush currents.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6128821,no
Specialist tool for monitoring the measurement degradation process of induction active energy meters,"This paper presents a methodology and a specialist tool for failure probability analysis of induction type watt-hour meters, considering the main variables related to their measurement degradation processes. The database of the metering park of a distribution company, named Elektro Electricity and Services Co., was used for determining the most relevant variables and to feed the data in the software. The modeling developed to calculate the watt-hour meters probability of failure was implemented in a tool through a user friendly platform, written in Delphi language. Among the main features of this tool are: analysis of probability of failure by risk range; geographical localization of the meters in the metering park, and automatic sampling of induction type watt-hour meters, based on a risk classification expert system, in order to obtain information to aid the management of these meters. The main goals of the specialist tool are following and managing the measurement degradation, maintenance and replacement processes for induction watt-hour meters.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6128831,no
Efficient mode selection with extreme value detection based pre-processing algorithm for H.264/AVC fast intra mode decision,"The mode decision in the intra prediction of a H.264/AVC encoder requires complicated computations, and spends much time to select the best mode that achieves the minimum rate-distortion (RD). The complicated computations for the mode decision cause the difficulty in real-time applications, especially for software based encoders. This study creates an efficient fast algorithm, which is called Extreme Value Detection (EVD), to predict the best direction mode except for the DC mode for fast intra mode decision. The EVD based edge detection predicts luma-44, luma-1616, and chroma-88 modes effectively. At the first step, we use the pre-processing mode selection algorithm to find the primary mode which is selected for fast prediction. At the second step, the selected fewer high-potential candidate modes are applied to calculate the RD cost for the mode decision. This method reduces encoding time effectively, and meanwhile also maintains the same video quality. Simulation results show that the proposed EVD method reduces the encoding time by 63%, and requires bit-rate increase about 2.6% and peak signal-to-noise ratio (PSNR) decrease about 0.08 dB in QCIF and CIF sequences, compared with the H.264/AVC JM 14.2 software. This method achieves less PSNR degradation and bit-rate increase than the previous methods with more encoding time reduction.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6129116,no
Prototype design of low cost four channels digital electroencephalograph for sleep monitoring,"The electrical activity in brain or known as electroencephalogram (EEG) signal is being used in the diagnosis of sleep quality. Based on EEG signal, power of brain wave that related with a sleep quality could be obtained by analysis of power spectral density. The problem in developing countries, for example in Indonesia, EEG instrument is not widely available in each region of the country. This project designed and implemented four channels digital EEG, in which the design of hardware and software concepts was adopted from OpenEEG project. A four channels EEG amplifier operated by battery with average gain magnitude of 6100 times, bandwidth 0.05-60Hz and slope gradient of -60.00 dB/decade is developed. Digital board consists of AT-mega8 and serial interface with optocoupler is used to interface and viewed EEG signal on notebook. The prototype has successfully detected patterns of cardiac signal simultaneously with good SNR. In EEG measurement through monitoring the brain wave sleep, the data generated by PSD (Power Spectral Density) graph show the dominance of the brain signals at 7-9Hz (alpha) and 3-5Hz (theta). From several tests and measurements, this research concludes that the prototype of low cost EEG 4 channels is capable of acquiring satisfactory brain wave monitoring during sleep from healthy volunteer.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6130154,no
Memory Leak Detection Based on Memory State Transition Graph,"Memory leak is a common type of defect that is hard to detect manually. Existing memory leak detection tools suffer from lack of precise interprocedural alias and path conditions. To address this problem, we present a static interprocedural analysis algorithm, which captures memory actions and path conditions precisely, to detect memory leak in C programs. Our algorithm uses path-sensitive symbolic execution to track the memory actions in different program paths guarded by path conditions. A novel analysis model called Memory State Transition Graph (MSTG) is proposed to describe the tracking process and its results. An MSTG is generated from a procedure. Nodes in an MSTG contain states of memory objects which record the function behaviors precisely. Edges in anMSTG are annotated with path conditions collected by symbolic execution. The path conditions are checked for satisfiability to reduce the number of false alarms and the path explosion. In order to do interprocedural analysis, our algorithm generates a summary for each procedure from the MSTG and applies the summary at the procedure's call sites. Our implemented tool has found several memory leak bugs in some open source programs and detected more bugs than other tools in some programs from the SPEC2000 benchmarks. In some cases, our tool produces many false positives, but most of them are caused by the same code patterns which are easy to check.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6130667,no
Use Cases Modeling for Scalable Model-Checking,"Formal methods are effective techniques for automating software verifications to satisfy quality and reliability. However, the application of these techniques within industrial settings remains limited due to the complexity of produced models. Context-aware verification can circumvent this complexity by reducing the scope of the verification to some specific environmental conditions. We previously proposed a Context Description Language (CDL) to facilitate the formalization of requirements and contexts. However, the number of CDL models required to precisely formalize contexts grow rapidly according to the complexity of the system and manually writing CDL models is difficult and error prone task. In this paper, we propose a tool-supported framework that assists engineers in describing system contexts. We extended UML use cases with scenarios descriptions and we linked a domain specification vocabulary to automatically generate CDL models. An industrial case study is presented to illustrate the effectiveness of our approach.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6130671,no
RobusTest: A Framework for Automated Testing of Software Robustness,"Robustness of a software system is defined as the degree to which the system can behave ordinarily and in conformance with the requirements in extraordinary situations. By increasing the robustness many failures which decrease the quality of the system can be avoided or masked. When it comes to specifying, testing and assessing software robustness in an efficient manner the methods and techniques are not mature yet. This paper presents RobusTest, a framework for testing robustness properties of a system with currently focus on timing issues. The expected robust behavior of the system is formulated as properties. The properties are then used to automatically generate robustness test cases and assess the results. An implementation of RobusTest in Java is presented here together with results from testing different, open-source implementations of the XMPP instant messaging protocol. By executing 400 test cases that were automatically generated from properties on two such implementations we found 11 critical failures and 15 nonconformance problems as compared to the XMPP specification.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6130684,no
Evotec: Evolving the Best Testing Strategy for Contract-Equipped Programs,"Automated random testing is efficient at detecting faults but it is certainly not an optimal testing strategy for every given program. For example, an automated random testing tool ignores that some routines have stronger preconditions, they use certain literal values, or they are more error-prone. Taking into account such characteristics may increase testing effectiveness. In this article, we present Evotec, an enhancement of random testing which relies on genetic algorithms to evolve a best testing strategy for contract-equipped programs. The resulting strategy is optimized for detecting more faults, satisfying more routine preconditions and establishing more object states on a given set of classes to test. Our experiment tested 92 classes over 1710 hours. It shows that Evotec detected 29% more faults than random+ and 18% more faults than the precondition-satisfaction strategy.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6130699,no
Towards Automatic Discovery of co-authorship Networks in the Brazilian Academic Areas,"In Brazil, individual curricula vitae of academic researchers, that are mainly composed of professional information and scientific productions, are managed into a single software platform called Lattes. Currently, the information gathered from this platform is typically used to evaluate, analyze and document the scientific productions of Brazilian research groups. Despite the fact that the Lattes curricula has semi-structured information, the analysis procedure for medium and large groups becomes a time consuming and highly error-prone task. In this paper, we describe an extension of the script Lattes (an open-source knowledge extraction system from the Lattes platform), for analysing individuals Lattes curricula and automatically discover large-scale co-authorship networks for any academic area. Given some knowledge domain (academic area), the system automatically allows to identify researchers associated with the academic area, extract every list of scientific productions of the researchers, discretized by type and publication year, and for each paper, identify the co-authors registered in the Lattes Platform. The system also allows the generation of different types of networks which may be used to study the characteristics of academic areas at large scale. In particular, we explored the node's degree and Author Rank measures for each identified researcher. Finally, we confirm through experiments that the system facilitates a simple way to generate different co-authorship networks. To the best of our knowledge, this is the first study to examine large-scale co-authorship networks for any Brazilian academic area.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6130731,no
A New Approach to Evaluate Performance of Component-Based Software Architecture,"Nowadays, by technology developments, software systems enlarge in scale and complexity. In large systems and to overcome complexity, software architecture has been considered as a connected notion with product quality and plays a crucial role in the quality of final system. The aim of the analysis of software architecture is to recognize potential risks and investigating qualitative needs of software design before the process of production and implementation. Achievement to this goal reduces the costs and improves the software quality. In this paper, a new approach is presented to evaluate performance of component-based software architecture for software systems with distributed architecture. In this approach, at first system is modeled as a Discrete Time Markov Chain and then the required parameters are taken from, to produce a Product Form Queueing Network. Limitations of source, like restrictions of the number of threads in a particular machine, are also regarded in the model. The prepared model is solved by the SHARPE software packages. As the result of the solution of the produced model in this approach, throughput and the average response time and bottlenecks in different workloads of system are predicted and some suggestions are presented to improve the system performance.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6131280,no
Non-intrusive reconfigurable HW/SW fault tolerance approach to detect transient faults in microprocessor systems,"This paper presents a non-intrusive hybrid fault detection approach that combines hardware and software techniques to detect transient faults in microprocessors. Such faults have a major influence in microprocessor systems, affecting both data and control flow. In order to protect the system, an application-oriented hardware module is automatically generated and reconfigured on the system during runtime. When combined with fault tolerance techniques based on software, this solution offers full system protection against transient faults. A fault injection campaign is performed using a MIPS microprocessor executing a set of applications. HW/SW implementation in a reprogrammable platform shows minimal memory area and execution time overhead. Fault injection results show the efficiency of this method on detecting 100% of faults.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6131362,no
Microprocessor soft error rate prediction based on cache memory analysis,"Static raw soft-error rates (SER) of COTS microprocessors are classically obtained with particle accelerators, but they are far larger than real application failure rates that depend on the dynamic application behavior and on the cache protection mechanisms. In this paper, we propose a new methodology to evaluate the real cache sensitivity for a given application, and to calculate a more accurate failure rate. This methodology is based on the monitoring of cache accesses, and requires a microprocessor simulator. It is applied in this paper to the LEON3 soft-core with several benchmarks. Results are validated by fault injections on one implementation of the processor running the same programs: the proposed tool predicted all errors with only a small over-estimation.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6131417,no
Automated wafer defect map generation for process yield improvement,"Spatial Signature Analysis (SSA) is used to detect a reoccurring failure signature in today wafer fabrication. In order for SSA to be effective, it must correlate the signature to a wafer defect maps library. However, classifying the signatures for the library is time consuming and tedious. The Manual Visual Inspection (MVI) of several failure bins in a wafer map for multiple lots can lead to fatigue for the operator and resulted in inaccurate representation of the failure signature. Hence, an automated wafer map extraction process is proposed here to replace the MVI while ensuring accuracy of the failure signature library. Clustering tool namely Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is utilized to extract the wafer spatial signature while ignoring the outliners. The appropriate size for the clustered signature is investigated and its performance is compared to the MVI signature. The analysis shows that for 3 selected failure modes, 20% occurrence rate clustered pattern provide similar performance to a 50% MVI signature. The proposed technique leads to a significant reduction in the time required for extracting current and new signatures, allowing faster yield response and improvement.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6131959,no
"High level synthesis of stereo matching: Productivity, performance, and software constraints","FPGAs are an attractive platform for applications with high computation demand and low energy consumption requirements. However, design effort for FPGA implementations remains high - often an order of magnitude larger than design effort using high level languages. Instead of this time-consuming process, high level synthesis (HLS) tools generate hardware implementations from high level languages (HLL) such as C/C++/SystemC. Such tools reduce design effort: high level descriptions are more compact and less error prone. HLS tools promise hardware development abstracted from software designer knowledge of the implementation platform. In this paper, we examine several implementations of stereo matching, an active area of computer vision research that uses techniques also common for image de-noising, image retrieval, feature matching and face recognition. We present an unbiased evaluation of the suitability of using HLS for typical stereo matching software, usability and productivity of AutoPilot (a state of the art HLS tool), and the performance of designs produced by AutoPilot. Based on our study, we provide guidelines for software design, limitations of mapping general purpose software to hardware using HLS, and future directions for HLS tool development. For the stereo matching algorithms, we demonstrate between 3.5X and 67.9X speedup over software (but less than achievable by manual RTL design) with a five-fold reduction in design effort vs. manual hardware design.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6132716,no
Software-Based Detecting and Recovering from ECC-Memory Faults,"According to the problem that the ECC cannot correct the multibit error in ECC memory, this paper proposes a memory error processing method on software level. On the foundation of revising the Linux kernel code, the method can discover this area of influence area of memory error according to seek the process information mapping to the mistaken address. This way can avoid wastage to the user due to the system halting caused by memory error. The experimental results show that the method can have a certain degree of memory error repair and do not affect the normal work of the system.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6132897,no
Using Behavioral Profiles to Detect Software Flaws in Network Servers,"Some software faults, namely security vulnerabilities, tend to elude conventional testing methods. Since the effects of these faults may not be immediately perceived nor have a direct impact on the server's execution (e.g., a crash), they can remain hidden even if exercised by the test cases. Our detection approach consists in inferring a behavioral profile of a network server that models its correct execution by combining information about the implemented state machine protocol and the server's internal execution. Flaws are automatically detected if the server's behavior deviates from the profile while processing the test cases. This approach was implemented in a tool, which was used to analyze several FTP vulnerabilities, showing that it can effectively find various kinds of flaws.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6132948,no
The Early Identification of Detector Locations in Dependable Software,"The dependability properties of a software system are usually assessed and refined towards the end of the software development lifecycle. Problems pertaining to software dependability may necessitate costly system redesign. Hence, early insights into the potential for error propagation within a software system would be beneficial. Further, the refinement of the dependability properties of software involves the design and location of dependability components called detectors and correctors. Recently, a metric, called spatial impact, has been proposed to capture the extent of error propagation in a software system, providing insights into the location of detectors and correctors. However, the metric only provides insights towards the end of the software development life cycle. In this paper, our objective is to investigate whether spatial impact can enable the early identification of locations for detectors. To achieve this we first hypothesise that spatial impact is correlated with module coupling, a metric that can be evaluated early in the software development life cycle, and show this relationship to hold. We then evaluate module coupling for the modules of a complex software system, identifying modules with high coupling values as potential locations for detectors. We then enhanced these modules with detectors and perform fault-injection analysis to determine the suitability of these locations. The results presented demonstrate that our approach can permit the early identification of possible detector locations.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6132952,no
Uncertainty Propagation through Software Dependability Models,"Stochastic models are often employed to study dependability of critical systems and assess various hardware and software fault-tolerance techniques. These models take into account the randomness in the events of interest (aleatory uncertainty) and are generally solved at fixed parameter values. However, the parameter values themselves are determined from a finite number of observations and hence have uncertainty associated with them (epistemic uncertainty). This paper discusses methods for computing the uncertainty in output metrics of dependability models, due to epistemic uncertainties in the model input parameters. Methods for epistemic uncertainty propagation through dependability models of varying complexity are presented with illustrative examples. The distribution, variance and expectation of model output, due to epistemic uncertainty in model input parameters are derived and analyzed to understand their limiting behavior.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6132956,no
Feature Interaction Faults Revisited: An Exploratory Study,"While a large body of research is dedicated to testing for feature interactions in configurable software, there has been little work that examines what constitutes such a fault at the code level. In consequence, we do not know how prevalent real interaction faults are in practice, what a typical interaction fault looks like in code, how to seed interaction faults, or whether current interaction testing techniques are effective at finding the faults they aim to detect. We make a first step in this direction, by deriving a white box criterion for an interaction fault. Armed with this criterion, we perform an exploratory study on hundreds of faults from the field in two open source systems. We find that only three of the 28 which appear to be interaction faults are in fact due to features' interactions. We investigate the remaining 25 and find that, although they could have been detected without interaction testing, varying the system configuration amplifies the fault-finding power of a test suite, making these faults easier to expose. Thus, we characterize the benefits of interaction testing in regards to both interaction and non-interaction faults. We end with a discussion of several mutations that can be used to mimic interaction faults based on the faults we see in practice.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6132957,no
Adaptive Regression Testing Strategy: An Empirical Study,"When software systems evolve, different amounts and types of code modifications can be involved in different versions. These factors can affect the costs and benefits of regression testing techniques in different ways, and thus, there may be no single regression testing technique that is the most cost-effective technique to use on every version. To date, many regression testing techniques have been proposed, but no research has been done on the problem of helping practitioners systematically choose appropriate techniques on new versions as systems evolve. To address this problem, we propose adaptive regression testing (ART) strategies that attempt to identify the regression testing techniques that will be the most cost-effective for each regression testing session considering organization's situations and testing environment. To assess our approach, we conducted an experiment focusing on test case prioritization techniques. Our results show that prioritization techniques selected by our approach can be more cost-effective than those used by the control approaches.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6132961,no
Parametric Bootstrapping for Assessing Software Reliability Measures,"The bootstrapping is a statistical technique to replicate the underlying data based on the resampling, and enables us to investigate the statistical properties. It is useful to estimate standard errors and confidence intervals for complex estimators of complex parameters of the probability distribution from a small number of data. In software reliability engineering, it is common to estimate software reliability measures from the fault data (fault-detection time data) and to focus on only the point estimation. However, it is difficult in general to carry out the interval estimation or to obtain the probability distributions of the associated estimators, without applying any approximate method. In this paper, we assume that the software fault-detection process in the system testing is described by a non-homogeneous Poisson process, and develop a comprehensive technique to study the probability distributions on significant software reliability measures. Based on the maximum likelihood estimation, we assess the probability distributions of estimators such as the initial number of software faults remaining in the software, software intensity function, mean value function and software reliability function, via parametric bootstrapping method.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6133060,no
Using Dependability Benchmarks to Support ISO/IEC SQuaRE,"The integration of Commercial-Off-The-Shelf (COTS) components in software has reduced time-to-market and production costs, but selecting the most suitable component, among those available, remains still a challenging task. This selection process, typically named benchmarking, requires evaluating the behaviour of eligible components in operation, and ranking them attending to quality characteristics. Most existing benchmarks only provide measures characterising the behaviour of software systems in absence of faults ignoring the hard impact that both accidental and malicious faults have on software quality. However, since using COTS to build a system may motivate the emergence of dependability issues due to the interaction between components, benchmarking the system in presence of faults is essential. The recent ISO/IEC 25045 standard copes with this lack by considering accidental faults when assessing the recoverability capabilities of software systems. This paper proposes a dependability benchmarking approach to determine the impact that faults (noted as disturbances in the standard) either accidental or malicious may have on the quality features exhibited by software components. As will be shown, the usefulness of the approach embraces all evaluator profiles (developers, acquirers and third-party evaluators) identified in the ISO/IEC 25000 """"SQuaRE"""" standard. The feasibility of the proposal is finally illustrated through the benchmarking of three distinct software components, which implement the OLSR protocol specification, competing for integration in a wireless mesh network.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6133063,no
RAMpage: Graceful Degradation Management for Memory Errors in Commodity Linux Servers,"Memory errors are a major source of reliability problems in current computers. Undetected errors may result in program termination, or, even worse, silent data corruption. Recent studies have shown that the frequency of permanent memory errors is an order of magnitude higher than previously assumed and regularly affects everyday operation. Often, neither additional circuitry to support hardware-based error detection nor downtime for performing hardware tests can be afforded. In the case of permanent memory errors, a system faces two challenges: detecting errors as early as possible and handling them while avoiding system downtime. To increase system reliability, we have developed RAMpage, an online memory testing infrastructure for commodity x86-64-based Linux servers, which is capable of efficiently detecting memory errors and which provides graceful degradation by withdrawing affected memory pages from further use. We describe the design and implementation of RAMpage and present results of an extensive qualitative as well as quantitative evaluation.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6133070,no
Automatic Robustness Assessment of DDS-Compliant Middleware,"The next generation of critical systems requires an efficient, scalable and robust data dissemination infrastructure. Middleware solutions compliant with the novel OMG standard, called Data Distribution Service (DDS), are being traditionally used for architecting large-scale systems, because they well meet the requirements of scalability, seamless decoupling and fault tolerance. Due to such features, industrial practitioners are enforcing the adoption of such middleware solutions also within the context of critical systems. However, these systems pose serious dependability requirements, which in turn demand DDS compliant products also to realize reliable data dissemination in different and heterogeneous contexts. Hence, assessing the supported reliability degree and proposing improvement strategies becomes crucial and requires a clear understanding of DDS compliant middleware failing behavior. This paper illustrates an innovative tool to automatically evaluate the robustness of DDS-compliant middleware based on a fault injection technique. Specifically, experiments have been conducted on an actual implementation of the DDS standard, by means of injecting a set of proper invalid inputs through its API and analyzing the achieved outcomes.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6133102,no
Autonomic Resource Management Handling Delayed Configuration Effects,"Today, cloud providers offer customers access to complex applications running on virtualized hardware. Nevertheless, big virtualized data centers become stochastic environments with performance fluctuations. The growing number of cloud services makes a manual steering impossible. An automatism on the provider side is needed. In this paper, we present a software solution located in the Software as a Service layer with autonomous agents that handle user requests. The agents allocate resources and configure applications to compensate performance fluctuations. They use a combination of Support Vector Machines and Model-Predictive Control to predict and plan future configurations. This allows them to handle configuration delays for requesting new virtual machines and to guarantee time-dependent service level objectives (SLOs). We evaluated our approach on a real cloud system with a high-performance software and a three-tier e-commerce application. The experiments show that the agents accurately configure the application and plan horizontal scalings to enforce SLO fulfillments even in the presence of noise.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6133137,no
Efficiently Synchronizing Virtual Machines in Cloud Computing Environments,"Infrastructure as a Service (IaaS), a form of cloud computing, is gaining attention for its ability to enable efficient server administration in dynamic workload environments. In such environments, however, updating the software stack or content files of virtual machines (VMs) is a time-consuming task, discouraging administrators from frequently enhancing their services and fixing security holes. This is because the administrator has to upload the whole new disk image to the cloud platform via the Internet, which is not yet fast enough that large amounts of data can be transferred smoothly. Although the administrator can apply only incremental updates directly to the running VMs, he or she has to carefully consider the type of update and perform operations on all the running VMs, such as application restarts and operating system reboots. This is a tedious and error-prone task. This paper presents a technique for synchronizing VMs with less time and lower administrative burden. We introduce the Virtual Disk Image Repository, which runs on the cloud platform and automatically updates the virtual disk image and the running VMs with only the incremental update information. We also show a mechanism that performs necessary operations on the running VM such as restarting server processes, based on the types of files that are updated. We implemented a prototype on Linux 2.6.31.14 and Amazon Elastic Compute Cloud. The experimental results show that our technique can synchronize VMs in an order-of-magnitude shorter time than the conventional disk-image-based VM cloning method. Although our system imposes about 30% overhead on the developer's environment, it imposes no observable overhead on public servers and correctly performs necessary operations to put updates into effect.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6133139,no
VM Leakage and Orphan Control in Open-Source Clouds,"Computer systems often exhibit degraded performance due to resource leakage caused by erroneous programming or malicious attacks, and computers can even crash in extreme cases of resource exhaustion. The advent of cloud computing provides increased opportunities to amplify such vulnerabilities, thus affecting a significant number of computer users. Using simulation, we demonstrate that cloud computing systems based on open-source code could be subjected to a simple malicious attack capable of degrading availability of virtual machines (VMs). We describe how the attack leads to VM leakage, causing orphaned VMs to accumulate over time, reducing the pool of resources available to users. We identify a set of orphan control processes needed in multiple cloud components, and we illustrate how such processes detect and eliminate orphaned VMs. We show that adding orphan control allows an open-source cloud to sustain a higher level of VM availability during malicious attacks. We also report on the overhead of implementing orphan control.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6133193,no
Valuing quality of experience: A brave new era of user satisfaction and revenue possibilities,"Telecommunication market today is defined by a plethora of innovative products and technologies that constantly raising the bar of technical feasibility in both hardware and software. Meanwhile users constantly demand better quality and improved attributes for all applications, becoming less and less tolerant in errors or inconsistencies. Evaluation methods that were dominant for several years in the field seem to have limited effect on assess end-user satisfaction, leading to unhappy customers and lower revenue for key market players. Ensuring Quality of Service (QoS) proved no longer capable to increase market share therefore a novel evaluation method is necessary. The aim of the present paper is to present a new framework of user-oriented quality assessment that tries to measure the overall experience derived from a telecommunication product. Provided that modern services are based over the principal of sharing an overall experience with others, it seems certain that the new method of estimating Quality of Experience (QoE) will produce much better results, needed by both providers and customers.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6133422,no
A method for copper lines classification,"Recently many end users show attention on the quality of Internet access; in Italy, a national wide measure campaign, sponsored by Italian Communication Regulatory Authority, allows user to evaluate his bandwidth using a licit software. In this context a primary end user need is to have the possibility to measure its bandwidth and to compare it with the parameters declared by ISPs. Assuming the availability of a standard recognized methodology to measure bandwidth on user access link, a problem with this approach arises when the measured performances are lower than the declared quality of service. When this happens, the problem could depend on several factors not directly attributable to the ISP. In this work, we propose a solution by which it is possible to characterize a physical line. The idea is to detect the situations in which the performances are degraded due to an unsatisfactory physical line state. To make this detection some real cases are considerate.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6133444,no
Performance Analysis of Cloud Centers under Burst Arrivals and Total Rejection Policy,"Quality of service, QoS, has a great impact on wider adoption of cloud computing. Maintaining the QoS at an acceptable level for cloud users requires an accurate and well adapted performance analysis approach. In this paper, we describe a new approximate analytical model for performance evaluation of cloud server farms under burst arrivals and solve it to obtain important performance indicators such as mean request response time, blocking probability, probability of immediate service and probability distribution of number of tasks in the system. This model allows cloud operators to tune the parameters such as the number of servers and/or burst size, on one side, and the values of blocking probability and probability that a task request will obtain immediate service, on the other.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6133765,no
Electrically detected magnetic resonance study of a near interface trap in 4H SiC MOSFETs,"It is well known that 4H silicon carbide (SiC) based metal oxide silicon field effect transistors (MOSFETs) have great promise in high power and high temperature applications. The reliability and performance of these MOSFETs is currently limited by the presence of SiC/SiO<sub>2</sub> interface and near interface traps which are poorly understood. Conventional electron paramagnetic resonance (EPR) studies of silicon samples have been utilized to argue for carbon dangling bond interface traps [1]. For several years, with several coworkers, we have explored these silicon carbide based MOSFETs with electrically detected magnetic resonance (EDMR), [2,3] establishing a connection between an isotropic EDMR spectrum with g=2.003 and deep level defects in the interface/near interface region of SiC MOSFETs. We tentatively linked the spectrum to a silicon vacancy or closely related defect. This assessment was tentative because we were not previously able to quantitatively evaluate the electron nuclear hyperfine interactions at the site. Through multiple improvements in EDMR hardware and data acquisition software, we have achieved a very large improvement in sensitivity and resolution in EDMR, which allows us to detect side peak features in the EDMR spectra caused by electron nuclear hyperfine interactions. This improved resolution allows far more definitive conclusions to be drawn about defect structure. In this work, we provide extremely strong experimental evidence identifying the structure of that defect. The evidence comes from very high resolution and sensitivity fast passage (FP) mode [4, 5] electrically detected magnetic resonance (EDMR) or FPEDMR of the ubiquitous EDMR spectrum.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6135142,no
PV system monitoring and performance of a grid connected PV power station located in Manchester-UK,"In the last two decades renewable resources have gained more attention due to continuing energy demand, along with the depletion in fossil fuel resources and their environmental effects to the planet. This paper presents a novel approach in monitoring PV power stations. The monitoring system enables system degradation early detection by calculating the residual difference between the model predicted and the actual measured power parameters. The model being derived using the MATLAB/SIMULINK software package and is designed with a dialog box to enable the user input of the PV system parameters. The performance of the developed monitoring system was examined and validated under different operating condition and faults e.g. dust, shadow and snow. Results were simulated and analyzed using the environmental parameters of irradiance and temperature. The irradiance and temperature data is gathered from a 28.8kW grid connected solar power system located on the tower block within the MMU campus in central Manchester. These real-time parameters are used as inputs of the developed PV model. Repeatability and reliability of the developed model performance were validated over a one and half year's period.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6136072,no
A mixed method study to identify factors affecting software reusability in reuse intensive development,"The objectives of reusing software are to reduce the cost and amount of resources used to produce quality software that is on time. These objectives are achieved by reusing software artefacts. The reuse insensitive software development approaches, such as component based software development (CBSD) and software product lines (SPL) development, make use of reusable software assets. The use of open source software (OSS) is common in the software industry, especially in CBSD. However, recent research suggests the use of OSS in SPL. In this paper the results of a mixed method study are presented. The study focuses on identifying the factors affecting reusability of software in a reuse intensive software development environment. The first part of the study is based on interviews with experts and professionals working with OSS in a reuse intensive environment. The next part describes a survey is conducted to assess the importance of the factors. The procedures followed and results obtained of the both research activities are presented.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6136324,no
Parallelization of an ultrasound reconstruction algorithm for non destructive testing on multicore CPU and GPU,"The CIVA software platform developed by CEA-LIST offers various simulation and data processing modules dedicated to non-destructive testing (NDT). In particular, ultrasonic imaging and reconstruction tools are proposed, in the purpose of localizing echoes and identifying and sizing the detected defects. Because of the complexity of data processed, computation time is now a limitation for the optimal use of available information. In this article, we present performance results on parallelization of one computationally heavy algorithm on general purpose processors (GPP) and graphic processing units (GPU). GPU implementation makes an intensive use of atomic intrinsics. Compared to initial GPP implementation, optimized GPP implementation runs up to 116 faster and GPU implementation up to 631. This shows that, even with irregular workloads, combining software optimization and hardware improvements, GPU give high performance.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6136904,no
Efficient Gender Classification Using Interlaced Derivative Pattern and Principal Component Analysis,"With the wealth of image data that is now becoming increasingly accessible through the advent of the world wide web and proliferation of cheap, high quality digital cameras it is becoming ever more desirable to be able to automatically classify Gender into appropriate category such that intelligent agents and other such intelligent software might make better informed decisions regarding them without a need for excessive human intervention. In this paper, we present a new technique which provides superior performance superior than existing gender classification techniques. We first detect the face portion using Voila Jones face detector and then Interlaced Derivative Pattern (IDP)extract discriminative facial features for gender which are passed through Principal Component Analysis (PCA) to eliminate redundant features and thus reduce dimension. Keeping in mind strengths of different classifiers three classifiers K-nearest neighbor, Support Vector Machine and Fisher Discriminant Analysis are combined, which minimizes the classification error rate. We have used Stanford University Medical students (SUMS) face database for our experiment. Comparing our results and performance with existing techniques our proposed method provides high accuracy rate and robustness to illumination change.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6137158,no
ASAP: A Self-Adaptive Prediction System for Instant Cloud Resource Demand Provisioning,"The promise of cloud computing is to provide computing resources instantly whenever they are needed. The state-of-art virtual machine (VM) provisioning technology can provision a VM in tens of minutes. This latency is unacceptable for jobs that need to scale out during computation. To truly enable on-the-fly scaling, new VM needs to be ready in seconds upon request. In this paper, We present an online temporal data mining system called ASAP, to model and predict the cloud VM demands. ASAP aims to extract high level characteristics from VM provisioning request stream and notify the provisioning system to prepare VMs in advance. For quantification issue, we propose Cloud Prediction Cost to encodes the cost and constraints of the cloud and guide the training of prediction algorithms. Moreover, we utilize a two-level ensemble method to capture the characteristics of the high transient demands time series. Experimental results using historical data from an IBM cloud in operation demonstrate that ASAP significantly improves the cloud service quality and provides possibility for on-the-fly provisioning.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6137322,no
Risk management assessment using SERIM method,"Software development is a complex process that involved many activities and has a big uncertainty to success. It is also a typical of activity that can be costly if mismanaged. Many factors can lead the success and also can cause software project failure. The failure actually can be detected early if we can adopt the concept of risk management and implemented it into software development project. SERIM is a method to measure risk in software engineering, proposed by Karolak [4]. SERIM is based on the mathematics of probability. SERIM uses some parameters which are derived from risk factors. The factors are: Organization, Estimation, Monitoring, Development Methodology, Tools, Risk Culture, Usability, Correctness, Reliability and Personnel. Each factor then measured by some questions metrics and there is 81 software metric questions for all factors. The factors then related and mapped into SDLC phases and risk management activities to calculate the probability (P). SERIM uses 28 probability variables to assess the risk potentials. The SERIM method then implemented to assess the risk of information system development, TrainSys, which is developed for a training and education unit in an organization. The result is useful to determine the low probability of TrainSys project success factor. The result also show the dominant and highest factor need to address in order to improve the quality of process and product of software development.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6137821,no
An integrated health and contingency management case study on an autonomous ground robot,"Autonomous robotic vehicles are playing an increasingly important role in support of a wide variety of present and future critical missions. Due to the absence of timely operator/pilot interaction and potential catastrophic consequence of unattended faults and failures, a real-time, onboard health and contingency management system is desired. This system would be capable of detecting and isolating faults, predicting fault progression and automatically reconfiguring the system to accommodate faults. This paper presents the implementation of an integrated health and contingency management system on an autonomous ground robot. This case study is conducted to demonstrate the feasibility and benefit of using real-time prognostics and health management (PHM) information in robot control and mission reconfiguration. Several key software modules including a HyDE-based diagnosis reasoner, particle filtering-based prognosis server and a prognostics-enhanced mission planner are presented in this paper with illustrative experimental results.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6137995,no
A data placement algorithm with binary weighted tree on PC cluster-based cloud storage system,"The need and use of scalable storage on cloud has rapidly increased in last few years. Organizations need large amount of storage for their operational data and backups. To address this need, high performance storage servers for cloud computing are the ultimate solution, but they are very expensive. Therefore we propose efficient cloud storage system by using inexpensive and commodity computer nodes. These computer nodes are organized into PC cluster as datacenter. Data objects are distributed and replicated in a cluster of commodity nodes located in the cloud. In the proposed cloud storage system, a data placement algorithm which provides a highly available and reliable storage is proposed. The proposed algorithm applies binary tree to search storage nodes. It supports the weighted allocation of data objects, balancing load on PC cluster with minimum cost. The proposed system is implemented with HDFS and experimental results prove that the proposed algorithm can balance storage load depending on the disk space, expected availability and failure probability of each node in PC cluster.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6138540,no
A Software-Based Self-Test methodology for on-line testing of processor caches,"Nowadays, on-line testing is essential for modern high-density microprocessors to detect either latent hardware defects or new defects appearing during lifetime both in logic and memory modules. For cache arrays, the flexibility to apply online different March tests is a critical requirement. For small memory arrays that may lack programmable Memory Built-In Self-Test (MBIST) circuitry, such as L1 cache arrays, Software-Based Self-Test (SBST) can be a flexible and low-cost solution for on-line March test application. In this paper, an SBST program development methodology is proposed for online periodic testing of L1 data and instruction cache, both for tag and data arrays. The proposed SBST methodology utilizes existing special purpose instructions that modern Instruction Set Architectures (ISAs) implement to access caches for debug-diagnostic and performance purposes, termed hereafter Direct Cache Access (DCA) instructions, as well as, performance monitoring mechanisms to overcome testability challenges. The methodology has been applied to 2 processor benchmarks, OpenRISC and LEON3 to demonstrate its high adaptability, and experimental comparison results against previous contributions show that the utilization of DCA instructions significantly improves test code size (83%) and test duration (72%) when applied to the same benchmark (LEON3).",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6139154,no
Performance assessment of ASD team using FPL football rules as reference,"Agile software development (ASD) teams are committed to frequent, regular, high-quality deliverables. Agile team requires to produce high-quality code in short time span. Agile suggests methodologies like extreme programming and scrum to resolve the issues faced by the developers. Extreme programming is a methodology of ASD which suggests pair programming. But for a number of reasons, pairing is the most controversial and least universally-embraced agile programmer practice [1]. The reason for this is that certain task requires lot of deep thinking and so pairing (lack of privacy) does not work here. Certain personalities too do not work well with pairing. In scrum, daily standup-meeting is the method used to resolve impediments. Those impediments that are not resolved are added to product backlog. This adds to cost. There can be online mentors (e-Mentors) to help programmers resolve their domain issues. The selection of such mentors depends on their skill set and availability [3]. In order to sustain e-Mentoring, the experts who act as mentors in the respective domain (application / technology / tools) have to be rewarded for their assists. The mentor could be within the development team or can be part of any other project team. By seeing the similarities between the sports team and the Agile team, a way of recognizing and rewarding these assists is suggested in this paper. The set of rules used in Fantasy Premier League for performance assessment of football players is taken here as a reference for assessing agile team.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6139390,no
Wavelet  ANN based fault diagnosis in three phase induction motor,"This paper proposes a protection scheme based on Wavelet Multi Resolution Analysis and Artificial Neural Networks which detects and classifies various faults like Single phasing, Under voltage, Unbalanced supply, Stator Turn fault, Stator Line to Ground fault, Stator Line to Line fault, Broken bars and Locked rotor of a three-phase induction motor. The three phase Induction Motor is represented by a universal model which is valid for a wide range of frequencies. The same has been simulated using MATLAB/Simulink software and tested for various types of motor faults. The wavelet decomposition of three-phase stator currents is carried out with Bi-Orthogonal 5.5 (Bior5.5). The maximum value of the absolute peak value of the highest level (d1) coefficients of three-phase currents is defined as fault index which is compared with a predefined threshold to detect the fault. The normalized fourth level approximate (a4) coefficients of these currents are fed to a Feedforward neural network to classify various faults. The normalized peak d1 coefficients of three-phase currents are fed to another Feedforward neural network to identify the faulty phase of stator internal faults. The algorithm has been tested for various incidence angles and proved to be simple, reliable and effective in detecting and classifying the various faults and also in identifying the faulty phase of stator.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6139530,no
Computerized instrumentation  Automatic measurement of contact resistance of metal to carbon relays used in railway signaling,"The Contact Resistance of metal to carbon relays used in railway signaling systems is a vital quality parameter. The manual measurement process is tedious, error prone and involves lot of time, effort and manpower. Besides, it is susceptible to manipulation and may adversely affect the functional reliability of relays due to erroneous measurements. To enhance the trustworthiness of measurement of contact resistance & to make the process faster, an automated measurement system having specially designed application software and a testing jig attachment has been developed. When the relay is fixed on the testing jig, the software scans all the relay contacts and measures the CR. The results are displayed on the computer screen and stored in a database file.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6139583,no
Approach to predict the software reliability with different methods,"This particular essay expounds upon how one can foresee and predict software reliability. There are two major components that exist within a computer system: hardware and software. The reliabilities between the two are comparable because both are stochastic processes, which can be described by probability distributions. With this said, software reliability is the probability that will function without failure in a given software and in a given environment during a specified period of time. Thus, this is why software reliability is a major and key factor in software developmental processes and quality. However, one can spot the difference between software reliability and hardware reliability where it concerns the quality duration and the fact that software reliability does not decrease its reliability over time.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6140140,no
Pair analysis of requirements in software engineering education,"Requirements Analysis and Design is found to be one of the crucial subjects in Software Engineering education. Students need to have deeper understanding before they could start to analyse and design the requirements, either using models or textual descriptions. However, the outcomes of their analysis are always vague and error-prone. We assume that this issue can be handled if pair analysis is conducted where all students are assigned with partners following the concept of pair-programming. To prove this, we have conducted a small preliminary evaluation to compare the outcomes of solo work and pair analysis work for three different groups of students. The performance, efficacy and students' satisfaction and confidence level are evaluated.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6140641,no
Adopting Six Sigma approach in predicting functional defects for system testing,"This research focuses on constructing a mathematical model to predict functional defects in system testing by applying Six Sigma approach. The motivation behind this effort is to achieve zero known post release defects of the software delivered to end-user. Besides serving as the indicator of optimizing testing process, predicting functional defects at the start of testing allows testing team to put comprehensive test coverage, find as many defects as possible and determine when to stop testing so that all known defects are contained within testing phase. Design for Six Sigma (DfSS) is chosen as the methodology as it emphasizes on customers' requirement and systematic techniques to build the model. Historical data becomes the crucial elements in this study. Metrics related to potential predictors and their relationships for the model are identified, which focuses on metrics in phases prior to testing phase. Repeatability and capability of testers' consistency in finding defects are analyzed. Type of data required are also identified and collected. The metrics of selected predictors which incorporate testing and development metrics are measured against total functional defects using multiple regression analysis. The best and most significant mathematical model generated by the regression analysis is selected as the proposed prediction model for functional defects in system testing phase. Validation of the model is then conducted to prove the goodness for implementation. Recommendation and future research work are provided at the end of this study.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6140677,no
Efficient prediction of software fault proneness modules using support vector machines and probabilistic neural networks,"A software fault is a defect that causes software failure in an executable product. Fault prediction models usually aim to predict either the probability or the density of faults that the code units contain. Many fault prediction models using software metrics have been proposed in the Software Engineering literature. This study focuses on evaluating high-performance fault predictors based on support vector machines (SVMs) and probabilistic neural networks (PNNs). Five public NASA datasets from the PROMISE repository are used to make these predictive models repeatable, refutable, and verifiable. According to the obtained results, the probabilistic neural networks generally provide the best prediction performance for most of the datasets in terms of the accuracy rate.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6140679,no
Power cable inspections using Matlab graphical user interface aided by thermal imaging,"This paper proposed an efficient method to predict and solve an abnormal conditions in all electrical cables, using Matlab GUI (Graphic User Interface) with thermal imaging (infrared (IR) camera). The use of traditional techniques (without IR camera) not easily to predict faults and give a complete diagonous about them. Using any type of thermal camera, which can detect the thermal state of abnormal conditions of cables by technical operator (thermographer) and send this thermal images to a novel software program (GUI) technique, which using cables data base able to : 1) obtain the thermal profile of the system; 2) process and analyze thermal data, and 3) apply a simulated artificial technique to determine the particular condition or fault corresponding to the thermal signature. The new performed report can contains: 1) problems that founds in the components and the system itself, 2) suggest remedy and perform any necessary corrective action in a suitable time, and 3) give the priority of these problems with respect to repair (maintenance) time.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6140681,no
Increasing test coverage using human-based approach of fault injection testing,"Fault injection testing (FIT) approach validates system's fault tolerance mechanism by actively injecting software faults into the targeted areas in the system in order to accelerate its failure rate. This highly complements other testing approaches such as requirements and regression testing implemented during the same testing phase. During testing, it is impossible to run all possible test scenarios. It is especially difficult to predict how the user might use the system functionality correctly as per design. The human interaction through the system may be varies and will leads to the functionality loophole. It is therefore important to have strategic testing approach for evaluating the dependability of computer systems especially in human errors. This paper proposed on applying Knowledge-Based, Fault Prediction Model and Test Case Prioritization approaches that can be combined to increase the test coverage. The goal of this paper is to highlight the needs and advantages of the selected approaches in performing FIT as one of effective testing techniques in the ongoing quest for increased software quality.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6140685,no
H.264 deblocking filter enhancement,This paper proposes new software-based techniques for speeding and reducing the complexity of the deblocking filter used in the state-of-the-art H.264 international video coding standard to improve the visual quality of the decoded video frames. The proposed techniques are classified as standard-compliant and standard-noncompliant techniques. The standard-compliant techniques optimize the standard filter through optimizing the boundary strength calculation and group filtering of macroblocks. The standard-noncompliant techniques predict the new boundary strength and edge detection conditions from previous values. Experimental results on both an embedded platform and a desktop PC show significant increment in performance improvement that reaches 47% for the standard-compliant techniques and 80% for the standard-noncompliant techniques. They also demonstrate that for standard-noncompliant techniques the quality degradation computed using the Peak Signal to Noise Ratio is insignificant.,2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6141046,no
Geometric mean based trust management system for WSNs (GMTMS),"The Wireless Sensor Network (WSN) nodes are high-volume in number, and their deployment environment may be hazardous, unattended and/or hostile and sometimes dangerous. The traditional cryptographic and security mechanisms in WSNs cannot detect the node physical capture, and due to the malicious or selfish nodes even total breakdown of network may take place. Also, the traditional security mechanisms in WSNs requires sophisticated software, hardware, large memory, high processing speed and communication bandwidth at node. Hence, they are not sufficient for secure routing of message from source to destination in WSNs. Alternatively, trust management schemes consist a powerful tool for the detection of unexpected node behaviours (either faulty or malicious). In this paper, we propose a new geometric mean based trust management system by evaluating direct trust from the QoS characteristics (trust metrics) and indirect trust from recommendations by neighbour nodes, which allows for trusted nodes only to participate in routing.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6141286,no
Incorporating fault tolerance in GA-based scheduling in grid environment,"Grid systems differ from traditional distributed systems in terms of their large scale, heterogeneity and dynamism. These factors contribute towards higher frequency of fault occurrences; large scale causes lower values of Mean Time To Failure (MTTF), heterogeneity results in interaction faults (protocol mismatches) between communicating dissimilar nodes and dynamism with dynamically varying resource availability due to resources autonomously entering and leaving the grid effects execution of jobs. Another factor that increases probability of failure of applications is that applications running on grid are long running computations taking days to finish. Incorporating fault tolerance in scheduling algorithms is one of the approaches for handling faults in grid environment. Genetic Algorithms are a popular class of meta-heuristic algorithms used for grid scheduling. These are stochastic search algorithms based on the natural process of fitness based selection and reproduction. This paper combines GA-based scheduling with fault tolerance techniques such as checkpointing (dynamic) by modifying the fitness function. Also certain scenarios such as checkpointing without migration for resources with different downtimes and autonomous nature of grid resource providers are considered in building fitness functions. The motivation behind the work is that scheduling-assisted fault tolerance would help in finding the appropriate schedule for the jobs which would complete in the minimum time possible even when resources are prone to failures and thus help in meeting job deadlines. Simulation results for the proposed techniques are presented with respect to makespan and flowtime and fitness value of the resultant schedule obtained. The results show improvement in makespan and flowtime of the adaptive checkpointing approaches over static checkpointing approach. Also the approach which takes into consideration the last failure times of resources perform better than the approach bas- d only on the mean failure times of resources.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6141344,no
Can Linux be Rejuvenated without Reboots?,"Operating systems (OSes) are crucial for achieving high availability of computer systems. Even if the applications running on the operating system are highly available, a bug inside the kernel may result in a failure of the entire software stack. Rejuvenating OSes is a promising approach to prevent and recover from transient errors. Unfortunately, OS rejuvenation takes a lot of time because we do not have any method other than rebooting the entire OS. In this paper we explore the possibility of rejuvenating Linux without reboots. In our previous research, we investigated the scope of error propagation in Linux. The propagation scope is process-local if the error is confined in the process context that activated it. The scope is kernel-global if the error propagates to other processes' contexts or global data structures. If most errors are process- local, we can rejuvenate the Linux kernel without rebooting the entire kernel because the kernel goes back to a consistent and clean state simply by killing and revoking the resources of the faulting process. Our conclusion is that Linux can be rejuvenated without reboots with high probability. Linux is coded in a defensive way and thus, most of the manifested errors (96%) were process-local and only one error was kernel- global.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6141725,no
Measuring the quality characteristics of assembly code on embedded platforms,The paper describes the implementation of programming tool for measuring quality characteristics of assembly code. The aim of this paper is to prove the usability of these metrics for assessing the quality of assembly code generated by C Compiler for DSP architecture in order to improve the Compiler. The analysis of test results showed that the compiler generates good quality assembly code.,2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6143798,no
Graphical tool for generating linker configuration files in embedded systems,"Absolute loader in embedded software is frequently used because its simplicity is superior for systems with limited resources. Absolute loader implies that memory map needs to be defined. Maintaining memory map by hand is hard and error prone process. This paper proposes a solution by implementing of the memory map graphical editor. The graphical editor is implemented using Graphical Modeling Framework in Eclipse IDE, using rapid model driven development.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6143854,no
A test method of interconnection online detection of NoC based on 2D Torus topology,"On the basis of the study in Network on Chip (NoC) topologies, routing algorithm, data exchange and virtual channel technology, we design an online detection method of interconnection for 2D torus structure of NoC system in this paper. This method can detect the data errors during transmission, and identify the error results from the routing switch failure or the data transmission interconnection line failure. Then we design a sub-router based on the wormhole exchange using E-cube routing algorithm, and a check module which is suitable for the original routing node functions and work feature. Finally, we simulate the method by Verilog HDL and quartus II software. The experiment results show that the method can detect data errors caused by the router failure or interconnect failure and can locate the fault.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6145096,no
Software Maintenance through Supervisory Control,"This work considers the case of system maintenance where systems are already deployed and for which some faults or security issues were not detected during the testing phase. We propose an approach based on control theory that allows for automatic generation of maintenance fixes. This approach disables faulty or vulnerable system functionalities and requires to instrument the system before deployment so that it can later be monitored and interact with a supervisor at runtime. This supervisor ensures some property designed after deployment in order to avoid future executions of faulty or vulnerable system functionalities. This property corresponds to a set of safe behaviors described as a Finite State Machine. The computation of supervisors can be performed automatically, relying on a sound Supervisory Control Theory. We first introduce some basic notions of Supervisory Control theory, then we present and illustrate our approach which also relies on automatic models extraction and instrumentation.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6146914,no
Toward Intelligent Software Defect Detection - Learning Software Defects by Example,"Source code level software defect detection has gone from state of the art to a software engineering best practice. Automated code analysis tools streamline many of the aspects of formal code inspections but have the drawback of being difficult to construct and either prone to false positives or severely limited in the set of defects that can be detected. Machine learning technology provides the promise of learning software defects by example, easing construction of detectors and broadening the range of defects that can be found. Pinpointing software defects with the same level of granularity as prominent source code analysis tools distinguishes this research from past efforts, which focused on analyzing software engineering metrics data with granularity limited to that of a particular function rather than a line of code.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6146920,no
Fault Detection through Sequential Filtering of Novelty Patterns,"Multi-threaded applications are commonplace in today's software landscape. Pushing the boundaries of concurrency and parallelism, programmers are maximizing performance demanded by stakeholders. However, multi-threaded programs are challenging to test and debug. Prone to their own set of unique faults, such as race conditions, testers need to turn to automated validation tools for assistance. This paper's main contribution is a new algorithm called multi-stage novelty filtering (MSNF) that can aid in the discovery of software faults. MSNF stresses minimal configuration, no domain specific data preprocessing or software metrics. The MSNF approach is based on a multi-layered support vector machine scheme. After experimentation with the MSNF algorithm, we observed promising results in terms of precision. However, MSNF relies on multiple iterations (i.e., stages). Here, we propose four different strategies for estimating the number of the requested stages.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6146973,no
New integrated hybrid evaporative cooling system for HVAC energy efficiency improvement,"Cooling systems in buildings are required to be more energy-efficient while maintaining the standard air quality. The aim of this paper is to explore the potential of reducing the energy consumption of a central air-conditioned building taking into account comfort conditions. For this, we propose a new hybrid evaporative cooling system for HVAC efficiency improvement. The integrated system will be modeled and analyzed to accomplish the energy conservation and thermal comfort objectives. Comparisons of the proposed hybrid evaporative cooling approach with current technologies are included to show its advantages. To investigate the potential of energy savings and air quality, a real-world commercial building, located in a hot and dry climate region, together with its central cooling plant is used in the case study. The energy consumption and relevant data of the existing central cooling plant are acquired in a typical summer week. The performance with different cooling systems is simulated by using a transient simulation software package. New modules for the proposed system are developed by using collected experimental data and implemented with the transient tool. Results show that more than 52% power savings can be obtained by this system while maintaining the predicted mean vote (PMV) between -1 to +1 for most of summer time.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6147546,no
The quality process in a professional context: Software industry case,"In the current context of the software's market, the stress is laid on the cost, the calendar and the functionalities. In order to ensure these criteria, the implementation of a step quality ISO 9001:2008 is necessary. The quality of a developed product is influenced by the quality of the production process. This is important in software development as some product quality attributes are hard to assess. For these reasons, there are several standards of quality management and processes' management that constitute an essential pathway to improve quality. In industry, speaking about quality implies focusing on production, but in software industry we must speak design. The text of ISO 9001 standard covers design but gives more importance to the production. So in order to be applied to the software field, the ISO 9001 standard must be explained further. The use of standards is an important factor of economy, efficiency and quality promoting better adaptation of products, processes and services to purposes assigned to them, through prevention of barriers to trade and facilitating international technology cooperation. So we start from a strong will to create the necessary conditions so that the quality standards in the companies would be oriented into the software field. It would be interesting to develop an approach to adapt quality standards (mainly ISO9001: 2008 standard) which are basically oriented to the industrial sector into the software field. The ISO 9001:2008 standard is general and provides the organizational requirements needed to implement a quality management system. Our work is based on the adaptation of this standard for a quality management system related to the software production. This communication includes the consideration of the requirements of ISO9001:2008 standard. We will provide more interpretations of these requirements, and we will study the state of the art of researches that have attempted to adapt the ISO 9001 standard for the software f- eld and working on the update of the guidelines of the ISO 90003 standard in relation to the requirements of ISO 9001:2008 standard. In addition to that we will search for the potential results for the adaptation of ISO 9001:2008 requirements to the field of software in order to consider these results as a starting step for our work.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6148589,no
Autonomic Computing: Applications of Self-Healing Systems,"Self-Management systems are the main objective of Autonomic Computing (AC), and it is needed to increase the running system's reliability, stability, and performance. This field needs to investigate some issues related to complex systems such as, self-awareness system, when and where an error state occurs, knowledge for system stabilization, analyze the problem, healing plan with different solutions for adaptation without the need for human intervention. This paper focuses on self-healing which is the most important component of Autonomic Computing. Self-healing is a technique that aims to detect, analyze, and repair existing faults within the system. All of these phases are accomplished in real-time system. In this approach, the system is capable of performing a reconfiguration action in order to recover from a permanent fault. Moreover, self-healing system should have the ability to modify its own behavior in response to changes within the environment. Recursive neural network has been proposed and used to solve the main challenges of self-healing, such as monitoring, interpretation, resolution, and adaptation.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6150010,no
An Automated Detection Method of Solder Joint Defects Using 3D Computed Tomography for IC Package Inspection,"Recent electronics parts continue to decrease in size so that it is more likely to exhibit defects. Lately, computed tomography scanning technique has been introduced to provide useful tools for the internal inspection of electronic packages. In this paper, we presents a novel method for detecting solder joint defects in the 3D packaging devices. Our method is composed of three steps. First, mis-alignment during the CT scan process is corrected. Second, open solder joints, missing solder joints, and solder bridges are detected using blob labeling procedure. Finally, head-in-pillow defect is inspected by the principal curvature analysis. The experimental results demonstrated that our method accurately detected solder joint defects within less than one second. Our method can be successfully applied to inline manufacturing, which requires rapid inspection of whole chips.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6150069,no
A synopsis of self-healing functions in wireless networks,"In early 20th century when technology evolve, the performance of systems suffered from the problems of complexity, increasing cost of maintenance and software of hardware failure caused by unpredictable behavior and poor manageability. This fostered researchers to discover new design and techniques that enable the systems to operate autonomously. In 2001, IBM introduces self-managing capabilities (self-organizing, self-healing, self-optimization and self-protection) with autonomous behavior. In this survey, the main concerns are self-healing autonomic computing. Self-healing is an autonomic computing system that detects and diagnoses errors without the need of human intervention. A number of concepts, techniques and functions have been developed in different application areas of self-healing. This survey gives an overview about some approaches and solutions of past and current research in self-healing classified to operating system, routing, security and web services. These proposed approaches and solutions were developed to solve the problems that arise in manual intervention system. To achieve the perfect of self-healing behaviors, its remains an open and significant challenge that can be accomplished through a combination of process changes, new technologies and architecture and open industry standards.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6151467,no
Improving path selection by handling loops in automatic test data generation,"Generating path oriented test data is one of the most powerful methods in generating appropriate test data which selects all complete paths in Control Flow Graph (CFG) and generates appropriate data to traverse the selected paths. In path selecting phase, different paths could be selected according to loops iteration that most of them are infeasible. Because the number of loops iteration is detected dynamically through the program execution in most cases. In earlier techniques, researchers either refused to handle loops or dealt with them by simplifying; thus, no effective solutions have been represented up to now. In paths with loops, proposed algorithm firstly attempts to determine the exact number of loops iteration. Then if the iterations remain unknown, this number will be decided by the tester. This technique is executed based on symbolic evaluation and loop information. Finally, selected paths can all be traversed; moreover, with reducing the number of infeasible paths, the time of generating test data will be reduced remarkably.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6151487,no
Full 4 emission data collection and reconstruction for small animal PET imaging,"Most of the current animal PET detector systems are cylindrical or similar multi-sided polygonal geometries with limited axial field of view. The object is placed near the center of the detector ring during the emission data collection. The signal that can be detected has a limited angular range; the paraxial signal cannot be detected. Moreover, the sensitivity for the objects positioned at different locations inside the FOV is different. The central part of the FOV has higher sensitivity than that near the end along the axis. The lack of paraxial detectability which means non-uniform sampling, along with the non-uniform sensitivity of a PET system, will affect the uniformity of the overall image quality. Also, currently widely used PET data processing and reconstruction algorithms are sinogram based, which usually uses different sizes for the voxel in axial and transaxial directions. Because of these non-uniformities, the resolution and quality of current PET image are anisotropic. In order to achieve isotropic results, the emissions from the object must be collected in full 4 space and reconstructed accordingly. Here we propose a full 4 emission data collection method, which involving the rotation of the object during the emission collection in a plane parallel to the detector ring axis. The full 4 emission data are then reconstructed and processed to generate the 3D image set with cubic voxel, uniform resolution and signal-to-noise ratio in all directions and locations. Both Monte Carlo simulations and experiments are carried out with simulated and real mouse phantoms. Emission data in both 4 and conventional modes are collected and then processed. Point source is used in the experiment as the fiducial mark. Our results show that with full 4 collection method, the image qualities are substantially improved in several aspects such as the axial distortions and the uniformity of the SNR. Moreover, the axial strip-like ar- ifacts in the conventional mode are canceled in the full 4 mode, therefore less smooth window is needed during the reconstruction which led to higher resolution.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6152506,no
Evaluation of the SensL SPMMatrix for use as a detector for PET and gamma camera applications,"The SPMMatrix from SensL (SensL, Cork, Ireland) is a large area photodetector consisting of a 4  4 array of SensL SPMArray4 detectors, each a 4  4 array of silicon photomultiplier (SiPM) pixels, giving a total of 256 SiPM pixels. In addition, the device has 32 amplifiers and analog-to-digital conversion (ADC) channels and a FPGA-based data acquisition board The anodes of the SiPMs are chained together according to an array/pixel wiring scheme developed by SensL to reduce the number of readout electronics channels to 32. In this work we conducted a preliminary evaluation of the SPMMatrix device to assess its suitability for PET and gamma camera applications. One commercially manufactured 44 array of 3.17  3.17  10 mm<sup>3</sup> LYSO crystals was coupled to one of the 44 SiPM pixel arrays, effectively giving a one-to-one coupling of the scintillator crystal and SiPM pixels. A custom data acquisition program that allowed acquisition of all 32 ADC channels was used to acquire data from the device. A <sup>68</sup>Ge source was used for all testing. High quality flood images were obtained from the SPMMatrix device, with all crystals being well resolved Two methods were investigated for determining the energy resolution. The better method, using the hardware sum of the pixels in one SPMArray4 detector, gave an energy resolution of 17%. The resolution degraded to 21% when the energy value was calculated by a software sum of the pixel values. This decrease in energy resolution is likely due to the contribution of dark noise from the pixels in the other arrays and due to the array/pixel multiplexing strategy. In comparison, the same L YSO array tested with a single SPMArray4 detector and NIM electronics gave an energy resolution of 14.6%.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6152613,no
Tomographic performance characteristics of the IQSPECT system,"The IQSPECT system was introduced by Siemens in 2010 to significantly improve the efficiency of myocardial perfusion imaging (MPI) using conventional, large field-of-view (FOV) SPECT and SPECTCT systems. With IQSPECT, it is possible to perform MPI scans in one-fourth the time or using one-fourth the administered dose as compared to a standard protocol using parallel-hole collimators. This improvement is achieved by means of a proprietary multifocal collimator that rotates around the patient in a cardio-centric orbit resulting in a four-fold magnification of the heart while keeping the entire torso in the FOV. The data are reconstructed using an advanced reconstruction algorithm that incorporates measured values for gantry deflections, collimator-hole angles, and system point response function. This article explores the boundary conditions of IQSPECT imaging, as measured using the Data Spectrum<sup></sup> cardiac torso phantom with the cardiac insert. Impact on reconstructed image quality was evaluated for variations in positioning of the myocardium relative to the sweet spot, scan-arc limitations, and for low-dose imaging protocols. Reconstructed image quality was assessed visually using the INVIA 4DMSPECT and quantitatively using Siemens internal IQ assessment software. The results indicated that the IQSPECT system is capable of tolerating possible mispositioning of the myocardium relative to the sweet spot by the operator, and that no artifacts are introduced by the limited angle coverage. We also found from the study of multiple low dose protocols that the dwell time will need to be adjusted in order to acquire data with sufficient signal-to-noise ratio for good reconstructed image quality.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6152666,no
Process automation of metal to Carbon relays: On  Line measurement of electrical parameters,"The manufacturing process of Metal to Carbon relays used in railway signaling systems for configuring various circuits of signals / points / track circuits etc. consists of seven phases from raw material to finished goods. To ensure in-process quality, the physical, electrical and various other parameters are measured manually with non-automated equipment, after each stage. Manual measurements are tedious, error prone and involve lot of time, effort and manpower. Besides, they are susceptible to manipulation and may lead to inferior quality products being passed, either due to deliberation or due to malefic intentions. Due to erroneous measurement of electrical parameters, the functional reliability of relays is adversely affected. To enhance the trustworthiness of measurement of electrical parameters & to make the process faster, an automated measurement system having proprietary application software and a testing jig attachment has been developed. When the relay was fixed on the testing jig, the software scanned all the relay contacts and measured all the electrical parameters viz. operating voltage / current, contact resistance, release voltage / current, coil resistance etc. The result was stored in a database file and ported on an internet website. Thus, the test results of individual relays were available on-line, with date & time tags and could be easily monitored.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6153311,no
Digital anthropomorphic phantoms of non-rigid human respiratory and voluntary body motions: A tool-set for investigating motion correction in 3D reconstruction,"Patient respiratory and body motions occurring during emission tomography create artifacts in the images, which can mislead diagnosis. For example, in myocardial-perfusion imaging these artifacts can be mistaken for perfusion defects. Various software and hardware approaches have been developed to detect and compensate for motion. A practical way to test these methods is to simulate realistic motion with digital anthropomorphic phantoms. However, simulated motions often do not correspond to real patient motions. In this study, we are creating XCAT phantoms based on real body and respiratory motion data acquired from MR scans of volunteers. We are exploring different MRI acquisition methods to allow respiratory amplitude-binned modeling of both inspiration and expiration, which portrays both non-rigid motion and motion hysteresis during breathing. Simultaneous to MRI, the positions of reflective markers placed on the body-surface are tracked in 3D via stereo optical imaging. This enables correlation of the internal organ motion (e.g. heart, liver etc.) in our models with external marker-motion as would be observed during clinical imaging. Our digital anthropomorphic phantoms can serve as a realistic dataset with known truth for investigating motion correction in 3D iterative reconstructions.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6153671,no
Functionality test of a readout circuit for a 1mm<sup>3</sup> resolution clinical PET system,"We are developing a 1mm<sup>3</sup> resolution Positron Emission Tomography (PET) camera dedicated to breast imaging, which collects high energy photons emitted from radioactively labeled agents injected in the patients to detect molecular signatures of breast cancer. The camera consists of 8  8 arrays of 1  1  1mm<sup>3</sup> lutetium yttrium oxyorthosilicate (LYSO) crystals coupled to position sensitive avalanche photo-diodes (PSAPDs). The camera is built out of 2 panels each having 9 cartridges. A cartridge houses 8 layers of 16 dual LYSO-PSAPD modules and features 1024 readout channels. Amplification, shaping and triggering is done using the 36 channel RENA-3TM chip. 32 of these chips are needed per cartridge, or 576 for the entire camera. The RENA-3TM needs functionality and performance validation before assembly in the camera. A Chip Tester board was built to ensure RENA functionality and quality, featuring a 20,000-cycle chip holder and the ability to charge inject each channel individually. Lab View software was written to collect data on all RENA-3TM channels automatically. Charge was injected using an arbitrary waveform generator. Software was written to validate gain, linearity, cross talk, and timing noise characteristics. Gain is tested using a linear fit, typical values of gain are 13 and -16 for positive and negative injected charges respectively. Timing analysis was based on analyzing the phase shift between U and V timing channels. A phase shift of 90  5 and timing noise of 2ns were considered acceptable.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6153750,no
Direct 3D PET image reconstruction into MR image space,"A method which includes both the motion correction and image registration transformation parameters from PET image space to MR image space within the system matrix of the MLEM algorithm is presented. This approach can be of particular significance in the fields of neuroscience and psychiatry, whereby PET is used to investigate differences in activation patterns between groups of participants (such as healthy controls and patients). This requires all images to be registered in a common spatial atlas. Currently, image registration is performed post-reconstruction. This introduces interpolation effects in the final image and causes image resolution degradation. Furthermore, motion correction introduces a further level of interpolation and possible resolution degradation. To include the transformation parameters (both for motion correction and registration) within the iterative PET reconstruction framework (through iterative use of actual software packages routinely applied after reconstruction) should reduce these interpolation effects and thus improve image resolution. Furthermore, it opens the possibility of direct reconstruction of the PET data into standardized stereotaxic atlases, e.g. ICBM152. To validate the proposed method, this work investigates registration, using 2D and 3D simulations based on the HRRT scanner geometry, between different image spaces using rigid body transformation parameters calculated using the mutual information similarity criterion. The quality of reconstruction was assessed using bias-variance and mean absolute error analyses to quantify differences with current post-reconstruction registration methods. We demonstrate a reduction in bias and in mean absolute error in reconstructed mean ROI activity when using the proposed method.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6153752,no
A Network Status Evaluation Method under Worm Propagation,"The measurement of worm propagation impact on network status remained an elusive goal. This paper analyzes the worm characteristics and network traffic and service, introduces evaluation metrics and presents a new method to assess the network situation under worm propagation. The applicability of this method is verified by simulated experiments with the network simulation tool LSNEMUlab test bed.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6154242,no
The Testing and Diagnostic System on AVR of the Movable Electricity Generating Set,"AVR (Automatic Voltage Regulator) is the most important module, which can control the output voltage of generating sets, guarantee the voltage stability, improve power quality and decide the performance of electricity generating sets, so this paper introduces the design method of the AVR detecting and diagnostic system, which based on the fault database. The paper introduces the platform of the testing and diagnostic system on AVR from the hardware and software design, composed of the industrial computer (main controller), the programmed power supply, the data acquisition unit, and the software programmed by Lab Windows/CVI. The FTA (Fault Tree Analysis) method, establish fault database and analyse fault of AVR, is Applyed. Through testing the certain type's AVR, the method, proposd in the paper, is proved to be feasibile and versatile, and is satisfied with the detection and diagnosis for AVR eventually.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6154271,no
Detection of power quality disturbances in presence of DFIG wind farm using wavelet transform based energy function,"Wavelet transform based energy function approach for detection of some power quality (PQ) disturbances such voltage sag, voltage flicker, voltage swell, harmonics, inter harmonics in grid connected wind power system is proposed in this paper. The current signal is processed through Wavelet transform for PQ events detection. Initially, the current is retrieved at a sampling frequency of 20 kHz and DWT is used to decompose the signals of PQ events and to extract its useful information. In the case study, the power quality disturbances are created in the grid, and proposed algorithm detects the power quality disturbances effectively within one and half cycles for 60 Hz system. Thus, a new diagnostic method based on the grid modulating signals pre-processed by Discrete Wavelet Transform (DWT) is proposed to detect grid power quality disturbances. The system is simulated using MATLAB software and simulation results demonstrate the effectiveness of the proposed approach under time-varying conditions.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6156675,no
Design and FPGA implementation of digital noise generator based on superposition of Gaussian process,"Currently in the design of digital communication system, in order to detect the communication quality, plenty of tests need to be done in a noisy environment. The common method is adding analog noise to the transmitted data on the radio. Adding digital noise has always been a difficulty. A digital noise generator based on superposition of Gaussian process is presented in this paper. And hardware program simulation is carried out using Quartus II combined with Modelsim software, even the performance of the digital noise generator is tested on FPGA, simultaneously, compared with the single Gaussian noise.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6157878,no
Impacts of automatic loop restoration schemes on service reliability,"Automatic loop restoration schemes are employed in electric power distribution systems to perform fault detection, isolation, and service restoration activities sequentially and automatically, so as to significantly reduce customer interruption time. This paper aims to quantitatively assess the impacts of employing an automatic loop restoration scheme, with and without a communication link, on the major attributes of the service reliability. In addition, the effect of the operational failure of communication facilities is taken into account. A typical Finnish urban distribution network is utilized in this paper for the quantitative reliability assessment studies. A powerful software package referred to as """"Smart Grid Simulator"""" is used for directing the reliability studies.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6162252,no
Increasing security of supply by the use of a Local Power Controller during large system disturbances,"This paper describes intelligent ways in which distributed generation and local loads can be controlled during large system disturbances, using Local Power Controllers. When distributed generation is available, and a system disturbance is detected early enough, the generation can be dispatched, and its output power can be matched as closely as possible to local microgrid demand levels. Priority-based load shedding can be implemented to aid this process. In this state, the local microgrid supports the wider network by relieving the wider network of the micro-grid load. Should grid performance degrade further, the local microgrid can separate itself from the network and maintain power to the most important local loads, re-synchronising to the grid only after more normal performance is regained. Such an intelligent system would be a suitable for hospitals, data centres, or any other industrial facility where there are critical loads. The paper demonstrates the actions of such Local Power Controllers using laboratory experiments at the 10kVA scale.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6162785,no
Simple scoring system for ECG quality assessment on Android platform,"Work presented in this paper was undertaken in response to the PhysioNet/CinC Challenge 2011: Improving the quality of ECGs collected using mobile phones. For the purpose of this challenge we have developed an algorithm that uses five simple rules, detecting the most common distortions of the ECG signal in the out of hospital environment. Using five if-then rules arranges for easy implementation and reasonably swift code on the mobile device. Our results on test set B were well-outside the top ten algorithms (Best score: 0.932; Our score: 0.828). Nevertheless our algorithm placed second among those providing open-source code for evaluation on the data set C, where neither data nor scores were released to the participants before the end of the challenge. The difference in the scores of the top two algorithms was minimal (Best score: 0.873; Our score: 0.872). As a consequence, relative success of simple algorithm on undisclosed set C raises questions about the over-fitting of more sophisticated algorithms - question that is hovering above many recently published results of automated methods for medical applications.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6164599,no
Mind-mapping: An effective technique to facilitate requirements engineering in agile software development,"Merging agile with more traditional approaches in software development is a challenging task, especially when requirements are concerned: the main temptation is to let two opposite schools of thought become rigid in their own assumptions, without trying to recognize which advantages could come from either side. Mind mapping seems to provide a suitable solution for both parties: those who develop within an agile method and those who advocate proper requirements engineering practice. In this paper, mind mapping has been discussed as a suitable technique to elicit and represent requirements within the SCRUM model: specifically, we have focused on whether and how mind maps could lead to the development of a suitable product backlog, which in SCRUM plays the role of an initial requirements specification document. In order to experimentally assess how effectively practitioners could rely on a product backlog for their first development sprint, we have identified the adoption of mind maps as the independent variable and the quality of the backlog as the dependent variable, the latter being measured against the """"function points"""" metric. Our hypothesis (i.e., mind maps are effective in increasing the quality of product backlogs) has been tested within an existing SCRUM project (the development of a digital library by an academic institution), and several promising data have been obtained and further discussed.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6164775,no
Translating unknown words using WordNet and IPA-based-transliteration,"Due to small available English-Bangla parallel corpus, Example-Based Machine Translation (EBMT) system has high probability of handling unknown words. To improve translation quality for Bangla language, we propose a novel approach for EBMT using WordNet and International-Phonetic-Alphabet(IPA)-based transliteration. Proposed system first tries to find semantically related English words from WordNet for the unknown word. From these related words, we choose the semantically closest related word whose Bangla translation exists in English-Bangla dictionary. If no Bangla translation exists, the system uses IPA-based-transliteration. For proper nouns, the system uses Akkhor transliteration mechanism. We implemented the proposed approach in EBMT, which improved the quality of good translation by 16 points.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6164838,no
Towards a performance estimate in semi-structured processes,"Semi-structured processes are business workflows, where the execution of the workflow is not completely controlled by a workflow engine, i.e., an implementation of a formal workflow model. Examples are workflows where actors potentially have interaction with customers reporting the result of the interaction in a process aware information system. Building a performance model for resource management in these processes is difficult since the information required for a performance model is only partially recorded. In this paper we propose a systematic approach for the creation of an event log that is suitable for available process mining tools. This event log is created by an incremental cleansing of data. The proposed approach is evaluated in a case study where the quality of the derived event log i assessed by domain experts.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6166256,no
Design pattern prediction techniques: A comparative analysis,"There are many design patterns available in literature to predict refactoring. However literature gives a comprehensive study to evaluate and compare various design patterns so that quality professionals may select an appropriate design pattern. To find a technique which performs better in general is an undesirable problem because behavior of a design pattern also depends on many other features like pre-deployment of design pattern, structural, behavioral etc. we have conducted an empirical survey of various design pattern in terms of various evaluation software metrics. In this paper we have presented comparison of few design patterns on metrics basis.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6168035,no
A new custom designed cleft lip and palate implant based on MARP,"Something about 1 million skeletal defects are reported each year, which are in need of bone-grafting to be cured. Note that population of the world is getting older and so the probability of bone fractures is increasing, while dealing with this would have lots of social and economical effects. Bone grafts can be both autologous and autogenous or as an alternative, be made of nonorganic materials such as metals, polymers, ceramics or composite materials. Maxillofacial problems can cause a variety of malfunctions and abnormalities in the body and to handle these defects, two solutions exist, including surgical approach and non-surgical approach. In the first method, one should wait until the end of bone growth ages, and for the second, due to the need of wearing prosthesis, modifications of the prosthesis is essential as child grows. In this research a patient with cleft lip and palate was chosen and an appropriate implant was designed for him. An AFM analysis was performed to make see the stress distribution pattern. Final results show that using this method can help both patients and surgeons by improving implant-tissue contact.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6168539,no
Semantic Process Management Environment,"As the knowledge-based society has been constructed, the size of work process grows bigger and the amount of the information that has to be analyzed increases. So the necessity of the process management and improvement has been required highly. This study suggests the process management method to support a company's survival strategy to get the competitive power in difficult situation to predict future business environment. The suggested process management method applies ontology for formalizing and sharing the several generalized process management concept. In ontology, several techniques from Six Sigma and PSP are defined for process definition, execution and measurement. With ontology, we provide formal knowledge base for both process management environment and human stakeholders. Also, we can easily improve our environment by extending our process ontology to adapt new management methods.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6172107,no
An open-source application to model and solve dynamic fault tree of real industrial systems,"In recent years, a new generation of modeling tools for the risk assessment have been developed. The concept of """"dynamic"""" was exported also in the field of reliability and techniques like dynamic fault tree, dynamic reliability block diagrams, boolean logic driven Markov processes, etc., have become of use. But, despite the promises of researchers and the efforts of end-users, the dynamic paradox hangs: risk assessment procedures are not as straight as they were with the traditional static methods and, what is worse, it is difficult to assess the reliability of these results. Far from deny the importance of the scientific achievement, we have tested and cursed some of these dynamic tools realizing that none of them was appropriate to solve a real case. In this context, we decided to develop a new DFT reliability solver, based on the Monte Carlo simulative approach. The tool is greatly powerful because it is written with Matlab<sup></sup> code, hence is open-source and can be extended. In this first version, we have implemented the most used dynamic gates (PAND, SEQ, FDEP and SPARE), the existence of repeated events and the possibility to simulate different cumulative distribution function of failure (Weibull, negative exponential CDF and constant). The tool is provided with a snappy graphic user interface written in Java<sup></sup>, which allows an easy but efficient modeling of any fault tree schema. The tool has been tested with many literature cases of study and results encourage other developments.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6174521,no
Impact of SIPS performance on power systems integrity,"An increasing number of utilities are using System Integrity Protection Schemes (SIPS) to minimize the probability of large disturbances and to enhance power system reliability. This trend leads to the use of an increased number of SIPS resulting in additional risks to system security. This paper proposes a procedure based on Markov Modeling for assessing the risk of a SIPS failure or misoperation. The proposed method takes into consideration failures in the three stages of SIPS operation: arming, activation and implementation. This method is illustrated using an example of a Generation Rejection Scheme (GRS) for preventing cascading outages that may lead to load shedding. In addition, system operators tend to have the SIPS always armed to prevent a failure to operate when required. However, this can result in increased probability of SIPS misoperation (operation when not needed). Therefore, the risk introduced to the system by having the SIPS always armed and ready to initiate actions is examined and compared with the risk of automatic or manual arming of SIPS only when required. Sensitivity analysis is also performed to determine how different factors can affect the ability of the SIPS to operate in a dependable and secure manner.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6180416,no
Design of adaptive line protection under smart grid,"Smart grid will bring new opportunities to development of relay protection, new sensor technology is used in smart grid. Simplified algorithm to protect data, reducing data processing time. With the State Grid Corporation of China launched the construction of smart grid, smart grid caused by the characteristics of network reconfiguration, distributed power access technologies such as micro-network operation, to put forward new demands on relay protection, based on local measurement information and a small amount of regional information makes conventional protection face greater difficulties to solve these problems; the same time, research and application on new technologies (such as new sensor technology, clock synchronization and data synchronization technology, computer technology, optical fiber communication technology, etc.) provided a broad space for development of relay protection. Adaptive protection means that the protection must adapt to changing system conditions, the computer relay protection must have a hierarchical configuration of communication lines to exchange information with computer network of other devices. For now, fiber optic communication lines are best medium in large amounts of information transmission and conversion in adaptive protection. Adaptive protection is a protection theory, according to this theory, which allows adjustment of the various protection functions, making them more adapted to practical power system operation. The key idea is to make certain changes to the protection system to respond due to load changes, such as power failures caused by switching operations or changes in the power system. Adaptive protection under the relevant literature to a definition: Adaptive protection is a basic principle of protection, this principle makes the relay can automatically adjust to various protection functions, or changes to more suitable for a given power system conditions.. In addition to conventional protection, b- t also must have a clear adaptive function modules, and only in this case can be called adaptive protection. For the general protection adaptive capacity and detect some aspects of complex fault there are some limitations, hardware circuit of microprocessor line protection device is discussed in this thesis, both hardware and algorithm considers the anti-jamming methods. On the software side, the use of a relatively new method of frequency measurement, dynamically tracking changes of frequency, real-time adjustment of sampling frequency for sampling; and using the new algorithm, improving data accuracy and simplifying the hardware circuit. The adaptive principle is applied to microprocessor line protection, adaptive instantaneous overcurrent protection, overcurrent protection principles, etc, to meet the requirements of rapid change operation mode to improve the performance of line protection. Relay protection needs to adapt to frequent changes in the power system operating mode, correctly remove various failure and equipment, and adaptive relay protection maintains a system of standard features in case of parameter changes. The simulation results show that it is an effective adaptive method.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6180471,no
A method for online analyzing excitation systems performance based on PMU measurements,"A method based on synchronized phasor measurement technology for analyzing and evaluating the dynamic regulating performance of excitation system using dynamic electrical data acquired by phasor measurement unit (PMU) is proposed. Combined with an engineering processing of corresponding excitation system performance parameters, the method realizes the calculation and analysis of the main excitation system performance indexes through detecting and extracting the course of a generator disturbance and its excitation system response. Meanwhile, its whole software system functions are designed and realized based on the system structure of a WAMS. It is concluded through the method introduction and practical project applications that compared with conventional analysis methods, this method has the advantages of online analysis, offline research, simpleness and practicality, convenient use, and its computed results can be treated as an important reference for the evaluation of dynamic regulating performances of a excitation system.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6180748,no
Comprehension oriented software fault location,"Software errors can potentially lead to disastrous consequences. Unfortunately, debugging software errors can be difficult and time-consuming. A comprehension oriented software fault location approach (COFL) is proposed in this paper to provide automated assistance in bug location. It not only locates program predicates predicting bugs, but also provides high efficiency demand-driven data flow and control flow analysis to help developers understand the causes and contexts of bugs.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6181971,no
A finite queuing model with generalized modified Weibull testing effort for software reliability,"This study incorporates a generalized modified Weibull (GMW) testing effort function (TEF) into failure detection process (FDP) and fault correction process (FCP). Although some researchers have been devoted to model these two processes, the influence of the amount of resources on lag of these two processes is not discussed. The amount of resources consumed can be depicted as testing effort function (TEF), and can largely influence failure detection speed and the time to correct a detected failure. Thus, in this paper, we will integrate a TEF into FDP and FCP. Further, we show that a GMW TEF can be expressed as a TEF curve, and present a finite server queuing (FSQ) model which permits a joint study of FDP and FCP two processes. An actual software failure data set is analyzed to illustrate the effectiveness of proposed model. Experimental results show that the proposed model has a fairly accurate predication capability.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6181985,no
A study on cooling efficiency improvement of thin film transistor Liquid Crystal Display (TFT-LCD) modules,"In recent years, LCD (Liquid Crystal Display) TVs are taking the place of CRT (Cathode Ray Tube) TVs very fast by bringing new display technologies into use. LCD module technology is divided into two main groups; the first one is CCFL (Cold Cathode Fluorescent Lamp) display which was the first type used in LCD TV, the other one is the LED (Light Emitting Diode) module which is the newest display technology comes to make slim TV design. There is a thermal challenge making slim TV design. The purpose of this paper is to investigate the thermal analysis and modeling of a 32"""" TFT-LCD LED module, The performance of LCD TV is strongly dependant on thermal effects such as temperature and its distribution on LCD displays The illumination of the display was insured by 180 light emitting diodes (LEDs) located at the top and bottom edges of the modules. Hence, in order to insure good image quality in display and long service life, an adequate thermal management is necessary. For this purpose, a commercially available computational fluid dynamics (CFD) simulation software FloEFD was used to predict the temperature distribution. This thermal prediction by computational method was validated by an experimental thermal analysis by attaching 10 thermocouples on the back cover of the modules and measuring the temperatures. Also, thermal camera images of the display by FLIR Thermacam SC 2000 test device were also analyzed.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6184479,no
UIO-based diagnosis of aircraft engine control systems using scilab,"Fault diagnosis is of significant importance to the robustness of aeroengine control systems. This paper makes use of full-order unknown input observers (UIOs) to facilitate the diagnosis of sensor/actuator faults in engine control systems. The built-in ui-observer function in Scilab, however, can not give satisfying performance, in terms of observer realization. Hence we rewrite this UIO program in standard Scilab scripts and decouple the effect of unknown disturbances upon state estimation to improve the sensitivity to engine faults. An evaluation platform is created on the basis of the Xcos tool in a Simulink-like manner. All the above work is accomplished in the Scilab environment. Experimental results on an aircraft turbofan engine demonstrate that the suggested UIO diagnostic method has good anti-disturbance ability and can effectively detect and isolate sensor/actuator faults under various fault conditions.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6184696,no
Assessing integrated measurement and evaluation strategies: A case study,"This paper presents a case study aimed at understanding and comparing integrated strategies for software measurement and evaluation, considering a strategy as a resource from the assessed entity standpoint. The evaluation focus is on the quality of the capabilities of a measurement and evaluation strategy taking into account three key aspects: i) the conceptual framework, centered on a terminological base, ii) the explicit specification of the process, and iii) the methodological/technological support. We consider a strategy is integrated if to great extent these three capabilities are met simultaneously. In the illustrated case study two strategies i.e. GQM<sup>+</sup>Strategies (Goal-Question-Metric), and GOCAME (Goal-Oriented Context-Aware Measurement and Evaluation) are evaluated. The given results allowed us the understanding of strengths and weaknesses for both strategies, and planning improvement actions as well.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6188462,no
Power system on-line risk assessment and decision support based on weighting fault possibility model,"The outdoor component weighting fault possibility model is established based on operational state of components, utility theory and probability theory. The model has considered the dispatchers' operation experience and sensitivity to the weather. Using this model, the risk indices of power system can be simulated and calculated. The online risk analysis software of power supply (RAPS) for regional grid has been developed by using AC-DC hybrid algorithm and introducing some advanced technology, including parallel computing, dynamic node ordering optimization, matrix inverse optimization, etc. The system can supply dispatchers with real-time decision information, and provide decision support for dispatchers. By applying this software in a regional grid, the validity and practicality of this model have been proved.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6199386,no
Research on the relationship between curvature radius of deflection basin and stress state on bottom of semi-rigid type base,"In China'current specification for the design of asphalt pavement, tensile stress of asphalt layer bottom is one of design indexes. However, the design index can not be detected and verified in practical engineering application. Research and analysis of deflection bowl in this paper show that there is a relationship between tensile stress of each layer bottom and deflections in different positions. The dynamic analysis model of rigid pavement under falling weight deflectometer load was established by utilizing ANSYS for researching the relationship between tensile stress of semi-rigid layer bottom and the curvature radius of deflection basin. This paper tries to seek a testing method to characterize pavement design indexes. It would be helpful to establish a relationship between theoretical calculation and the actual test of engineering. It also contribute to evaluate the performance of pavement and riding quality.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6199608,no
An ultrasonic testing method of multi-layer adhesive joint based on wavelet analysis,"In order to detect the debond defect of multilayer metal-rubber adhesive joint, the mature ultrasonic detecting technique was used. By rebuilding an existent ultrasonic detecting instrument and utilizing a high speed data acquisition card and LabVIEW developing environment, an ultrasonic signal acquisition and analysis platform was successfully built. Echo signal acquisition was programmed. The signal was decomposed and reconstructed by utilizing db7 wavelet in Matlab script node of LabVIEW. A contrast experiment was carried out on two specimens which have debond defect. The result shows that the reconstructed signals are remarkably different between the well-bonded areas and the debonded areas, which indicates that the wavelet analysis method is effective in detecting the debond defect under the first rubber layer.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6199675,no
Sampling + DMR: Practical and low-overhead permanent fault detection,"With technology scaling, manufacture-time and in-field permanent faults are becoming a fundamental problem. Multi-core architectures with spares can tolerate them by detecting and isolating faulty cores, but the required fault detection coverage becomes effectively 100% as the number of permanent faults increases. Dual-modular redundancy(DMR) can provide 100% coverage without assuming device-level fault models, but its overhead is excessive. In this paper, we explore a simple and low-overhead mechanism we call Sampling-DMR: run in DMR mode for a small percentage (1% of the time for example) of each periodic execution window (5 million cycles for example). Although Sampling-DMR can leave some errors undetected, we argue the permanent fault coverage is 100% because it can detect all faults eventually. SamplingDMR thus introduces a system paradigm of restricting all permanent faults' effects to small finite windows of error occurrence. We prove an ultimate upper bound exists on total missed errors and develop a probabilistic model to analyze the distribution of the number of undetected errors and detection latency. The model is validated using full gate-level fault injection experiments for an actual processor running full application software. Sampling-DMR outperforms conventional techniques in terms of fault coverage, sustains similar detection latency guarantees, and limits energy and performance overheads to less than 2%.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6307759,no
Monitoring high performance data streams in vertical markets: Theory and applications in public safety and healthcare,"Over the last several years, monitoring high performance data stream sources has become very important in various vertical markets. For example, in the public safety sector, monitoring and automatically identifying individuals suspected of terrorist or criminal activity without physically interacting with them has become a crucial security function. In the healthcare industry, noninvasive mechanical home ventilation monitoring has allowed patients with chronic respiratory failure to be moved from the hospital to a home setting without jeopardizing quality of life. In order to improve the efficiency of large data stream processing in such applications, we contend that data stream management systems (DSMS) should be introduced into the monitoring infrastructure. We also argue that monitoring tasks should be performed by executing data stream queries defined in Continuous Query Language (CQL), which we have extended with: 1) new operators that allow creation of a sophisticated event-based alerting system through the definition of threshold schemes and threshold activity scheduling, and 2) multimedia support, which allows manipulation of continuous multimedia data streams using a similarity-based join operator which permits correlation of data arriving in multimedia streams with static content stored in a conventional multimedia database. We developed a prototype in order to assess these proposed concepts and verified the effectiveness of our framework in a lab environment.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6770306,no
Distortion estimation for reference frame modification methods,"Due to the transmission of encoded video over error prone channels, using error resilient techniques at the encoder has become an essential issue. These techniques try to decrease the impact of transmission errors by using different approaches such as inserting Intra MacroBlocks (MBs), changing the prediction structure, or considering the channel state in selecting the best MB modes. In this work, we make use of the channel aware mode decision scheme used in the Loss Aware Rate Distortion Optimization (LARDO) method while simultaneously using the prediction structure of the Improved Generalized Source Channel Prediction (IGSCP) technique. In order to combine these two schemes, we estimate the end-to-end distortion for the IGSCP prediction structure in the H.264/AVC encoder. Simulation results, using the JSVM software, demonstrate the effectiveness of our technique for different sequences.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7073894,no
Architecture for Embedding Audiovisual Feature Extraction Tools in Archives,"Soon, it will no longer be sufficient for only archivists to annotate audiovisual material. Not only is the number of archivists limited, but the time they spend on annotating one item is insufficient to create time-based and detailed descriptions about the content to make fully optimized video search possible. Furthermore, as a result of file-based production methods, we observe an accelerated increase in newly created audiovisual material that must be described. Fortunately, high-quality feature extraction (FE) tools are increasingly being developed by research institutes. These tools examine the audiovisual essence and return particular information about the analyzed video, audio, or both streams. For example, the tools can automatically detect shot boundaries, detect and recognize faces and objects, and segment audio streams. As a result, they quickly and cheaply generate metadata that can be used for indexing and searching. In addition, they relieve archivists of the need to perform tedious, repetitive, but necessary low-added value tasks, such as identifying within an audio stream the speech and the music segments. Although most tools are not yet commercially offered, these solutions are expected to become available soon for broadcasters and media companies alike. This paper describes a solution for integrating such FE tools within the annotation workflow of a media company. This solution, in the form of an architecture and workflow, is scalable, extensible, and loosely coupled and has clear and easy-to-implement interfaces. As such, our architecture allows additional tools to be plugged in irrespective of the software and hardware used by the media company. By integrating FE tools within the workflow of the annotating audiovisual essence, more and better metadata can be created, allowing other tools to improve indexing, search, and retrieval of media material within audiovisual archives.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7308639,no
Practical development of an Eclipse-based software fault prediction tool using Naive Bayes algorithm,"Despite the amount of effort software engineers have been putting into developing fault prediction models, software fault prediction still poses great challenges. This research using machine learning and statistical techniques has been ongoing for 15years, and yet we still have not had a breakthrough. Unfortunately, none of these prediction models have achieved widespread applicability in the software industry due to a lack of software tools to automate this prediction process. Historical project data, including software faults and a robust software fault prediction tool, can enable quality managers to focus on fault-prone modules. Thus, they can improve the testing process. We developed an Eclipse-based software fault prediction tool for Java programs to simplify the fault prediction process. We also integrated a machine learning algorithm called Naive Bayes into the plug-in because of its proven high-performance for this problem. This article presents a practical view to software fault prediction problem, and it shows how we managed to combine software metrics with software fault data to apply Naive Bayes technique inside an open source platform.",2011,https://www.researchgate.net/profile/Banu_Diri/publication/220216023_Practical_development_of_an_Eclipse-based_software_fault_prediction_tool_using_Naive_Bayes_algorithm/links/5458df340cf2bccc4912aad1.pdf,yes
Comparing Boosting and Bagging Techniques With Noisy and Imbalanced Data,"This paper compares the performance of several boosting and bagging techniques in the context of learning from imbalanced and noisy binary-class data. Noise and class imbalance are two well-established data characteristics encountered in a wide range of data mining and machine learning initiatives. The learning algorithms studied in this paper, which include SMOTEBoost, RUSBoost, Exactly Balanced Bagging, and Roughly Balanced Bagging, combine boosting or bagging with data sampling to make them more effective when data are imbalanced. These techniques are evaluated in a comprehensive suite of experiments, for which nearly four million classification models were trained. All classifiers are assessed using seven different performance metrics, providing a complete perspective on the performance of these techniques, and results are tested for statistical significance via analysis-of-variance modeling. The experiments show that the bagging techniques generally outperform boosting, and hence in noisy data environments, bagging is the preferred method for handling class imbalance.",2011,http://ieeexplore.ieee.org/document/5645694/,yes
An industrial case study of classifier ensembles for locating software defects,"As the application layer in embedded systems dominates over the hardware, ensuring software quality becomes a real challenge. Software testing is the most time-consuming and costly project phase, specifically in the embedded software domain. Misclassifying a safe code as defective increases the cost of projects, and hence leads to low margins. In this research, we present a defect prediction model based on an ensemble of classifiers. We have collaborated with an industrial partner from the embedded systems domain. We use our generic defect prediction models with data coming from embedded projects. The embedded systems domain is similar to mission critical software so that the goal is to catch as many defects as possible. Therefore, the expectation from a predictor is to get very high probability of detection (pd). On the other hand, most embedded systems in practice are commercial products, and companies would like to lower their costs to remain competitive in their market by keeping their false alarm (pf) rates as low as possible and improving their precision rates. In our experiments, we used data collected from our industry partners as well as publicly available data. Our results reveal that ensemble of classifiers significantly decreases pf down to 15% while increasing precision by 43% and hence, keeping balance rates at 74%. The cost-benefit analysis of the proposed model shows that it is enough to inspect 23% of the code on local datasets to detect around 70% of defects.",2011,https://pdfs.semanticscholar.org/1fbb/9e93574ae29d5290d4c44d0e11f7dd6a52e1.pdf,yes
An ant colony optimization algorithm to improve software quality prediction models: Case of class stability,"ContextAssessing software quality at the early stages of the design and development process is very difficult since most of the software quality characteristics are not directly measurable. Nonetheless, they can be derived from other measurable attributes. For this purpose, software quality prediction models have been extensively used. However, building accurate prediction models is hard due to the lack of data in the domain of software engineering. As a result, the prediction models built on one data set show a significant deterioration of their accuracy when they are used to classify new, unseen data. ObjectiveThe objective of this paper is to present an approach that optimizes the accuracy of software quality predictive models when used to classify new data. MethodThis paper presents an adaptive approach that takes already built predictive models and adapts them (one at a time) to new data. We use an ant colony optimization algorithm in the adaptation process. The approach is validated on stability of classes in object-oriented software systems and can easily be used for any other software quality characteristic. It can also be easily extended to work with software quality predictive problems involving more than two classification labels. ResultsResults show that our approach out-performs the machine learning algorithm C4.5 as well as random guessing. It also preserves the expressiveness of the models which provide not only the classification label but also guidelines to attain it. ConclusionOur approach is an adaptive one that can be seen as taking predictive models that have already been built from common domain data and adapting them to context-specific data. This is suitable for the domain of software quality since the data is very scarce and hence predictive models built from one data set is hard to generalize and reuse on new data.",2011,http://www.sciencedirect.com/science/article/pii/S0950584910002144,yes
Effective and Efficient Memory Protection Using Dynamic Tainting,"Programs written in languages allowing direct access to memory through pointers often contain memory-related faults, which cause nondeterministic failures and security vulnerabilities. We present a new dynamic tainting technique to detect illegal memory accesses. When memory is allocated, at runtime, we taint both the memory and the corresponding pointer using the same taint mark. Taint marks are then propagated and checked every time a memory address m is accessed through a pointer p; if the associated taint marks differ, an illegal access is reported. To allow always-on checking using a low overhead, hardware-assisted implementation, we make several key technical decisions. We use a configurable, low number of reusable taint marks instead of a unique mark for each allocated area of memory, reducing the performance overhead without losing the ability to target most memory-related faults. We also define the technique at the binary level, which helps handle applications using third-party libraries whose source code is unavailable. We created a software-only prototype of our technique and simulated a hardware-assisted implementation. Our results show that 1) it identifies a large class of memory-related faults, even when using only two unique taint marks, and 2) a hardware-assisted implementation can achieve performance overheads in single-digit percentages.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5611490,no
Evaluation and Measurement of Software Process ImprovementA Systematic Literature Review,"BACKGROUND-Software Process Improvement (SPI) is a systematic approach to increase the efficiency and effectiveness of a software development organization and to enhance software products. OBJECTIVE-This paper aims to identify and characterize evaluation strategies and measurements used to assess the impact of different SPI initiatives. METHOD-The systematic literature review includes 148 papers published between 1991 and 2008. The selected papers were classified according to SPI initiative, applied evaluation strategies, and measurement perspectives. Potential confounding factors interfering with the evaluation of the improvement effort were assessed. RESULTS-Seven distinct evaluation strategies were identified, wherein the most common one, Pre-Post Comparison, was applied in 49 percent of the inspected papers. Quality was the most measured attribute (62 percent), followed by Cost (41 percent), and Schedule (18 percent). Looking at measurement perspectives, Project represents the majority with 66 percent. CONCLUSION-The evaluation validity of SPI initiatives is challenged by the scarce consideration of potential confounding factors, particularly given that Pre-Post Comparison was identified as the most common evaluation strategy, and the inaccurate descriptions of the evaluation context. Measurements to assess the short and mid-term impact of SPI initiatives prevail, whereas long-term measurements in terms of customer satisfaction and return on investment tend to be less used.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5728832,no
Invariant-Based Automatic Testing of Modern Web Applications,"Ajax-based Web 2.0 applications rely on stateful asynchronous client/server communication, and client-side runtime manipulation of the DOM tree. This not only makes them fundamentally different from traditional web applications, but also more error-prone and harder to test. We propose a method for testing Ajax applications automatically, based on a crawler to infer a state-flow graph for all (client-side) user interface states. We identify Ajax-specific faults that can occur in such states (related to, e.g., DOM validity, error messages, discoverability, back-button compatibility) as well as DOM-tree invariants that can serve as oracles to detect such faults. Our approach, called Atusa, is implemented in a tool offering generic invariant checking components, a plugin-mechanism to add application-specific state validators, and generation of a test suite covering the paths obtained during crawling. We describe three case studies, consisting of six subjects, evaluating the type of invariants that can be obtained for Ajax applications as well as the fault revealing capabilities, scalability, required manual effort, and level of automation of our testing approach.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5728834,no
A Fiber-Optic Multisensor System for Predischarges Detection on Electrical Equipment,"An innovative detection prototype, developed to improve the reliability of distribution networks is described. It is based on a multisensing approach including three different types of fiber-optic sensors. These sensors are based on different detection principles to measure, respectively, light ignition, sound pressure, and ozone changes produced by predischarge phenomena on medium voltage (MV) electrical equipments. A multifunctional software interface was developed to manage simultaneous acquisition and processing of all signal outputs. Preliminary tests were performed inside a MV switchboard inducing defects to simulate predischarge phenomena. A first analysis of simultaneous responses of the three sensors confirmed the feasibility of this combined approach as a potential diagnostic tool to assess the condition of MV electrical components.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5756210,no
Automating Data Analysis and Acquisition Setup in a Silicon Debug Environment,"With the growing size of modern designs and more strict time-to-market constraints, design errors can unavoidably escape pre-silicon verification and reside in silicon prototypes. Due to those errors and faults in the fabrication process, silicon debug has become a necessary step in the digital integrated circuit design flow. Embedded hardware blocks, such as scan chains and trace buffers, provide a means to acquire data of internal signals in real time for debugging. However, the amount of the data is limited compared to pre-silicon debugging. This paper presents an automated software solution to analyze this sparse data to detect suspects of the failure in both the spatial and temporal domain. It also introduces a technique to automate the configuration process for trace-buffer-based hardware in order to acquire helpful information for debugging the failure. The technique takes the hardware constraints into account and identifies alternatives for signals not part of the traceable set so that their values can be restored by implications. The experiments demonstrate the effectiveness of the proposed software solution in terms of run-time and resolution.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5771587,no
Towards Better Fault Localization: A Crosstab-Based Statistical Approach,"It is becoming prohibitively expensive and time consuming, as well as tedious and error-prone, to perform debugging manually. Among the debugging activities, fault localization has been one of the most expensive, and therefore, a large number of fault-localization techniques have been proposed over the recent years. This paper presents a crosstab-based statistical technique that makes use of the coverage information of each executable statement and the execution result (success or failure) with respect to each test case to localize faults in an effective and efficient manner. A crosstab is constructed for each executable statement, and a statistic is computed to determine the suspiciousness of the corresponding statement. Statements with a higher suspiciousness are more likely to contain bugs and should be examined before those with a lower suspiciousness. Case studies are performed on both small- (the Siemens and Unix suites) and large-sized programs (space, grep, gzip, and make), and results suggest that the crosstab-based technique (CBT) is more effective (in terms of a smaller percentage of executable statements that have to be examined until the first statement containing the fault is reached) than other techniques, such as Tarantula. Further studies using the Siemens suite reveal that the proposed technique is also more effective at locating faults than other statistically oriented techniques, such as SOBER and Liblit05. Additional experiments evaluate the CBT from other perspectives, such as its efficiency in terms of time taken, its applicability to object-oriented languages (on a very large Java program: Ant), and its sensitivity to test suite size, and demonstrate its superior performance.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5772029,no
Adaptive Estimation-Based Leakage Detection for a Wind Turbine Hydraulic Pitching System,"Operation and maintenance (OM) cost has contributed a major share in the cost of energy for wind power generation. Condition monitoring can help reduce the OM cost of wind turbine. Among the wind turbine components, the fault diagnosis of the hydraulic pitching system is investigated in this study. The hydraulic pitching system is critical for energy capture, load reduction, and aerodynamic braking. The fault detection of internal and external leakages in the hydraulic pitching system is studied in this paper. Based on the dynamic model of the hydraulic pitching system, an adaptive parameter estimation algorithm has been developed in order to identify the internal and external leakages under the time-varying load on the pitch axis. This scheme can detect and isolate individual faults in spite of their strong coupling in the hydraulic model. A scale-down setup has been developed as the hydraulic pitch emulator, with which the proposed method is verified through experiments. The pitching-axis load input is obtained from simulation of a 1.5-MW variable-speed-variable-pitch turbine model under turbulent wind profiles on the FAST (fatigue, aerodynamics, structural, and tower) software developed by the National Renewable Energy Laboratory. With the experimental data, the leakage and leakage coefficients can be predicted via the proposed method with good performance.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5772931,no
Apply Quantitative Management Now,"The Assessment Approach for Quantitative Process Management (A2QPM) helps identify software process measures for quantitative analysis even when organizations lack formal systems for process measurement. A2QPM is the first approach to quantitative management that offers software organizations a well-defined, detailed guideline for assessing their software processes and applying beneficial quantitative techniques to improve them. All the A2QPM applications we've described resulted in quantitative analysis implementations. Although the organizations had institutionalized neither a measurement process nor the processes that were subject to assessment, A2QPM nevertheless enabled quantitative improvement of the issues under study, such as process performance and product quality. Although we didn't intend the approach to be a shortcut for high ML appraisals, it has also helped organizations on their way to high maturity.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5963630,no
Software Fault Prediction Using Quad Tree-Based K-Means Clustering Algorithm,"Unsupervised techniques like clustering may be used for fault prediction in software modules, more so in those cases where fault labels are not available. In this paper a Quad Tree-based K-Means algorithm has been applied for predicting faults in program modules. The aims of this paper are twofold. First, Quad Trees are applied for finding the initial cluster centers to be input to the A'-Means Algorithm. An input threshold parameter  governs the number of initial cluster centers and by varying  the user can generate desired initial cluster centers. The concept of clustering gain has been used to determine the quality of clusters for evaluation of the Quad Tree-based initialization algorithm as compared to other initialization techniques. The clusters obtained by Quad Tree-based algorithm were found to have maximum gain values. Second, the Quad Tree- based algorithm is applied for predicting faults in program modules. The overall error rates of this prediction approach are compared to other existing algorithms and are found to be better in most of the cases.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5963674,yes
Monetary Cost-Aware Checkpointing and Migration on Amazon Cloud Spot Instances,"Recently introduced spot instances in the Amazon Elastic Compute Cloud (EC2) offer low resource costs in exchange for reduced reliability; these instances can be revoked abruptly due to price and demand fluctuations. Mechanisms and tools that deal with the cost-reliability tradeoffs under this schema are of great value for users seeking to lessen their costs while maintaining high reliability. We study how mechanisms, namely, checkpointing and migration, can be used to minimize the cost and volatility of resource provisioning. Based on the real price history of EC2 spot instances, we compare several adaptive checkpointing schemes in terms of monetary costs and improvement of job completion times. We evaluate schemes that apply predictive methods for spot prices. Furthermore, we also study how work migration can improve task completion in the midst of failures while maintaining low monetary costs. Trace-based simulations show that our schemes can reduce significantly both monetary costs and task completion times of computation on spot instance.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5975137,no
"Reasoning about the Reliability of Diverse Two-Channel Systems in Which One Channel Is """"Possibly Perfect""""","This paper refines and extends an earlier one by the first author [1]. It considers the problem of reasoning about the reliability of fault-tolerant systems with two channels (i.e., components) of which one, A, because it is conventionally engineered and presumed to contain faults, supports only a claim of reliability, while the other, B, by virtue of extreme simplicity and extensive analysis, supports a plausible claim of perfection. We begin with the case where either channel can bring the system to a safe state. The reasoning about system probability of failure on demand (pfd) is divided into two steps. The first concerns aleatory uncertainty about 1) whether channel A will fail on a randomly selected demand and 2) whether channel B is imperfect. It is shown that, conditional upon knowing p<sub>A</sub> (the probability that A fails on a randomly selected demand) and p<sub>B</sub> (the probability that channel B is imperfect), a conservative bound on the probability that the system fails on a randomly selected demand is simply p<sub>A</sub> X p<sub>B</sub>. That is, there is conditional independence between the events A fails and B is imperfect. The second step of the reasoning involves epistemic uncertainty, represented by assessors' beliefs about the distribution of (p<sub>A</sub>, p<sub>B</sub>), and it is here that dependence may arise. However, we show that under quite plausible assumptions, a conservative bound on system pfd can be constructed from point estimates for just three parameters. We discuss the feasibility of establishing credible estimates for these parameters. We extend our analysis from faults of omission to those of commission, and then combine these to yield an analysis for monitored architectures of a kind proposed for aircraft.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5975177,no
EasyPDP: An Efficient Parallel Dynamic Programming Runtime System for Computational Biology,"Dynamic programming (DP) is a popular and efficient technique in many scientific applications such as computational biology. Nevertheless, its performance is limited due to the burgeoning volume of scientific data, and parallelism is necessary and crucial to keep the computation time at acceptable levels. The intrinsically strong data dependency of dynamic programming makes it difficult and error-prone for the programmer to write a correct and efficient parallel program. Therefore, this paper builds a runtime system named EasyPDP aiming at parallelizing dynamic programming algorithms on multicore and multiprocessor platforms. Under the concept of software reusability and complexity reduction of parallel programming, a DAG Data Driven Model is proposed, which supports those applications with a strong data interdependence relationship. Based on the model, EasyPDP runtime system is designed and implemented. It automatically handles thread creation, dynamic data task allocation and scheduling, data partitioning, and fault tolerance. Five frequently used DAG patterns from biological dynamic programming algorithms have been put into the DAG pattern library of EasyPDP, so that the programmer can choose to use any of them according to his/her specific application. Besides, an ideal computing distribution model is proposed to discuss the optimal values for the performance tuning arguments of EasyPDP. We evaluate the performance potential and fault tolerance feature of EasyPDP in multicore system. We also compare EasyPDP with other methods such as Block-Cycle Wavefront (BCW). The experimental results illustrate that EasyPDP system is fine and provides an efficient infrastructure for dynamic programming algorithms.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5989801,no
Meeting Soft Deadlines in Scientific Workflows Using Resubmission Impact,"We propose a new heuristic called Resubmission Impact to support fault tolerant execution of scientific workflows in heterogeneous parallel and distributed computing environments. In contrast to related approaches, our method can be effectively used on new or unfamiliar environments, even in the absence of historical executions or failure trace models. On top of this method, we propose a dynamic enactment and rescheduling heuristic able to execute workflows with a high degree of fault tolerance, while taking into account soft deadlines. Simulated experiments of three real-world workflows in the Austrian Grid demonstrate that our method significantly reduces the resource waste compared to conservative task replication and resubmission techniques, while having a comparable makespan and only a slight decrease in the success probability. On the other hand, the dynamic enactment method manages to successfully meet soft deadlines in faulty environments in the absence of historical failure trace information or models.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5999661,no
Formal Analysis of the Probability of Interaction Fault Detection Using Random Testing,"Modern systems are becoming highly configurable to satisfy the varying needs of customers and users. Software product lines are hence becoming a common trend in software development to reduce cost by enabling systematic, large-scale reuse. However, high levels of configurability entail new challenges. Some faults might be revealed only if a particular combination of features is selected in the delivered products. But testing all combinations is usually not feasible in practice, due to their extremely large numbers. Combinatorial testing is a technique to generate smaller test suites for which all combinations of t features are guaranteed to be tested. In this paper, we present several theorems describing the probability of random testing to detect interaction faults and compare the results to combinatorial testing when there are no constraints among the features that can be part of a product. For example, random testing becomes even more effective as the number of features increases and converges toward equal effectiveness with combinatorial testing. Given that combinatorial testing entails significant computational overhead in the presence of hundreds or thousands of features, the results suggest that there are realistic scenarios in which random testing may outperform combinatorial testing in large systems. Furthermore, in common situations where test budgets are constrained and unlike combinatorial testing, random testing can still provide minimum guarantees on the probability of fault detection at any interaction level. However, when constraints are present among features, then random testing can fare arbitrarily worse than combinatorial testing. As a result, in order to have a practical impact, future research should focus on better understanding the decision process to choose between random testing and combinatorial testing, and improve combinatorial testing in the presence of feature constraints.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5999671,no
Structural Complexity and Programmer Team Strategy: An Experimental Test,"This study develops and empirically tests the idea that the impact of structural complexity on perfective maintenance of object-oriented software is significantly determined by the team strategy of programmers (independent or collaborative). We analyzed two key dimensions of software structure, coupling and cohesion, with respect to the maintenance effort and the perceived ease-of-maintenance by pairs of programmers. Hypotheses based on the distributed cognition and task interdependence theoretical frameworks were tested using data collected from a controlled lab experiment employing professional programmers. The results show a significant interaction effect between coupling, cohesion, and programmer team strategy on both maintenance effort and perceived ease-of-maintenance. Highly cohesive and low-coupled programs required lower maintenance effort and were perceived to be easier to maintain than the low-cohesive programs and high-coupled programs. Further, our results would predict that managers who strategically allocate maintenance tasks to either independent or collaborative programming teams depending on the structural complexity of software could lower their team's maintenance effort by as much as 70 percent over managers who use simple uniform resource allocation policies. These results highlight the importance of achieving congruence between team strategies employed by collaborating programmers and the structural complexity of software.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5999673,no
Clone Management for Evolving Software,"Recent research results suggest a need for code clone management. In this paper, we introduce JSync, a novel clone management tool. JSync provides two main functions to support developers in being aware of the clone relation among code fragments as software systems evolve and in making consistent changes as they create or modify cloned code. JSync represents source code and clones as (sub)trees in Abstract Syntax Trees, measures code similarity based on structural characteristic vectors, and describes code changes as tree editing scripts. The key techniques of JSync include the algorithms to compute tree editing scripts, to detect and update code clones and their groups, to analyze the changes of cloned code to validate their consistency, and to recommend relevant clone synchronization and merging. Our empirical study on several real-world systems shows that JSync is efficient and accurate in clone detection and updating, and provides the correct detection of the defects resulting from inconsistent changes to clones and the correct recommendations for change propagation across cloned code.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6007141,no
Mutation-Driven Generation of Unit Tests and Oracles,"To assess the quality of test suites, mutation analysis seeds artificial defects (mutations) into programs; a nondetected mutation indicates a weakness in the test suite. We present an automated approach to generate unit tests that detect these mutations for object-oriented classes. This has two advantages: First, the resulting test suite is optimized toward finding defects modeled by mutation operators rather than covering code. Second, the state change caused by mutations induces oracles that precisely detect the mutants. Evaluated on 10 open source libraries, our test prototype generates test suites that find significantly more seeded defects than the original manually written test suites.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6019060,no
Statistical Reliability Estimation of Microprocessor-Based Systems,"What is the probability that the execution state of a given microprocessor running a given application is correct, in a certain working environment with a given soft-error rate? Trying to answer this question using fault injection can be very expensive and time consuming. This paper proposes the baseline for a new methodology, based on microprocessor error probability profiling, that aims at estimating fault injection results without the need of a typical fault injection setup. The proposed methodology is based on two main ideas: a one-time fault-injection analysis of the microprocessor architecture to characterize the probability of successful execution of each of its instructions in presence of a soft-error, and a static and very fast analysis of the control and data flow of the target software application to compute its probability of success. The presented work goes beyond the dependability evaluation problem; it also has the potential to become the backbone for new tools able to help engineers to choose the best hardware and software architecture to structurally maximize the probability of a correct execution of the target software.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6035678,no
Formal Specification-Based Inspection for Verification of Programs,"Software inspection is a static analysis technique that is widely used for defect detection, but which suffers from a lack of rigor. In this paper, we address this problem by taking advantage of formal specification and analysis to support a systematic and rigorous inspection method. The aim of the method is to use inspection to determine whether every functional scenario defined in the specification is implemented correctly by a set of program paths and whether every program path of the program contributes to the implementation of some functional scenario in the specification. The method is comprised of five steps: deriving functional scenarios from the specification, deriving paths from the program, linking scenarios to paths, analyzing paths against the corresponding scenarios, and producing an inspection report, and allows for a systematic and automatic generation of a checklist for inspection. We present an example to show how the method can be used, and describe an experiment to evaluate its performance by comparing it to perspective-based reading (PBR). The result shows that our method may be more effective in detecting function-related defects than PBR but slightly less effective in detecting implementation-related defects. We also describe a prototype tool to demonstrate the supportability of the method, and draw some conclusions about our work.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6035726,no
Visual Readability Analysis: How to Make Your Writings Easier to Read,"We present a tool that is specifically designed to support a writer in revising a draft version of a document. In addition to showing which paragraphs and sentences are difficult to read and understand, we assist the reader in understanding why this is the case. This requires features that are expressive predictors of readability, and are also semantically understandable. In the first part of the paper, we, therefore, discuss a semiautomatic feature selection approach that is used to choose appropriate measures from a collection of 141 candidate readability features. In the second part, we present the visual analysis tool VisRA, which allows the user to analyze the feature values across the text and within single sentences. Users can choose between different visual representations accounting for differences in the size of the documents and the availability of information about the physical and logical layout of the documents. We put special emphasis on providing as much transparency as possible to ensure that the user can purposefully improve the readability of a sentence. Several case studies are presented that show the wide range of applicability of our tool. Furthermore, an in-depth evaluation assesses the quality of the measure and investigates how well users do in revising a text with the help of the tool.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6051432,no
Data Acquisition of a Tensile Test Stand for Cryogenic Environment,Superconducting magnets and components are exposed to mechanical forces during cool down or current operation. The mechanical strength of used materials has to fulfill the specified requirements. A tensile test in cryogenic environment is one option in material testing to assess usability of materials. The PHOENIX facility at Karlsruhe Institute of Technology-Institute for Technical Physics is designated to analyse specimen on tensile load under cryogenic conditions. PHOENIX was adapted for economic operation. A total number of ten samples can be tested one after the other during one cool down cycle. PHOENIX is subject to a quality management system and it is planned to be accredited according to ISO 17025 standard in the near future. Focus of work will be the qualification of steel samples for quality assurance of cryogenic magnet components of the poloidal field and toroidal field coils in the framework of the ITER project. Recent instrumentation and software provide a standard degree of automation for the measurement tasks to be performed during a tensile test. The paper describes instrumentation equipment and implemented software features of the measurement and control system.,2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6086575,no
Location of DC Line Faults in Conventional HVDC Systems With Segments of Cables and Overhead Lines Using Terminal Measurements,"This paper presents a novel algorithm to determine the location of dc line faults in an HVDC system with a mixed transmission media consisting of overhead lines and cables, using only the measurements taken at the rectifier and inverter ends of the composite transmission line. The algorithm relies on the traveling-wave principle, and requires the fault-generated surge arrival times at two ends of the dc line as inputs. With accurate surge arrival times obtained from time-synchronized measurements, the proposed algorithm can accurately predict the faulty segment as well as the exact fault location. Continuous wavelet transform coefficients of the input signal are used to determine the precise time of arrival of traveling waves at the dc line terminals. Two possible input signals-the dc voltage measured at the converter terminal and the current through the surge capacitors connected at the dc line end-are examined and both signals are found to be equally effective for detecting the traveling-wave arrival times. Performance of the proposed fault-location scheme is analyzed through detailed simulations carried out using the electromagnetic transient simulation software PSCAD. The impact of measurement noise on the fault-location accuracy is also studied in this paper.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6095644,no
Analyzing Massive Machine Maintenance Data in a Computing Cloud,"We present a novel framework, CloudView, for storage, processing and analysis of massive machine maintenance data, collected from a large number of sensors embedded in industrial machines, in a cloud computing environment. This paper describes the architecture, design, and implementation of CloudView, and how the proposed framework leverages the parallel computing capability of a computing cloud based on a large-scale distributed batch processing infrastructure that is built of commodity hardware. A case-based reasoning (CBR) approach is adopted for machine fault prediction, where the past cases of failure from a large number of machines are collected in a cloud. A case-base of past cases of failure is created using the global information obtained from a large number of machines. CloudView facilitates organization of sensor data and creation of case-base with global information. Case-base creation jobs are formulated using the MapReduce parallel data processing model. CloudView captures the failure cases across a large number of machines and shares the failure information with a number of local nodes in the form of case-base updates that occur in a time scale of every few hours. At local nodes, the real-time sensor data from a group of machines in the same facility/plant is continuously matched to the cases from the case-base for predicting the incipient faults-this local processing takes a much shorter time of a few seconds. The case-base is updated regularly (in the time scale of a few hours) on the cloud to include new cases of failure, and these case-base updates are pushed from CloudView to the local nodes. Experimental measurements show that fault predictions can be done in real-time (on a timescale of seconds) at the local nodes and massive machine data analysis for case-base creation and updating can be done on a timescale of minutes in the cloud. Our approach, in addition to being the first reported use of the cloud architecture for maintenance data storag- , processing and analysis, also evaluates several possible cloud-based architectures that leverage the advantages of the parallel computing capabilities of the cloud to make local decisions with global information efficiently, while avoiding potential data bottlenecks that can occur in getting the maintenance data in and out of the cloud.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6104038,no
Robust White Matter Lesion Segmentation in FLAIR MRI,"This paper discusses a white matter lesion (WML) segmentation scheme for fluid attenuation inversion recovery (FLAIR) MRI. The method computes the volume of lesions with subvoxel precision by accounting for the partial volume averaging (PVA) artifact. As WMLs are related to stroke and carotid disease, accurate volume measurements are most important. Manual volume computation is laborious, subjective, time consuming, and error prone. Automated methods are a nice alternative since they quantify WML volumes in an objective, efficient, and reliable manner. PVA is initially modeled with a localized edge strength measure since PVA resides in the boundaries between tissues. This map is computed in 3-D and is transformed to a global representation to increase robustness to noise. Significant edges correspond to PVA voxels, which are used to find the PVA fraction  (amount of each tissue present in mixture voxels). Results on simulated and real FLAIR images show high WML segmentation performance compared to ground truth (98.9% and 83% overlap, respectively), which outperforms other methods. Lesion load studies are included that automatically analyze WML volumes for each brain hemisphere separately. This technique does not require any distributional assumptions/parameters or training samples and is applied on a single MR modality, which is a major advantage compared to the traditional methods.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6111456,no
Virtual Appliance Size Optimization with Active Fault Injection,"Virtual appliances store the required information to instantiate a functional Virtual Machine (VM) on Infrastructure as a Service (IaaS) cloud systems. Large appliance size obstructs IaaS systems to deliver dynamic and scalable infrastructures according to their promise. To overcome this issue, this paper offers a novel technique for virtual appliance developers to publish appliances for the dynamic environments of IaaS systems. Our solution achieves faster virtual machine instantiation by reducing the appliance size while maintaining its key functionality. The new virtual appliance optimization algorithm identifies the removable parts of the appliance. Then, it applies active fault injection to remove the identified parts. Afterward, our solution assesses the functionality of the reduced virtual appliance by applying the-appliance developer provided-validation algorithms. We also introduce a technique to parallelize the fault injection and validation phases of the algorithm. Finally, the prototype implementation of the algorithm is discussed to demonstrate the efficiency of the proposed algorithm through the optimization of two well-known virtual appliances. Results show that the algorithm significantly decreased virtual machine instantiation time and increased dynamism in IaaS systems.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6122019,no
Altered Fingerprints: Analysis and Detection,"The widespread deployment of Automated Fingerprint Identification Systems (AFIS) in law enforcement and border control applications has heightened the need for ensuring that these systems are not compromised. While several issues related to fingerprint system security have been investigated, including the use of fake fingerprints for masquerading identity, the problem of fingerprint alteration or obfuscation has received very little attention. Fingerprint obfuscation refers to the deliberate alteration of the fingerprint pattern by an individual for the purpose of masking his identity. Several cases of fingerprint obfuscation have been reported in the press. Fingerprint image quality assessment software (e.g., NFIQ) cannot always detect altered fingerprints since the implicit image quality due to alteration may not change significantly. The main contributions of this paper are: 1) compiling case studies of incidents where individuals were found to have altered their fingerprints for circumventing AFIS, 2) investigating the impact of fingerprint alteration on the accuracy of a commercial fingerprint matcher, 3) classifying the alterations into three major categories and suggesting possible countermeasures, 4) developing a technique to automatically detect altered fingerprints based on analyzing orientation field and minutiae distribution, and 5) evaluating the proposed technique and the NFIQ algorithm on a large database of altered fingerprints provided by a law enforcement agency. Experimental results show the feasibility of the proposed approach in detecting altered fingerprints and highlight the need to further pursue this problem.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6136517,no
Joint H.264/scalable video coding-multiple input multiple output rate control for wireless video applications,"Integrating H.264 scalable video coding (SVC) technology with a multiple input multiple output (MIMO) wireless system can significantly enhance the overall performance of high-quality real-time wireless video transmissions. However, the state-of-the-art techniques in these two areas are largely developed independently. In this research, with the objective to deliver the optimal visual quality and accurate rate regulation for wireless video applications, the authors propose a novel joint H.264/SVC-MIMO rate control (RC) algorithm for video compression and transmission over MIMO systems. The authors first present a systematic architecture for H.264/SVC compression and transmission over MIMO systems. Then, based on MIMO channel properties, the authors use a packet-level two-state Markov model to estimate MIMO channel states and predict the number of retransmitted bits in the presence of automatic repeat request. Finally, an efficient joint rate controller is proposed to regulate the output bit rate of each layer according to the available channel throughput and buffer fullness. The authors' extensive simulation results demonstrate that their algorithm outperforms JVT-W043 RC algorithm, adopted in the H.264/SVC reference software, by providing more accurate output bit rate, reducing buffer overflow, lessen frame skipping, and finally, improves the overall coding quality.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6141177,no
A Collaboration Maturity Model: Development and Exploratory Application,This paper presents a Collaboration Maturity Model (Col-MM) to assess an organization's team collaboration quality. The Col-MM is intended to be sufficiently generic to be applied to any type of collaboration and useable by practitioners for con-ducting self-assessments. The Col-MM was developed during a series of Focus Group meetings with professional collaboration experts. The model was piloted and subsequently applied in the automotive industry. This paper reports on the development and first field application of the Col-MM. The paper further serves as a starting point for future research in this area.,2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6148645,no
Short-Circuit Fault Protection Strategy for High-Power Three-Phase Three-Wire Inverter,"This paper proposes a four-stage fault protection scheme against the short-circuit fault for the high-power three-phase three-wire combined inverter to achieve high reliability. The short-circuit fault on the load side is the focus of this paper, and the short-circuit fault of switching devices is not involved. Based on the synchronous rotating frame, the inverter is controlled as a voltage source in the normal state. When a short-circuit fault (line-to-line fault or balanced three-phase fault) occurs, the hardware-circuit-based hysteresis current control strategy can effectively limit the output currents and protect the switching devices from damage. In the meantime, the software controller detects the fault and switches to the current controlled mode. Under the current controlled state, the inverter behaves as a current source until the short-circuit fault is cleared by the circuit breaker. After clearing the fault, the output voltage recovers quickly from the current controlled state. Therefore, the selective protection is realized and the critical loads can be continuously supplied by the inverter. The operational principle, design consideration, and implementation are discussed in this paper. The simulation and experimental results are provided to verify the validity of theoretical analysis.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6152146,no
Predicting expert developers for newly reported bugs using frequent terms similarities of bug attributes,"A software bug repository not only contains the data about software bugs, but also contains the information about the contribution of developers, quality engineers (testers), managers and other team members. It contains the information about the efforts of team members involved in resolving the software bugs. This information can be analyzed to identify some useful knowledge patterns. One such pattern is identifying the developers, who can help in resolving the newly reported software bugs. In this paper a new algorithm is proposed to discover experts for resolving the newly assigned software bugs. The purpose of proposed algorithm is two fold. First is to identify the appropriate developers for newly reported bugs. And second is to find the expertise for newly reported bugs that can help other developers to fix these bugs if required. All the important information in software bug reports is of textual data types like bug summary, description etc. The algorithm is designed using the analysis of this textual information. Frequent terms are generated from this textual information and then term similarity is used to identify appropriate experts (developers) for the newly reported software bug.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6152388,no
Techniques and Tools for Parallelizing Software,"With the emergence of multicore and manycore processors, engineers must design and develop software in drastically new ways to benefit from the computational power of all cores. However, developing parallel software is much harder than sequential software because parallelism can't be abstracted away easily. Authors Hans Vandierendonck and Tom Mens provide an overview of technologies and tools to support developers in this complex and error-prone task.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6155141,no
Evaluating Stratification Alternatives to Improve Software Defect Prediction,"Numerous studies have applied machine learning to the software defect prediction problem, i.e. predicting which modules will experience a failure during operation based on software metrics. However, skewness in defect-prediction datasets can mean that the resulting classifiers often predict the faulty (minority) class less accurately. This problem is well known in machine learning, and is often referred to as learning from imbalanced datasets. One common approach for mitigating skewness is to use stratification to homogenize class distributions; however, it is unclear what stratification techniques are most effective, both generally and specifically in software defect prediction. In this article, we investigate two major stratification alternatives (under-, and over-sampling) for software defect prediction using Analysis of Variance. Our analysis covers several modern software defect prediction datasets using a factorial design. We find that the main effect of under-sampling is significant at  = 0.05, as is the interaction between under- and over-sampling. However, the main effect of over-sampling is not significant.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6156808,yes
A Dependent Model for Fault Tolerant Software Systems During Debugging,"This paper proposes a special redundant model for describing the <i>s</i>-dependency of multi-version programming software during testing and debugging. N-version programming (NVP) is one of the most important software fault tolerance techniques. Many papers have studied the issue of fault correlation among versions. However, only a few of them consider this issue during the testing and debugging part of the software development life cycle. During testing and debugging, faults may not be successfully removed. Imperfect debugging may result in unsuccessful removal, and the introduction of new faults. Different from existing NVP models, the model proposed in this paper allows an assessment of <i>s</i>-dependency when correlated failures may not necessarily occur at the same execution time point. The model focuses on 2 VP systems. It is developed to be a bivariate counting process by assuming positive <i>s</i>-dependency among versions. Considering imperfect debugging, this bivariate process characterizes dynamic changes of fault contents for each version during testing and debugging. The system reliability, expected number of faults, probability of perfect debugging, and parameter estimation of model parameters are presented. An application example is given to illustrate the proposed model. The paper provides an alternative approach for evaluating the reliability of 2 VP software systems when there is positive <i>s</i> -dependency between versions.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6156809,no
Low-cost control flow error protection by exploiting available redundancies in the pipeline,"Due to device miniaturization and reducing supply voltage, embedded systems are becoming more susceptible to transient faults. Specifically, faults in control flow can change the execution sequence, which might be catastrophic for safety critical applications. Many techniques are devised using software, hardware or software-hardware co-design for control flow error checking. Software techniques suffer from a significant amount of code size overhead, and hence, negative impact on performance and energy consumption. On the other hand, hardware-based techniques have a significant amount of hardware and area cost. In this research we exploit the available redundancies in the pipeline. The branch target buffer stores target addresses of taken branches, and ALU generates target addresses using the low-order branch displacement bits of branch instructions. To exploit these redundancies in the pipeline, we propose a control flow error checking (CFEC) scheme. It can detect control flow errors and recover from them with negligible energy and performance overhead.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6164941,no
Analysis of Clustering Techniques for Software Quality Prediction,"Clustering is the unsupervised classification of patterns into groups. A clustering algorithm partitions a data set into several groups such that similarity within a group is larger than among groups The clustering problem has been addressed in many contexts and by researchers in many disciplines, this reflects its broad appeal and usefulness as one of the steps in exploratory data analysis. There is need to develop some methods to build the software fault prediction model based on unsupervised learning which can help to predict the fault -- proneness of a program modules when fault labels for modules are not present. One of the such method is use of clustering techniques. This paper presents a case study of different clustering techniques and analyzes their performance.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6168323,no
A Clustered Approach to Analyze the Software Quality Using Software Defects,As the software development begins there also exists the probability of occurrence of some defect in the software system. Software defects plays important role to take the decision about when the testing will be stopped. Software defects are one of the major factors that can decide the time of software delivery. Not only has the number of defects also the type of defect as well the criticality of a software defect affected the software quality. Software cannot be presented with software defects. All the Software Quality estimation approaches like CMM etc. follow the software defects as a parameter to estimate the software quality. The proposed work is also in the same direction. We are trying to categorize the software defects using some clustering approach and then the software defects will be measured in each clustered separately. The proposed system will analyze the software defect respective the software criticality and its integration with software module.,2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6168329,no
Flexible Discrete Software Reliability Growth Model for Distributed Environment Incorporating Two Types of Imperfect Debugging,"In literature we have several software reliability growth models developed to monitor the reliability growth during the testing phase of the software development. These models typically use the calendar / execution time and hence are known as continuous time SRGM. However, very little seems to have been done in the literature to develop discrete SRGM. Discrete SRGM uses test cases in computer test runs as a unit of testing. Debugging process is usually imperfect because during testing all software faults are not completely removed as they are difficult to locate or new faults might be introduced. In real software development environment, the number of failures observed need not be same as the number of errors removed. If the number of failures observed is more than the number of faults removed then we have the case of imperfect debugging. Due to the complexity of the software system and the incomplete understanding of the software requirements, specifications and structure, the testing team may not be able to remove the fault perfectly on detection of the failure and the original fault may remain or get replaced by another fault. In this paper, we discuss a discrete software reliability growth model for distributed system considering imperfect debugging that faults are not always corrected/removed when they are detected and fault generation. The proposed model assumes that the software system consists of a finite number of reused and newly developed sub-systems. The reused sub-systems do not involve the effect of severity of the faults on the software reliability growth phenomenon because they stabilize over a period of time i.e. the growth is uniform whereas, the newly developed subsystem does involve. For newly developed component, it is assumed that removal process follows logistic growth curve due to the fact that learning of removal team grows as testing progresses. The fault removal phenomena for reused and newly developed sub-systems have been modeled separa- ely and are summed to obtain the total fault removal phenomenon of the software system. The model has been validated on two software data sets and it is shown that the proposed model fairs comparatively better than the existing one.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6168333,no
Assessing HPC Failure Detectors for MPI Jobs,"Reliability is one of the challenges faced by exascale computing. Components are poised to fail during large-scale executions given current mean time between failure (MTBF) projections. To cope with failures, resilience methods have been proposed as explicit or transparent techniques. For the latter techniques, this paper studies the challenge of fault detection. This work contributes a study on generic fault detection capabilities at the MPI level and beyond. The objective is to assess different detectors, which ultimately may or may not be implemented within the application's runtime layer. A first approach utilizes a periodic liveness check while a second method promotes sporadic checks upon communication activities. The contributions of this paper are two-fold: (a) We provide generic interposing of MPI applications for fault detection. (b) We experimentally compare periodic and sporadic methods for liveness checking. We show that the sporadic approach, even though it imposes lower bandwidth requirements and utilizes lower frequency checking, results in equal or worse application performance than a periodic liveness test for larger number of nodes. We further show that performing liveness checks in separation from MPI applications results in lower overhead than inter-positioning, as demonstrated by our prototypes. Hence, we promote separate periodic fault detection as the superior approach for fault detection.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6169533,no
Generative Inspection: An Intelligent Model to Detect and Remove Software Defects,"Software inspection covers the defects related to software tests Incompetence. The proposed model of this research performs defect removal actions as an important duty of inspection, as well as, using the capabilities of collaborative and knowledge base systems. The process improvement is continuously in progress by creating swap iteration in inspection model kernel. Making and modifying some rules related to defects, adds intelligence and learning features to the model. In order to validate the model, it is implemented in a real software inspection project. The varieties of detected and removed defects show the potential performance of the model and make the process reliable.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6169786,no
ERSA: Error Resilient System Architecture for Probabilistic Applications,"There is a growing concern about the increasing vulnerability of future computing systems to errors in the underlying hardware. Traditional redundancy techniques are expensive for designing energy-efficient systems that are resilient to high error rates. We present Error Resilient System Architecture (ERSA), a robust system architecture which targets emerging killer applications such as recognition, mining, and synthesis (RMS) with inherent error resilience, and ensures high degrees of resilience at low cost. Using the concept of configurable reliability, ERSA may also be adapted for general-purpose applications that are less resilient to errors (but at higher costs). While resilience of RMS applications to errors in low-order bits of data is well-known, execution of such applications on error-prone hardware significantly degrades output quality (due to high-order bit errors and crashes). ERSA achieves high error resilience to high-order bit errors and control flow errors (in addition to low-order bit errors) using a judicious combination of the following key ideas: 1) asymmetric reliability in many-core architectures; 2) error-resilient algorithms at the core of probabilistic applications; and 3) intelligent software optimizations. Error injection experiments on a multicore ERSA hardware prototype demonstrate that, even at very high error rates of 20 errors/flip-flop/10<sup>8</sup> cycles (equivalent to 25000 errors/core/s), ERSA maintains 90% or better accuracy of output results, together with minimal impact on execution time, for probabilistic applications such as K-Means clustering, LDPC decoding, and Bayesian network inference. In addition, we demonstrate the effectiveness of ERSA in tolerating high rates of static memory errors that are characteristic of emerging challenges related to SRAM <i>V</i><sub>ccmin</sub> problems and erratic bit errors.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6171059,no
Crosstalk Issues of Vicinity Magnets Studied With a Novel Rotating-Coil System,"Taiwan Photon Source (TPS) is a low-emittance synchrotron radiation factory. The lattice magnets are located compactly within the limited space in the storage ring. The mutual proximity of the magnets induces field crosstalk and influences the dynamic aperture of the electron beam. The field becomes distorted, induced by the crosstalk within not only the iron yoke but also the edges of the magnets. Precise measurements with rotating-coil systems were conducted to characterize the integral field quality of the magnets; a new method to detect the center of magnets is presented. Simulation with TOSCA software was undertaken for comparison with these experimental results. We report the field distortions induced by crosstalk on the basis of our measurements and simulations. A misalignment effect between the quadrupole and sextupole magnets is discussed.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6172556,no
Risk-informed Preventive Maintenance optimization,"The risk management group at the South Texas Project Electric Generating Station (STPEGS) has successfully developed a Preventive Maintenance (PM) optimization application based on a new mathematical model developed in collaboration with the University of Texas at Austin. This model uses historical maintenance data from the STPEGS work management database. Robust statistical analysis, coupled with an efficient algorithm generates an optimal PM schedule, based on a Non-Homogenous Poisson Process (NHPP) with a power law failure rate function. In addition, the risk associated with significant plant events triggered by a component failure is appropriately captured in the Corrective Maintenance (CM) cost estimates. The probabilities of such events are modeled via fault tree analysis, and consequences re expressed as monetary costs. The net cost of CM is then modified by a weighted sum of the probability of each event multiplied its monetary cost. The ratio of risk-adjusted CM cost to PM cost is used with the failure rate parameters to calculate the optimum PM frequency that minimizes combined CM and PM costs. The software can evaluate individual components or entire systems of components. Several low-risk ranked systems have been evaluated. In this paper we present the results of these evaluations.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6175441,no
Comparison modeling of system reliability for future NASA projects,"A National Aeronautics and Space Administration (NASA) supported Reliability, Maintainability, and Availability (RMA) analysis team developed a RMA analysis me thodology that uses cut set and importance measure analyses to compare model proposed avionics computing architectures. In this paper we will present an effective and efficient application of the RMA analysis methodology for importance measures that includes Reliability Block Diagram (RBD) Analysis, Comparison modeling, Cut Set Analysis, and Importance Measure Analysis. In addition, we will also demonstrate that integrating RMA early in the system design process is a key and fundamental decision metric that supports design selection. The RMA analysis methodology presented in this paper and applied to the avionics architectures enhances the usual way of predicting the need for redundancy based on failure rat es or subject matter expert opinion. Typically, RBDs and minimal cut sets along with the Fussell-Vesely (FV) method is used to calculate importance measures are calculated for each functional element in the architecture [1]. This paper presents an application of the FV importance measures and presents it as a methodology for using importance measures in success space to compare architectures. These importance measures are used to identify which functional element is most likely to cause a system failure, thus, quickly identifying the path to increase the overall system reliability by either procuring more reliable functional elements or adding redundancy [2]. This methodology that used RBD analysis, cut set analysis, and the FV importance measures allowed the avionics design team to better understand and compare the vulnerabilities in each scenario of the architectures. It also enabled the design team to address the deficiencies in the design architectures more efficiently, while balancing the need to design for optimum weight and space allocations.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6175444,no
Reliability analysis of substation automation system functions,"This paper presents a case study applying a framework developed for the analysis of substation automation system function reliability. The analysis framework is based on Probabilistic Relational Models (PRMs) and includes the analysis of both primary equipment and the supporting information and communication (ICT) systems. Furthermore, the reliability analysis also considers the logical structure and its relation to the physical infrastructure. The system components that are composing the physical infrastructure are set with failure probabilities and depending of the logical structure the reliability of the studied functionality is evaluated. Software failures are also accounted for in the analysis. As part of the case study failure rates of modern digital control and protection relays were identified by studying failure logs from a Nordic power utility. According to the failure logs software counts for approximately 35% of causes of failures related to modern control and protection relays. The framework including failure probabilities is applied to a system for voltage control that consists of a voltage transformer with an on-load tap changer and a control system for controlling the tap. The result shows a 96% probability of successful operation over period of one year for the automatic voltage control. A concluding remark is that when analyzing substation automation system business functions it is important to reduce the modeling effort. The expressiveness of the presented modeling framework has shown somewhat cumbersome when modeling a single business function with a small number of components. Instead the analysis framework's full usefulness may expect to arise when a larger number of business functions are evaluated for a system with a high degree of dependency between the components in the physical infrastructure. The identification of accurate failure rates is also a limiting factor for the analysis and is something that is interesting for further work.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6175459,no
On ESL verification of memory consistency for system-on-chip multiprocessing,"Chip multiprocessing is key to Mobile and high-end Embedded Computing. It requires sophisticated multilevel hierarchies where private and shared caches coexist. It relies on hardware support to implicitly manage relaxed program order and write atomicity so as to provide well-defined shared-memory semantics (captured by the axioms of a memory consistency model) at the hardware-software interface. This paper addresses the problem of checking if an executable representation of the memory system complies with a specified consistency model. Conventional verification techniques encode the axioms as edges of a single directed graph, infer extra edges from memory traces, and indicate an error when a cycle is detected. Unlike them, we propose a novel technique that decomposes the verification problem into multiple instances of an extended bipartite graph matching problem. Since the decomposition was judiciously designed to induce independent instances, the target problem can be solved by a parallel verification algorithm. Our technique, which is proven to be complete for several memory consistency models, outperformed a conventional checker for a suite of 2400 randomly-generated use cases. On average, it found a higher percentage of faults (90%) as compared to that checker (69%) and did it, on average, 272 times faster.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6176424,no
A new SBST algorithm for testing the register file of VLIW processors,"Feature size reduction drastically influences permanent faults occurrence in nanometer technology devices. Among the various test techniques, Software-Based Self-Test (SBST) approaches have been demonstrated to be an effective solution for detecting logic defects, although achieving complete fault coverage is a challenging issue due to the functional-based nature of this methodology. When VLIW processors are considered, standard processor-oriented SBST approaches result deficient since not able to cope with most of the failures affecting VLIW multiple parallel domains. In this paper we present a novel SBST algorithm specifically oriented to test the register files of VLIW processors. In particular, our algorithm addresses the cross-bar switch architecture of the VLIW register file by completely covering the intrinsic faults generated between the multiple computational domains. Fault simulation campaigns comparing previously developed methods with our solution demonstrate its effectiveness. The results show that the developed algorithm achieves a 97.12% fault coverage which is about twice better than previously developed SBST algorithms. Further advantages of our solution are the limited overhead in terms of execution cycles and memory occupation.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6176506,no
"CrashTest'ing SWAT: Accurate, gate-level evaluation of symptom-based resiliency solutions","Current technology scaling is leading to increasingly fragile components, making hardware reliability a primary design consideration. Recently researchers have proposed low-cost reliability solutions that detect hardware faults through software-level symptom monitoring. SWAT (SoftWare Anomaly Treatment), one such solution, demonstrated with microarchitecture-level simulations that symptom-based solutions can provide high fault coverage and a low Silent Data Corruption (SDC) rate. However, more accurate evaluations are needed to validate such solutions for hardware faults in real-world processor designs. In this paper, we evaluate SWAT's symptom-based detectors on gate-level faults using an FPGA-based, full-system prototype. With this platform, we performed a gate-level accurate fault injection campaign of 51,630 fault injections in the OpenSPARC T1 core logic across five SPECInt 2000 benchmarks. With an overall SDC rate of 0.79%, our results are comparable to previous microarchitecture-level evaluations of SWAT, demonstrating the effectiveness of symptom-based software detectors for permanent faults in real-world designs.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6176660,no
Flexible and Smart Online Monitoring and Fault Diagnosis System for Rotating Machinery,"Monitoring the vibration signals of rotating machinery, ulteriorly, assessing the safety of equipment plays a significant role in ensuring the security of equipment and in saving maintenance fee. This paper integrated the idea of """"configuration"""" in the industry control software, developed the """"flexible"""" network-based online monitoring and fault diagnosis system. The network topology, configuration module, database, data acquisition workstation and monitoring components were presented. With the smart data acquisition strategy and strong adaptive monitoring tools, the system can be applied on kinds of rotating machinery, and the practical application of the system was introduced.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6178486,no
Impact Analysis Using Static Execute After in WebKit,"Insufficient propagation of changes causes the majority of regression errors in heavily evolving software systems. Impact analysis of a particular change can help identify those parts of the system that also need to be investigated and potentially propagate the change. A static code analysis technique called Static Execute After can be used to automatically infer such impact sets. The method is safe and comparable in precision to more detailed analyses. At the same time it is significantly more efficient, hence we could apply it to different large industrial systems, including the open source Web Kit project. We overview the benefits of the method, its existing implementations, and present our experiences in adapting the method to such a complex project. Finally, using this particular analysis on the Web Kit project, we verify whether applying the method we can actually predict the required change propagation and hence reduce regression errors. We report on the properties of the resulting impact sets computed for the change history, and their relationship to the actual fixes required. We looked at actual defects provided by the regression test suite along with their fixes taken from the version control repository, and compared these fixes to the predicted impact sets computed at the changes that caused the failing tests. The results show that the method is applicable for the analysis of the system, and that the impact sets can predict the required changes in a fair amount of cases, but that there are still open issues for the improvement of the method.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6178857,no
On the Comparison of User Space and Kernel Space Traces in Identification of Software Anomalies,"Corrective software maintenance consumes 30-60% time of software maintenance activities. Automated failure reporting has been introduced to facilitate developers in debugging failures during corrective maintenance. However, reports of software with large user bases overwhelm developers in identification of the origins of faults, and in many cases it is not known whether reports of failures contain information about faults. Prior techniques employ different classification or anomaly detection algorithms on user space traces (e.g., function calls) or kernel space traces (e.g., system calls) to detect anomalies in software behaviour. Each algorithm and type of tracing (user space or kernel space) has its advantages and disadvantages. For example, user space tracing is useful in detailed analysis of anomalous (faulty) behaviour of a program whereas kernel space tracing is useful in identifying system intrusions, program intrusions, or malicious programs even if source program code is different. If one type of tracing or algorithm is infeasible to implement then it is important to know whether we can substitute another type of tracing and algorithm. In this paper, we compare user space and kernel space tracing by employing different types of classification algorithms on the traces of various programs. Our results show that kernel space tracing can be used to identify software anomalies with better accuracy than user space tracing. In fact, the majority of software anomalies (approximately 90%) in a software application can be best identified by using a classification algorithm on kernel space traces.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6178860,no
Software Evolution Prediction Using Seasonal Time Analysis: A Comparative Study,"Prediction models of software change requests are useful for supporting rational and timely resource allocation to the evolution process. In this paper we use a time series forecasting model to predict software maintenance and evolution requests in an open source software project (Eclipse), as an example of projects with seasonal release cycles. We build an ARIMA model based on data collected from Eclipse's change request tracking system since the project's start. A change request may refer to defects found in the software, but also to suggested improvements in the system under scrutiny. Our model includes the identification of seasonal patterns and tendencies, and is validated through the forecast of the change requests evolution for the next 12 months. The usage of seasonal information significantly improves the estimation ability of this model, when compared to other ARIMA models found in the literature, and does so for a much longer estimation period. Being able to accurately forecast the change requests' evolution over a fairly long time period is an important ability for enabling adequate process control in maintenance activities, and facilitates effort estimation and timely resources allocation. The approach presented in this paper is suitable for projects with a relatively long history, as the model building process relies on historic data.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6178868,no
Filtering Bug Reports for Fix-Time Analysis,"Several studies have experimented with data mining algorithms to predict the fix-time of reported bugs. Unfortunately, the fix-times as reported in typical open-source cases are heavily skewed with a significant amount of reports registering fix-times less than a few minutes. Consequently, we propose to include an additional filtering step to improve the quality of the underlying data in order to gain better results. Using a small-scale replication of a previously published bug fix-time prediction experiment, we show that the additional filtering of reported bugs indeed improves the outcome of the results.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6178883,no
Feature Identification from the Source Code of Product Variants,"In order to migrate software products which are deemed similar into a product line, it is essential to identify the common features and the variations between the product variants. This can however be tedious and error-prone as it may involve browsing complex software and a lot of more or less similar variants. Fortunately, if arte facts of the product variants (source code files and/or models) are available, feature identification can be at least partially automated. In this paper, we thus propose a three-step approach to feature identification from source code of which the first two steps are automated.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6178889,no
Pragmatic design quality assessment,"Summary form only given. Assessing and improving quality is paramount in every engineering discipline. Software engineering, however, is not considered a classical engineering activity for several reasons, such as intrinsic complexity and lack of rigor. In general, if a software system is delivering the expected functionality, only in few cases people see the need to analyze the internals. This tutorial is aimed to offer a pragmatic approach to analyzing the quality of software systems. On the one hand, it will offer a brief theoretical background on detecting quality problems by using and combining metrics, and by providing visual evidence of the state of affairs in the system. On the other hand, as analyzing real systems requires adequate tool support, the tutorial will offer an overview of the problems that occur in using such tools and provide a practical demonstration of using state-of-the-art tools on a real case study.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6178903,no
Software Quality Model and Framework with Applications in Industrial Context,"Software Quality Assurance involves all stages of the software life cycle including development, operation and evolution as well. Low level measurements (product and process metrics) are used to predict and control higher level quality attributes. There exists a large body of proposed metrics, but their interpretation and the way of connecting them to actual quality management goals is still a challenge. In this work, we present our approach for modelling, collecting, storing and evaluating such software measurements, which can deal with all types of metrics collected at any stage of the life cycle. The approach is based on the Goal Question Metric paradigm, and its novelty lies in a unified representation of the metrics and the questions that evaluate them. It allows the definition of various complex questions involving different types of metrics, while the supporting framework enables the automatic collection of the metrics and the calculation of the answers to the questions. We demonstrate the applicability of the approach in three industrial case studies: two instances at local software companies with different quality assurance goals, and an application to a large open source system with a question related to testing and complexity, which demonstrates the complex use of different metrics to achieve a higher level quality goal.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6178920,no
Optical Fiber Bus Protection Network to Multiplex Sensors: Experimental Validation of Self-Diagnosis,"The experimental demonstration of a resilient wavelength division multiplexed fiber bus network to interconnect sensors is reported. The network recovers operation after failures and it performs self-diagnosis, the identification of the failed constituent(s) from the patterns of surviving end-to-end connections at its operating wavelengths. We provide clear evidence for the channel arrivals predicted by theory. In doing so, we explore the potential for spurious signals caused by reflections from broken fiber ends. Appropriate precautionary measures, especially the imposition of electronic thresholds at the receivers, can greatly reduce the scope for false diagnoses. Software to predict the failure site within the network from the arriving channels at the receivers is also reported. We describe how to coordinate self-diagnosis with protection switching so as to reduce the momentary service interruption.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6183457,no
Current injection disturbance based voltage drift control for anti-islanding of photovoltaic inverter system,"Islanding detection is necessary as it causes power quality issues, equipment damage and personnel hazards. This paper proposes a new active anti-islanding scheme for inverter based photovoltaic system connected to grid. This method is based on injecting a current disturbance at the PV inverter output and observing the behavior of voltage at the point of common coupling (PCC) in absence of grid which depends upon the load connected to the PV inverter in an island condition. The proposed control scheme is based on Synchronous Reference Frame or dq frame. The voltage drift scheme using control gain and current command reference as positive feedback is utilized to drift the PCC voltage beyond the threshold limits to detect island within 2 seconds as prescribed by IEEE 1547. The test system configuration and parameters for anti-islanding study is prepared on IEEE 929 standards. The proposed control scheme is implemented on constant power controlled inverter system with reliable islanding detection. The effectiveness of proposed anti-islanding scheme is validated by extensive simulation done in MATLAB platform.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6184779,no
Induction machine fault diagnosis using microcontroller and Real Time Digital Simulation unit,"In an approach to diagnose the various types of fault, generally occurred in Induction machines, this paper describes a monitoring and analysis system. The induction machine model and its various types of fault are simulated using a Real Time Digital Simulation (RTDS) unit. The signal corresponding to the simulation can be taken out of the RTDS unit which is interfaced with a microcontroller for its acquisition in a PC. The PC based software can store it and the fault detection algorithm (sequence component based) runs over it to detect and diagnose the fault. Encouraging results are obtained.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6184832,no
An Effective Solution to Task Scheduling and Memory Partitioning for Multiprocessor System-on-Chip,"The growing trend in current complex embedded systems is to deploy a multiprocessor system-on-chip (MPSoC). A MPSoC consists of multiple heterogeneous processing elements, a memory hierarchy, and input/output components which are linked together by an on-chip interconnect structure. Such an architecture provides the flexibility to meet the performance requirements of multimedia applications while respecting the constraints on memory, cost, size, time, and power. Many embedded systems employ software-managed memories known as scratch-pad memories (SPM). Unlike caches, SPMs are software-controlled and hence the execution time of applications on such systems can be accurately predicted. Scheduling the tasks of an embedded application on the processors and partitioning the available SPM budget among these processors are two critical issues in such systems. Often, these are considered separately; such a decoupled approach may miss better quality schedules. In this paper, we present an integrated approach to task scheduling and SPM partitioning to further reduce the execution time of embedded applications. Results on several real-life benchmarks show the significant improvement from our proposed technique.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6186867,no
International Space Station power system requirements models and simulation,"International Space Station (ISS) Payload Engineering Integration (PEI) organization adopted the advanced computation and simulation technology to develop integrated electrical system models based on the test data of the various sub-units to addressing specific power system design requirements. This system model was used to assess the power system requirements for assuring: (1) Compatibility of loads with delivered power, (2) Compatibility of loads with protective devices, (3) Stability of integrated system, and (4) Fault tolerance of the EPS and other loads. PEI utilizes EMA Design Automation PSPICE software for modeling and simulating the steady-state voltage, voltage transients, reverse current, surge current, source and load impedance, large signal stability, and fault characteristics of the integrated electrical systems based on the various sub-unit test data. PSPICE provides dynamic system modeling, simulation and data analysis for large-scale system integration. Modeling is valuable at the initial design stage since it enables experimentation, exploration and development without expensive and time-consuming modifications. However, with the complexity of the system interactions among all sub-units provided by various developers and suppliers, it is difficult to model an integrated system or verify that a system model meets all the requirements of its design specifications. In addition, the changes to system requirements demand frequent redesigns and reimplementation of many systems and sub-unit components. The benefits provided by modeling from conventional test data are: (1) Relatively low cost, (2) Identification of potential system integration problems early in the program, (3) Extrapolation of test data to verify system performance to cover the entire operating envelope, (4) Provide flexibility in development system integration modeling. The modeling of an integrated system based on system and sub-unit test data enable organizations to predict and improve- system performance and to conduct efficient trade studies of system architecture. This comprehensive model can then be used directly in standard downstream processes such as rapid prototyping and risk mitigation in the product life cycle. The detailed modeling from conventional data will be discussed in the presentation.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6187245,no
Wind shear detection for small and improvised airfields,"The goal of this project is to produce an inexpensive yet highly reliable system for detecting wind shear at small and improvised airfields. While the largest commercial airports do have wind shear detection systems and they have proven to be life savers, the overwhelming majority of places where aircraft land have no such protection. The system described here is self-organizing, redundant and highly fault tolerant. It has no single point of failure and can continue operation even after a significant number of node failures. It uses inexpensive, off-the-shelf hardware and can quickly be deployed in rough conditions by minimally skilled personnel, providing pilots with potentially life-saving real time data. It is eminently suitable for use on improvised airfields for military and disaster response purposes. It can also be easily reprogrammed to detect wake turbulence instead of or in addition to low-level wind shear. The working prototype has only five nodes and is too small to protect a real airfield, but tests show that the architecture is scalable to over 100 nodes without modification, which is enough for an airfield of significant size. Using nothing but off the shelf components with novel software, the prototypes were ready for initial field trials in less than six months from initial concept.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6187319,no
Detection Technology of Train Signal Based on Loop Current Acquisition System,"Adopt ARM processor and external hardware circuit to assemble an embeded acquisition system of train signal. This system is complete and reliable. It can accurately measure the signal loop current RMS which will be sent to the monitoring equipment as required at the same time. The computing speed is fast, and the sampling accuracy and speed improves a lot. Sampling can be carried out several times in a half cycle. We take a series of links such as hardware filter, transformers isolation and software filter to process the input signal so that the external interference would be avoided. The system works well to predict the reliability of the signal fault.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6187808,no
Single-path and multi-path label switched path allocation algorithms with quality-of-service constraints: performance analysis and implementation in NS2,"The choice of the path computation algorithm is a key factor to design efficient traffic engineering strategies in multi-protocol label switching networks and different approaches have been proposed in the literature. The effectiveness of a path computation algorithm should be evaluated against its ability to optimise the utilisation of network resources as well as to satisfy both current and future label switched paths allocation requests. Although powerful and flexible simulation tools might be useful to assist a network manager in the selection of proper algorithms, state-of-the-art simulators and network planning tools do not currently offer a suitable support. This study deals with the design and performance evaluation of multi-constraints path computation algorithms. To this aim, <i>ad hoc</i> software modules have been developed and integrated within the MTENS simulator. New single-path and multi-path computation algorithms have been proposed and compared in terms of number of accepted requests, success probability, network resources utilisation and execution time. Finally, some guidelines and recommendations for the selection of path computation algorithms have also been provided.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6189152,no
Combined profiling: A methodology to capture varied program behavior across multiple inputs,"This paper introduces combined profiling (CP): a new practical methodology to produce statistically sound combined profiles from multiple runs of a program. Combining profiles is often necessary to properly characterize the behavior of a program to support Feedback-Directed Optimization (FDO). CP models program behaviors over multiple runs by estimating their empirical distributions, providing the inferential power of probability distributions to code transformations. These distributions are build from traditional single-run point profiles; no new profiling infrastructure is required. The small fixed size of this data representation keeps profile sizes, and the computational costs of profile queries, independent of the number of profiles combined. However, when using even a single program run, a CP maintains the information available in the point profile, allowing CP to be used as a drop-in replacement for existing techniques. The quality of the information generated by the CP methodology is evaluated in LLVM using SPEC CPU 2006 benchmarks.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6189227,no
An Efficient Run Time Control Flow Errors Detection by DCT Technique,"DCT is usually used in image processing but in this paper we use it to detect the run time control errors. In this paper, using the branch instruction, a program is first divided into several data computing blocks (DCBs), each DCB can then be recognized as an image. To get the signatures of each DCB, we then use one dimension discrete cosine transform (1-DDCT) to compute each DCB to generate the 5-bits relay DCT signature (R-DCT-S) and 32-bits final DCT signature (F-DCT-S).These generated signatures are then embedded into the instruction memory and then used to do the run time error checking. For watchdog, the extra hardware should not reduce the processor performance, not increase the fault detection latency and not increase the memory overhead to store the signatures. As for improving the processor degradation, the whole block error checking is done after the branch instruction, the fault detection latency is improved by checking the intermediate error at the R-type instruction, and the memory overhead is reduced by storing the R-DCT-S to the unused sections of the R-type instruction. The experimental results show that the proposed watchdog gets very high error detection coverage and shortest error detection latency to detect either single fault or multi-faults.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6189642,no
"The 18 mm<formula formulatype=""""inline""""> <img src=""""/images/tex/732.gif"""" alt=""""^{2}""""> </formula> Laboratory: Teaching MEMS Development With the SUMMiT Foundry Process","This paper describes the goals, pedagogical system, and educational outcomes of a three-semester curriculum in microelectromechanical systems (MEMS). The sequence takes engineering students with no formal MEMS training and gives them the skills to participate in cutting-edge MEMS research and development. The evolution of the curriculum from in-house fabrication facilities to an industry-standard foundry process affords an opportunity to examine the pedagogical benefits of the latter approach. Outcomes that are assessed include the number of students taking the classes, the quality of work produced by students, and the research that has emanated from class projects. Three key elements of the curriculum are identified: 1) extensive use of virtual design and process simulation software tools; 2) fabrication of student-designed devices for physical characterization and testing; and 3) integration of a student design competition. This work strongly leveraged the university outreach activities of Sandia National Laboratories (SNL) and the SNL SUMMiT MEMS design and fabrication system. SNL provides state-of-the-art design tools and device fabrication and hosts a yearly nationwide student design competition. Student MEMS designs developed using computer-aided design (CAD) and finite element analysis (FEA) software are fabricated at SNL and returned on 18-mm die modules for characterization and testing. One such module may contain a dozen innovative student projects. Important outcomes include an increase in enrollment in the introductory MEMS class, external research funding and archival journal publications arising from student designs, and consistently high finishes in the SNL competition. Since the SNL offerings are available to any US college or university, this curriculum is transportable in its current form.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6189763,no
Utilizing a Smart Grid Monitoring System to Improve Voltage Quality of Customers,"The implementation of smart grids will fundamentally change the approach of assessing and mitigating system voltage deficiencies on an electric distribution system. Many distribution companies have historically identified customer level voltage deficiencies utilizing a reactive approach that relies upon customer complaints. The monitoring capabilities of a smart grid will allow utilities to proactively identify events that exceed the voltage threshold limitations set forth by ANSI Std. C84.1 before they become a concern to end-users. This proactive approach can reduce customer complaints and possibly operational costs. This paper describes an approach for determining voltage threshold limits as a function of duration for proactive voltage investigations utilizing smart grid monitoring equipment. The described approach was applied to a smart grid located in Boulder, Colorado. This paper also describes the results of this two-year study.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6191343,no
Fault detection of system level SoC model,"The effective process models and methods for diagnosing the functional failures in software and/or hardware are offered. The register or matrix (tabular) data structures, focused to parallel execution of logic operations, are used for detecting the faulty components.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6192565,no
The evaluation of 3D traverses of three different distance lengths toward the quality of the network for Deformation Survey,"3D traverse network survey is one of the methods use by surveyor or engineer as a control network for deformation monitoring, as not all the area/position surrounding the building or high rise building suitable for GPS observation and that position unavoidablely required for monitoring of the building. Generally, this Deformation Surveys can be able to detect relative even absolute movements for monitoring of a pump base, storage tanks, retaining walls, slope areas and general ground subsidence. According to this investigation, there are errors that need to be evaluated in this observation because of the data from the length lines observed, equipment used or the condition of the study area could produce some errors. Therefore, 3D traverse network is performed using a high accuracy Topcon Total Station (ES 105) with 1 second angular and 2 mm distance accuracy to evaluate the impact of distance of each traverse line toward the quality of the closed 3D traverse. Hence, this 3D traversing is expected to generate the statistical summary test after a proper processing in the Microsurvey StarNet software systems. In the StarNet system, the quality of 3D traverse will be evaluated based on the chi-square level achievement produced after the data has been processed in the systems. Then, the result of Chi-Square Test (Total Error Factor) from the adjustment must be in the middle within Upper and Lower Bounds for the data being process to passes the adjustment. Obviously, this adjustment result may be producing the output and the quality can be evaluated.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6194761,no
Using Parameterized Attributes to Improve Testing Capabilities with Domain-Specific Modeling Languages,"Domain-specific modeling languages (DSMLs) show promise in improving model-based testing and experimentation (T&E) capabilities for software systems. This is because its intuitive graphical languages reduce complexities associated with error-prone, tedious, and time-consuming tasks. Despite the benefits of using DSMLs to facilitate model-based T&E, it is hard for testers to capture many variations of similar tests without manually duplicating modeling effort. This paper therefore presents a method called parameterized attributes that is used to capture points-of-variation in models. It also shows how parameterized attributes is realized in an open-source tool named the Generic Modeling Environment (GME) Template Engine. Finally, this paper quantitatively evaluates applying parameterized attributes to T&E of a representative distributed software system. Experience and results so show that parameterized attributes can reduce modeling effort after an initial model (or design) is constructed.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6195167,no
UFlood: High-throughput flooding over wireless mesh networks,"This paper proposes UFlood, a flooding protocol for wireless mesh networks. UFlood targets situations such as software updates where all nodes need to receive the same large file of data, and where limited radio range requires forwarding. UFlood's goals are high throughput and low airtime, defined respectively as rate of completion of a flood to the slowest receiving node and total time spent transmitting. The key to achieving these goals is good choice of sender for each transmission opportunity. The best choice evolves as a flood proceeds in ways that are difficult to predict. UFlood's core new idea is a distributed heuristic to dynamically choose the senders likely to lead to all nodes receiving the flooded data in the least time. The mechanism takes into account which data nearby receivers already have as well as internode channel quality. The mechanism includes a novel bit-rate selection algorithm that trades off the speed of high bit-rates against the larger number of nodes likely to receive low bitrates. Unusually, UFlood uses both random network coding to increase the usefulness of each transmission and detailed feedback about what data each receiver already has; the feedback is critical in deciding which node's coded transmission will have the most benefit to receivers. The required feedback is potentially voluminous, but UFlood includes novel techniques to reduce its cost. The paper presents an evaluation on a 25-node 802.11 test-bed. UFlood achieves 150% higher throughput than MORE, a high-throughput flooding protocol, using 65% less airtime. UFlood uses 54% less airtime than MNP, an existing efficient protocol, and achieves 300% higher throughput.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6195831,no
An Optimized Compilation of UML State Machines,"Due to the definition of fUML (Foundational Subset for Executable UML Models) along with its action language Alf (Action Language for fUML), UML (Unified Modeling Language) allows the production of executable models on which early verification and validation activities can be conducted. Despite this effort of standardization and the large use of UML in industry, developers still hand tune the code generated from models to correct, enhance or optimize it. This results in a gap between the model and the generated code. Manual code tuning except from being error prone can invalidate all the analysis and validations already done in the model. To avoid the code hand tuning drawbacks and, since UML is becoming an executable language, we propose a new Model Based Development (MBD) approach that skips the code generation step by compiling directly UML models. The biggest challenge for this approach - tackled in this paper is to propose a model compiler that is more efficient than a code compiler for UML models. Our model compiler performs optimizations that code compilers are unable to perform resulting in a more compact assembly code.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6195875,no
A Novel Self-Adaptive Fault-Tolerant Mechanism and Its Application for a Dynamic Pervasive Computing Environment,"In pervasive computing system, the increasing dynamic and complexity of software and hardware resources and frequentative interaction among function components make fault-tolerant design very challenging. In this paper, we propose a novel self-adaptive fault-tolerant mechanism for a dynamic pervasive computing environment such as mobile ad hoc network. In our approach, the self-adaptive fault-tolerant mechanism is dynamically built according to various types of detected faults based on continuous monitoring and analysis of the component states. We put forward the architecture of fault-tolerant system and the policy-based fault-tolerant scheme, which adopts three-dimensional array of core features to capture spatial and temporal variability and the Event-Condition-Action rules. The mentioned mechanism has been designed and implemented as self-adaptive fault-tolerant middleware, shortly called SAFTM, on a preliminary prototype for a dynamic pervasive computing environment such as mobile ad hoc network. We have performed the experiments to evaluate the efficiency of the fault-tolerant mechanism. The results of the experiments show that the performance of the self-adaptive fault tolerant mechanism is realistic.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6196102,no
Behavioral patterns in voltage transformer for ferroresonance detection,"Ferroresonance can severely affect voltage transformers, causing quality and security problems. The possibility of appearing a ferroresonance phenomenon is mainly based on an existing series connected capacitance and a nonlinear inductance. However, factors that may influence on it are not only limited to these considerations, but also to several constructive, design, operation and protective parameters. This paper analyses the process of obtaining the ferroresonant behaviour of a medium voltage (MV) phase-to-phase voltage transformer under different operation conditions. The study is developed by software simulation in order to characterize several ferroresonant behavioral patterns of the voltage transformer. This characterization may be essential for future methodologies to detect and suppress this phenomenon, frequently regarded as an unpredictable or random.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6196544,no
Decision fusion software system for turbine engine fault diagnostics,"Sophistication and complexity of current turbine engines have mandated the need for advanced fault diagnostic for monitoring the health condition of turbine engines. A critical component of these advanced diagnostic systems is the decision fusion software. The purpose of the decision fusion software system is to increase diagnostic reliability, accuracy, and improve safety of the engine operation. It also helps decrease diagnostic false alarms hence save maintenance time. This paper focuses on the development and implementation of decision-fusion software system for enhancing the diagnosis of turbine engines. The paper describes how a fuzzy logic system is used to predict and diagnose turbine engine health conditions at different levels based on the health parameters, i.e., efficiency and flow. In this paper, the decision fusion software system was broken down into two subsystems namely, Decision Making Subsystem (DMS) and Decision Fusion Subsystem (DFS). The goal of the DMS is to predict the health condition of the engine components. While the objective of DFS is to assess the overall health condition of the engine based on information provided by the DMS. The test results of developed fusion software system are promising in providing reliable diagnostics for turbine engine, subsequently reducing maintenance cost. All the system development steps and testing results on the commercial grade turbine engine model C-MAPSS will be presented in this paper.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6197000,no
On Resource Overbooking in an Unmanned Aerial Vehicle,"Large variations in the execution times of algorithms characterize many cyber-physical systems (CPS). For example, variations arise in the case of visual object-tracking tasks, whose execution times depend on the contents of the current field of view of the camera. In this paper, we study such a scenario in a small Unmanned Aerial Vehicle (UAV) system with a camera that must detect objects in a variety of conditions ranging from the simple to the complex. Given resource, weight and size constraints, such cyber-physical systems do not have the resources to satisfy the hard-real-time requirements of safe flight along with the need to process highly variable workloads at the highest quality and resolution levels. Hence, tradeoffs have to be made in real-time across multiple levels of criticality of running tasks and their operating points. Specifically, the utility derived from tracking an increasing number of objects may saturate when the mission software can no longer perform the required processing on each individual object. In this paper, we evaluate a new approach called ZS-QRAM (Zero-Slack QoS-based Resource Allocation Model) that maximizes the UAV system utility by explicitly taking into account the diminishing returns on tracking an increasing number of objects. We perform a detailed evaluation of our approach on our UAV system to clearly demonstrate its benefits.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6197392,no
WatchMyPhone  Providing developer support for shared user interface objects in collaborative mobile applications,"Developing collaborative mobile applications is a tedious and error-prone task, as collaboration functionality often has do be developed from scratch. To ease this process, we propose WatchMyPhone, a developer toolkit being focused on the creation of collaborative applications on mobile devices, especially sharing user interface components like text views in a multi-user setting. We provide an implementation of the toolkit as well as a demo application for shared text editing based on Android. WatchMyPhone is integrated in the Mobilis open source framework thus adding another facet to enable fast and efficient development of mobile social apps.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6197471,no
Development of an inductive concentration measurement sensor of nano sized zero valent iron,"The injection of colloidal nano sized zero valent iron (nZVI) into a contaminated aquifer is a promising new in-situ groundwater remediation technique. An inductive sensor is presented to directly detect and measure the concentration of nZVI in the subsurface. The method is based on the inductive measurement of magnetic material properties of nZVI within an alternating magnetic field. The change of magnetic flux density generated by one coil is determined by measuring an induced voltage in a second coil. Numerical simulations with the finite element software COMSOL Multiphysics were performed to optimize the sensor design. Furthermore, these components were used to analyze the possible measuring range, taking into account the accuracy of the measuring device to be used. Since the susceptibility of nZVI in the aquifer is very small, it is necessary to use a background measurement to improve the sensitivity of the measurement system. Finally, the measuring concept was experimentally verified.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6198042,no
Assessment of free basic electricity and use of pre-paid meters in South Africa,"In 2000, the African National Congress (ANC) through its election manifesto, made promises to provide free basic services to all poor South Africans. This was later quantified as 6 000 litres of water and 50 kWh of free basic electricity (FBE) monthly per household. Regarding the issuance of FBE, qualifying residents were registered and had to agree to a pre-paid meter being installed. It is argued that the quantity of free basic electricity provided to poor households is inadequate to meet basic needs and improvement of the quality of life. Conversely, there has been resistance to installation and use of pre-paid electricity meters, especially in townships around Johannesburg. Although prepayment systems have been proposed as innovative solutions to the problem of non-payment and affordability in utility services, the use of such mechanisms is still controversial. This paper reviews and assesses free basic electricity and the use of pre-paid electricity meters in South Africa. It also contributes to the on-going debate on FBE and prepayment systems. Recommendations are given on creating viable and stable institutions to curb uncertainties in the provision of electricity services, and methods for identifying changes in aggregate welfare resulting in the adoption of pre-paid electricity meters. Information from this article can be useful for policy-making purposes in other developing countries facing resistance in marketing, dissemination and installation of pre-paid meters.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6198227,no
Overbooking-Based Resource Allocation in Virtualized Data Center,"Efficient resource management in the virtualized data center is always a practical concern and has attracted significant attention. In particularly, economic allocation mechanism is desired to maximize the revenue for commercial cloud providers. This paper uses overbooking from Revenue Management to avoid resource over-provision according to its runtime demand. We propose an economic model to control the overbooking policy while provide users probability based performance guarantee using risk estimation. To cooperate with overbooking policy, we optimize the VM placement with traffic-aware strategy to satisfy application's QoS requirement. We design GreedySelePod algorithm to achieve traffic localization in order to reduce network bandwidth consumption, especially the network bottleneck bandwidth, thus to accept more requests and increase the revenue in the future. The simulation results show that our approach can greatly improve the request acceptance rate and increase the revenue by up to 87% while with acceptable resource confliction.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6198235,no
Fingerprint enhancement using contextual iterative filtering,"The performance of Automatic Fingerprint Identification Systems (AFIS) relies on the quality of the input fingerprints, so the enhancement of noisy images is a critical step. We propose a new fingerprint enhancement algorithm that selectively applies contextual filtering starting from automatically-detected high-quality regions and then iteratively expands toward low-quality ones. The proposed algorithm does not require any prior information like local orientations or frequencies. Experimental results over both real (FVC2004 and FVC2006) and synthetic (generated by the SFinGe software) fingerprints demonstrate the effectiveness of the proposed method.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6199773,no
@tComment: Testing Javadoc Comments to Detect Comment-Code Inconsistencies,"Code comments are important artifacts in software. Javadoc comments are widely used in Java for API specifications. API developers write Javadoc comments, and API users read these comments to understand the API, e.g., reading a Javadoc comment for a method instead of reading the method body. An inconsistency between the Javadoc comment and body for a method indicates either a fault in the body or, effectively, a fault in the comment that can mislead the method callers to introduce faults in their code. We present a novel approach, called @TCOMMENT, for testing Javadoc comments, specifically method properties about null values and related exceptions. Our approach consists of two components. The first component takes as input source files for a Java project and automatically analyzes the English text in Javadoc comments to infer a set of likely properties for a method in the files. The second component generates random tests for these methods, checks the inferred properties, and reports inconsistencies. We evaluated @TCOMMENT on seven open-source projects and found 29 inconsistencies between Javadoc comments and method bodies. We reported 16 of these inconsistencies, and 5 have already been confirmed and fixed by the developers.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200082,no
Behaviourally Adequate Software Testing,"Identifying a finite test set that adequately captures the essential behaviour of a program such that all faults are identified is a well-established problem. Traditional adequacy metrics can be impractical, and may be misleading even if they are satisfied. One intuitive notion of adequacy, which has been discussed in theoretical terms over the past three decades, is the idea of behavioural coverage, if it is possible to infer an accurate model of a system from its test executions, then the test set must be adequate. Despite its intuitive basis, it has remained almost entirely in the theoretical domain because inferred models have been expected to be exact (generally an infeasible task), and have not allowed for any pragmatic interim measures of adequacy to guide test set generation. In this work we present a new test generation technique that is founded on behavioural adequacy, which combines a model evaluation framework from the domain of statistical learning theory with search-based white-box test generation strategies. Experiments with our BESTEST prototype indicate that such test sets not only come with a statistically valid measurement of adequacy, but also detect significantly more defects.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200086,no
A Scalable Distributed Concolic Testing Approach: An Empirical Evaluation,"Although testing is a standard method for improving the quality of software, conventional testing methods often fail to detect faults. Concolic testing attempts to remedy this by automatically generating test cases to explore execution paths in a program under test, helping testers achieve greater coverage of program behavior in a more automated fashion. Concolic testing, however, consumes a significant amount of computing time to explore execution paths, which is an obstacle toward its practical application. To address this limitation, we have developed a scalable distributed concolic testing framework that utilizes large numbers of computing nodes to generate test cases in a scalable manner. In this paper, we present the results of an empirical study that shows that the proposed framework can achieve a several orders-of-magnitude increase in test case generation speed compared to the original concolic approach, and also demonstrates clear potential for scalability.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200090,no
Dynamic Backward Slicing of Model Transformations,"Model transformations are frequently used means for automating software development in various domains to improve quality and reduce production costs. Debugging of model transformations often necessitates identifying parts of the transformation program and the transformed models which have causal dependence on a selected statement. In traditional programming environments, program slicing techniques are widely used to calculate control and data dependencies between the statements of the program. Here, we introduce program slicing for model transformations where the main challenge is to simultaneously assess data and control dependencies over the transformation program and the underlying models of the transformation. In this paper, we present a dynamic backward slicing approach for both model transformation programs and their transformed models based on automatically generated execution trace models of transformations. We evaluate our approach using different transformation case studies.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200091,no
AutoFLox: An Automatic Fault Localizer for Client-Side JavaScript,"Java Script is a scripting language that plays a prominent role in modern web applications today. It is dynamic, loosely typed, and asynchronous. In addition, it is extensively used to interact with the DOM at runtime. All these characteristics make Java Script code error-prone and challenging to debug. Java Script fault localization is currently a tedious and mainly manual task. Despite these challenges, the problem has received very limited attention from the research community. We propose an automated technique to localize Java Script faults based on dynamic analysis of the web application, tracing, and backward slicing of Java Script code. Our fault localization approach is implemented in an open source tool called Auto Lox. The results of our empirical evaluation indicate that (1) DOM-related errors are prominent in web applications, i.e., they form at least 79% of reported Java Script bugs, (2) our approach is capable of automatically localizing DOM-related Java Script errors with a high degree of accuracy (over 90%) and no false-positives, and (3) our approach is capable of isolating Java Script errors in a production web application, viz., Tumbler.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200094,no
Tester Feedback Driven Fault Localization,"Coincidentally correct test cases are those that execute faulty statements but do not cause failures. Such test cases reduce the effectiveness of spectrum-based fault localization techniques, such as Ochiai, because the correlation of failure with the execution of a faulty statement is lowered. Thus, coincidentally correct test cases need to be predicted and removed from the test suite used for fault localization. Techniques for predicting coincidentally correct test cases can produce false positives, such as when one predicts a fixed percentage that is higher than the actual percentage of coincidentally correct test cases. False positives may cause non-faulty statements to be assigned higher suspiciousness scores than the faulty statements. We propose an approach that iteratively predicts and removes coincidentally correct test cases. In each iteration, we present the tester the set of statements that share the highest Ochiai suspiciousness score. If the tester reports that these statements are not faulty, we use that feedback to determine a number that is guaranteed to be less than or equal to the actual number of coincidentally correct test cases. We predict and remove that number of coincidentally correct test cases, recalculate the suspiciousness scores of the remaining statements, and repeat the process. We evaluated our approach with the Siemens benchmark suite and the Unix utilities, grep and gzip. Our approach outperformed an existing approach that predicts a fixed percentage of test cases as coincidentally correct. The results with Ochiai were mixed. In some cases, our approach outperformed Ochiai by up to 67%. In others, Ochiai was more effective.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200095,no
A Unified Approach for Localizing Non-deadlock Concurrency Bugs,"This paper presents UNICORN, a new automated dynamic pattern-detection-based technique that finds and ranks problematic memory access patterns for non-deadlock concurrency bugs. UNICORN monitors pairs of memory accesses, combines the pairs into problematic patterns, and ranks the patterns by their suspiciousness scores. UNICORN detects significant classes of bug types, including order violations and both single-variable and multi-variable atomicity violations, which have been shown to be the most important classes of non-deadlock concurrency bugs. The paper also describes implementations of UNICORN in Java and C++, along with empirical evaluation using these implementations. The evaluation shows that UNICORN can effectively compute and rank the patterns that represent concurrency bugs, and perform computation and ranking with reasonable efficiency.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200096,no
Test Adequacy Evaluation for the User-database Interaction: A Specification-Based Approach,"Testing a database application is a challenging process where both the database and the user interaction have to be considered in the design of test cases. This paper describes a specification-based approach to guide the design of test inputs (both the test database and the user inputs) for a database application and to automatically evaluate the test adequacy. First, the system specification of the application is modelled: (1) the structure of the database and the user interface are represented in a single model, called Integrated Data Model (IDM), (2) the functional requirements are expressed as a set of business rules, written in terms of the IDM. Then, a MCDC-based criterion is applied over the business rules to automatically derive the situations of interest to be tested (test requirements), which guide the design of the test inputs. Finally, the adequacy of these test inputs is automatically evaluated to determine whether the test requirements are covered. The approach has been applied to the TPC-C benchmark. The results show that it allows designing test cases that are able to detect interesting faults which were located in the procedural code of the implementation.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200098,no
CrossCheck: Combining Crawling and Differencing to Better Detect Cross-browser Incompatibilities in Web Applications,"One of the consequences of the continuous and rapid evolution of web technologies is the amount of inconsistencies between web browsers implementations. Such inconsistencies can result in cross-browser incompatibilities (XBIs)-situations in which the same web application can behave differently when run on different browsers. In some cases, XBIs consist of tolerable cosmetic differences. In other cases, however, they may completely prevent users from accessing part of a web application's functionality. Despite the prevalence of XBIs, there are hardly any tools that can help web developers detect and correct such issues. In fact, most existing approaches against XBIs involve a considerable amount of manual effort and are consequently extremely time consuming and error prone. In recent work, we have presented two complementary approaches, WEBDIFF and CROSST, for automatically detecting and reporting XBIs. In this paper, we present CROSSCHECK, a more powerful and comprehensive technique and tool for XBI detection that combines and adapts these two approaches in a way that leverages their respective strengths. The paper also presents an empirical evaluation of CROSSCHECK on a set of real-world web applications. The results of our experiments show that CROSSCHECK is both effective and efficient in detecting XBIs, and that it can outperform existing techniques.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200112,no
An Empirical Study of Pre-release Software Faults in an Industrial Product Line,"There is a lack of published studies providing empirical support for the assumption at the heart of product line development, namely, that through structured reuse later products will be less fault-prone. This paper presents results from an empirical study of pre-release fault and change proneness from four products in an industrial software product line. The objectives of the study are (1) to determine the association between various software metrics, as well as their correlation with the number of faults at the component level, (2) to characterize the fault and change proneness at various degrees of reuse, and (3) to determine how existing products in the software product line affect the quality of subsequently developed products and our ability to make predictions. The research results confirm, in a software product line setting, the findings of others that faults are more highly correlated to change metrics than to static code metrics. Further, the results show that variation components unique to individual products have the highest fault density and are the most prone to change. The longitudinal aspect of our research indicates that new products in this software product line benefit from the development and testing of previous products. For this case study, the number of faults in variation components of new products is predicted accurately using a linear model built on data from the previous products.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200114,no
Challenges for Addressing Quality Factors in Model Transformation,"Designing a high quality model transformation is critical, because it is the pivotal mechanism in many mission applications for evolving the intellectual design described by models. This paper proposes solution ideas to assist modelers in developing high quality transformation models. We propose to initiate a design pattern movement in the context of model transformation. The resulting catalog of patterns shall satisfy quality attributes identified beforehand. Verification and validation of these patterns allow us to assess whether the cataloged design patterns are sound and complete with respect to the quality criteria. This will lead to techniques and tools that can detect bad designs and propose alternatives based on well-thought design patterns during the development or maintenance of model transformation.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200115,no
Identifying Failure-Inducing Combinations in a Combinatorial Test Set,"A t-way combinatorial test set is designed to detect failures that are triggered by combinations involving no more than t parameters. Assume that we have executed a t-way test set and some tests have failed. A natural question to ask is: What combinations have caused these failures? Identifying such combinations can facilitate the debugging effort, e.g., by reducing the scope of the code that needs to be inspected. In this paper, we present an approach to identifying failure-inducing combinations, i.e., combinations that have caused some tests to fail. Given a t-way test set, our approach first identifies and ranks a set of suspicious combinations, which are candidates that are likely to be failure-inducing combinations. Next, it generates a set of new tests, which can be executed to refine the ranking of suspicious combinations in the next iteration. This process can be repeated until a stopping condition is satisfied. We conducted an experiment in which our approach was applied to several benchmark programs. The experimental results show that our approach can effectively and efficiently identify failure-inducing combinations in these programs.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200129,no
Industrial Application of Concolic Testing on Embedded Software: Case Studies,"Current industrial testing practices often build test cases in a manual manner, which is slow and ineffective. To alleviate this problem, concolic testing generates test cases that can achieve high coverage in an automated fashion. However, due to a large number of possible execution paths, concolic testing might not detect bugs even after spending significant amount of time. Thus, it is necessary to check if concolic testing can detect bugs in embedded software in a practical manner through case studies. This paper describes case studies of applying the concolic testing tool CREST to embedded Applications. Through this project, we have detected new faults in the Samsung Linux Platform (SLP) file manager, Samsung security library, and busy box ls.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200131,no
Securing Opensource Code via Static Analysis,"Static code analysis (SCA) is the analysis of computer programs that is performed without actually executing the programs, usually by using an automated tool. SCA has become an integral part of the software development life cycle and one of the first steps to detect and eliminate programming errors early in the software development stage. Although SCA tools are routinely used in proprietary software development environment to ensure software quality, application of such tools to the vast expanse of open source code presents a forbidding albeit interesting challenge, especially when open source code finds its way into commercial software. Although there have been recent efforts in this direction, in this paper, we address this challenge to some extent by applying static analysis on a popular open source project, i.e., Linux kernel, discuss the results of our analysis and based on our analysis, we propose an alternate workflow that can be adopted while incorporating open source software in a commercial software development process. Further, we discuss the benefits and the challenges faced while adopting the proposed alternate workflow.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200135,no
A Smart Structured Test Automation Language (SSTAL),"In model-based testing, abstract tests are designed in terms of a model, for example, a path in a graph and concrete tests are expressed in terms of the implementation of the model. Two problems exist while converting abstract tests to concrete tests: the """"mapping'' problem and test oracle problem. Abstract tests cannot be applied directly to the actual program and they must first be mapped to concrete tests. Testers currently solve this mapping problem by hand. If one basic action is used multiple times in different abstract tests, testers must write a lot of redundant code for the same action. This process is time-consuming, labor-intensive, and error-prone. For the """"mapping'' problem, this research will design a structured test automation language to partially automate the mapping from abstract tests to concrete tests. First, programmers or testers use the automation language to create mappings from each basic identifiable part (e.g. an action in a finite-state machine) of the model to the corresponding executable programming code. Once the mappings are generated, concrete tests can be generated automatically from the abstract tests. This structured test automation language will be used to improve the efficiency of generating concrete tests from a variety of abstract tests and reduce the potential errors. Test oracle problem refers to how to determine whether a test has passed or fail. While writing executable code, it is difficult to decide which parts of the program state should be compared by the concrete tests. A guideline will be established to evaluate stats of the program effectively and efficiently.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200140,no
Mitigating the Effect of Coincidental Correctness in Spectrum Based Fault Localization,"Coincidentally correct test cases are those that execute faulty statements but do not cause failures. Such test cases reduce the effectiveness of spectrum-based fault localization techniques, such as Ochiai. These techniques calculate a suspiciousness score for each statement. The suspiciousness score estimates the likelihood that the program will fail if the statement is executed. The presence of coincidentally correct test cases reduces the suspiciousness score of the faulty statement, thereby reducing the effectiveness of fault localization. We present two approaches that predict coincidentally correct test cases and use the predictions to improve the effectiveness of spectrum based fault localization. In the first approach, we assign weights to passing test cases such that the test cases that are likely to be coincidentally correct obtain low weights. Then we use the weights to calculate suspiciousness scores. In the second approach, we iteratively predict and remove coincidentally correct test cases, and calculate the suspiciousness scores with the reduced test suite. In this dissertation, we investigate the cost and effectiveness of our approach to predicting coincidentally correct test cases and utilizing the predictions. We report the results of our preliminary evaluation of effectiveness and outline our research plan.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200142,no
Web Mutation Testing,"Web application software uses new technologies that have novel methods for integration and state maintenance that amount to new control flow mechanisms and new variables coping. Although powerful, these bring in new problems that current testing techniques do not adequately test for. Testing individual web software component in isolation cannot detect interaction faults, which occur in communication among web software components. Improperly implementing and testing the communications among web software components is a major source of faults. As mutation analysis has been shown to be effective in testing traditional software, the proposed project will investigate the usefulness of applying mutation testing to web applications. In a preliminary study, several new web mutation operators were defined specifically for web interaction faults. These operators were implemented in a prototype tool for a feasibility study. The resulting paper appeared in Mutation 2010 and the experimental results evince that mutation analysis can potentially help create tests that are effective at finding web application faults. To improve web fault coverage, the initial set of web mutation operators will be extended and evaluated. Additional web mutation operators will be defined. I intend to validate the proposed technique, web mutation testing, by comparing with other existing approaches used for web application testing.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200146,no
Experimental Comparison of Test Case Generation Methods for Finite State Machines,"Testing from finite state machines has been widely investigated due to its well-founded and sound theory as well as its practical application in different areas, e.g., Web-based systems and protocol testing. There has been a recurrent interest in developing methods capable of generating test suites that detect all faults in a given fault domain. However, the proposal of new methods motivates the comparison with traditional methods. In this context, we conducted a set of experiments that compares W, HSI, H, SPY, and P methods. The results have shown that H, SPY, and P methods produce smaller test suites than traditional methods (W, HSI). Although the P method presented the shortest test suite in most cases, its reduction is smaller compared with H and SPY. We have also observed that the reduction ratio in partial machines is smaller than that in complete machines.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200152,no
Overcoming Web Server Benchmarking Challenges in the Multi-core Era,"Web-based services are used by many organizations to support their customers and employees. An important consideration in developing such services is ensuring the Quality of Service (QoS) that users experience is acceptable. Recent years have seen a shift toward deploying Web services on multi-core hardware. Leveraging the performance benefits of multi-core hardware is a non-trivial task. In particular, systematic Web server benchmarking techniques are needed so organizations can verify their ability to meet customer QoS objectives while effectively utilizing such hardware. However, our recent experiences suggest that the multi-core era imposes significant challenges to Web server benchmarking. In particular, due to limitations of current hardware monitoring tools, we found that a large number of experiments are needed to detect complex bottlenecks that can arise in a multi-core system due to contention for shared resources such as cache hierarchy, memory controllers and processor inter-connects. Furthermore, multiple load generator instances are needed to adequately stress multi-core hardware. This leads to practical challenges in validating and managing the test results. This paper describes the automation strategies we employed to overcome these challenges. We make our test harness available for other researchers and practitioners working on similar studies.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200166,no
Test Case Prioritization Due to Database Changes in Web Applications,"A regression test case prioritization (TCP) technique reorders test cases for regression testing to achieve early fault detection. Most TCP techniques have been developed for regression testing of source code in an application. Most web applications rely on a database server for serving client requests. Any changes in the database result in erroneous client interactions and may bring down the entire web application. However, most prioritization techniques are unsuitable for prioritizing test suites for early detection of changes in databases. There are very few proposals in the literature for prioritization of test cases that can detect faults in the database early. We propose a new automated TCP technique for web applications that automatically identifies the database changes, prioritizes test cases related to database changes and executes them in priority order to detect faults early.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200175,no
Towards Symbolic Model-Based Mutation Testing: Pitfalls in Expressing Semantics as Constraints,"Model-based mutation testing uses altered models to generate test cases that are able to detect whether a certain fault has been implemented in the system under test. For this purpose, we need to check for conformance between the original and the mutated model. We have developed an approach for conformance checking of action systems using constraints. Action systems are well-suited to specify reactive systems and may involve non-determinism. Expressing their semantics as constraints for the purpose of conformance checking is not totally straight forward. This paper presents some pitfalls that hinder the way to a sound encoding of semantics into constraint satisfaction problems and gives solutions for each problem.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200181,no
Automatic XACML Requests Generation for Policy Testing,"Access control policies are usually specified by the XACML language. However, policy definition could be an error prone process, because of the many constraints and rules that have to be specified. In order to increase the confidence on defined XACML policies, an accurate testing activity could be a valid solution. The typical policy testing is performed by deriving specific test cases, i.e. XACML requests, that are executed by means of a PDP implementation, so to evidence possible security lacks or problems. Thus the fault detection effectiveness of derived test suite is a fundamental property. To evaluate the performance of the applied test strategy and consequently of the test suite, a commonly adopted methodology is using mutation testing. In this paper, we propose two different methodologies for deriving XACML requests, that are defined independently from the policy under test. The proposals exploit the values of the XACML policy for better customizing the generated requests and providing a more effective test suite. The proposed methodologies have been compared in terms of their fault detection effectiveness by the application of mutation testing on a set of real policies.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200197,no
Adding Criteria-Based Tests to Test Driven Development,"Test driven development (TDD) is the practice of writing unit tests before writing the source. TDD practitioners typically start with example-based unit tests to verify an understanding of the software's intended functionality and to drive software design decisions. Hence, the typical role of test cases in TDD leans more towards specifying and documenting expected behavior, and less towards detecting faults. Conversely, traditional criteria-based test coverage ignores functionality in favor of tests that thoroughly exercise the software. This paper examines whether it is possible to combine both approaches. Specifically, can additional criteria based tests improve the quality of TDD test suites without disrupting the TDD development process? This paper presents the results of an observational study that generated additional criteria-based tests as part of a TDD exercise. The criterion was mutation analysis and the additional tests were designed to kill mutants not killed by the TDD tests. The additional unit tests found several software faults and other deficiencies in the software. Subsequent interviews with the programmers indicated that they welcomed the additional tests, and that the additional tests did not inhibit their productivity.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200203,no
Languages and Their Importance in Quality Software,"Computer software succeeds - When it meets the needs of the people who use it, when it performs flawlessly over a long period of time, when it is easy to modify and even easier to use-it can does change things for the better. But When software fails-When its users are dissatisfied, When it is error prone, When it is difficult to change and even harder to use - bad things can and do happen. Generally software quality has been defined by various characteristics of the software product. This paper analyzes the various languages based on various characteristics and shown their impact on software quality.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200777,no
A multi-frame and multi-slice H.264 parallel video encoding approach with simultaneous encoding of prediction frames,"This paper describes a novel multi-frame and multi-slice parallel video encoding approach with simultaneous encoding of predicted frames. The approach, when applied to H.264 encoding, leads to speedups comparable to those obtained by state-of-the-art approaches, but without the disadvantage of requiring bidirectional frames. The new approach uses a number of slices equal or greater than the number of cores used and supports three motion estimation modes. Their combination leads to various tradeoffs between speedup and visual quality loss. For an H.264 baseline profile encoder based on Intel IPP code samples running on a two quad core Xeon system (8 cores in total), our experiments show an average speedup of 7.20, with an average quality loss of 0.22 dB (compared to a non-parallelized version) for the most efficiency motion estimation mode, and an average speedup of 7.95, with a quality loss of 1.85 dB for the faster motion estimation mode.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6202018,no
Apply embedded openflow MPLS technology on wireless Openflow  OpenRoads,"Openflow is one of the most important and popular next generation internet structure and technology. Its thought of software decide network makes dispatch of controller and date layer. Controller uses the openflow protocol to communicate and control the openflow switch to generate flow table, thus to achieve the centralized control of the whole network. OpenRoads is the framework about the wireless Openflow environment. It successfully apply the thought of SDN on the wireless mobile environment. As a propose to improve the performance and quality of the OpenRoads, The new embedded Openflow-MPLS(EOF-MPLS) is presented. An important technology key point is that both traditional MPLS and Openflow support the thought of separation between controller and date layer which makes a solid foundation for the new EOF-MPLS. EOF-MPLS offered forward equal class(FEC), Label distribute Protocol(LDP) to improve the efficiency of the nodes forward. It makes tight coupling with Openflow structure and support the QoS and traffic engineering. At last an entropy is came up with to assess the communication effectiveness of the EOF-MPLS to the OpenRoads.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6202066,no
Formal specification of humanitarian disaster management processes,"Disaster situations are dynamic and demanding a change in response, and information is received in a fragmented manner during the early stages of the event. Therefore, a Rapid Assessment and Intervention Team (RAIT Team) is established to respond quickly to the event, by achieving an initial assessment helping to understand the nature and scope of the incident and to determine the required assistance. The RAIT Team can be considered as a Reactive Collaborative Network, having to coordinate numerous actors and two or more reactive parallel sub-processes. Therefore, formal specification and validation methods and tools are needed while specifying and verifying RAIT processes to detect and correct deficiencies and faults In this perspective, the present paper proposes to use the Decisional Reactive Agents (DRA) based approach for the formal modeling and checking of RAIT system processes, so that they can be directly implemented in software environments with the maximum of logical correctness. The proposed approach is illustrated by formally checking temporal constraints of a transversal RAIT process acting in case of medical or humanitarian emergency.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6203056,no
TIP-EXE: A Software Tool for Studying the Use and Understanding of Procedural Documents,"Research problem: When dealing with procedural documents, individuals sometimes encounter comprehension problems due to poor information design. Researchers studying the use and understanding of procedural documents, as well as technical writers charged with the design of these documents, or usability specialists evaluating their quality, would all benefit from tools allowing them to collect real-time data concerning user behavior in user-centered studies. With this in mind, the generic software Technical Instructions Processing-Evaluations and eXperiments Editor (TIP-EXE) was designed to facilitate the carrying out of such studies. Research questions: Does document design, and specifically the matching or mismatching of the terms employed in a user manual and on the corresponding device, affect the cognitive processes involved in the comprehension of procedural instructions? Can we use a software tool like TIP-EXE to assess the impact of document design on the use and understanding of a procedural document? Literature review: A review of the methods employed to study either the use of procedural documents or their cognitive processing, and to evaluate the quality of these documents, revealed the lack of tools for collecting relevant data. Methodology: TIP-EXE software was used to set up and run a laboratory experiment designed to collect data concerning the effect of document design on the performance of a task. The experiment was conducted with 36 participants carrying out tasks involving the programming of a digital timer under one of three conditions: matching instructions, mismatching instructions, mismatching instructions + picture. Based on a click-and-read method for blurred text, TIP-EXE was used to collect data on the time the users spent reading the instructions, as well as the time spent handling the timer. Results and discussion: Results show that matching instructions (when the te- ms employed in the user manual match the terms on the device) enhance user performance. This instructional format results in less time spent consulting the instructions and handling the device, as well as fewer errors. This research shows that TIP-EXE software can be used to study the way in which operating instructions are read, and the time spent consulting specific information contained therein, thereby revealing the effects of document design on user behavior.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6203646,no
"Indoor propagation model in 2.4 GHz with QoS parameters estimation in VoIP calls, considering different types of walls and floors",This paper presents an empirical propagation model for indoor environments with different types of walls and floors the predicts not only the power level but also QoS parameters to ensure quality in VoIP calls.,2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6206494,no
"Improving public auditability, data possession in data storage security for cloud computing","Cloud computing is Internet based technology where the users can subscribe high quality of services from data and software that resides solely in the remote servers. This provides many benefits for the users to create and store data in the remote servers thereby utilizing fewer resources in client system. However management of the data and software may not be fully trustworthy which possesses many security challenges. One of the security issues is the data storage security where frequent integrity checking of remotely stored data is carried out. RSA based storage security (RSASS) method uses public auditing of the remote data by improving existing RSA based signature generation. This public key cryptography technique is widely used for providing strong security. Using this RSASS method, the data storage correctness is assured and identification of misbehaving server with high probability is achieved. This method also supports dynamic operation on the data and tries to reduce the server computation time. The preliminary results achieved through RSASS, proposed scheme outperforms with improved security in data storage when compared with the existing methods.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6206835,no
A framework to select an approach for Web services and SOA development,"Service-Orientation Architecture (SOA) is an architectural style for software, where the main components are loosely coupled, interoperable, distributed pieces of logic provided as Web services (WSs). Service-Orientation (SO) is a paradigm that provides WSs with a set of design principles in order to conform to SOA. Service-Oriented Software Engineering (SOSE) concerns with processes and tools to build and provide software systems as composition of WSs with respect to SOA. This work proposes to assess different types of approaches within a framework that considers critical perspectives such as: (i) Building blocks used to specify functional requirements with respect to business/IT alignment, (ii) SOA principles and drivers, (iii) SO design paradigm, (iv) solution lifecycle, modeling, views, and CASE tools, and (v) inspection of solution quality attributes. The framework is meant to answer the question to what extent a solution provided by any service-oriented development method would conform to SO? in order to be used in: (a) comparing the approaches themselves, and (b) highlighting issues that need further research.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6207747,no
WESPACT:  Detection of web spamdexing with decision trees in GA perspective,"Internet today is huge, dynamic, self-organized, and strongly interlinked. Web spam can significantly worsen the quality of search engine results. The motivation of the paper is based on the logical perspective of approaching the web spam problem as cancer caused to the internet, and the solution could be derived by formulating the algorithms based on genetic algorithm (GA) based on content and link attributes. Web mining tools GATree [15] and PermutMatrix [14] has been used to simulate the experiments. JAVA is used to develop program that analyze and report the spamdexing instance. This paper proposes an algorithm WESPACT, to detect the web spam. This algorithm performs well as shown through experiments.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6208376,no
A Tool for Teaching Risk,"Students tend to think optimistically about the software they construct. They believe the software will be defect free, and underestimate apparent risks to the development process. In the Software Enterprise, a 4-course upper division project sequence, student team failures to predict and prevent these risks lead to various problems like schedule delays, frustration, and dissatisfaction from external customer sponsors. The Enterprise uses the IBM Rational Jazz platform, but it does not have a native risk management capability. Instead, project teams were recording risks associated with their projects on paper reports. To facilitate maintaining and managing the risks associated with their projects, we developed a risk management component in the Jazz environment. This component complements Jazz by providing features of the risk management process like risk control and monitoring. The risk management component was used and evaluated by student capstone project teams.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6209177,no
When the Software Goes Beyond its Requirements -- A Software Security Perspective,"Evidences from current events have shown that, in addition to virus and hacker attacks, many software systems have been embedded with """"agents"""" that pose security threats such as allowing someone to """"invade"""" into computers with such software installed. This will eventually grow into a more serious problem when Cluster and Cloud Computing becomes popular. As this is an area that few have been exploring, we discuss in this paper the issue of software security breaches resulting from embedded sleeping agents. We also investigate some patterns of embedded sleeping agents utilized in software industry. In addition, we review these patterns and propose a security model that identifies different scenarios. This security model will provide a foundation for further study on how to detect and prevent such patterns from becoming security breaches.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6209207,no
A Novel Flit Serialization Strategy to Utilize Partially Faulty Links in Networks-on-Chip,"Aggressive MOS transistor size scaling substantially increase the probability of faults in NoC links due to manufacturing defects, process variations, and chip wire-out effects. Strategies have been proposed to tolerate faulty wires by replacing them with spare ones or by partially using the defective links. However, these strategies either suffer from high area and power overheads, or significantly increase the average network latency. In this paper, we propose a novel flit serialization method, which divides the links and flits into several sections, and serializes flit sections of adjacent flits to transmit them on all available fault-free link sections to avoid the complete waste of defective links bandwidth. Experimental results indicate that our method reduces the latency overhead significantly and enables graceful performance degradation, when compared with related partially faulty link usage proposals, and saves area and power overheads by up to 29% and 43.1%, respectively, when compared with spare wire replacement methods.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6209271,no
Quality model based on ISO/IEC 9126 for internal quality of MATLAB/Simulink/Stateflow models,"In a model-based approach, models are considered as the prime artefacts for the software specification, design and implementation. Quality assurance for program codes has been discussed a lot, however equivalent methods for model quality assessment remain rareness. Assessing quality is of particular importance for technical models (e.g. MATLAB/Simulink/Stateflow models), since they are often used for production code generation. Our main contribution is a quality model based on ISO/IEC 9126, which defines the internal model quality as well as measures for the assessments. Our quality model shall not only show improvement potentials in model, but also provide evidence about quality evolution of a model.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6209958,no
Fault detection and diagnosis of voltage source inverter using the 3D current trajectory mass center,"This paper investigates the use of the current trajectory mass center in a three dimensional referential. The proposed approach uses the inverter output currents. These currents are used to obtain a typical pattern in a three dimensional referential. According the fault type different patterns is obtained. In this way, with the proposed approach it is possible to detect and identify the faulty power switch In order to automatically identify the different patterns, it is used an algorithm to obtain a mass center of the 3D current trajectory. This results in a fast and reliable fault detection method. The applicability of the proposed technique, are confirmed through several simulation and experimental results.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6210026,no
Application of multi-fuzzy system for condition monitoring of liquid filling machines,"In this paper a novel approach is implemented for investigation of failures in Stork bottle filling machines. Fuzzy based system is used to detect the abnormalities present in machine by using time and frequency domain statistical features. Statistical analysis of vibration data determined the gearbox failure which correlated with engineer's findings. The method used has shown promising results to predict the failure in this case of low speed rotary machines. It has been concluded that statistical based analysis of vibration signal is a suitable for predicting machine faults with low rotating speeds. This paper presents a system, implemented on the industrial process machine, which has successfully predicted the faults in the gearbox before the catastrophic failure.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6210054,no
Voltage Unbalance Emission Assessment in Radial Power Systems,"Voltage unbalance (VU) emission assessment is an integral part in the VU-management process where loads are allocated a portion of the unbalance absorption capacity of the power system. The International Electrotechnical Commission Report IEC/TR 61000-3-13:2008 prescribes a VU emission allocation methodology establishing the fact that the VU can arise at the point of common connection (PCC) due to upstream network unbalance and load unbalance. Although this is the case for emission allocation, approaches for post connection emission assessment do not exist except for cases where the load is the only contributor to the VU at the PCC. Such assessment methods require separation of the post connection VU emission level into its constituent parts. In developing suitable methodologies for this purpose, the pre and postconnection data requirements need to be given due consideration to ensure that such data can be easily established. This paper presents systematic, theoretical bases which can be used to assess the individual VU emission contributions made by the upstream source, asymmetrical line, and the load for a radial power system. The methodology covers different load configurations including induction motors. Assessments obtained by employing the theoretical bases on the study system were verified by using unbalanced load-flow analysis in MATLAB and using DIgSILENT PowerFactory software.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6210412,no
Security Assessment of Code Refactoring Rules,"Refactoring is a common approach to producing better quality software. Its impact on many software quality properties, including reusability, maintainability and performance, has been studied and measured extensively. However, its impact on the information security of programs has received relatively little attention. In this work, we assess the impact of a number of the most common code-level refactoring rules on data security, using security metrics that are capable of measuring security from the point view of potential information flow. The metrics are calculated for a given Java program using a static analysis tool we have developed to automatically analyse compiled Java bytecode. We ran our Java code analyser on various programs which were refactored according to each rule. New values of the metrics for the refactored programs then confirmed that the code changes had a measurable effect on information security.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6210569,no
Achieving High Reliability on Linux for K2 System,"Driver faults are the main reasons of causing failure in operating system. In order to address this issue and improve the kernel reliability, this paper presents an intelligent kernel-mode driver enhancement mechanism - Style Box which can limit the driver's rights to access kernel by a private page table and a call control list. This method captures a variety of type errors, synchronization errors and behavior errors of the driver, and intelligently predicts and rapidly recovers driver errors. Experimental results show that Style Box can effectively detect and deal with driver errors, and obviously improve the reliability of the operating system.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6211085,no
Revenue-maximizing server selection and admission control for IPTV content servers using available bandwidth estimates,"We present a server selection and admission control algorithm for IPTV networks that uses available bandwidth estimation to assess bandwidth available on the path from an end-user point of attachment to one or more IPTV content servers and that employs a revenue maximising admission decision process that prioritizes requests for high revenue content item types over requests for lower revenue item types. The algorithm operates by estimating expected request arrival rates for different content item types based on past arrival rates and, based on these and available bandwidth estimates decides whether to accept a new request and, when accepting requests, which of the available content servers to use. Results of a simulation study show that the algorithm succeeds in 1) maintaining acceptable packet delays for accepted flows in the presence of fluctuating background traffic on network paths and 2) when available bandwidth is limited prioritizing requests for higher revenue content types.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6211914,no
Evaluating compressive sampling strategies for performance monitoring of data centers,"Performance monitoring of data centers provides vital information for dynamic resource provisioning, fault diagnosis, and capacity planning decisions. However, the very act of monitoring a system interferes with its performance, and if the information is transmitted to a monitoring station for analysis and logging, this consumes network bandwidth and disk space. This paper proposes a low-cost monitoring solution using compressive sampling - a technique that allows certain classes of signals to be recovered from the original measurements using far fewer samples than traditional approaches - and evaluates its ability to measure typical signals generated in a data-center setting using a testbed comprising the Trade6 enterprise application. The results open up the possibility of using low-cost compressive sampling techniques to detect performance bottlenecks and anomalies that manifest themselves as abrupt changes exceeding operator-defined threshold values in the underlying signals.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6211979,no
Extensive DBA-CAC mechanism for maximizing efficiency in 3GPP: LTE networks,"In this continuous fast world of mobile devices there is always a growing demand of high rate services. So a call has to be continuous with same data rates during a handoff. This paper deals with a novel approach DBA-CAC to reduce the call dropping probability while ensuring QoS demands are met in LTE wireless networks. The reduction is based on Adaptive Call Admission Control (Ad-CAC) scheme which gives priority to handoff call over the new calls. The Dynamic Bandwidth Adaptation (DBA) approach is used to maximize the overall system utilization while keeping the blocking rates low. DBA algorithm is used in two phases, when a call arrives and when a call ends. The DBA approach helps in predicting the user behavior and allocation the resources in advance, hence utilizing the resources more efficiently. This approach also maintains a low new call blocking rates.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6212673,no
Software defect prediction using Two level data pre-processing,"Defect prediction can be useful to streamline testing efforts and reduce the development cost of software. Predicting defects is usually done by using certain data mining and machine learning techniques. A prediction model is said to be effective if it is able to classify defective and non defective modules accurately. In this paper we investigate the result of data pre-processing on the performance of four different K-NN classifiers and compare the results with random forest classifier. The method used for pre-processing includes attribute selection and instance filtering. We observed that Two-level data pre-processing enhances defect prediction results. We also report how these two filters influence the performance independently. The observed performance improvement can be attributed to the removal of irrelevant attributes by dimension (attribute) reduction and of class imbalance problem by Resampling, together leading to the improved performance capabilities of the classifiers.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6212686,no
Analytical model for channel allocation scheme in macro/femto-cell based BWA networks,"The femtocellular technology is observed to be quite promising for mobile operators as it improves their network coverage and capacity at the outskirts of the macro cell. In this paper, we have developed an analytical model for channel allocation in macro/femto-cell based BWA (Broadband Wireless Access) networks using Continuous Time Markov Chain (CTMC). The focus of the work is to analyze various QoS parameters like connection blocking probability, system capacity enhancement and channel utilization of the network for performance evaluation. We have considered a hierarchical WiMAX BWA network consisting of single macro BS along with `m' femto BS and a total number of `ch' orthogonal channels are assumed to be available in the network. The macro BS will receive the channel requests from the users either directly or via femto BS. Four types of services i.e. UGS, rtPS, nrtPS and BE request for a channel to be admitted. Pareto distribution is considered for the arrival process of the newly originated service type. The hierarchical WiMAX network is analytically modeled in the form of a `6+m' dimensional Markov Chain based on the number of admitted services of each type under macro BS and `m' femto BSs. Extensive analysis has been performed to evaluate the effectiveness and efficiency of the hierarchical WiMAX networks along with the concept of channel reuse. As per the analysis, the connection blocking probability is observed to fall drastically from about 0.8 to about 0.02 for up to 10 femto cells in the network with channel reuse. On the other hand, the system capacity and channel utilization is observed to improve acutely with the introduction of femto cells with channel reuse. Enhancement in the system capacity is observed to be up to 120% and channel utilization increases from 95% to 220% with the introduction of up to 10 femto cells in the network with channel reuse. Thus, our developed analytical model exhibits that the system performance is enhanced with th- introduction of the femto cells and the concept of channel reuse in the hierarchical WiMAX networks. The contribution of the work lies within the scope of developing an analytical model for channel allocation using CTMC to evaluate the performance of the hierarchical WiMAX networks. However, this analytical model would be equally applicable to any femto-cellular based BWA networks.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6212717,no
Adaptive Quality of Service in ad hoc wireless networks,"In high criticality crisis scenarios, such as disaster management, ad hoc wireless networks are quickly assembled in the field to support decision makers through situational awareness using messaging-, voice-, and video-based applications. These applications cannot afford the luxury of stalling or failing due to overwhelming bandwidth demand on these networks as this could contribute to overall mission failure. This paper describes an approach for satisfying application-specific Quality of Service (QoS) expectations operating on ad hoc wireless networks where available bandwidth fluctuates. The proposed algorithm, D-Q-RAM (Distributed QoS Resource Allocation Model) incorporates a distributed optimization heuristic that results in near optimal adaptation without the need to know, estimate, or predict available bandwidth at any moment in time.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6214067,no
A Mirrored Data Structures Approach to Diverse Partial Memory Replication,"Software memory errors are a growing threat to software dependability. In previous work, we proposed an approach for detecting memory errors, called Diverse Partial Memory Replication (DPMR), that utilized automated program diversity and memory replication. The original design aimed to maximize coverage by making the pointers stored in different memory replicas comparable. In this paper, we propose and evaluate an alternative design called Mirrored Data Structures (MDS), which sacrifices pointer comparability to gain three primary benefits. 1) MDS significantly increases DPMR's applicability by eliminating all DPMR restrictions on memory allocation, pointer arithmetic, and pointer-to-pointer casts. 2) For programs that store many pointers to memory, MDS reduces DPMR's overhead, as is demonstrated in experimental results. 3) MDS significantly reduces DPMR's memory footprint.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6214761,no
The Provenance of WINE,"The results of cyber security experiments are often impossible to reproduce, owing to the lack of adequate descriptions of the data collection and experimental processes. Such provenance information is difficult to record consistently when collecting data from distributed sensors and when sharing raw data among research groups with variable standards for documenting the steps that produce the final experimental result. In the WINE benchmark, which provides field data for cyber security experiments, we aim to make the experimental process self-documenting. The data collected includes provenance information -- such as when, where and how an attack was first observed or detected -- and allows researchers to gauge information quality. Experiments are conducted on a common test bed, which provides tools for recording each procedural step. The ability to understand the provenance of research results enables rigorous cyber security experiments, conducted at scale.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6214767,no
Experimental Analysis of Binary-Level Software Fault Injection in Complex Software,"The injection of software faults (i.e., bugs) by mutating the binary executable code of a program enables the experimental dependability evaluation of systems for which the source code is not available. This approach requires that programming constructs used in the source code should be identified by looking only at the binary code, since the injection is performed at this level. Unfortunately, it is a difficult task to inject faults in the binary code that correctly emulate software defects in the source code. The accuracy of binary-level software fault injection techniques is therefore a major concern for their adoption in real-world scenarios. In this work, we propose a method for assessing the accuracy of binary-level fault injection, and provide an extensive experimental evaluation of a binary-level technique, G-SWFIT, in order to assess its limitations in a real-world complex software system. We injected more than 12 thousand binary-level faults in the OS and application code of the system, and we compared them with faults injected in the source code by using the same fault types of G-SWFIT. The method was effective at highlighting the pitfalls that can occur in the implementation of G-SWFIT. Our analysis shows that G-SWFIT can achieve an improved degree of accuracy if these pitfalls are avoided.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6214771,no
Changeloads for Resilience Benchmarking of Self-Adaptive Systems: A Risk-Based Approach,"Benchmarking self-adaptive software systems calls for a new model that takes into account a distinctive characteristic of such systems: alterations over time (i.e., self-achieved modifications or adjustments triggered by changes in the external or internal contexts of the system). Changes are thus a fundamental component of a resilience benchmark, raising an intrinsic research problem: how to identify and select the most realistic and relevant (sequences of) changes to be included in the benchmarking procedure. The problem is that defining a representative change load would require access to a large amount of field data, which is not available for most systems. In this paper we propose an approach based on risk analysis to tackle this key issue, debating its effectiveness and usability with a simple case study. The procedure, that combines field data with expert knowledge and experimental data, allows moving from the identification of the generic goals of systems in the benchmarking domain to the identification of the most relevant change scenarios (based on probability and impact) that may prevent those systems from achieving their goals.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6214772,no
Incipient fault detection of industrial pilot plant machinery via acoustic emission,Numerous condition monitoring techniques and identification algorithms for detection and diagnosis of faults in industrial plants have been proposed for the past few years. Motors are one of the common used elements in almost all plant machinery. They cause the machine failure upon getting faulty. Therefore advance and effective condition monitoring techniques are required to monitor and detect the motor problems at incipient stages. This avoids catastrophic machine failure and costly unplanned shutdown. In this paper the acoustic emission (AE) monitoring system is established. It discusses a method based on time and frequency domain analysis of AE signals acquired from motors used in chemical process pilot plant. A real time measurement system is developed. It utilizes MatLAB to process and analyze the data to provide valuable information regarding the process being monitored.,2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6215196,no
MATLAB based defect detection and classification of printed circuit board,"A variety of ways has been established to detect defects found on printed circuit boards (PCB). In previous studies, defects are categories into seven groups with a minimum of one defect and up to a maximum of 4 defects in each group. Using Matlab image processing tools this research separates two of the existing groups containing two defects each into four new groups containing one defect each by processing synthetic images of bare through-hole single layer PCBs.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6215366,no
Software quality in use characteristic mining from customer reviews,"Reviews from customers who have experience with the software product are an important information decision making for software product acquisition. They usually appear on ecommerce websites or any online download market. If some products have a large number of reviews, customer may not have time to read all of them. Therefore, we need to extract software information characteristic from reviews in order to provide product review representation. Customer can further use it to compare one software product attributes and other products' attributes. Software product quality from user point of view may be used to characterize each software product. ISO 9126 is widely used among software engineer to assess software quality in use. It covers software quality model and contains the quality model characteristic from user perspective: effectiveness, productivity, safety and satisfaction. We propose a methodology for software product reviews mining based on software quality ontology constructed from ISO 9126 and a rule-based classification to finally produce software quality in use scores for software product Representation. The quality in use score for each software characteristic can be used to preliminary determine the quality of the software.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6215397,no
An adaptive and Efficient Data Delivery Scheme for DFT-MSNs (delay and disruption tolerant Mobile Sensor Networks),"Delay and disruption tolerant networking (DTN) architecture has been developed for networks operating under extreme conditions. DTN architecture is built using a set of new protocol family including Bundle Protocol and Licklider Transmission Protocol which relax almost all the assumptions for the existence of a network, like presence of end-to-end connectivity among communicating nodes, low propagation delays etc and allow the communication to take place. Routing is one of the biggest challenges in DTNs. A number of routing schemes have been proposed but all such schemes are either history based and deterministic or application specific and thus do not offer a stable solution for the routing problem for DTNs. Direct transmission scheme involves single hop forwarding which results in poor delivery efficiency. Epidemic routing is a flooding based approach resulting in high delivery efficiency at the cost of high network congestion which eventually reduces the network performance. Probabilistic routing is a moderately efficient scheme in terms of delivery efficiency and usage of network resources. In this paper, we propose an adaptive and efficient Bundle Fault Tolerance based Probabilistic Routing (BFPR) scheme, which offers significantly improved performance over existing DTN routing schemes. Simulations and results analysis have been carried out using MATLAB.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6215580,no
Event driven test case selection for regression testing web applications,"Traditional testing techniques are not apt for the multifaceted web-based applications, since they miss the additional features of web applications such as their multi-tier nature, hyperlink-based structure, and event-driven feature. As software systems evolve, errors sometimes sneak in; software that has been tested on certain inputs may fail to work on those same inputs in the future. Regression testing aims to detect these errors by comparing present behavior with past behavior. Although regression testing has been widely used to gain confidence in the reliability of software by providing information about the quality of an application, it has suffered limited use in this domain due to the frequent nature of updates to websites and the difficulty of automatically comparing test case output. In this paper we propose a new paradigm that exploits regression testing to be used by web applications. This event-driven technique is based on the creation of event-dependency graph of the original and modified web application, then converting the original and modified web application graph into event test tree, followed by the comparison of both trees to identify affected and potentially affected nodes which enables selection of test cases for regression testing web applications finally reducing the test set size. We apply this technique to a case study to demonstrate the usefulness of the proposed paradigm.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6215584,no
Probabilistic duration of power estimation for Nickel- metal- hydride (NiMH) battery under constant load using Kalman filter on chip,"For a battery powered safety critical system the safe duration of power for executing a specific task is extremely important. It is necessary to avoid unacceptable consequences due to unwanted battery power failure. An early stage estimation of this duration reduces the overall risk through optimization of current consumption by switching off noncritical load ahead of delivery of power to a critical load. In order to address this issue, an online battery state of charge estimator on chip is conceived and implemented using Kalman filter. The Kalman filter estimates the true values of measurements by predicting a value, considering the estimated uncertainty of the predicted value, and then computing a weighted average of the predicted value and the measured value. The basic idea is more accurate state prediction is possible when the state predicted value is fused with sensor prediction under any uncertain disturbance. The state estimator is developed in the form of an algorithm and stored into a single chip microcontroller. It is finally used to generate an early stage warning signal against battery failure. The paper presents a methodology for creating energy aware system that would avoid sudden system failure due to power outage. The authors used a generalized state space model of the battery to estimate the effect of unobserved battery parameters for duration estimation. An experiment was conducted in this regard through discharging the battery under constant load. Subsequently the internal parameters of battery were calculated. The model was simulated through MATLAB/simulink R2008a software and efficiency was tested. The program for prediction was finally emulated in a microcontroller and found satisfactory result.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6216075,no
A new algorithm for voltage sag detection,"Voltage sags is a common power system disturbance, usually associated with power system faults. Therefore the effective detection of voltage sag event is an important issue for voltage sag analysis and mitigation. There are several detection methods for voltage sags such as RMS voltage detection, peak voltage detection, and Fourier transform methods. The problem with these methods is that they use a windowing technique and can therefore be too slow when applied to detect voltages sags for mitigation since they use historical value, not instantaneous value which may lead to long detection time when voltage sag has occurred. This paper presents a new algorithm for voltage sag detection. The algorithm can extract a single non-stationary sinusoidal signal out of a given multi-component input signal. The algorithm is capable of estimating the amplitude, phase and frequency of an input signal in real-time. It is compared to other methods of sag detection.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6216106,no
Parallelization and performance optimization on face detection algorithm with OpenCL: A case study,"Face detect application has a real time need in nature. Although Viola-Jones algorithm can handle it elegantly, today's bigger and bigger high quality images and videos still bring in the new challenge of real time needs. It is a good idea to parallel the Viola-Jones algorithm with OpenCL to achieve high performance across both AMD and NVidia GPU platforms without bringing up new algorithms. This paper presents the bottleneck of this application and discusses how to optimize the face detection step by step from a very naive implementation. Some brilliant tricks and methods like CPU execution time hidden, stubbles usage of local memory as high speed scratchpad and manual cache, and variable granularity were used to improve the performance. Those technologies result in 413 times speedup varying with the image size. Furthermore, those ideas may throw on some light on the way to parallel applications efficiently with OpenCL. Taking face detection as an example, this paper also summarizes some universal advice on how to optimize OpenCL program, trying to help other applications do better on GPU.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6216758,no
An Autonomous Reliability-Aware Negotiation Strategy for Cloud Computing Environments,"Cloud computing paradigm allows subscription-based access to computing and storages services over the Internet. Since with advances of Cloud technology, operations such as discovery, scaling, and monitoring are accomplished automatically, negotiation between Cloud service requesters and providers can be a bottleneck if it is carried out by humans. Therefore, our objective is to offer a state-of-the-art solution to automate the negotiation process in Cloud environments. In previous works in the SLA negotiation area, requesters trust whatever QoS criteria values providers offer in the process of negotiation. However, the proposed negotiation strategy for requesters in this work is capable of assessing reliability of offers received from Cloud providers. In addition, our proposed negotiation strategy for Cloud providers considers utilization of resources when it generates new offers during negotiation and concedes more on the price of less utilized resources. The experimental results show that our strategy helps Cloud providers to increase their profits when they are participating in parallel negotiation with multiple requesters.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6217433,no
Self-Healing of Operational Workflow Incidents on Distributed Computing Infrastructures,"Distributed computing infrastructures are commonly used through scientific gateways, but operating these gateways requires important human intervention to handle operational incidents. This paper presents a self-healing process that quantifies incident degrees of workflow activities from metrics measuring long-tail effect, application efficiency, data transfer issues, and site-specific problems. These metrics are simple enough to be computed online and they make little assumptions on the application or resource characteristics. Incidents are classified in levels and associated to sets of healing actions that are selected based on association rules modeling correlations between incident levels. The healing process is parametrized on real application traces acquired in production on the European Grid Infrastructure. Implementation and experimental results obtained in the Virtual Imaging Platform show that the proposed method speeds up execution up to a factor of 4 and properly detects unrecoverable errors.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6217437,no
Scalable Join Queries in Cloud Data Stores,"Cloud data stores provide scalability and high availability properties for Web applications, but do not support complex queries such as joins. Web application developers must therefore design their programs according to the peculiarities of No SQL data stores rather than established software engineering practice. This results in complex and error-prone code, especially with respect to subtle issues such as data consistency under concurrent read/write queries. We present join query support in Cloud TPS, a middleware layer which stands between a Web application and its data store. The system enforces strong data consistency and scales linearly under a demanding workload composed of join queries and read-write transactions. In large-scale deployments, Cloud TPS outperforms replicated Postgre SQL up to three times.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6217465,no
Automated Tagging for the Retrieval of Software Resources in Grid and Cloud Infrastructures,"A key challenge for Grid and Cloud infrastructures is to make their services easily accessible and attractive to end-users. In this paper we introduce tagging capabilities to the Miner soft system, a powerful tool for software search and discovery in order to help end-users locate application software suitable to their needs. Miner soft is now able to predict and automatically assign tags to software resources it indexes. In order to achieve this, we model the problem of tag prediction as a multi-label classification problem. Using data extracted from production-quality Grid and Cloud computing infrastructures, we evaluate an important number of multi-label classifiers and discuss which one and with what settings is the most appropriate for use in the particular problem.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6217475,no
Data Outsourcing Simplified: Generating Data Connectors from Confidentiality and Access Policies,"For cloud-based outsourcing of confidential data, various techniques based on cryptography or data-fragmentation have been proposed, each with its own tradeoff between confidentiality, performance, and the set of supported queries. However, it is complex and error-prone to select appropriate techniques to individual scenarios manually. In this paper, we present a policy-based approach consisting of a domain specific language and a policy-transformator to automatically generate scenario-specific software adapters called mediators that set up data outsourcing and govern data access. Mediators combine state-of-the-art confidentiality techniques to ensure a user-specified level of confidentiality while still offering efficient data access. Thus, our approach simplifies data outsourcing by decoupling policy decisions from their technical implementation and realizes appropriate tradeoffs between confidentiality and efficiency.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6217534,no
Time-Domain Analysis of Differential Power Signal to Detect Magnetizing Inrush in Power Transformers,"In this paper, a novel power-based algorithm to discriminate between switching and internal fault conditions in power transformers is proposed and evaluated. First, the differential power signal is scrutinized and its intrinsic features during inrush conditions are introduced. Afterwards, a combined time-domain-based waveshape classification technique is proposed. This technique exploits the suggested features and provides two discriminative indices. Based on the values of these indices, inrush power signals are identified after only half a cycle. This method is founded upon some inherent low-frequency features of power waveforms and is independent of the magnitude of differential power. The approach is also unaffected by power system parameters, operating conditions, noise and transformer magnetizing curves. Simplicity of the suggested features and equations describe how the proposed method can help make it a practical solution for the inrush problem. Extensive simulations carried out in PSCAD/EMTDC software validate the merit of this technique for various conditions, such as current-transformer saturation. Furthermore, real-time testing of the proposed method using real fault and inrush signals confirms the possibility of implementing this algorithm for industrial applications.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6218211,no
An adaptive self-test routine for in-field diagnosis of permanent faults in simple RISC cores,"The localization of permanent faults in a processor is a precondition for applying (self-)repair functions to that processor core. This paper presents a software-based self-test technique that can be used in the field for test and fault localization, there-by providing a high diagnostic resolution. It is shown how the self-test routine is adapted in the field to already detected faults in the processor, such that these faults do not affect the test- and diagnostic capability of the self-test routine. By this it becomes reasonable to localize multiple permanent faults in the processor. The proposed self-test is software-based, but it requires a few modifications of the processor. The feasibility of the technique is presented by an example; limitations are discussed, too.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6219080,no
Study the impact of improving source code on software metrics,"The process of improving the quality of the software products is a continuous process where software developers learn from their previous experience and from previous software releases to improve the future products or release. In this paper, we evaluate the ability of Software source code analysis process and tools to predict possible defects, errors or problem in the software products. More specifically, we evaluate the effect of improving the code according to recommendations from source code tools on software metrics. Several open source software projects are selected for the case study. The output of applying source code analysis tools on those projects result in several types of warning. After performing manual correction of those warning, we compare the metrics of the evaluated projects before and after applying the corrections. Results showed that the size and structural complexity in most cases are increases. On the other hand, some of the complexities related to coupling and maintainability are decreases.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6220379,no
Design of cyber-physical interface for automated vital signs reading in electronic medical records systems,The focus of this project is to study the design of a cyber-physical interface for automated vital sign readings in Electronic Medical Record Systems. This is presented as a solution for a need in actual EMR systems where the reading of vital signs is done manually which is error prone and time consuming. The domain application knowledge and prototype used for the development of this paper is made possible with the collaboration of the Alliance of Chicago Community Health Center LLC.,2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6220696,no
Towards single-chip diversity TMR for automotive applications,"The continuous requirement to provide safe, low-cost, compact systems makes applications such as automotive more prone to increasing types of faults. This may result in increased system failure rates if not addressed correctly. While some of the faults are not permanent in nature, they can lead to malfunctioning in complex circuits and/or software systems. Moreover, automotive applications have recently adopted the ISO26262 to provide a standard for defining functional safety. One of the recommended schemes to tolerate faults is Triple Modular Redundancy (TMR). However, traditional TMR designs typically consume too much space, power, and money all of which are undesirable for automotive. In addition, common mode faults have always been a concern in TMR which their effects would be increasing in compact systems. Errors such as noise and offset that impact a TMR sensor input can potentially cause common mode failures that lead to an entire system failure. In this paper, we introduce a new architecture and implementation for diverse TMR in a speed measurement system that would serve automotive cost and safety demands. Diversity TMR is achieved on a single chip by designing functionally identical circuits each in a different design domain to reduce the potential of common mode failures. Three versions of a speed sensing application are implemented on a mixed-signal Programmable System on Chip (PSoC) from Cypress Semiconductors. We introduce errors that impact speed sensor signals as defined by the ISO26262 standard to evaluate DTMR. Our testing shows how DTMR can be effective to different types of errors that impact speed sensor signals.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6220736,no
Voltage sag calculation based on Monte Carlo technique,"Fault occurrence in power system not only affects on reliability of system but also stability too. In many cases, fault occurrence in power system leads to voltage depressions -dips or sags- that damages to power system components. In this paper, using stochastic technique in appropriate modeled power system simulated in PSCAD/EMTDC software, maximum voltage sag will be calculated based on the Monte Carlo technique. The results show that the presented technique has excellent ability in order to detect and estimate the amplitude of voltage sags in different conditions.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6221458,no
High speed adaptive auto reclosing of 765 kV transmission lines,"This paper proposes a new adaptive auto reclosing scheme for extra high voltage transmission lines. The performance of an auto reclosing scheme depends not only on power system behavior and transmission lines characteristic but also secondary arc current. In this paper at first, an appropriate model of arc in extra high voltage transmission overhead line is investigated and after that, using symmetrical components a novel criterion based on apparent power is presented in order to detect the exact time of arc extinguishing. In fact this criterion has the ability to distinguish between transient faults and permanent faults by detecting the exact time of arc extinguishing. Also, a generalized scheme is presented in order to have high speed auto reclosing in 765 kV transmission lines. This method is very simple and only uses two filters, so it needs to low hardware requirements. Therefore, application of this method is highly recommended in extra high voltage transmission lines. The simulation studies of this paper are performed by PSCAD/EMTDC software and the results show satisfactory performance of the proposed method.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6221468,no
A new fuzzy fault locator for series compensated transmission lines,"Series capacitors (SCs) are installed on long transmission lines to reduce the inductive reactance of the lines. SCs and their associated over-voltage protection devices (typically metal oxide varistors, and/or air gaps) create several problems for distance protection relays and fault locators including voltage and/or current inversion, sub-harmonic oscillations, transients caused by the air-gap flashover and sudden changes in the operating reach. In this paper, using fuzzy logic a simple and accurate fault location algorithm is presented for series compensated transmission lines. The fault region is determined by a fuzzy identifier, firstly. Then the distance to fault is calculated from the fault loop quantities similar to the classic fault locators, but in case of voltages additionally the compensation for the voltage drop across the bank (or banks) of series capacitors is performed. The power system is simulated on a PC using PSCAD software to provide fault data. The fuzzy fault region identifier and fault locator are designed and implemented using MATLAB software. The operating behaviour of the fault locator was assessed using a 400 kV, 400km double end fed simulated transmission line with three phase faults at various locations on the line. It relies totally on locally derived information.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6221511,no
A new approach to high impedance fault location in three-phase underground distribution system using combination of fuzzy logic & wavelet analysis,"This paper presents the results of investigation into a new fault classification and location, in the EMTP software. The simulated data is then analyzed using advanced signal processing technique based on wavelet analysis to extract useful information from signals and this is then applied to the fuzzy logic system (FLS) for detect of the type and location ground high impedance faults in a practical underground radial distribution system. The paper concludes by comprehensively evaluation the performance of the technique developed in the case of ground high impedance faults. The results indicate that the fault location technique has an acceptable accuracy under a whole variety of different systems and fault conditions.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6221551,no
The detection of VFC and STATCOM faults in Doubly Fed Induction Generator,"The wind power has the most rapid growth in comparison with other renewable energies. During last decade wind energy has became an important part of the electricity production throughout the world. As the amount of wind energy increase, the reliability of the wind turbines becomes crucial. A low reliability would result in an unstable energy source with poor economical performance. Monitoring the condition of vital components is a key element to keep a high reliability. Condition monitoring of Doubly Fed Induction Generators (DFIG) is growing in importance for wind turbines. This paper investigates the effect of VFC (Variable Frequency Converter) and STATCOM Faults on wind turbine equipped with DFIG operation in order to condition monitoring of wind turbine. Consequently, a proposed method is used to detecting these faults means of harmonic components analyzing of DFIG rotor current. The simulation has been done with PSCAD/EMTDC software.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6221565,no
A reputation model based on hierarchical bayesian estimation for Web services,"The motivation of Web service comes from its interoperational ability so a large number of Web services can interact with others and constitute an open network, Web service network. The success of Web services selection rely on, not only its Qos capability advertised, but the trustworthy of QoS to large degree. How to evaluate the trustworthy of services QoS information, however, is a challenge in Web service network. Reputation system, a mechanism which assesses the future QoS performance by the past behavior of service, is one of promising approaches to facilitate users make optimal decision. In this paper, we present a hybrid framework of reputation model for Web service. Based on this hybrid architecture, clients build their specific social communities, by which they obtain service's prior reputation. At the same time, the central reputation system fuses the rating data from clients by bayesian estimation. The result of experiments illustrated our approach is more efficiency and accuracy in several aspects, especially when dealing with strategics services.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6221802,no
Sequence-based interaction testing implementation using Bees Algorithm,"T-way strategies is used to generate test data to detect faults due to interaction. In the literature, there are many t-way strategies developed by researchers for the past 10 years. However, most of the strategies assumed sequence-less parameter interaction. In the real world, there are many systems that consider the sequence of the input parameter in order to produce correct output. These interactions of the sequence of inputs need to be tested to avoid fault due to sequence interaction. In this paper we present a sequence-based interaction testing strategy (termed as sequence covering array) using Bees Algorithm. We discuss the implementation, present and compare the results with existing sequence covering array algorithm.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6222671,no
A proposal for enhancing user-developer communication in large IT projects,"A review of the literature showed that the probability of system success, i.e. user acceptance, system quality and system usage, can be increased by user-developer communication. So far most research on user participation focuses either on early or on late development phases. Especially large IT projects require increased participation, due to their high complexity. We believe that the step in software development when user requirements are translated (and thus interpreted) by developers into a technical specification (i.e. system requirements, architecture and models) is a critical one for user participation. In this step a lot of implicit decisions are taken, some of which should be communicated to the end users. Therefore, we want to create a method that enhances communication between users and developers during that step. We identified trigger points (i.e. changes on initial user requirements), and the granularity level on which to communicate with the end users. Also, representations of changes and adequate means of communication are discussed.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6223014,no
The new DC system insulation monitoring device based on phase differences of magnetic modulation,"According to the exiting issues of currently DC system insulation monitoring device, this paper developed a more comprehensive function monitoring device. The device adopted phase different of magnetic modulation detection principle. Though access to the unbalanced grounding relay, it can solve the conventional devices which unable to detect issue like: the DC system grounding simultaneity or equivalent insulation deterioration, with this method the device can determine a variety of ground failure correctly, and it also can predict the insulation decline. Because the 500kV transfer substation exiting serious electromagnetic interference which can make the data of insulation monitoring device inaccurate and work unstable. According to these phenomenon, we use a variety of anti-jamming measures on both hardware and software. This paper describes the overall hardware of the device composition and a detailed description of key technologies. Field experimental results show that the device may change from time to time to monitor the insulation resistance and have the ability to find out the grounding branches accurately; also it has no effects on disturbed capacitance. Currently, the device has been putting into use and working well.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6223065,no
"A cross layer, adaptive data aggregation algorithm utilizing spatial and temporal correlation for fault tolerant Wireless Sensor Networks","Wireless Sensor Networks (WSNs) are ad hoc networks formed by tiny, low powered, and low cost devices. WSNs take advantage of distributed sensing capability of the sensor nodes such that several sensors can be used collaboratively to detect events or perform monitoring of specific environmental attributes. Since sensor nodes are often exposed to harsh environmental elements, and normally operate in an unsupervised fashion over long periods of time, within their MTBF, some of them are subject to partial failure in form of A/D readings that are permanently off the correct levels. Additionally, due to glitches in timing and in hardware or software, even healthy sensor nodes can occasionally report readings that are outside of the expected range. In this paper we present a novel approach that combines spatial and temporal correlation of the data collected by neighboring sensors to combat both error modes described above. We combine the weighted averaging algorithm across multiple sensors, with the LMS adaptive filtering of individual sensor data, in order to improve fault tolerance of WSNs. We present performance gains achieved by combining these methods; and analyze the computational and memory costs of these algorithms.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6223204,no
A phase space method to assess and improve autocorrelation and RFM autocorrelation performances of chaotic sequences,"Some chaotic sequences have poor autocorrelation performance or modulated autocorrelation performance, but previously we didn't know the rule and couldn't improve their performances effectively. Using recently presented Autocorrelation and modulated Autocorrelation theorems based on a Phase Space method, we can find and mend the structure defects of chaotic sequences and improve their autocorrelation or modulated autocorrelation performance. Using well known Bernoulli and Skew Tent sequences as examples, we have assessed and improved their autocorrelation and RFM autocorrelation performances through the phase space method to validate that the method is simple yet effective.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6223339,no
The scheme design of distributed systems service fault management based on active probing,"Service fault management in distributed computer systems and networks is a difficult task that requires high efficient inferences from mass data. In this paper, we propose a corresponding solution. Firstly, challenges of distributed systems service fault management are analyzed, and a multilayer model is recommended. Then, a dependency matrix to represent the causal relationship between faults and probes is defined and the framework of fault management is built. After these, a service fault management scheme using active probing is proposed. This scheme is composed of two phases: fault detection and fault localization. In first phase, we propose a probe selection algorithm, which selects a minimal set of probes while remaining a high probability of fault detection. In second phase, we propose a fault localization probe selection algorithm, which selects probes to obtain more system information based on the symptoms observed in previous phase. Finally, the instance proves the validity and efficiency of our scheme.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6223356,no
Why do software packages conflict?,"Determining whether two or more packages cannot be installed together is an important issue in the quality assurance process of package-based distributions. Unfortunately, the sheer number of different configurations to test makes this task particularly challenging, and hundreds of such incompatibilities go undetected by the normal testing and distribution process until they are later reported by a user as bugs that we call conflict defects. We performed an extensive case study of conflict defects extracted from the bug tracking systems of Debian and Red Hat. According to our results, conflict defects can be grouped into five main categories. We show that with more detailed package meta-data, about 30 % of all conflict defects could be prevented relatively easily, while another 30 % could be found by targeted testing of packages that share common resources or characteristics. These results allow us to make precise suggestions on how to prevent and detect conflict defects in the future.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6224274,no
Do faster releases improve software quality? An empirical case study of Mozilla Firefox,"Nowadays, many software companies are shifting from the traditional 18-month release cycle to shorter release cycles. For example, Google Chrome and Mozilla Firefox release new versions every 6 weeks. These shorter release cycles reduce the users' waiting time for a new release and offer better marketing opportunities to companies, but it is unclear if the quality of the software product improves as well, since shorter release cycles result in shorter testing periods. In this paper, we empirically study the development process of Mozilla Firefox in 2010 and 2011, a period during which the project transitioned to a shorter release cycle. We compare crash rates, median uptime, and the proportion of post-release bugs of the versions that had a shorter release cycle with those having a traditional release cycle, to assess the relation between release cycle length and the software quality observed by the end user. We found that (1) with shorter release cycles, users do not experience significantly more post-release bugs and (2) bugs are fixed faster, yet (3) users experience these bugs earlier during software execution (the program crashes earlier).",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6224279,no
Explaining software defects using topic models,"Researchers have proposed various metrics based on measurable aspects of the source code entities (e.g., methods, classes, files, or modules) and the social structure of a software project in an effort to explain the relationships between software development and software defects. However, these metrics largely ignore the actual functionality, i.e., the conceptual concerns, of a software system, which are the main technical concepts that reflect the business logic or domain of the system. For instance, while lines of code may be a good general measure for defects, a large entity responsible for simple I/O tasks is likely to have fewer defects than a small entity responsible for complicated compiler implementation details. In this paper, we study the effect of conceptual concerns on code quality. We use a statistical topic modeling technique to approximate software concerns as topics; we then propose various metrics on these topics to help explain the defect-proneness (i.e., quality) of the entities. Paramount to our proposed metrics is that they take into account the defect history of each topic. Case studies on multiple versions of Mozilla Firefox, Eclipse, and Mylyn show that (i) some topics are much more defect-prone than others, (ii) defect-prone topics tend to remain so over time, and (iii) defect-prone topics provide additional explanatory power for code quality over existing structural and historical metrics.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6224280,no
Co-evolution of logical couplings and commits for defect estimation,"Logical couplings between files in the commit history of a software repository are instances of files being changed together. The evolution of couplings over commits' history has been used for the localization and prediction of software defects in software reliability. Couplings have been represented in class graphs and change histories on the class-level have been used to identify defective modules. Our new approach inverts this perspective and constructs graphs of ordered commits coupled by common changed classes. These graphs, thus, represent the co-evolution of commits, structured by the change patterns among classes. We believe that co-evolutionary graphs are a promising new instrument for detecting defective software structures. As a first result, we have been able to correlate the history of logical couplings to the history of defects for every commit in the graph and to identify sub-structures of bug-fixing commits over sub-structures of normal commits.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6224283,no
Can we predict types of code changes? An empirical analysis,"There exist many approaches that help in pointing developers to the change-prone parts of a software system. Although beneficial, they mostly fall short in providing details of these changes. Fine-grained source code changes (SCC) capture such detailed code changes and their semantics on the statement level. These SCC can be condition changes, interface modifications, inserts or deletions of methods and attributes, or other kinds of statement changes. In this paper, we explore prediction models for whether a source file will be affected by a certain type of SCC. These predictions are computed on the static source code dependency graph and use social network centrality measures and object-oriented metrics. For that, we use change data of the Eclipse platform and the Azureus 3 project. The results show that Neural Network models can predict categories of SCC types. Furthermore, our models can output a list of the potentially change-prone files ranked according to their change-proneness, overall and per change type category.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6224284,no
Who? Where? What? Examining distributed development in two large open source projects,"To date, a large body of knowledge has been built up around understanding open source software development. However, there is limited research on examining levels of geographic and organizational distribution within open source software projects, despite many studies examining these same aspects in commercial contexts. We set out to fill this gap in OSS knowledge by manually collecting data for two large, mature, successful projects in an effort to assess how distributed they are, both geographically and organizationally. Both Firefox and Eclipse have been the subject of many studies and are ubiquitous in the areas of software development and internet usage respectively. We identified the top contributors that made 95% of the changes over multiple major releases of Firefox and Eclipse and determined their geographic locations and organizational affiliations. We examine the distribution in each project's constituent subsystems and report the relationship of pre- and post-release defects with distribution levels.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6224286,no
Developing an h-index for OSS developers,"The public data available in Open Source Software (OSS) repositories has been used for many practical reasons: detecting community structures; identifying key roles among developers; understanding software quality; predicting the arousal of bugs in large OSS systems, and so on; but also to formulate and validate new metrics and proof-of-concepts on general, non-OSS specific, software engineering aspects. One of the results that has not emerged yet from the analysis of OSS repositories is how to help the career advancement of developers: given the available data on products and processes used in OSS development, it should be possible to produce measurements to identify and describe a developer, that could be used externally as a measure of recognition and experience. This paper builds on top of the h-index, used in academic contexts, and which is used to determine the recognition of a researcher among her peers. By creating similar indices for OSS (or any) developers, this work could help defining a baseline for measuring and comparing the contributions of OSS developers in an objective, open and reproducible way.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6224288,no
Incorporating version histories in Information Retrieval based bug localization,"Fast and accurate localization of software defects continues to be a difficult problem since defects can emanate from a large variety of sources and can often be intricate in nature. In this paper, we show how version histories of a software project can be used to estimate a prior probability distribution for defect proneness associated with the files in a given version of the project. Subsequently, these priors are used in an IR (Information Retrieval) framework to determine the posterior probability of a file being the cause of a bug. We first present two models to estimate the priors, one from the defect histories and the other from the modification histories, with both types of histories as stored in the versioning tools. Referring to these as the base models, we then extend them by incorporating a temporal decay into the estimation of the priors. We show that by just including the base models, the mean average precision (MAP) for bug localization improves by as much as 30%. And when we also factor in the time decay in the estimates of the priors, the improvements in MAP can be as large as 80%.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6224299,no
"Think locally, act globally: Improving defect and effort prediction models","Much research energy in software engineering is focused on the creation of effort and defect prediction models. Such models are important means for practitioners to judge their current project situation, optimize the allocation of their resources, and make informed future decisions. However, software engineering data contains a large amount of variability. Recent research demonstrates that such variability leads to poor fits of machine learning models to the underlying data, and suggests splitting datasets into more fine-grained subsets with similar properties. In this paper, we present a comparison of three different approaches for creating statistical regression models to model and predict software defects and development effort. Global models are trained on the whole dataset. In contrast, local models are trained on subsets of the dataset. Last, we build a global model that takes into account local characteristics of the data. We evaluate the performance of these three approaches in a case study on two defect and two effort datasets. We find that for both types of data, local models show a significantly increased fit to the data compared to global models. The substantial improvements in both relative and absolute prediction errors demonstrate that this increased goodness of fit is valuable in practice. Finally, our experiments suggest that trends obtained from global models are too general for practical recommendations. At the same time, local models provide a multitude of trends which are only valid for specific subsets of the data. Instead, we advocate the use of trends obtained from global models that take into account local characteristics, as they combine the best of both worlds.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6224300,no
Characterizing verification of bug fixes in two open source IDEs,"Data from bug repositories have been used to enable inquiries about software product and process quality. Unfortunately, such repositories often contain inaccurate, inconsistent, or missing data, which can originate misleading results. In this paper, we investigate how well data from bug repositories support the discovery of details about the software verification process in two open source projects, Eclipse and NetBeans. We have been able do identify quality assurance teams in NetBeans and to detect a well-defined verification phase in Eclipse. A major challenge, however, was to identify the verification techniques used in the projects. Moreover, we found cases in which a large batch of bug fixes is simultaneously reported to be verified, although no software verification was actually done. Such mass verifications, if not acknowledged, threatens analyses that rely on information about software verification reported on bug repositories. Therefore, we recommend that the exploratory analyses presented in this paper precede inferences based on reported verifications.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6224301,no
Mining usage data and development artifacts,"Software repository mining techniques generally focus on analyzing, unifying, and querying different kinds of development artifacts, such as source code, version control meta-data, defect tracking data, and electronic communication. In this work, we demonstrate how adding real-world usage data enables addressing broader questions of how software systems are actually used in practice, and by inference how development characteristics ultimately affect deployment, adoption, and usage. In particular, we explore how usage data that has been extracted from web server logs can be unified with product release history to study questions that concern both users' detailed dynamic behaviour as well as broad adoption trends across different deployment environments. To validate our approach, we performed a study of two open source web browsers: Firefox and Chrome. We found that while Chrome is being adopted at a consistent rate across platforms, Linux users have an order of magnitude higher rate of Firefox adoption. Also, Firefox adoption has been concentrated mainly in North America, while Chrome users appear to be more evenly distributed across the globe. Finally, we detected no evidence in age-specific differences in navigation behaviour among Chrome and Firefox users; however, we hypothesize that younger users are more likely to have more up-to-date versions than more mature users.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6224305,no
Performance analysis of hybrid robust automatic speech recognition system,"In this paper, we evaluate the performance of several objective measures in terms of predicting the quality of noisy input speech signal through the Hybrid method using Voice Activity Detection (VAD) and Speech Enhancement Algorithm (SEA). Demand for Speech Recognition technology is expected to rise dramatically over the next few years as people use their mobile phones and voice recognition system everywhere. This paper enlighten the implementation process which includes a speech-to-text system using isolated word recognition with a vocabulary of ten words (digits 0 to 9). In the training period, the uttered digits are recorded using 8-bit Pulse Code Modulation (PCM) with a sampling rate of 8 KHz and save as a wave format file using sound recorder software. For a given word in the vocabulary, the system builds an Hidden Markov Model (HMM) model and trains the model during the training phase. The training steps, from VAD, Speech Enhancement to HMM model building, are performed using PC-based Matlab programs.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6224381,no
Evaluation of resilience in self-adaptive systems using probabilistic model-checking,"The provision of assurances for self-adaptive systems presents its challenges since uncertainties associated with its operating environment often hamper the provision of absolute guarantees that system properties can be satisfied. In this paper, we define an approach for the verification of self-adaptive systems that relies on stimulation and probabilistic model-checking to provide levels of confidence regarding service delivery. In particular, we focus on resilience properties that enable us to assess whether the system is able to maintain trustworthy service delivery in spite of changes in its environment. The feasibility of our proposed approach for the provision of assurances is evaluated in the context of the Znn.com case study.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6224391,no
A taxonomy and survey of self-protecting software systems,"Self-protecting software systems are a class of autonomic systems capable of detecting and mitigating security threats at runtime. They are growing in importance, as the stovepipe static methods of securing software systems have shown inadequate for the challenges posed by modern software systems. While existing research has made significant progress towards autonomic and adaptive security, gaps and challenges remain. In this paper, we report on an extensive study and analysis of the literature in this area. The crux of our contribution is a comprehensive taxonomy to classify and characterize research efforts in this arena. We also describe our experiences with applying the taxonomy to numerous existing approaches. This has shed light on several challenging issues and resulted in interesting observations that could guide the future research.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6224397,no
Fault detection and isolation from uninterpreted data in robotic sensorimotor cascades,"One of the challenges in designing the next generation of robots operating in non-engineered environments is that there seems to be an infinite amount of causes that make the sensor data unreliable or actuators ineffective. In this paper, we discuss what faults are possible to detect using zero modeling effort: we start from uninterpreted streams of observations and commands, and without a prior knowledge of a model of the world. We show that in sensorimotor cascades it is possible to define static faults independently of a nominal model. We define an information-theoretic usefulness of a sensor reading and we show that it captures several kind of sensorimotor faults frequently encountered in practice. We particularize these ideas to models proposed in previous work as suitable candidates for describing generic sensorimotor cascades. We show several examples with camera and range-finder data, and we discuss a possible way to integrate these techniques in an existing robot software architecture.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6225311,no
Semi-automatic establishment and maintenance of valid traceability in automotive development processes,"The functionality realized by software in modern cars is increasing and as a result the development artifacts of automotive systems are getting more complex. The existence of traceability along these artifacts is essential, since it allows to monitor the product development from the initial requirements to the final code. However, traceability is established and maintained mostly manually, which is time-consuming and error-prone. A further crucial problem is the assurance of the validity of the trace links, that is, the linked elements are indeed related to each other. In this paper we present a semiautomatic approach to create, check, and update trace links between artifacts along an automotive development process.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6225489,no
Research challenges on adaptive software and services in the future internet: towards an S-Cube research roadmap,This paper introduces research challenges on future service-oriented systems and software services. Those research challenges have been identified in a coordinated effort by researchers under the umbrella of the EU FP7 Network of Excellence S-Cube. We relate this effort to previous and related research roadmap activities and discuss the approach and results on identifying and assessing those challenges.,2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6225501,no
Verification and testing at run-time for online quality prediction,"This paper summarizes two techniques for online failure prediction allowing to anticipate the need for adaptation of service-oriented systems: (1) SPADE, employing run-time verification to predict failures of service compositions. (2) PROSA, building on online testing to predict failures of individual services.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6225511,no
Dependability-driven runtime management of service oriented architectures,"Software systems are becoming more and more complex due to the integration of large scale distributed entities and the continuous evolution of these new infrastructures. All these systems are progressively integrated in our daily environment and their increasing importance have raised a dependability issue. While Service oriented architecture is providing a good level of abstraction to deal with the complexity and heterogeneity of these new infrastructures, current approaches are limited in their ability to monitor and ensure the system dependability. In this paper, we propose a framework for the autonomic management of service oriented application based on a dependability objective. Our framework proposes a novel approach which leverages peer to peer evaluation of service providers to assess the system dependability. Based on this evaluation, we propose various strategies to dynamically adapt the system to maintain the dependability level of the system to the desired objective.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6225934,no
Building software process lines with CASPER,"Software product quality and project productivity require defining suitable software process models. The best process depends on the circumstances where it is applied. Typically, a process engineer tailors a specific process for each project or each project type from an organizational software process model. Frequently, tailoring is performed in an informal and reactive fashion, which is expensive, unrepeatable and error prone. Trying to deal with this challenge, we have built CASPER, a meta-process for defining adaptable software process models. This paper presents CASPER illustrating it using the ISPW-6 process. CASPER meta-process allows producing project specific processes in a planned way using four software process principles and a set of process practices that enable a feasible production strategy. According to its application to a canonical case, this paper concludes that CASPER enables a practical technique for tailoring a software process model.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6225962,no
Towards patterns for MDE-related processes to detect and handle changeability risks,"One of the multiple technical factors which affect changeability of software is model-driven engineering (MDE), where often several models and a multitude of manual as well as automated development activities have to be mastered to derive the final software product. The ability to change software with only reasonable costs, however, is of uppermost importance for the iterative and incremental development of software as well as agile development in general. Thus, the effective applicability of agile processes is influenced by the used MDE activities. However, there is currently no approach available to systematically detect and handle such risks to the changeability that result from the embedded MDE activities. In this paper we extend our beforehand-introduced process modeling approach by a notion of process pattern to capture typical situations that can be associated with risk or benefit with respect changeability. In addition, four candidates for the envisioned process patterns are presented in detail in the paper. Further, we developed strategies to handle changeability risks associated to these process patterns.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6225978,no
Investigating the impact of code smells debt on quality code evaluation,"Different forms of technical debt exist that have to be carefully managed. In this paper we focus our attention on design debt, represented by code smells. We consider three smells that we detect in open source systems of different domains. Our principal aim is to give advice on which design debt has to be paid first, according to the three smells we have analyzed. Moreover, we discuss if the detection of these smells could be tailored to the specific application domain of a system.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6225993,no
A rigorous approach to availability modeling,"Modeling and analyzing the dependability of software systems is a key activity in the development of embedded systems. An important factor of dependability is availability. Current modeling methods that support availability modeling are not based on a rigorous modeling theory. Therefore, when the behavior of the system influences the availability, as it is the case for fault-tolerant systems, the resulting analysis is imprecise or relies on external information. Based on a probabilistic extension of the Focus theory, we present a modeling technique that allows specifiying availability with a clear semantics. This semantics is a transformation of the original behavior to one that includes failures. Our approach enables modeling and verifying availability properties in the same way as system behavior.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6226009,no
Creating visual Domain-Specific Modeling Languages from end-user demonstration,"Domain-Specific Modeling Languages (DSMLs) have received recent interest due to their conciseness and rich expressiveness for modeling a specific domain. However, DSML adoption has several challenges because development of a new DSML requires both domain knowledge and language development expertise (e.g., defining abstract/concrete syntax and specifying semantics). Abstract syntax is generally defined in the form of a metamodel, with semantics associated to the metamodel. Thus, designing a metamodel is a core DSML development activity. Furthermore, DSMLs are often developed incrementally by iterating across complex language development tasks. An iterative and incremental approach is often preferred because the approach encourages end-user involvement to assist with verifying the DSML correctness and feedback on new requirements. However, if there is no tool support, iterative and incremental DSML development can be mundane and error-prone work. To resolve issues related to DSML development, we introduce a new approach to create DSMLs from a set of domain model examples provided by an end-user. The approach focuses on (1) the identification of concrete syntax, (2) inducing abstract syntax in the form of a metamodel, and (3) inferring static semantics from a set of domain model examples. In order to generate a DSML from user-supplied examples, our approach uses graph theory and metamodel design patterns.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6226010,no
How helpful are automated debugging tools?,"The field of automated debugging, which is concerned with the automation of identifying and correcting a failure's root cause, has made tremendous advancements in the past. However, some of the reported progress may be due to unrealistic assumptions that underlie the evaluation of automated debugging tools. These unrealistic assumptions concern the work process of developers and their ability to detect faulty code without explanatory context, as well as the size and arrangement of fixes. Instead of trying to locate the fault, we propose to help the developer understand it, thus enabling her to decide which fix she deems most appropriate. This would entail the need to employ a completely different evaluation scheme that bases on feedback from actual users of the tools in realistic usage scenarios. With this paper we propose the details for a first such user study.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6226573,no
Revisiting bug triage and resolution practices,"Bug triaging is an error-prone, tedious and time-consuming task. However, little qualitative research has been done on the actual use of bug tracking systems, bug triage, and resolution processes. We are planning to conduct a qualitative study to understand the dynamics of bug triage and fixing process, as well as bug reassignments and reopens. We will study interviews conducted with Mozilla Core and Firefox developers to get insights into the primary obstacles developers face during the bug fixing process. Is the triage process flawed? Does bug review slow things down? Does approval takes too long? We will also categorize the main reasons for bug reassignments and reopens. We will then combine results with a quantitative study of Firefox bug reports, focusing on factors related to bug report edits and number of people involved in handling the bug.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6226578,no
"An experimental study of a design-driven, tool-based development approach","Design-driven software development approaches have long been praised for their many benefits on the development process and the resulting software system. This paper discusses a step towards assessing these benefits by proposing an experimental study that involves a design-driven, tool-based development approach. This study raises various questions including whether a design-driven approach improves software quality and whether the tool-based approach improves productivity. In examining these questions, we explore specific issues such as the approaches that should be involved in the comparison, the metrics that should be used, and the experimental framework that is required.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6226581,no
Making exceptions on exception handling,"The exception-handling mechanism has been widely adopted to deal with exception conditions that may arise during program executions. To produce high-quality programs, developers are expected to handle these exception conditions and take necessary recovery or resource-releasing actions. Failing to handle these exception conditions can lead to not only performance degradation, but also critical issues. Developers can write formal specifications to capture expected exception-handling behavior, and then apply tools to automatically analyze program code for detecting specification violations. However, in practice, developers rarely write formal specifications. To address this issue, mining techniques have been used to mine common exception-handling behavior out of program code. In this paper, we discuss challenges and achievements in precisely specifying and mining formal exception-handling specifications, as tackled by our previous work. Our key insight is that expected exception-handling behavior may be conditional or may need to accommodate exceptional cases.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6226593,no
Towards a formal model to reason about context-aware exception handling,"The context-awareness is a central aspect in the design of pervasive systems, characterizing their ability to adapt its structure and behavior. The context-aware exception handling (CAEH) is an existing approach employed to design exception handling in pervasive systems. In this approach, the context is used to define, detect, propagate, and handle exceptions. CAEH is a complex and error prone activity, needing designers' insights and domain expertise to identify and characterize contextual exceptions. However, despite the existence of formal methods to analyze the adaptive behavior of pervasive systems, such methods lack specific support to specify the CAEH behavior. In this paper, we propose a formal model to reason about the CAEH behavior. It comprises an extension of the Kripke Structure to model the context evolution of a pervasive system and a transformation function that derivates the CAEH control flow from that proposed structure.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6226595,no
A generic approach for deploying and upgrading mutable software components,"Deploying and upgrading software systems is typically a labourious, error prone and tedious task. To deal with the complexity of a software deployment process and to make this process more reliable, we have developed Nix, a purely functional package manager as well as an extension called Disnix, capable of deploying service-oriented systems in a network of machines. Nix and its applications only support deployment of immutable components, which never change after they have been built. However, not all components of a software system are immutable, such as databases. These components must be deployed by other means, which makes deployment and upgrades of such systems difficult, especially in large networks. In this paper, we analyse the properties of mutable components and we propose Dysnomia, a deployment extension for mutable components.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6226613,no
Welcome to 3rd International Workshop on Emerging Trends in Software Metrics (WETSoM 2012),"Welcome to WETSoM2012, the 3rd International Workshop on Emerging Trends in Software Metrics. Since its start, WETSoM attracted a blend of academic and industrial researchers, creating a stimulating atmosphere to discuss the progresses of software metrics. A key motivation for this workshop is to help overcoming the low impact that software metrics has on current software development. This is pursued by critically examining the evidence for the effectiveness of existing metrics and identifying new directions for metrics. Evidence for existing metrics includes how the metrics have been used in practice and studies showing their effectiveness. Identifying new directions includes use of new theories, such as complex network theory, on which to base metrics. We are pleased that this year WETSoMfeatures 12 technical paper and an exciting keynote on mining developers' communication to assess software quality by Massimiliano di Penta. The program of WETSoM2012 is the result of hard work by many dedicated people; we especially thank the authors of submitted papers and the members of the program committee. Above all, the greatest richness of this workshop is its participants, who shape the discussion and points into new directions for software metrics research and practice. We hope you will have a great time and an unforgettable experience at WETSoM2012.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6226985,no
"Mining developers' communication to assess software quality: Promises, challenges, perils","Summary form only given. In the recent decades, power consumption of System on Chip (SoC) is getting more dominant and Through-Silicon Via (TSV) technology has emerged as a promising solution to enhance system integration at lower cost and reduce footprint. Powerful microprocessor and immense memory capability integrated in standard 2D IC enabled to improve IC performance by shrinking IC dimensions. Our research evaluates the impact of Through-Silicon Via (TSV) on 3D chip performance as well as power consumption and investigates to understand the optimum TSV dimension (i.e., diameter, height, etc...) for 3D IC fabrication. The key idea is using the physical and electrical modeling of TSV which considers the coupling effects as well as TSV-to-bulk silicon parameters in 3D circuitry. In addition, by combining the conventional metrics for planar IC technology with TSV modeling, several methodologies are developed to evaluate the 3D chip's behavior with respect to interconnect and repeaters. For example, by exploiting 101-stage Ring Oscillator and 100-inverter chain into 3D IC, it can be said that the through silicon via brings substantial benefits on local interconnect layers by improving overall transmission speed and reducing power consumption. The results in our research show that by adopting TSV infusion we can both reduce the power dissipation of interconnect and improve overall performance up to 35% in 4-die stacking case. Like all ICs, the TSV based 3D stacked IC need to be analyzed for manufacturing process variation. Hence, we investigate the variation of TSV dimension and then propose the optimal shape of TSV for the best performance of 3D systems integration. From simultaneous Monte Carlo simulations of TSV height and diameter, we can conclude that for given specific pitch in 3D IC technology, TSV with a small diameter is best fo",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6226987,no
Fourth international workshop on Software Engineering in Health Care (SEHC 2012),"Healthcare Informatics is one of the fastest growing economic sectors in the world today. With the anticipated future advances and investments in this field, it is expected that Healthcare Informatics will become one of the dominant economic factors in the 21st century. In addition to economic importance, this field has the potential to make substantial contributions to the comfort and longevity of every human being on the face of the earth. Software, and thus Software Engineering, has an important role to play in all of this. Medical devices, electronic medical records, robotic-driven surgery are just some examples where software plays a critical role. In addition, medical processes are known to be error prone and prime targets for process improvement technology. Moreover, there are important questions about software quality, user interfaces, systems interoperability, process automation, regulatory regimes, and many other concerns familiar to software engineering practitioners and researchers.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227003,no
Security testing of web applications: A research plan,"Cross-site scripting (XSS) vulnerabilities are specific flaws related to web applications, in which missing input validation can be exploited by attackers to inject malicious code into the application under attack. To guarantee high quality of web applications in terms of security, we propose a structured approach, inspired by software testing. In this paper we present our research plan and ongoing work to use security testing to address problems of potentially attackable code. Static analysis is used to reveal candidate vulnerabilities as a set of execution conditions that could lead to an attack. We then resort to automatic test case generation to obtain those input values that make the application execution satisfy such conditions. Eventually, we propose a security oracle to assess whether such test cases are instances of successful attacks.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227054,no
ConcernReCS: Finding code smells in software aspectization,"Refactoring object-oriented (OO) code to aspects is an error-prone task. To support this task, this paper presents ConcernReCS, an Eclipse plug-in to help developers to avoid recurring mistakes during software aspectization. Based on a map of concerns, ConcernReCS automatically finds and reports error-prone scenarios in OO source code; i.e., before the concerns have been refactored to aspects.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227063,no
Modeling Cloud performance with Kriging,"Cloud infrastructures allow service providers to implement elastic applications. These can be scaled at runtime to dynamically adjust their resources allocation to maintain consistent quality of service in response to changing working conditions, like flash crowds or periodic peaks. Providers need models to predict the system performances of different resource allocations to fully exploit dynamic application scaling. Traditional performance models such as linear models and queueing networks might be simplistic for real Cloud applications; moreover, they are not robust to change. We propose a performance modeling approach that is practical for highly variable elastic applications in the Cloud and automatically adapts to changing working conditions. We show the effectiveness of the proposed approach for the synthesis of a self-adaptive controller.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227075,no
Engineering and verifying requirements for programmable self-assembling nanomachines,"We propose an extension of van Lamsweerde's goal-oriented requirements engineering to the domain of programmable DNA nanotechnology. This is a domain in which individual devices (agents) are at most a few dozen nanometers in diameter. These devices are programmed to assemble themselves from molecular components and perform their assigned tasks. The devices carry out their tasks in the probabilistic world of chemical kinetics, so they are individually error-prone. However, the number of devices deployed is roughly on the order of a nanomole (a 6 followed by fourteen 0s), and some goals are achieved when enough of these agents achieve their assigned subgoals. We show that it is useful in this setting to augment the AND/OR goal diagrams to allow goal refinements that are mediated by threshold functions, rather than ANDs or ORs. We illustrate this method by engineering requirements for a system of molecular detectors (DNA origami pliers that capture target molecules) invented by Kuzuya, Sakai, Yamazaki, Xu, and Komiyama (2011). We model this system in the Prism probabilistic symbolic model checker, and we use Prism to verify that requirements are satisfied, provided that the ratio of target molecules to detectors is neither too high nor too low. This gives prima facie evidence that software engineering methods can be used to make DNA nanotechnology more productive, predictable and safe.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227079,no
BRACE: An assertion framework for debugging cyber-physical systems,"Developing cyber-physical systems (CPS) is challenging because correctness depends on both logical and physical states, which are collectively difficult to observe. The developer often need to repeatedly rerun the system while observing its behavior and tweak the hardware and software until it meets minimum requirements. This process is tedious, error-prone, and lacks rigor. To address this, we propose BRACE, A framework that simplifies the process by enabling developers to correlate cyber (i.e., logical) and physical properties of the system via assertions. This paper presents our initial investigation into the requirements and semantics of such assertions, which we call CPS assertions. We discusses our experience implementing and using the framework with a mobile robot, and highlight key future research challenges.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227084,no
Mining input sanitization patterns for predicting SQL injection and cross site scripting vulnerabilities,"Static code attributes such as lines of code and cyclomatic complexity have been shown to be useful indicators of defects in software modules. As web applications adopt input sanitization routines to prevent web security risks, static code attributes that represent the characteristics of these routines may be useful for predicting web application vulnerabilities. In this paper, we classify various input sanitization methods into different types and propose a set of static code attributes that represent these types. Then we use data mining methods to predict SQL injection and cross site scripting vulnerabilities in web applications. Preliminary experiments show that our proposed attributes are important indicators of such vulnerabilities.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227096,no
Release engineering practices and pitfalls,"The release and deployment phase of the software development process is often overlooked as part of broader software engineering research. In this paper, we discuss early results from a set of multiple semi-structured interviews with practicing release engineers. Subjects for the interviews are drawn from a number of different commercial software development organizations, and our interviews focus on why release process faults and failures occur, how organizations recover from them, and how they can be predicted, avoided or prevented in the future. Along the way, the interviews provide insight into the state of release engineering today, and interesting relationships between software architecture and release processes.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227099,no
Extending static analysis by mining project-specific rules,"Commercial static program analysis tools can be used to detect many defects that are common across applications. However, such tools currently have limited ability to reveal defects that are specific to individual projects, unless specialized checkers are devised and implemented by tool users. Developers do not typically exploit this capability. By contrast, defect mining tools developed by researchers can discover project-specific defects, but they require specialized expertise to employ and they may not be robust enough for general use. We present a hybrid approach in which a sophisticated dependence-based rule mining tool is used to discover project-specific programming rules, which are then transformed automatically into checkers that a commercial static analysis tool can run against a code base to reveal defects. We also present the results of an empirical study in which this approach was applied successfully to two large industrial code bases. Finally, we analyze the potential implications of this approach for software development practice.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227114,no
A tactic-centric approach for automating traceability of quality concerns,"The software architectures of business, mission, or safety critical systems must be carefully designed to balance an exacting set of quality concerns describing characteristics such as security, reliability, and performance. Unfortunately, software architectures tend to degrade over time as maintainers modify the system without understanding the underlying architectural decisions. Although this problem can be mitigated by manually tracing architectural decisions into the code, the cost and effort required to do this can be prohibitively expensive. In this paper we therefore present a novel approach for automating the construction of traceability links for architectural tactics. Our approach utilizes machine learning methods and lightweight structural analysis to detect tactic-related classes. The detected tactic-related classes are then mapped to a Tactic Traceability Information Model. We train our trace algorithm using code extracted from fifteen performance-centric and safety-critical open source software systems and then evaluate it against the Apache Hadoop framework. Our results show that automatically generated traceability links can support software maintenance activities while helping to preserve architectural qualities.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227153,no
Amplifying tests to validate exception handling code,"Validating code handling exceptional behavior is difficult, particularly when dealing with external resources that may be noisy and unreliable, as it requires: 1) the systematic exploration of the space of exceptions that may be thrown by the external resources, and 2) the setup of the context to trigger specific patterns of exceptions. In this work we present an approach that addresses those difficulties by performing an exhaustive amplification of the space of exceptional behavior associated with an external resource that is exercised by a test suite. Each amplification attempts to expose a program exception handling construct to new behavior by mocking an external resource so that it returns normally or throws an exception following a predefined pattern. Our assessment of the approach indicates that it can be fully automated, is powerful enough to detect 65% of the faults reported in the bug reports of this kind, and is precise enough that 77% of the detected anomalies correspond to faults fixed by the developers.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227157,no
Detecting and visualizing inter-worksheet smells in spreadsheets,"Spreadsheets are often used in business, for simple tasks, as well as for mission critical tasks such as finance or forecasting. Similar to software, some spreadsheets are of better quality than others, for instance with respect to usability, maintainability or reliability. In contrast with software however, spreadsheets are rarely checked, tested or certified. In this paper, we aim at developing an approach for detecting smells that indicate weak points in a spreadsheet's design. To that end we first study code smells and transform these code smells to their spreadsheet counterparts. We then present an approach to detect the smells, and to communicate located smells to spreadsheet users with data flow diagrams. To evaluate our apporach, we analyzed occurrences of these smells in the Euses corpus. Furthermore we conducted ten case studies in an industrial setting. The results of the evaluation indicate that smells can indeed reveal weaknesses in a spreadsheet's design, and that data flow diagrams are an appropriate way to show those weaknesses.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227171,no
Graph-based analysis and prediction for software evolution,"We exploit recent advances in analysis of graph topology to better understand software evolution, and to construct predictors that facilitate software development and maintenance. Managing an evolving, collaborative software system is a complex and expensive process, which still cannot ensure software reliability. Emerging techniques in graph mining have revolutionized the modeling of many complex systems and processes. We show how we can use a graph-based characterization of a software system to capture its evolution and facilitate development, by helping us estimate bug severity, prioritize refactoring efforts, and predict defect-prone releases. Our work consists of three main thrusts. First, we construct graphs that capture software structure at two different levels: (a) the product, i.e., source code and module level, and (b) the process, i.e., developer collaboration level. We identify a set of graph metrics that capture interesting properties of these graphs. Second, we study the evolution of eleven open source programs, including Firefox, Eclipse, MySQL, over the lifespan of the programs, typically a decade or more. Third, we show how our graph metrics can be used to construct predictors for bug severity, high-maintenance software parts, and failure-prone releases. Our work strongly suggests that using graph topology analysis concepts can open many actionable avenues in software engineering research and practice.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227173,no
A history-based matching approach to identification of framework evolution,"In practice, it is common that a framework and its client programs evolve simultaneously. Thus, developers of client programs may need to migrate their programs to the new release of the framework when the framework evolves. As framework developers can hardly always guarantee backward compatibility during the evolution of a framework, migration of its client program is often time-consuming and error-prone. To facilitate this migration, researchers have proposed two categories of approaches to identification of framework evolution: operation-based approaches and matching-based approaches. To overcome the main limitations of the two categories of approaches, we propose a novel approach named HiMa, which is based on matching each pair of consecutive revisions recorded in the evolution history of the framework and aggregating revision-level rules to obtain framework-evolution rules. We implemented our HiMa approach as an Eclipse plug-in targeting at frameworks written in Java using SVN as the version-control system. We further performed an experimental study on HiMa together with a state-of-art approach named AURA using six tasks based on three subject Java frameworks. Our experimental results demonstrate that HiMa achieves higher precision and higher recall than AURA in most circumstances and is never inferior to AURA in terms of precision and recall in any circumstances, although HiMa is computationally more costly than AURA.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227179,no
Improving early detection of software merge conflicts,"Merge conflicts cause software defects which if detected late may require expensive resolution. This is especially true when developers work too long without integrating concurrent changes, which in practice is common as integration generally occurs at check-in. Awareness of others' activities was proposed to help developers detect conflicts earlier. However, it requires developers to detect conflicts by themselves and may overload them with notifications, thus making detection harder. This paper presents a novel solution that continuously merges uncommitted and committed changes to create a background system that is analyzed, compiled, and tested to precisely and accurately detect conflicts on behalf of developers, before check-in. An empirical study confirms that our solution avoids overloading developers and improves early detection of conflicts over existing approaches. Similarly to what happened with continuous compilation, this introduces the case for continuous merging inside the IDE.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227180,no
Reconciling manual and automatic refactoring,"Although useful and widely available, refactoring tools are underused. One cause of this underuse is that a developer sometimes fails to recognize that she is going to refactor before she begins manually refactoring. To address this issue, we conducted a formative study of developers' manual refactoring process, suggesting that developers' reliance on chasing error messages when manually refactoring is an error-prone manual refactoring strategy. Additionally, our study distilled a set of manual refactoring workflow patterns. Using these patterns, we designed a novel refactoring tool called BeneFactor. BeneFactor detects a developer's manual refactoring, reminds her that automatic refactoring is available, and can complete her refactoring automatically. By alleviating the burden of recognizing manual refactoring, BeneFactor is designed to help solve the refactoring tool underuse problem.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227192,no
Characterizing logging practices in open-source software,"Software logging is a conventional programming practice. While its efficacy is often important for users and developers to understand what have happened in the production run, yet software logging is often done in an arbitrary manner. So far, there have been little study for understanding logging practices in real world software. This paper makes the first attempt (to the best of our knowledge) to provide a quantitative characteristic study of the current log messages within four pieces of large open-source software. First, we quantitatively show that software logging is pervasive. By examining developers' own modifications to the logging code in the revision history, we find that they often do not make the log messages right in their first attempts, and thus need to spend a significant amount of efforts to modify the log messages as after-thoughts. Our study further provides several interesting findings on where developers spend most of their efforts in modifying the log messages, which can give insights for programmers, tool developers, and language and compiler designers to improve the current logging practice. To demonstrate the benefit of our study, we built a simple checker based on one of our findings and effectively detected 138 pieces of new problematic logging code from studied software (24 of them are already confirmed and fixed by developers).",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227202,no
EVOSS: A tool for managing the evolution of free and open source software systems,"Software systems increasingly require to deal with continuous evolution. In this paper we present the EVOSS tool that has been defined to support the upgrade of free and open source software systems. EVOSS is composed of a simulator and of a fault detector component. The simulator is able to predict failures before they can affect the real system. The fault detector component has been defined to discover inconsistencies in the system configuration model. EVOSS improves the state of the art of current tools, which are able to predict a very limited set of upgrade faults, while they leave a wide range of faults unpredicted.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227234,no
Fault detection of a series compensated line during the damping process of Inter-area mode of oscillation,"The presence of supplementary controllers for thyristor controlled series compensators (TCSC), to damp the Inter-area power system oscillations makes the resultant transmission line (T.L), reactance varying during the damping process. This variation causes an undesirable effect on the distance relay DR, as its setting depends on the physical impedance of the T.L. In this paper, a novel algorithm is designed for the DR to allow it to update its impedance setting smoothly based on the measured level of series compensation. This adaptive setting allows the DR to detect the correct zone for the low current faults. PSCAD software is used to simulate the tuneable zones setting of the DR during the damping process.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227611,no
Optimal protection devices allocation and coordination in MV distribution networks,"Historically, the distribution network has been planned in order to operate in radial configuration. Protective relays are adopted to detect system abnormalities and to execute appropriate commands to isolate swiftly only the faulty component from the healthy system. Nowadays, the regulation schemes implemented by Regulators require that Distribution Companies reduce number and duration of supply interruptions by adopting new strategies in order to identify and isolate faults along distribution feeders. Two optimization problems arise: finding the optimal location of circuit breakers and the coordination among the overcurrent relay characteristics. In the paper, a novel algorithm to solve the overcurrent relay allocation and coordination problem in distribution networks based on genetic algorithm is proposed. Examples derived by a representative distribution network are presented.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227614,no
Mining object-oriented design models for detecting identical design structures,"The object-oriented design is the most popular design methodology of the last twenty-five years. Several design patterns and principles are defined to improve the design quality of object-oriented software systems. In addition, designers can use unique design motifs which are particular for the specific application domain. Another common habit is cloning and modifying some parts of the software while creating new modules. Therefore, object-oriented programs can include many identical design structures. This work proposes a sub-graph mining based approach to detect identical design structures in object-oriented systems. By identifying and analyzing these structures, we can obtain useful information about the design, such as commonly-used design patterns, most frequent design defects, domain-specific patterns, and design clones, which may help developers to improve their knowledge about the software architecture. Furthermore, problematic parts of frequent identical design structures are the appropriate refactoring opportunities because they affect multiple areas of the architecture. Experiments with several open-source projects show that we can successfully find many identical design structures in each project. We observe that usually most of the identical structures are an implementation of common design patterns; however we also detect various anti-patterns, domain-specific patterns, and design-level clones.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227865,no
"We have all of the clones, now what? Toward integrating clone analysis into software quality assessment","Cloning might seems to be an unconventional way of designing and developing software, yet it is very widely practised in industrial development. The cloning research community has made substantial progress on modeling, detecting, and analyzing software clones. Although there is continuing discussion on the real role of clones on software quality, our community may agree on the need for advancing clone management techniques. Current clone management techniques concentrate on providing editing tools that allow developers to easily inspect clone instances, track their evolution, and check change consistency. In this position paper, we argue that better clone management can be achieved by responding to the fundamental needs of industry practitioners. And the possible research directions include a software problem-oriented taxonomy of clones, and a better structured clone detection report. We believe this line of research should inspire new techniques, and reach to a much wider range of professionals from both the research and industry community.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227878,no
Assessing risk in Grids at resource level considering Grid resources as repairable using two state Semi Markov model,"Service Level Agreements in Grids improve upon the Best Effort Approach which provides no guarantees for provision of any Quality of Service (QoS) between the End User and the Resource Provider. Risk Assessment in Grids improves upon SLA by provision of Risk information to resource provider. Most of the previous studies of Risk Assessment in Grids work at node level. As a node failure can be a failure of any component such as Disk, CPU, Memory, Software, etcetera, the risk assessment at component level in Grids was introduced. In this work, we propose a Risk Assessment Model at component level while considering Grid resources as repairable. This work can be differentiated from the other works by the fact that the past efforts in Risk Assessment in Grids consider Grid Resources as replaceable rather than repairable. This Semi Markov model relies on the distribution fitting for both time to Failure and Time to Repair, extracted from the Grid Failure data during the data analysis section. By using Grid Failure data, the utilization of this Grid model is demonstrated by providing (Probability of Failure) PoF and (Probability of Repair) PoR values for different components. The experimental results indicate the PoF and PoR behave vary differently with the latter showing considerably times required for repair as compared to expectance of a failure. The risk information provided by this Risk Assessment Model will help Resource provider to use the Grid Resources efficiently and achieve effective scheduling.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227906,no
Automated prediction of defect severity based on codifying design knowledge using ontologies,"Assessing severity of software defects is essential for prioritizing fixing activities as well as for assessing whether the quality level of a software system is good enough for release. In filling out defect reports, developers routinely fill out default values for the severity levels. The purpose of this research is to automate the prediction of defect severity. Our aim is to research how this severity prediction can be achieved through reasoning about the requirements and the design of a system using ontologies. In this paper we outline our approach based on an industrial case study.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227962,no
Predicting mutation score using source code and test suite metrics,"Mutation testing has traditionally been used to evaluate the effectiveness of test suites and provide confidence in the testing process. Mutation testing involves the creation of many versions of a program each with a single syntactic fault. A test suite is evaluated against these program versions (mutants) in order to determine the percentage of mutants a test suite is able to identify (mutation score). A major drawback of mutation testing is that even a small program may yield thousands of mutants and can potentially make the process cost prohibitive. To improve the performance and reduce the cost of mutation testing, we propose a machine learning approach to predict mutation score based on a combination of source code and test suite metrics.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227969,no
A Heuristic Model-Based Test Prioritization Method for Regression Testing,"Due to the resource and time constraints for re-executing large test suites in regression testing, developers are interested in detecting faults in the system as early as possible. Test case prioritization seeks to order test cases in such a way that early fault detection is maximized. In this paper, we present a model-based heuristic method to prioritize test cases for regression testing, which takes into account two types of information collected during execution of the modified model on the test suite. The experiment shows that our algorithm has better effectiveness of early fault detection.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6228450,no
Simulation of temperature effects on GaAs MESFET based on physical model,"The Prognostics and Health Management (PHM) system for radar equipment is paid more and more attention by researchers in recent years. Owing to its high reliability and performance, the Active Phased Array Radar (APAR) has been playing an increasingly important role in the modern radar field which is composed of thousands of solid-state Transmit/Receive (T/R) modules. As the power source of the T/R module, gallium arsenide metal-semiconductor field effect transistor (GaAs MESFET) has been widely used due to its higher electron mobility, operating frequency, power-added efficiency and lower noise figures than silicon MOSFET. However, the performance of GaAs MESFET is influenced by its operating temperature significantly. In order to achieve effective fault injection for the PHM system, it's necessary to get temperature effects on GaAs MESFET. A simplified GaAs MESFET equivalent circuit model based on specific physical properties is proposed and realized on the EDA software. It can help the optimizing of device's structure and materials' parameters. What is more, it realizes the performance simulation under varied temperatures, thus the degradation of GaAs MESFET's output parameters can be predicted by monitoring its temperature.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6228796,no
Simulation of stress-magnetization coupling about magnetic memory effect of gear defects,"As an important component in the transmission system, gear endures complex stress during the meshing process. The tiny defects of the gear are likely to occur after some amount of load cycles. This will lead to a huge economic loss if the tiny defects are not detected in time. Metal magnetic memory (MMM) technique can effectively find the early defects of ferromagnetic material and it has attracted great attentions. However, the mechanism of metal magnetic memory on ferromagnetic materials under loading and geomagnetic field has not been thoroughly addressed, and the studies on gear defects are rarely reported. This paper adopts the finite element analysis (FEA) software ANSYS to construct a defective gear model and the air around it. The contact-stress and magnetic flux leakage distribution is given through the stress-magnetization coupling model. The simulation results are in accord with the common MMM principle, which are helpful to the understanding of the stress-magnetization coupling mechanism and provide the gist for the further study of gears early defects detection based on MMM technique.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6228902,no
Critical review of system failure behavior analysis method,"Failure behavior is the state change process of product or part of a product which is relative to its environment, over time performance and can be detected from the outside. According to the different level of analysis, failure behavior analysis method can be divided into element failure behavior analysis method and system failure behavior analysis method. The formal reveals the variety failure mechanisms under the alone or coupled action of internal cause and external cause using coupling analysis method; the latter focuses on product failure performance law under the effect of variety failure mechanisms by means of failure propagation analysis or state analysis. This critical review from two aspects of element and system investigates and summarizes the current research status of failure behavior analysis method. The result shows that coupling analysis method has been mature at present, and there is plenty of supporting software for computer-aided analysis. Failure propagation analysis method consists of graph theory based method, Petri Net method and complex network method. But the above-mentioned methods are unilateral and isolated from each other. System failure behavior analysis method needs to synthetically use the current methods - coupling analysis, failure propagation analysis and state analysis method so that forms an analysis methodology which needs to clear the input and output of each method and improve the interface between application software. The comprehensive methodology that the failure behavior analysis follows will provide support to product reliability analysis and design improvement.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6228917,no
Test case prioritization incorporating ordered sequence of program elements,"Test suites often grow very large over many releases, such that it is impractical to re-execute all test cases within limited resources. Test case prioritization, which rearranges test cases, is a key technique to improve regression testing. Code coverage information has been widely used in test case prioritization. However, other important information, such as the ordered sequence of program elements measured by execution frequencies, was ignored by previous studies. It raises a risk to lose detections of difficult-to-find bugs. Therefore, this paper improves the similarity-based test case prioritization using the ordered sequence of program elements measured by execution counts. The empirical results show that our new technique can increase the rate of fault detection more significantly than the coverage-based ART technique. Moreover, our technique can detect bugs in loops more quickly and be more cost-benefits than the traditional ones.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6228980,no
Grammar based oracle for security testing of web applications,"The goal of security testing is to detect those defects that could be exploited to conduct attacks. Existing works, however, address security testing mostly from the point of view of automatic generation of test cases. Less attention is paid to the problem of developing and integrating with a security oracle. In this paper we address the problem of the security oracle, in particular for Cross-Site Scripting vulnerabilities. We rely on existing test cases to collect HTML pages in safe conditions, i.e. when no attack is run. Pages are then used to construct the safe model of the application under analysis, a model that describes the structure of an application response page for safe input values. The oracle eventually detects a successful attack when a test makes the application display a web page that is not compliant with the safe model.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6228984,no
Automated test-case generation by cloning,"Test cases are often similar. A preliminary study of eight open-source projects found that on average at least 8% of all test cases are clones; the maximum found was 42 %. The clones are not identical with their originals - identifiers of classes, methods, attributes and sometimes even order of statements and assertions differ. But the test cases reuse testing logic and are needed for testing. They serve a purpose and cannot be eliminated. We present an approach that generates useful test clones automatically, thereby eliminating some of the grunt work of testing. An important advantage over existing automated test case generators is that the clones include the test oracle. Hence, a human decision maker is often not needed to determine whether the output of a test is correct. The approach hinges on pairs of classes that provide analogous functionality, i.e., functions that are tested with the same logic. TestCloner transcribes tests involving analogous functions from one class to the other. Programmers merely need to indicate which methods are analogs. Automatic detection of analogs is currently under investigation. Preliminary results indicate a significant reduction in the number of boilerplate tests that need to be written by hand. The transcribed tests do detect defects and can provide hints about missing functionality.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6228995,no
BlackHorse: Creating smart test cases from brittle recorded tests,"Testing software with a GUI is difficult. Manual testing is costly and error-prone, but recorded test cases frequently break due to changes in the GUI. Test cases intended to test business logic must therefore be converted to a less brittle form to lengthen their useful lifespan. In this paper, we describe BlackHorse, an approach to doing this that converts a recorded test case to Java code that bypasses the GUI. The approach was implemented within the testing environment of Research In Motion. We describe the design of the toolset and discuss lessons learned during the course of the project.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6228996,no
Testing of PolPA authorization systems,"The implementation of an authorization system is a difficult and error-prone activity that requires a careful verification and testing process. In this paper, we focus on testing the implementation of the PolPA authorization system and in particular its Policy Decision Point (PDP), used to define whether an access should be allowed or not. Thus exploiting the PolPA policy specification, we present a fault model and a test strategy able to highlight the problems, vulnerabilities and faults that could occur during the PDP implementation, and a testing framework for the automatic generation of a test suite that covers the fault model. Preliminary results of the test framework application to a realistic case study are presented.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6228997,no
How much information could be revealed by analyzing data from pressure sensors attached to shoe insole?,"Data collected from pressure sensors attached to shoe insole is a rich source of information. (1) We can detect faults in walking and balancing problems for old people. (2) The pressure sensor data can be used to design personalized foot orthoses. (3) We can calculate the calorie burnt, even when walking and jogging are mixed, and the road slope changes. (4) We can use the data to train sprinters or tennis players. (5) We can even use the data for person identification. In addition, it can be used for alarms for situations arising from mis-handling of machines, like accelerator pedal in a car. We attached very thin pressure sensors on top of a shoe insole and collected data. A few important and readily detectable features from the time series data collected by those sensors are extracted and used for person identification, or to classify whether a person is walking or jogging. Nearly 100% classification accuracy was achieved. Thus, the target to classify whether the person is walking or jogging or climbing up or down the stairs is possible. This success also encouraged us to investigate whether it is possible to find the body weight and the step-length from this data. Once that is possible, the system can accurately deliver the calorie burnt at the end of the day. We will further explore the possibility, using sophisticated feature extraction and classification techniques, to detect faults in walking and predict the probability of fall for elderly people, or people with problem in balancing due to various diseases or caused by accidents.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6229115,no
EMFtoCSP: A tool for the lightweight verification of EMF models,"The increasing popularity of MDE results in the creation of larger models and model transformations, hence converting the specification of MDE artefacts in an error-prone task. Therefore, mechanisms to ensure quality and absence of errors in models are needed to assure the reliability of the MDE-based development process. Formal methods have proven their worth in the verification of software and hardware systems. However, the adoption of formal methods as a valid alternative to ensure model correctness is compromised for the inner complexity of the problem. To circumvent this complexity, it is common to impose limitations such as reducing the type of constructs that can appear in the model, or turning the verification process from automatic into user assisted. Since we consider these limitations to be counterproductive for the adoption of formal methods, in this paper we present EMFtoCSP, a new tool for the fully automatic, decidable and expressive verification of EMF models that uses constraint logic programming as the underlying formalism.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6229788,no
Evaluation of testability enhancement using software prototype,"The functional delay-fault models, which are based on the input stimuli and correspondent responses at the outputs, cover transition faults at the gate level quite well. This statement forms the basis for the analysis and comparison of different methods of design for testability (DFT) using software prototype model of the circuit and to select the most appropriate one before the structural synthesis of the circuit. Along with known DFT methods (enhanced scan, launch-on-shift scan and launch-on-capture scan), the authors introduce the method, which is based on the addition of new connections to the circuit in the non-scan testing mode. In order to assess the DFT methods, the functional test is generated for the analysed circuit and the functional delay-fault coverage for this test is evaluated. Each of the considered DFT methods has its own advantages and disadvantages, since they have different delay fault coverage, and require different hardware for their implementation. These differences depend on the function of circuit. The experimental results are provided for the ITC'99 benchmark circuits. The obtained results proved the applicability of the proposed method.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6230788,no
A new tracking method of symmetrical fault during Power Swing Based on S-Transform,"Current distance relay is accommodated with Power Swing Blocking (PSB) scheme. However, this blocking scheme prove to vulnerable to distance relay operation as it could block the trip signals if the symmetrical fault occur during power swing. Hence, it is important to develop the proper fault detection scheme during power swing to avoid such undesirable circumstances. This paper presents a new detection technique to detect symmetrical fault during power swing by using S-Transform analysis based on the current, voltage, and three-phase active and reactive power signals waveform. To evaluate the effectiveness of the proposed technique, testing has been conducted under IEEE 9 bus test system. Simulation results show that the proposed technique can reliably detect symmetrical fault occurring during power swing.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6230850,no
Impact of a 200 MW concentrated receiver solar power plant on steady-state and transient performances of Oman transmission system,"The paper presents steady-state and transient studies to assess the impact of a 200 MW Central Receiver Solar Power Plant (CRSPP) connection on the Main Interconnected Transmission System (MITS) of Oman. The CRSPP consists mainly of a central solar receiver, power tower, thousands of heliostats, molten salt storage tanks, heat exchangers, steam generator, steam turbine, synchronous generator, and step-up transformer. Two proposed locations are considered to connect the CRSPP plant to MITS: Manah 132 kV and Adam 132/33 kV grid stations. The 2015 transmission grid model has been updated to include the simulation of the proposed 200 MW CRSPP using the DIgSILENT PowerFactory professional software. The studies include load flow analysis and short-circuit level calculations in addition to transient responses to three-phase fault and complete CRSPP outage. The results have shown that the connection of the proposed CRSPP plant to the MITS is acceptable.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6230897,no
Towards spatial fault resilience in array processors,"Computing with large die-size graphical processors (that need huge arrays of identical structures) in the late CMOS era is abounding with challenges due to spatial non-idealities arising from chip-to-chip and within-chip variation of MOSFET threshold voltage. In this paper, we propose a machine learning based software-framework for in-situ prediction and correction of computation corrupted due to threshold voltage variation of transistors. Based on semi-supervised training imparted to a fully connected cascade feed-forward neural network (FCCFF-NN), the NN makes an accurate prediction of the underlying hardware, creating a spatial map of faulty processing elements (PE). The faulty elements identified by the NN are avoided in future computing. Further, any transient faults occurring over and above these spatial faults are tracked, and corrected if the number of PEs involved in a particle strike is above a preset threshold. For the purposes of experimental validation, we consider a 256  256 array of PE. Each PE is comprised of a multiply-accumulate (MAC) block with three 8 bit registers (two for inputs and one for storing the computed result). One thousand instances of this processor array are created and PEs in each instance are randomly perturbed with threshold voltage variation. Common image processing operations such as low pass filtering and edge enhancement are performed on each of these 1000 instances. A fraction of these images (about 10%) is used to train the NN for spatial non-idealities. Based on this training, the NN is able to accurately predict the spatial extremities in 95% of all the remaining 90% of the cases. The proposed NN based error tolerance results in superior quality images whose degradation is no longer visually perceptible.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6231068,no
Book of abstracts,"Summary form only given. Programming assignments (PAs) are very important to many computer science courses. Traditionally, the grading of a programming assignment is based mainly on the correctness of the code. However, from the view point of software engineering education, such a grading does not encourage students to develop code that is easy to read and maintain. Thus, the authors created a grading policy that considers not only the correctness but also the quality of the code, expecting students to follow the most important discipline  the source code should be written in a way that is readable and maintainable. Instead of using pure subjective code-quality ratings, bad smells are used to assess the code quality of PAs. When a PA is graded by the teaching assistant, a list of bad smells is identified and given to the student so that the student can use refactoring methods to improve the code.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6232111,no
On-line software-based self-test of the Address Calculation Unit in RISC processors,"Software-based Self-Test (SBST) can be used during the mission phase of microprocessor-based systems to periodically assess the hardware integrity. However, several constraints are imposed to this approach, due to the coexistence of test programs with the mission application. This paper proposes a method for the generation of SBST programs to test on-line the Address Calculation Unit of embedded RISC processors, which is one of the most heavily impacted by the online constraints. The proposed strategy achieves high stuck-at fault coverage on both a MIPS-like processor and an industrial 32-bit pipelined processor; these two case studies show the effectiveness of the technique and the low effort.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6233004,no
Test tool qualification through fault injection,"According to ISO 26262, a recent automotive functional safety standard, verification tools shall undergo qualification, e.g. to ensure that they do not fail to detect faults that can lead to violation of functional safety requirements. We present a semi-automatic qualification method involving a monitor and fault injection that reduce cost in the qualification process. We experiment on a verification tool implemented in LabVIEW.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6233042,no
Annotation support for generic patches,"In large projects parallelization of existing programs or refactoring of source code is time consuming as well as error-prone and would benefit from tool support. However, existing automatic transformation systems are not extensively used because they either require tedious definitions of source code transformations or they lack general adaptability. In our approach, a programmer changes code inside a project, resulting in before and after source code versions. The difference (the generated transformation) is stored in a database. When presented with some arbitrary code, our tool mines the database to determine which of the generalized transformations possibly apply. Our system is different from a pure compiler based (semantics preserving) approach as we only suggest code modifications. Our contribution is a set of generalizing annotations that we have found by analyzing recurring patterns in open source projects. We show the usability of our system and the annotations by finding matches and applying generated transformations in real-world applications.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6233400,no
The analysis of complex networks research based on scientific knowledge mapping,"In this paper, research papers of complex network which published between 2000 and 2011 are indexed based on SCI database. From quotation analysis, content analysis and statistics analysis, the knowledge mapping of major research country and top quality institution are drawn using Pajek software. Through the knowledge mapping, the detecting research frontier, scientific cooperation are discovered. We also discuss the hot topic of complex network research and forecast the future direction in different country. At the same time, some advice was given to Chinese researchers.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6233934,no
Data mining T-RFLP profiles from urban water system sampling using self-organizing maps,"Descriptions of urban water system microbiological properties can range from single parameters such as microbial biomass to multiparameter qualitative and quantitative data that describes biochemical profiles, measurements of enzyme activities, and molecular analyses of microbial communities. Whilst most of the hydraulic and physico-chemical variables are quite well understood, measures of microbiological processes have so far been more difficult to use as part of decision support tools. The methods commonly used to assess the microbial quality of water and wastewater are mainly culture-dependent methods, which underestimate the actual microbial diversity within the system. To circumvent this limitation, DNA-based molecular techniques are now being used to analyze environmental samples. In the past few decades, technological innovations have led to the development of a new biological research paradigm, one that is data intensive and computer-driven. A range of data driven tools have been applied for exploring the interrelationships between various types of variables. A number of studies have used Artificial Neural Networks (ANNs) to probe such complex data sets. This paper demonstrates how Kohonen self-organizing maps (SOM) can be used for data mining of microbiological data sources from urban water systems. Genetic signatures acquired by terminal restriction fragment length polymorphisms (TRFLP) were obtained from samples and then post processed by the T-Align software tool before being reduced in dimensionality with Principal Component Analysis (PCA). These datasets were then analyzed by SOM networks and additional characteristics were used in the map labeling. Initial results show that the visual output of the SOM analysis provides a rapid and intuitive means of exploring hypotheses for increased understanding and interpretation of microbial ecology.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6234528,no
A Fault Tolerant Approach to Detect Transient Faults in Microprocessors Based on a Non-Intrusive Reconfigurable Hardware,"This paper presents a non-intrusive hybrid fault detection approach that combines hardware and software techniques to detect transient faults in microprocessors. Such faults have a major influence in microprocessor-based systems, affecting both data and control flow. In order to protect the system, an application-oriented hardware module is automatically generated and reconfigured on the system during runtime. When combined with fault tolerance techniques based on software, this solution offers full system protection against transient faults. A fault injection campaign is performed using a MIPS microprocessor executing a set of applications. HW/SW implementation in a reprogrammable platform shows smaller memory area and execution time overhead when compared to related works. Fault injection results show the efficiency of this method by detecting 100% of faults.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6236246,no
Traveling-Wave-Based Line Fault Location in Star-Connected Multiterminal HVDC Systems,"This paper presents a novel algorithm to determine the location of dc line faults in an HVDC system with multiple terminals connected to a common point, using only the measurements taken at the converter stations. The algorithm relies on the traveling-wave principle, and requires the fault-generated surge arrival times at the converter terminals. With accurate surge arrival times obtained from time-synchronized measurements, the proposed algorithm can accurately predict the faulty segment as well as the exact fault location. Continuous wavelet transform coefficients of the input signal are used to determine the precise time of arrival of traveling waves at the dc line terminals. Performance of the proposed fault-location scheme is analyzed through detailed simulations carried out using the electromagnetic transient simulation software PSCAD. The algorithm does not use reflected waves for its calculations and therefore it is more robust compared to fault location algorithms previously proposed for teed transmission lines. Furthermore, the algorithm can be generalized to handle any number of line segments connected to the star point.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6236278,no
Comparing maintainability evolution of object-oriented and aspect-oriented software product lines,"Software Product Line aims at improving productivity and decrease realization times by gathering the analysis, design and implementation activities of a family of systems. Evaluating the quality attributes for SPL architectures is very crucial especially architecture maintainability as SPL are expected to have longer lifetime span. Aspect-orientation offers a modularization way by separating crosscutting concerns from non-crosscutting ones. Aspect-oriented programming is assumed to endorse better modularity and changeability of product lines than traditional variability mechanisms. In this paper, we show that change propagation probability (CP) is helpful and effective in assessing the design quality of software architectures. We propose to use the CP to assess the evolution of the architecture of software product lines through different releases. We use CP to investigate whether aspect oriented SPL has better maintainability evolution than object-oriented SPL.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6236580,no
Harmonic study in electromagnetic voltage transformers,"Ferroresonance is a complex electrical phenomenon which may cause overvoltages and overcurrents in electrical power system disturbing the system reliability and continuous safe operating. The ability to predict or confirm ferroresonance depends primarily on the accuracy of the voltage transformer model used in the computer simulation. In this study, at first an overview of the subject in the literature is provided. Then, occurrence of ferroresonance and the resulting harmonics in a voltage transformer are simulated. The effect of iron core saturation characteristic on the occurrence of harmonic modes has been studied. The system under study has a voltage transformer rated 100VA, 275 kV. The nonlinear magnetization cure index of the autotransformer is chosen with q=7. The linear core loss of the transformer core is modeled with linear resistance. Harmonic modes in the proposed power system are analyzed using MATLAB software. The results show that harmonics are produced in the considered substation and causes great effect on voltage transformer failure.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6237264,no
On prediction of defect rates,"The prediction of software reliability can determine the current reliability of a product, using statistical techniques based on the failures data, obtained during testing or system usability. The major problem in predicting software reliability is their high complexity, as well as the excessive limitations of existing models. Therefore, choosing the appropriate reliability prediction model is essential. The purpose of this article is to study the defect prediction models, using the Rayleigh function. This function forecasts the defect discovery rate as a function of time through the software development process. Starting from an existing static prediction model, we have introduced a new approach, using the time-scale. Also, we have considered two methods for computing the model parameters, and have compared them to the real data set. The results have validated this model as a good estimation of the defect rate behavior. We will also suggest possible directions for extending the paper in the future.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6237703,no
Curve fitting method for modeling and analysis of photovoltaic cells characteristics,This paper deals with the mathematical model to assess the performances of photovoltaic (PV) cells. The PV system characteristics are modeled and analyzed by using the curve fitting method referred to the different connections of PV cells and different solar irradiance. The results are compared with those resulting from measured data in a real case. Specific LabVIEWTM and MatlabTM software applications are implemented to prove the theoretical methods.,2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6237722,no
Virtual instrumentation in power engineering,"Focusing on the investigation of virtual instrumentation systems, this paper explores certain software platforms to develop interface for virtual instruments, then assesses and puts side by side their performance. There are introduced controls generating methods, based on ActiveX/beans, for virtual instruments development. Authors attempts were converged in achieving the overall integration between virtual instrumentation components and third-party applications supporting COM technology.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6237734,no
An initial study on ideal GUI test case replayability,"In this paper we investigate the effect of long-term GUI changes occurring during application development on the reusability of existing GUI test cases. We conduct an empirical evaluation on two complex, open-source GUI-driven applications for which we generate test cases of various lengths. We then assess the replayability of generated test cases using simulation on newer versions of the target applications and partition them according to the type of repairing change required for their reuse.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6237736,no
Video based control of a 6 degrees-of-freedom serial manipulator,This paper presents a video based control system for a Fanuc M-6iB/2HS articulated robot. The system uses a CMUCam3 video camera connected to PC. The industrial robot is controlled via the TCP/IP protocol using a custom simulation software created in previous researches for the industrial robot. The simulation software implements additional classes designed to control and monitor the video camera. The paper presents in detail the calibration and testing stages. The system is capable to detect cylindrical objects of any stored color and is able to determine their position in the working space of the robot.,2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6237746,no
"Simulations for wind, solar and pumping facilities and hybrid systems design with KOGAION","KOGAION is a computer modeling application that simplifies the task of designing distributed renewable energy generation systems - both on and off-grid. KOGAION's optimization and sensitivity analysis algorithms allows to evaluate the economic and technical feasibility of a photovoltaic, wind and hydro turbines technology options and to account for variations in technology costs and energy resource availability. It's adaptable to a wide variety of projects like wind, photovoltaic and hybrid energy systems, both for large on grid projects or small hybrid isolated projects (for a village, community-scale power systems or individual systems). KOGAION can model both the technical and economic factors involved in the project. For larger systems, it can provide an important overview that compares the cost and feasibility of different configurations. It is accessible to broad spectrum of users, from financial decision makers to engineers and others. More than other software applications, Kogaion supports all types of wind descriptions: by Weibull coefficients, probability of density function (pdf), time series (TS). Time references simulation is crucial for modeling variable resources, such as wind power. KOGAION's sensitivity analysis helps to determine the potential impact of uncertain factors wind speed on a given system, over time. It doesn't cover bio fuel, hydrogen, fuel cells.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6237785,no
FEM simulation for Lamb wave evaluate the defects of plates,"The experimental method is hard to predict and explain the received Lamb signal of plates due to its dispersive nature. The method to evaluate the depth and defects in plates is investigated theoretically using the finite element method(FEM) in this paper. At first, the theories of Lamb waves in a plate was introduced. Then, the commercial FEM software, ABAQUS EXPLICIT, model lamb wave in plates. The monitor points of healthy plate and defective plate were predicted. The result shows that FEM simulation can effective evaluate the notch by delay time between reflection signal and first echo signal.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6238184,no
Specifying Compiler Strategies for FPGA-based Systems,"The development of applications for high-performance Field Programmable Gate Array (FPGA) based embedded systems is a long and error-prone process. Typically, developers need to be deeply involved in all the stages of the translation and optimization of an application described in a high-level programming language to a lower-level design description to ensure the solution meets the required functionality and performance. This paper describes the use of a novel aspect-oriented hardware/software design approach for FPGA-based embedded platforms. The design-flow uses LARA, a domain-specific aspect-oriented programming language designed to capture high-level specifications of compilation and mapping strategies, including sequences of data/computation transformations and optimizations. With LARA, developers are able to guide a design-flow to partition and map an application between hardware and software components. We illustrate the use of LARA on two complex real-life applications using high-level compilation and synthesis strategies for achieving complete hardware/software implementations with speedups of 2.5 and 6.8 over software-only implementations. By allowing developers to maintain a single application source code, this approach promotes developer productivity as well as code and performance portability.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6239814,no
A unifying framework for the definition of syntactic measures over conceptual schema diagrams,"There are many approaches that propose the use of measures for assessing the quality of conceptual schemas. Many of these measures focus purely on the syntactic aspects of the conceptual schema diagrams, e.g. their size, their shape, etc. Similarities among different measures may be found both at the intra-model level (i.e., several measures over the same type of diagram are defined following the same layout) and at the intermodel level (i.e., measures over different types of diagrams are similar considering an appropriate metaschema correspondence). In this paper we analyse these similarities for a particular family of diagrams used in conceptual modelling, those that can be ultimately seen as a combination of nodes and edges of different types. We propose a unifying measuring framework for this family to facilitate the measure definition process and illustrate its application on a particular type, namely business process diagrams.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6240423,no
Program complexity metrics and programmer opinions,"Various program complexity measures have been proposed to assess maintainability. Only relatively few empirical studies have been conducted to back up these assessments through empirical evidence. Researchers have mostly conducted controlled experiments or correlated metrics with indirect maintainability indicators such as defects or change frequency. This paper uses a different approach. We investigate whether metrics agree with complexity as perceived by programmers. We show that, first, programmers' opinions are quite similar and, second, only few metrics and in only few cases reproduce complexity rankings similar to human raters. Data-flow metrics seem to better match the viewpoint of programmers than control-flow metrics, but even they are only loosely correlated. Moreover we show that a foolish metric has similar or sometimes even better correlation than other evaluated metrics, which raises the question how meaningful the other metrics really are. In addition to these results, we introduce an approach and associated statistical measures for such multi-rater investigations. Our approach can be used as a model for similar studies.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6240486,no
A semantic relatedness approach for traceability link recovery,"Human analysts working with automated tracing tools need to directly vet candidate traceability links in order to determine the true traceability information. Currently, human intervention happens at the end of the traceability process, after candidate traceability links have already been generated. This often leads to a decline in the results' accuracy. In this paper, we propose an approach, based on semantic relatedness (SR), which brings human judgment to an earlier stage of the tracing process by integrating it into the underlying retrieval mechanism. SR tries to mimic human mental model of relevance by considering a broad range of semantic relations, hence producing more semantically meaningful results. We evaluated our approach using three datasets from different application domains, and assessed the tracing results via six different performance measures concerning both result quality and browsability. The empirical evaluation results show that our SR approach achieves a significantly better performance in recovering true links than a standard Vector Space Model (VSM) in all datasets. Our approach also achieves a significantly better precision than Latent Semantic Indexing (LSI) in two of our datasets.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6240487,no
Toward an effective automated tracing process,"The research on automated tracing has noticeably advanced in the past few years. Various methodologies and tools have been proposed in the literature to provide automatic support for establishing and maintaining traceability information in software systems. This movement is motivated by the increasing attention traceability has been receiving as a de jure standard in software quality assurance. Following that effort, in this research proposal we describe several research directions related to enhancing the effectiveness of automated tracing tools and techniques. Our main research objective is to advance the state of the art in this filed. We present our suggested contributions through a set of incremental enhancements over the conventional automated tracing process, and briefly describe a set of strategies for assessing these contributions impact on the process.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6240502,no
Non-blocking N-version programming for message passing systems,"N-version programming (NVP) employs masking redundancy: N equivalent modules (called versions) are implemented independently and run concurrently. The results of their execution are adjudicated by a special component that defines the correct majority result and eliminates the results of the versions in which design faults have been triggered. The disadvantage of using these schemes is that they need to block the receiver process until each received message is confirmed by the other version, which results in high time overhead. In the case of variant response latencies, consisting of processing time and message transmission delay, these techniques would not be efficient. In this paper a new non-blocking NVP approach based on capturing the causality between requests and response is proposed. This approach does not need to block the versions to confirm the output. The simulations result show that for acceptable values for probability of failure per demand (pfd) and simultaneous active requests, our approach has lower execution time.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6240668,no
Multivariate logistic regression prediction of fault-proneness in software modules,"This paper explores additional features, provided by stepwise logistic regression, which could further improve performance of fault predicting model. Three different models have been used to predict fault-proneness in NASA PROMISE data set and have been compared in terms of accuracy, sensitivity and false alarm rate: one with forward stepwise logistic regression, one with backward stepwise logistic regression and one without stepwise selection in logistic regression. Despite an obvious trade-off between sensitivity and false alarm rate, we can conclude that backward stepwise regression gave the best model.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6240735,no
On software design for stochastic processors,"Much recent research [8, 6, 7] suggests significant power and energy benefits of relaxing correctness constraints in future processors. Such processors with relaxed constraints have often been referred to as stochastic processors [10, 15, 11]. In this paper we present three approaches for building applications for such processors. The first approach relies on relaxing the correctness of the application based upon an analysis of application characteristics. The second approach relies upon detecting and then correcting faults within the application as they arise. The third approach transforms applications into more error tolerant forms. In this paper, we show how these techniques that enhance or exploit the error tolerance of applications can yield significant power and energy benefits when computed on stochastic processors.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6241613,no
An useful system based data mining of the ore mixing in sintering process,"The article introduced the system of ore mixing of sintering based on data mining and neuron network. Appling the method of cluster to the saved history data of the sintering process to classify the history data into three categories. And then, established neuron network model of each cluster. Every new sample should be put into the Corresponding cluster first by the distance of Eucliden, and then we can predict the quality index of the new sample by the neuron network model. The method is more effective and accurate, and it can extend to other production of the enterprise.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6244025,no
Design of the remote monitoring system for mine hoists,"In this paper we present a remote monitoring system for mine hoist based on computer technology, network technology and monitoring technology. The functional and structural design of the system is addressed. The system consists of data acquisition module, signal transmission module and software working on workstation and network server. An algorithm that can detect multiple faults simultaneously is proposed in detail. The software of the system is designed in a browser/server (Browse/Server, referred to as the B/S) architecture frame based on the Microsoft Visual Basic.NET platform. Such architecture for software is often used in industrial applications and it becomes the mainstream of remote monitoring and control architecture model. The validity and superiority of the proposed system are supported by field test.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6244567,no
Design and Implementation of a Comprehensive Information System for Detection of Urban Active Faults,"This paper presents a practical comprehensive information system for detecting urban active faults using WebGIS, data fusion, the Spatial Database Engine (SDE), network and RAID5, and other technologies. This system is designed to provide data and a platform for further quantitative analysis of active faults, to protect against and mitigate earthquake disasters, and to serve as a scientific reference for future urban planning, hazard prevention, and emergency response in Hangzhou, China. The system network adopts ordinary three-tier architecture, a Web server, WebGIS application server and database server, to maximize the performance of the overall architecture. The system software adopts a three-layer structure that consists of both C/S and B/S: C/S for LAN and B/S for WAN. The system comprises five models: system management, database management, image and data services, information distribution, and results management. The system employs ArcGIS GeoDataBase and ArcSDE to effectively organize, store and manage four-category formats and 16 sub-databases. Technical issues in system development, such as multi-source heterogeneous data fusion, massive data manipulation, and system security, are analyzed, their solutions and system realization are provided. This system has successfully been used in the last two years in urban planning, hazard prevention, and emergency response.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6245373,no
User-Centered Evaluation of Recommender Systems with Comparison between Short and Long Profile,"The growth of the social web poses new challenges and opportunities for recommender systems. The goal of Recommender Systems (RSs) is to filter information from a large data set and to recommend to users only the items that are most likely to interest and/or appeal to them. The quality of a RS is typically defined in terms of different attributes, the principal ones being relevance, novelty, serendipity and global satisfaction. Most existing works evaluate quality of Recommender Systems in terms of statistical factors that are algorithmically measured. This paper aims to explore whether (i) algorithmic measures of RS quality are in accordance with user-based measure and (ii) the user perceived quality of a RS is affected by the number of movies rated by the user. For this purpose we designed, developed and tested a web recommender system, TheBestMovie4You (http://www.moviers.it), which allows us to collect questionnaires about the quality of recommendations. We made a questionnaire and gave it to 240 subjects and we wanted to have as wide a set of users as possible using social web. In a experiment we asked the users to choose five movies (short profile), in a second to choose ten (long profile). Our results show that statistical properties fail in fully describing the quality of algorithms, because with user-centered metrics we can outline an algorithm's features that otherwise could not be detected. The comparison between the two phases highlighted a difference only in three cases out of twenty.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6245614,no
A Framework for User Feedback Based Cloud Service Monitoring,"The increasing popularity of the cloud computing paradigm and the emerging concept of federated cloud computing have motivated research efforts towards intelligent cloud service selection aimed at developing techniques for enabling the cloud users to gain maximum benefit from cloud computing by selecting services which provide optimal performance at lowest possible cost. Given the intricate and heterogeneous nature of current clouds, the cloud service selection process is, in effect, a multi criteria optimization or decision-making problem. The possible criteria for this process are related to both functional and nonfunctional attributes of cloud services. In this context, the two major issues are: (1) choice of a criteria-set and (2) mechanisms for the assessment of cloud services against each criterion for thorough continuous cloud service monitoring. In this paper, we focus on the issue of cloud service monitoring wherein the existing monitoring and assessment mechanisms are entirely dependent on various benchmark tests which, however, are unable to accurately determine or reliably predict the performance of actual cloud applications under a real workload. We discuss the recent research aimed at achieving this objective and propose a novel user-feedback-based approach which can monitor cloud performance more reliably and accurately as compared with the existing mechanisms.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6245621,no
Protein Structure Alignment: Is There Room for Improvement?,"Recent years have seen rapid development of methods for approximate and optimal solutions to the protein structure alignment problem. Albeit slow, these methods can be extremely useful in assessing the accuracy of more efficient, heuristic algorithms. We utilize a recently developed approximation algorithm for protein structure matching to demonstrate that a deep search of the protein superposition space leads to increased alignment accuracy with respect to many well established measures of alignment quality. The results of our study suggest that a large and important part of the protein superposition space remains unexplored by current techniques for protein structure alignment.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6245634,no
Assessing the Impact of E-Government on Users: A Case Study of Indonesia,"This paper aims to identify problems with the online information services for assessing impacts of the e-Government website. Some governments claimed that the phenomenal developing of e-Government system in some countries was driven mainly by the demand in the society especially of the phenomenal growth of computer and mobile phone users. However, this claim requires further validation with socio-economic and web usability approach to make clear what are actual factors? To achieve this goal, it is required to conduct analyzing some factors with broader socio-economic and usability framework, to find out the most dominant factors influenced the phenomenal growth of the e-Government systems and its users. Based on a quantitative analyses approach, this study analyzes user's preference to warde-Government system, and analysis factors influencing users of e-Government. This paper discusses the development of e-Government in Indonesia, and a case study to evaluate information quality, usability and impacts of the e-Government of Indonesia's central government institutions. The paper also describe designing evaluation sheet, data collection and analysis strategies of a case study in Indonesian centralgovernment's e-government portals evaluation.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6245661,no
Voice Quality Assessment and Visualization,"Commercial voice analysis systems are expensive and complicated. These equipments are only available at medical centers and need to be operated by well trained physicians. Thus the treatment costs of speech disorders are usually high. To improve this situation, we develop a software system dedicated to the assessment of human voice qualities. Our system is implemented in an ordinary personal computer. High precision electronic devices are not required for acquiring and analyzing voices. Therefore, the installation costs are low. Our system relies on four voice parameters to evaluate the qualities of voice signals. These parameters are fundamental frequency, jitter, shimmer, and harmonic-to-noise ratio. These measurements are widely used in otolaryngology to assess the pitch, variation of frequency, perturbation of amplitude, and harmony of human voice. Our system extracts these information from voice data and displays them by using graphical media. Consequently, the qualities of voice are more comprehensible. Users with little training and background knowledge can operate this system in their living rooms to assess their voices.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6245677,no
Fault-Tolerant Distributed Knowledge Discovery Services for Grids,"Fault tolerance is an important issue in service oriented architectures like Grid and Cloud systems, where many and heterogeneous machines are used. In this paper we present a flexible failure handling framework which extends a service-oriented architecture for Distributed Data Mining previously proposed, addressing the requirements for handling fault tolerance in service-oriented Grids. In particular, two different solutions are described and experimentally evaluated on a real Grid setting, aimed at assessing their effectiveness and performance.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6245693,no
Analysis of Gromacs MPI Using the Opportunistic Cloud Infrastructure UnaCloud,"This paper shows and analyzes the execution of a molecular dynamic application that uses Message Passing Interface (MPI) mechanism-Gromacs MPI-over a cloud infrastructure (UnaCloud) supported by desktop computers. The main objective is to find a solution to support Gromacs-MPI on UnaCloud. This coupling was carried out in order to predict and to redefine the Helicobacter pylori Cag A protein 3D structure. Although the structure of eight indigenous sequences was obtained, the handle of resource discovery and failure recovery on the opportunistic infrastructure was achieved manually, restricting the application scope of the solution. To eliminate these restrictions, a mechanism to automate the process execution on UnaCloud was identified and proposed.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6245724,no
Evaluating fault tolerance in security requirements of web services,"It is impossible to identify all of the internal and external security faults (vulnerabilities and threats) during the security analysis of web services. Hence, complete fault prevention would be impossible and consequently a security failure may occur within the system. To avoid security failures, we need to provide a measurable level of fault tolerance in the security requirements of target web service. Although there are some approaches toward assessing the security of web services but still there is no well-defined evaluation model for security improvement specifically during the requirement engineering phase. This paper introduces a measurement model for evaluating the degree of fault tolerance (FTMM) in security requirements of web services by explicitly factoring the mitigation techniques into the evaluation process which eventually contributes to required level of fault tolerance in security requirements. Our approach evaluates overall tolerance of the target service in the presence of the security faults through evaluating the computational security requirement model (SRM) of the service. We measure fault tolerance of the target web service by taking into consideration the cost, technical ability, impact and flexibility of the security goals established to mitigate the security faults.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6246125,no
Security metrics to improve misuse case model,"Assessing security at an early stage of the web application development life cycle helps to design a secure system that can withstand malicious attacks. Measuring security at the requirement stage of the system development life cycle assists in mitigating vulnerabilities and increasing the security of the developed system, which reduces cost and rework. In this paper, we present a security metrics model based on the Goal Question Metric approach, focusing on the design of the misuse case model. The security metrics model assists in examining the misuse case model to discover and fix defects and vulnerabilities before moving to the next stages of system development. The presented security metrics are based on the OWASP top 10-2010, in addition to misuse case modelling antipattern.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6246129,no
Component-based software system reliability allocation and assessment based on ANP and petri,"There are various dependencies in the component-based software systems. ANP can describe the complex structure of system using network representation for system structure. Therefore, it can assign reliability target to each component by comparing the relative importance to the users and considering constraints cost. However, ANP method can only estimate software system reliability statically and indirectly in the design stage. During the service process of software, how to assess the software system reliability, and further to validate the rationality of software reliability allocation in the design stage are dynamic problems, which require a dynamic method. In this paper, firstly, the dynamics model for state changes of software system reliability over time is developed using Petri network, then the software system reliability can be assessed dynamically combined with the dependency graph of components. Finally, the allocation and assessment for component-based software system reliability by ANP combining Petri network is presented and discussed by one example.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6246225,no
A Physical Model based research for fault diagnosis of gear crack,"Because of the advantage of gear, gearbox was used widely. But its fault also brought many losses of the production and society. It was necessary to research and analysis on the dynamical behavior of the gear system. One kind of gear fault was numerically simulated so that the influence of gear fault to change of vibration state was studied theoretically, and the symptom generated by faults was found. A finite element model with crack in gear roots was established with mixed meshing of singularity and isoparametric elements. The automatic analysis program of crack propagation was developed based on the ABAQUS software and linear elasticity fracture mechanics. The crack propagation of involutes' gear was simulated. The structural experiments of fault crack with different types and sizes were done, the vibration signals of the fault gear body and system were tested, and the dynamical characteristics of the structure and system were gained. The results were compared to that of simulation and theory, the validity of the theoretic and simulative was testified and the reliability was proved. There was important guidance for predicting the residual life of the gear with crack.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6246299,no
Design and development of the reliability prediction software for smart meters,"By analyzing the characteristics and specifying the reliability prediction process of smart meters, the reliability prediction software for smart meters is designed and developed in this paper. Three methods to predict the reliability of smart meters based on Telcordia SR-332, SN29500 and the combination of Telcordia SR-332 and SN29500 are provided in this software which can quickly calculate the reliability indicators including failure rate and MTTF on the component level, unit level and system level. The software consists of the data interface module, database management module, reliability prediction module, results output module and help module. The application of this software will help in shortening the development cycle and reducing the research and development cost of smart meters and will become a useful tool in achieving the reliability growth of smart meters.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6246308,no
Reliability study of the digital rocket-ground signal detection and analysis system,"The digital rocket-ground signal detection and analysis system is mainly used to remote and local real-time detect and acquire the signals from the key equipments of the control system of the carrier rockets. As a supporting product of the launch vehicles, this system has strict reliability requirements for the hardware and software. With the design of a multi-channel, integrated, real-time detection scheme, the methods of how to improve the hardware and software reliability of the system was discussed. The reliability of related hardware were expected with the method of stress analysis from the component level, unit circuit level, function modules and system-level, and the results show that the reliability of the system is great and can meet the system requirements.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6246321,no
Design of the electric properties test system of capacitance proximity fuze for HE,"In order to estimate the quality variety law of the stockpile capacitance proximity fuze for HE scientifically and provide a kind of technical instrument for the army to monitor the quality of the capacitance proximity fuze for HE, this article designs an electric properties test system of capacitance proximity fuze for HE. This test system is designed based on the virtual instrument technology and it uses a varactor network as the core. The article firstly analyses the basic principles of the capacitance fuze to detecting targets. Then the article points out that the essential of actualizing normal function of the fuze in laboratory conditions is to simulate the target function process on the fuze. The purpose to do this is to make the detector of the test fuze generate a variable demodulation signal just like the test fuze encountered a real target. Based on the above principle, this article puts forward the overall design scheme of the electric properties test system of capacitance proximity fuze for HE and detailedly designs the virtual instrument test platform, the signal conditioning circuit, the target effect simulation platform and the power supply. Taking the varactor network as the core, using the shielding box and the test interface equipment, the target effect simulation platform is constructed. On the basis of the virtual instrument technique, the test platform of the virtual instrument is constructed. According to modularizing design principle, the test software is developed on the platform of Lab VIEW edition 8.5. This article mainly designs modules of simulation bias voltage generation, target signatures signal fitting, data output and acquisition, data analyse and display, data storage and data called and displayed. Lastly, the engineering prototype of the test system is manufactured. In order to validate the function of the test system, we draw out stochastically 5 capacitance proximity fuzes for HE as the test object and finish the functionality - xperiment. The test results indicate that the designing scheme is reasonable, the technic is feasible, and the test system can satisfy the needs of electric properties test of the capacitance proximity fuze for HE.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6246424,no
Link-level reliability control for wireless electrocardiogram monitoring in indoor hospital,"Reliability is an essential quality of safety-critical wireless systems for medical applications. However, wireless links are typically prone to bursts of errors, with characteristics which vary over time. We propose a wireless system suitable for realtime remote patient monitoring in which the necessary reliability is achieved by an efficient error control in the link layer. We have paired an example electrocardiography application to this wireless system. We also developed a tool chain to determine the reliability, in terms of the packet-delivery ratio, for various combinations of system parameters. A realistic case study, based on data from the MIT-BIT arrhythmia database, shows that the proposed wireless system can achieve an appropriate level of reliability for electrocardiogram monitoring if link-level error control is correctly implemented.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6249361,no
On extended Similarity Scoring and Bit-vector Algorithms for design smell detection,"The occurrence of design smells or anti-patterns in software models complicate development process and reduce the software quality. The contribution proposes an extension to Similarity Scoring Algorithm and Bit-vector Algorithm, originally used for design patterns detection. This paper summarizes both original approaches, important differences between design patterns and anti-patterns structures, modifications and extensions of algorithms and their application to detect selected design smells.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6249814,no
Evaluation of FDTD modelling as a tool for predicting the response of UHF partial discharge sensors,"Ultra high frequency (UHF) partial discharge sensors are important tools for condition monitoring and fault location in high voltage equipment. There are many designs of UHF sensors which can detect electromagnetic waves that radiate from partial discharge sources. The general types of UHF PD sensors are disc, monopole, probe, spiral, and conical types with each type of sensor having different characteristics and applications. Computational modelling of UHF PD sensors using Finite-difference time-domain (FDTD) simulation can simplify the process of sensor design and optimisation, reducing the development cost for repeated testing (in order to select the best materials and designs for the sensors), and giving greater insight into how the mechanical design and mounting will influence frequency response. This paper reports on an investigation into the application of FDTD methods in modelling and calibrating UHF PD sensors. This paper focuses on the disc-type sensor where the sensor has been modelled in software and the predicted responses are compared with experimental measurements. Results indicate that the FDTD method can accurately predict the output voltages and frequency responses of disc-type sensors. FDTD simulation can reduce reliance upon costly experimental sensor prototypes, leading to quicker assessment of design concepts, improved capabilities and reduced development costs.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6251520,no
Microprocessor Soft Error Rate Prediction Based on Cache Memory Analysis,"Static raw soft-error rates (SER) of COTS microprocessors are classically obtained with particle accelerators, but they are far larger than real application failure rates that depend on the dynamic application behavior and on the cache protection mechanisms. In this paper, we propose a new methodology to evaluate the real cache sensitivity for a given application, and to calculate a more accurate failure rate. This methodology is based on the monitoring of cache accesses, and requires a microprocessor simulator. It is applied in this paper to the LEON3 soft-core with several benchmarks. Results are validated by fault injections on one implementation of the processor running the same programs: the proposed tool predicted all errors with only a small over-estimation.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6253280,no
Expertus: A Generator Approach to Automate Performance Testing in IaaS Clouds,"Cloud computing is an emerging technology paradigm that revolutionizes the computing landscape by providing on-demand delivery of software, platform, and infrastructure over the Internet. Yet, architecting, deploying, and configuring enterprise applications to run well on modern clouds remains a challenge due to associated complexities and non-trivial implications. The natural and presumably unbiased approach to these questions is thorough testing before moving applications to production settings. However, thorough testing of enterprise applications on modern clouds is cumbersome and error-prone due to a large number of relevant scenarios and difficulties in testing process. We address some of these challenges through Expertus---a flexible code generation framework for automated performance testing of distributed applications in Infrastructure as a Service (IaaS) clouds. Expertus uses a multi-pass compiler approach and leverages template-driven code generation to modularly incorporate different software applications on IaaS clouds. Expertus automatically handles complex configuration dependencies of software applications and significantly reduces human errors associated with manual approaches for software configuration and testing. To date, Expertus has been used to study three distributed applications on five IaaS clouds with over 10,000 different hardware, software, and virtualization configurations. The flexibility and extensibility of Expertus and our own experience on using it shows that new clouds, applications, and software packages can easily be incorporated.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6253496,no
Formalizing the Cloud through Enterprise Topology Graphs,"Enterprises often have no integrated and comprehensive view of their enterprise topology describing their entire IT infrastructure, software, on-premise and off-premise services, processes, and their interrelations. Especially due to acquisitions, mergers, reorganizations, and outsourcing there is no clear 'big picture' of the enterprise topology. Through this lack, management of applications becomes harder and duplication of components and information systems increases. Furthermore, the lack of insight makes changes in the enterprise topology like consolidation, migration, or outsourcing more complex and error prone which leads to high operational cost. In this paper we propose Enterprise Topology Graphs (ETG) as formal model to describe an enterprise topology. Based on established graph theory ETG bring formalization and provability to the cloud. They enable the application of proven graph algorithms to solve enterprise topology research problems in general and cloud research problems in particular. For example, we present a search algorithm which locates segments in large and possibly distributed enterprise topologies using structural queries. To illustrate the power of the ETG approach we show how it can be applied for IT consolidation to reduce operational costs, increase flexibility by simplifying changes in the enterprise topology, and improve the environmental impact of the enterprise IT.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6253574,no
An anti-islanding for multiple photovoltaic inverters using harmonic current injections,"Islanding phenomenon is one of the major concerns in the distributed generator (DG) systems. Islanding occurs when one or more DGs supplied local loads without connecting to the grid utility. This unintentional condition must be detected with the minimum time possible to prevent the hazardous effects to a person worked in the system or equipment connected to the network. The two main methods of anti-islanding are passive and active methods. The passive methods are excellent in the speed of detection and power quality. However, the passive have relatively large non-detection zone (NDZ). On the other hands, the active methods have less relatively small NDZ and still provide a good speed of detection. This paper proposes an anti-islanding technique for multiple grid-connected inverters in photovoltaic (PV) system based on an active method. By injecting harmonic currents with the same or different harmonic orders and monitoring the change at the point of common coupling (PCC), the islanding condition can be diagnosed. Due to its excellent algorithm looking at specific frequencies, the Goertzel algorithm is employed in this paper to identify frequency components. Simulation results using PSIM software confirm the effectiveness to the technique in accordance with the requirements of the interconnection standards, IEEE Std. 1547 and IEEE Std. 9292000.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6254190,no
Analysis of islanding detection methods for grid-connected distributed generation in provincial electricity authority distribution systems,"This paper presents a comparison study of two major distributed generation islanding detection methods including rate of change of frequency (ROCOF) and over/under frequency (OUF) techniques which could be used for Provincial Electricity Authority (PEA) distribution systems. The analysis is based on the detection time in different scenarios and false operations. Two practical distribution systems including 133-bus and 54-bus systems from PEA are used as case studies and DIgSILENT PowerFactory software is used as simulation tool. Simulation and test results show that ROCOF technique can detect islanding phenomena faster than OUF technique when the system has only one distributed generation. However, ROCOF technique is more sensitive to false operation than OUF technique.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6254253,no
Design of fault detection system for high voltage cable,"This paper proposes the design of an algorithm to detect fault in underground cable for blocking auto-reclosing in distribution feeder of Metropolitan Electricity Authority (MEA). The algorithm consists of two major steps. First, overcurrent detection algorithm is applied to compare fault current with current setting. Next process, the impedance detection is used to identify the fault location base on comparing between the measuring and the setting of impedance value. Finally, these results (current & impedance) are used to make a decision of auto-recloser blocking. This paper also shows the results from software simulation and hardware simulation.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6254364,no
Bridge deck survey with high resolution Ground Penetrating Radar,"Ground Penetrating Radar applications for structure surveying started to grow in the 1980s; amongst these, initial civil engineering applications included condition assessment of highway pavements and their foundations, with applications to structural concrete focusing on inspection of bridge decks. There are many factors that can cause or contribute to the damage of the top layer of concrete in bridge decks including the corrosion of steel rebar, freeze and thaw cycles, traffic loading, initial damage resulting from poor design and/or construction, and inadequate maintenance. When applied to the analysis of bridge decks, GPR can be successfully used for detecting internal corrosion of steel reinforcement within the concrete deck, which can be an indicator of poor quality overlay bonding or delamination at the rebar level. Therefore, this equipment has the ability to gain information about the condition of bridge decks in a more rapid and less costly fashion than coring and will perhaps yield a more reliable assessment than current geotechnical procedures. However, this application requires suitably designed equipment; for instance, optimization of antenna orientation to take advantage of signal polarization is an important feature for successfully locating reinforcing bars in a time-depth slice. Novel equipment has recently been developed to enable the nondestructive analysis of bridge decks; the IDS RIS Hi-Bright runs two arrays of high frequency sensors featuring a rapid, but very dense data collection, thus dramatically increasing the resolution of the GPR survey. Antenna dipoles in these arrays are deployed to collect two data sets with orthogonal antenna orientations, one with the electric field parallel to the scanning direction (VV), the other perpendicular to it (HH); in this way, the equipment is capable of collecting 16 profiles, 10 cm spaced in a single swath, thus collecting an incredible amount of information. Dedicated data analysis software provides- a 2-D tomography of the underground layers and a 3-D view of the surveyed volume. Main output include the determination of pavement and concrete thickness, the detection of moist areas as well as concrete damage and the location of rebars and ducts within the concrete slab.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6254915,no
A study of the interference stripe phenomenon caused by electromagnetic wave propagating in multi-layered medium,"The interference stripe appears in the penetrating scan experiments of multi-layered medium detection, the experiments are implemented by the single-frequency plane array or the moved single transmitting/receiving (T/R) antenna. Based on the medium boundary conditions of Maxwell equations from the classic electromagnetic theory, the interference stripe phenomenon is studied in the paper. And the commercial simulation software CST Microwave Studio is used to construct the simulation models. Under the near-field condition, the propagation and reflection of electromagnetic wave in the monolayer medium and the multi-layered media are simulated. Two situations, the electromagnetic wave being perpendicular and oblique to the dielectric plate, are specially carried out to compare. The echo waves received by the antenna changed position are analyzed. It can be found that there is no interference stripe when the incident wave is perpendicular to the dielectric plate. However, the interference stripe phenomenon will arise when there is a certain angle between the incident wave and the normal of the dielectric plate. This can be proved by the scan experiments results. The existence of the interference stripe will cause serious disturbance to the target echo, because the former is greater than the latter. So it will affect the performance of target detection and identification. From the point of view of the electromagnetic wave incidence and reflection theorem, the existence reason of this phenomenon is discussed. The relationship between the incident angle and interference stripe is illuminated. The study of this phenomenon will be helpful for the signal processing of the plane array and the moved single T/R antenna scanning system, such as the interference stripe phenomenon reducing and the target imaging quality improving. It can also be helpful for solving some problems encountered in the highways detecting, archeology, geology survey, etc.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6254966,no
Pipe condition assessments using Pipe Penetrating Radar,"This paper describes the development of Pipe Penetrating Radar (PPR), the underground in-pipe application of GPR, a non-destructive testing method that can detect defects and cavities within and outside mainline diameter (>;18 in/450mm) non-metallic (concrete, PVC, HDPE, etc.) underground pipes. The method uses two or more high frequency GPR antennae carried by a robot into underground pipes. The radar data are transmitted to the surface via fibre optic cable and are recorded together with the output from CCTV (and optionally sonar and laser). Proprietary software analyzes the data and pinpoints defects or cavities within and outside the pipe. Thus the testing can identify existing pipe and pipe bedding symptoms that can be addressed to prevent catastrophic failure due to sinkhole development and can provide useful information about the remaining service life of the pipe, enabling accurate predictability of needed intervention or the timing of replacement. This reliable non-destructive testing method significantly impacts subsurface infrastructure condition based asset management by supplying previously unattainable measurable conditions.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6254979,no
An evolutionary algorithm for optimization of affine transformation parameters for dose matrix warping in patient-specific quality assurance of radiotherapy dose distributions,"Patient-specific quality assurance for sophisticated treatment delivery modalities in radiation oncology, such as intensity-modulated radiation therapy (IMRT), necessitates the measurement of the delivered dose distribution, and the subsequent comparison of the measured dose distribution with that calculated by the treatment planning software. The degree to which the calculated dose distribution is reproduced upon delivery is an indication that the treatment received by the patient is acceptable. A new method for the comparison between the planned and delivered dose distributions is introduced; it assesses the agreement between planar two-dimensional dose distributions. The method uses an evolutionary algorithm to optimize the parameters of the affine transformation, consisting of translation, rotation, and dilation (or erosion), that will warp the measured dose distribution to the planned dose distribution. The deviation of the composite transformation matrix from the identity matrix is an indication of an average geometrical error in the delivered dose distribution, and represents a systematic error in dose delivery. The residual errors in radiation dose at specific points are local errors in dose delivery. The method was applied to IMRT fields consisting of horizontal intensity bands. Analysis shows the method to be a promising tool for patient-specific quality assurance.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6256139,no
Dual scheme based mathematical modeling of Magnetically Controlled Shunt Reactors 6500 kV,"Topicality of mathematic modeling of Magnetically Controlled Shunt Reactors (MCSR) as a network element using common existing software elements is shown in the article. Principles of the MCSR dual scheme creation as a thyristor controlled reactor or a static var compensator (in case of modeling of Source of Reactive Power, including a MCSR and a capacitor bank) are explained. Principles of development the mathematic model of the MCSR dual scheme are demonstrated. Example of development and application of MCSR dual mathematical model for calculating steady-state modes of high-voltage electric grid is shown. The potential of the MCSR dual mathematic model application for development of the algorithm of MCSR (SRP) automatic control at the point of connection to electrical grid is assessed.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6256197,no
Development of the algorithm and software MVES-TV 2012 for assessment of touch voltage in MV networks with compensated neutral earthing,"The paper presents an algorithm for assessment of touch voltage in medium voltage networks with compensated neutral earthing. Proposed algorithm takes into account power line type (overhead line, underground cable line), earth fault value, number of substations, etc. Based on this algorithm, is developed a new software MVES-TV 2012 (Medium Voltage Electrical System Touch Voltage) by the authors of this paper. Calculation with this software ensures the safety of human life in any switchgear place to which persons have legitimate accesses. This software can be useful for distribution network project and exploiting engineers when trying to assess touch voltage in medium voltage networks with compensated neutral earthing.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6256228,no
Trustworthy Web Service Selection Using Probabilistic Models,"Software architectures of large-scale systems are perceptibly shifting towards employing open and distributed computing. Service Oriented Computing (SOC) is a typical example of such environment in which the quality of interactions amongst software agents is a critical concern. Agent-based web services in open and distributed architectures need to interact with each other to achieve their goals and fulfill complex user requests. Two common tasks are influenced by the quality of interactions among web services: the selection and composition. Thus, to ensure the maximum gain in both tasks, it is essential for each agent-based web service to maintain a model of its environment. This model then provides a means for a web service to predict the quality of future interactions with its peers. In this paper, we formulate this model as a machine learning problem which we analyze by modeling the trustworthiness of web services using probabilistic models. We propose two approaches for trust learning of single and composed services; Bayesian Networks and Mixture of Multinomial Dirichlet Distributions (MMDD). The effectiveness of our approaches is empirically assessed using a simulation study. Our results show that representing the quality of a web service by Multinomial Dirichlet Distribution (MDD) provides high flexibility and accuracy in modeling trust. They also show that using our approaches to estimate trust enhances web services selection and composition.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6257785,no
A Hybrid Diagnosis Approach for QoS Management in Service-Oriented Architecture,"Service flow in SOA systems need to detect quality of service (QoS) problems and to guarantee end-to-end performance. In previous work, we have proposed two faulty service identification methods: a dependency matrix based diagnosis and a Bayesian network based diagnosis. In this paper, we present a hybrid diagnosis to achieve high diagnosis accuracy and low diagnosis cost. The hybrid diagnosis reduces the problem size by applying dependency matrix based diagnosis result in Bayesian network and excluding services that are not critical to the end-to-end QoS from the diagnosis. Our experimental results show that the accuracy of the hybrid diagnosis is similar to the Bayesian network diagnosis yet reduces more than 90% of the diagnosis time.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6257793,no
Location-Aware Collaborative Filtering for QoS-Based Service Recommendation,"Collaborative filtering is one of widely used Web service recommendation techniques. In QoS-based Web service recommendation, predicting missing QoS values of services is often required. There have been several methods of Web service recommendation based on collaborative filtering, but seldom have they considered locations of both users and services in predicting QoS values of Web services. Actually, locations of users or services do have remarkable impacts on values of QoS factors, such as response time, throughput, and reliability. In this paper, we propose a method of location-aware collaborative filtering to recommend Web services to users by incorporating locations of both users and services. Different from existing user-based collaborative filtering for finding similar users for a target user, instead of searching entire set of users, we concentrate on users physically near to the target user. Similarly, we also modify existing service similarity measurement of collaborative filtering by employing service location information. After finding similar users and services, we use the similarity measurement to predict missing QoS values based on a hybrid collaborative filtering technique. Web service candidates with the top QoS values are recommended to users. To validate our method, we conduct series of large-scale experiments based on a real-world Web service QoS dataset. Experimental results show that the location-aware method improves performance of recommendation significantly.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6257808,no
An Approach of QoS-Guaranteed Web Service Composition Based on a Win-Win Strategy,"In Web service composition, the benefit conflicts between the user and the service provider ask the so-called both win to be supported. To address this concern, a novel QoS-guaranteed service composition approach based on a win-win strategy is proposed in this paper. First, a QoS model based on the probability interval is built to adapt to the dynamic nature of the Internet, and a corresponding user satisfaction evaluation method is designed. Next, a mathematical model of service composition based on game theory is proposed. Finally, a Genetic Algorithm (GA) is used to search an appropriate composite service with the Pareto optimum under the Nash equilibrium on both the user utility and the service provider utility achieved or approached. Simulation results have shown that the proposed approach is feasible and effective.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6257930,no
Extending the Reliability of Wireless Sensor Networks through Informed Periodic Redeployment,"This paper investigates the reliability of wireless sensor networks, deployed over a square area, in regards to two aspects: network connectivity and node failures. Analyzing the phenomenon known as the border effects on the connectivity of such networks, we derive exact expressions for the expected effective connectivity degree of border nodes. We show that the relative average number of neighbors for nodes in the borders is independent of the node transmission range and of the overall network node density. Assuming a network composed of N uniformly distributed nodes over a square area of side L, our simulation experiments demonstrate that the connectivity of the overall network is dominated by the average node degree in the corner borders of the square network area. Using this result, and considering sensor node failure rates, we derive analytical expressions for the mean time to disconnect (MTTD) and the mean number of sensors remaining (MNSR) upon disconnection for a given network. For precise reliability estimates we also calculate the sensor redeployment period T and the number of sensors per redeployment N, that should be effected in order to keep the network continuously connected with probability higher than 99%. We then run additional simulations for a network subject to sensors failures to obtain experimental MTTD and MNSR values which we found to be very close to the analytically derived ones. These experiments also ratified that periodic sensor redeployments characterized by the pair (N, T), resulting from our analysis, can continuously extend the reliability of wireless sensor networks.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6258306,no
TIL: Mutation-based Statistical Test Inputs Generation for Automatic Fault Localization,"Automatic Fault Localization (AFL) is a process to locate faults automatically in software programs. Essentially, an AFL method takes as input a set of test cases including failed test cases, and ranks the statements of a program from the most likely to the least likely to contain a fault. As a result, the efficiency of an AFL method depends on the """"quality"""" of the test cases used to rank statements. More specifically, in order to improve the accuracy of their ranking within test budget constraints, we have to ensure that program statements are executed by a reasonably large number of test cases which provide a coverage as uniform as possible of the input domain. This paper proposes TIL, a new statistical test inputs generation method dedicated to AFL, based on constraint solving and mutation testing. Using mutants where the locations of injected faults are known, TIL is able to significantly reduce the length of an AFL test suite while retaining its accuracy (i.e., the code size to examine before spotting the fault). In order to address the motivations stated above, the statistical generator objectives are two-fold: 1) each feasible path of the program is activated with the same probability, 2) the sub domain associated to each feasible path is uniformly covered. Using several widely used ranking techniques (i.e., Tarantula, Jaccard, Ochiai), we show on a small but realistic program that a proof-of-concept implementation of TIL can generate test sets with significantly better fault localization accuracy than both random testing and adaptive random testing. We also show on the same program that using mutation testing enables a 75% length reduction of the AFL test suite without decrease in accuracy.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6258309,no
Semi-Automatic Security Testing of Web Applications from a Secure Model,"Web applications are a major target of attackers. The increasing complexity of such applications and the subtlety of today's attacks make it very hard for developers to manually secure their web applications. Penetration testing is considered an art, the success of a penetration tester in detecting vulnerabilities mainly depends on his skills. Recently, model-checkers dedicated to security analysis have proved their ability to identify complex attacks on web-based security protocols. However, bridging the gap between an abstract attack trace output by a model-checker and a penetration test on the real web application is still an open issue. We present here a methodology for testing web applications starting from a secure model. First, we mutate the model to introduce specific vulnerabilities present in web applications. Then, a model-checker outputs attack traces that exploit those vulnerabilities. Next, the attack traces are translated into concrete test cases by using a 2-step mapping. Finally, the tests are executed on the real system using an automatic procedure that may request the help of a test expert from time to time. A prototype has been implemented and evaluated on Web Goat, an insecure web application maintained by OWASP. It successfully reproduced Role-Based Access Control (RBAC) and Cross-Site Scripting (XSS) attacks.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6258315,no
An Investigation of Classification-Based Algorithms for Modified Condition/Decision Coverage Criteria,"During software development, white-box testing is used to examine the internal design of the program. One of the most important aspects of white-box testing is the code coverage. Among various test coverage measurements, the Modified Condition/Decision Coverage (MC/DC) is a structural coverage measure and can be used to assess the adequacy and quality of the requirements-based testing (RBT) process. NASA has proposed a method to select the needed test cases for satisfying this criterion. However, there may have some flaws in NASA's method. That is, the selected test cases may not satisfy the original definition of the MC/DC criterion in some particular situations and perhaps can not detect errors completely. On the other hand, NASA's method may be hard to detect some operator errors. For example, we may not be able to detect the incorrectly coding or for xor in some cases. Additionally, this method is too complex and could take a lot of time to obtain the needed test cases. In this paper, we will propose a classification-based algorithm to select the needed test cases. First, test cases will be classified based on the outcome value of expression and the target condition. After classifying all test cases, MC/DC pairs can be found quickly, conveniently and effectively. Also, if there are some missing (unfound) test cases, our proposed classification-based method can also suggest to developers what kinds of test cases have to be generated. Finally, some experiments are performed based upon real programs to evaluate the performance and effectiveness of our proposed classification-based algorithm.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6258460,no
Study of Safety Analysis and Assessment Methodology for AADL Model,"This paper focuses on safety model of embedded system architecture using AADL (Architecture Analysis and Design Language). For further integration of safety analysis and system modeling, we propose a new approach to evaluate and assess the safety property of embedded systems quantitatively. We establish the safety model of embedded systems by extending AADL with fault model, identify causal relationships between elementary failure modes, put forward the formal method to transform this safety model to DSPN (Deterministic Stochastic Petri Net) model for quantitative analysis and made transforming rules to support safety assessment automatically. A fire alarm system is discussed for further explanation.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6258466,no
Paradigm in Verification of Access Control,"Access control (AC) is one of the most fundamental and widely used requirements for privacy and security. Given a subject's access request on a resource in a system, AC determines whether this request is permitted or denied based on AC policies (ACPs). This position paper introduces our approach to ensure the correctness of AC using verification. More specifically, given a model of an ACP, our approach detects inconsistencies between models, specifications, and expected behaviors of AC. Such inconsistencies represent faults (in the ACP), which we target at detecting before ACP deployment.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6258470,no
A magnitude-phase detection method for grid-connected voltage of wind power generation system,"In order to keep low-voltage ride-through under the grid faults for wind power generation system, the magnitude and phase information of the system grid-connected voltage must be detected fleetly and precisely so that the control subsystem of wind power generation system can tackle the faults. The conventional three-phase software phase-locked loop (SPLL) has a low speed of dynamic response because of the effects of negative-sequence and harmonic components. Also it can not exactly extract the fundamental positive-sequence magnitude and phase of three-phase voltage. This paper proposes a novel three-phase magnitude-phase detection method based on PQR transformation. The proposed method can decouple the fundamental positive-sequence component and negative-sequence component of three-phase voltage, and detect the magnitudes and phases of them respectively. The simulation results verify that the proposed method can overcome the shortcomings of conventional three-phase SPLL, track the magnitude and phase information of the fundamental positive-sequence voltage accurately and improve the speed of dynamic response effectively under the bad harmonic condition. The novel magnitude-phase detection method can be used to detect exactly the grid-connected voltage of wind power generation system.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6260104,no
Low voltage testing for interconnect opens under process variations,"Advances in test methodologies to deal with subtle behavior of some defects mechanisms as the technology scale are required. Among these interconnect opens are an important defect mechanism that requires detailed knowledge of its physical properties. Furthermore, in nanometer process variability is predominant and considering only nominal value of parameters is not realistic. In this work the detection capability of Low Voltage Testing for interconnect opens, considering process variations, is evaluated using a statistical model. To account for this the Probability of Detection of the defect is obtained. The proposed methodology is implemented in a software tool to determine the probability of detection of via opens for some ISCAS85 benchmark circuits. The results suggest that using Low Vdd in conjunction with favorable test vectors allow to improve the Probability of Detection of interconnect opens leading to better test quality.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6261231,no
Non-intrusive fault tolerance in soft processors through circuit duplication,"The flexibility introduced by Commercial-Off-The-Shelf (COTS) SRAM based FPGAs in on-board system designs make them an attractive option for military and aerospace applications. However, the advances towards the nanometer technology come together with a higher vulnerability of integrated circuits to radiation perturbations. In mission critical applications it is important to improve the reliability of applications by using fault-tolerance techniques. In this work, a non-intrusive fault tolerance technique has been developed. The proposed technique targets soft processors (e.g. LEON3), and its detection mechanism uses a Bus Monitor to compare output data of a main soft-processor with its redundant module. In case of a mismatch, an error signal is activated, triggering the proposed fault tolerance strategy. This approach shows to be more efficient than the state-of-the-art Triple Modular Redundancy (TMR) and Software Implemented Hardware Fault Tolerance (SIHFT) approaches in order to detect and to correct faults on the fly with low area overhead and with no major performance penalties. The chosen case study is an under development On-Board Computer (OBC) system, conceived to be employed in future missions of the Brazilian Institute of Space Research (INPE).",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6261264,no
Modelling and symmetry reduction of a target-tracking protocol using wireless sensor networks,"To achieve precise modelling of real-time systems stochastic behaviours are considered which lead towards probabilistic modelling. Probabilistic modelling has been successfully employed in wide array of application domains including, for example, randomised distributed algorithms, communication, security and power management protocols. This study is an improvement over our previous work, which was based on the probabilistic analysis of a cluster-based fault tolerant target-tracking protocol (FTTT) using only grid-based sensor nodes arrangement. Probabilistic modelling is chosen for the analysis of FTTT protocol to facilitate benefits of symmetry reduction in conjunction with modelling. It is believed that for the first time correctness of the simplified version of a target-tracking protocol is verified by developing its continuous-time Markov chain (CTMC) model using symbolic modelling language. The proposed probabilistic model of a target-tracking wireless sensor networks will help to analyse the phases of FTTT protocol on a limited scale with finite utilisation of time. There are three main contributions of this study; first consideration of synchronised events between the modules, second, random placement of sensor nodes is taken into account in addition to grid-based sensor node arrangement, third one is the reduction in state space size through symmetry reduction technique, which also facilitates to analyse a larger size network. Symmetry reduction on Probabilistic Symbolic Model (PRISM) checker models is performed by PRISM-symm and the generic representatives in PRISM (GRIP) tool. Modelling of FTTT protocol is proved better with the usage of PRISM-symm after comparing the results of PRISM model, PRISM-symm and GRIP.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6261625,no
Available car parking space detection from webcam by using adaptive mixing features,"This paper presents a robust approach for detection of available car parking spaces. With low quality of video camera as webcam and dynamic change of light around the car parking, it is hard to accurately detect or recognize the cars. Moreover the proposed appearance-based approach is efficient than recognition-based approach because it do not need to learn a huge of multi-view objects. In this paper, we propose adaptive background model-based object detection with dynamic mixing features of masked-area and edge orientation histogram (EOH) density. The average variance of variance of intensity change for dynamic background model is used to change ratio of mixing features dynamically. The masked-area density is density of predefined area of a parking slot that is weighted by Gaussian mask to robust density computation and the edge orientation histogram (EOH) density is density of the EOH in the predefined area that can be used under low contrast image as night scene. The experiments are performed both in simulation model and real scenes. The results show the proposed approach can handle dynamic change of light efficiently.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6261917,no
Neural computing-based pedestrian detection from image sequence,"Preventing a traffic accident is a good way to solve many problems in the world for the current generation surrounding with many automotive technologies causing many people's death from the accident. The prevention makes an important impact to every society for making many people more safety and improving their lives' quality. In the fact, the primary cause is mostly drivers' carelessness and lacking of control, which might be because of addiction of drugs, resulting that a pedestrian walking on a road pains and then becomes dead. Therefore the problem can be solved using a computer for analysis and making a decision as a human called pedestrian detection. This study uses many several enhancement algorithms and processes for detecting integrated with an analysis based on a neural computing to decide whether a detected object is a pedestrian. Moreover the study shows an experimental result that the procedure is sufficient and efficient to be a fundamental knowledge for the next related work including a real-life traffic situation.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6261921,no
A software monitoring framework for quality verification,"Software functional testing can unveil a wide range of potential malfunctions in applications. However, there is a significant fraction of errors that will be hardly detected through a traditional testing process. Problems such as memory corruptions, memory leaks, performance bottlenecks, low-level system call failures and I/O errors might not surface any symptoms in a tester's machine while causing disasters in production. On the other hand, many handy tools have been emerging in all popular platforms allowing a tester or an analyst to monitor the behavior of an application with respect to these dark areas in order to identify potential fatal problems that would go unnoticed otherwise. Unfortunately, these tools are not yet in widespread use due to few reasons. First, the usage of tools requires a certain amount of expertise on system internals. Furthermore, these monitoring tools generate a vast amount of data even with elegant filtering and thereby demand a significant amount of time for an analysis even from experts. As the end result, using monitoring tools to improve software quality becomes a costly operation. Another facet of this problem is the lack of infrastructure to automate recurring analysis patterns. This paper describes the current state of an ongoing research in developing a framework that automates a significant part of the process of monitoring various quality aspects of a software application with the utilization of tools and deriving conclusions based on results. According to our knowledge this is the first framework to do this. It formulates infrastructure for analysts to extract relevant data from monitoring tool logs, process those data, make inferences and present analysis results to a wide range of stakeholders in a project.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6261971,no
UMAM-Q: An instrument to assess the intention to use software development methodologies,"The Software Engineering discipline has devoted much effort to the definition of new methods and paradigms that, even if empirically proven to provide certain gains in terms of process productivity and product quality, are difficult to transfer to industry. We claim that this fact is largely due to methodologists not taking into account the - largely subjective - set of variables that influence innovation adoption, together with its tailoring and operationalization to the particulars of software methodologies: we lack reliable and valid measurement instruments that allow for the early detection of methodologies weaknesses with respect to their ability to catch among practitioners. This paper reports on the development of one such instrument designed to measure the various perceptions that an individual may have with respect to adopting a software development methodology innovation. Our questionnaire is aimed at allowing methodologists not only to compare their methodologies with respect to others - in terms of variables such as ease of use, usefulness or compatibility - but also to avoid well-known mistakes such as forcing - as opposed to convincing - method adoption in organizations.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6263157,no
TRHIOS: Trust and reputation in hierarchical and quality-oriented societies,"In this paper we present TRHIOS: a Trust and Reputation system for HIerarchical and quality-Oriented Societies. We focus our work on hierarchical medical organizations. The model estimates the reputation of an individual, R<sub>TRHIOS</sub>, taking into account information from three trust dimensions: the hierarchy of the system; the source of information; and the quality of the results. Besides the concrete reputation value, it is important to know how reliable that value is; for each of the three dimensions we calculate the reliability of the assessed reputations; and aggregating them, the reliability of the reputation of an individual. The modular approach followed in the definition of the different types of reputations provides the system with a high flexibility that allows adapting the model to the peculiarities of each society.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6263223,no
A numerical simulation study on the CO<inf>2</inf> leakage through the fault,"Many carbon capture and storage projects are underway all over the world to reduce greenhouse gas emissions. In this study, we numerically analyze the movement of injected CO<sub>2</sub> through the faults undetected prior to injection. We use TOUGH2-MP ECO2N software to estimate the behavior of injected and leaked CO<sub>2</sub>. The storage site is 100 m thick saline aquifer located 850 m under the shallow continental shelf. It is assumed that CO<sub>2</sub> is first leaked through the 1<sup>st</sup> fault located 400 m away from injection well and then leaked to the seabed through the 2<sup>nd</sup> fault located 400 m away from 1<sup>st</sup> fault. We vary the injection rate of CO<sub>2</sub> to 0.25, 0.50, and 0.75 MtCO2/year to analyze the effect of injection rate. For 0.25 MtCO<sub>2</sub>/year injection rate, no leakage is calculated, however, for 0.50 and 0.75 MtCO<sub>2</sub>/year, the leakages of CO<sub>2</sub> to seabed are detected. The starting times of leakage at 0.5 and 0.75 MtCO<sub>2</sub>/year injection rates are 22.9 and 17.8 years, respectively. The ratios of total leaked CO<sub>2</sub> to total injected CO<sub>2</sub> at 0.5 and 0.75 MtCO<sub>2</sub>/year injection rates are 8% and 16.4%, respectively.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6263432,no
Automatic fault characterization via abnormality-enhanced classification,"Enterprise and high-performance computing systems are growing extremely large and complex, employing many processors and diverse software/hardware stacks. As these machines grow in scale, faults become more frequent and system complexity makes it difficult to detect and to diagnose them. The difficulty is particularly large for faults that degrade system performance or cause erratic behavior but do not cause outright crashes. The cost of these errors is high since they significantly reduce system productivity, both initially and by time required to resolve them. Current system management techniques do not work well since they require manual examination of system behavior and do not identify root causes. When a fault is manifested, system administrators need timely notification about the type of fault, the time period in which it occurred and the processor on which it originated. Statistical modeling approaches can accurately characterize normal and abnormal system behavior. However, the complex effects of system faults are less amenable to these techniques. This paper demonstrates that the complexity of system faults makes traditional classification and clustering algorithms inadequate for characterizing them. We design novel techniques that combine classification algorithms with information on the abnormality of application behavior to improve detection and characterization accuracy significantly. Our experiments demonstrate that our techniques can detect and characterize faults with 85% accuracy, compared to just 12% accuracy for direct applications of traditional techniques.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6263926,no
BLOCKWATCH: Leveraging similarity in parallel programs for error detection,"The scaling of Silicon devices has exacerbated the unreliability of modern computer systems, and power constraints have necessitated the involvement of software in hardware error detection. Simultaneously, the multi-core revolution has impelled software to become parallel. Therefore, there is a compelling need to protect parallel programs from hardware errors. Parallel programs' tasks have significant similarity in control data due to the use of high-level programming models. In this study, we propose BLOCKWATCH to leverage the similarity in parallel program's control data for detecting hardware errors. BLOCKWATCH statically extracts the similarity among different threads of a parallel program and checks the similarity at runtime. We evaluate BLOCKWATCH on seven SPLASH-2 benchmarks to measure its performance overhead and error detection coverage. We find that BLOCKWATCH incurs an average overhead of 16% across all programs, and provides an average SDC coverage of 97% for faults in the control data.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6263959,no
Power quality analysis based on LABVIEW for current power generation system,"With the development of the current power generation technology, power quality of the generation system becomes a new problem, because of ocean energy variable and converters with large power electronic device applied. A real-time signal detection and harmonic analysis is great significance to improve power quality. The paper mainly proposes how to use LABVIEW software platform to develop a power quality detection and analysis system to detect the signals from the current power generation. Moreover, a simulation model of the current power generation using MATLAB is built to generate the analog signals, so we can also monitor the analog signals with the power quality system. The system is successfully designed to detect the electric power quality indexes after test results.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6264479,no
6th workshop on recent advances in intrusion tolerance and reSilience (WRAITS 2012),"Now entering its sixth consecutive year, the last four being in conjunction with DSN, the primary theme of WRAITS is intrusion tolerance (IT for short). IT starts with the premise that software-based components will always contain bugs and misconfigurations that can be discovered, exposed and enabled by the increasingly new ways in which distributed and networked computer systems are being created today. IT acknowledges that it is impossible to completely prevent intrusions and attacks, and it is often impossible to accurately detect the act of intrusion and stop it early enough. Intrusion tolerant systems therefore must have the means to continue to operate correctly despite attacks and intrusions, and deny the attacker/intruder the success they seek as much as possible. For instance, an intrusion tolerant system may suffer loss of service or resources due to the attack but it may continue to provide critical services in a degraded mode or trigger automatic mechanisms to regain and recover the compromised services and resources. Other descriptions used for similar themed research include Survivability, Resilience, Trustworthy Systems, Byzantine Fault Tolerance, and Autonomic Self-Healing Systems. Indeed, this year's workshop has been slightly renamed from its predecessors (by also including reSilience in the title) to explicitly underscore the breadth of the topics involved.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6264678,no
TinyChecker: Transparent protection of VMs against hypervisor failures with nested virtualization,"The increasing amount of resources in a single machine constantly increases the level of server consolidation for virtualization. However, along with the improvement of server efficiency, the dependability of the virtualization layer is not being progressed towards the right direction; instead, the hypervisor level is more vulnerable to diverse failures due to the increasing complexity and scale of the hypervisor layer. This makes tens to hundreds of production VMs in a machine easily risk a single point of failure. This paper tries to mitigate this problem by proposing a technique called TinyChecker, which uses a tiny nested hypervisor to transparently protect guest VMs against failures in the hypervisor layer. TinyChecker is a very small software layer designated for transparent failure detection and recovery, whose reliability can be guaranteed by its small size and possible further formal verification. TinyChecker records all the communication context between VM and hypervisor, protects the critical VM data, detects and recovers the hypervisors among failures. TinyChecker is currently still in an early stage, we report our design consideration and initial evaluation results.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6264691,no
The application of decay rate analysis for WSN buffer dimensioning,"The provisioning of a Quality of Service (QoS) for applications requiring real-time service demands low latency, low Bit Error Rate (BER) and bounded end to end delay guarantees. Facilitating this within a Wireless Sensor Network (WSN) is Guaranteed Time slots (GTS's) activated using a beacon enabled mode and under the governance of the accepted protocol of the IEEE 802.15.4 standard. However the limitation in design of only seven GTS per superframe duration pre-determines a problem of allocation when requirements by applications in need of good QoS exceed the supply of GTS's. This results in the QoS aware traffic requiring differentiation of service in the buffer of the Personal Area Network Coordinator (PANC). A question arises as to the true ability of service dimensioning of networks without the prior knowledge of buffer allocation. In some WSN the buffer shares memory with the application and radio stack. This memory requirement will vary depending on the application. This research complements a previous paper in which the application of Decay Rate Analysis in relation to the dimensioning of buffer allocation of the various nodes was analysed using simulation and analytical methodology. The results illustrated the probability of queue occupancy for a specific application namely VoIP. This paper proposes that by employing the decay rate analysis a better understanding of buffer requirements at the PANC can be determined for different sensor applications. This analysis will further enhance the pre-deployment dimensioning of a network for greater operational efficiency and overall cost effectiveness.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6266145,no
HCMM - a maturity model for measuring and assessing the quality of cooperation between and within hospitals,"Increased competition and market dynamics in healthcare force hospitals to intensify their efforts toward specialization and cooperation with others. In this paper, a maturity model is discussed that assists hospitals in evolving the required strategic, organizational, and technical capabilities in a systematic way so that the formation of collaborative structures and processes is efficient and effective. The so-called Hospital Cooperation Maturity Model (HCMM), queries a total of 36 reference points reflecting 3 distinct organizational dimensions relevant for the ability to cooperate. On the one hand it can be used as basis for benchmarking the quality of cooperation between a particular hospital and its business partners; on the other hand, it can also be applied as common ground for shared learning and improvement initiatives. In order to demonstrate its usability and applicability, an instantiation in form of a software prototype is presented. The paper ends with recommendations for healthcare practice and future research.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6266397,no
Taming of the Shrew: Modeling the Normal and Faulty Behaviour of Large-scale HPC Systems,"HPC systems are complex machines that generate a huge volume of system state data called """"events"""". Events are generated without following a general consistent rule and different hardware and software components of such systems have different failure rates. Distinguishing between normal system behaviour and faulty situation relies on event analysis. Being able to detect quickly deviations from normality is essential for system administration and is the foundation of fault prediction. As HPC systems continue to grow in size and complexity, mining event flows become more challenging and with the upcoming 10 Pet flop systems, there is a lot of interest in this topic. Current event mining approaches do not take into consideration the specific behaviour of each type of events and as a consequence, fail to analyze them according to their characteristics. In this paper we propose a novel way of characterizing the normal and faulty behaviour of the system by using signal analysis concepts. All analysis modules create ELSA (Event Log Signal Analyzer), a toolkit that has the purpose of modelling the normal flow of each state event during a HPC system lifetime, and how it is affected when a failure hits the system. We show that these extracted models provide an accurate view of the system output, which improves the effectiveness of proactive fault tolerance algorithms. Specifically, we implemented a filtering algorithm and short-term fault prediction methodology based on the extracted model and test it against real failure traces from a large-scale system. We show that by analyzing each event according to its specific behaviour, we get a more realistic overview of the entire system.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6267920,no
High-Level Model for Educational Collaborative Virtual Environments Development,"The paper presents a proposal high-level model for the development of Educational Collaborative virtual Environments based on engineering software and quality concepts for software development. A life cycle was identified, which was detailed the phases of development, taking into account different techniques and methods and related documentation. The main goal of this research is to demonstrate how we can use the model to develop our applications by virtual world's platforms. The model was conceived based on research carried out in area the development of models that integrate all phases of process development software and models for assessing collaborative virtual environments. The model used contains a set of diagrams to support developer teams in their tasks, mainly in the creation of educational collaborative virtual environments to be used in an e-learning context.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6268118,no
Systematic mapping study of quality attributes measurement in service oriented architecture,Background: Service oriented architecture (SOA) promotes software reuse and interoperability as its niche. Developer usually expects that services being used are performing as promised. Capability to measure quality of services helps developer to predict the software behavior. Measuring quality attributes of software can be treated as a way to ensure that the developed software will/does meet expected qualities.,2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6269349,no
Fault Detection and Diagnosis for Component-based Robotic Systems,"In the software engineering domain, much work has been done for fault detection and diagnosis (FDD) and many methods and technologies have been developed, especially for safety-critical systems. In the meantime, component-based software engineering has emerged and been widely adopted as an effective way to deal with various issues of modern systems such as complexity, scalability, and reusability. Robotics is one of the representative domains where this trend appears. As technology advances, robots are beginning to inhabit the same physical space as humans and this makes the safety issue more important, even critical. However, the safety of recent component-based robotic systems has not been extensively investigated yet. One effective way to achieve system safety is fault tolerance based on FDD which recent robot systems can benefit from. For this purpose, we propose a FDD scheme for component-based software systems with the requirements of flexibility, extendability, and efficiency. The proposed FDD scheme consists of three main components: filter and history buffer, filtering pipeline, and FDD pipeline. We implemented this scheme using the cisst framework and show how it can be systematically deployed in an actual system. As an illustrative example, a FDD pipeline is set up to detect a thread scheduling fault on various operating systems (Linux, RTAI, and Xenomai) and experimental results are presented. Although the target of this example is only one type of fault, it demonstrates how the proposed FDD scheme can be introduced to component-based environments in flexible and systematic ways and how system designers can define a fault and FDD pipeline for it. It is obvious that the importance of dependability-especially safety-of robots will significantly increase as robots are deployed in our daily lives, directly operate on us, or interact closely with us. Thus, the FDD scheme proposed in this paper can be a useful basis for robot dependability research in the fut- re.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6269387,no
On the relationship between defects of nodes and characteristics of their neighbors in software structural network,"Software development is a complex task as there exist various relationships between different pieces of code. If software entities (e.g., classes) are considered as nodes and the relationships between entities (e.g., inheritance) are considered as edges, the static structure of software can be viewed as a software structural network. In this paper, the relationship between the quality of nodes and the characteristics of their neighbors in the software structural network has been investigated. The following observations have been made: On most occasions, the neighbors of the defect-prone nodes tend to have higher code complexity and be more centralized in the software structural network and undergo more frequent changes than those which are defect-free. The observations made in our research can be used to help software engineers assess risks during the software evolution activities (e.g., adding new entities and relationships) with the purpose of improving software designs.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6269400,no
Sole error locating array and approximate error locating array,"Combinatorial interaction testing (CIT) is a method to detect the fault interactions among the parameters or components in a system. However the most works in the field of combinatorial interaction testing focus on detecting interaction faults rather than locating them. (d, t)-locating arrays and (d, t)-detecting arrays were proposed by C. J. Colbourn and D. W. McClary to locate and detect the interaction fault. In the paper, we study the structure of the special fault locating array that is able to locate sole interaction fault among the parameters or components, namely (1, t)-detecting arrays, and then propose the concept and construction method of approximate error locating array based on the special error locating array. The AETG-like algorithms to generate these special arrays are provided.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6269509,no
Prediction of software maintainability using fuzzy logic,"The relationship between object oriented metrics and software maintainability is complex and non-linear. Therefore, there is considerable research interest in development and application of sophisticated techniques which can be used to build models for predicting software maintainability. However, when predicting maintainability not only product quality measurements are surrounded with imprecision and uncertainty, but also the relationships between the external and internal quality attributes suffer from imprecision and uncertainty. The reason behind that, there are at least two important sources of information for building the prediction model: historical data and human experts. Therefore, in this paper an attempt has been made to utilize the capability of fuzzy logic in handling imprecision and uncertainty to come up with an efficient maintainability prediction model. The proposed model is constructed using object-oriented metrics data in Li and Henry's datasets collected from two different object-oriented systems.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6269563,no
Recognising the Capacities of Dynamic Reconfiguration for the QoS Assurance of Running Systems in Concurrent and Parallel Environments,"Recognizing the impact of reconfiguration on the QoS of running systems is especially necessary for choosing an appropriate approach to dealing with dynamic evolution of mission-critical or non-stop business systems. The rationale is that the impaired QoS caused by inappropriate use of dynamic approaches is unacceptable for such running systems. To predict in advance the impact, the challenge is two-fold. First, a unified benchmark is necessary to expose QoS problems of existing dynamic approaches. Second, an abstract representation is necessary to provide a basis for modeling and comparing the QoS of existing and new dynamic reconfiguration approaches. Our previous work [8] has successfully evaluated the QoS assurance capabilities of existing dynamic approaches and provided guidance of appropriate use of particular approaches. This paper reinvestigates our evaluations, extending them into concurrent and parallel environments by abstracting hardware and software conditions to design an evaluation context. We report the new evaluation results and conclude with updated impact analysis and guidance.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6269642,no
Dataflow Weaknesses Analysis of Scientific Workflow Based on Fault Tree,"If potential contributors leading to system failure can be identified when a scientific workflow is modeled, a lot of system weaknesses may thus be revealed and improved. In this paper, we first identify a number of data dependency patterns in scientific workflows and their corresponding state functions. Then, a method to transform the state functions into fault tree symbols is presented. We use fault tree analysis method to identify critical elements and elements combinations that lead to the incorrect state of a final output and calculate the probability of the incorrect state of a final output based on the probabilities of the basic events in the analyzed workflow. Moreover, an importance measure is designed to prioritize the contributors leading to the incorrect state of a final output. Finally, the feasibility and effectiveness of the proposed methods are proved by example and experiments.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6269649,no
Studies on the Effect of Laser Cooling in Atom Lithography for Nanometrology,"To meet the requirement of nanoscale dimensional metrology, lengthy standards with features below 100 nanometers are indispensable instruments. Our group has successfully fabricated length standards through atom lithography. For further improvement of the quality of these standards, laser cooling of Chromium atom beam was studied through a transverse Doppler cooling scheme. Moreover, utilizing the software developing kit (SDK) of CCD camera, we developed an image collecting software, which had functions of real-time display for gray-scale image and image measure. In our experimental setup, with the software, we could detect the laser induced fluorescence spots from marginal beams to monitor and analyze the effect of laser cooling.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6270455,no
Communication Library to Overlap Computation and Communication for OpenCL Application,"User-friendly parallel programming environments, such as CUDA and OpenCL are widely used for accelerators. They provide programmers with useful APIs, but the APIs are still low level primitives. Therefore, in order to apply communication optimization techniques, such as double buffering techniques, programmers have to manually write the programs with the primitives. Manual communication optimization requires programmers to have significant knowledge of both application characteristics and CPU-accelerator architecture. This prevents many application developers from effective utilization of accelerators. In addition, managing communication is a tedious and error-prone task even for expert programmers. Thus, it is necessary to develop a communication system which is highly abstracted but still capable of optimization. For this purpose, this paper proposes an OpenCL based communication library. To maximize performance improvement, the proposed library provides a simple but effective programming interface based on Stream Graph in order to specify an applications communication pattern. We have implemented a prototype system on OpenCL platform and applied it to several image processing applications. Our evaluation shows that the library successfully masks the details of accelerator memory management while it can achieve comparable speedup to manual optimization in which we use existing low level interfaces.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6270691,no
Monitoring and Predicting Hardware Failures in HPC Clusters with FTB-IPMI,"Fault-detection and prediction in HPC clusters and Cloud-computing systems are increasingly challenging issues. Several system middleware such as job schedulers and MPI implementations provide support for both reactive and proactive mechanisms to tolerate faults. These techniques rely on external components such as system logs and infrastructure monitors to provide information about hardware/software failure either through detection, or as a prediction. However, these middleware work in isolation, without disseminating the knowledge of faults encountered. In this context, we propose a light-weight multi-threaded service, namely FTB-IPMI, which provides distributed fault-monitoring using the Intelligent Platform Management Interface (IPMI) and coordinated propagation of fault information using the Fault-Tolerance Backplane (FTB). In essence, it serves as a middleman between system hardware and the software stack by translating raw hardware events to structured software events and delivering it to any interested component using a publish-subscribe framework. Fault-predictors and other decision-making engines that rely on distributed failure information can benefit from FTB-IPMI to facilitate proactive fault-tolerance mechanisms such as preemptive job migration. We have developed a fault-prediction engine within MVAPICH2, an RDMA-based MPI implementation, to demonstrate this capability. Failure predictions made by this engine are used to trigger migration of processes from failing nodes to healthy spare nodes, thereby providing resilience to the MPI application. Experimental evaluation clearly indicates that a single instance of FTB-IPMI can scale to several hundreds of nodes with a remarkably low resource-utilization footprint. A deployment of FTB-IPMI that services a cluster with 128 compute-nodes, sweeps the entire cluster and collects IPMI sensor information on CPU temperature, system voltages and fan speeds in about 0.75 seconds. The average CPU utilization of th- s service running on a single node is 0.35%.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6270765,no
Towards High-Level Programming of Multi-GPU Systems Using the SkelCL Library,"Application programming for GPUs (Graphics Processing Units) is complex and error-prone, because the popular approaches - CUDA and OpenCL - are intrinsically low-level and offer no special support for systems consisting of multiple GPUs. The SkelCL library presented in this paper is built on top of the OpenCL standard and offers pre-implemented recurring computation and communication patterns (skeletons) which greatly simplify programming for multi-GPU systems. The library also provides an abstract vector data type and a high-level data (re)distribution mechanism to shield the programmer from the low-level data transfers between the system's main memory and multiple GPUs. In this paper, we focus on the specific support in SkelCL for systems with multiple GPUs and use a real-world application study from the area of medical imaging to demonstrate the reduced programming effort and competitive performance of SkelCL as compared to OpenCL and CUDA. Besides, we illustrate how SkelCL adapts to large-scale, distributed heterogeneous systems in order to simplify their programming.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6270864,no
Laser Methane Sensor with the Function of Self-Diagnose,"Using the technology of tunable diode laser absorption spectroscopy and the technology of micro-electronics, a fiber laser methane sensor based on the microprocessor C8051F410 is given. In this paper, we use the DFB Laser as the light source of the sensor. By tuning temperature and driver current of the DFB laser, we can scan the laser over the methane absorption line, Based on the Beer-Lambert law, through detect the variation of the light power before and after the absorption we realize the methane detection. It makes the real-time and online detection of methane concentration to be true, and it has the advantages just as high accuracy, immunity to other gases, long calibration cycle and so on. The sensor has the function of adaptive gain and self-diagnose. By introducing digital potentiometers, the gain of the photo-electric conversion operational amplifier can be controlled by the microprocessor according to the light power. When the gain and the conversion voltage achieve the set value, then we can consider the sensor in a fault status, and then the software will alarm us to check the status of the probe. In addition, we introduce the feedback of the temperature to compensate the environment effects, So we improved the dependence and the stability of the measured results. At last we give some analysis on the sensor according the field application and according the present working, we have a look of our next work in the distance.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6271069,no
Research and Develop on PCB Defect Intelligent Visual Inspection Robot,"In order to realize the fast detection of the printed circuit board's quality, a set of PCB defect intelligent automatic detection device based on machine vision is analysed and designed. The paper introduced the robot system construction, machine system, electronic control system, the design and realization of vision imaging system, software used a fast iterative algorithm for image segmentation based on 2D maximum between-cluster variance, using the template matching method for PCB image of the automatic detection and recognition defects. Experiment results on model machine show that the design of the intelligent visual inspection robot can detect PCB defect exactly and effectively, it also can be used in the actual PCB online detection.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6271078,no
Experimental implementation of dynamic spectrum access for video transmission using USRP,In this paper we present the experimental implementation of dynamic spectrum access (DSA) algorithm using universal software radio peripheral (USRP) and GNU Radio. The setup contains two primary users and two cognitive radios or secondary users. One primary user is fixed and the other is allowed to change its position randomly. Depending upon the position of the primary user the cognitive user will use the spectrum band where the detected energy is below certain predefined threshold level. The cognitive radio users are also programmed to operate independently without interfering with each other using energy detection algorithm for spectrum sensing. The modulation scheme is set to GMSK for secondary user performing data transmission. This experimental setup is used to analyze the quality of video transmission using DSA which provides the insight regarding the possibility of using free spectrum space to improve the performance of the system and its advantage over a non-DSA system. From the experiment it is shown that under congestion and interference DSA perform better than a non- DSA system.,2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6271185,no
Multi-purpose systems: A novel dataflow-based generation and mapping strategy,"The manual creation of specialized hard-ware infrastructures for complex multi-purpose systems is error-prone and time-consuming. Moreover, lots of effort is required to define an optimized and heterogeneous components library. To tackle these issues, we propose a novel design flow based on the Dataflow Process Networks Model of Computation. In particular, we have combined the operation of two state of the art tools, the Multi-Dataflow Composer and the Open RVC-CAL Compiler, handling respectively the automatic mapping of a reconfigurable multi-purpose substrate and the high level synthesis of hardware components. Our approach guarantees runtime efficiency and on-chip area saving both on FPGAs and ASICs.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6271969,no
"Real-time 360 panoramic views using BiCa360, the fast rotating dynamic vision sensor to up to 10 rotations per Sec","This paper presents a novel smart camera BiCa360 for real-time 360 panoramic views using a rotating dynamic vision sensor at up to 10 rotations per sec. The system consists of (1) a dual-line dynamic vision sensor generating events at high temporal resolution, on-chip time stamping (1s resolution), having a high dynamic range and the sparse visual coding of the information, (2) a high-speed mechanical device rotating at up to 10 revolutions per sec (rps) where the sensor is mounted and (3) a real-time embedded software for panoramic reconstruction of the 360 panoramic views. Within this work, we show the capabilities of the system in terms of data quality (scene reconstruction). We made several experiments to assess the angular resolution as well as the visual quality of the data for rotations ranging from 1 to 10 rps. All evaluations were performed on natural scene with ambient illuminations. Within the live demonstration, we will show BiCa360 providing 360 panoramic views of the natural scene in real-time and at different rotations.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6272139,no
Systematic literature reviews in global software development: A tertiary study,"Context: There has been an increase in research into global software development (GSD) and in the number of systematic literature reviews (SLRs) addressing this topic. Objective: The aim of this research is to catalogue GSD SLRs in order to identify the topics covered, the active researchers, the publication vehicles, and to assess the quality of the SLRs identified. Method: We performed a broad automated search to find SLRs dealing with GSD. We differentiate between SLR studies and papers reporting those studies. Data relating to each of the following was extracted and synthesized from each study: authors and their affiliation at the time of publication, the journal or conference in which the paper was published, the quality of each study and the main GSD study topic. Results: Twenty-four GSD SLR studies and 37 papers reporting those studies were identified. Major GSD topics covered include: (1) organizational environment, (2) project execution, and (3) project planning and control. The main research groups are based in Brazil (17), Ireland (8), and Sweden (7). Conclusions: GSD SLR studies are most frequently reported in the International Conference on Global Software Engineering and IEEE Software; the two most popular topics for research are risk factors due to the organizational environment and the development process. The most active researchers are based in Brazil. The quality of the SLR studies has not changed over time.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6272490,no
Preliminary results of a systematic review on requirements evolution,"Background: Software systems must evolve in order to adapt in a timely fashion to the rapid changes of stakeholder needs, technologies, business environment and society regulations. Numerous studies have shown that cost, schedule or defect density of a software project may escalate as the requirements evolve. Requirements evolution management has become one important topic in requirements engineering research. Aim: To depict a holistic state-of-the-art of requirement evolution management. Method: We undertook a systematic review on requirements evolution management. Results: 125 relevant studies were identified and reviewed. This paper reports the preliminary results from this review: (1) the terminology and definition of requirements evolution; (2) fourteen key activities in requirements evolution management; (3) twenty-eight metrics of requirements evolution for three measurement goals. Conclusions: Requirements evolution is a process of continuous change of requirements in a certain direction. Most existing studies focus on how to deal with evolution after it happens. In the future, more research attention on exploring the evolution laws and predicting evolution is encouraged.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6272491,no
Reporting guidelines for simulation-based studies in software engineering,"Background: Some scientific fields, such as automobile, drugs discovery or engineer have used simulation-based studies (SBS) to faster the observation of phenomena and evolve knowledge. All of them organize their working structure to perform computerized experiments based on explicit research protocols and evidence. The benefits have been many and great advancements are continuously obtained for the society. However, could the same approach be observed in Software Engineering (SE)? Are there research protocols and evidence based models available in SE for supporting SBS? Are the studies reports good enough to support their understanding and replication? AIM: To characterize SBS in SE and organize a set of reporting guidelines aiming at improving SBS' understandability, replicability, generalization and validity. METHOD: To undertake a secondary study to characterize SBS. Besides, to assess the quality of reports to understand the usually reported information regarding SBS. RESULTS: From 108 selected papers, it has been observed several relevant initiatives regarding SBS in software engineering. However, most of the reports lack information concerned with the research protocol, simulation model building and evaluation, used data, among others. SBS results are usually specific, making their generalization and comparison hard. No reporting standard has been observed. CONCLUSIONS: Advancements can be observed in SBS in Software Engineering. However, the lack of reporting consistency can reduce understandability, replicability, generalization and compromise their validity. Therefore, an initial set of guidelines is proposed aiming at improving SBS report quality. Further evaluation must be accomplished to assess the guidelines feasibility when used to report SBS in Software Engineering.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6272508,no
An experimental setup to assess design diversity of functionally equivalent services,"Background: A number of approaches leverage design diversity to tolerate software design faults in service-oriented applications. The use of design diversity depends on the assumption that functionally equivalent services, i.e., variant services, rarely fail on the same input case. However, there are no directives to assess whether variant services are actually diverse and fail on disjoint subsets of the input space. Aim: To provide proper assessment of service diversity in order to achieve a high level of reliability by employing either a diversity-based solution with the variant services or a single service that exhibits higher reliability than would be the case if design diversity was adopted. Method: We propose an experimental setup that encompasses (i) a set of directives to organize the preparation and execution of the experiment to investigate service diversity; (ii) investigation of whether variant services are actually diverse by using statistical tests; and (iii) an analysis of if and by how much the reliability of a diversity-based solution that leverages voters is an improvement over one that uses a single service. We evaluated the applicability and usefulness of the proposed experimental setup by employing it to assess diversity of variant services adhering to four different requirements specifications. For each specification, we analysed three different services. Results: We found that the proposed directives were effective for the purposes of this assessment. Assessment results demonstrated that services implementing the four requirements specifications are actually diverse at a 0.05 significance level. For two of the specifications, coincident failures of two or more services are infrequent enough to promote gains in overall system reliability. Conclusions: Our findings reveal threats to the effectiven",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6272513,no
Aspect-oriented software maintenance metrics: A systematic mapping study,"Background: Despite the number of empirical studies that assess Aspect-Oriented Software Development (AOSD) techniques, more research is required to investigate, for example, how software maintainability is impacted when these techniques are employed. One way to minimize the effort and increase the reliability of results in further research is to systematize empirical studies in Aspect-Oriented Software Maintainability (AOSM). In this context, metrics are useful as indicators to quantify software quality attributes, such as maintenance. Currently, a high number of metrics have been used throughout the literature to measure software maintainability. However, there is no comprehensive catalogue showing which metrics can be used to measure AOSM. Aim: To identify an AOSM metrics suite to be used by researchers in AOSM research. Method: We performed a systematic mapping study based on Kitchenham and Charters' guidelines, which derived a research protocol, and used well known digital libraries engines to search the literature. Conclusions: A total of 138 primary studies were selected. They describe 67 aspect-oriented (AO) maintainability metrics. Also, out of the 575 object-oriented maintainability metrics that we analyzed, 469 can be adapted to AO software. This catalogue provides an objective guide to researchers looking for maintainability metrics to be used as indicators in their quantitative and qualitative assessments. We provide information such as authors, metrics description, and studies that used the metric. Researchers can use this information to decide which metrics are more suited for their studies.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6272522,no
A mapping study of software code cloning,"Background: Software Code Cloning is widely used by developers to produce code in which they have confidence and which reduces development costs and improves the software quality. However, Fowler and Beck suggest that the maintenance of clones may lead to defects and therefore clones should be re-factored out. Objective: We investigate the purpose of code cloning, the detection techniques developed and the datasets used in software code cloning studies between the years of 2007 and 2011. This is to analyse the current research trends in code cloning to try and find techniques which have been successful in identifying clones used for defect prediction. Method: We used a mapping study to identify 220 software code cloning studies published from January 2007 to December 2011. We use these papers to answer six research questions by analysing their abstracts, titles and reading the papers themselves. Results: The main focus of studies is the technique of software code clone detection. In the past four years the number of studies being accepted at conferences and in journals has risen by 71%. Most datasets are only used once, therefore the performance reported by one paper is not comparable with the performance reported by another study. Conclusion: The techniques used to detect clones seem to be the main focus of studies. However it is difficult to compare the performance of the detection tools reported in different studies because the same dataset is rarely used in more than one paper. There are few benchmark datasets where the clones have been correctly identified. Few studies apply code cloning detection to defect prediction.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6272524,no
Dynamic quality evaluation of elbow flowmeter,"In order to assess the dynamic quality of the elbow flowmeter, the influence of ambient temperature, inner-pipe medium temperature, and pipeline lengths of the front and rear straight sections on the geometrical shape of the elbow pipe was presented in the paper. The change rule of the curvature radius to inside diameter ratio (R/D) and the quality index in various operating conditions were acquired quantitatively. The significance of the dynamic quality evaluation lies in that the knowledge of the change rule can guide users to select the suitable flowmeters in some specific application, rather than simply choosing the meters with larger R/D, which may help to reduce the system cost. In addition, the actual R/D instead of the theoretical manufacturing R/D is used to calculate the volume of flow, which can improve the measurement accuracy. The evaluation system developed with VB, Access, and MATLAB has been applied to some thermal power plant, the good usage effect having been obtained.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6272628,no
Haptic data compression based on quadratic curve prediction,"In this paper, a new haptic data compression method is presented. A quadratic curve predictor is constructed to improve the data reduction rate. Knowledge from human haptic perception is incorporated into the architecture to assess the perceptual quality of the compressed haptic signals. Experiments prove the effectiveness of the proposed approach in data reduction rate.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6272682,no
The research on defect recognition method for rail magnetic flux leakage detecting,"In this paper, the ANSYS finite element software was used to establish a two-dimensional mathematical model of defect leakage field for the simulation of the cracks, cone-shaped, pits and other regular defects of rail with the FEM simulation method, through which axial and radial magnetic flux density distribution curves and other data of defect leakage field was acquired. By comparison of the simulation data and peak, peak-valley, peak-valley spacing and other characteristic values of the leakage magnetic flux density curves, the characteristics of different defect types and identical defect types of magnetic leakage signal was acquired to identify the defect information such as defect types, internal or external distribution, shapes and dimensions and other information.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6273398,no
Fail-safe over-the-air programming and error recovery in wireless networks,"Wireless networks are an emerging technology which are employed for a growing number of applications. Maintenance and extensibility necessitate software updates after an initial deployment. Wireless updates are in most cases preferred, e.g. in large-scale networks or inaccessible deployments. Recent research either focused on reliable and fast transmission of a new firmware, or on a modular update of firmware parts. In this paper we additionally address the problem of error-prone software which can permanently disable the update functionality. A comprehensive and fail-safe update system is proposed fulfilling requirements for the usage in industrial environments.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6273600,no
SecAgreement: Advancing Security Risk Calculations in Cloud Services,"By choosing to use cloud services, organizations seek to reduce costs and maximize efficiency. For mission critical systems that must satisfy security constraints, this push to the cloud introduces risks associated with cloud service providers not implementing organizationally selected security controls or policies. As internal system details are abstracted away as part of the cloud architecture, the organization must rely on contractual obligations embedded in service level agreements (SLAs) to assess service offerings. Current SLAs focus on quality of service metrics and lack the semantics needed to express security constraints that could be used to measure risk. We create a framework, called SecAgreement (SecAg), that extends the current SLA negotiation standard, WS-Agreement, to allow security metrics to be expressed on service description terms and service level objectives. The framework enables cloud service providers to include security in their SLA offerings, increasing the likelihood that their services will be used. We define and exemplify a cloud service matchmaking algorithm to assess and rank SecAg enhanced WS-Agreements by their risk, allowing organizations to quantify risk, identify any policy compliance gaps that might exist, and as a result select the cloud services that best meet their security needs.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6274042,no
Improving Cloud Service Reliability -- A System Accounting Approach,"Nowadays an increasing number of companies only deploy their enterprise application services over the Internet. Software as a Service (SaaS) in a cloud computing environment allows these companies to focus on providing more competitive services instead of maintenance. As delivery of computing as a service, a trustworthy cloud service widely depends upon its reliability. For this reason, a newly defined Quality of Reliability (QoR) for cloud services is proposed in this paper. To achieve a good QoR, we not only analyze system events from both service consumers and providers, but also provide a layered composable system accounting architecture for cloud systems. A pipelined approach and a dependence estimation algorithm are introduced for pattern recognition and event analysis and prediction. A self-healing layer is also designed to achieve automatic recovery by re-composing services according to their functionalities and non-functional requirements. An implementation of this framework in an education services environment confirms the advantages over extant system accounting systems.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6274131,no
A Framework for Detecting Anomalous Services in OSGi-Based Applications,"The service-centric applications are composed of third-party services. These services delivered by different vendors are usually black-box components which lack source code and design documents. It is difficult to evaluate their quality by static code analysis. Detecting anomalous services online is important to improve the reliability of these applications. This paper presents a framework for detecting anomalous services in the OSGi-based applications, followed by a method of monitoring services. We propose a method to monitor the resource utilization and interaction of services through tracing thread transfer. In addition, we detect anomalous services with XmR control charts. A prototype tool was implemented and applied in an application server. The experimental results show that our method 1) is of high accuracy for monitoring the resource utilization of the OSGi-based services; 2) does not introduce significant overhead; 3) can detect anomalous services effectively.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6274151,no
ProPRED: A probabilistic model for the prediction of residual defects,"In this paper, we propose ProPRED, a probabilistic model for predicting residual defects based on Bayesian Networks (BN) in the software development lifecycle. With the chain rule for BN, ProPRED can be used to take the evidence of the influential factors to the activities (Analyze and Design, Development, Maintain, and Review and Test) that bring about the defects introduction and removal to reason and predict the probable residual defects. We refine and classify the influential factors to the four basic activities, and construct the ProPRED. Giving a case study, we conclude that the ProPRED improve its performance in reasoning under uncertainty and convenience in decision-making and quality control.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6275569,no
SdDirM: A dynamic defect prediction model,"Defect prediction and estimation techniques play an important role in software reliability engineering. This paper proposes a dynamic defect prediction model named SdDirM (System dynamic based Defect injection and removal Model) to improve the quantitative defect management process. Using SdDirM, we can simulate defect introduction and removal processes, and predict and estimate the residual defects in different phases. We describe the modeling process, the validation and the results with the empirical and real project data compared with the other well known models. These experiments show that the managers can use the model to explore and analyze the potential improvements before the practice.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6275570,no
Reducing Application-level Checkpoint File Sizes: Towards Scalable Fault Tolerance Solutions,"Systems intended for the execution of long-running parallel applications require fault tolerant capabilities, since the probability of failure increases with the execution time and the number of nodes. Checkpointing and rollback recovery is one of the most popular techniques to provide fault tolerance support. However, in order to be useful for large scale systems, current checkpoint-recovery techniques should tackle the problem of reducing checkpointing cost. This paper addresses this issue through the reduction of the checkpoint file sizes. Different solutions to reduce the size of the checkpoints generated at application level are proposed and implemented in a checkpointing tool. Detailed experimental results on two multicore clusters show the effectiveness of the proposed methods.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6280315,no
Using distribution automation for a self-healing grid,"One of the principal characteristics of the modern grid identified by the National Energy Technology Laboratory (NETL) in its 2007 report to the US Department of Energy is self healing. That is, The modern grid will perform continuous self-assessments to detect, analyze, respond to, and as needed, restore grid components or network sections. (Reference: The NETL Modern Grid Initiative - Powering our 21st-Century Economy - Modern Grid Benefits; National Energy Technology Laboratory; August 2007.). Distribution Automation (DA) is able to assist in achieving this objective through the use of computer and communication technology, advanced software, and remotely operable high voltage switchgear. The Fault Location Isolation and Service Restoration (FLISR) application can improve reliability dramatically without compromising safety and asset protection. This article briefly describes the FLISR function, provides information on the major trends of the day and issues that need to be resolved, and suggests where the industry should go from here..",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6281582,no
Evaluating source trustability with data provenance: A research note,"One of the main challenges in intelligence work is to assess the trustworthiness of data sources. In an adversarial setting, in which the subjects under study actively try to disturb the data gathering process, trustworthiness is one of the most important properties of a source. The recent increase in usage of open source data has exacerbated the problem, due to the proliferation of sources. In this paper we propose computerized methods to help analysts evaluate the truthfulness of data sources (open or not). We apply methods developed in database and Semantic Web research to determine data quality (which includes truthfulness but also other related aspects like accuracy, timeliness, etc.). Research on data quality has made frequent use of provenance metadata. This is metadata related to the origin of the data: where it comes from, how and when it was obtained, and any relevant conditions that might help determine how it came to be in its current form. We study the application of similar methods to the particular situation of the Intelligence analyst, focusing on trust. This paper describes ongoing research; what is explained here is a first attempt at tackling this complex but very important problem. Due to lack of space, relevant work in the research literature is not discussed, and several technical considerations are omitted; finally, further research directions are only sketched.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6284145,no
Design and implementation of detecting instrument for an airborne gun fire control computer,"This paper proposes a detecting instrument for an airborne gun fire control computer. Shooting system is a typical opened loop control system, whose most important part is fire control computer. The state of fire control computer directly effects shooting precision of airborne gun, therefore it's necessary to detect faults in cycle. Based on analysing interface characteristic of fire control computer, the paper proposed a testing scheme of in-situ detection and ex-situ detection, introduced testing principle, designed hardware and software of the detecting instrument, analysed testing precision and data. After application in the army, it is proved that the detecting instrument has high precision and speed, supports training and combat effectively. The design idea and realizing technique is a useful reference for detecting navy gun servo system.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6285729,no
Interval-based algorithms to extract fuzzy measures for Software Quality Assessment,"In this paper, we consider the problem of automatically assessing sofware quality. We show that we can look at this problem, called Software Quality Assessment (SQA), as a multicriteria decision-making problem. Indeed, just like software is assessed along different criteria, Multi-Criteria Decision Making (MCDM) is about decisions that are based on several criteria that are usually conflicting and non-homogenously satisfied. Nonadditive (fuzzy) measures along with the Choquet integral can be used to model and aggregate the levels of satisfaction of these criteria by considering their relationships. However, in practice, fuzzy measures are difficult to identify. An automated process is necessary and possible when sample data is available. Several optimization approaches have been proposed to extract fuzzy measures from sample data; e.g., genetic algorithms, gradient descent algorithms, and the Bees algorithm, all local search techniques. In this article, we propose a hybrid approach, combining the Bees algorithm and an interval constraint solver, resulting in a focused search expected to be less prone to falling into local results. Our approach, when tested on SQA decision data, shows promise and compares well to previous approaches to SQA that were using machine learning techniques.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6291044,no
Toward optimizing static target search path planning,"Discrete static open-loop target search path planning is known to be a NP (non-deterministic polynomial) - Hard problem, and problem-solving methods proposed so far rely on heuristics with no way to properly assess solution quality for practical size problems. Departing from traditional nonlinear model frameworks, a new integer linear programming (ILP) exact formulation and an approximate problem-solving method are proposed to near-optimally solve the discrete static search path planning problem involving a team of homogeneous agents. Applied to a search and rescue setting, the approach takes advantage of objective function separability to efficiently maximize probability of success. A network representation is exploited to simplify modeling, reduce constraint specification and speed-up problem-solving. The proposed ILP approach rapidly yields near-optimal solutions for realistic problems using parallel processing CPLEX technology, while providing for the first time a robust upper bound on solution quality through Lagrangean programming relaxation. Problems with large time horizons may be efficiently solved through multiple fast subproblem optimizations over receding horizons. Computational results clearly show the value of the approach over various problem instances while comparing performance to a myopic heuristic.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6291538,no
FAST: Formal specification driven test harness generation,"Full coverage testing is commonly perceived as a mission impossible because software is more complex than ever and produces vast space to cover. This paper introduces a novel approach which uses ACSL formal specifications to define and reach test coverage, especially in the sense of data coverage. Based on this approach, we create a tool chain named FAST which can automatically generate test harness code and verify program's correctness, turning formal specification and static verification into coverage definition and dynamic testing. FAST ensures completeness of test coverage and result checking by leveraging the formal specifications. We have applied this methodology and tool chain to a real-world mission critical software project that requires high quality standard. Our practice shows using FAST detects extra code bugs that escape from other validation methods such as manually-written tests and random/fuzz tests. It also costs much less human efforts with higher bug detection rate and higher code and data coverage.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6292298,no
A comprehensive frequency domain identification of a coastal p atrol vessel,"In This paper detailed frequency-domain system identification method is applied to identify steering dynamics of a naval coastal patrol vessel using a data analysis software tool, called CIFER. Advanced features such as Chirp-Z transform and composite windowing techniques are used to extract high quality frequency responses. An accurate, linear and robust transfer function models are derived for yaw and roll dynamics of the vessel. In addition, to evaluate the accuracy of the identified model, time-domain responses from a 45-45 zig-zag test are compared with the responses predicted by the identified model. The model shows excellent predictive capability that is well suited for simulation applications as well as control design.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6292482,no
Application of decay rate analysis for GTS provisioning in Wireless Sensor Networks,"In a Wireless Sensor Network (WSN) the provision of a Guaranteed Time Slot (GTS) in a beacon enabled mode is best suited for applications with low latency and high Quality of Service (QoS) requirements such as those of real time traffic. However the advancements in technology have dramatically increased the volume of applications which require access to this real time service provision in such areas as healthcare, asset tracking, environmental monitoring, and military projects. Each super-frame structure contains only seven GTS's with the remaining time slots provisioned for the Contention Access Period (CAP) where applications with higher tolerance of latency vie for a position to transmit data. The restrictions within the Contention Free Period (CFP) to seven time slots per superframe structure, places enormous responsibilities on the network designers to dimension networks that can guarantee a high QoS. This work plans to apply decay rate analysis to GTS provisioning. Appropriate buffer dimensioning for specific applications can be achieved by predetermining the probability of buffer capacity. This will enable adequate parameter settings to be configured within a WSN to meet all real time traffic needs.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6292681,no
Checkpointing in selected most fitted resource task scheduling in grid computing,"Grid applications run on environment that is prone to different kinds of failures. Fault tolerance is the ability to ensure successful delivery of services despite faults that may occur. Our research adds fault tolerance capacity with checkpointing and machine failure, to the current research, Selected Most Fitted (SMF) Task Scheduling for grid computing. This paper simulates one of fault tolerance techniques for grid computing, which is implementing checkpointing into Select Most Fitting Resource for Task Scheduling algorithm (SMF). We applied the algorithm of MeanFailure with Checkpointing in the SMF algorithm and named it MeanFailureCP-SMF. The MeanFailureCP-SMF is simulated using Gridsim with initial checkpointing interval at 20% job execution time. Results show that with MeanFailureCP-SMF has reduce the average execution time (AET) compare to the current SMF and MeanFailure Algorithm.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6295085,no
Assessment model for educational collaborative virtual environments,The paper presents an proposal model for assessing the quality of educational collaborative virtual environments. This model is based on the Quantitative Evaluation framework developed by Escudeiro in 2008 and it consists in five phases. The purpose is to establish a theoretical model that highlights a number of relevant set of requirements in relation to the quality in educational collaborative virtual environments in order to facilitate the assessment of educational collaborative virtual environments. It is intended to apply the model during the lifecycle of product development and the selection of environment to support the learning/teaching process.,2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6295359,no
Educational policy-making in managing undergraduate English majors' graduation thesis writing,"The frequent complaints from various levels about undergraduate thesis quality and its chronic problems over the years have brought up the current reformative policy exploration on teaching and tutoring undergraduate English majors' thesis writing online. This policy, characteristic of distributive, regulatory, evidence-based, democratic, sustainable and lifelong learning frameworks, was formulated after consulting higher level authorities, surveying students' and tutors' opinions, and then reviewed by the department academic committee before its implementation. It neither rejects nor belittles the teaching and tutoring of students' graduation thesis in the conventional way. Instead, it provides an alternative of distance learning for migrating seniors, opens up the possibility of writing and defending thesis online, facilitates the communication between all parties involved, and enables the administrators to monitor the learning, teaching and tutoring processes and to receive feedbacks from all anonymous users. It is argued that the e-governance of undergraduate English majors' graduation thesis writing can better counter possible policy risks with the combined efforts of all parties, and therefore predicts an optimistically foreseeable result.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6295417,no
Transient Fault Tolerance for ccNUMA Architecture,"Transient fault is a critical concern in the reliability of microprocessors system. The software fault tolerance is more flexible and lower cost than the hardware fault tolerance. And also, as architectural trends point toward multi core designs, there is substantial interest in adapting parallel and redundancy hardware resources for transient fault tolerance. The paper proposes a process-level fault tolerance technique, a software centric approach, which efficiently schedule and synchronize of redundancy processes with ccNUMA processors redundancy. So it can improve efficiency of redundancy processes running, and reduce time and space overhead. The paper focuses on the researching of redundancy processes error detection and handling method. A real prototype is implemented that is designed to be transparent to the application. The test results show that the system can timely detect soft errors of CPU and memory that cause the redundancy processes exception, and meanwhile ensure that the services of application is uninterrupted and delay shortly.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6296854,no
Fuzzy Fault Tree Based Fault Detection,"In this article, for the Linux operating system environment, with the characteristics of ambiguity and uncertainty for the occurrence probability of system failures, fuzzy theory is introduced into the fault tree analysis. The occurrence probability of basic events in the conventional fault tree is made fuzzy by introducing the concept of fuzzy sets. Using the upstream method for solving the minimum cut sets and transferring the different fuzzy numbers into triangular fuzzy numbers, the method is validated with the CPU error detection. This way provides a theoretical basis and implementation for system reliability evaluation, fault diagnosis and maintenance decisions.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6296855,no
A Novel Framework of Self-Adaptive Fault-Tolerant for Pervasive Computing,"The increasing complexity of software and hardware resources and frequentative interaction among function components make fault-tolerant very challenging in pervasive computing system. In our paper, we propose a novel framework of self-adaptive fault-tolerant mechanism for pervasive computing environments. In our approach, the self-adaptive fault-tolerant mechanism is dynamically built according to various types of detected fault based on continuous monitoring, analysis of the component state. We put forward the architecture of fault-tolerant system and the policy-based fault-tolerant scheme, which adopt three-dimensional array of core features to capture spatial and temporal variability and the Event-Condition-Action rules. The mentioned mechanism has been designed and implemented on a prototype of office pervasive computing application systems, called POPCAS System. We have performed the experiments to evaluate the efficiency of the fault-tolerant mechanism. The results of the experiments show that the proposed mechanism can obviously improve reliability of the POPCAS System.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6296867,no
Mean Opinion Score performance in classifying voice-enabled emergency communication systems,"Freedom Fone (FF) is an easy to use Interactive Voice Response (IVR) System that integrates with Global System for Mobile (GSM) telecommunications [1]. Sahana is a disaster management expert system [2]. Project intent was to interconnect the FF and Sahana free and open source software systems. The research adopted Emergency Data Exchange Language (EDXL) interoperable content standard [3] for data interchange between the two platforms. It was an initiative to enable Sarvodaya, Sri Lanka's largest humanitarian organization, with voice-enabled services for exchanging disaster information. An early automation challenge was introducing Sinhala and Tamil language Automatic Speech Recognition (ASR) and Text-To-Speech (TTS) software algorithms for interchanging information between the two disparate software systems [4]. Experiments with human substitution for ASR and TTS with decoupled less streamlined systems revealed inefficiencies [5]. Voice quality was a key factor affecting the Mean Time To Completion (MTTC). The research applied the International Telecommunication Union (ITU) recommended R800 Mean Opinion Score (MOS) and Difficulty Score (DS) voice quality evaluation methods [4]. The overall system was classified with a 3.52 MOS and predicted with a 29.44% DS [4]. This paper justifies the MOS classification accuracy in setting a 4.0 MOS threshold for differentiating good emergency communication IVRs from bad ones.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6297114,no
Techniques for data-race detection and fault tolerance: A survey,"There are two primary methods for interactions among processes in concurrent software, i.e., shared memory and message passing. Both of these methods require synchronization routines implicitly or explicitly for concurrency control. Explicit synchronization techniques are language independent, while implicit techniques depend on the programming language. Synchronization techniques are prone to various types of faults which may cause the software to fail. Fault tolerance techniques have been effectively employed to tolerate such failures. In this paper, we present a critical analysis of the existing fault tolerance techniques designed to tolerate a particular type of synchronization failure that is caused by data race condition. Previous work shows that synchronization faults occur primarily due to large communication between processes. We provide an overview of techniques used for reducing communication and concurrency control faults. To analyze the existing fault tolerance techniques for synchronization failures, we have identified a set of criteria. The results of our evaluation have been summarized in a table at the end.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6297164,no
PDF Scrutinizer: Detecting JavaScript-based attacks in PDF documents,"For a long time PDF documents have arrived in the everyday life of the average computer user, corporate businesses and critical structures, as authorities and military. Due to its wide spread in general, and because out-of-date versions of PDF readers are quite common, using PDF documents has become a popular malware distribution strategy. In this context, malicious documents have useful features: they are trustworthy, attacks can be camouflaged by inconspicuous document content, but still, they can often download and install malware undetected by firewall and anti-virus software. In this paper we present PDF Scrutinizer, a malicious PDF detection and analysis tool. We use static, as well as, dynamic techniques to detect malicious behavior in an emulated environment. We evaluate the quality and the performance of the tool with PDF documents from the wild, and show that PDF Scrutinizer reliably detects current malicious documents, while keeping a low false-positive rate and reasonable runtime performance.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6297926,no
System Design of Perceptual Quality-Regulable H.264 Video Encoder,"In this work, a perceptual quality-regulable H.264 video encoder system has been developed. Exploiting the relationship between the reconstructed macro block and its best predicted macro block from mode decision, a novel quantization parameter prediction method is built and used to regulate the video quality according to a target perceptual quality. An automatic quality refinement scheme is also developed to achieve a better usage of bit budget. Moreover, with the aid of salient object detection, we further improve the quality on where human might focus on. The proposed algorithm achieves better bit allocation for video coding system by changing quantization parameters at macro block level. Compared to JM reference software with macro block layer rate control, the proposed algorithm achieves better and more stable quality with higher average SSIM index and smaller SSIM variation.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6298452,no
Query Range Sensitive Probability Guided Multi-probe Locality Sensitive Hashing,"Locality Sensitive Hashing (LSH) is proposed to construct indexes for high-dimensional approximate similarity search. Multi-Probe LSH (MPLSH) is a variation of LSH which can reduce the number of hash tables. Based on the idea of MPLSH, this paper proposes a novel probability model and a query-adaptive algorithm to generate the optimal multi-probe sequence for range queries. Our probability model takes the query range into account to generate the probe sequence which is optimal for range queries. Furthermore, our algorithm does not use a fixed number of probe steps but a query-adaptive threshold to control the search quality. We do the experiments on an open dataset to evaluate our method. The experimental results show that our method can probe fewer points than MPLSH for getting the same recall. As a result, our method can get an average acceleration of 10% compared to MPLSH.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6299251,no
Developing a Bayesian Network Model Based on a State and Transition Model for Software Defect Detection,"This paper describes a Bayesian Network model-to diagnose the causes-effect of software defect detection in the process of software testing. The aim is to use the BN model to identify defective software modules for efficient software test in order to improve the quality of a software system. It can also be used as a decision tool to assist software developers to determine defect priority levels for each phase of a software development project. The BN tool can provide a cause-effect relationship between the software defects found in each phase and other factors affecting software defect detection in software testing. First, we build a State and Transition Model that is used to provide a simple framework for integrating knowledge about software defect detection and various factors. Second, we convert the State and Transition Model into a Bayesian Network model. Third, the probabilities for the BN model are determined through the knowledge of software experts and previous software development projects or phases. Last, we observe the interactions among the variables and allow for prediction of effects of external manipulation. We believe that both STM and BN models can be used as very practical tools for predicting software defects and reliability in varying software development lifecycles.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6299295,no
A Study of Student Experience Metrics for Software Development PBL,"In recent years, the increased failure originated in the software defects, in various information systems causes a serious social problem. In order to build a high-quality software, cultivation of ICT (Information and Communication Technology) human resources like a software engineer is required. A software development PBL (Project-based Learning) is the educational technique which lets students acquire knowledge and skill spontaneously through practical software development. In PBL, on the other hand, it is difficult to evaluate not only the quality of the product but also the quality of the development process in the project. In this paper, we propose the student evaluation metrics to assess the development process in PBL. The student evaluation metrics represent LOC (Lines of Code) and development time for each product developed by a student. By using online storage, these metrics can be measured and visualized automatically. We conducted an experiment to evaluate the accuracy of the metrics about development time. As a result, we confirmed that development time metrics can be measured with approximately 20% of error.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6299322,no
An Ensemble Approach of Simple Regression Models to Cross-Project Fault Prediction,"In software development, prediction of fault-prone modules is an important challenge for effective software testing. However, high prediction accuracy may not be achieved in cross-project prediction, since there is a large difference in distribution of predictor variables between the base project (for building prediction model) and the target project (for applying prediction model.) In this paper we propose an prediction technique called """"an ensemble of simple regression models"""" to improve the prediction accuracy of cross-project prediction. The proposed method uses weighted sum of outputs of simple (e.g. 1-predictor variable) logistic regression models to improve the generalization ability of logistic models. To evaluate the performance of the proposed method, we conducted 132 combinations of cross-project prediction using datasets of 12 projects from NASA IV&V Facility Metrics Data Program. As a result, the proposed method outperformed conventional logistic regression models in terms of AUC of the Alberg diagram.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6299324,no
Detecting Bad SNPs from Illumina BeadChips Using Jeffreys Distance,"Current microarray technologies are able to assay thousands of samples over million of SNPs simultaneously. Computational approaches have been developed to analyse a huge amount of data from microarray chips to understand sophisticated human genomes. The data from microarray chips might contain errors due to bad samples or bad SNPs. In this paper, we propose a method to detect bad SNPs from the probe intensities data of Illumina Beadchips. This approach measures the difference among results determined by three software Illuminus, GenoSNP and Gencall to detect the unstable SNPs. Experiment with SNP data in chromosome 20 of Kenyan people demonstrates the usefulness of our method. This approach reduces the number of SNPs that are needed to check manually. Furthermore, it has the ability in detecting bad SNPs that have not been recognized by other criteria.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6299393,no
Directional undervoltage pilot scheme for distribution generation networks protection,"The trends of the actual distribution networks are moving toward a high penetration of distributed generation and power electronics converters. These technologies modify contribution-to-fault current magnitude and raise concern about new protection systems to accurately detect faults on distribution networks. This paper proposes a directional under-voltage pilot scheme to detect faulted branches in distribution networks. The aim of the proposed scheme is to provide an efficient algorithm with functions for fault detection, fault localization and fault isolation. The fault detection is based on the voltage measurements at each node of the distribution network when a fault occurs, which are compared with the prefault values. Then, once the fault is detected, the proposed scheme locates the fault comparing the current direction at each node. As the direction of the current when a fault occurs is known, the scheme uses this information to locate the faulted branch. After that, a trip signal is sent to the corresponding breakers in order to isolate the branch under fault. Besides, the proposed scheme enables back up protection using communication between adjacents nodes. A distribution network has been modeled in PSCAD/EMTDC software to verify the proposed algorithm, taking into account distributed generation provided by both wind turbines (doubly fed induction generator and permanent magnet generator with full converter) and solar photovoltaic installations. The behavior of the under-voltage measurements and the current direction has been studied for both generation and loads nodes. This algorithm has been tested varying fault location and resistance along the modeled distribution network.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6302448,no
Automated ontology construction from scenario based software requirements using clustering techniques,"Ontologies have been utilized in many different areas of software engineering. As software systems grow in size and complexity, the need to devise methodologies to manage the amount of information and knowledge becomes more apparent. Utilizing ontologies in requirement elicitation and analysis is very practical as they help to establish the scope of the system and facilitate information reuse. Moreover ontologies can serve as a natural bridge to transition from the requirements gathering stage to designing the architecture for the system. However manual construction of ontologies is time consuming, error prone and subjective. Therefore it is greatly beneficial to devise automated methodologies which allow knowledge extraction from system requirements using an automated and systematic approach. This paper introduces an approach to systematically extract knowledge from system requirements to construct different views of ontologies for the system as a part of a comprehensive framework to analyze and validate software requirements and design.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6303056,no
Relationship of intangible assets and competitive advantage for software production: A Brazilian companies study,"The competitive strategies of software-producing organizations are generally based on the opportunities and risks of each business deal and do not sufficiently explore the potential of their intangible assets. This study aims to evaluate the impact of intangible resources management in the formulation of competitive strategies in software-production. To assess the existence, intensity, and conditions of the cause-effect relationship between intangible assets and competitive advantages, it was conducted an exploratory survey among Brazilian companies that produce software, belonging to the same economic cluster. The attributes of intangible resources were the same used in a study conducted in Europe in 2004 (value, rareness, imitation and substitution) and the results were statistically evaluated, using contingency tables and Chi-square tests. At same time, it was performed a case study with some of these Brazilian organizations to know the intensity and conditions under which the relationship could occur. The research found that intangible resources are structured according to the type of business involved, and they emphasize the basic management's elements: schedule - cost and quality as the most visible for the organization and customers. The intangible resource knowledge provides opportunities for the organization to identify effective partnerships and establish strong and long relationship.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6304377,no
Steady-state and transient performances of Oman transmission system with 200 MW photovoltaic power plant,"The paper presents steady-state and transient studies to assess the impact of a 200 MW Photovoltaic Power plant (PVPP) connection on the Main Interconnected Transmission System (MITS) of Oman. The PVPP consists mainly of a large number of solar arrays, DC/DC converters, DC/AC inverters, filters, and step-up transformers. Two proposed locations are considered to connect the PVPP plant to MITS: Manah 132 kV and Adam 132/33 kV grid stations in Al-Dakhiliah region. The transmission grid model of 2016 has been updated to include the simulation of the proposed 200 MW PVPP at either Manah or Adam. The DIgSILENT PowerFactory professional software is used to simulate the system and to obtain the results. The results include percentage of transmission line loadings, percentage of transformer loadings, busbar voltages, grid losses, in addition to 3-phase and 1-phase fault levels. Also, simulation studies have been performed to assess the transmission system transient responses to the PVPP outage. Steady state and transient analyses have shown that the connection of the PVPP plant at Manah or Adam to the transmission system is acceptable. The transient responses have proved that the system remains stable when it is subjected to the PVPP forced outage.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6304669,no
Android-based universal vehicle diagnostic and tracking system,"This system aims to provide a low-cost means of monitoring a vehicle's performance and tracking by communicating the obtained data to a mobile device via Bluetooth. Then the results can be viewed by the user to monitor fuel consumption and other vital vehicle electromechanical parameters. Data can also be sent to the vehicle's maintenance department which may be used to detect and predict faults in the vehicle. This is done by collecting live readings from the engine control unit (ECU) utilizing the vehicle's built in on-board diagnostics system (OBD). An electronic hardware unit is built to carry-out the interface between the vehicle's OBD system and a Bluetooth module, which in part communicates with an Android-based mobile device. The mobile device is capable of transmitting data to a server using cellular internet connection.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6305105,no
Framework for effective utilization of e-content in engineering education,"Even though lot of useful content is available on the Internet that is relevant to engineering courses, the scattered nature of the content being available from different websites, wastage of time in finding the right content, absence of any mechanism that certifies the correctness of the accessed content, different levels of the content on a given topic, too-lengthy non-interactive text content, are some of the problems faced by learners, in using this available content. In this context it is important to have a system that can direct the learner to right content, which is presented in an easily understandable manner, with virtual experimentation options wherever applicable, along with appropriate assessments carried out to certify the assessed. In this backdrop, a framework is proposed in this paper that can offer the details of e-content, in terms of its type, relevance, level, correctness, extent of coverage, and usage statistics. By making this info available from authenticated portals like University websites, updating it frequently by sharing information from similar other portals, taking into account the user feedbacks and subject experts' ratings to decide the content quality, learners can be enabled to access good quality e-content in less time and effort. Software agents similar to citation agents are proposed to be used for collecting the details from multiple sites in Internet. For assessment also, the software clients that run on content pages can bring the result of such assessments to the portals where the data belonging to such other previous assessments taken by the user were also stored to offer assessment scores of the learner over a period of time on multiple subjects and skills. Overall, this system can reduce the burden of the learner in accessing the required content and can make his learning more interesting by having competitive learning with online assessments and credits obtained thereon.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6306706,no
Condition Monitoring for Detecting VFC and STATCOM Faults in Wind Turbine Equipped with DFIG,"Condition monitoring of Doubly Fed Induction Generators (DFIG) is growing in importance for wind turbines. This paper investigates the effect of VFC (Variable Frequency Converter) and STATCOM Faults on wind turbine equipped with DFIG operation in order to condition monitoring of wind turbine. Consequently, a proposed method is used to detecting these faults means of harmonic components analyzing of DFIG rotor current. The simulation has been done with PSCAD/EMTDC software.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6307123,no
A Study on Reclosing of Asaluyeh-Isfahan 765 kV Transmission Line Considering the Effect of Neutral Reactor in Reducing Resonant Voltages,"Shunt reactors are utilized on long transmission lines (in this case a 640km 765kV line) to reduce over voltages experienced under light load conditions. When the compensated line is opened to clear a fault, it is found that the open-phase voltage does not disappear. In some cases, a dangerous transient voltage with a resonant frequency between 30Hz and 55Hz can be seen which gradually reduces in magnitude. This over voltage can damage line connected equipments like shunt reactors and open circuit-breakers. This phenomenon is because of the trapped charges existing on transmission line during the dead time of secondary arc extinction. In case of high transient over voltages, we need additional equipments to reduce them. Normally, closing resistors are used to limit the over voltages. This solution is expensive and prone to failure. The proposed method is the usage of neutral reactors in the neutral of shunt reactors which is a cost effective solution. The technique is shown to reduce the resultant over voltages on the power system and allows for faster reclosing thus improving stability. In this research work, the investigation of a neutral reactor application is performed for transmission lines using transient simulation with appropriate models. The application of neutral reactors for reducing the resonant over voltages and reclosing over voltages during fault clearing is analyzed. The simulations are carried out by ATP/EMTP software and the advantages of using neutral reactors for Asaluyeh-Isfahan 765 kV transmission line are discussed.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6307333,no
Research on the intelligent protection system of coal conveyor belt,"A sort of intelligent protection system for conveyor belt was designed, which mainly aimed at the phenomenon of belt slipped, belt broken, overloaded and coupling broken etc. It was designed by using intelligent instrument, monitoring software and fault detection technology, furthermore, the software and hardware were described. The protection system can automatically detect, diagnose corresponding fault, and give alarm signal with sound and light to stop the fault belt machine in due course. The application effect has proved that the protection system can improve intelligent degree of the whole burning coal transport and reduce the manual greatly and save a lot of funds for enterprise.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6308233,no
Prediction of armored equipment maintenance support materials' consumption based on imitation,"On the foundation of equipment fault regulation's research, through the analysis of the influential main factors of equipment maintenance materials' consumption, making use of Anylogic software, combining the actual usage of armored equipment in the army, setting up a equipment peacetime usage model, and then to predict the consumption of armored equipment maintenance material. The result express that it can carry on prediction to armored equipment maintenance materials' consumption in this way. Then to make a solid foundation for armored equipment maintenance resources transport ion and maintenance support ability estimation research.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6308831,no
Analyzing system of electric signals in spot welding process,"Analyzing system of electric signals in spot welding process is a very important detecting instrument for d enveloping high quality welding equipment. Based on general platform with Windows operation circumstance, high speed data acquisition card PCL1800, and industrial computer. A software for detecting and analyzing electric signals in spot welding process with high speed and multifunction has been developed by using Visual C++6.0. The system can detect current and voltage waveform of spot welding process, and extract dynamic characteristic information of spot welding process. Thus, it provides a powerful tool for developing high performance welding and optimizing parameters in spot welding process.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6308965,no
Document Quality Checking Tool for Global Software Development,"Software development projects often utilize global resources to reduce costs. Typically a large volume of unstructured office documents are involved. Unfortunately, in many cases the low quality of unstructured documents due to various location-related barriers (e.g. time zones, languages, and cultures) can cause negative effects on the outcomes of projects. Several approaches have been introduced for document quality checking but they have not generalized well enough to handle various unstructured documents in a broad range of projects. Based on past experience, we have prepared guidelines, templates, rules, and document quality-checking tools for designing and developing global software development projects. In this paper we specifically focus on the effectiveness of our document quality checking tool. The challenges for such a checking tool are that it must be generally adaptive and also highly accurate to be practical for industrial use. Our approach is template-based and consists of an extraction process for the physical-syntactic structure, a transformation process for the logical-semantic structure and an analysis process. Our experiments inspected 66 authentic customer documents, detecting 118 errors. The accuracy as measured by the true-positive ratio (accurately detected true errors) was 98.3% and the true-negative ratio (accurately detected non-errors) was 99.4%.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6311005,no
Temporal segmentation tool for high-quality real-time video editing software,"The increasing use of video editing software requires faster and more efficient editing tools. As a first step, these tools perform a temporal segmentation in shots that allows a later building of indexes describing the video content. Here, we propose a novel real-time high-quality shot detection strategy, suitable for the last generation of video editing software requiring both low computational cost and high quality results. While abrupt transitions are detected through a very fast pixel-based analysis, gradual transitions are obtained from an efficient edge-based analysis. Both analyses are reinforced with a motion analysis that helps to detect and discard false detections. This motion analysis is carried out exclusively over a reduced set of candidate transitions, thus maintaining the computational requirements demanded by new applications to fulfill user needs.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6311337,no
Fault coverage of a timing and control flow checker for hard real-time systems,"Dependability is a crucial requirement of today's embedded systems. To achieve a higher level of fault tolerance, it is necessary to develop and integrate mechanisms for a reliable fault detection. In the context of hard real-time computing, such a mechanism should also guarantee correct timing behavior, an essential requirement for these systems. In this paper, we present results of the fault coverage of a lightweight timing and control flow checker for hard real-time systems. An experimental evaluation shows that more than 30% of injected faults can be detected by our technique, while the number of errors leading to an endless loop is reduced by around 80 %. The check mechanism causes only very low overhead concerning additional memory usage (15.0% on average) and execution time (12.2% on average).",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6313855,no
OPAL 2: Rapid optical simulation of silicon solar cells,"The freeware program OPAL 2 computes the optical losses associated with the front surface of a Si solar cell. It calculates the losses for any angle of incidence within seconds, where the short computation time is achieved by decoupling the ray tracing from the Fresnel equations. Amongst other morphologies, OPAL 2 can be used to assess the random-pyramid texture of c-Si solar cells, or the `isotexture' of mc-Si solar cells, and to determine (i) the optimal thickness of an antireflection coating with or without encapsulation, (ii) the impact of imperfect texturing, such as non-ideal texture angles, over-etched isotexture, and flat regions, and (iii) the subsequent 1D generation profile in the Si. This paper describes the approach and assumptions employed by OPAL 2 and presents examples that demonstrate the dependence of optical losses on texture quality and incident angle.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6317616,no
Modeling and simulation based study for on-line detection of partial discharge of solid dielectric,"Nowadays electric utilities are facing major problems due to the ageing and deterioration of high voltage (HV) power equipments in their operating service period. There are several solid materials are used in high voltage power system equipments for insulation purpose. The insulators used in HV power equipment always have a small amount of impurity inside it. The impurity is mainly in the form of solid, gas or liquid. In most cases the impurity is in the form of air bubbles (void) which creates a weak zone inside the insulator. Therefore, this void is the reason for the occurrence of partial discharge in high voltage power equipments while sustaining the high voltage. Ageing and deterioration is mainly occurs due to the presence of partial discharge in such insulator used in the high voltage power equipments. The presence of partial discharge for a long period of time is also causes the insulation failure of high voltage equipments used in power system. Therefore, the partial discharge detection and measurement is necessary for prediction and reliable operation of insulation in high voltage power equipments. In this work, to study the on-line detection of partial discharge an epoxy resin is taken as a solid dielectric for simulating and modeling purpose. This epoxy resin with small impurity (air bubble) under high voltage stress creates a source of partial discharge inside the dielectric. The generated partial discharge is continuously detected and monitored by using LabVIEW software. Simulation of real time detection, de-noising and different analytic techniques of partial discharge signal by using LabVIEW software is proposed which gives the real time visualization of partial discharge signal produced inside the high voltage power equipment.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6318904,no
Application of neural networks for transformer fault diagnosis,"Power transformer is one of the most important components in a power system. It experiences thermal and electrical stresses during its operation. The insulation system consisting of mineral oil and the insulation paper used in transformer undergoes chemical changes under these stresses and gases are generated. These gases dissolve in oil. The extracted dissolved gases are analysed in the laboratory using gas chromatograph. The fault identifications in a transformer are based on certain key- gas ratios. International standards such as IEEE and ASTM are in use for fault identification. However, these standards are not able to diagnose the fault under certain conditions. Hence, there is a need to improve the diagnostic accuracy. In this paper an attempt has been made to diagnose the faults in a power transformer using a three level perceptron network. Three types of neural network simulation models are developed using MATLABTM Software and trained using the IEC TC 10 databases of faulty equipments inspected in service. The outputs of the Neural Network models are compared with the IEEE and ASTM methods. The comparison of the results indicates that the condition assessments offered by the models are capable of predicting the fault with higher success rate than the conventional diagnostic methods.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6318975,no
Compressed C4.5 Models for Software Defect Prediction,"Defects in every software must be handled properly, and the number of defects directly reflects the quality of a software. In recent years, researchers have applied data mining and machine learning methods to predicting software defects. However, in their studies, the method in which the machine learning models are directly adopted may not be precise enough. Optimizing the machine learning models used in defects prediction will improve the prediction accuracy. In this paper, aiming at the characteristics of the metrics mined from the open source software, we proposed three new defect prediction models based on C4.5 model. The new models introduce the Spearman's rank correlation coefficient to the basis of choosing root node of the decision tree which makes the models better on defects prediction. In order to verify the effectiveness of the improved models, an experimental scheme is designed. In the experiment, we compared the prediction accuracies of the existing models and the improved models and the result showed that the improved models reduced the size of the decision tree by 49.91% on average and increased the prediction accuracy by 4.58% and 4.87% on two modules used in the experiment.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6319220,no
Efficient Refinement Checking for Model-Based Mutation Testing,"In model-based mutation testing, a test model is mutated for test case generation. The resulting test cases are able to detect whether the faults in the mutated models have been implemented in the system under test. For this purpose, a conformance check between the original and the mutated model is required. We have developed an approach for conformance checking of action systems, which are well-suited to specify reactive and non-deterministic systems. We rely on constraint solving techniques. Both, the conformance relation and the transition relation are encoded as constraint satisfaction problems. Earlier results showed the potential of our constraint-based approach to outperform explicit conformance checking techniques, which often face state space explosion. In this work, we go one step further and show optimisations that really boost our performance. In our experiments, we could reduce our runtimes by 80%.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6319222,no
Reliability Prediction for Component-Based Systems: Incorporating Error Propagation Analysis and Different Execution Models,"Reliability, one of the most important quality attributes of a software system, should be predicted early in the development. This helps to improve the quality of the system in a cost-effective way. Existing reliability prediction methods for component-based systems use Markov models and are often limited to a model of stopping failures and sequential executions. Our approach relaxes these constraints by incorporating error propagation analysis and multiple execution models together consistently. We demonstrate the applicability of our approach by modeling the reliability of the reporting service of a document exchange server and conduct reliability predictions.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6319232,no
Redefinition of Fault Classes in Logic Expressions,"Fault-based testing selects test cases to detect hypothesized faults. In logic expression testing, many fault classes have been defined by researchers based on the syntax of the expressions. Due to the syntactic nature of the logic expressions, some fault classes may exist in one form (say, disjunctive normal form - DNF) of the logic expressions but not in other forms (say, general form). As a result, different fault-based testing techniques have been developed for different types of logic expressions and these techniques have different fault detecting capabilities. For example, some have high detecting power in DNF but low detecting power in the general form. Another complication arises when software developers decide which forms of logic expressions should be used in the first place. Should software developers use the general form for flexibility but compromise that with fewer fault classes and less fault detection? Or should they use DNF for more fault classes and, hence, better fault detection (because software developers have more ''hypothesized faulty'' scenarios to test) but sacrificing the generality of the expressions. In this paper, we propose a set of uniform definitions of fault classes such that they can be applied irrespective of the syntactic nature of the logic expressions, to produce consistent fault-based testing techniques and fault detection capabilities.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6319240,no
Multi-valued Decision Diagrams for the Verification of Consistency in Automotive Product Data,"Highly customizable products and mass customization - as increasing trends of the last years - are mainly responsible for an immense growth of complexity within the digital representations of knowledge of car manufacturers. We developed a method to detect and analyze inconsistencies by employing a Multi-Valued Decision Diagram (MDD) which issued to encode the set of all valid product configurations. On this basis, we stated a number of rules of consistency that are checked by a set-based verification scheme.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6319247,no
Modular Heap Abstraction-Based Code Clone Detection for Heap-Manipulating Programs,"Code clone is a prevalent activity during the development of softwares. However, it is harmful to the maintenance and evolution of softwares. Current techniques for detecting code clones are most syntax-based, and cannot detect all code clones. In this paper, we present a novel semantic-based clone detection technique by obtaining the similarity about the precondition and post condition of each procedure, which are computed by a context and field sensitive fix point iteration algorithm based on modular heap abstraction in heap-manipulating programs. Experimental evaluation about a set of C benchmark programs shows that the proposed approach can be scalable to detect various clones that existing syntax-based clone detectors have missed.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6319249,no
Fingertips Detection Algorithm Based on Skin Colour Filtering and Distance Transformation,"Multi-Fingertip location algorithm is always difficult and hot in Finger-based human-computer interaction systems. There are two major difficulties in this field: 1) obtaining accurate hand binary image, 2) locating fingertips in hand binary image. This article presented a multi-fingertip real-time track and location algorithm based on skin colour and distance transformation. The algorithm consists of the following steps. First, it use elliptical boundary model to detect skin colour of human hand in YCbCr colour spaces. After that, we use distance transformation to filter finger from hand, leaving palm area only. Meanwhile, it uses zero and first moment to calculate center of gravity of palm. Then the initial position of fingertips and finger-roots can be located by pixels of hand edge to the center of gravity of palm. Last, it accurately locates fingertips according to the position relationship between fingertips and finger-roots. Experimental results show fingertips can be located quickly and accurately by the algorithm, which fully meets the requirements of real-time computing tasks.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6319261,no
Robustness validation of integrated circuits and systems,"Robust system design is becoming increasingly important, because of the ongoing miniaturization of integrated circuits, the increasing effects of aging mechanisms, and the effects of parasitic elements, both intrinsic and external. For safety reasons, particular emphasis is placed on robust system design in the automotive and aerospace sectors. Until now, the term robustness has been applied very intuitively and there has been no proper way to actually measure robustness. However, the complexity of contemporary systems makes it difficult to fulfill tight specifications. For this reason, robustness must be integrated into a partially automated design flow. In this paper, a new approach to robustness modeling is presented, in addition to new ways to quantify or assess the robustness of a design. To demonstrate the flexibility of the proposed approach, it is adapted and applied to several different scenarios. These include the robustness evaluation of digital circuits under aging effects, such as NBTI; the robustness modeling of analog and mixed signal circuits using affine arithmetic; and the robustness study of software algorithms on a high system level.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6320491,no
Towards a lightweight model driven method for developing SOA systems using existing assets,"Developing SOA based systems and migrating legacy systems to SOA are difficult and error prone tasks, where approaches, methods and tools play a fundamental role. For this reason, several proposals have been brought forward in literature to help SOA developers. This paper sketches a novel method for the development of systems based on services, i.e., adhering to the SOA paradigm, which follows the model driven paradigm. Our method is based on a meet-in-the-middle approach that allows the reuse of existing assets (e.g., legacy systems). The starting point of this method is a UML model representing the target business process and the final result is a detailed design model of the SOA system. The method, explained in this paper using a simple running example, has been applied successfully within an industrial project.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6320532,no
Hardware implementation of GMDH-type artificial neural networks and its use to predict approximate three-dimensional structures of proteins,"Implementation of artificial neural networks in software on general purpose computer platforms are brought to an advanced level both in terms of performance and accuracy. Nonetheless, neural networks are not so easily applied in embedded systems, specially when the fully retraining of the network is required. This paper shows the results of the implementation of artificial neural networks based on the Group Method of Data Handling (GMDH) in reconfigurable hardware, both in the steps of training and running. A hardware architecture has been developed to be applied as a co-processing unit and an example application has been used to test its functionality. The application has been developed for the prediction of approximate 3-D structures of proteins. A set of experiments have been performed on a PC using the FPGA as a co-processor accessed through sockets over the TCP/IP protocol. The design flow employed demonstrated that it is possible to implement the network in hardware to be easily applied as an accelerator in embedded systems. The experiments show that the proposed implementation is effective in finding good quality solutions for the example problem. This work represents the early results of the novel technique of applying the GMDH algorithms in hardware for solving the problem of protein structures prediction.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6322907,no
Binary-channel can covers defects detection system based on machine vision,"In order to realize the on-line detection and elimination of unqualified can covers, a binary-channel inspection system based on machine vision is proposed in this article. The system mainly consists of two illumination sources, two cameras, two sensors, an IPC, an interface circuit, two eliminating devices, a set of algorithms for image processing and a software for the control of independent binary-channel. In working status, each channel of the system is placed directly above a conveyor, which transports can covers to the detection position so that the camera is triggered, and then an image is captured with a flash. The cover images are transferred to the IPC and then processed by the algorithm that based on template matching and variation model. Depending on the processing results unqualified covers are eliminated. The system is proved to be non-pollution, low-cost, and defects such as double covers, no glue, shoulder scratch, distortion can be detected with a 98.7% accuracy and a speed of 1200 covers per minute.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6325323,no
Detecting the Onset of Dementia Using Context-Oriented Architecture,"In the last few years, Aspect Oriented Software Development (AOSD) and Context Oriented Software Development (COSD) have become interesting alternatives for the design and construction of self-adaptive software systems. An analysis of these technologies shows them all to employ the principle of the separation of concerns, Model Driven Architecture (MDA) and Component-based Software Development (CBSD) for building high quality of software systems. In general, the ultimate goal of these technologies is to be able to reduce development costs and effort, while improving the adaptability, and dependability of software systems. COSD, has emerged as a generic development paradigm towards constructing self-adaptive software by integrating MDA with context-oriented component model. The self-adaptive applications are developed using a Context-Oriented Component-based Applications Model-Driven Architecture (COCA-MDA), which generates an Architecture Description language (ADL) presenting the architecture as a components-based software system. COCA-MDA enables the developers to modularise the application based on their context-dependent behaviours, and separate the context-dependent functionality from the context-free functionality of the application. In this article, we wish to study the impact of the decomposition mechanism performed in MDA approaches over the software self-adaptability. We argue that a better and significant advance in software modularity based on context information can increase software adaptability and increase their performance and modifiability.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327926,no
Varying Topology of Component-Based System Architectures Using Metaheuristic Optimization,"Today's complex systems require software architects to address a large number of quality properties. These quality properties can be conflicting. In practice, software architects manually try to come up with a set of different architectural designs and then try to identify the most suitable one. This is a time-consuming and error-prone process. Also this may lead the architect to sub optimal designs. To tackle this problem, metaheuristic approaches, such as genetic algorithms, for automating architecture design have been proposed. Metaheuristic approaches use degrees of freedom to automatically generate new solutions. In this paper we present how to address topology of the hardware platform as a degree of freedom for system architectures. This aspect of varying architectures has not yet been addressed in existing metaheuristic approaches to architecture design. Our approach is implemented as part of the AQOSA (Automated Quality-driven Optimization of Software Architectures) framework. AQOSA aids architects by automatically synthesizing optimal solutions by using multiobjective evolutionary algorithms and it reports the trade-offs between multiple quality properties as output. In this paper we use an example system to show that the hardware-topology degree of freedom helps evolutionary algorithm to explore a larger design space. It can find new architectural solutions which would not be found otherwise.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6328129,no
TIRT: A Traceability Information Retrieval Tool for Software Product Lines Projects,"Software Product Line has proven to be an effective methodology for developing a diversity of software products at lower costs, in shorter time, and with higher quality. However, the adoption and maintenance of traceability in the context of product lines is considered a difficult task, due to the large number and heterogeneity of assets developed during product line engineering. Furthermore, the manual creation and management of traceability relations is difficult, error-prone, time consuming and complex. In this sense, Traceability Information Retrieval Tool (TIRT) was proposed in order to mitigate the maintenance traceability problem. An experimental study was performed in order to identify the viability of the proposed tool and traceability scenarios.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6328134,no
Path Coverage Criteria for Palladio Performance Models,"Component-based software engineering is supported by performance prediction approaches on the design level ensuring desired properties of systems throughout their entire lifecycle. The achievable prediction quality is a direct result of the quality of the used performance models, which is usually assured by validation. Existing approaches often rely solely on the expertise of performance engineers to determine if sufficient testing has occurred. There is a lack of quantitative criteria capturing which aspects of a model have been assessed and covered successfully. In this paper, we define path coverage criteria for Palladio performance models and show how the required testing effort can be estimated for arbitrary Palladio models. We demonstrate the applicability of effort estimation for each coverage criterion, provide estimates for a complex model from the Common Component Modelling Example, and show how these estimates can guide criteria selection.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6328140,no
A Model-Driven Dependability Analysis Method for Component-Based Architectures,"Critical distributed real-time embedded component-based systems must be dependable and thus be able to avoid unacceptable failures. To efficiently evaluate the dependability of the assembly obtained by selecting and composing components, well-integrated and tool-supported techniques are needed. Currently, no satisfying tool-supported technique fully integrated in the development life-cycle exists. To overcome this limitation, we propose CHESS-FLA, which is a model-driven failure logic analysis method. CHESS-FLA allows designers to: model the nominal as well as the failure behaviour of their architectures, automatically perform dependability analysis through a model transformation, and, finally, ease the interpretation of the analysis results through back-propagation onto the original architectural model. CHESS-FLA is part of an industrial quality tool-set for the functional and extra-functional development of high integrity embedded component-based systems, developed within the EU-ARTEMIS funded CHESS project. Finally, we present a case study taken from the telecommunication domain to illustrate and assess the proposed method.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6328156,no
Random Test Case Generation and Manual Unit Testing: Substitute or Complement in Retrofitting Tests for Legacy Code?,"Unit testing of legacy code is often characterized by the goal to find a maximum number of defects with minimal effort. In context of restrictive time frames and limited resources, approaches for generating test cases promise increased defect detection effectiveness. This paper presents the results of an empirical study investigating the effectiveness of (a) manual unit testing conducted by 48 master students within a time limit of 60 minutes and (b) tool-supported random test case generation with Randoop. Both approaches have been applied on a Java collection class library containing 35 seeded defects. With the specific settings, where time and resource restrictions limit the performance of manual unit testing, we found that (1) the number of defects detected by random test case generation is in the range of manual unit testing and, furthermore, (2) the randomly generated test cases detect different defects than manual unit testing. Therefore, random test case generation seems a useful aid to jump start manual unit testing of legacy code.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6328163,no
From Assumptions to Context-Specific Knowledge in the Area of Combined Static and Dynamic Quality Assurance,"High-quality software is an indispensable requirement today. Low-quality products can result in high overall costs (e.g., due to rework). Quality assurance can help to reduce the number of defects before a software product is delivered. However, quality assurance itself can be a major cost driver, especially testing activities. One solution for balancing these costs is to focus testing on defect-prone parts, which is nowadays often done by using product and process metrics. However, data from static quality assurance activities that is available early is usually not considered when focusing testing activities. Integration of static and dynamic quality assurance activities is a promising strategy for exploiting synergy effects and, consequently, one way to reduce costs and effort. For effective and efficient integration, knowledge about the relationships between the integrated techniques is necessary, which is often not available. Thus, assumptions have to be stated and evaluated. Existing approaches for this typically describe procedures only on a high level. Therefore, this paper presents procedures how to define, derive, and evaluate assumptions in a systematic and detailed manner for the integrated inspection and testing (In<sup>2</sup>Test) approach.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6328165,no
Micro Pattern Fault-Proneness,"One of the goals of Software Engineering is to reduce, or at least to try to control, the defectiveness of software systems during the development phase. The aim of our study is to analyze the relationship between micro patterns (introduced by Gil and Maman) and faults in a software system. Micro patterns are similar to design patterns, but their characteristic is that they can be identified automatically, and are at a lower level of abstraction with respect to design patterns. Our study aims to show, through empirical studies of open source software systems, which categories of micro patterns are more correlated to faults. Gil and Maman demonstrated, and subsequent studies confirmed, that 75% of the classes of a software system are covered by micro patterns. In our study we also analyze the relationship between faults and the remaining 25% of classes that do not match with any micro pattern. We found that these classes are more likely to be fault-prone than the others. We also studied the correlation among all the micro patterns of the catalog, in order to verify the existence of relationships between them.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6328166,no
Guiding Testing Activities by Predicting Defect-Prone Parts Using Product and Inspection Metrics,"Product metrics, such as size or complexity, are often used to identify defect-prone parts or to focus quality assurance activities. In contrast, quality information that is available early, such as information provided by inspections, is usually not used. Currently, only little experience is documented in the literature on whether data from early defect detection activities can support the identification of defect prone parts later in the development process. This article compares selected product and inspection metrics commonly used to predict defect-prone parts. Based on initial experience from two case studies performed in different environments, the suitability of different metrics for predicting defect-prone parts is illustrated. These studies revealed that inspection defect data seems to be a suitable predictor, and a combination of certain inspection and product metrics led to the best prioritizations in our contexts.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6328182,no
Towards a uniform evaluation of the science quality of SKA technology options: Polarimetrie aspects,"We discuss how to evaluate SKA technology options with regard to science output quality. In this work we will focus on polarimetry. We review the SKA specification for polarimetry and assess these requirements. In particular we will use as a illustrative case study a comparison of two dish types combined with two different feeds. The dish types we consider are optimized axi-symmetric prime-focus and offset Gregorian reflector systems; and the two feeds are the Eleven-feed (wideband) and a choked horn (octave band). To evaluate the imaging performance we employ end-to-end simulations in which given sky models are, in software, passed through a model of the telescope design according to its corresponding radio interferometrical measurement equation to produce simulated visibilities. The simulated visibilities are then used to generate simulated sky images. These simulated sky images are then compared to the input sky models and various figures-of-merit for the imaging performance are computed. A difficulty is the vast parameter space for observing modes and configurations that exists even when the technology is fixed. However one can fixed certain standard benchmark observation modes that can be applied across the board to the various technology options. The importance of standardized, end-to-end simulations, such as the one presented here, is that they address the high-level science output from SKA as a whole rather than low-level specifications of its individual parts.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6328686,no
Capability of single hardware channel for automotive safety applications according to ISO 26262,"There is no doubt that electromobility will be the future. All-electric vehicles were already available on the market in 2011 and 14 new vehicles will be commercially available in 2012. Due to the fact that automotive applications are influenced by the safety requirements of the ISO 26262, nowadays the use of new technologies requires more and more understanding for fail-safe and fault-tolerant systems due to increasingly complex systems. The safety of electric vehicles has the highest priority because it helps contribute to customer confidence and thereby ensures further growth of the electromobility market. Therefore in series production redundant hardware concepts like dual core microcontrollers running in lock-step-mode are used to reach ASIL D requirements given from the ISO 26262. In this paper redundant hardware concepts and the coded processing will be taken into account, which are listed in the current standard ISO 26262 as recommended safety measures.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6328876,no
An Empirical Study on Design Diversity of Functionally Equivalent Web Services,"A number of approaches based on design diversity moderate the communication between clients and functionally equivalent services, i.e., variant services, to tolerate software faults in service-oriented applications. Nevertheless, it is unclear whether variant services are actually diverse and fail on disjoint subsets of the input space. In a previous work, we proposed an experimental setup to assess design diversity of variant services that realize a requirements specification. In this work, we utilize the proposed experimental setup to assess the design diversity of a number of third-party Web services adhering to seven different requirements specifications. In this paper, we describe in detail the main findings and lessons learnt from this empirical study. Firstly, we investigate whether variant services are in fact diverse. Secondly, we investigate the effectiveness of service diversity for tolerating faults. The results suggest that there is diversity in the implementation of variant services. However, in some cases, this diversity might not be sufficient to improve system reliability. Our findings provide an important knowledge basis for engineering effective fault-tolerant service applications.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6329188,no
A Critical Survey of Security Indicator Approaches,"To better control IT security in software engineering and IT management, we need to assess security qualities in the different phases of a system's lifecycle. To this end, various security indicators, measures, and metrics have been proposed by scientists and practitioners, but few have gained general acceptance. We surveyed the current state of the art in qualita-tive and quantitative security measurement to characterize the available measurement strategies, their maturity, and the conceptual or technical obstacles preventing further progress in this field of research. We classified the proposed security indicators with respect to their characteristic properties and derived a classification tree delineating the different security assessment strategies and their derived security measures. Based on this overview, we analyzed the relative merits and deficiencies of current approaches, and we suggested future steps towards better security metrics. This paper summarizes the main results of our survey.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6329197,no
Type Classification against Fault Enabled Mutant in Java Based Smart Card,"Smart card are often the target of software or hardware attacks. For instance the most recent attacks are based on fault injection which can modify the behavior of applications loaded in the card, changing them as mutant application. In this paper, we propose a new protection mechanism which makes application to be less prone to mutant generation. This countermeasure requires a transformation of the original program byte codes which remains semantically equivalent. It requires a modification of the Java Virtual Machine which remains backward compatible and a dedicated framework to deploy the applications. Hence, our proposition improves the ability of the platform to resist to Fault Enabled Mutant.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6329230,no
Application of a reliability model generator to a pressure tank system,"A number of mathematical modelling techniques exist which are used to measure the performance of a given system, by assessing each individual component within the system. This can be used to determine the failure frequency or probability of failure of the system. Software is available to undertake the task of analysing these mathematical models after an individual or group of individuals manually create the models. The process of generating these models is time consuming and reduces the impact of the model on the system design. One way to improve this would be to automatically generate the model. In this work the procedure to automatically construct a model, based on Petri nets, for systems undergoing a phased-mission is applied to a pressure tank system, undertaking a four phase mission.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6330545,no
A Lingustic Approach for Robustness in Context Aware Applications,"Context-aware applications are vulnerable to errors due to the devices and networks engaged in the systems, as well as the complex control and data structures in the applications. Although usually fault tolerant technologies and software verifications are widely used to prevent and remedy errors, we notice that programming languages used in developing context-aware applications also play important roles in generating less error-prone programs. In this paper we introduce our recent efforts devoted to devising a programming language supporting safety related features and formal semantics for context aware applications.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6331958,no
Performance Management of Virtual Machines via Passive Measurement and Machine Learning,"Virtualization is commonly used to efficiently operate servers in data centers. The autonomic management of virtual machines enhances the advantages of virtualization. For the development of such management, it is important to establish a method to accurately detect performance degradation in virtual machines. This paper proposes a method that detects degradation via the passive measurement of traffic exchanged by virtual machines. Using passive traffic measurement is advantageous because it is robust against heavy loads, nonintrusive to the managed machines, and independent of hardware/software platforms. From the measured traffic metrics, performance state is determined by a machine learning technique that algorithmically determines the complex relationship between traffic metrics and performance degradation from training data. Moreover, the feasibility and effectiveness of the proposed method are confirmed experimentally.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6332044,no
Cloud Resource Provisioning to Extend the Capacity of Local Resources in the Presence of Failures,"In this paper, we investigate Cloud computing resource provisioning to extend the computing capacity of local clusters in the presence of failures. We consider three steps in the resource provisioning including resource brokering, dispatch sequences, and scheduling. The proposed brokering strategy is based on the stochastic analysis of routing in distributed parallel queues and takes into account the response time of the Cloud provider and the local cluster while considering computing cost of both sides. Moreover, we propose dispatching with probabilistic and deterministic sequences to redirect requests to the resource providers. We also incorporate check pointing in some well-known scheduling algorithms to provide a fault-tolerant environment. We propose two cost-aware and failure-aware provisioning policies that can be utilized by an organization that operates a cluster managed by virtual machine technology and seeks to use resources from a public Cloud provider. Simulation results demonstrate that the proposed policies improve the response time of users' requests by a factor of 4.10 under a moderate load with a limited cost on a public Cloud.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6332189,no
Sensor Placement with Multiple Objectives for Structural Health Monitoring in WSNs,"Sensor placement plays a vital role in deploying wireless sensor networks (WSNs) for structural health monitoring (SHM) efficiently and effectively. Existing civil engineering approaches do not seriously consider WSN constraints, such as communication load, network connectivity, and fault tolerance. In this paper, we study the methodology of sensor placement optimization for SHM that addresses three key aspects: finding a high quality placement of a set of sensors that satisfies civil engineering requirements; ensuring the communication efficiency and low complexity for sensor placement; and reducing the probability of a network failure. Particularly, after the placement of a subset of sensors, we find some distance-sensitive, but unused, near optimal locations for the remaining sensors to achieve a communication-efficient WSN. By means of the placement, we present a """"connectivity tree"""" by which structural health state or network maintenance can be achieved in a decentralized manner. We then optimize the system performance by considering multiple objectives: lifetime prolongation, low communication cost, and fault tolerance. We validate the efficiency and effectiveness of this approach through extensive simulations and a proof-ofconcept implementation on a real physical structure.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6332237,no
A Partial Reconstruction of Connected Dominating Sets in the Case of Fault Nodes,"Node failure in a connected dominating set (CDS) is an event of non-negligible probability. For applications where fault tolerance is critical, a traditional dominating-set based routing may not be a desirable form of clustering. For a typical localized algorithm to construct CDS, it has the time complexity of O(<sup>II</sup>), where  is the maximum degree of an input graph. In this paper we inspect the problem of load balancing in a dominating-set based routing. The motivation of load balancing is to prolong the network lifetime, while minimize the partitions of the network due to node failure, where they cause interruptions in communication among nodes. The idea is that by finding alternative nodes within a restricted range and locally reconstructing a CDS to include them, instead of totally reconstructing a new CDS. The number of nodes which should be awaken during partial reconstruction is less than 2(-1)p, where  is the nodes from CDS and the neighbor of the faulty node.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6332260,no
Double Mutual-Aid Checkpointing for Fast Recovery,"Because of the enlarging system size and the increasing number of processors, the probability of errors and multiple simultaneously failures become the norm rather than the exception. Therefore, to tolerate multiple failures is indispensable. Normally, most diskless checkpointing need the maximum recovery overhead no mater how many failures happen at the same time. However, a small number of processors' failures happen more frequently than the worse case. This study resolves the dilemma between more fault tolerance and fast recovery by presenting a novel diskless checkpointing which makes use of double mutual-aid checkpoints. It not only gives the necessary and sufficient condition but also proposes a method for determination the setting of double mutual-aid checkpoints.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6332284,no
Reliability Enhancement of Fault-prone Many-core Systems Combining Spatial and Temporal Redundancy,"The increasing transistor integration capacity will entail hundreds of processors on a single chip. Further, this will lead to an inherent susceptibility to errors of these systems. To obtain reliable systems again, various redundancy techniques can be applied. Of course, the usage of those techniques involves a significant overhead. Therefore, the identification of the optimal degree of redundancy is an important objective. In this paper we focus on core-level redundancy and checkpointing rollback-recovery. A model to determine the optimal degree of spatial and temporal redundancy regarding the minimal expected execution time will be introduced. Further, we will show that in several cases, the minimal expected execution time is achieved just by a simultaneous combination of both techniques, spatial redundancy and temporal redundancy.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6332368,no
Accelerated aging experiments for capacitor health monitoring and prognostics,"This paper discusses experimental setups for health monitoring and prognostics of electrolytic capacitors under nominal operation and accelerated aging conditions. Electrolytic capacitors have higher failure rates than other components in electronic systems like power drives, power converters etc. Our current work focuses on developing first-principles-based degradation models for electrolytic capacitors under varying electrical and thermal stress conditions. Prognostics and health management for electronic systems aims to predict the onset of faults, study causes for system degradation, and accurately compute remaining useful life. Accelerated life test methods are often used in prognostics research as a way to model multiple causes and assess the effects of the degradation process through time. It also allows for the identification and study of different failure mechanisms and their relationships under different operating conditions. Experiments are designed for aging of the capacitors such that the degradation pattern induced by the aging can be monitored and analyzed. Experimental setups and data collection methods are presented to demonstrate this approach.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6334580,no
Intelligent software sensors and process prediction for glass container forming processes based on multivariate statistical process control techniques,"Glass container forming processes have attracted more attention over the past years due to the problem of lacking process information and correlation for key variables within the processes. In this paper an approach to develop process modeling and intelligent software sensing is presented for application based on multivariate statistical process control methods. The intelligent software sensors are able to provide real time estimation of key variables, and Partial Least Squares (PLS) techniques have allowed for forward prediction of final product quality variables. An application of software sensors used for container forming blank temperature is presented along with PLS being applied to predict the wall and base dimensions of glass container products. Initial results show that these methods are very promising in providing a significant improvement within this area which is usually unmonitored and is susceptible to long time delays between forming and quality inspection.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6334643,no
Finite element approach for performances prediction of a small synchronous generator using ANSYS software,"The paper presents a finite element (FE) based efficient analysis procedure for very small three-phase synchronous machines. Two FE formulation approaches are proposed to achieve this goal: the magnetostatic and the non-linear transient time stepped formulations. This combination allows us to predict the steady-state and the transient performance at no-load and in the case of a line-to-line short circuit fault. The method is successfully applied for replication and modeling of a small 120-VA, 4-salient pole, 208-V and 60-Hz, wound rotor laboratory synchronous generator. The closeness of FE simulated and experimental results greatly attest to the effectiveness of the proposed FE based small-generator modeling framework.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6334879,no
Boundary-aided Extreme Value Detection based pre-processing algorithm for H.264/AVC fast intra mode prediction,"The mode decision in the intra prediction of an H.264/AVC encoder requires complex computations and a significant amount of time to select the best mode that achieves the minimum rate-distortion (RD). The complex computations for the mode decision cause difficulty in real-time applications, especially for software-based H.264/AVC encoders. This study proposes an efficient fast algorithm called Boundary-aided Extreme Value Detection (BEVD) to predict the best direction mode, excluding the DC mode, for fast intra-mode decision. The BEVD-based edge detection can predict luma-44, luma-1616, and chroma-88 modes effectively. The first step involves using the pre-processing mode selection algorithm to find the primary mode that can be selected for fast prediction. The second step requires applying the selected fewer high-potential candidate modes to calculate the RD cost for the mode decision. The encoding time is largely reduced, and similar video quality is also maintained. Simulation results show that the proposed BEVD method reduces encoding time by 63 %, and requires a bit-rate increase of approximately 1.7 %, and a decrease in peak signal-to-noise ratio (PSNR) by approximately 0.06 dB in QCIF and CIF sequences, compared with the H.264/AVC JM 14.2 software. The proposed method achieves less PSNR degradation and bit-rate increase compared to previous methods with more encoding time reduction.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6335597,no
Supporting Acceptance Testing in Distributed Software Projects with Integrated Feedback Systems: Experiences and Requirements,"During acceptance testing customers assess whether a system meets their expectations and often identify issues that should be improved. These findings have to be communicated to the developers -- a task we observed to be error prone, especially in distributed teams. Here, it is normally not possible to have developer representatives from every site attend the test. Developers who were not present might misunderstand insufficiently documented findings. This hinders fixing the issues and endangers customer satisfaction. Integrated feedback systems promise to mitigate this problem. They allow to easily capture findings and their context. Correctly applied, this technique could improve feedback, while reducing customer effort. This paper collects our experiences from comparing acceptance testing with and without feedback systems in a distributed project. Our results indicate that this technique can improve acceptance testing -- if certain requirements are met. We identify key requirements feedback systems should meet to support acceptance testing.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6337345,no
A Framework for Obtaining the Ground-Truth in Architectural Recovery,"Architectural recovery techniques analyze a software system's implementation-level artifacts to suggest its likely architecture. However, different techniques will often suggest different architectures for the same system, making it difficult to interpret these results and determine the best technique without significant human intervention. Researchers have tried to assess the quality of recovery techniques by comparing their results with authoritative recoveries: meticulous, labor-intensive recoveries of existing well-known systems in which one or more engineers is integrally involved. However, these engineers are usually not a system's original architects or even developers. This carries the risk that the authoritative recoveries may miss domain-, application-, and system context-specific information. To deal with this problem, we propose a framework comprising a set of principles and a process for recovering a system's ground-truth architecture. The proposed recovery process ensures the accuracy of the obtained architecture by involving a given system's architect or engineer in a limited, but critical fashion. The application of our work has the potential to establish a set of """"ground truths"""" for assessing existing and new architectural recovery techniques. We illustrate the framework on a case study involving Apache Hadoop.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6337738,no
Automated Reliability Prediction from Formal Architectural Descriptions,"Quantitative assessment of quality attributes (i.e., non-functional requirements, such as performance, safety or reliability) of software architectures during design supports important early decisions and validates the quality requirements established by the stakeholder. In current practice, these quality requirements are most often manually checked, which is time-consuming and error-prone due to the overwhelmingly complex designs. We propose an automated approach to assess the reliability of software architectures. It consists in extracting a Markov model from the system specification written in an Architecture Description Language (ADL). Our approach translates the specified architecture to a high-level probabilistic model-checking language, supporting system validation and quantitative reliability prediction against usage profile, component arrangement and architectural styles. We validate our approach by applying it to different architectural styles and comparing those with two different quantitative reliability assessment methods presented in the literature: the composite and the hierarchical methods.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6337740,no
TracQL: A Domain-Specific Language for Traceability Analysis,"Traceability analysis is used to improve quality in the software development process. As such an analysis is complex to implement and often requires a lot of dense code that is specific to the system being traced, there is a need for a framework to express traceability analysis tasks. This paper presents the Traceability Query Language TracQL, an expressive, extensible, representation-independent, and fast domain-specific language. Known approaches do not fulfill all these requirements. We examine TracQL and compare it to other approaches on a software ageing problem, namely to detect divergence between architecture and code. The necessary TracQL code is much shorter (by a factor of 1.7) and about twice as fast as what known approaches can achieve.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6337743,no
Documenting Early Architectural Assumptions in Scenario-Based Requirements,"In scenario-based requirement elicitation techniques such as quality attribute scenario elicitation and use case engineering, the requirements engineer is typically forced to make some implicit early architectural assumptions. These architectural assumptions represent initial architectural elements such as supposed building blocks of the envisaged system. Such implicitly specified assumptions are prone to ambiguity, vagueness, duplication, and contradiction. Furthermore, they are typically scattered across and tangled within the different scenario-based requirements. This lack of modularity hinders navigability of the requirement body as a whole. This paper discusses the need to explicitly document otherwise implicit architectural assumptions. Such an explicit intermediary between quality attribute scenarios and use cases enables the derivation and exploration of interrelations between these different requirements. This is essential to lower the mental effort required to navigate these models and facilitates a number of essential activities in the early development phases such as the selection of candidate drivers in attribute-driven design, architectural trade-off analysis and architectural change impact analysis.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6337745,no
Workload-aware System Monitoring Using Performance Predictions Applied to a Large-scale E-Mail System,"Offering services in the internet requires a dependable operation of the underlying software systems with guaranteed quality of service. The workload of such systems typically significantly varies throughout a day and thus leads to changing resource utilisations. Existing system monitoring tools often use fixed threshold values to determine if a system is in an unexpected state. Especially in low load situations, deviations from the system's expected behaviour are detected too late if fixed value thresholds (leveled for peak loads) are used. In this paper, we present our approach of a workload-aware performance monitoring process based on performance prediction techniques. This approach allows early detections of performance problems before they become critical. We applied our approach to the e-mail system operated by Germany's largest e-mail provider, the 1&1 Internet AG. This case study demonstrates the applicability of our approach and shows its accuracy in the predicted resource utilisation with an error of mostly less than 10%.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6337759,no
Extracting and Facilitating Architecture in Service-Oriented Software Systems,"In enterprises using service-oriented architecture (SOA) architectural information is used for various activities including analysis, design, governance, and quality assurance. Architectural information is created, stored and maintained in various locations like enterprise architecture management tools, design tools, text documents, and service registries/repositories. Capturing and maintaining this information manually is time-intensive, expensive and error-prone. To address this problem we present an approach for automatically extracting architectural information from an actual SOA implementation. The extracted information represents the currently implemented architecture and can be used as the basis for quality assurance tasks and, through synchronization, for keeping architectural information consistent in various other tools and locations. The presented approach has been developed for a SOA in the banking domain. Aside from presenting the main drivers for the approach and the approach itself, we report on experiences in applying the approach to different applications in this domain.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6337764,no
On a Feature-Oriented Characterization of Exception Flows in Software Product Lines,"The Exception Handling (EH) is a widely used mechanism for building robust systems. In Software Product Line (SPL) context it is not different. As EH mechanisms are embedded in most of mainstream programming languages, we can find exception signalers and handlers spread over code assets associated to common and variable SPL features. When exception signalers and handlers are added to an SPL in an unplanned way, one of the possible consequences is the generation of faulty family instances (i.e., instances on which common or variable features signal exceptions that are mistakenly caught inside the system). This paper reports a first systematic study, based on manual inspection and static code analysis, in order to categorize the possible ways exceptions flow in SPLs, and analyze its consequences. Fault-prone exception handling flows were consistently detected during this study, such as flows on which a variable feature signaled an exception a different variable feature handled it.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6337865,no
An Introspection Mechanism to Debug Distributed Systems,"Distributed systems are hard to debug due to the difficulty to collect, organize and relate information about their behavior. When a failure is detected the task to infer the system's state and the operations that have some connection with the problem is often quite difficult and usual debugging techniques often do not apply and, when they do, they are not very effective. This work presents a mechanism based on event logs annotated with contextual information, allowing visualization tools to organize events according to the context of interest for the system operator. We applied this mechanism to a real system and its the effort and cost to detect and diagnose the cause of problems was dramatically reduced.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6337890,no
ET-DMD: An Error-Tolerant Scheme to Detect Malicious File Deletion on Distributed Storage,"Distributed storage is a scheme to store data in networked storage services, which is the basis of popular cloud storage solutions. Although this scheme has huge benefits in reducing maintenance and operation cost, it has several security concerns. Among them, malicious file deletion by the storage providers is a top concern. In this paper, we develop a novel error-tolerant solution, ET-DME, to effectively detect malicious file deletion behaviors in distributed storage services. Our approach prevents malicious servers from forging evidence to bypass data auditing test. In addition, our approach does not limit the number of challenges made by the client. Our approach also has low computation and communication overhead.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6337945,no
"Some framework, Architecture and Approach for analysis a network vulnerability","Network administrators must rely on labour-intensive processes for tracking network configurations and vulnerabilities, which requires a lot of expertise and error prone. Organizational network vulnerabilities and interdependencies are so complex to make traditional vulnerability analysis become inadequate. Decision support capabilities let analysts make tradeoffs between security and optimum availability, and indicates how best to apply limited security resources. Recent work in network security has focused on the fact that a combination of exploitation is the typical way in which the invader breaks into the network. Researchers have proposed various algorithms to generate graphs based attack tree (or graph). In this paper, we present a framework, Architecture and Approach to Vulnerability Analysis.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6339320,no
A comparison on fish freshness determination method,"Basically, freshness contributes a major factor to quality of fishery products. Several methods have been used to measure fish freshness which are sensory analysis, chemical method and physical method. The aim of the study is to make a comparison between fish freshness meter and quantification of RGB color indices in order to detect fish freshness. The sensor used in this study is Torrymeter which would measure three types of species while quantification of RGB color is focused on the fish eyes and gills.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6339329,no
Heterogeneous tasks and conduits framework for rapid application portability and deployment,"Emerging heterogeneous and homogeneous processing architectures demonstrate significant increases in throughput for scientific applications over traditional single core processors. Each of these processing architectures vary widely in their processing capabilities, memory hierarchies, and programming models. Determining the system architecture best suited to an application or deploying an application that is portable across a number of different platforms is increasingly complex and error prone within this rapidly increasing and evolving design space. Quickly and easily designing portable, high-performance applications that can function and maintain their correctness properly across these widely varied systems has become paramount. To deal with these programming challenges, there is a great need for new models and tools to be developed. One example is MIT Lincoln Laboratory's Parallel Vector Tile Optimizing Library (PVTOL) which simplifies the task of developing software in C++ for these complex systems. This work extends the Tasks and Conduits framework in PVTOL to support GPU architectures and other heterogeneous platforms supported by the NVIDIA CUDA and OpenCL programming models. This allows the rapid portability of applications to a very wide range of architectures and clusters. Using this framework, porting applications from a single CPU core to a GPU requires a change of only 5 source lines of code (SLOC) in addition to the CUDA or OpenCL kernel. Using GPU-PVTOL we have achieved 22x speedup in an application of Monte Carlo simulations of photon propagation through a biological medium, and a 60x speedup of a 3D cone beam computed tomography (CT) image reconstruction algorithm.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6339588,no
A path selection decision-making model at the application layer for Multipath Transmission,"With development of web application, humans have much more requirement on network Quality of Service (QoS). This paper proposes an path selection decision-making model used in application layer for Multipath Transmission (MPT) to overcome the challenge which QoS of network layer will be corrupted in multiple paths transmission mechanism. P2FT method is proposed in this paper. That module can decide path selections according to characteristics of application layer. We dynamically provide data that should transmits in each path. Further, due to characteristics of multiple paths transmission, the model macroscopically regulates the application layer to choice more appropriate path real time. In performance evaluation part, we obtain transmission time by calculating in this simulation. The simulation results from the graphs illustrate that our proposed model decrease 61.627% of transmission time and achieve 1.0000 probability of successful transmission respectively compared with traditional bearer network. The path to cooperate with each other, to achieve network resources optimization, high efficiency, high speed transmission, and make the transmission have QoS guarantee.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6339833,no
Adaptive Random Test Case Generation for Combinatorial Testing,"Random testing (RT), a fundamental software testing technique, has been widely used in practice. Adaptive random testing (ART), an enhancement of RT, performs better than original RT in terms of fault detection capability. However, not much work has been done on effectiveness analysis of ART in the combinatorial test spaces. In this paper, we propose a novel family of ART-based algorithms for generating combinatorial test suites, mainly based on fixed-size-candidate-set ART and restricted random testing (that is, ART by exclusion). We use an empirical approach to compare the effectiveness of test sets obtained by our proposed methods and random selection strategy. Experimental data demonstrate that the ART-based tests cover all possible combinations at a given strength more quickly than randomly chosen tests, and often detect more failures earlier and with fewer test cases in simulations.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6340123,no
"Trustworthiness of Open Source, Open Data, Open Systems and Open Standards","A compelling direction of improving trustworthiness of software-based systems is to open their ingredients: Open source software, open data sets, open system interfaces like open technical standards allow constructing major or up to all elements of a software-based systems. That gives means to use the wisdom of the crowd to assess and evaluate the quality, security and trustworthiness of software components. In addition, the software components can mature along a continuous feedback and revision loop with the crowd.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6340126,no
An Effective Defect Detection and Warning Prioritization Approach for Resource Leaks,"Failing to release unneeded system resources such as I/O streams can result in resource leaks, which can lead to performance degradation and system crashes. Existing resource-leak detectors are usually based on predefined defect patterns to detect resource leaks in software. However, they typically report too many false positives and negatives, and also lack effective warning prioritization. Our empirical investigation shows that, their predefined defect patterns are not precise enough, and moreover, their used defect detection processes are not suitable enough for the defect patterns. In our approach, we introduce a novel Expressive Defect Pattern Specification Notation (EDPSN). With EDPSN, a resource-leak defect pattern can be defined more precisely by specifying conditional method calls and more expressively by including guiding information for the defect detection and warning prioritization process, such as the characteristics of its preferred defect detection process and the effective prioritization impact factors for its related warnings. Based on the EDPSN-based defect pattern, our approach tries to flexibly tune out a suitable defect detection and warning prioritization process. Through evaluations on three real-world projects (Eclipse-3.0.1, JBoss-3.0.6, and Weka-3.6.4), we show that our approach achieves high average precision (96%) and recall (74%), 26% and 49% higher than existing approaches, respectively.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6340134,no
CATest: A Test Automation Framework for Multi-agent Systems,"Agents are difficult to test because it is notoriously complicated to observe their proactive, autonomous and non-deterministic behaviours and hard to judge their correctness in dynamic environments. This paper proposes a specification-based test automation framework and presents a tool called CATest for testing multi-agent systems (MAS). The agent-based formal specification language SLABS plays three roles in the framework. First, it is used to guide the instrumentation of the agent under test so that its behaviour can be observed and recorded systematically. Second, the correctness of agent's behaviours recorded during test executions are automatically checked against the formal specifications. Finally, the test adequacy is measured by the coverage of the specification and determined according to a set of adequacy criteria specifically designed for testing MAS. An experiment with the tool has demonstrated its capability of detecting faults in MAS.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6340137,no
Towards Dynamic Random Testing for Web Services,"In recent years, Service Oriented Architecture (SOA) has been increasingly adopted to develop applications in the context of Internet. To develop reliable SOA-based applications, an important issue is how to ensure the quality of Web services. In this paper, we propose a dynamic random testing (DRT) technique for Web services which is an improvement of the widely practiced random testing. We examine key issues when adapting DRT to the context of SOA and develop a prototype for such an adaptation. Empirical studies are reported where DRT is used to test two real-life Web services and mutation analysis is employed to measure the effectiveness. The experimental results show that DRT can save up to 24% test cases in terms of detecting the first seeded fault, and up to 21% test cases in terms of detecting all seeded faults, both with the cases of uniformed mutation analysis and distribution-aware mutation analysis, which refer to faults being seeded in an even or clustered way, respectively. The proposed DRT and the prototype provide an effective approach to testing Web Services.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6340139,no
"Software Testing, Software Quality and Trust in Software-Based Systems","In our daily life we increasingly depend on software-based systems deployed as embedded software control systems in the automotive domain, or the numerous health or government applications. Software-based systems are more and more developed by reusable components available as commercial off-the-shelf components or open source components. The successful introduction of such integrated systems into businesses however does depend whether we trust the system or not. Trust and therewith the quality of software-based systems is determined by many properties such as completeness, consistency, maintainability, security, safety, reliability, and usability, among others. However during the development of software-based systems there are many opportunities to introduce errors in the different phases of the software development lifecycle. Testing is commonly applied as the predominant activity in industry to ensure high software quality providing a wide variety of methods and techniques to detect different types of errors in software-based systems. The panel goal is to discuss software testing strategy and techniques to improve the quality of the software and at the same time to build trust with customers. The panel will discuss the experts view on what the key factors are in developing high quality software-based systems. Through the panel, the discussions shall include the impact of testing on software quality within several domains and their businesses.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6340150,no
Using Program Dynamic Analysis for Weak Algorithm Detection,"Most of the proposed software testing approaches have concentrated on identifying faults in the implemented programs with the premise that the program may be implemented incorrectly. Although such premise is valid for many developed software systems and applications, it has been observed that many detected defects are caused by poor designed software. In this paper we present a dynamic analysis approach that identifies program deficiency that may be caused by poorly designed algorithm. Detecting such deficiency may help in identifying a weakly implemented program algorithm by identifying places in the program that may cause such weakness and suggest to the software designer/developer to redesign the program with a better algorithm.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6340168,no
Image Analysis Using Machine Learning: Anatomical Landmarks Detection in Fetal Ultrasound Images,"Accurate and robust image analysis software is crucial for assessing the quality of ultrasound images of fetal biometry. In this work, we present the result of our automated image analysis method based on a machine learning algorithm in detecting important anatomical landmarks employed in manual scoring of ultrasound images of the fetal abdomen. Experimental results on 2384 images are promising and the clinical validation using 300 images demonstrates a high level agreement between the automated method and experts.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6340174,no
CASViD: Application Level Monitoring for SLA Violation Detection in Clouds,"Cloud resources and services are offered based on Service Level Agreements (SLAs) that state usage terms and penalties in case of violations. Although, there is a large body of work in the area of SLA provisioning and monitoring at infrastructure and platform layers, SLAs are usually assumed to be guaranteed at the application layer. However, application monitoring is a challenging task due to monitored metrics of the platform or infrastructure layer that cannot be easily mapped to the required metrics at the application layer. Sophisticated SLA monitoring among those layers to avoid costly SLA penalties and maximize the provider profit is still an open research challenge. This paper proposes an application monitoring architecture named CASViD, which stands for Cloud Application SLA Violation Detection architecture. CASViD architecture monitors and detects SLA violations at the application layer, and includes tools for resource allocation, scheduling, and deployment. Different from most of the existing monitoring architectures, CASViD focuses on application level monitoring, which is relevant when multiple customers share the same resources in a Cloud environment. We evaluate our architecture in a real Cloud testbed using applications that exhibit heterogeneous behaviors in order to investigate the effective measurement intervals for efficient monitoring of different application types. The achieved results show that our architecture, with low intrusion level, is able to monitor, detect SLA violations, and suggest effective measurement intervals for various workloads.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6340204,no
Workload-Aware Online Anomaly Detection in Enterprise Applications with Local Outlier Factor,"Detecting anomalies are essential for improving the reliability of enterprise applications. Current approaches set thresholds for metrics or model correlations between metrics, and anomalies are detected when the thresholds are violated or the correlations are broken. However, we have found that the dynamic workload fluctuating over multiple time scales causes system metrics and their correlations to change. Moreover, it is difficult to model various metric correlations in complex applications. This paper addresses these problems and proposes an online anomaly detection approach for enterprise applications. A method is presented for recognizing workload patterns with an incremental clustering algorithm. The Local Outlier Factor (LOF) based on the specific workload pattern is adopted for detecting anomalies. Our approach is evaluated on a testbed running the TPC-W benchmark. The experimental results show that our approach can capture workload fluctuations accurately and detect the typical faults effectively.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6340251,no
Photo-generated carriers decay behavior of nano-crystalline -SiC thin film grown at different substrate bias voltage,"The photo-generated carriers decay behavior of nanocrystalline -SiC film grown by helicon wave plasma-enhanced chemical vapor deposition process at different substrate bias voltage is measured by microwave absorption technique. The probability of nanosecond fast decay of the photo-generated carriers concentration increases with substrate bias. The fast decay of photo generated carriers related to the radiative recombination process and substrate bias led to an increase in trap depth in the slow decay. The decay of nano-SiC thin films at different substrate bias voltage with different time constant indicates that the increase of substrate negative bias led to the increase in the density of deep trap levels and different trap level position related to different decay time. Photo-carrier transient behavior is closely related to the film microstructure, The high defect states density of nano-silicon carbide grain boundary lead to carriers trapping probability increase and the nonradiative recombination probability decreases.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6340699,no
Automated Web Service Composition Using Genetic Programming,"Automated web service composition can largely reduce human efforts in business integration. We present an approach to fully automate web service composition without workflow or knowing the semantic meaning of atomic web service. The experiment results show that the accuracy of our composition method using Genetic Programming (GP), in terms of the number of times an expected composition that can be derived versus the total number of runs, can be over 90%. Based on the traditional GP used in web service composition, our algorithm achieved improvements in three aspects: 1. We do black-box testing on each individual in each population. The success rate of tests is taken into account by the fitness function of GP so that the convergence rate can be faster; 2. We comply with services knowledge rules such as service dependency graph (SDG) when generating individual web service compositions in each population to improve the convergence process and population quality; 3. We choose cross-over or mutation operation based on the parent individuals' input and output analysis instead of by probability as typically done in related work. In this way, GP can generate better children even under the same parents.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6341542,no
A Prototype of Network Failure Avoidance Functionality for SAGE Using OpenFlow,"A tiled display wall (TDW), which is a single large display device composed of multiple sets of computers and displays, has recently gained the attention of scientists. In particular, SAGE, which is a middleware for building TDW, allows scientists to browse a multiple series of visualized results of computer simulation and analysis through the use of network streaming. Each visualized result can be generated on a different remote computer. For this reason SAGE has been increasingly hailed as a promising visualization technology that will solve the geographical distribution problem of computational and data resources. SAGE depends heavily on a network streaming technique in its architecture, but does not have any recovery mechanism against cases of network problems. This research, therefore, aims at realizing a network failure avoidance functionality for SAGE, focusing on OpenFlow against SAGE vulnerability to network failure. Specifically, the functionality is designed and developed as a composition of the following three functions: network failure detection, network topology understanding, and a packet forwarding control function. The key concept behind our design is that the network control function from OpenFlow should be built into SAGE. The evaluation in the paper confirms that the proposed and prototyped network failure avoidance functionality can detect failures on network routes and then reroute network streaming for visualization on TDW.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6341556,no
A Reputation System for Trustworthy QoS Information in Service-Based Systems,"During service-based systems (SBS) development, the qualities of service (QoS) are significant factors to compose high-quality workflows and the QoS claimed by service providers may be not trustworthy enough. In this paper, a reputation system supervising both service providers and service clients without much monitoring cost is introduced. The approach is based on the claimed QoS deviation from the feedback QoS reported by monitors or clients and is able to provide more trustworthy predicted QoS. An experiment conducted at last validates that the reputation system is effective and efficient.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6341571,no
Simplifying the Design of Signature Workflow with Patterns,"Signatures responsible for authentication, authorization, etc, are important in many workflow applications. Most studies associated with signatures are focused on digital signatures only, and modeling of signature workflows is seldom studied. However, the dependencies between signatures can be complex, and thus modeling signature workflows becomes time consuming and error-prone. In this paper, we propose six patterns to simplify the design of signature workflows. All the patterns are described in BPMN and a case study is made to illustrate how to apply these patterns in construction of a workflow. A method for applying these patterns in development of workflows is sketched, and the advantages, simplifying the construction of a workflow with BPMN, are also revealed with the case study.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6341590,no
SE-EQUAM - An Evolvable Quality Metamodel,"Quality has become a key assessment factor for organizations to determine if their software ecosystems are capable to meet constantly changing environmental factors and requirements. Many quality models exist to assess the evolvability or maintainability of software systems. Common to these models is that they, contrary to the software ecosystems they are assessing, are not evolvable or reusable. In this research, we introduce SE-EQUAM a novel ontology-based quality assessment metamodel that was designed from ground up to support model reuse and evolvability. SE-EQUAM takes advantage of Semantic Web technologies such as support for the open world assumption, incremental knowledge population, and knowledge inference. We present a case study that illustrates the reusability and evolvability of our SE-EQUAM approach.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6341597,no
Integration and Analysis of Design Artefacts in Embedded Software Development,"In model-based development of embedded software product lines, artefacts, i. e. the requirements document, implementation model, and tests, often become extremely complex w. r. t. size and dependencies. Moreover, the interrelationships among the artefacts are not obvious and information about development, design decisions as well as variability-related aspects are missing. Hence, engineers have to thoroughly analyse such dependencies to incorporate changes during evolution of the product (line) to assure quality. As this task is time-intensive and error-prone such analysis efforts have to be automated. This paper presents a comprehensive and extensible framework under development which provides (1) artefact integration and (2) analysis functionality to address these issues by following an approach based on a central database.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6341626,no
Internet-Based Evaluation and Prediction of Web Services Trustworthiness,"As most Web services are delivered by third parties over unreliable Internet and are late bound at run-time, it is reasonable and useful to evaluate and predict the trustworthiness of Web services. In this paper, we propose a novel approach to evaluate and predict Web services trustworthiness using comprehensive trustworthy evidences collected from the Internet. First, we use an effective way to collect comprehensive trustworthy evidences from the Internet, which include both objective evidences (e.g. QoS) and subjective evidences (e.g. reputation). Second, Web services trustworthiness is evaluated with collected trustworthy evidences on a regular basis. Finally, the cumulative evaluation records are modeled as time series, and we propose a multi-step Web services trustworthiness prediction process, which can automatically and iteratively identify and optimize the model to fit the trustworthiness series data. Experiments conducted on a large-scale real-world dataset show that our method manages to collect comprehensive trustworthy evidences from the Internet and can effectively evaluate and predict the trustworthiness of Web services, which helps users to reuse Web services.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6341637,no
Applying Distributed Object Technology to Distributed Embedded Control Systems,"In this paper, we describe our Java RMI inspired Object Request Broker architecture MicroRMI for use with networked embedded devices. MicroRMI relieves the software developer from the tedious and error- prone job of writing communication protocols for interacting with such embedded devices. MicroRMI supports easy integration of high-level application specific control logic with low-level device specific control logic. Our experience from applying MicroRMI in the context of a distributed robotics control application, clearly demonstrates that it is feasible to use distributed object technology in developing control systems for distributed embedded platforms possessing severe resource restrictions.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6341954,no
Enhance Software Quality Using Data Mining Algorithms,"In recent decades the production of large software projects are very large and is costly and time consuming during the phases of software development there are some bugs. Some of the errors generated by the software to detect errors in the initial is phases these errors and may not be seen until the final phases. To clear this error may be the next generation of software. Time and expense of producing the software is error. Error in this phase will increase the cost and time. Over time, larger projects And the error in estimating software cost is higher and higher. and these days detecting the possible defect is one of consideration to rely on software quality. So there is a need to create a prediction model and we can use data mining methods to predict defects. This paper examined ways of imposing clustering on various projects and putting them in groups with the similar characteristics. By using this pattern we can choose a defect predication model that is able to predict the defect of whole group.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6342020,no
Code Smell Detecting Tool and Code Smell-Structure Bug Relationship,"This paper proposes an approach for detecting the so- called bad smells in software known as Code Smell. In considering software bad smells, object-oriented software metrics were used to detect the source code whereby Eclipse Plugins were developed for detecting in which location of Java source code the bad smell appeared so that software refactoring could then take place. The detected source code was classified into 7 types: Large Class, Long Method, Parallel Inheritance Hierarchy, Long Parameter List, Lazy Class, Switch Statement, and Data Class. This work conducted analysis by using 323 java classes to ascertain the relationship between the code smell and structural defects of software by using the data mining techniques of Naive Bayes and Association Rules. The result of the Naive Bayes test showed that the Lazy Class caused structural defects in DLS, DE, and Se. Also, Data Class caused structural defects in UwF, DE, and Se, while Long Method, Large Class, Data Class, and Switch Statement caused structural defects in UwF and Se. Finally, Parallel Inheritance Hierarchy caused structural defects in Se. However, Long Parameter List caused no structural defects whatsoever. The results of the Association Rules test found that the Lazy Class code smell caused structural defects in DLS and DE, which corresponded to the results of the Naive Bayes test.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6342082,no
Efficient binary representation of delta Quantization Parameter for High Efficiency Video Coding,"This paper proposes an efficient binary representation of delta Quantization Parameter (QP) for High Efficiency Video Coding (HEVC). Video encoders adapt QPs of coding blocks for visual quality optimization and rate control. Although they send only delta QPs (dQPs) obtained by causal prediction, the side information overhead is expensive. Therefore the HEVC design necessitates an efficient dQP coding. The proposed scheme converts a dQP to a binary string in which the first and second bins indicate the significance and sign of the dQP respectively and the rest represents the magnitude minus 1. Furthermore, it detects and truncates redundant bins in the binary strings by using the sign and an admissible dQP range. Thus it reduces the length of dQP binary strings and improves dQP coding efficiency. Simulation results using HEVC reference software demonstrate that the proposed scheme improves the dQP coding efficiency by 6% while reducing its bin rates by 25%.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6343414,no
Location of DC line faults in conventional HVDC systems with segments of cables and overhead lines using terminal measurements,"Summary form only given. This paper presents a novel algorithm to determine the location of DC line faults in an HVDC system with a mixed transmission media consisting of overhead lines and cables, using only the measurements taken at the rectifier and inverter ends of the composite transmission line. The algorithm relies on the travelling wave principle, and requires the fault generated surge arrival times at two ends of the DC line as inputs. With accurate surge arrival times obtained from time synchronized measurements, the proposed algorithm can accurately predict the faulty segment as well as the exact fault location. Continuous wavelet transform coefficients of the input signal are used to determine the precise time of arrival of travelling waves at the DC line terminals. Two possible input signals, the DC voltage measured at the converter terminal and the current through the surge capacitors connected at the DC line end, are examined and both signals are found to be equally effective for detecting the travelling wave arrival times. Performance of the proposed fault-location scheme is analyzed through detailed simulations carried out using the electromagnetic transient simulation software PSCAD. The impact of measurement noise on the fault location accuracy is also studied in the paper.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6343919,no
Guidelines for selection of an optimal structuring element for Mathematical Morphology based tools to detect power system disturbances,"Mathematical Morphology (MM) has been reported as a promising application to detect power system disturbances. The real-time applications of MM based tools are also reported to detect disturbances. However, there is no clear guideline for selection of the structuring element for a particular application, despite the fact that the structuring element is a key component of any MM based tool. This paper shows a method to generalize and numerically optimize the structuring element to detect power system disturbances. Power system fault cases are simulated using a professional time-domain software, and the current and voltage waveforms from these cases are used to illustrate the methodology. Results are observed and analyzed. Some guidelines to select an optimum structuring element to detect power system disturbances are provided based on the results.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6345105,no
Model-based integration technology for next generation electric grid simulations,"Simulation-based evaluation of the behavior of the electric grid is complex, as it involves multiple, heterogeneous, interacting cyber-physical system like domains. Each simulation domain has sophisticated tools, but their integration into a coherent framework is a very difficult, time-consuming, laborintensive, and error-prone task. This means that computational studies cannot be done rapidly and the process does not provide timely answers to the planners, operators and policy makers. Furthermore, grid behavior has to be tested against a number of scenarios and situations, meaning that a huge number of simulations must be executed covering the potential space of possibilities. Designing and efficiently deploying such computational experiments by utilizing multi-domain tools for integrated smart grid is a major challenge. This paper addresses these important issues by integrating multiple modeling tools from diverse domains in a single coherent framework for integrated simulation of smart grids.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6345323,no
An analysis of free Web-based PHRs functionalities and I18n,"The growth of the Internet, Web technologies, and other electronic tools are allowing the public to become more informed and actively engaged in their health care than was possible in the past. Personal Health Records (PHR) offer users possibility of managing their own health data. Many patients are using PHRs to communicate with doctors in order to improve healthcare quality and efficiency. A large number of companies have emerged to provide consumers with the opportunity to use online PHRs within a healthcare platform, proposing different functionalities and services. This paper analyzes and assesses the functionalities and internationalization (i18n) of free Web based PHRs.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6346172,no
Detecting flash artifacts in fundus imagery,"In a telemedicine environment for retinopathy screening, a quality check is needed on initial input images to ensure sufficient clarity for proper diagnosis. This is true whether the system uses human screeners or automated software for diagnosis. We present a method for the detection of flash artifacts found in retina images. We have collected a set of retina fundus imagery from February 2009 to August 2011 from several clinics in the mid-South region of the USA as part of a telemedical project. These images have been screened with a quality check that sometimes omits specific flash artifacts, which can be detrimental for automated detection of retina anomalies. A multi-step method for detecting flash artifacts in the center area of the retina was created by combining characteristic colorimetric information and morphological pattern matching. The flash detection was tested on a dataset of 5218 images representative of the population. The system achieved a sensitivity of 96.54% and specificity of 70.16% for the detection of the flash artifacts. The flash artifact detection can serve as a useful tool in quality screening of retina images in a telemedicine network. The detection can be expected to improve automated detection by either providing special handling for these images in combination with a flash mitigation or removal method.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6346211,no
Online monitoring and diagnosis of RFID readers and tags,"The need to fault-tolerance in RFID systems is increasing with the increased use of this technology in critical domains such as real-time processing fields. Although many efforts have been made to make this technology more dependable, it is still unreliable. In this paper, we propose a complementary approach to existing ones. It consists of a probabilistic monitoring that detects failures of RFID system components and a verification process that refines the diagnosis process by finding the causes of the detected failures.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6347629,no
An approach of attribute selection for reducing false alarms,"Defect Prediction is one of the method in SQA (Software Quality Assurance) that attracts developers because it can reduce the testing efforts as well as development time. One problem in defect prediction is `curse of dimensionality', as hundreds of attributes are there in a dataset in software repository. In this paper we try to analyze whether there is any way to remove more attributes after attribute selection and the effect of this reduction of attributes on performance of defect prediction. We found that False positive rate (False Alarms) is reduced by using our method of attribute selection, which in turn can be used to reduce the resource allocation for detecting defective modules.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6349479,no
Investigating object-oriented design metrics to predict fault-proneness of software modules,"This paper empirically investigates the relationship of class design level object-oriented metrics with fault proneness of object-oriented software system. The aim of this study is to evaluate the capability of the design attributes related to coupling, cohesion, complexity, inheritance and size with their corresponding metrics in predicting fault proneness both in independent and combine basis. In this paper, we conducted two set of systematic investigations using publicly available project datasets over its multiple subsequent releases to performed our investigation and four machine learning techniques to validated our results. The first set of investigation consisted of applying the univariate logistic regression (ULR), Spearman's correlation and AUC (Area under ROC curve) analysis on four PROMISE datasets. This investigation evaluated the capability of each metric to predict fault proneness, when used in isolation. The second set of experiments consisted of applying the four machine learning techniques on the next two subsequent versions of the same project datasets to validate the effectiveness of the metrics. Based on the results of individual performance of metrics, we used only those metrics that are found significant, to build multivariate prediction models. Next, we evaluated the significant metrics related to design attributes both in isolation and in combination to validated their capability of predicting fault proneness. Our results suggested that models built on coupling and complexity metrics are better and more accurate than those built on using the rest of metrics.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6349484,no
A quantitative model for the evaluation of reengineering risk in infrastructure perspective of legacy system,"Competitive business environment wants to revolutionize existing legacy system in to self-adaptive ones. Nowadays legacy system reengineering has emerged as a well-known system renovation technique. Reengineering rapidly replace legacy development for keeping up with modern business and user requirements. However renovation of legacy system through reengineering is a risky and error-prone mission due to widespread changes it requires in the majority of case. Quantifiable risk measures are necessary for the measurement of reengineering risk to take decision about when the modernization of legacy system through reengineering is successful. We present a quantifiable measurement model to measure comprehensive impact of different reengineering risk arises from infrastructure perspective of legacy system. The model consists of five reengineering risk component, including Deployment Risk, Organizational Risk, Resource Risk, Development Process Risk and Personal Risk component. The results of proposed measurement model provide guidance to take decision about the evolution of a legacy system through reengineering.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6349488,no
Incorporating fault dependent correction delay in SRGM with testing effort and release policy analysis,"Software Reliability growth models are helping the software society in predicting and analyzing the software product in terms of quality. In this context several software reliability growth models are proposed in the literature. Majority of models concentrated on fault detection process, ignoring the correction. Error detection, correction and dependency are the important phenomenon for the software reliability models. In this paper we proposed a new SRGM model based on correction lag and error dependency with incorporating the testing effort. All numerical calculations are carried out on real datasets and results are analyzed. By analyzing the results our proposed model fits well for the datasets.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6349508,no
Impedance angle changes analysis applied to short circuit fault detection,"Induction motor winding fault is one of the frequent faults, and one of the most important reasons of making traction motors out of order. In this paper, an appropriate and effective method based on impedance angle changes is proposed to detect turn to turn fault. To do this, finite element method (FEM) by helping of ANSOFT software is used for creating different turn to turn faults conditions. A 1.5 KW, 3-phase squirrel cage induction motor has been used for experimental tests and verifying simulation results.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6350113,no
Changes in submerged macrophyte communities in southern Lake Garda in the last 14-years,"In this study, in situ data and hyperspectral MIVIS (Multispectral Infrared and Visible Imaging Spectrometer) images collected over a period of 14 years were used to assess changes in submerged macrophytes colonization patterns in southern Lake Garda.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6351936,no
Artificial neural network-based metric selection for software fault-prone prediction model,"The identification of a module's fault-proneness is very important for minimising cost and improving the effectiveness of the software development process. How to obtain the relation between software metrics and a module's fault-proneness has been the focus of much research. One technical challenge to obtain this relation is that there is relevance between software metrics. To overcome this problem, the authors propose a reduction dimensionality phase, which can be generally implemented in any software fault-prone prediction model. In this study, the authors present applications of artificial neural network (ANN) and support vector machine in software fault-prone prediction using metrics. A new evaluation function for computing the contribution of each metric is also proposed in order to adapt to the characteristics of software data. The vital characteristic of this approach is the automatic determination of ANN architecture during metrics selection. Four software datasets are used for evaluating the performance of the proposed model. The experimental results show that the proposed model can establish the relation between software metrics and modules' fault-proneness. Moreover, it is also very simple because its implementation requires neither extra cost nor expert's knowledge. The proposed model has good performance, and can provide software project managers with trustworthy indicators of fault prone components.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6353333,yes
Cloud Monitor: Monitoring Applications in Cloud,"With the advent of cloud computing applications, monitoring becomes a valid concern. Monitoring for failures in a cloud application is difficult because of multiple failure points spanning both hardware and software. Moreover the cluster nature of a cloud application increases the scope of failure and it becomes even harder to detect the same. This paper presents Cloud Monitor - a scalable framework for monitoring cloud applications. Cloud Monitor monitors cluster nodes for errors. It supports dependent monitors, redundancy, multiple notification levels and auto-healing. Cloud Manager supports a flexible architecture where users can add custom monitors and associated self-heal actions.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6354603,no
Detecting Workload Hotspots and Dynamic Provisioning of Virtual Machines in Clouds,"One of the primary goals of Cloud Computing is to provide reliable QoS. The users of the cloud applications may access their applications from any Region. The cloud infrastructure must be Elastic enough to improve the QoS requirements. In order to provide reliable QoS, the cloud infrastructure must be able to detect the potential workload hotspots for various cloud applications across Regions and take appropriate measures. This paper presents an approach to detect workload hotspots using application access pattern based method in the cloud. This paper also presents how the existing VDN based Virtual Machine provisioning approach [1] can be used to provision new Virtual Appliances at the detected hotspots dynamically and efficiently at the potential hotspots to improve the QoS.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6354606,no
Anomaly Teletraffic Intrusion Detection Systems on Hadoop-Based Platforms: A Survey of Some Problems and Solutions,"Telecommunication networks are getting more important in our social lives because many people want to share their information and ideas. Thanks to the rapid development of the Internet and ubiquitous technologies including mobile devices such as smart phones, mobile phones and tablet PCs, the quality of our lives has been greatly influenced and rapidly changed in recent years. Internet users have exponentially increased as well. Meanwhile, the explosive growth of teletraffic called big data for user services threatens the current networks, and we face menaces from various kinds of intrusive incidents through the Internet. A variety of network attacks on network resources have continuously caused serious damage. Thus, active and advanced technologies for early detecting of anomaly teletraffic on Hadoop-based platforms are required. In this paper, a survey of some problems and technical solutions for anomaly teletraffic intrusion detection systems based on the open-source software platform Hadoop has been investigated and proposed.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6354921,no
Transient-Error Detection and Recovery via Reverse Computation and Checkpointing,"The integration of error detection and recovery mechanisms becomes mandatory as the probability of the occurrence of transient errors increases. The current study proposes a software-based fault tolerant technique that achieves both detection and recovery. The proposed technique is based on two main mechanisms, namely, reverse computation and check pointing. This study is the first to introduce reverse computation for error detection by comparing the input data of the original computation and the output data of the reverse computation. Live variable analysis is introduced to reduce the overhead of the check pointing technique. A translation tool is implemented to make the original source code fault tolerant with automatic error detection and recovery abilities. Fault injection and performance overhead experiments are performed to evaluate the proposed technique. Experimental results show that most errors can be recovered with relatively low performance overhead.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6355861,no
Partial discharge monitoring system for PD characteristics of typical defects in GIS using UHF method,"GIS is now widely used in power system because of its compact structure, easy maintenance, and reliable operation. It is necessary to detect partial discharge (PD) in GIS because partial discharge (PD) causes deterioration of insulation and losses of power system. UHF method has great advantages of high sensitivity, strong anti-interference ability, the ability of locating PD sources, and recognizing the defect type. In this, the partial discharge monitoring system for GIS Based on UHF Method is introduced including both the hardware and software systems. A PD detection and diagnosis test system for GIS is also established based on the monitoring system above. The PD characteristics of two typical defects in GIS are researched, and the Phase Resolved Partial Discharge (PRPD) spectrograms are drawn after dealing with the PD Pulse Sequence of typical defects models, by which the type of defect could be identified.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6357053,no
Discrete wavelet transform and probabilistic neural network algorithm for classification of fault type in underground cable,"This paper proposes an algorithm based on a combination of discrete wavelet transform (DWT) and probabilistic neural network (PNN) for classifying fault types on underground cable. Simulations and the training process for the PNN are performed using ATPIEMTP and MATLAB. The mother wavelet daubechies4 (db4) is employed to decompose high frequency component from these signals. The maximum coefficients of DWT of phase A, B, C and zero sequence for post-fault current waveforms are used as an input for the training pattern. Various cases studies based on Thailand electricity distribution underground systems have been investigated so that the algorithm can be implemented. The coefficients of DWT are also compared with those of PNN in this paper. The results show that the proposed algorithm is capable of performing the fault classification with satisfactory accuracy.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6358940,no
Using the fuzzy analytic hierarchy process to the balanced scorecard: A case study for the elementary and secondary schools' information department of south Taiwan,"The purpose of this study is to establish balanced scorecard (BSC) in performance measurement of elementary and secondary schools' MIS Department. We take a broader definition of elementary and secondary schools' MIS Department as an assembly which brings forth some specific functional activities to fulfill the task of MIS. BSC used as a measurement tool to assess study subjects, according to its strategy and goal formed by its assignment property, can be divided into four dimensions: finance, customer, inter process, learning and growth, which can provide us with a timely, efficient, flexible, simple, accurate, and highly overall reliable measurement tool. In order to extract the knowledge and experience from related experts to pick out important evaluation criteria and opinion, this study combines fuzzy theory and the analytical hierarchy process (AHP) to calculate the weights. After completing weighted calculation of every dimension and indicator, the BSC model is thus established. The findings of this study show that the indicator weightings between and among all the levels are not the same, rather there exists certain amount of differences. The degrees of attention drawing in order of importance among all dimensions are customer, financial, internal process and learning and growth dimension. After comprehensively analyzing indictors of performance measurement included in every level, the highly valued top five indictors are, when conducting dimension performance measurement in elementary and secondary schools' MIS Department, Rationalize software and hardware and maintenance expenses, Budget satisfy and control, Quick response and handling, Improve service quality, and High effective information system.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6359574,no
Narrowing the gaps in Concern-Driven Development,"Concern-Driven Development (CDD) promises improved productivity, reusability, and maintainability because high-level concerns that are important to stakeholders are encapsulated regardless of how these concerns are distributed over the system structure. However, to truly capitalize on the benefits promised by CDD, concerns need to be encapsulated across software development phases, i.e., across different types of models at different levels of abstraction. Model-Driven Engineering plays an important role in this context as the automated transformation of concern-oriented models (a) allows a software engineer to use the most appropriate modeling notation for a particular task, (b) automates error-prone tasks, and (c) avoids duplication of modeling effort. The earlier transformations can be applied in a CDD process, the greater the potential cost savings. Hence, we report on our experiences in applying tool supported transformations from scenario-based requirements models to structural and behavioral design models during CDD. While automated model transformations certainly contribute to the three benefits mentioned above, they can also lead to more clearly and succinctly defined modeling activities at each modeling level and aid in the precise definition of the semantics of the used modeling notations.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6360085,no
Impact assessment of AC and DC electric rope shovels on coal mine power distribution system,"Electric rope shovel is a major piece of equipment in coal mines. They consume significant amounts of power and each of their motions require few thousands kVA. Therefore, commissioning a new shovel in the existing power system of a mine has to be done with very careful analysis of its impact on the network. Aspects such as load flow studies, fault analysis, protection co-ordination, harmonic analysis and arc flash studies are of great interest for the power system engineers on site. The aim of this research paper is to present the modeling process used to simulate the operation of both an electric rope shovel in operation and another new shovel to be commissioned in a coal mine and assess their impact on the power system.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6360243,no
Classification and tendencies of evaluations in e-learning,"The use of information technology in education is a very promising field which has led to many paradigms, educational models and the implementation of e-learning systems that are effectively used in education and training. For educators, training professionals and corporate managers, it is always hectic to choose the right approach and accordingly the e-learning system that fit their actual business needs. This is mainly due to the lack of comprehensive studies done to assess and compare existing systems that allow guiding stakeholders choosing the right e-learning system for the right learners in the right learning context. This paper is a contribution towards this direction; it proposes an overview of approaches that have been conducted to assess e-learning systems. From this study we propose a classification of approaches and e-learning systems that is based on four main criteria: i) Who: that deals with the stakeholders and actors of learning systems; ii) What: which represents the elements of an e-learning system to evaluate; iii) When: addresses the phase of development of the e-learning system in which the assessment is done; iv) Which: the method used to evaluate the systems. The results of this work are presented and discussed in this paper.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6360570,no
Implementation of remote temperature-measuring by using a thermopile for Wireless Sensor Network,"Wireless Sensor Networks (WSN) is an important platform to build an intelligent house in the future. This paper describes a non-contact thermometer by detecting an object's radiant power. We attempt to design a portable device for remote measuring temperature, and then the device will be developed as a sensing node for WSN. A thermopile equipped with a lens was used to implement the performance. The detection range is from 0C to 300C. The average error is within 3C. In this study, LabVIEW software was used to perform the design of a thermistor linearization and data logging. This study may provide a useful reference for researchers attempting to increase quality of remote radiometry<sup>.</sup>",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6360927,no
Context and policy based fault-tolerant scheme in mobile ubiquitous computing environment,"In ubiquitous computing system, the increasing mobile and dynamic of software and hardware resources and frequentative interaction among function components make fault-tolerant design very challenging. In this paper, we propose a context and policy based self-adaptive fault-tolerant mechanism for a mobile ubiquitous computing environment such as a mobile ad hoc network. In our approach, the fault-tolerant mechanism is dynamically built according to various types of detected faults based on continuous monitoring and analysis of the component states. We put forward the architecture of fault-tolerant system and the context-based and policy-based fault-tolerant scheme, which adopts ontology-based context modeling method and the Event-Condition-Action execution rules. The mechanism has been designed and implemented as self-adaptive fault-tolerant middleware, shortly called SAFTM, on a preliminary prototype for a dynamic ubiquitous computing environment such as mobile ad hoc network. We have performed the experiments to evaluate the efficiency of the fault-tolerant mechanism. The results of the experiments show that the performance of the self-adaptive fault tolerant mechanism is realistic.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6361001,no
An Empirical Analysis on Fault-Proneness of Well-Commented Modules,"Comment statements are useful to enhance the readability and/or understandability of software modules. However, some comments may adjust the readability/understandability of code fragments that are too complicated and hard to understand-a kind of code smell. Consequently, some well-written comments may be signs of poorquality modules. This paper focuses on the lines of comments written in modules, and performs an empirical analysis with three major open source software and their fault data. The empirical results show that the risk of being faulty in wellcommented modules is about 2 to 8 times greater than noncommented modules.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6363289,no
Locating Source Code to Be Fixed Based on Initial Bug Reports - A Case Study on the Eclipse Project,"In most software development, a Bug Tracking System is used to improve software quality. Based on bug reports managed by the bug tracking system, triagers who assign a bug to fixers and fixers need to pinpoint buggy files that should be fixed. However if triagers do not know the details of the buggy file, it is difficult to select an appropriate fixer. If fixers can identify the buggy files, they can fix the bug in a short time. In this paper, we propose a method to quickly locate the buggy file in a source code repository using 3 approaches, text mining, code mining, and change history mining to rank files that may be causing bugs. (1) The text mining approach ranks files based on the textual similarity between a bug report and source code. (2) The code mining approach ranks files based on prediction of the fault-prone module using source code product metrics. (3) The change history mining approach ranks files based on prediction of the fault-prone module using change process metrics. Using Eclipse platform project data, our proposed model gains around 20% in TOP1 prediction. This result means that the buggy files are ranked first in 20% of bug reports. Furthermore, bug reports that consist of a short description and many specific words easily identify and locate the buggy file.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6363290,no
Predicting Fault-Prone Modules Using the Length of Identifiers,"Identifiers such as variable names and function names in source code are essential information to understand code. The naming for identifiers affects on code understandability, thus, we expect that they affect on software quality. In this study, we examine the relationship between the length of identifiers and existence of software faults in a software module. The results of experiment using the random forest technique show that there is a positive relationship between the length of identifier and existence of software faults.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6363293,no
Table of contents,The following topics are dealt with: software engineering; source code location;p QORAL; fault-prone modules prediction; service-oriented MSR integration; and coding patterns.,2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6363311,no
A Static Approach to Prioritizing JUnit Test Cases,"Test case prioritization is used in regression testing to schedule the execution order of test cases so as to expose faults earlier in testing. Over the past few years, many test case prioritization techniques have been proposed in the literature. Most of these techniques require data on dynamic execution in the form of code coverage information for test cases. However, the collection of dynamic code coverage information on test cases has several associated drawbacks including cost increases and reduction in prioritization precision. In this paper, we propose an approach to prioritizing test cases in the absence of coverage information that operates on Java programs tested under the JUnit framework-an increasingly popular class of systems. Our approach, JUnit test case Prioritization Techniques operating in the Absence of coverage information (JUPTA), analyzes the static call graphs of JUnit test cases and the program under test to estimate the ability of each test case to achieve code coverage, and then schedules the order of these test cases based on those estimates. To evaluate the effectiveness of JUPTA, we conducted an empirical study on 19 versions of four Java programs ranging from 2K-80K lines of code, and compared several variants of JUPTA with three control techniques, and several other existing dynamic coverage-based test case prioritization techniques, assessing the abilities of the techniques to increase the rate of fault detection of test suites. Our results show that the test suites constructed by JUPTA are more effective than those in random and untreated test orders in terms of fault-detection effectiveness. Although the test suites constructed by dynamic coverage-based techniques retain fault-detection effectiveness advantages, the fault-detection effectiveness of the test suites constructed by JUPTA is close to that of the test suites constructed by those techniques, and the fault-detection effectiveness of the test suites constructed by some of - UPTA's variants is better than that of the test suites constructed by several of those techniques.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6363461,no
AltAnalyze - An Optimized Platform for RNA-Seq Splicing and Domain-Level Analyses,"The deep sequencing of transcriptomes has revolutionized our ability to detect known and novel RNA variants at a never before observed resolution. To capitalize on these ever improving technologies, we require functionally rich methods of annotation to predict and evaluate the consequences of RNA isoform variation at the level of proteins, domains and microRNA binding sites. We introduce a new version of the popular open-source application AltAnalyze, capable of analyzing RNA-Sequencing (RNA-Seq) datasets as well as splicing-sensitive or conventional arrays. This software can be run through an intuitive graphical user interface or command-line. Over 60 species and data from various RNA-Seq alignment workflows are immediately supported without any specialized configuration. AltAnalyze provides multiple options for gene expression quantification, filtering, quality control and biological interpretation. Hierarchical clustering heatmaps, principal component analysis plots, lineage correlation diagrams and visualization of enriched pathways are automatically produced for differentially expressed genes. For detection of alternative splicing, promoter or polyadenylation events, AltAnalyze combines both reciprocal-junction and alternative-exon expression approaches to identify annotated and novel RNA variation. By connecting these regulated splicing-events with optimal inclusion and exclusion isoforms, AltAnalyze is able to evaluate the impact of alternative RNA expression on protein domains, annotated motifs and binding sites for microRNAs. From a broader perspective, AltAnalyze examines the enrichment of effected domains and microRNA binding sites, to highlight the global impact of alternative splicing. Together, AltAnalyze provides an efficient, streamlined and comprehensive set of analysis results, to determine the biological impact of transcriptome regulation.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6366207,no
ACCGen: An Automatic ArchC Compiler Generator,"The current level of circuit integration led to complex designs encompassing full systems on a single chip, known as System-on-a-Chip (SoC). In order to predict the best design options and reduce the design costs, designers are required to perform a large design space exploration on early stages of the design. To speed up this process, Electronic Design Automation (EDA) tools are employed to model and experiment with the system. ArchC is an """"Architecture Description Language"""" (ADL) and a set of tools that can be leveraged to automatically build SoC simulators based on high-level system models, enabling easy and fast design space exploration in early stages of the design. Currently, ArchC is capable of automatically generating hardware simulators, assemblers, and linkers for a given architecture model. In this work, we present ACCGen, an automatic Compiler Generator for ArchC, the missing link on the automatic generation of compiler tool chains for ArchC. Our experimental results show that compilers generated by ACCGen are correct for Mibench applications. They compare, as well, the generated code quality with LLVM and gcc, two well-known open-source compilers. We also show that ACCGen is fast and has little impact on the design space exploration turnaround time, allowing the designer to, using an easy and fully automated workflow, completely assess the outcome of architectural changes in less than 2 minutes.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6374799,no
A Framework for Generating Integrated Component Fault Trees from Architectural Views,"Safety is a property of a system which can only be assessed by conducting analysis which reveals how interacting components create situations that are unsafe because components that individually fulfill their requirements do not ensure safety at the system level. CFTs(Component Fault Trees) [1] which are specialized fault trees have been used as models to analyze systems. Systems today are typically built by groups of people who expertise in different disciplines. One of the problems of the current state of art is that there is no structured way of combining information obtained from experts in various disciplines who have different views of a system into a CFT. We provide a framework using which one can semi-automatically combine CFTs created by several stakeholders/experts into a single integrated CFT. This enables one to effectively combine experience and wisdom of experts obtained from diverse perspectives of the system into a single, more complete CFT. The resulting integrated CFT(which we call iCFT) allows safety engineers or other stakeholders to see the influences that components have on one another in a manner that would not have been revealed unless a system was viewed from varied perspectives.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6375605,no
Using Tool-Supported Model Based Safety Analysis -- Progress and Experiences in SAML Development,"Software controls in technical systems are becoming more and more important and complex. Model based safety analysis can give provably correct and complete results, often in a fully automatic way. These methods can answer both logical and probabilistic questions. In common practice, the needed models must be specified in different input languages of different tools depending on the chosen verification tool for the desired aspect. This is time consuming and error-prone. To cope with this problem we developed the safety analysis modeling language (SAML). In this paper, we present a new tool to intuitively create probabilistic, non-deterministic and deterministic specifications for formal analysis. The goal is to give tool-support during modeling and thus make building a formal model less error-prone. The model is then automatically transformed into the input language of state of the art verification engines. We illustrate the approach on a case-study from nuclear power plant domain.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6375611,no
Real-Time Anomaly Detection in Streams of Execution Traces,"For deployed systems, software fault detection can be challenging. Generally, faulty behaviors are detected based on execution logs, which may contain a large volume of execution traces, making analysis extremely difficult. This paper investigates and compares the effectiveness and efficiency of various data mining techniques for software fault detection based on execution logs, including clustering based, density based, and probabilistic automata based methods. However, some existing algorithms suffer from high complexity and do not scale well to large datasets. To address this problem, we present a suite of prefix tree based anomaly detection techniques. The prefix tree model serves as a compact loss less data representation of execution traces. Also, the prefix tree distance metric provides an effective heuristic to guide the search for execution traces having close proximity to each other. In the density based algorithm, the prefix tree distance is used to confine the K-nearest neighbor search to a small subset of the nodes, which greatly reduces the computing time without sacrificing accuracy. Experimental studies show a significant speedup in our prefix tree based and prefix tree distance guided approaches, from days to minutes in the best cases, in automated identification of software failures.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6375634,no
An Autonomic Reliability Improvement System for Cyber-Physical Systems,"System reliability is a fundamental requirement of cyber-physical systems. Unreliable systems can lead to disruption of service, financial cost and even loss of human life. Typical cyber-physical systems are designed to process large amounts of data, employ software as a system component, run online continuously and retain an operator-in-the-loop because of human judgment and accountability requirements for safety-critical systems. This paper describes a data-centric runtime monitoring system named ARIS (Autonomic Reliability Improvement System) for improving the reliability of these types of cyber-physical systems. ARIS employs automated online evaluation, working in parallel with the cyber-physical system to continuously conduct automated evaluation at multiple stages in the system workflow and provide real-time feedback for reliability improvement. This approach enables effective evaluation of data from cyber-physical systems. For example, abnormal input and output data can be detected and flagged through data quality analysis. As a result, alerts can be sent to the operator-in-the-loop, who can then take actions and make changes to the system based on these alerts in order to achieve minimal system downtime and higher system reliability. We have implemented ARIS in a large commercial building cyber-physical system in New York City, and our experiment has shown that it is effective and efficient in improving building system reliability.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6375637,no
On the development of Software-Based Self-Test methods for VLIW processors,"Software-Based Self-Test (SBST) approaches are an effective solution for detecting permanent faults; this technique has been widely used with a good success on generic processors and processors-based architectures; however, when VLIW processors are addressed, traditional SBST techniques and algorithms must be adapted to each particular VLIW architecture. In this paper, we present a method that formalizes the development flow to write effective SBST programs for VLIW processors, starting from known algorithms addressing traditional processors. In particular, the method addresses the parallel Functional Units, such as ALUs and MULs, embedded into a VLIW processor. Fault simulation campaigns confirm the validity of the proposed method.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6378194,no
Software exploitable hardware Trojans in embedded processor,"Growing threat of hardware Trojan attacks in untrusted foundry or design house has motivated researchers around the world to analyze the threat and develop effective countermeasures. In this paper, we focus on analyzing a specific class of hardware Trojans in embedded processor that can be enabled by software or data to leak critical information. These Trojans pose a serious threat in pervasively deployed embedded systems. An attacker can trigger these Trojans to extract valuable information from a system during field deployment. We show that an adversary can design a low-overhead hard-to-detect Trojan that can leak either secret keys stored in a processor, the code running in it, or the data being processed.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6378199,no
Old wine in new wineskins: Upgrading the liquids reflectometer instrument user control software at the Spallation Neutron Source,"The Liquids Reflectometer (LR) Instrument installed at the Spallation Neutron Source (SNS) enables observations of chemical kinetics, solid-state reactions, phase-transitions and chemical reactions in general [1]. The ability of the instrument to complete measurements quickly and therefore process many samples is a key capability inherent in the system design [2]. Alignment and sample environment management are a time consuming and error prone process that has led to the development of automation in the control software operating the instrument. In fact, the original LR user interface, based on the Python scripting language, has been modularized and adapted to become the standard interface on many other instruments. A project to convert the original Python [3] implementation controlling the LR instrument into the modular version standardized at SNS was undertaken in the spring of 2012. The key features of automated sample alignment and robot-driven sample management system enable the instrument to reduce the manual labor required to prepare and execute observations, freeing up precious time for analysis and reporting activity. We present the modular PyDas control system [4], its implementation for the LR, and the lessons learned during the upgrade process.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6378322,no
Stealth assessment of hardware Trojans in a microcontroller,"Many experimental hardware Trojans from the literature explore the potential threat vectors, but do not address the stealthiness of the malicious hardware. If a Trojan requires a large amount of area or power, then it can be easier to detect. Instead, a more focused attack can potentially avoid detection. This paper explores the cost in both area and power consumption of several small, focused attacks on an Intel 8051 microcontroller implemented with a standard cell library. The resulting cost in total area varied from a 0.4% increase in the design, down to a 0.150% increase in the design. Dynamic and leakage power showed similar results.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6378631,no
Probe-based distributed algorithms for deadlock detection,"Distributed algorithms discussed in this paper provide a better performance than other well known algorithms designed for detecting deadlock, with respect to the communication bus load and fault tolerance, while the same assumptions for the operating conditions are preserved. Several concepts are defined for a group of deadlocked distributed tasks: probe messages, propagation law of a probe message, centralized and distributed management of probe messages, and cyclic trace of a probe message. Also, a propagation law for probe messages and two theorems are formulated and tested to establish valuable conditions of deadlock detection in distributed applications.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6379236,no
Failure analysis of distributed scientific workflows executing in the cloud,"This work presents models characterizing failures observed during the execution of large scientific applications on Amazon EC2. Scientific workflows are used as the underlying abstraction for application representations. As scientific workflows scale to hundreds of thousands of distinct tasks, failures due to software and hardware faults become increasingly common. We study job failure models for data collected from 4 scientific applications, by our Stampede framework. In particular, we show that a Naive Bayes classifier can accurately predict the failure probability of jobs. The models allow us to predict job failures for a given execution resource and then use these failure predictions for two higher-level goals: (1) to suggest a better job assignment, and (2) to provide quantitative feedback to the workflow component developer about the robustness of their application codes.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6379991,no
Autoconfiguration of enterprise-class application deployment in virtualized infrastructure using OVF activation mechanisms,"IT-based services existing today, such as the ones supporting e-commerce systems or corporate applications, demand complex architectures to address enterprise-class requirements (high availability, vast user demand, etc.). In particular, most enterprise-class applications are multi-tiered and multi-node, i.e. composed of many independent systems with complex relationships among them. In recent years, virtualization technologies have brought many advantages to enterprise-class application implementation, such as cost consolidation and ease of management. However, even when using virtualization, the configuration operations associated to the deployment of enterprise-class applications are still a challenging task since they constitute a mostly manual, time-consuming and error prone process. In this paper we propose a solution to that problem based on the automation of that procedure by means of the OVF activation mechanism. Our work focuses on a practical case, which has been used to assess the feasibility of our solution and to extract valuable lessons that we expose as part of the article.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6380050,no
Power quality event source directivity detection based on V-I scatter graph,"This paper presents a power quality event source directivity detection based on V-I scatter graph. The proposed method is capable to detect both voltage sag and transient power quality event source direction in term of upstream and downstream based on one bus measurement. The V-I scatter graph are plotted based on the RMS voltage and current magnitude at the same instant of time sample onto the voltage versus current graph. Based on the basic principle of power flow, the power quality event source directivity detection are determine by the sector where the plotted VI samples exceeded the voltage and current limits set on the V-I scatter graph. The proposed method is evaluated using simulation model waveforms. The evaluation shows a very promising result of power quality event source direction detection for voltage sag caused by line fault and transient caused by capacitor bank energizing.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6381205,no
Real time power system harmonic distortion assessment virtual instrument,"This paper presents a real time power system harmonic distortion assessment instrument implemented using virtual instrument concept. The measurement hardware consists of measurement probes with signal conditioning circuit connected to a data acquisition module. The data acquisition module is interface to computer through a USB interface. Harmonic distortion assessment algorithm using Fourier transform was implemented using MATLAB. A graphical user interface was developed to display the harmonic distortion assessment in real time. The harmonic distortion is able assess individual harmonic order up to 50th order and the total harmonic distortion according to IEEE 519, IEC 61000-2-2 or EN 50160 standards selected by the user. An acceptability index is also proposed in this paper for trend assessment. The proposed real time harmonic distortion assessment virtual instrument was evaluated through field testing. The test results show promising performance for the proposed real time power system harmonic distortion assessment virtual instrument.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6381294,no
Jaguar: Time shifting air traffic scenarios using a genetic algorithm,"This paper describes the redesign of the Federal Aviation Administration's implementation of a genetic algorithm used for time shifting flights in air traffic scenarios. Time shifted scenarios are used in testing decision support tools that predict the potential loss of separation between aircraft. This paper describes the improvements that resulted when this application was redesigned and coded in Java. The improvements described in this paper include the following: Maintainability improved as a result of a modular design using object-oriented techniques. : Usability improved as a result of more efficient logging techniques, configuration methods, and user interfaces. : Quality of the solution improved as a result of a more accurate method for calculating of aircraft-to-aircraft conflicts. : Timeliness for obtaining a solution improved as a result of using modern software engineering techniques, such as distributing the fitness function across multiple processors and caching fitness scores.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6382328,no
A feasibility study for ARINC 653 based operational flight program development,"The aircraft manufacturers are constantly driving to reduce manufacturing lead times and cost at the same time as the product complexity increases and technology continues to change. As avionics systems have evolved, particularly over the past two or three decades, the level of functional integration has increased dramatically. Integrated modular avionics (IMA) is a solution that allows the aviation industry to manage their avionics complexity. IMA defines an integrated system architecture that preserves the fault containment and `separation of concerns' properties of the federated architectures, where independent functional chains share a common computing resource. In software side, the air transport industry has developed ARINC 653 specification as a standardized real time operating system (RTOS) interface definition for IMA. It allows hosting multiple applications of different software levels with partitions on the same hardware in the context of IMA architecture. The primary components of the ARINC 653 are core and applications software. This paper describes a study that assessed the feasibility of developing an ARINC 653 based operational flight program (OFP) prototype and will provide valuable lessons learned through OFP development. The OFP architecture consists of two distinct modules: a core that interfaces and monitors the hardware and provides a standard and common environment for software applications; and an application module that performs the avionics functions. The prototype OFP is being integrated with the FA-50 simulator at the avionics laboratory of Korea Aerospace Industries.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6382391,no
Maximizing fault tolerance in a low-s WaP data network,"The BRAIN (Braided Ring Availability/Integrity Network) is a radically different type of data network technology that uses a combination of a braided ring topology and high-integrity message propagation mechanisms. The BRAIN was originally designed to tolerate two passive failures or one passive and one active failure (including a Byzantine failure). In recent developments, the BRAIN's fault tolerance has been increased to the level where it can tolerate two active failures (including two Byzantine failures), as long as the two failures are not colluding. A colluding failure is an active failure that supports one or more other active failures to cause a system failure. To be effective, these active failures must be syntactically correct - i.e., cannot be detected by inline error detection, such as CRCs, checksums, physical encoding (e.g. 8B/10B), protocol rules, or reasonableness checks. The probability of colluding failures happening is so low that this new BRAIN, for all practical purposes, is a two-fault tolerant network. This improvement in fault tolerance comes at no additional cost. That is, it uses exactly the same minimal amount of hardware as the original BRAIN. As an example comparison, this new version of the BRAIN requires less size, weight, and power (SWaP) than a typical two-channel AFDX network, while tolerating more faults and more types of faults. The nodes used by the BRAIN, are simplex (they require no redundancy in themselves for integrity) and the fault tolerance provided by the BRAIN can be made transparent to all application software. The BRAIN can check that redundant nodes (e.g. pair-wise adjacent nodes) produce bit-for-bit identical outputs, without resorting to clock-step self-checking pair processing that is rapidly becoming technologically infeasible due to the higher speeds of modern processors. The BRAIN also simplifies the creation of architectures with dissimilar redundancy. The design of these BRAIN improvements were guided by the- use of the Symbolic Analysis Laboratory (SAL) model-checker in a novel use of formal methods for exploratory development early in the design cycle of a new protocol.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6382406,no
Semi-supervised learning of decision making for parts faults to system-level failures diagnosis in avionics system,"Supervised fault detection and fault diagnosis are the techniques for recognizing small faults with abrupt or incipient time behavior in closed loops. Thus the acquired data scale and software scale became more and more huge that active fault diagnosis treats with the data hardly. After decades of Artificial Intelligence development, AI technology has achieved significant results. Machine learning methods in AI have been widely used and developed in the field of fault diagnosis and prognosis. This paper discusses and demonstrates a complete machine learning fault diagnosis structure based on support vector regression, neural gas clustering, multiple-classes support vector machine, and Bayesian fuzzy fault tree, which are semi-supervised to isolate and predict faults from a component to a system/subsystem when there are partly uncertainty faults and finally provide a decision for the maintenance. It is crucial that machine learning methods are applied in the fault detection and prediction. Furthermore, the diagnostic intelligence can be found in multi-dimension empirical data and from granularity partition of avionics system based on the knowledge found and representation. Therefore the symptom-knowledge-information is suitable for representing the faults or failures in a system. The presented structure is generic and can be extended to the verification and validation of other diagnosis and prognostic algorithms on different platforms. It has been successfully preventing aircraft system/subsystem failures, identifying and predicting failures that will occur, which provides real application on making health management information and decisions.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6382418,no
A QoS-Aware Service Optimization Method Based on History Records and Clustering,"The number of alternative Web services that provide the same functionality but differ in non-functional characteristics, i.e., Quality of Service (QoS) parameters is more and more larger, and the service providers could not always deliver their promised quality. In view of this challenge, we propose a QoS-aware service optimization method based on history records and clustering, named QHRC. In this method, we take advantage of the history records of services' past performance quality rather than using the tentative QoS values provided by the service providers. And an adaptive hierarchical fuzzy clustering algorithm named H2D-SC algorithm is adopted to cluster the QoS history records for each Web service, then using the centroids of the subclusters to generate QoS history-record based composition plans. Our method aims at ranking the service composition plans by the corresponding QoS history-record based composition plans to select a most qualified service composition plan. At last, we assess the efficiency of the proposed method with an example and experiments.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6382802,no
A QoS-Aware Performance Prediction for Self-Healing Web Service Composition,"As composition consists of different Web Services invocations, when one component service fails, composite Web Service will not operate appropriately. The easy solution to this problem is to reselect the service every time service fails. However, it is not feasible due to the high complexity of the reselection, which will interrupt the execution of composite service, lead to an extra delay and influence the performance of the composite service. In this paper we propose an approach on Quality of Service (QoS) aware performance prediction for self-healing Web Service Composition. In our approach, we first propose a self-healing cycle which has three phases such as monitoring, diagnostics and repair. Next, in order to minimize a number of reselections we propose Decision Tree based performance prediction approach. With our approach, the component services which have previously violated QoS parameter values can be predicted. We will demonstrate that proposed solution has better performance in supporting the self-healing Web Service composition comparing to traditional way.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6382909,no
Visualizing concurrency faults in ARINC-653 real-time applications,"The ARINC-653 standard architecture for flight software specifies an application executive (APEX) which provides an application programming interface and defines a hierarchical framework which provides health management for error detection and recovery. In every partition of the architecture, however, processes may have to deal with asynchronous realtime signals from peripheral devices or may communicate with other processes through blackboards or buffers. This configuration may lead programs into concurrency faults such as unintended race conditions which are common and difficult to be removed by testing. Unfortunately, existing tools for reporting concurrency faults in applications that use concurrent signal handlers can neither represent the complex interactions between an ARINC-653 application and its error handlers nor provide effective means for understanding the dynamic behavior of concurrent signal handlers involved into data races. Thus, this paper presents an intuitive tool that visualizes the partial ordering of runtime events to detect concurrency faults in an ARINC-653 application that uses concurrent signal handlers. It uses vertically parallel arrows with different colors to capture the logical concurrency between the application, its error handlers and concurrent signal handlers, and materializes synchronization operations with differently colored horizontal arrows. Our visualization tool allows at a glance, to visually detect data races and provides a great understanding of the program internal for an easy debugging process.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6382940,no
Visualizing concurrency faults in ARINC-653 real-time applications,"Presents a collection of slides covering the following topics: the ARINC-653 standard; defines an application executive (APEX) to provide services for integrated modular avionics; provides temporal- and spatial-partitioning to enable applications, each executing in a partition, to run simultaneously and independently on the same architecture; health monitor to detect and provide recovery mechanisms for hardware and software failures.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6383137,no
A model-driven approach for configuring and deploying Systems of Systems,"Configuration and deployment of systems for defense and air traffic control is often a complex task because a System of Systems (SoS) is always distributed on different geographic areas, composed by hundreds of components (e.g. applications, processes, services, hosts), running under multiple hardware constraints, on different resources, and subject to mission critical requirements. The configuration of such SoS or a part of it, involves the production of many configuration files describing the structure of the SoS in general, the configuration parameters of each component, and how each component has to interact with the others. Due to the considerable size and complexity of the configuration files (i.e. hundreds of lines of code), the adoption of a manual approach is clearly error prone. This work presents a model-driven approach for supporting the configuration of mission-critical SoS or part of it.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6384139,no
CDA: A Cloud Dependability Analysis Framework for Characterizing System Dependability in Cloud Computing Infrastructures,"Cloud computing has become increasingly popular by obviating the need for users to own and maintain complex computing infrastructure. However, due to their inherent complexity and large scale, production cloud computing systems are prone to various runtime problems caused by hardware and software failures. Dependability assurance is crucial for building sustainable cloud computing services. Although many techniques have been proposed to analyze and enhance reliability of distributed systems, there is little work on understanding the dependability of cloud computing environments. As virtualization has been an enabling technology for the cloud, it is imperative to investigate the impact of virtualization on the cloud dependability, which is the focus of this work. In this paper, we present a cloud dependability analysis (CDA) framework with mechanisms to characterize failure behavior in cloud computing infrastructures. We design the failure-metric DAGs (directed a cyclic graph) to analyze the correlation of various performance metrics with failure events in virtualized and non-virtualized systems. We study multiple types of failures. By comparing the generated DAGs in the two environments, we gain insight into the impact of virtualization on the cloud dependability. This paper is the first attempt to study this crucial issue. In addition, we exploit the identified metrics for failure detection. Experimental results from an on-campus cloud computing test bed show that our approach can achieve high detection accuracy while using a small number of performance metrics.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6385066,no
Entropy-Based Detection of Incipient Faults in Software Systems,"This paper develops and validates a methodology to detect small, incipient faults in software systems. Incipient faults such as memory leaks slowly deteriorate the software's performance over time and if left undetected, the end result is usually a complete system failure. The proposed method combines tools from information theory and statistics: entropy and principal component analysis (PCA). The entropy calculation summarizes the information content associated with the collected low-level metrics and reduces the computational burden incurred by the subsequent PCA step which detects underlying patterns and correlations present in the multivariate data, as well as distortions in the correlations indicative of an incipient fault. We use the technique to detect memory bloat within the Trade6 enterprise application under dynamic workload patterns, showing that small leaks can be detected quickly and with a low false alarm rate. Our method is also robust to the periodic/seasonal patterns affecting the metrics used to detect the fault.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6385072,no
Assuring software quality by code smell detection,"In this retrospective we will review the paper """"Java Quality Assurance by Detecting Code Smells"""" that was published ten years ago at WCRE. The work presents an approach for the automatic detection and visualization of code smells and discusses how this approach could be used in the design of a software inspection tool. The feasibility of the proposed approach was illustrated with the development of jCOSMO, a prototype code smell browser that detects and visualizes code smells in JAVA source code. It was the first tool to automatically detect code smells in source code, and we demonstrated the application of this tool in an industrial quality assessment case study. In addition to reviewing the WCRE 2002 work, we will discuss subsequent developments in this area by looking at a selection of papers that were published in its wake. In particular, we will have a look at recent related work in which we empirically investigated the relation between code smells and software maintainability in a longitudinal study where professional developers were observed while maintaining four different software systems that exhibited known code smells. We conclude with a discussion of the lessons learned and opportunities for further research.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6385092,no
Can Lexicon Bad Smells Improve Fault Prediction?,"In software development, early identification of fault-prone classes can save a considerable amount of resources. In the literature, source code structural metrics have been widely investigated as one of the factors that can be used to identify faulty classes. Structural metrics measure code complexity, one aspect of the source code quality. Complexity might affect program understanding and hence increase the likelihood of inserting errors in a class. Besides the structural metrics, we believe that the quality of the identifiers used in the code may also affect program understanding and thus increase the likelihood of error insertion. In this study, we measure the quality of identifiers using the number of Lexicon Bad Smells (LBS) they contain. We investigate whether using LBS in addition to structural metrics improves fault prediction. To conduct the investigation, we assess the prediction capability of a model while using i) only structural metrics, and ii) structural metrics and LBS. The results on three open source systems, ArgoUML, Rhino, and Eclipse, indicate that there is an improvement in the majority of the cases.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6385119,no
A Framework to Compare Alert Ranking Algorithms,"To improve software quality, rule checkers statically check if a software contains violations of good programming practices. On a real sized system, the alerts (rule violations detected by the tool) may be numbered by the thousands. Unfortunately, these tools generate a high proportion of """"false alerts"""", which in the context of a specific software, should not be fixed. Huge numbers of false alerts may render impossible the finding and correction of """"true alerts"""" and dissuade developers from using these tools. In order to overcome this problem, the literature provides different ranking methods that aim at computing the probability of an alert being a """"true one"""". In this paper, we propose a framework for comparing these ranking algorithms and identify the best approach to rank alerts. We have selected six algorithms described in literature. For comparison, we use a benchmark covering two programming languages (Java and Smalltalk) and three rule checkers (Find Bug, PMD, Small Lint). Results show that the best ranking methods are based on the history of past alerts and their location. We could not identify any significant advantage in using statistical tools such as linear regression or Bayesian networks or ad-hoc methods.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6385123,no
The Secret Life of Patches: A Firefox Case Study,"The goal of the code review process is to assess the quality of source code modifications (submitted as patches) before they are committed to a project's version control repository. This process is particularly important in open source projects to ensure the quality of contributions submitted by the community, however, the review process can promote or discourage these contributions. In this paper, we study the patch lifecycle of the Mozilla Fire fox project. The model of a patch lifecycle was extracted from both the qualitative evidence of the individual processes (interviews and discussions with developers), and the quantitative assessment of the Mozilla process and practice. We contrast the lifecycle of a patch in pre- and post-rapid release development. A quantitative comparison showed that while the patch lifecycle remains mostly unchanged after switching to rapid release, the patches submitted by casual contributors are disproportionately more likely to be abandoned compared to core contributors. This suggests that patches from casual developers should receive extra care to both ensure quality and encourage future community contributions.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6385140,no
Software Aging Detection Based on NARX Model,"Software aging is a severe test on the reliability of the software. In this paper, we present a method of nonlinear autoregressive models with exogenous inputs to detect the aging phenomenon of the software system. This method considered the relationship between multivariable and the influence of the delay of historical data. The experimental analysis shows that, using the NARX model to detect fault can be effectively applied in the software aging test.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6385193,no
Quality Measurement for Cloud Based E-commerce Applications,"Cloud based e-Commerce applications are favored over traditional systems due to their capacity to reduce costs in various aspects like use of resources, low operating costs, eliminate capital costs, low maintenance and service costs. Its core functionality of optimizing performance and its automatic system recovery is crucial in web applications. Using a cloud platform for web applications increases productivity and decreases the replication of business documents saving businesses money in the current economic climate. A stable system is needed to achieve this and quality measurement is crucial to establish baselines to help predict resources for the future of the business. The proposed quality measurement model is one such designed for Cloud based e-Commerce applications. It aims to create a repository or an error-Knowledge Management System(e-KMS) for known online defects with capacity to add in future defects as they occur when using the applications. By mapping these defects directly to quality factors affected, accurate quality measurement can be achieved.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6385273,no
Functional safety aspects of pattern detection algorithms,"Pattern detection algorithms may be used as part of safety-relevant processes employed by industrial systems. Current approaches to functional safety mainly focus on random faults in hardware and the avoidance of systematic faults in both software and hardware. In this paper we build on the concepts of the international standard for functional safety IEC 61508 to extend safety-relevant notions to numerical and logical processes (algorithms) employed in pattern detection systems. In particular, we target the uncertainty pertaining to face detection systems where incorrect detection affects the overall system performance. We discuss a dual channel system that comprises two of the most commonly used and widely available face detection algorithms, Viola-Jones and Kienzle et al. We present a method for deriving the probability of failure in demand (PFD) from the combination of these two channels using both: 1oo2 and 2oo2 voting schemes. Finally, we compare experimental results from both the perspectives of availability and safety, and present conclusions with respect to the appropriate choice of information combination schemes and system architectures.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6386319,no
Dynamic redeployment of control software in distributed industrial automation systems during runtime,"Current research on the reconfiguration of automated industrial manufacturing systems focuses mainly on the reconfiguration of the production process. For uninterrupted operation also the distributed control system has to allow for reconfiguration during runtime. In this paper a method for dynamic redeployment of control software during runtime is presented, comprising a structural and behavioral concept. To enable this dynamic redeployment, control hardware faults are detected and the control software deployment is adjusted to the smaller number of controllers. Likewise, additional hardware resources are integrated to the system. An implementation and evaluation within a demonstration scenario shows the feasibility of the concept.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6386445,no
Variable neighborhood search-based subproblem solution procedures for a parallel shifting bottleneck heuristic for complex job shops,"The shifting bottleneck heuristic (SBH) for complex job shops decomposes the overall scheduling problem into a series of scheduling problems related to machine groups. These smaller, more tractable, scheduling problems are called subproblems. The heuristic is based on a disjunctive graph that is used to model the relationship between the sub-problems. In this paper, we use a parallel implementation of the SBH to assess the impact of a variable neighborhood search-based subproblem solution procedure (SSP) on global performance measures like the total weighted tardiness. Based on designed simulation experiments, we demonstrate the advantage of using high-quality SSPs.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6386462,no
Controlling Hardware Synthesis with Aspects,"The synthesis and mapping of applications to configurable embedded systems is a notoriously hard process. Tools have a wide range of parameters, which interact in very unpredictable ways, thus creating a large and complex design space. When exploring this space, designers must understand the interfaces to the various tools and apply, often manually, a sequence of tool-specific transformations making this an extremely cumbersome and error-prone process. This paper describes the use of aspect-oriented techniques for capturing synthesis strategies for tuning the performance of applications' kernels. We illustrate the use of this approach when designing application-specific architectures generated by a high-level synthesis tool. The results highlight the impact of the various strategies when targeting custom hardware and expose the difficulties in devising these strategies.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6386895,no
Analytical Design Space Exploration Based on Statistically Refined Runtime and Logic Estimation for Software Defined Radios,"The exploration of the design space for complex hardware-software systems requires accurate models for the system components, which are often not available in early design phases, resulting in error-prone resource estimations. For a HWSW system with a finite set of design points, we present an analytical approach to evaluate the quality of a distinctive design point choice. Our approach enables the designer to gain a measure for statistical confidence whether an application with realtime requirements can be successfully implemented on a chosen set of processors and reconfigurable logic. By a statistical evaluation of runtime, latency, logic resources and memory requirements, a probability metric for each realization alternative in the system is derived, that gives a realization probability for different mappings and different combinations of chips. We apply our principles to an FPGA/DSP digital radio receiver system and evaluate the realization probabilities for a different combination of chip sizes and mappings. Finally, we compare our approach against conventional estimation techniques, such as worst-case evaluation.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6386925,no
PROCOMON: An Automatically Generated Predictive Control Signal Monitor,"Today security and safety applications are often a large conglomerate of complex different components. Because of a strong trend to high system integration to fulfill financial and production cost constraints, as much of these components as possible are combined to form large-scale system-on-chips. Risks of dependability and security problems caused by device degradation and adversaries lead to a wide range of research concerning fault detection and recovery techniques in recent years. Especially in safety systems the concurrent use of different checking techniques, to protect the integrity of the operation, is preferred. Standard duplication or triplication methods for such critical devices are not completely fulfilling this property and raising a need for new on-line testing and recovery methodologies. Furthermore, the smart-card sector produces a strong need for new checking techniques with a low resource footprint. Therefore, this paper presents a novel automatized hardware generation flow to create a predictive control signal monitor unit in an automatized way. Depending on the instruction loaded by the processor pipeline this unit will predict the signature of following control signal changes. Hence, a new way of fault detection, weak checking, is implemented without introducing any large additional hardware blocks. A case study using an open-source processor is also presented to show the applicability of our approach.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6386954,no
Concurrent error detection scheme for HaF hardware,"HaF (Hash Function) is a dedicated cryptographic hash function considered for verification of the integrity of data. It is suitable for both software and hardware implementation. HaF has an iterative structure. This implies that even a single transient error at any stage of the computation of hash value results in a large number of errors in the final hash value. Hence, detection of errors becomes a key design issue. In the hardware design of cryptographic algorithms, concurrent error detection (CED) techniques have been proposed not only to protect the encryption and decryption process from random faults but also from the intentionally injected faults by some attackers. In this paper, we show the propagation of errors in the VHDL model of HaF-256 and then we propose and analyse some error detection schemes. In proposed CED scheme all the components are protected and all single and multiple, transient and permanent bit flip faults will be detected.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6387897,no
Detecting partially fallen-out magnetic slot wedges in AC machines based on electrical quantities only,"The winding system of high voltage machines is usually composed of pre-formed coils. To facilitate the winding fitting process stator slots are usually wide opened. These wide opened slots are known to cause disturbances of the magnetic field distribution. Thus losses are increased and machine's efficiency is reduced. A common way to counteract this drawback is given by placing magnetic slot wedges in the slots. During operation the wedges are exposed to high magnetic and mechanical forces. As a consequence wedges can get loose and finally fall out into the air-gap. State-of-the-art missing slot wedge detection techniques deal with the drawback that the machine must be disassembled, what is usually very time consuming. In this paper a method is investigated which provides the possibility of detecting missing magnetic slot wedges based only on measurement of electrical quantities and without machine disassembling. The method is based on exploitation of machine reaction on transient voltage excitation. The resulting current response contains information on machine's magnetic state. This information is composed of several machine asymmetries including the fault (missing wedge) induced asymmetry. A specific signal processing chain provides a distinct separation of all asymmetry components and delivers a high sensitive fault indicator. Measurements for several fault cases are presented and discussed. A sensitivity analysis shows the high accuracy of the method and the ability to detect even partially missing slot wedges.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6389271,no
On the expressiveness of business process modeling notations for software requirements elicitation,"Business process models have proved to be useful for requirements elicitation. Since software development depends on the quality of the requirements specifications, generating high-quality business process models is therefore critical. A key factor for achieving this is the expressiveness in terms of completeness and clarity of the modeling notation for the domain being modeled. The Bunge-Wand-Weber (BWW) representation model is frequently used for assessing the expressiveness of business process modeling notations. This article presents some propositions to adapt the BWW representation model to allow its application to the software requirements elicitation domain. These propositions are based on the analysis of the Guide to the Software Engineering Body of Knowledge (SWEBOK) and the Guide to the Business Analysis Body of Knowledge (BABOK). The propositions are validated next by experts in business process modeling and software requirements elicitation. The results show that the BWW representation model requires to be specialized by including concepts specific to software requirements elicitation.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6389398,no
From requirements to software trustworthiness using scenarios and finite state machine,"The notion of software trustworthiness evaluation in the literature is inherently subjective. It depends on how the software is used and in what context it is used. Moreover different users evaluate a software system according to different criteria, point of view and background. Therefore to assess the software trustworthiness, it is not wise to look for a general set of characteristics and parameters; instead, there is need to define a model that is tailored to the functional and quality requirements that the software has to fulfill. This paper shows a way to model software trustworthiness by using Finite State Machine (FSM) notation and scenarios. The approach introduces a novel behavioristic model for verifying software trustworthiness based on scenarios of interactions between the software and its users and environment. These interactions consist of simple scenarios of examples or counterexamples of desired behavior. The approach supports incremental changes in requirements/scenarios. An experiment of application of the model for verifying software trustworthiness based on the scenarios of interactions between the software and its users and environment is presented in a separate case study [40].",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6389399,no
A Survey of Key Factors Affecting Software Maintainability,"Today software quality is a major point of concern and it is important to be able to assess the maintainability of a software system. Maintainability is the ability of the system to undergo modifications with a degree of ease. These changes could impact interfaces, components, and features when adding or modifying the functionality and meeting the new customer's requirements to cope with the changing environment. This paper describes the several types of vital factors that affect the maintainability of software systems as proposed by different researchers in different software maintainability models. These factors play a crucial role in the maintainability assessments. The maintainability models can be used to improve the quality of the software product so that maintenance can be performed efficiently and effectively.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6391686,no
Reliability Analysis of Task Model in Real-Time Fault-Tolerant Systems,"One notable advantage of Model-Driven Architecture(MDA) method is that software developers could do sufficient analysis and tests on software models in the design phase, which helps construct high confidence on the expected software performance and behaviors. In this paper, we present a general reliability model, based on the relationship between real-time requirements and time costs of fault tolerance, to analyze reliability of the task execution model in real-time software design phase when using MDA method. This reliability model defines arrival rates of faults and fault-tolerant mechanisms to model non-permanent faults and the corresponding fault handling costs. By analyzing the probability of tasks being schedulable in the worst-case execution scenario, reliability and schedulability are combined into an unified analysis framework, and an algorithm for reliability analysis is also given under static priority scheduling. When no assumptions of fault occurrences made on the task model, this reliability model regresses to a generic schedulability model.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6391929,no
Building Useful Program Analysis Tools Using an Extensible Java Compiler,"Large software companies need customized tools to manage their source code. These tools are often built in an ad-hoc fashion, using brittle technologies such as regular expressions and home-grown parsers. Changes in the language cause the tools to break. More importantly, these ad-hoc tools often do not support uncommon-but-valid code code patterns. We report our experiences building source-code analysis tools at Google on top of a third-party, open-source, extensible compiler. We describe three tools in use on our Java code base. The first, Strict Java Dependencies, enforces our dependency policy in order to reduce JAR file sizes and testing load. The second, error-prone, adds new error checks to the compilation process and automates repair of those errors at a whole-code base scale. The third, Thindex, reduces the indexing burden for a Java IDE so that it can support Google-sized projects.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6392098,no
Impact Analysis in the Presence of Dependence Clusters Using Static Execute after in WebKit,"Impact analysis based on code dependence can be an integral part of software quality assurance by providing opportunities to identify those parts of the software system that are affected by a change. Because changes usually have far reaching effects in programs, effective and efficient impact analysis is vital, which has different applications including change propagation and regression testing. Static Execute After (SEA) is a relation on program elements (procedures) that is efficiently computable and accurate enough to be a candidate for use in impact analysis in practice. To assess the applicability of SEA in terms of capturing real defects, we present results on integrating it into the build system of Web Kit, a large, open source software system, and on related experiments. We show that a large number of real defects can be captured by impact sets computed by SEA, albeit many of them are large. We demonstrate that this is not an issue in applying it to regression test prioritization, but generally it can be an obstacle in the path to efficient use of impact analysis. We believe that the main reason for large impact sets is the formation of dependence clusters in code. As apparently dependence clusters cannot be easily avoided in the majority of cases, we focus on determining the effects these clusters have on impact analysis.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6392099,no
When Does a Refactoring Induce Bugs? An Empirical Study,"Refactorings are - as defined by Fowler - behavior preserving source code transformations. Their main purpose is to improve maintainability or comprehensibility, or also reduce the code footprint if needed. In principle, refactorings are defined as simple operations so that are """"unlikely to go wrong"""" and introduce faults. In practice, refactoring activities could have their risks, as other changes. This paper reports an empirical study carried out on three Java software systems, namely Apache Ant, Xerces, and Ar-go UML, aimed at investigating to what extent refactoring activities induce faults. Specifically, we automatically detect (and then manually validate) 15,008 refactoring operations (of 52 different kinds) using an existing tool (Ref-Finder). Then, we use the SZZ algorithm to determine whether it is likely that refactorings induced a fault. Results indicate that, while some kinds of refactorings are unlikely to be harmful, others, such as refactorings involving hierarchies (e.g., pull up method), tend to induce faults very frequently. This suggests more accurate code inspection or testing activities when such specific refactorings are performed.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6392107,no
Using Coding-Based Ensemble Learning to Improve Software Defect Prediction,"Using classification methods to predict software defect proneness with static code attributes has attracted a great deal of attention. The class-imbalance characteristic of software defect data makes the prediction much difficult; thus, a number of methods have been employed to address this problem. However, these conventional methods, such as sampling, cost-sensitive learning, Bagging, and Boosting, could suffer from the loss of important information, unexpected mistakes, and overfitting because they alter the original data distribution. This paper presents a novel method that first converts the imbalanced binary-class data into balanced multiclass data and then builds a defect predictor on the multiclass data with a specific coding scheme. A thorough experiment with four different types of classification algorithms, three data coding schemes, and six conventional imbalance data-handling methods was conducted over the 14 NASA datasets. The experimental results show that the proposed method with a one-against-one coding scheme is averagely superior to the conventional methods.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6392473,yes
Test effectiveness index: Integrating product metrics with process metrics,"Defect measurement is an important method in the improvement of software quality. Recent approaches of defect measurement are inappropriate to small software organizations by reason of their intricacy. This paper gives a simple approach of defect measurement, which integrates the power of product metrics with process metrics, i.e., it can not only detect the defect-prone modules, but also find the problems in the software process. This approach uses the results of successive two rounds of testing to create the test effectiveness index constructively. A case study is conducted and the results indicate that the defect-prone modules can be identified and problems of testing process can be discovered by test effectiveness index.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6392526,no
De novo co-assembly of bacterial genomes from multiple single cells,"Recent progress in DNA amplification techniques, particularly multiple displacement amplification (MDA), has made it possible to sequence and assemble bacterial genomes from a single cell. However, the quality of single cell genome assembly has not yet reached the quality of normal multiceli genome assembly due to the coverage bias and errors caused by MDA. Using a template of more than one cell for MDA or combining separate MDA products has been shown to improve the result of genome assembly from few single cells, but providing identical single cells, as a necessary step for these approaches, is a challenge. As a solution to this problem, we give an algorithm for de novo co-assembly of bacterial genomes from multiple single cells. Our novel method not only detects the outlier cells in a pool, it also identifies and eliminates their genomic sequences from the final assembly. Our proposed co-assembly algorithm is based on colored de Bruijn graph which has been recently proposed for de novo structural variation detection. Our results show that de novo co-assembly of bacterial genomes from multiple single cells outperforms single cell assembly of each individual one in all standard metrics. Moreover, co-assembly outperforms mixed assembly in which the input datasets are simply concatenated. We implemented our algorithm in a software tool called HyDA which is available from http://compbio.cs.wayne.edu/software/hyda.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6392618,no
A method of zero self-modification and temperature compensation for indoor air quality etection based on a software model,"It is very difficult to apply non-dispersive infrared sensor to detect the indoor air quality and maintain very low zero and temperature drift over long periods. Frequently manual zero setting and calibration are required. To solve the issues of zero and temperature drift of non-dispersive infrared sensor, a software model based on zero gas intensity, reference channels intensity, standard temperature, environmental temperature, temperature drift coefficient, etc. has been established to automatically modify and compensate the zero and temperature drift existing in the long-term continuous operation of the infrared sensor. The test result and long-term application indicate the detection precision of the instrument is less than 5%F.S in various changing environmental conditions. The average detection precision of carbon dioxide has been improved from 9.26% before comprehensive processing to 1.23% after processing, while the average detection precision of methane has been improved from 10.61% before comprehensive processing to 0.70% after processing. As a result, the disadvantages existing in many gas detectors including poor stability and short calibration cycle have been overcome, thus effectively improving the detection precision and stability of the instrument and reducing the maintenance cost.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6393324,no
Initial results of web based blended learning in the field of air cargo security,"With the currently implemented high standards in passenger screening, air cargo is being perceived as the security chain's weakest link in civil aviation and therefore becomes an attractive target for terrorists. Detailed regulations exist to harden air cargo against terrorist attacks. Blended learning training methods can be used to enable screeners to detect suspicious consignments even in situations when technical measures (e.g. x-ray) do not indicate any threat In this study, blended learning was conducted at a handling agents premises at a Swiss airport in three courses (seven trainees in total) and evaluated subsequently. Results show a very high satisfaction with the training and very high scores in the final exam. However, trainees repeatedly skipped text inside the web based training (WBT) leading to the conclusion that the WBT has to be optimized in terms of presentation modes. Suggestions on how to create even more engaging WBT content can be found in various methods of classification of computer based training (CBT) and are discussed in this paper.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6393572,no
Method of Information Extracting from Section Map Based on ActiveX Technology,"In the production of hydrographic surveying, a lot of inside work are still manually completed, time-consuming and error-prone, such as dimensioning of Basal Area and table. This paper discusses extracting river cross-sectional information from CAD with ActiveX software integration technology, some information is also processed and excavated, and Basal Area and earthwork table are automatically completed. Therefore these works achieve greater efficiency and quality of work. Undoubtedly, it has great applied value.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6394282,no
The Forensics for Detecting Manipulation on Part of Text,"The adventure of sophisticated photo editing software has made it increasingly easier to manipulate text in digital images. Visual inspection cannot definitively distinguish the forgeries from authentic photographs. In the imaging process, as defocusing, atmospheric turbulence, diffraction and the defects of image device, digital image can't reproduce the details of texture perfectly, while the text image that obtained by photo-editing software will do better, and will show no difference with the authentic image in the appearance. In this paper, we describe a new forensics technique that focuses on manipulation on part of text. By decomposition of the text and to extract the characteristics of edge points in text image, we use support vector machine (SVM) to training classification model which is used for the identification of the authenticity of text message in images. Our test result indicates that this method can detect the manipulation on part of text, and the general detection rate is promised.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6394325,no
Research on Dynamic Message Routing for ESB Based on Message System,"This paper applied the message routing components in the Enterprise Integration Patterns to the routing model of enterprise service bus, and made up a Reliable fault-tolerant dynamic message routing model. Give the core modules, algorithms and principles of the message router, and compare with existing reactive routing model and predict the dynamic routing model. On this basis, analysis the problems to be solved for dynamic routing in enterprise service bus as well as future research directions.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6394409,no
Fault Testing Device of Fire Control System Based on Virtual Instrumentation Technology,"The fire control system of the mine dispenser vehicle is the core of rocket launching and mine configuration. However it is prone to various faults and failures due to the complex battlefield environments. In this paper the fault testing device for fire control system with diversified functions, easier portability and better performance is designed and developed. The main work is as follows. The hardware platform of fault testing device consists of upper computer and lower computer. And the upper computer undertakes the display of diagnosis process, fault causes and troubleshooting results. It is mainly composed of industrial processing computer (IPC), touch screen, LCD and other peripheral interface. The lower computer is responsible for the acquisition, conditioning and connection of the electrical control signals. It contains two CPLD chips, signal conditioning circuit, serial interface as well as input interfaces for large amount of electrical control signals of the fire control system. In addition, the fault testing software application implements the display of diagnosis process, fault causes and troubleshooting results. It is developed by virtual instrumentation software platform LabVIEW. The fault testing device actually offers effective technical method in improving the repair and protection skills of mine dispenser vehicle.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6394845,no
Pattern-Based Modifiability Analysis of EJB Architectures,"Over the last years, several techniques to evaluatemodifiability of software architectureshave been developed. One of such techniques is change impact analysis (CIA), which aids developers in assessing the effects of change scenarios on architectural modules. However, CIA does not take into account the pattern structures behind those modules. In architectural frameworks, such as the Enterprise Java Beans (EJB) architecture, the use of patterns is a key practice to achieve modifiability goals. Although patterns can be easily understood individually, when an application combines several pattern instances the analysis is not straightforward. In practice, many EJB designsare assessed in an ad-hoc manner and relying on the developers' experience. A way of dealing with this problem is through the integration of modifiabiliy analysis models and patterns. We propose a knowledge-based approach that explicitly links the EJB patterns to a scenario-based analysis for multi-tier architectures. Specifically, we have developed a modifiability reasoning framework that reifies the EJB patterns present in a given design solution and, for a set of predetermined scenarios, the reasoning framework identifies which architectural elements can be affected by the scenarios. The reasoning framework outputs metrics for each of the scenarios regarding specific EJB tiers. The main contribution of this approach is that assists developers to evaluate EJB alternatives, providing quantitativeinformation about the modifiability implications of their decisions. A preliminary evaluation has shown that the the reasoning framework is viable to analyze EJB designs.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6394973,no
A Semantic Web based approach for design pattern detection from source code,"Design patterns provide experience reusability and increase quality of object oriented designs. Knowing which design patterns are implemented in a software is important in comprehending, maintaining and refactoring its design. However, despite the interest in using design patterns, traditionally, their usage is not explicitly documented. Therefore, a method is required to reveal this information from some artifacts of the systems (e.g. source codes, models, and executables). In this paper, an approach is proposed which uses the Semantic Web technologies for automatically detecting design patterns from Java source code. It is based on the semantic data model as the internal representation, and on SPARQL query execution as the analysis mechanism. Experimental evaluations demonstrate that this approach is both feasible and effective, and it reduces the complexity of detecting design patterns to creating a set of SPARQL queries.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6395394,no
Linear parameter-varying model identification with structure selection for autonomic web service systems,"Information technology (IT) systems must provide their users with prescribed quality of service (Qos) levels, usually defined in terms of application performance. Qos requirements are in general difficult to satisfy, since the system workload may vary by orders of magnitude within the same business day. To meet Qos requirements, resources have to dynamically be allocated among running applications, re-configuring them at run-time. To deal with resource allocation so as to manage system overload issues admission control and server virtualization are typically used: the former is a protection mechanism that rejects requests under peak workload, whereas the latter allows partitioning physical resources such as CPU and disks into multiple virtual ones. For designing effective controllers to ensure the desired Qos levels, a reliable model of the server dynamics is needed. The given systems, while retaining the time-varying nature that allows one to model workload variations. To address this issue, a constrained black-box subspace identification approach endowed with a novel structure selection is designed, and the performance of the identified models are assessed on experimental data.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6397114,no
Software complexity: A fuzzy logic approach,Software complexity is one of the important quality attribute and predicting complexity is a difficult task for software engineers. Current measures can be used to compute complexity but these methods are not sufficient. New methods or paradigms are being searched for predicting complexity because complexity prediction can help us in estimating many other quality attributes like testability and maintainability. The main goal of this paper is to explore the role of new paradigms like fuzzy logic in complexity prediction. In this paper we have proposed a fuzzy logic based approach to predict software complexity.,2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6398233,no
An automatic transient detection system which can be incorporated into an algorithm to accurately determine the fault level in networks with DG,"The use of distributed generation is on the increase within the United Kingdom and the Distribution Network Operators (DNO's) require a novel approach of assessing potential fault levels in near real-time to assist with network planning and design. The short circuit current is the current expected to flow into a short circuit fault at a known point on the system, and therefore, the fault level is the product of the open circuit voltage and short circuit current. Recent techniques used by the industry involve power system software that calculates the fault level in accordance with BS EN 60909, however, this frequently provides a conservative answer and possibly this will be a factor restricting future connections of distributed generation. This paper will describe the initial stages of the development of an algorithm which can be used alongside a digital signal controller (a Texas Instruments TMS320F28335) to calculate in near real-time the fault level at a specified point on the distribution network. Matlab & Simulink are utilised to both simulate source faults and to create the initial elements of the algorithm which are analysed utilsing the test program. The implementation of a Short Time Fourier Transform (STFT) to determine when a fault occurs is discussed. Finally the results from these simulations are examined and presented alongside a discussion of future work.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6398419,no
Evaluating the reliability & availability of more-electric aircraft power systems,"With future aircraft designs increasingly embracing the more-electric concept, there is likely to be a greater reliance on electrical systems for safe flight. More-electric aircraft (MEA) will have a greater number of electrical loads which are critical to the aircraft flight. It therefore is essential that the design of aircraft power systems embraces new technologies and methods in order to achieve targets for aircraft certification. The various design drivers (e.g. weight, space) for aircraft will also have to be considered when incorporating reliability into future MEA. This paper will investigate options for future platforms to meet reliability and availability targets whilst continuing to improve overall efficiency. The paper proposes a software tool that has the ability to determine the reliability of a number of potential alternative design architectures. This paper outlines present designs of such systems and how reliability is enhanced through the use of redundancy and back-up generation. The regulatory challenges associated with aircraft are summarised, including a discussion on reliability targets for various loads. Techniques for assessing the reliability of aircraft systems are described and simple examples of their application to aircraft electrical systems are provided. These examples will highlight the advantages and drawbacks attributed to each method and the reasoning behind the selection of these techniques on which an analysis tool can be based.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6398542,no
SNR estimation techniques for low SNR signals,"Radio receivers contain a set of adaptive algorithms that estimate the received signal's unknown parameters required by the receiver to demodulate the signal. Often missing from the standard parameter list is Signal to Noise ratio (SNR) or equivalently Eb/No. The SNR estimate is the ubiquitous scale factor associated with all maximum likelihood estimators. The SNR qualifies the signal quality letting the estimator algorithms know whether the observables are reliable, hence should make significant contribution to the estimate or are unreliable and should make limited contribution to the estimate. Error correcting algorithms also use SNR to set soft decision probabilities and likelihood ratios. Many SNR estimates are accurate at high SNR when we really don't need the estimates and are inaccurate at low SNR when we have most need for them. This paper discusses two SNR estimator techniques which maintain estimation accuracy down to very low SNR values.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6398797,no
Virtual synchrophasor monitoring network,"Synchrophasors are considered one of the most important measuring quantities in the future of power systems. The main reason of synchrophasor monitoring is to prevent or detect in advance events that may cause failure or even damages of the transmission network. That is why Phasor Measurement Units (PMU) getting increased attention nowadays. Main function of PMU is to measure and report the synchrophasors in widely dispersed locations in the power system network. Main PMU features in terms of evaluation and reporting describes the IEEE C37.118 document. Data stream from PMUs receives Phasor Data Concentrator (PDC), synchronize data streams and provides output data stream with defined reporting rate. The paper describes developed software suite which simulates set of PMUs and Phasor Data Concentrator (PDC). The aim of this work was to create educative implementation of PMU and PDC in graphical programming language LabVIEW. This software enables creation of virtual synchrophasor monitoring network without using of any other hardware than PC and can be used as a teaching tool.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6401463,no
Transient stability assessment of synchronous generator in power system with high-penetration photovoltaics,"As photovoltaic (PV) capacity in power system increases, the capacity of synchronous generator needs to be reduced relatively. This leads to the lower system inertia and the higher generator reactance, and hence the generator transient stability may negatively be affected. In particular, the impact on the transient stability may become more serious when the considerable amounts of PV systems are disconnected simultaneously during voltage sag. In this work, the generator transient stability in the power system with significant PV penetration is assessed by a numerical simulation. In order to assess the impact from various angles, simulation parameters such as levels of PV penetration, variety of power sources (inverter or rotational machine), and existence of LVRT capability are considered. The simulation is performed by using PSCAD/EMTDC software.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6401908,no
Taking control: Modular and adaptive robotics process control systems,"Robotics systems usually comprise sophisticated sensor and actuator systems with no less complex control applications. These systems are subject to frequent modifications and extensions and have to adapt to their environment. While automation systems are tailored to particular production processes, autonomous vehicles must adaptively switch their sensors and controllers depending on environmental conditions. However, when designing and implementing the process control system, traditional control theory focuses on the control problem at hand without having this variability in mind. Thus, the resulting models and implementation artefacts are monolithic, additionally complicating the real-time system design. In this paper, we present a modularisation approach for the design of robotics process control systems, which not only aims for variability at design-time but also for adaptivity at run-time. Our approach is based on a layered control architecture, which includes an explicit interface between the two domains involved: control engineering and computer science. Our architecture provides separation of concerns in terms of independent building blocks and data flows. For example, the replacement of a sensor no longer involves the tedious modification of downstream filters and controllers. Likewise, the error-prone mapping of high-level application behaviour to the process control system can be omitted. We validated our approach by the example of an autonomous vehicle use case. Our experimental results demonstrate ease of use and the capability to maintain quality of control on par with the original monolithic design.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6402632,no
SOA-based platform implementing a structural modelling for large-scale system fault detection: Application to a board machine,"This paper presents a tool designed for analysing fault propagation and fault impact on large-scale process performances. The analysis is based on structural description of the process. The main physical variables are associated to each subsystem and a relational model linking these variables for all the different functioning modes of the system is determined. In large-scale systems every component must provide a certain function in order to make the overall system working satisfactorily. When a fault or a badly tuned parameter affects a control loop, the required function cannot be fulfilled which may cause a failure. Therefore, some Loop Performance Indexes (LPI) indicating if the control loops operate properly are necessary to evaluate the impact of the failure on the overall process performances represented by a high level index Key Performance Index (KPI). Structural models provide an interesting approach for the analysis of a system and also studying the impact of a fault because they only need a limited knowledge about the behaviour of the system. Generic component models can be used to describe the system architecture. At the first level different statistical tests are applied to the KPI. When a set of LPI or KPI deviate from their nominal or desired values, the elements which are source of an eventual malfunctioning can be searched in the structural graph by searching the nodes predecessors. The selected LPI are tested in their turn by mean of statistical tests. A node is declared to be faulty if the value of the corresponding LPI is out of the acceptable (pre defined) limits. The procedure is iterated until the last level of the model is reached. This procedure researches the possible cause of KPI value significant deviation. The procedure was applied on a board machine. In this process, the main KPI is the value of moisture of the board at the end of the production chain. The corresponding structural model which relates the moisture (to- node) to the control loops (nodes) has been developed. In order to validate the large-scale capabilities of such approach, the model has been integrated within the PREDICT's SOA(Services Oriented Archiecture) software platform: KASEM (Kowledge and Advanced Services for E-Monitoring). The platform enables to apply the on-line statistical test to the KPI and LPIs of the board machine and supports the iterative procedure Indeed, the iterative procedure based on the structural graph was integrated as one of the KASEM diagnostic tools with a dynamic and animated graph and used during the KASEM workflow to solve the problem.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6402705,no
Configurable RTL model for level-1 caches,"Level-1 (L1) cache memories are complex circuits that tightly integrate memory, logic, and state machines near the processor datapath. During the design of a processor-based system, many different cache configurations that vary in, for example, size, associativity, and replacement policies, need to be evaluated in order to maximize performance or power efficiency. Since the implementation of each cache memory is a time-consuming and error-prone process, a configurable and synthesizable model is very useful as it helps to generate a range of caches in a quick and reproducible manner. Comprising both a data and instruction cache, the RTL cache model that we present in this paper has a wide array of configurable parameters. Apart from different cache size parameters, the model also supports different replacement policies, associativities, and data write policies. The model is written in VHDL and fits different processors in ASICs and FPGAs. To show the usefulness of the model, we provide an example of cache configuration exploration.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6403112,no
Integrated didactic software package for computer based analysis of power quality,"The actual state and operation of the Romanian power system ask for a certain expertise in detecting the causes of electromagnetic perturbations and their evaluation in power grids. Consequently, all the aspects regarding the power quality issues became a common characteristic of the power systems' curricula within Romanian power engineering faculties. The students attending these classes are involved in computer-based laboratory works. This paper describes the authors' contribution regarding the development of an integrated software package for power quality analysis at the Faculty of Electrical Engineering, University of Craiova. This software structure is built on a link between professional specialized software packages and software subroutines conceived by the authors. The parameters related to voltage/current harmonics can be analyzed using MATLAB subroutines and EDSA package - Electrical Power System Design Software. The results can be visualized as different types of reports. They can be further exported to EDSA program or/and MATLAB subroutine in order to size the harmonic filters and evaluate their effect on power quality in the analyzed power grids. EDSA programs package, as well as the subroutines developed in MATLAB environment are traditional tools used by the students attending the Power Quality classes within the Faculty of Electrical Engineering.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6403400,no
An FPGA-based probability-aware fault simulator,"A recent approach to deal with the challenges that come along with the shrinking feature size of CMOS circuits is probabilistic computing. Those challenges, such as noise or process variations, result in a certain probabilistic behavior of the circuit and its gates. Probabilistic Computing, also referred to as pCMOS, does not try to avoid the occurrence of errors, but tries to determine the probability of errors at the output of the circuit, and to limit it to a value that the specific application can tolerate. Past research has shown that probabilistic computing has potential to drastically reduce the power consumption of circuits by scaling the supply voltage of gates to a value where they become non-deterministic, while tolerating a certain amount of probabilistic behavior at the output. Therefore, one main task in the design of pCMOS circuits is to determine the error probabilities at the output of the circuit, given a combination of error probabilities at the gates. In earlier work, pCMOS circuits have been characterized by memory-consuming and complex analytical calculations or by time-consuming software-based simulations. Hardware-accelerated emulators exist in large numbers, but miss the support of injecting errors with specified probabilities into as many circuit elements the user specifies at the same time. In this paper, we propose an FPGA-based fault simulator that allows for fast error probability classification, injection of errors at gate- and RT-level, and that is furthermore independent on the target architecture. Moreover, we demonstrate the usefulness of such a simulator by characterizing the probabilistic behavior of two benchmark circuits and reveal their energy-saving capability.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6404190,no
From off-Line to continuous on-line maintenance,"Summary form only given. Software is the cornerstone of the modern society. Many human activities rely on software systems that shall operate seamlessly 24/7, and failures in such systems may cause severe problems and considerable economic loss. To efficiently address a growing variety of increasingly complex activities, software systems rely on sophisticated technologies. Most software systems are assembled from modules and subsystems that are often developed by third party organization, and sometime are not even available at the system build time. This is the case for example of many Web applications that link Web services built and changed independently by third party organizations while the Web applications are running. The progresses of software engineering in the last decades have increased the productivity, reduced the costs and improved the reliability of software products, but have not eliminated the occurrence of field failures. Detecting and removing all faults before deployment is practically too expensive even for systems that are simple and fully available at design time, and impossible when systems are large and complex, and are dynamically linked to modules that may be developed and distributed only after the deployment of the system. The classic stop-and-go maintenance approaches that locate and fix field faults offline before deploying new system versions are important, but not sufficient to guarantee a seamless 24/7 behavior, because the faulty systems remain in operation until the faults have been removed and new systems redeployed [1]. On the other hand, classic fault tolerant approaches that constrain developers' freedom and rely on expensive mechanisms to avoid or mask faults do not match the cost requirements of many modern systems, and do not extend beyond the set of safety critical systems [2]. Self-healing systems and autonomic computing tackle these new challenges by moving activities from design to runtime. In self-healing systems, the- borderline between design and runtime activities fades, and both design and maintenance activities must change to enable activities such as fault diagnoses and fixes to be performed fully automatically and at runtime. Maintenance activities rely on information that are usually available at design time are are not part of the system runtime infrastructure. For example, corrective maintenance requires some knowledge about the expected system behavior to locate and fix the faults, while adaptive and perfective maintenance requires some knowledge about libraries and components to identify new modules that better cope with the the changes in the requirements and in the environment. In classic maintenance approaches, this knowledge is mastered by the developers, who gather and use the required information offline to deal with the emerging maintenance problems. In self healing systems the knowledge required for maintenance activities shall be available at runtime. Self healing systems shall be designed with enough embedded knowledge to deal with unplanned events, and shall be able to exploit this information automatically and at runtime to recover from unexpected situations, like field failures. The challenges of designing powerful self-healing systems relies in the ability to minimize the amount of extra knowledge to be provided at design time, while feeding a powerful automatic recovery mechanism. An interesting approach relies on the observation that software systems are redundant by nature, and exploits the intrinsic redundancy of software system to fix faults, thus minimizing the extra effort required at design time to feed the self-healing mechanism [3]. The intrinsic redundancy of software stems from design and reusability practice: the reuse of libraries may results in different ways to achieve the same or similar results, the design for modularity may produce methods with equivalent behavior, backward compatibility may keep deprecated and new implementations in t",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405244,no
"Leveraging natural language analysis of software: Achievements, challenges, and opportunities","Summary form only given. Studies continue to report that more time is spent reading, locating, and comprehending code than actually writing code. The increasing size and complexity of software systems makes it significantly more challenging for humans to perform maintenance tasks on software without automated and semi-automated tools to support them, especially in the error-prone tasks. Thus, software engineers increasingly rely on software engineering tools to automate maintenance tasks as much as possible. The program analyses that drive today's software engineering tools have historically focused on analyzing the program's data and control flow, dependencies, and other structural information about the program to uncover and prove program properties. Yet, a software system is more than just the source code and its structure. To build effective software tools, the underlying automated analyses need to use all the information available to make the tools as intelligent and useful as possible. By adapting natural language processing (NLP) to source code analysis, and integrating information retrieval (IR), NLP, and traditional program analyses, we can expect significant improvement in automated and semi-automated software engineering tools for many different software engineering tasks. In this talk, I will overview research in text analysis of software and discuss our achievements to date, the challenges faced in text analysis, and the opportunities for text analysis of software in the future.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405245,no
Finding errors from reverse-engineered equality models using a constraint solver,"Java objects are required to honor an equality contract in order to participate in standard collection data structures such as List, Set, and Map. In practice, the implementation of equality can be error prone, resulting in subtle bugs. We present a checker called EQ that is designed to automatically detect such equality implementation bugs. The key to EQ is the automated extraction of a logical model of equality from Java code, which is then checked, using Alloy Analyzer, for contract conformance. We have evaluated EQ on four open-source, production code bases in terms of both scalability and usefulness. We discuss in detail the detected problems, their root causes, and the reasons for false alarms.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405256,no
Assessing the effect of requirements traceability for software maintenance,"Advocates of requirements traceability regularly cite advantages like easier program comprehension and support for software maintenance (i.e., software change). However, despite its growing popularity, there exists no published evaluation about the usefulness of requirements traceability. It is important, if not crucial, to investigate whether the use of requirements traceability can significantly support development tasks to eventually justify its costs. We thus conducted a controlled experiment with 52 subjects performing real maintenance tasks on two third-party development projects: half of the tasks with and the other half without traceability. Our findings show that subjects with traceability performed on average 21% faster on a task and created on average 60% more correct solutions - suggesting that traceability not only saves downstream cost but can profoundly improve software maintenance quality. Furthermore, we aimed for an initial cost-benefit estimation and set the measured time reductions by using traceability in relation to the initial costs for setting-up traceability in the evaluated systems.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405269,no
Modelling the Hurried bug report reading process to summarize bug reports,"Although bug reports are frequently consulted project assets, they are communication logs, by-products of bug resolution, and not artifacts created with the intent of being easy to follow. To facilitate bug report digestion, we propose a new, unsupervised, bug report summarization approach that estimates the attention a user would hypothetically give to different sentences in a bug report, when pressed with time. We pose three hypotheses on what makes a sentence relevant: discussing frequently discussed topics, being evaluated or assessed by other sentences, and keeping focused on the bug report's title and description. Our results suggest that our hypotheses are valid, since the summaries have as much as 12% improvement in standard summarization evaluation metrics compared to the previous approach. Our evaluation also asks developers to assess the quality and usefulness of the summaries created for bug reports they have worked on. Feedback from developers not only show the summaries are useful, but also point out important requirements for this, and any bug summarization approach, and indicates directions for future work.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405303,no
Domain specific warnings: Are they any better?,"Tools to detect coding standard violations in source code are commonly used to improve code quality. One of their original goals is to prevent bugs, yet, a high number of false positives is generated by the rules of these tools, i.e., most warnings do not indicate real bugs. There are empirical evidences supporting the intuition that the rules enforced by such tools do not prevent the introduction of bugs in software. This may occur because the rules are too generic and do not focus on domain specific problems of the software under analysis. We underwent an investigation of rules created for a specific domain based on expert opinion to understand if such rules are worthwhile enforcing in the context of defect prevention. In this paper, we performed a systematic study to investigate the relation between generic and domain specific warnings and observed defects. From our experiment on a real case, long term evolution, software, we have found that domain specific rules provide better defect prevention than generic ones.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405305,no
A structured approach to assess third-party library usage,"Modern software systems build on a significant number of external libraries to deliver feature-rich and high-quality software in a cost-efficient and timely manner. As a consequence, these systems contain a considerable amount of third-party code. External libraries thus have a significant impact on maintenance activities in the project. However, most approaches that assess the maintainability of software systems largely neglect this important factor. Hence, risks may remain unidentified, threatening the ability to effectively evolve the system in the future. We propose a structured approach to assess the third-party library usage in software projects and identify potential problems. Industrial experience strongly influences our approach, which we designed in a lightweight way to enable easy adoption in practice. We present an industrial case study showing the applicability of the approach to a real-world software system.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405311,no
Time-leverage point detection for time sensitive software maintenance,"Correct real-time behavior is an important aspect for time sensitive software, but it is difficult to get right. Time faults can be introduced not just during software development but also maintenance. So software maintainers without time information tend to have more chances to introduce unintended time behaviors. In this paper, we propose time change impact analysis to help maintainers estimate the potential influence of time changes on programs before the software evolves. Our main insight is that by being reminded and warned that a small-time change at some places in the source code will largely affect the whole task execution time, maintainers can be more cautious when updating such places. Because these places have a leverage effect that multiplies the task execution time in a subtle way, we call them time-leverage points. We give an approach to detect the time-leverage points based on a dynamic testing method, which instruments the program at a point for introducing a small delay and observes its impact on the task execution time. We implement a prototype tool and empirically evaluate the approach.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405322,no
Move code refactoring with dynamic analysis,"In order to reduce coupling and increase cohesion, we refactor program source code. Previous research efforts for suggesting candidates of such refactorings are based on static analysis, which obtains relations among classes or methods from source code. However, these approaches cannot obtain runtime information such as repetition count of loop, dynamic dispatch and actual execution path. Therefore, previous approaches might miss some refactoring opportunities. To tackle this problem, we propose a technique to find refactoring candidates by analyzing method traces. We have implemented a prototype tool based on the proposed technique and evaluated the technique on two software systems. As a result, we confirmed that the proposed technique could detect some refactoring candidates, which increase code quality.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405324,no
Applying technical stock market indicators to analyze and predict the evolvability of open source projects,"For decades stock market traders make financially critical buy/sell decisions depending on external and internal factors affecting the price of individual stocks. Moving Averages combined with technical analysis patterns are some of the most basic and widely used indicators to support buy/sell decisions. In this research, we present a novel cross-disciplinary approach that uses these technical stock market indicators for analyzing the community evolvability of open source software systems.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405335,no
Adapting Linux for mobile platforms: An empirical study of Android,"To deliver a high quality software system in a short release cycle time, many software organizations chose to reuse existing mature software systems. Google has adapted one of the most reused computer operating systems (i.e., Linux) into an operating system for mobile devices (i.e., Android). The Android mobile operating system has become one of the most popular adaptations of the Linux kernel with approximately 60 millions new mobile devices running Android each year. Despite many studies on Linux, none have investigated the challenges and benefits of reusing and adapting the Linux kernel to mobile platforms. In this paper, we conduct an empirical study to understand how Android adapts the Linux kernel. Using software repositories from Linux and Android, we assess the effort needed to reuse and adapt the Linux kernel into Android. Results show that (1) only 0.7% of files from the Linux kernel are modified when reused for a mobile platform; (2) only 5% of Android files are affected by the merging of changes on files from the Linux repository to the Android repository; and (3) 95% of bugs experienced by users of the Android kernel are fixed in the Linux kernel repository. These results can help development teams to better plan software adaptations.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405339,no
The demacrofier,"C++ programs can be rejuvenated by replacing error-prone usage of the C Preprocessor macros with type safe C++11 declarations. We have developed a classification of macros that directly maps to corresponding C++11 expressions, statements, and declarations. We have built a set of tools that replaces macros with equivalent C++ declarations and iteratively introduces the refactorings into the software build.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405347,no
Improving Coverage-Based Localization of Multiple Faults Using Algorithms from Integer Linear Programming,"Coverage-based fault localization extends the utility of testing from detecting the presence of faults to their localization. While coverage-based fault localization has shown good evaluation results for the single fault case, its ability to localize several faults at once appears to be limited. In this paper, we show how two partitioning procedures borrowed from integer linear programming can help improve the accuracy of standard coverage-based fault locators in presence of multiple faults by breaking down the localization problem into several smaller ones that can be dealt with independently. Experimental results suggest that our approach is indeed useful, the more so as its cost appears to be negligible.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405360,no
What Is System Hang and How to Handle It,"Almost every computer user has encountered an un-responsive system failure or system hang, which leaves the user no choice but to power off the computer. In this paper, the causes of such failures are analyzed in detail and one empirical hypothesis for detecting system hang is proposed. This hypothesis exploits a small set of system performance metrics provided by the OS itself, thereby avoiding modifying the OS kernel and introducing additional cost (e.g., hardware modules). Under this hypothesis, we propose SHFH, a self-healing framework to handle system hang, which can be deployed on OS dynamically. One unique feature of SHFH is that its """"light-heavy"""" detection strategy is designed to make intelligent tradeoffs between the performance overhead and the false positive rate induced by system hang detection. Another feature is that its diagnosis-based recovery strategy offers a better granularity to recover from system hang. Our experimental results show that SHFH can cover 95.34% of system hang scenarios, with a false positive rate of 0.58% and 0.6% performance overhead, validating the effectiveness of our empirical hypothesis.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405362,no
A Light-Weight Defect Classification Scheme for Embedded Automotive Software and Its Initial Evaluation,"Objective: Defect classification is an essential part of software development process models as a means of early identification of patterns in defect inflow profiles. Such classification, however, may often be a tedious task requiring analysis work in addition to what is necessary to resolve the issue. To increase classification efficiency, adapted schemes are needed. In this paper a light-weight defect classification scheme adapted for minimal process footprint -- in terms of learning and classification effort -- is proposed and initially evaluated. Method: A case study was conducted at Volvo Car Corporation to adapt the IEEE Std. 1044 for automotive embedded software. An initial evaluation was conducted by applying the adapted scheme to defects from an existing software product with industry professionals as subjects. Results: The results showed that the classification scheme was quick to learn and understand -- required classification time stabilized around 5-10 minutes already after practicing on 3-5 defects. The results also showed that the patterns in the classified defects were interesting for the professionals, although in order to apply statistical methods more data was needed. Conclusions: We conclude that the adapted classification scheme captures what is currently tacit knowledge and has the potential of revealing patterns in the defects detected in different project phases. Furthermore, we were, in the initial evaluation, able to contribute with new information about the development process. As a result we are currently in the process of incorporating the classification scheme into the company's defect reporting system.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405374,no
Static Analysis of Model Transformations for Effective Test Generation,"Model transformations are an integral part of several computing systems that manipulate interconnected graphs of objects called models in an input domain specified by a metamodel and a set of invariants. Test models are used to look for faults in a transformation. A test model contains a specific set of objects, their interconnections and values for their attributes. Can we automatically generate an effective set of test models using knowledge from the transformation? We present a white-box testing approach that uses static analysis to guide the automatic generation of test inputs for transformations. Our static analysis uncovers knowledge about how the input model elements are accessed by transformation operations. This information is called the input metamodel footprint due to the transformation. We transform footprint, input metamodel, its invariants, and transformation pre-conditions to a constraint satisfaction problem in Alloy. We solve the problem to generate sets of test models containing traces of the footprint. Are these test models effective? With the help of a case study transformation we evaluate the effectiveness of these test inputs. We use mutation analysis to show that the test models generated from footprints are more effective (97.62% avg. mutation score) in detecting faults than previously developed approaches based on input domain coverage criteria (89.9% avg.) and unguided generation (70.1% avg.).",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405377,no
Oracle-Centric Test Case Prioritization,"Recent work in testing has demonstrated the benefits of considering test oracles in the testing process. Unfortunately, this work has focused primarily on developing techniques for generating test oracles, in particular techniques based on mutation testing. While effective for test case generation, existing research has not considered the impact of test oracles in the context of regression testing tasks. Of interest here is the problem of test case prioritization, in which a set of test cases are ordered to attempt to detect faults earlier and to improve the effectiveness of testing when the entire set cannot be executed. In this work, we propose a technique for prioritizing test cases that explicitly takes into account the impact of test oracles on the effectiveness of testing. Our technique operates by first capturing the flow of information from variable assignments to test oracles for each test case, and then prioritizing to ``cover'' variables using the shortest paths possible to a test oracle. As a result, we favor test orderings in which many variables impact the test oracle's result early in test execution. Our results demonstrate improvements in rate of fault detection relative to both random and structural coverage based prioritization techniques when applied to faulty versions of three synchronous reactive systems.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405379,no
The Nature of the Times to Flight Software Failure during Space Missions,"The growing complexity of mission-critical space mission software makes it prone to suffer failures during operations. The success of space missions depends on the ability of the systems to deal with software failures, or to avoid them in the first place. In order to develop more effective mitigation techniques, it is necessary to understand the nature of the failures and the underlying software faults. Based on their characteristics, software faults can be classified into Bohrbugs, non-aging-related Mandelbugs, and aging-related bugs. Each type of fault requires different kinds of mitigation techniques. While Bohrbugs are usually easy to fix during development or testing, this is not the case for non-aging-related Mandelbugs and aging-related bugs due to their inherent complexity. Systems need mechanisms like software restart, software replication or software rejuvenation to deal with failures caused by these faults during the operational phase. In a previous study, we classified space mission flight software faults into the three above-mentioned categories based on problems reported during operations. That study concentrated on the percentages of the faults of each type and the variation of these percentages within and across different missions. This paper extends that work by exploring the nature of the times to software failure due to Bohrbugs and non-aging-related Mandelbugs for eight JPL/NASA missions. We start by applying trend tests to the times to failure to check if there is any reliability growth (or decay) for each type of failure. For those times to failure sequences with no trend, we fit distributions to the data sets and carry out goodness-of-fit tests. The results will be used to guide the development of improved operational failure mitigation techniques, thereby increasing the reliability of space mission software.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405381,no
On the Use of Boundary Scan for Code Coverage of Critical Embedded Software,"Code coverage tools are becoming increasingly popular as valuable aids in assessing and improving the quality of software structural tests. For some industries, such as aeronautics or space, they are mandatory in order to comply with standards and to help reduce the validation time of the applications. These tools usually rely on code instrumentation, thus introducing important time and memory overheads that may jeopardize its applicability to embedded and real-time systems. This paper explores the use of IEEE 1149.1 (boundary scan) infrastructure and on-chip debugging facilities from embedded processors for collecting the program execution trace during tests, without the introduction of any extra code, and then extracting detailed code coverage analysis and profiling information. We are currently developing an extension to the csXception tool to include such capabilities, in order to study the advantages, difficulties and impediments of using boundary scan for code coverage.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405382,no
Using Non-redundant Mutation Operators and Test Suite Prioritization to Achieve Efficient and Scalable Mutation Analysis,"Mutation analysis is a powerful and unbiased technique to assess the quality of input values and test oracles. However, its application domain is still limited due to the fact that it is a time consuming and computationally expensive method, especially when used with large and complex software systems. Addressing these challenges, this paper makes several contributions to significantly improve the efficiency of mutation analysis. First, it investigates the decrease in generated mutants by applying a reduced, yet sufficient, set of mutants for replacing conditional (COR) and relational (ROR) operators. The analysis of ten real-world applications, with 400,000 lines of code and more than 550,000 generated mutants in total, reveals a reduction in the number of mutants created of up to 37% and more than 25% on average. Yet, since the isolated use of non-redundant mutation operators does not ensure that mutation analysis is efficient and scalable, this paper also presents and experimentally evaluates an optimized workflow that exploits the redundancies and runtime differences of test cases to reorder and split the corresponding test suite. Using the same ten open-source applications, an empirical study convincingly demonstrates that the combination of non-redundant operators and prioritization leveraging information about the runtime and mutation coverage of tests reduces the total cost of mutation analysis further by as much as 65%.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405400,no
Mutation Testing of Event Processing Queries,"Event processing queries are intended to process continuous event streams. These queries are partially similar to traditional SQL queries, but provide the facilities to express rich features (e.g., pattern expression, sliding window of length and time). An error while implementing a query may result in abnormal program behaviors and lost business opportunities. Moreover, queries can be generated with unsanitized inputs and the structure of intended queries might be altered. Thus, a tester needs to test the behavior of queries in presence of malicious inputs. Mutation testing has been found to be effective to assess test suites quality and generating new test cases. Unfortunately, there is no effort to perform mutation testing of event processing queries. In this work, we propose mutation-based testing of event processing queries. We choose Event Processing Language (EPL) as our case study and develop necessary mutation operators and killing criteria to generate high quality event streams and malicious inputs. Our proposed operators modify different features of EPL queries (pattern expression, windows of length and time, batch processing of events). We develop an architecture to generate mutants for EPL and perform mutation analysis. We evaluate our proposed EPL mutation testing approach with a set of developed benchmark containing diverse types EPL queries. The evaluation results indicate that the proposed operators and mutant killing criteria are effective to generate test cases capable of revealing anomalous program behaviors (e.g., event notification failure, delay of event reporting, unexpected event), and SQL injection attacks. Moreover, the approach incurs less manual effort and can complement other testing approach such as random testing.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405401,no
"An Empirical Study of the Effectiveness of """"Forcing"""" Diversity Based on a Large Population of Diverse Programs","Use of diverse software components is a viable defence against common-mode failures in redundant software-based systems. Various forms of """"Diversity-Seeking Decisions"""" (""""DSDs"""") can be applied to the process of developing, or procuring, redundant components, to improve the chances of the resulting components not failing on the same demands. An open question is how effective these decisions, and their combinations, are for achieving large enough reliability gains. Using a large population of software programs, we studied experimentally the effectiveness of specific """"DSDs"""" (and their combinations) mandating differences between redundant components. Some of these combinations produced much better improvements in system probability of failure per demand (PFD) than """"uncontrolled"""" diversity did. Yet, our findings suggest that the gains from such """"DSDs"""" vary significantly between them and between the application problems studied. The relationship between DSDs and system PFD is complex and does not allow for simple universal rules (e.g. """"the more diversity the better"""") to apply.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405403,no
Software at Scale for Building Resilient Wireless Sensor Networks,"Wireless Sensor Networks (WSNs) are widely recognized as a promising solution to build next-generation monitoring systems. Their industrial uptake is however still compromised by the low level of trust on their performance and dependability. Whereas analytical models represent a valid mean to assess nonfunctional properties via simulation, their wide use is still limited by the complexity and dynamicity of WSNs, which lead to unaffordable modeling costs. This paper proposes an approach to characterize the resiliency of the software of WSN software to failures. The focus is in providing a procedure and related tools to assess i) how the node software, hardware platforms, topology and routing protocols impact on the failure behavior of nodes and and of the network, and, vice-versa, ii) how the failure of a node mutates the behavior of the running software and routing protocol. The approach adopts a software characterization process which is based on i) Failure Mode and Effect Analysis, ii) automated fault injection experiments, iii) high-level description of the fault tolerant mechanisms of WSN software in a proposed framework.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405411,no
Early Performance Estimation for Industrial Component-Based Design of Reliable Software Defined Radio System,"The growing complexity of software applications, combined with increasing reliability requirements and constant quality and time-to-market constraints, creates new challenges for performance engineering practices in the area of real-time embedded systems. It is namely expected that delivered products combine timing garantees with fault tolerant behavior, e.g. by switching to fault tolerant modes in case of errors, while respecting strict real-time requirements. When developing such real-time systems according to a traditional application of the """"Va""""-cycle, performance verification and validation activities start only when development and integration are completed. As a consequence, performance issues are detected at a late stage. At this time, they are more difficult and expensive to fix. At Thales, we have therefore focused on the automation of performance engineering activities and their application as early as possible in the industrial design process of reliable Software Defined Radio (SDR) systems, as a mean to shorten the design time and reduce risks of timing failures. The SDR particularity consists in implementing the signal modulation of a communication radio using software rather than hardware. It is namely much easier to reprogram the modulation carrier in order to fit different situations with the same hardware radio (e.g. each national army typically uses specific waveforms to guarantee the confidentiality of communications). The implementation of a reliable software waveform can be a difficult task, as it involves real-time constraints as well as embedded aspects while dealing with complex algorithms including those for fault recovery. Thales created a software framework family, named MyCCM (Make your Component Container Model), to support the implementation of such real-time embedded software. MyCCM is a tailorable component based design approach that takes inspiration from the Lightweight Component Container Model (LwCCM) defined by the OMG. It impl- ments the concept of functional components that encapsulate algorithms. The components correspond to passive code controlled by an underlying runtime, and are connected through communication ports to create a complete application. This allows the construction of applications by assembling independent functional components. It also enforces the separation of concerns between the functional aspects described by component ports and the non functional elements that stay outside the components (message chaining across the whole component architecture, FIFO sizes, task priorities, communications mechanisms, execution times, etc). MyCCM models can be designed either using Thales internal modeling tools or using UML modelers with plain UML. Our developed framework for the early performance estimation of SDR MyCCM models is represented in the figure below. The first step consists in extending the SDR MyCCM model with performance properties, i.e. timing and behavior characteristics of the application (e.g. execution times and activation frequencies for threads and methods, data dependencies and communication protocols between threads) and execution characteristics of the hardware platform (e.g. speed, scheduling policy, etc). The OMG standard MARTE is key language for this purpose. However, due to the complexity of its syntax, it may result in very complex and confusing diagrams and models. We have therefore adapted the MARTE syntax based on the Thales SDR designers' feedback, thus allowing representing the performance properties in an easier and much more intuitive manner. We have opted for scheduling analysis techniques for the performance estimation of the extended MyCCM models, which is the next step in our framework. These techniques are well adapted for this purpose, since they rely on an abstraction of the timing relevant characteristics and behaviors. From these characteristics, the scheduling analysis systematically derives worst-case scheduling scenarios, and timi",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405415,no
Quality Playbook: Ensuring Release to Release Improvement,"Summary form only given. Before a major feature release is made available to customers, it is important to be able to anticipate if the release will be of lesser quality than its predecessor release. Our research group has developed models that use development and test times, resource levels, code added, and bugs found and fixed (or not fixed) to predict whether or not a new feature release will achieve a key quality goal - to be of better quality than its predecessor release. If the release quality prediction models, developed early in the development branches integration phase, indicate a likely upcoming quality problem in the field, another set of predictive models ('playbook' models) are then developed and used by our team to identify development or test practices that are in need of improvement. These playbook models are key components of what we call 'quality playbooks,' that are designed to address several objectives: . Identify 'levers' that positively influence feature release quality. Levers are in-process engineering metrics that are associated with specific development or test processes/practices and measure their adoption and effectiveness. . If possible, identify levers that can be invoked early in the lifecycle, to enable the development and test teams to improve deficient practices and remediate the current release under development. If it is not possible to identify early levers but possible to identify levers later in the lifecycle, we can only change deficient practices to improve the quality of future successor releases. . Determine the potential quality impact of changes suggested by the profile of significant levers. Low impact levers are likely not to be addressed by development teams. . Determine the resource and schedule investments needed to change and implement practices: Training, disruption, additional engineering time, etc. . Using impact and investment calculations identify which practices to change, either for the current relea- e or just for subsequent releases. Develop a prioritization/ROI scheme to provide planning guidance to development and test teams. . Identify specific practice changes needed, or new practices to adopt. . Design and plan pilot programs to test the models, including the impact and investment components. Using this 'playbook' approach, our team has developed models for 31 major feature releases that are resident on 11 different hardware platforms. These models have identified six narrowly-defined classes of metrics that include both actionable levers and 'indicator' metrics that correlate well with release quality. (Indicator metrics do also correlate well, but are less specifically actionable.) The models for these six classes of metrics (and their associated practices) include strong levers and strong indicators for all releases and platforms thus far examined. Impact and investment results are also described in this paper, as are pilot programs that have tested the validity of the modeling and business calculation results. Two additional large-scale pilots of the 'playbook' approach are underway, and these are also described.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405417,no
Assessing Product Quality through PMR Analysis: A Perspective,"Success of a software product in the marketplace is defined by Quality of the product among other factors Assessing the current quality of the product is essential before it can be improved The assessment needs to be accurate in order to have the right action plan to improve quality Test methods and code reviews used to identify quality gaps in pre-release software are inadequate Product quality can be assessed by analysing PMRs (Problem Management Reports) in post-release software a) A PMR is a well defined document that is used to Report a customer found issue with the product Track the customer issue to closure b) Analysis of PMRs opened in the past are a clear indication of: i) Product components that are Most commonly used and having the most number of bugs, classified by sub-component Least commonly used ii) Sub-components to be focused on, to provide a visible improvement in the overall product quality c) PMR analysis improves the accuracy of the assessment as it is based on actual product usage In turn improves the accuracy / relevance of the action plan formulated to improve quality The paper outlines the quality assessment of a software product through PMR analysis.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405426,no
The Effect of Testability on Fault Proneness: A Case Study of the Apache HTTP Server,"Numerous studies have identified measures that relate to the fault-proneness of software components. An issue practitioners face in implementing these measures is that the measures tend to provide predictions at a very high level, for instance the per-module level, so it is difficult to provide specific recommendations based on those predictions. We examine a more specific measure, called software testability, based on work in test case generation. We discuss how it could be used to make more specific code improvement recommendations at the line-of-code level. In our experiment, we compare the testability of fault prone lines with unchanged lines. We apply the experiment to Apache HTTP Server and find that developers more readily identify faults in highly testable code. We then compare testability as a fault proneness predictor to McCabe's cyclomatic complexity and find testability has higher recall.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405434,no
Debugging Spreadsheets: A CSP-based Approach,"Despite being staggeringly error prone, spreadsheets can be viewed as a highly flexible end-users programming environment. As a consequence, spreadsheets are widely adopted for decision making, and may have a serious economical impact for the business. Hence, approaches for aiding the process of pinpointing the faulty cells in a spreadsheet are of great value. We present a constrain-based approach, CONBUG, for debugging spreadsheets. The approach takes as input a (faulty) spreadsheet and a test case that reveals the fault and computes a set of diagnosis candidates for the debugging problem we are trying to solve. To compute the set of diagnosis candidates we convert the spreadsheet and test case to a constraint satisfaction problem. From our experimental results, we conclude that CONBUG can be of added value for the end user to pinpoint faulty cells.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405435,no
Predicting Data Dependences for Slice Inspection Prioritization,"Data dependences play a central role in program debugging and comprehension. They serve as building blocks for program slicing and statistical fault localization, among other debugging approaches. Unfortunately, static analysis reports many data dependences that, in reality, are infeasible or unlikely to occur at runtime. This phenomenon is exacerbated by the extensive use of pointers and object-oriented features in modern software. Dynamic analysis, in contrast, reports only data dependences that occur in an execution but misses all other dependences that can occur in the program. To tackle the imprecision of data-dependence analysis, we present a novel static analysis that predicts the likelihood of occurrence of data dependences. Although it is hard to predict execution frequencies accurately, our preliminary results suggest that our analysis can distinguish the data dependences most likely to occur from those less likely to occur, which helps engineers prioritize their inspection of dependences in slices. These are promising results that encourage further research.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405438,no
Wielding Statistical Fault Localization Statistically,"Program debugging is a laborious but necessary phase of software development. It generally consists of fault localization, bug fix, and regression testing. Statistical software fault localization automates the manual and error-prone first task. It predicts fault locations by analyzing dynamic program spectrum captured in program runs. Previous studies mostly focused on how to provide reliable input data to such a technique and how to process the data accurately, but inadequately studied how to wield the output result of such a technique. In this work, we raise the assumption of symmetric distribution on the effectiveness of such a technique in locating faults, based on empirical results. We use maximum likelihood estimate and linear programming to develop a tuning method to enhance the result of a statistical fault localization technique. Experiments with two representative such techniques on two realistic UNIX utility programs validate our assumption and show our method effective.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405440,no
Automated Risk-Based Testing by Integrating Safety Analysis Information into System Behavior Models,"The development of safety-critical software-intensive systems requires systematic quality assurance on all stages of the development process. Executable development artifacts are validated against the system specifications. Risk-based test approaches enable the distribution of test effort in a specific way to cover critical system parts, functions, and requirements. The development process of safety-critical systems usually implies analysis activities for determining and understanding hazards and risks. Moreover, it requires a systematic design of the system structure and behavior based on the specification. For achieving a high degree of automation of test case derivation, existing formal models from the risk analysis and system design phases are combined. The approach presented here focuses on integration of fault trees into state-based behavior models. Therefore, fault trees are analyzed and their elements are assessed for their validity and significance for the test modeling. The approach systematically transforms the relevant fault tree elements like single critical basic events, system states, or sequences of events into elements of the state-based behavior model. The resulting model enables the automated generation of test cases considering risk-based test purposes such as the coverage of critical states, transitions, or sub-models. The feasibility of the approach is shown in a small case study.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405444,no
Assessing AUTOSAR Systems Using Fault Injection,"This fast abstract introduces a fault injection approach that achieves the mentioned objective while facing a number of facts and technical limitations discussed onwards. While commercially available AUTOSAR basic software (BSW) implementations are certified and ISO 26262 complaint, third party hardware and application software might have not gone through the same rigorous and extensive non-simulation-based validation activities. Also, AUTOSAR was built without taking explicitly fault injection needs into account, which resulted in the lack of required accessibility to either hardware or software interfaces in order to support the injection of faults.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405502,no
An LED Monitoring System Based on the Real-Time Power Consumption Detection Technology,"A Kind of LED lighting system using AD7755, micro controller and Ethernet drives is presented, to control multiple LED in remote place and detect power consumption of the system. The hardware and software designs are also described. The system can not only adjust the three-color LED light group brightness of each node through host computer, but also get the energy consumption data of each node in time. All the information can be processed in host computer. The results of experiments show that the system works steadily and has good communication quality, achieving the purpose for controlling LED lights brightness and monitoring LED lights power consumption.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405704,no
Notice of Violation of IEEE Publication Principles<BR>Services Selection of Transactional Property for Web Service Composition,"Notice of Violation of IEEE Publication Principles<BR><BR>""""Services Selection of Transactional Property for Web Service Composition""""<BR>by Guojun Zhang<BR> in the Proceedings of the Eighth International Conference on Computational Intelligence and Security, November 2012, pp. 605-608<BR><BR>After careful and considered review of the content and authorship of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>This paper has copied significant portions of the original text from the paper cited below. The original text was copied without attribution (including appropriate references to the original author(s) and/or paper title) and without permission.<BR><BR>""""TQoS: Transactional and QoS-Aware Selection Algorithm for Automatic Web Service Composition""""<BR>by Joyce El Haddad, Maude Manouvrier, and Marta Rukoz<BR>in the IEEE Transactions on Services Computing, Vol 3, No. 1, March 2010, pp. 73-85<BR><BR>Web service composition enables seamless and dynamic integration of business applications on the web. Due to the inherent autonomy and heterogeneity of component Web services it is difficult to predict the behavior of the overall composite service. Therefore, transactional properties are crucial for selecting the web services to take part in the composition. Transactional properties ensure reliability of composite Web service. In this paper we propose a novel selection approach based on transactional properties of ensuring reliability. We build a model to implement transactional-aware service selection, and use the model composite Web service to guarantee reliable execution. We evaluate our approach experimentally using both real and synthetically generated datasets.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405912,no
Improved Differential Fault Analysis of SOSEMANUK,"We present a more efficient differential fault analysis (DFA) attack on SOSEMANUK, a new synchronous software-oriented stream cipher, which is contained in the current eSTREAM Portfolio. In the previous study, it is required around 6144 faults, 2<sup>48</sup> SOSEMANUK iterations and 2<sup>38.17</sup> bytes storage to recovers the secret inner state of the cipher. We offer an improved attack and show that only around 4608 faults, 2<sup>35.16</sup> SOSEMANUK iterations and 2<sup>23.46</sup> bytes storage are needed under the same or even weaker fault model. The simulation results of the proposed attack show that it takes about 11.35 hours when using a PC.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6406066,no
Streamlining Service Levels for IT Infrastructure Support,"For IT Infrastructure Support (ITIS), it is crucial to identify opportunities for reducing service costs and improving service quality. We focus on streamlining service levels i.e., finding right resolution level for each ticket, to reduce time, efforts and cost for ticket handling, without affecting workloads and user satisfaction. We formalize this problem and present two statistics-based search algorithms for identifying problems suitable for left-shift (from expensive, expertise intensive L2 level to cheaper, simpler L1 level) and right-shift (from L1 to L2). The approach is domain-driven: it produces directly usable and often novel results, without any trial-and error experimentation, along with detailed justifications and predicted impacts. This helps in acceptance among end-users and more active use of the results. We discuss one real-life case-study of results produced by the algorithms.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6406456,no
Independent Assessment of Safety-Critical Systems: We Bring Data!,"Safety-critical systems are systems where failures lead to catastrophic results: resulting in loss of life, significant property damage, or damage to the environment. These systems range from aerospace on-board control systems, ground flight control systems, medical devices, nuclear power plants control, automotive systems, military systems, just to name a few. Other information systems are becoming more and more """"safety-critical"""" due to the financial impacts of failures and the fact that human lives depend on them.The constant technology evolution is making these systems more complex and more common, and we depend more on them. Thus, we need to guarantee maximum dependability and safety properties with better processes and tools. The systems complexity is usually boosted by the flexibility of software, and thus software is becoming both a solution and a problem. Keeping dependability of software at the highest level requires evolutions that cover the processes and tools and all the development/qualification life-cycle phases: Specification; Architecture; Coding; Verification and Validation. Independent (software) verification and validation (ISVV) activities have been used and evolving since the seventies to ensure high safety and dependability and to take advantage of organizational and technical independence by avoiding biased assessments. Critical Software has been involved in developing and applying ISVV methods and techniques since the early 2000's, and this experience collected a significant amount of data that cover different domains: space on-board and ground systems, aeronautics, transportation, financial and banking, amongst others. This industrial paper will not cover the technical details of the processes, methods and tools applied, but will instead present important metrics and subsequent findings from the collected data. The results presented cover both technical issues found per each phase of the development life-cycle and required effort to perfor- the independent assessments. The outcome is interesting since it allows to compare data between similar or different industries (process/organizational maturities), same and different domains/criticalities, software developed either by experienced industrial partners or less experienced, life-cycle phase where more problems are detected or where they are easily detected, software/system dependability evolution after the first assessments, efficiency of the applied techniques and return on investment according to consultants level, initial systems maturity, criticality, project life-cycle phase, etc. All these factors can be analyzed from the collected metrics, and we can also conclude on the number non-common issues found, that include abnormal behavior of the systems (for example under non-nominal conditions), significant organizational factors (yes, they are really important) or human factors (operator related risks, security threats, etc). A study from Johnson and Holloway over some of the major aviation and maritime accidents in North America during 1996-2006 concluded that the proportion of causal and contributory factors related to organizational issues exceeded those due to human errors. The study showed that the causal and contributory factors in the USA aviation accidents have the following distribution: 48% is related to organizational factors, the equivalent human factors represented 37%, equipment factors represented 12%, other causes represented 3%; The same exercise for maritime accidents classified: 53% due to organizational factors, 24-29% as human error, 10-19% to equipment failures, and 2-4% as other causes. The data presented and analyzed in this industrial paper comes from dozens of projects, and originated over 3000 issues. This article will present the facts related to safety-critical software development quality metrics performed by independent assessments of quite mature systems, and will infer also return on investment (R",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6407380,no
AFD: Adaptive failure detection system for cloud computing infrastructures,"Cloud computing has become increasingly popular by obviating the need for users to own and maintain complex computing infrastructure. However, due to their inherent complexity and large scale, production cloud computing systems are prone to various runtime problems caused by hardware and software failures. Autonomic failure detection is a crucial technique for understanding emergent, cloud-wide phenomena and self-managing cloud resources for system-level dependability assurance. To detect failures, we need to monitor the cloud execution and collect runtime performance data. These data are usually unlabeled, and thus a prior failure history is not always available in production clouds, especially for newly managed or deployed systems. In this paper, we present an Adaptive Failure Detection (AFD) framework for cloud dependability assurance. AFD employs data description using hypersphere for adaptive failure detection. Based on the cloud performance data, AFD detects possible failures, which are verified by the cloud operators. They are confirmed as either true failures with failure types or normal states. AFD adapts itself by recursively learning from these newly verified detection results to refine future detections. Meanwhile, AFD exploits the observed but undetected failure records reported by the cloud operators to identify new types of failures. We have implemented a prototype of the AFD system and conducted experiments in an on-campus cloud computing environment. Our experimental results show that AFD can achieve more efficient and accurate failure detection than other existing schemes.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6407740,no
Novel approach for Interference Management in cognitive radio,"Cognitive radio based on a software-defined radio, is an emerging wireless communication system today. It is regarded as fifth generation (5G) mobile system. Cognitive radio research can be categorized in three parts, which are Spectrum Management, Intelligence Management and Interference Management. Spectrum management detects white space and manages different spectrum issues between primary and secondary users. Intelligence management on the other hand use different artificial intelligence techniques such as Neural Networks, Rule based system, Genetic Algorithm etc. in order to develop efficient cognitive engine. Finally Interference management focus on implementation issues of cognitive radio by dealing with channel awareness, link quality and resource allocation which mainly depend on the right choice of transmit power. This paper proposes an algorithm based on MAC scheduling techniques and uses two parallel processes to control transmit power. The first process go through different mathematical calculations of SINR, channel capacity etc of all links between different radio nodes within a network. The second process deals with environmental conditions with the help of Fuzzy logic. These both processes works together to adjust the value of transmit power of every node in order to mitigate interference and maintain QoS of the service.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6408448,no
Region-Based perceptual quality regulable bit allocation and rate control for video coding applications,"In this paper, a perceptual quality regulable H.264 video encoder system has been developed. We use structure similarity index as the quality metric for distortion-quantization modeling and develop a bit allocation and rate control scheme for enhancing regional perceptual quality. Exploiting the relationship between the reconstructed macroblock and its best predicted macroblock from mode decision, a novel quantization parameter prediction method is built and used to regulate the video quality of the processing macroblock according to a target perceptual quality. Experimental results show that the model can achieve high accurate. Compared to JM reference software with macroblock layer rate control, the proposed encoding system can effectively enhance perceptual quality for target video regions.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6410836,no
An Optimal Approach Towards Recognizing Broken Thai Characters in OCR Systems,"This paper presents a novel technique for recognizing broken Thai characters found in degraded Thai text documents by modeling it as a set-partitioning problem (SPP). The technique searches for the optimal set-partition of the connected components by which each subset yields a reconstructed Thai character. Given the non-linear nature of the objective function needed for optimal set-partitioning, we design an algorithm we call Heuristic Incremental Integer Programming (HIIP), that employs integer programming (IP) with an incremental approach using heuristics to hasten the convergence. To generate corrected Thai words, we adopt a probabilistic generative approach based a Thai dictionary corpus. The proposed technique is applied successfully to a Thai historical document and poor quality Thai fax document with promising accuracy rates over 93%.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6411736,no
Hierarchical prosodic boundary prediction for Uyghur TTS,"Correct prosodic boundary prediction is crucial for the quality of synthesized speech. This paper presents the prosodic hierarchy of Uyghur-language which belongs to agglutinative language. A two-layer bottom-up hierarchical approach based on conditional random fields (CRF) is used for predicting prosodic word (PW) and prosodic phrase (PP) boundaries. In order to disambiguate the confusion between different prosodic boundaries at punctuation sites, CRF based prosodic boundary determination model is used and integrated with bottom-up hierarchical approach. Word suffix feature is considered useful for prosodic boundary prediction and added into the feature sets. The experimental results show that the proposed method successfully resolves the confusion between different prosodic boundaries. Consequently, further enhance the accuracy of prosodic boundary prediction.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6411971,no
New scientific contributions to the prediction of the reliability of critical systems which based on imperfect debugging method and the increase of quality of service,"This paper presents a new method by which it is possible to realistically predict the software reliability of critical systems. The main feature of this method is that it allows estimating the number of remaining critical faults in the software. The algorithm employs well-known methods such as Imperfect Debugging and it provides a more reliable prognosis than the methods conventionally used for this purpose. Furthermore, the new approach describes two processes of handling critical failures (one for detection and one for correction). The new algorithm also takes into account the socalled repair time, a measurement that is vitally important for a reliable prognosis. For use in the prediction model, it is mathematically described as a time function. As every programmer knows, it can be difficult to have even the simplest program run without faults. So-called software reliability models (SRM's), based on stochastic and aiming to predict the reliability of both software and hardware, have been used since the 70's. SRM's rely on certain model assumptions some of which cannot be deemed realistic anymore. Hence, for today's reliability engineering, these models are insufficient. At this point in time, though, there are hardly any methods that enable us to obtain predictions as to how the reliability of critical faults or the failure rate of critical systems behave over time. Currently, there is no mathematical model distinguishing between critical and non-critical faults, and only few models consider Imperfect Debugging (ID). The method presented here, however, is based on ID and it is able to distinguish between critical and non-critical software faults. Moreover, this new method employs a so-called Time-Delay and thus two new processes have to be designed. Mathematically, these processes describe the detection of faults and their correction, respectively. It is necessary to define appropriate distribution functions and to clearly state the requisite model assumption- .",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6412091,no
Software Aging in Virtualized Environments: Detection and Prediction,"Software aging has been cited in many scenarios including Operating System, Web Servers, Real-time Systems. However, few studies have been conducted in long running virtualized environments where more and more software is being delivered as a service. Furthermore, state-of-the-art methods lack the ability to deal with miscellaneous upper applications and underlying systems transparently in virtualized scenarios. In this paper, we detect aging phenomenon by conducting experiments in physical and virtual machines and identify the differences between the two, and propose a feature code-based methodology for failure prediction through system call, then implement a prototype in virtual machine manager layer to predict failure time and rejuvenate transparently, which is suitable in virtualized scenarios. The evaluation shows the prediction deviation against reality is less than 10%.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6413621,no
Improving accuracy of DGA interpreation of oil-filled power transformers needed for effective condition monitoring,"The probability of equipment failure increases over time as the age and rate of use increases. Since faults are the major cause of these failures, there are several ways and means used towards predicting fault occurrence and thus preventing the equipment from failing by diagnosing its condition. In oil filled transformers, the Dissolve Gas Analysis (DGA) is used as one of the well-established techniques to predict incipient faults inside its enclosure. With the existence of more than 6 known methods of DGA fault interpretation techniques; there is the likelihood that they may give different conditions for the same sample. Using a combination of many of the diagnostic methods will therefore increase the accuracy of the interpretation and so increase the certainty of the transformer condition. This paper presents a computer program based condition diagnosis system developed to combine four DGA assessment techniques; Rogers Ratio Method, IEC Basic Ratio Method, Duval Triangle method and Key Gas Method. A user friendly GUI presented to give a visual display of the four techniques and the output of the combined interpretation. The result of the prediction analyses done to test the accuracy of the program shows an overall DGA prediction accuracy of 97.03% compared to the 91% of the most reliable individual method; Duval Triangle and near elimination of `no prediction' condition.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6416458,no
Improving design quality by automatic verification of activity diagram syntax,"The quality of the product is an important issue in software development and quality assurance is an important aspect of any software design. One of the factors that affect the software quality is the correctness of its design. Any defect in the design can lead to high cost for defect correction. Activity diagrams are used to model the dynamic or behavioral aspects of the system. In this paper, an algorithm that analyzes activity diagrams and automatically verifies the syntax of each of its components is presented. Incomplete workflow can lead to incorrect results and a missing edge can lead to incomplete workflow. Mismatch in fork, join pair can lead to concurrency issues and synchronization problems. Detection of such errors in the design phase ensures product quality. The activity diagram is transformed to its components and analysis is performed on the components based on the syntactic specifications to detect errors. The workflow in the diagram and syntactic correctness of control flow are analyzed by the algorithm. Errors, if any, in the diagram are identified and a log of the errors is maintained in the error table. Analysis of the activity diagram and verification of its syntax can help in the development of a product whose quality is assured.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6416555,no
Entropy based bug prediction using support vector regression,"Predicting software defects is one of the key areas of research in software engineering. Researchers have devised and implemented a plethora of defect/bug prediction approaches namely code churn, past bugs, refactoring, number of authors, file size and age, etc by measuring the performance in terms of accuracy and complexity. Different mathematical models have also been developed in the literature to monitor the bug occurrence and fixing process. These existing mathematical models named software reliability growth models are either calendar time or testing effort dependent. The occurrence of bugs in the software is mainly due to the continuous changes in the software code. The continuous changes in the software code make the code complex. The complexity of the code changes have already been quantified in terms of entropy as follows in Hassan [9]. In the available literature, few authors have proposed entropy based bug prediction using conventional simple linear regression (SLR) method. In this paper, we have proposed an entropy based bug prediction approach using support vector regression (SVR). We have compared the results of proposed models with the existing one in the literature and have found that the proposed models are good bug predictor as they have shown the significant improvement in their performance.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6416630,no
An efficient programming rule extraction and detection of violations in software source code using neural networks,The larger size and complexity of software source code builds many challenges in bug detection. Data mining based bug detection methods eliminate the bugs present in software source code effectively. Rule violation and copy paste related defects are the most concerns for bug detection system. Traditional data mining approaches such as frequent Itemset mining and frequent sequence mining are relatively good but they are lacking in accuracy and pattern recognition. Neural networks have emerged as advanced data mining tools in cases where other techniques may not produce satisfactory predictive models. The neural network is trained for possible set of errors that could be present in software source code. From the training data the neural network learns how to predict the correct output. The processing elements of neural networks are associated with weights which are adjusted during the training period.,2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6416837,no
A Compositional Trust Model for Predicting the Trust Value of Software System QoS Properties,"Trust of a software system (i.e., the degree of confidence that a system conforms to its specification) can be defined in terms the composition of trust values for each individual software component (or service) used in the creation of that software system and their interaction patterns. This paper therefore presents the different composition patterns, and the common associations between various quality-of-service (QoS) properties of the composed system for the identified composition patterns. It also presents composition rules for deriving systemic trust values for each composition pattern. Lastly, results from applying the composition patterns to a case study show our trust composition model can predict trust of a composed system with 55%-70% uncertainty that increases with system complexity.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6417348,no
Intelligent system for predicting wireless sensor network performance in on-demand deployments,"The need for advanced tools that provide efficient design and planning of on-demand deployment of wireless sensor networks (WSN) is critical for meeting our nation's demand for increased intelligence, reconnaissance, and surveillance in numerous safety-critical applications. For practical applications, WSN deployments can be time-consuming and error-prone, since they have the utmost challenge of guaranteeing connectivity and proper area coverage upon deployment. This creates an unmet demand for decision-support systems that help manage this complex process. This paper presents research-in-progress to develop an advanced decision-support system for predicting the optimal deployment of wireless sensor nodes within an area of interest. The proposed research will have significant impact on the future application of WSN technology, specifically in the emergency response, environmental quality, national security, and engineering education domains.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6417620,no
High-performance scalable information service for the ATLAS experiment,"The ATLAS experiment is being operated by highly distributed computing system which is constantly producing a lot of status information which is used to monitor the experiment operational conditions as well as to assess the quality of the physics data being taken. For example the ATLAS High Level Trigger(HLT) algorithms are executed on the online computing farm consisting from about 1500 nodes. Each HLT algorithm is producing few thousands histograms, which have to be integrated over the whole farm and carefully analyzed in order to properly tune the event rejection. In order to handle such non-physics data the Information Service (IS) facility has been developed in the scope of the ATLAS Trigger and Data Acquisition (TDAQ) project. The IS provides high-performance scalable solution for information exchange in distributed environment. In the course of an ATLAS data taking session the IS handles about hundred gigabytes of information which is being constantly updated with the update interval varying from a second to few tens of seconds. IS provides access to any information item on request as well as distributing notification to all the information subscribers. In latter case IS subscribers receive information within few milliseconds after it was updated. IS can handle arbitrary types of information including histograms produced by the HLT applications and provides C++, Java and Python API. The Information Service is a primarily and in most cases a unique source of information for the majority of the online monitoring analysis and GUI applications, used to control and monitor the ATLAS experiment. Information Service provides streaming functionality allowing efficient replication of all or part of the managed information. This functionality is used to duplicate the subset of the ATLAS monitoring data to the CERN public network with the latency of few milliseconds, allowing efficient real-time monitoring of the data taking from outside the protected ATLAS network. Ea- h information item in IS has an associated URL which can be used to access that item online via HTTP protocol. This functionality is being used by many online monitoring applications which can run in a WEB browser, providing real-time monitoring information about ATLAS experiment over the globe. This paper will describe design and implementation of the IS and present performance results which have been taken in the ATLAS operational environment.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6418091,no
Experience with the custom-developed ATLAS offline trigger monitoring framework and reprocessing infrastructure,"The offline trigger monitoring of the ATLAS experiment is assessing the data quality and analyses those events where no trigger decision could be made. Within the offline monitoring, which is started shorty after the data acquisition has finished, the online data quality assessment is reflected. Additionally, a reprocessing system tests changes to the trigger software and configuration to ensure their stability and reliability before they can become operating. This note explains the activities performed to provide a flawless monitoring of the operation of the ATLAS trigger system and how to assess the quality of the recorded data.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6418212,no
An application using micro TCA for real-time event assembly,"The Electronic Systems Engineering Department of the Computing Sector at the Fermi National Accelerator Laboratory has undertaken the effort of designing an AMC that meets the specifications within the MicroTCA framework. The application chosen to demonstrate the hardware is the real-time event assembly of data taken by a particle tracking pixel telescope. In the past, the telescope would push all of its data to a PC where the data was stored to disk. Then event assembly, geometry inference, and particle tracking were all done at a later time. This approach made it difficult to efficiently assess the quality of the data as it was being taken - at times, resulting in wasted test beam time. Now, we can insert in the data path, between the telescope and the PC, a commercial MicroTCA crate housing our AMC. The AMC receives, buffers, and processes the data from the tracking telescope and transmits complete, assembled events to the PC in real-time. In this paper, we report on the design approach and the results achieved when the MicroTCA hardware was employed for the first time during a test beam run at the Fermi Test Beam Facility in 2012.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6418371,no
Simplistic concept of low-cost virtual training environment capable of automatic user input data evaluation/validation,"Evaluation and validation user data during practical exams, trainings, interviews or another type of testing is tedious, error prone mission. The more difficult this mission becomes when there is more than one way for job-seeker or student to do the desired task. Setting up the environment consisting of multiple nodes on which the challenge should be accomplished is equally tedious also. This paper presents novel approach which makes this mission easier. We present an easy way how to setup environment based on open-source software which can be use in real world scenarios like training, testing or practicing.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6418617,no
Sleep in the cloud: On how to use available heart rate monitors to track sleep and improve quality of life,"As the modern society accumulates sleep debt that jeopardizes health, performance and wellbeing, people become increasingly interested in self-assessment. We aim to enable sleep self-evaluation using available Heart Rate (HR) monitors, mobile and cloud technology. Sleep was evaluated using a proprietary ECG-based validated sleep diagnostic software adapted to HR data obtained from HR monitor belts (HRMs) which are widely used to monitor HR during physical activity. Data were transmitted and stored on an iPhone, using a dedicated application. Two wireless communication channels are used for HRMs: (P1) Wearlink and (P2) ANT+. The stored information was uploaded to the cloud and automatically analyzed. The functionality of an automated sleep monitoring and analysis, using HRMs, with either P1 or P2 transmission, iPhone, and cloud based SleepRate software analysis has been checked. HR belts with ANT+ were the most suitable for recording HR during sleep. Millions of people own HRMs to assess their training. Now they can use the same device to evaluate and improve their sleep, thus improving their daytime physical and cognitive performance , wellbeing and overall health.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6420397,no
Quantitative 3D evaluation of myocardial perfusion during regadenoson stress using multidetector computed tomography,"We tested the hypothesis that quantitative 3D analysis of myocardial perfusion from MDCT images obtained during regadenoson stress would more accurately detect the presence of significant coronary artery disease (CAD) than the same analysis when performed on resting MDCT images. Fifty consecutive patients referred for CT coronary angiography (CTCA) underwent additional imaging with regadenoson (0.4mg, Astellas) using prospective gating (256-channel, Philips). Custom software was used to calculate for each myocardial segment an index of severity and extent of perfusion abnormality, Qh, which was compared to perfusion defects predicted by the presence and severity of coronary stenosis on CTCA. In segments supplied by arteries with luminal narrowing >;50%, myocardial attenuation was slightly reduced compared to normally perfused segments at rest (9121 vs. 9326 HU, NS), and to a larger extent at stress (10221 vs. 11220 HU, p<;0.05). In contrast, index Qh was significantly increased in these segments at rest (0.400.48 vs. 0.260.41, p<;0.05) and reached a nearly 3-fold difference at stress (0.660.74 vs. 0.280.51, p<;0.05). The addition of regadenoson improved the diagnosis of CAD, as reflected by an increase in sensitivity (from 0.57 to 0.91) and improvement in accuracy (0.65 to 0.77). In conclusion, quantitative 3D analysis of MDCT images allows objective detection of CAD, the accuracy of which is improved by regadenoson stress.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6420408,no
A management system for adult cardiac surgery,"A new system for the computerized management of the surgical path was developed by Tuscany Gabriele Monasterio Foundation at the Heart Hospital of """"G. Pasquinucci"""" Massa. The system has been in operating since 2009 and manages the paths of more than 2500 surgical patients / year in cardiology. The system was developed from the need to make more efficient and flexible operating room activities, which are related / linked to the waiting lists and the availability of medical resources (beds, staff, implantable devices, etc.). The surgical path is characterized by many professionals and clinical settings that make it difficult to maintain a timely and efficient global unity. In addition to this, to assess the quality of the hospital and take action for improvement, it is also necessary to extend the surgical path to the postoperative period.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6420442,no
On the use of failure detection and recovery mechanism for network failure,"Future of internet is predicted to be multi-interfaced. Site Multi-homing by IPv6 Intermediation-Shim6 is a proposal presented in IETF to provide multi-homing support in IPv6 based networks. Although initially it was intended for static networks but recently it has been tested to provide end host mobility. Failure detection and recovery in Shim6 is performed through REAchability (REAP) Protocol. This protocol shows significant improvements over MIPv6. Recently, due to inherited flaws in MIPv6, more protocols and combination of protocols are implemented and also been tested. In this contribution we implemented LinShim6 test bed and observed the behavior of switching locators under the use of REAP. This work is done by keeping in view that how the QoS factors are affected when REAP is used for network failure and recovery in home environment. REAP is activated when the communication path is failed. Sometimes host has access to multiple ISPs through its multiple locators. If one locator fails, communication is shifted to another locator. In Shim6 context it is called locator change. We experimentally validated the working of LinShim6 and condition of network considering packet loss, jitter, throughput and data transferred. In addition we propose that if an intelligent approach is used, switching delay can be reduced. We called this Shim6 Assisted Mobility Scheme (SAMS). Some initial experimental results are also presented in this work.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6421437,no
Microcontroller based automatic fault identification in a distribution system,A delta-star transformer connected to distribution system has been analyzed with the approach of generalized theory of electrical machines and expressions for symmetrical and different unsymmetrical faults are derived. The detailed theoretical analysis shows that magnitude and phase angle of fault current varies depending on nature and types of faults. A microcontroller based continuous monitoring unit is developed to detect and identify types of faults in the transformer connected distribution system. Software executed in microcontroller evaluates nature of a fault based on measured magnitudes and phase angles of voltages and currents under faulty condition. Theoretical analysis and experimental results validate acceptability of the developed unit..,2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6422249,no
Impact weighted uptime in hierarchical LTE networks: Application and measurement,"The success of 4G LTE networks will depend upon the quality of new real-time high bandwidth applications like streaming video, on-line gaming, and Telepresence. These applications are very sensitive to short duration outages (SDOs) of network elements. The classical availability metric is not adequate for 4G LTE network evaluation because it is not sensitive enough to SDOs. We introduce a new metric: the Impact Weighted Network Uptime (IWNU), whereby uptime at each hierarchical level is weighted by the respective number of base stations affected as a result of that level failure. We illustrate its usefulness for segments of LTE networks having up to three levels of hierarchy. The reliability model of each hierarchical level is described by an absorbing Markov chain whose absorption states correspond to failures affecting base stations. We use the level uptime to compare different redundancy configurations at upper hierarchical levels and provide numerical results demonstrating that additional redundancy may not visibly increase the level uptime in the presence of silent failures which are not immediately detected. We propose also a new method for tracking the field reliability of the 4G LTE service by utilizing existing software features and protocols readily available within Cisco network elements. Service impacting hardware and software outages are registered by this method with an accuracy of one second. The outage data is then used for calculation of the downtime and the proposed IWNU metric which are not currently automated by any tool.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6424057,no
Automated Visual Quality Analysis for Media Production,"Automatic quality control for audiovisual media is an important tool in the media production process. In this paper we present tools for assessing the quality of audiovisual content in order to decide about the reusability of archive content. We first discuss automatic detectors for the common impairments noise and grain, video breakups, sharpness, image dynamics and blocking. For the efficient viewing and verification of the automatic results by an operator, three approaches for user interfaces are presented. Finally, we discuss the integration of the tools into a service oriented architecture, focusing on the recent standardization efforts by EBU and AMWA's Joint Task Force on a Framework for Interoperability of Media Services in TV Production (FIMS).",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6424696,no
SAFER: System-level Architecture for Failure Evasion in Real-time Applications,"Recent trends towards increasing complexity in distributed embedded real-time systems pose challenges in designing and implementing a reliable system such as a self-driving car. The conventional way of improving reliability is to use redundant hardware to replicate the whole (sub)system. Although hardware replication has been widely deployed in hard real-time systems such as avionics, space shuttles and nuclear power plants, it is significantly less attractive to many applications because the amount of necessary hardware multiplies as the size of the system increases. The growing needs of flexible system design are also not consistent with hardware replication techniques. To address the needs of dependability through redundancy operating in real-time, we propose a layer called SAFER(System-level Architecture for Failure Evasion in Real-time applications) to incorporate configurable task-level fault-tolerance features to tolerate fail-stop processor and task failures for distributed embedded real-time systems. To detect such failures, SAFER monitors the health status and state information of each task and broadcasts the information. When a failure is detected using either time-based failure detection or event-based failure detection, SAFER reconfigures the system to retain the functionality of the whole system. We provide a formal analysis of the worst-case timing behaviors of SAFER features. We also describe the modeling of a system equipped with SAFER to analyze timing characteristics through a model-based design tool called SysWeaver. SAFER has been implemented on Ubuntu 10.04 LTS and deployed on Boss, an award-winning autonomous vehicle developed at Carnegie Mellon University. We show various measurements using simulation scenarios used during the 2007 DARPA Urban Challenge. Finally, we present a case study of failure recovery by SAFER when node failures are injected.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6424806,no
Model-Driven Comparison of State-Machine-Based and Deferred-Update Replication Schemes,"In this paper, we analyze and experimentally compare state-machine-based and deferred-update (or transactional) replication, both relying on atomic broadcast. We define a model that describes the upper and lower bounds on the execution of concurrent requests by a service replicated using either scheme. The model is parametrized by the degree of parallelism in either scheme, the number of processor cores, and the type of requests. We analytically compared both schemes and a non-replicated service, considering a bcast- and request-execution-dominant workloads. To evaluate transactional replication experimentally, we developed Paxos STM---a novel fault-tolerant distributed software transactional memory with programming constructs for transaction creation, abort, and retry. For state-machine-based replication, we used JPaxos. Both systems share the same implementat ion of atomic broadcast based on the Paxos algorithm. We present the results of performance evaluation of both replication schemes, and a non-replicated (thus prone to failures) service, considering various workloads. The key result of our theoretical and experimental work is that neither system is superior in all cases. We discuss these results in the paper.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6424844,no
FORTRESS: Adding Intrusion-Resilience to Primary-Backup Server Systems,"Primary-backup replication enables arbitrary services, which need not be built as deterministic state machines, to be reliable against server crashes. Further, when the primary does not crash, the performance can be close to that of an un-replicated, 1-server system and is arguably far better than what state machine replication can offer. These advantages have made primary-backup replication a widely used technique in commercial provisioning of services, even though the technique assumes that residual software bugs in a server system can lead only to crashes and cannot result in state corruption. This assumption cannot hold against an attacker intent on exploiting vulnerabilities and corrupting the service state when attacks lead to intrusions. This paper presents a system, called FORTRESS, which can encapsulate a primary-backup system and safeguard it from being intruded. At its core, FORTRESS applies proactive obfuscation techniques in a manner appropriate to primary-backup replication and deploys proxy servers for additional defence. Gain in intrusion resilience is shown to be substantial when assessed through analytical evaluations and simulations for a range of attacker scenarios. Further, by implementing two web-based applications, the average performance drop is demonstrated to be in the order of tens of milliseconds even when obfuscation intervals are as small as tens of seconds.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6424846,no
AAD: Adaptive Anomaly Detection System for Cloud Computing Infrastructures,"Cloud computing has become increasingly popular by obviating the need for users to own and maintain complex computing infrastructure. However, due to their inherent complexity and large scale, production cloud computing systems are prone to various runtime problems caused by hardware and software failures. Autonomic failure detection is a crucial technique for understanding emergent, cloudwide phenomena and self-managing cloud resources for system-level dependability assurance. To detect failures, we need to monitor the cloud execution and collect runtime performance data. These data are usually unlabeled, and thus a prior failure history is not always available in production clouds, especially for newly managed or deployed systems. In this paper, we present an Adaptive Anomaly Detection (AAD) framework for cloud dependability assurance. It employs data description using hypersphere for adaptive failure detection. Based on the cloud performance data, AAD detects possible failures, which are verified by the cloud operators. They are confirmed as either true failures with failure types or normal states. The algorithm adapts itself by recursively learning from these newly verified detection results to refine future detections. Meanwhile, it exploits the observed but undetected failure records reported by the cloud operators to identify new types of failures. We have implemented a prototype of the algorithm and conducted experiments in an on-campus cloud computing environment. Our experimental results show that AAD can achieve more efficient and accurate failure detection than other existing scheme.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6424881,no
On atomicity enforcement in concurrent software via Discrete Event Systems theory,"Atomicity violations are among the most severe and prevalent defects in concurrent software. Numerous algorithms and tools have been developed to detect atomicity bugs, but few solutions exist to automatically fix such bugs. Some existing solutions add locks to enforce atomicity, which can introduce deadlocks into programs. Our recent work avoids deadlock bugs in concurrent programs by adding control logic synthesized using Discrete Event Systems theory. In this paper, we extend this control framework to address single-variable atomicity violation bugs. We use the same class of Petri net models as in our prior work to capture program semantics, and handle atomicity violations by control specifications in the form of linear inequalities. We propose two methodologies for synthesizing control logic that enforces these linear inequalities without causing deadlocks; the resulting control logic is embedded into the program's source code by program instrumentation. These results extend the scope of concurrency bugs in software systems that can be handled by techniques from control engineering. Case studies involving two real Java programs demonstrate our solution procedure.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6426112,no
Checking consistency between documents of requirements engineering phase,"During the initial software specification phase, requirement document, use cases description and interface prototypes can be generated as a way to aid in the construction of system data. The consistency among these documents is a quality attribute which must be emphasized at this phase of the software development process. The QualiCES method is presented herein; it allows assessing the consistency among these software documents, and is supported by a checklist and by a consistency metrics developed to this end. As benefits, there is defect detection and a software quality warranty from the beginning of software development. The method was executed in a case study. Based on the results, the viability for applying the method can be verified, as well as the proposal innovation degree.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6427206,no
Mining crosscutting concerns with ComSCId: A rule-based customizable mining tool,"One of the first steps when reengineering legacy systems into aspect-oriented ones is to identify the crosscutting concerns (CCC) presented in the architecture of the former; a process known as aspect mining. However, this is a time- consuming and error-prone task when conducted manually. In this paper, we present a customizable mining tool, called ComSCId, which searches for the CCC in legacy Java systems in an automatic way. ComSCId owns a repository which stores all the rules used as base for the mining process. In this repository there are pre-defined rules for some ordinary CCC like persistence, buffering and logging. Moreover, the main characteristic of this repository is its flexibility, since it allows adding new rules or customizing the existing ones to specific contexts or domains. We conducted two studies to evaluate ComSCId and we have observed high percentages of identification coverage when using this tool in an incremental way.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6427207,no
An assessment of security requirements compliance of cloud providers,"Cloud provider assessment is important for cloud consumers to determine, when outsourcing computing work, which providers can serve their business and system requirements. This paper presents an initial attempt to assess security requirements compliance of cloud providers by following the Goal Question Metric approach and defining a weighted scoring model for the assessment. The security goals and questions that address the goals are taken from Cloud Security Alliance's Cloud Controls Matrix and Consensus Assessments Initiative Questionnaire. We then transform such questions into more detailed ones and define metrics that help provide quantitative answers to the transformed questions based on evidence of security compliance provided by the cloud providers. The scoring is weighted by quality of evidence, i.e. its compliance with the associated questions and its completeness. We propose a scoring system architecture which utilizes CloudAudit and assess Amazon Web Services as an example.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6427484,no
SLA-driven capacity planning for Cloud applications,"Cloud computing paradigm has become the solution to provide good service quality and exploit economies of scale. However, the management of such elastic resources, with different Quality-of-Service (QoS) combined with on-demand self-service, is a complex issue. This paper proposes an approach driven by Service Level Agreement (SLA) for optimizing the capacity planning for Cloud applications. The main challenge for a service provider is to determine the best trade-off between profit and customer satisfaction. In order to address this issue, we follow a queueing network proposal and present an analytical performance model to predict Cloud service performance. Based on a utility function and a capacity planning method, our solution calculates the optimal configuration of a Cloud application. We rely on autonomic computing to adjust continuously the configuration. Simulation experiments indicate that our model i) faithfully captures the performance of Cloud applications for a number of workloads and configurations and ii) successfully keeps the best trade-off.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6427519,no
CloudGuide: Helping users estimate cloud deployment cost and performance for legacy web applications,"With cloud business growing, many companies are joining the market as cloud service providers. Most providers offer similar services with slightly different pricing models, and performance data remains scarce. This leaves cloud users with the puzzle of guessing what costs they will need to pay to run their legacy applications in a cloud environment. Cloud Guide is a tool suite that provides users with an estimated cost of running a legacy application on various cloud providers based on specific performance requirements. CloudGuide predicts the cloud computing resources required by a targeted application based on a queuing model and estimates the deployment cost for the application. CloudGuide allows users to explore cloud configurations that meet different performance requirements and cost constraints, and can be used to find new configuration when workload changes. The experiments presented in this study evaluated a multi-tiered network application and showed that CloudGuide can choose high-quality cloud configuration and can be used to assist system administrators with dynamic provisioning decisions.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6427577,no
Classification of color objects like fruits using probability density function (PDF),"Fruits like apples are valued based on their appearance (i.e. color, sizes, shapes, presence of surface defects) and hence classified into different grades. Grading process helps in achieving better standards and quality of fruits. Of the many available color models, HSI model provides a highly effective color evaluation particularly for analyzing biological products. Human assessment furnishes only qualitative data and such inspection is time consuming and cost-intensive. Machine vision systems with specialized image processing software provide a solution that may satisfy the demand. The analysis was carried out on images of 187 apple fruits, shows that classification done based on median of PDF. In order to avoid the mismatch in grading the same it has been classified further using Histogram Intersection, which determines the closeness between two images i.e. 1 if two images are similar and 0 if they are dissimilar.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6428746,no
Design and Realization of Timely Auto-Detection Based on High-precision Photoelectric Encoder,"A high-speed processing circuit based on DSP is designed to meet the increasing demand for the reliability of the high-precision encoder, and all analog signals are acquired by AD converter. The processing circuit can timely online automatically detect, diagnose and repair the fault point in time. The time of diagnosis and maintenance is highly reduced while the reliability is enhanced. The high-precision, high-intelligence and high-reliability photoelectric encoder is achieved by this method in practical application and the expected result is obtained.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6428873,no
RC-Finder: Redundancy Detection for Large Scale Source Code,"Redundant code not only causes noise in code debugging which confuses developers, but also correlates with the presence of traditional severe software errors. RC-Finder, a redundancy detection system for large-scale code is proposed to detect six kinds of redundancy. This paper analyzes each kind of redundant code, and provides the detailed algorithm respectively. The experiments on large scale open source software systems show that RC-Finder can find redundant code efficiently. With RC-Finder, it is very convenient for developers to detect and correct these kinds of defects, and thereby to further guarantee the software quality.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6428895,no
Detecting Bad Smells with Weight Based Distance Metrics Theory,"Detecting bad smells in program design and implementation is a challenging task. Manual detection is proved to be time-consuming and inaccurate under complex situation. Weight based distance metrics and relevant conceptions are introduced in this paper, and the automatic approach for bad smells detection is proposed based on Jaccard distance. The conception of distance between entities and classes is defined and relevant computing formulas are applied in detecting. New weight based distance metrics theory is proposed to detect feature envy bad smell. This improved approach can express more detailed design quality and invoking relationship than the original distance metrics theory. With these improvements the automation of bad smells detection can be achieved with high accuracy. And then the approach is applied to detect bad smells in JFreeChart open source code. The experimental results show that the weight based distance metrics theory can detect the bad smell more accurately with low time complexity.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6428907,no
Optimized Collaborative Filtering Algorithm Based on Item Rating Prediction,"Collaborative filtering recommendation algorithm is currently the most widely used personalized recommendation algorithm. Sparsity problem of user rating data led to the recommendation quality of traditional collaborative filtering algorithms are far from ideal. To solve the problem, the paper first cloud model and project characteristic attributes to calculate the similarity between the project has taken into consideration in computing project similarity scores were similar between the project and consider the project between the characteristic attribute similarity, and then to predict ungraded items rated. Finally, the cloud model to calculate the similarity between users to obtain the target user's nearest neighbor. Experimental results show that the algorithm improves the accuracy of the similarity of the calculated project, and effectively solve the problem of data sparsity, and improve the quality of the recommendation system recommended.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6428992,no
Spectrum-based fault diagnosis for service-oriented software systems,"Due to the loosely coupled and highly dynamic nature of service-oriented systems, the actual configuration of such system only fully materializes at runtime, rendering many of the traditional quality assurance approaches useless. In order to enable service systems to recover from and adapt to runtime failures, an important step is to detect failures and diagnose problematic services automatically. This paper presents a lightweight, fully automated, spectrum-based diagnosis technique for service-oriented software systems that is combined with a framework-based online monitor. An experiment with a case system is set up to validate the feasibility of pinpointing problematic service operations. The results indicate that this approach is able to identify problematic service operations correctly in 72% of the cases.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6449440,no
Revenue maximization with quality assurance for composite web services,"Service composition is one of the major approaches in service oriented architecture (SOA) based systems. Due to the inherent stochastic nature of services execution environment the issue of composite service quality assurance within SOA is a very challenging one. Such heterogeneous environment requires dynamic, run-time composition of services. We show how to determine a policy that satisfies the quality assurance for the composite service provider with the aim of revenue maximization for this provider. The quality assurance is defined as the probability that end-to-end deadline will be met, while taking into account service availability, (composite) service response-time and costs. The calculated policy is determined using dynamic programming and allows fast decision making for run-time composition. Besides, we determine the end-to-end response-time distributions resulting from determined policies. We illustrate the proposed solution with a number of experiments.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6449452,no
BFT-r: A proactive Byzantine Fault-Tolerant agreement with rotating coordinator and mutable blacklist mechanism,"With the advent of replication-based approach for a distributed environment, a major coordination problem i.e., Consensus can be solved in the presence of some malicious replicas. Therefore, we attempt to design an agreement algorithm with proactive detection of such malicious replicas. The paper presents an algorithm BFT-r i.e., Byzantine Fault Tolerance with rotating coordinator. The basic idea is to rotate the role of the primary coordinator among all the participating replicas. Undoubtedly, the assignment of each participating replica to be primary increases the possibility of a faulty replica to be selected as primary. Therefore, in order to avoid such problem, our protocol runs a mutable blacklist mechanism in which an array of previously detected faulty replicas is maintained and propagated among the different nodes so as to avoid the decision from a faulty replica. The mutable blacklist mechanism is in line with the proactive nature of the proposed protocol. The necessary correctness proof has also been presented along with the simulation analysis. The protocol is robust and exhibits better efficiency for long-lived applications/systems.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6449783,no
Using host criticalities for fault tolerance in mobile agent systems,"Monitoring is a crucial factor for smooth run of distributed systems such as mobile agent based system. Various activities in such systems require monitoring such as performance analysis and tuning, scheduling strategies and fault detection. In this paper we present monitoring and fault tolerance technique for mobile agent based systems. We present mobile agent based fault prevention and detection technique where the team of mobile agents monitor each host in mobile agent based system. This research focuses on building an automatic, adaptive and predictive determining policy where critical host agents are identified in advance by monitoring agents, to avoid their failures. The novelty of proposed approach is constant collection and updating of local as well as global information of the system. This policy is determined by calculating weights; taking into account the criticality of the hosts by their monitoring agents which keep updating the weights of hosts. These weights act are used for decision making of checkpointing. These monitoring mobile agents act together to detect undesirable behaviors and also provide support for restoring the system back to normalcy. We also report on the result of reliability and performance issue of our proposed approach.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6449793,no
Application of Custom Power Park to improve power quality of sensitive loads,"This paper presents the performance of custom power devices in a Custom Power Park (CPP) by series and shunt compensation in a distribution supply system. A static transfer switch (STS) provides alternate feeder supply to park loads where shunt active power filter (SAPF), and a dynamic voltage restorer (DVR) are employed to compensate current harmonics, voltage sag, and voltage interruptions. A novel voltage detection controller based on single-phase synchronous d-q reference frame is developed which can detect single or three-phase balanced/unbalanced voltage imperfections. Extensive simulation studies are carried out by using PSCAD/EMTDC software and the ability of CPP to provide a group of customers with premium quality of power is analyzed.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6450429,no
Fault tolerant gripper in robotics,"The work presented in this article shows, through a case study, the different stages of designing a fault tolerant system; a system which can tolerate faults without losing its operational capability. This system includes a gripper mounted on the wrist of a robot manipulator. First the faults were identified, then the residual generation stage was designed. Analyzing this residual allows us to show that artificially injected faults can be detected and an alarm is generated for an emergency stop command. With regards to the gripper, we have only dealt with the faults we were able to solve thanks to the accommodation stage which must control both hardware and software reconfigurations. The considered faults originated in the electronic interfaces and for an efficient supervision we have designed the whole gripper control. The electronic interfaces concerned by fault occurrence are duplicated to ensure robot task continuity.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6450997,no
A Cloud Service Design for Quality Evaluation of Design Patterns,"In recent years, the influences of design patterns on software quality have attracted increasing attention in the area of software engineering, as design patterns encapsulate valuable knowledge to resolve design problems, and more importantly to improve the design quality. Our previous research has proposed an effectiveness evaluation method to assess the design pattern's quality. the proposed method performs experiments in open source projects. as the version of open source project grows, the analysis time increases in pace with the increasing source code size. the computation performance cannot be applicable to the actual real time data analysis. in this research, we develop an effectiveness evaluation cloud service based on powerful computing capability to process a large amount of data. We collect design pattern's applications in open source projects, and provide programmers valuable and real-time analysis information to help them inspect the value of deployed design patterns.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6457255,no
FPGA based RNG for random WOB method in unit cube capacitance calculation,"Monte Carlo (MC) method is widely used in resolving mathematical problems that are too complicated to solve analytically. The method involves with sampling process of the random numbers and probability to estimate the result. As MC method depending on a large number of good quality random numbers to produce a high accuracy result, developing a good random number generator (RNG) is vital. Most of random number generators are developed in software based, with the improvement of Field Programmable Gate Arrays (FPGA) density and speed in recent days, implementing random number generator (RNG) directly into hardware is feasible. Random Walk on the Boundary (WOB) is one of MC methods that applied to calculate the unit cube capacitance. The unique requirement of this method is the random numbers produced by RNG must fall within the Gaussian distribution and the maximum decimal values in the range of [0, 1]. Thus, in this paper we presented a novel hardware RNG for Random WOB method to calculate unit cube capacitance in FPGA. The RNG is implemented in floating-point base, using the combination of Cellular Automata Shift Register (CASR) and Linear Feedback Shift Register (LFSR) then channeled through Box-Muller transformation. There is linear approximation in computing logarithmic function applied in Box-Muller transformation. Based on statistical tests, the random numbers generated is nearly 97.5% resembling the standard normal distribution.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6457622,no
Assessing and improving software quality in safety critical systems by the application of a SOFTWARE TEST MATURITY MODEL,"Drawing on wide experience over many years, BitWise has evolved a SOFTWARE TEST MATURITY MODEL. This is of particular value in the testing of safety critical software. This brings significant benefits in terms of cost and effective quality. This paper explains the Model and enables development groups to assess their current capabilities and plan any required improvements.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6458950,no
"Remote prognosis, diagnosis and maintenance for automotive architecture based on least squares support vector machine and multiple classifiers","Software issues related to automotive controls account for an increasingly large percentage of the overall vehicles recalled. To alleviate this problem, vehicle diagnosis and maintenance systems are increasingly being performed remotely, that is while the vehicle is being driven without need for factory recall and there is strong consumer interest in Remote Diagnosis and Maintenance (RD&M) systems. Such systems are developed with different building blocks/elements and various capabilities. This paper presents a novel automotive RD&M system and prognosis architecture. The elements of the proposed system include vehicles, smart phones, maintenance service centers, vehicle manufacturer, RD&M experts, RD&M service centers, logistics carry centers, and emergency centers. The system promotes the role of smart phones used to run prognosis and diagnosis tools based on Least Squares Support Vector Machine (LS-SVM) multiple classifiers. During the prognosis phase, the smart phone stores the history of any forecast failures and send them, only if any failure already occurred during the diagnosis, to the RD&M service centre. The later will then forward it to RD&M experts as a real failure data to improve the training data used in prognosis classification and predication of the remaining useful life (RUL). The (LS-SVM) is used widely in prognostics and system health management of spacecraft in-orbit and it is applied to monitor spacecraft's performance, detect faults, identify the root cause of the fault, and predict RUL. The same approach is applied in this paper. Finally, the RD&M software architectures for the vehicle and the smart phone are presented.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6459652,no
Application stress testing Achieving cyber security by testing cyber attacks,"Application stress testing applies the concept of computer network penetration testing to software applications. Since software applications may be attacked - from inside or outside a protected network boundary - they are threatened by actions and conditions which cause delays, disruptions, or failures. Stress testing exposes software systems to simulated cyber attacks, revealing potential weaknesses and vulnerabilities in their implementation. By using such testing, these internal weaknesses and vulnerabilities can be discovered earlier in the software development life cycle, corrected prior to deployment, and lead to improved software quality. Application stress testing is a process and software prototype for verifying the quality of software applications under severe operating conditions. Since stress testing is rarely - if at all - performed today, the possibility of deploying critical software systems that have been stress tested provides a much stronger indication of their ability to withstand cyber attacks. Many possible attack vectors against critical software can be verified as true threats and mitigated prior to deployment. This improves software quality and serves as a tremendous risk reduction for critical software systems used in government and commercial enterprises. The software prototype models and verifies failure conditions of a system under test (SUT). The SUT is first executed in a virtual environment and its normal operational modes are observed. A normal behavior model is generated in order to predict failure conditions based on attack models and external SUT interfaces. Using off-the-shelf software tools, the predictions are verified in the virtual environment by stressing the executing SUT with attacks against the SUT. Results are presented to testers and system developers for dispensation or mitigation.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6459909,no
Learning features for predicting OCR accuracy,"In this paper, we present a new method for assessing the quality of degraded document images using unsupervised feature learning. The goal is to build a computational model to automatically predict OCR accuracy of a degraded document image without a reference image. Current approaches for this problem typically rely on hand-crafted features whose design is based on heuristic rules that may not be generalizable. In contrast, we explore an unsupervised feature learning framework to learn effective and efficient features for predicting OCR accuracy. Our experimental results, on a set of historic newspaper images, show that the proposed method outperforms a baseline method which combines features from previous works.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6460846,no
Teaching software inspection effectiveness: An active learning exercise,"This paper discusses a novel active learning exercise which teaches students how to perform and assess the effectiveness of formal software inspections. In this exercise, students are responsible for selecting an artifact from their senior capstone design projects. The students then use fault injection to strategically place faults within the artifact that should be caught by the inspection exercise. Based on the needs of the team, students prepare an inspection packet consisting of a set of inspection instructions, applicable checklists, and the inspection artifact. Students then hire a set of inspectors based on classmates' backgrounds and experiences. The team leader then holds two inspection meetings and reports the results. The results are then used to assess the effectiveness of the inspection. Overall, in analyzing 5 years worth of data from this exercise, it is found that students are capable of selecting appropriate materials for inspection and performing appropriate software inspections. The yield of students is lower than an experienced professional might have and the inspection rates tend to be slightly higher than desired for their experience. However, the yield is related to individual preparation time. Students overall find this to be a highly educational experience and highly recommend it be continued for future classes.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6462206,no
Work in progress: Engaging faculty for program improvement via EvalTools: A new software model,"In this paper we present our experiences with a new software-tool model for program assessment and evaluation, and engineering programs improvement in assessing learning effectiveness. In the past, the assessment plan was based on the typical practice of assigning a person in the department for overseeing the whole process. The non-collaborative evaluation that is managed by a single program leader is not effective and problematic. To involve more faculty and in an effort to prepare for the most recent ABET visit, we decided to adopt EvalTools in fall of 2010. EvalTools is designed and developed according to ABET standards to provide a mechanism for collecting and analyzing data about the program, students' performance and their learning achievements. In addition, EvalTools is instrumental in providing a mechanism to simplify the process of inspecting the assessment results as well as identifies strengths and shortcomings of the program before ABET review. More importantly getting faculty members excited about results and involved in the process of program improvement is a major accomplishment. Our experience via first-time implementation of EvalTools shows very useful results for this model that can be easily disseminated for various programs in various disciplines. In this paper we will show: process of best use of relevant features in aid of streamlining faculty's time in data collection as well as evaluation was achieved; our results and how we succeeded in improving our program quality in an effective, efficient and systematic way; that simple curriculum revisions for multiple programs as a result of using EvalTools for programs under going ABET is possible; that capturing the process of effective trainings needed for faculty and staff in a simple manner; faculty's experience in a constructive and engaging manner.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6462443,no
Integration of Safety Verification with Conformance Testing in Real-Time Reactive System,"In the paper, we propose a method that can be applied to verify implementation in real-time reactive system. Different from other software model checking approaches, our method is based on testing. This approach allows the verification of safety property to be conducted directly on real code instead of models extracted from final implementation. Verifying that kind of models is a hard work and can only be applied to parts of the implementation. The method is done by establishing a connection between safety verification and conformance testing in real-time system. We first prove a theorem that in real-time system, under the input enabled precondition, if an implementation conforms to its specification and the specification satisfies the safety properties, the implementation satisfies it either. Then, based on contropositivity of the former conclusion, we present a test case generation framework which forms basis for generating test cases that can be used to detect violations of safety properties in the implementation. In addition, this test generation framework can also detect more nonconformance defects when compared with other real time test generation methods. The method is illustrated with a train gate control system.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6462633,no
A Preference and Honesty Aware Trust Model for Web Services,"Trust is one of the most critical factors for a service requestor when selecting the best one from a large pool of services. However, existing web service trust models either do not focus on satisfying customer's preference for different quality of service (QoS) attributes, or do not pay enough attention to the impact of malicious ratings on trust evaluation. To address these gaps, a dynamic trust evaluation model considering customer's preference and false ratings is proposed in this paper. The model introduces an approach automatically mining customer's preference from their requirements. The preference is used to determine the weights on each QoS attribute when integrating trust of the multi-dimensional QoS attributes. The local trust of a service for the customer is derived by combining trust of QoS attributes and customer's ratings. Then, the customers are divided into different groups according to their preferences, and the honesty of each group is assessed by filtering out dishonest customers based on a hybrid approach combining rating consistency clustering and average method. Finally, the weight on ratings is dynamically adjusted according to the results of honesty assessment when calculating the global trustworthiness of a service for the user group. The simulation results indicate that the model works well on personalized evaluation of trust, and it can effectively dilute the influence of malicious ratings.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6462638,no
Software-Based Online Monitoring of Cache Contents on Platforms without Coherence Fabric,"In favor of smaller chip areas and associated fabrication costs, designers of embedded multi-core systems on occasion decide not to include cache coherence logic in the hardware design. However, handling all cache coherence exclusively in software is error-prone, and there are presently no tools supporting developers in this task. Thus, we propose a new software testing method, based on online inspection of the cache contents, to pinpoint programming mistakes related to cache handling. This concept helps localizing the causing data symbol even for complicated cache handling errors, e.g. where the causing and manifesting code-location of an error differ. Our solution is a pure software solution and does not require any specialized hardware. We evaluate our approach by using it in a large application, and show that we can detect typical cache-related errors.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6462654,no
An Empirical Study on Improving Severity Prediction of Defect Reports Using Feature Selection,"In software maintenance, severity prediction on defect reports is an emerging issue obtaining research attention due to the considerable triaging cost. In the past research work, several text mining approaches have been proposed to predict the severity using advanced learning models. Although these approaches demonstrate the effectiveness of predicting the severity, they do not discuss the problem of how to find the indicators in good quality. In this paper, we discuss whether feature selection can benefit the severity prediction task with three commonly used feature selection schemes, Information Gain, Chi-Square, and Correlation Coefficient, based on the Multinomial Naive Bayes classification approach. We have conducted empirical experiments with four open-source components from Eclipse and Mozilla. The experimental results show that these three feature selection schemes can further improve the predication performance in over half the cases.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6462659,no
Linking Functions and Quality Attributes for Software Evolution,"Software quality properties, normally derived from non-functional requirements, are becoming more important for software. A main reason for software evolution is the unsatisfaction to software quality properties. When improving these properties through software evolution, it is essential to know whether software functions are affected and by how much. This paper proposes an approach to linking the functions with the quality properties of software for evolution via software architecture styles, aiming at contributing to (1) predicting evolution efforts and (2) transforming software for improving its quality.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6462660,no
Validating the Effectiveness of Object-Oriented Metrics over Multiple Releases for Predicting Fault Proneness,"In this paper, we empirically investigate the re-lationship of existing class level object-oriented metrics with fault proneness over the multiple releases of the software. Here we first, evaluate each metric for their potential to predict faults independently by performing univariate logistic regression analysis. Next, we perform cross-correlation analysis between the significant metrics to find the subset of these metrics for an improved performance. The obtained metrics subset was then used to predict faults over the subsequent releases of the same project datasets. In this study, we used five publicly available project datasets over their multiple successive releases. Our results reported that the identified subset metrics demonstrated an improved fault prediction with higher accuracy and reduced misclassification errors.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6462679,no
An Empirical Analysis of the Impact of Comment Statements on Fault-Proneness of Small-Size Module,"Code size metrics are commonly useful in predicting fault-prone modules, and the larger module tends to be more faulty. In other words, small-size modules are considered to have lower risks of fault. However, since the majority of modules in a software are often small-size, many ``small but faulty'' modules have been found in the real world. Hence, another fault-prone module prediction method, intended for small-size module, is also required. Such a new method for small-size module should use metrics other than code size since all modules are small size. This paper focuses on ``comments'' written in the source code from a novel perspective of size-independent metrics, comments have not been drawn much attention in the field of fault-prone module prediction. The empirical study collects 11,512 small-size modules, whose LOC are less than the median, from three major open source software, and analyzes the relationship between the lines of comments and the fault-proneness in the set of small-size modules. The empirical results show the followings: 1) A module in which some comments are written is more likely to be faulty than non-commented ones, the fault rate of commented modules is about 1.8-3.5 times higher than that of non-commented ones. 2) Writing one to four lines of comments would be thresholds of the above tendency.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6462681,no
An Approach to Estimating Cost of Running Cloud Applications Based on AWS,"Estimating the cost is important for cloud application developers to services in clouds, and becomes even important when it needs remaining a certain service level at the same time. Though currently much work has been down to predict cost and performance of cloud applications, most of them perform either before application design or after the application construction, which leads to either imprecise estimation or irreparable design fault. In this paper, we propose an approach to estimate the cost of running typical applications in Amazon Web Service (AWS) cloud during design phase. We propose an UML Activity-extended model (AeModel) to describe execution of application service and introduce an extraction algorithm to extract information contained in the AeModel automatically. We propose a cost model on AWS, which can help developers to estimate operating cost during design phase and satisfy performance needs, with an algorithm to produce suitable purchase solutions automatically. We perform case studies using a web-based business application to show effectiveness of our approach, and find that our approach can help developers lessen cost by adjusting application models.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6462712,no
A Guided Mashup Framework for Rapid Software Analysis Service Composition,"Historical data about software projects is stored in repositories such as version control, bug tracking and mailing lists. Analyzing such data is vital to discover unthought-of-yet-interesting insights of a software project. Even though a wide range of software analysis techniques are already available, integration of such analyses is yet to be systematically addressed. Inspired from the recently introduced concept of Software as a Service, our research group investigated the concept of Software Analysis as a Service (SOFAS), a distributed and collaborative software analysis platform. SOFAS allows software analyses to be accessed, composed into workflows, and executed over the Internet. However, traditional service composition is a complex, time consuming and error-prone process, which requires experts in both composition languages and existing standards. In this paper, we propose a mashup platform to address the problem of software analysis composition in a light-weight, programming-free process-centric way. Our proposed mashup platform provides design-time guidance to the users throughout the mashup design by integrating a continuous feedback mechanism. It requires exploiting semantic web technologies and Software Engineering Ontologies (SEON).",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6462735,no
Assessing Platform Suitability for Achieving Quality in Guest Applications,"Selecting a computing platform, such as private cloud or stand-alone virtualization based, is arguably a very critical task in an enterprise. It impacts several aspects of software systems -- from architecture to post-deployment support and operations. Emergence of various virtualization and cloud based platforms has added to the complexity of the said assessment and selection process. The main reason for this complexity is that each such platform possesses unique characteristics, and each such characteristic impacts Quality Attributes (QA) achievable by the guest applications. A novel method is presented to perform assessment of platforms on QA criteria. This method makes use of fuzzy sets techniques for performing multi-criteria evaluation of platforms. Taking a set of platforms and QA criteria as inputs, this method produces an ordered ranking of platforms. This output can be used in architecture design activities. Efficacy of the proposed approach has been demonstrated by assessing several variants of virtualization and cloud based platforms on a set of QA criteria.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6462742,no
A Heuristic Rule Reduction Approach to Software Fault-proneness Prediction,"Background: Association rules are more comprehensive and understandable than fault-prone module predictors (such as logistic regression model, random forest and support vector machine). One of the challenges is that there are usually too many similar rules to be extracted by the rule mining. Aim: This paper proposes a rule reduction technique that can eliminate complex (long) and/or similar rules without sacrificing the prediction performance as much as possible. Method: The notion of the method is to removing long and similar rules unless their confidence level as a heuristic is high enough than shorter rules. For example, it starts with selecting rules with shortest length (length=1), and then it continues through the 2nd shortest rules selection (length=2) based on the current confidence level, this process is repeated on the selection for longer rules until no rules are worth included. Result: An empirical experiment has been conducted with the Mylyn and Eclipse PDE datasets. The result of the Mylyn dataset showed the proposed method was able to reduce the number of rules from 1347 down to 13, while the delta of the prediction performance was only. 015 (from. 757 down to. 742) in terms of the F1 prediction criteria. In the experiment with Eclipsed PDE dataset, the proposed method reduced the number of rules from 398 to 12, while the prediction performance even improved (from. 426 to. 441.) Conclusion: The novel technique introduced resolves the rule explosion problem in association rule mining for software proneness prediction, which is significant and provides better understanding of the causes of faulty modules.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6462753,no
Quality-Aware Academic Research Tool Development,"Many organizations have adopted several different kinds of commercial software tools for the purpose of developing quality software, reducing time-to-market, and automating labor intensive and error-prone tasks. Academic researchers have also developed various types of tools, primarily as a means toward providing a prototype reference implementation that corresponds to some new research concept. In addition, academic researchers also use the tool building task itself as a mechanism for students to learn and practice various software engineering principles (e.g., requirements management, design, implementation, testing, configuration management, and release management) from building the tools. Although some academic tools have been developed with observance of sound software engineering practices, most academic research tool development still remains an ad hoc process because tools tend to be developed quickly and without much consideration for quality. In this paper, we present several quality factors to be considered when developing software tools for academic research purposes. We also present a survey of tools that have been presented at major conferences to examine the status quo of academic research tool development in terms of these factors.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6462783,no
Generation of Character Test Input Data Using GA for Functional Testing,"Typically, test input data is manually created for the functional testing of software applications hence is a time consuming and error prone activity. In this paper, we present an approach to generate test input data from structured requirements specifications models such as UML use case activity diagrams (UCADs). We propose Constraint Representation Syntax (CRS) for framing software attribute properties as a part of structuring the Software Requirements Specifications (SRS). Then, structured models are parsed into a set of functional paths along with their predicates containing attribute constraints. Genetic algorithm is used to generate test input data that satisfy these predicates. Based on our approach a prototype tool has been developed and a case study results are evaluated.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6462786,no
An Interval-Based Model for Detecting Software Defect Using Alias Analysis,"Alias analysis is a branch of static program analysis aiming at computing variables which are alias of each other. It is a basis research for many analyses and optimizations in software engineering and compiler construction. Precise modeling of alias analysis is fundamental for software analysis. This paper presents two practical approximation models for representing and computing alias: memory-sensitive model (MSM) and value-sensitive model (VSM). Based on defect-oriented detecting, we present a method to detect software defect using VSM and MSM, which realizes inter-procedure detecting by procedure summary. According to whether type of analysis object coming from defect is value-sensitive or memory-sensitive, we propose two detecting algorithms based on two alias models respectively. One is for memory leak (ML) based on MSM, and the other is for invalid arithmetic operation (IAO) based on VSM. We apply a defect testing system (DTS) to detect six C++ open source projects for proving our models effectiveness. Experimental results show that applying our technique to detect IAO and ML defect can improve detecting efficiency, at the same time reduce potential false positives and false negatives.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6462792,no
A Two-Level Prioritization Approach for Regression Testing of Web Applications,A test case prioritization technique reschedules test cases for regression testing in an order to achieve specific goals like early fault detection. We propose a new two level prioritization approach to prioritize test cases for web applications as a whole. Our approach automatically selects modified functionalities in a web application and executes test cases on the basis of the impact of modified functionalities. We suggest several new prioritization strategies for web applications and examine whether these prioritization strategies improve the rate of fault detection for web applications. We propose a new automated test suite prioritization model for web applications that selects test cases related to modified functionalities and reschedules them using our new prioritization strategies to detect faults early in test suite execution.,2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6462796,no
"E-Net, system for energy management","Energy management software E-Net, developed by Quartz Matrix, is a powerful tool for evaluating energy efficiency and discovering consumption reduction opportunities. Based on a modular architecture, service oriented, E-Net software offers users a powerful environment for generating consumption forecasting, tracking budgets and assessing quality energy distribution. At the same time it is a technical, commercial and managerial tool.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6463586,no
Faults detection on a wound rotor induction machines by principal components analysis,"This paper deals with faults detection and localization of wound rotor induction machines based on principal components analysis method. Both, localization and detection approaches consist in analyzing a detection index which is established on principal components. Once the faults are detected, the affected state variables are localized. The EWMA filter is applied to improve the fault detection quality by reducing the rate of false alarms. An accurate analytical modeling of the wound rotor induction machines is proposed and implemented on the software Matlab to obtain the state variables data of both healthy and faulted machines. Several simulation results are presented and analyzed.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6463819,no
Cognitive behavior analysis framework for fault prediction in cloud computing,"Complex computing systems, including clusters, grids, clouds and skies, are becoming the fundamental tools of green and sustainable ecosystems of future. However, they can also pose critical bottlenecks and ignite disasters. The complexity and high number of variables could easily go beyond the capacity of any analyst or traditional operational research paradigm. In this work, we introduce a multi-paradigm, multi-layer and multi-level behavior analysis framework which can adapt to the behavior of a target complex system. It not only learns and detects normal and abnormal behaviors, it could also suggest cognitive responses in order to increase the system resilience and its grade. The multi-paradigm nature of the framework provides a robust redundancy in order to cross-cover possible hidden aspects of each paradigm. After providing the high-level design of the framework, three different paradigms are discussed. We consider the following three paradigms: Probabilistic Behavior Analysis, Simulated Probabilistic Behavior Analysis, and Behavior-Time Profile Modeling and Analysis. To be more precise and because of paper limitations, we focus on the fault prediction in the paper as a specific event-based abnormal behavior. We consider both spontaneous and gradual failure events. The promising potential of the framework has been demonstrated using simple examples and topologies. The framework can provide an intelligent approach to balance between green and high probability of completion (or high probability of availability) aspects in computing systems.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6464008,no
Tutorial on building M&S software based on reuse,"The development of software for modeling and simulation is still a common step in the course of projects. Thereby any software development is error prone and expensive and it is very likely that the software produced contains flaws. This tutorial will show which techniques are needed in modeling and simulation software independent from application domains and model description means and how reuse and the use of state of the art tools can improve the software production process. The tutorial is based on our experiences made on developing and using JAMES II, a flexible framework created for building specialized M&S software products, for research on modeling and simulation, and for applying modeling and simulation.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6465306,no
ARMISCOM: Autonomic reflective middleware for management service composition,"In services composition the failure of a single service generates an error propagation in the others services involved, and therefore the failure of the system. Such failures often cannot be detected and corrected locally (single service), so it is necessary to develop architectures to enable diagnosis and correction of faults, both at individual (service) as global (composition levels). The middlewares, and particularly reflective middlewares, have been used as a powerful tool to cope with inherent heterogeneous nature of distributed systems in order to give them greater adaptability capacities. In this paper we propose a middleware architecture for the diagnosis of fully distributed service compositions called ARMISCOM, which is not coordinated by any global diagnoser. The diagnosis of faults is performed through the interaction of the diagnoser present in each service composition, and the repair strategies are developed through consensus of each repairer distributed equally in each service composition.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6466760,no
SR-SIM: A fast and high performance IQA index based on spectral residual,"Automatic image quality assessment (IQA) attempts to use computational models to measure the image quality in consistency with subjective ratings. In the past decades, dozens of IQA models have been proposed. Though some of them can predict subjective image quality accurately, their computational costs are usually very high. To meet real-time requirements, in this paper, we propose a novel fast and effective IQA index, namely spectral residual based similarity (SR-SIM), based on a specific visual saliency model, spectral residual visual saliency. SR-SIM is designed based on the hypothesis that an image's visual saliency map is closely related to its perceived quality. Extensive experiments conducted on three large-scale IQA datasets indicate that SR-SIM could achieve better prediction performance than the other state-of-the-art IQA indices evaluated. Moreover, SR-SIM can have a quite low computational complexity. The Matlab source code of SR-SIM and the evaluation results are available online at http://sse.tongji.edu.cn/linzhang/IQA/SR-SIM/SR-SIM.htm.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6467149,no
Anti-ghost of differently exposed images with moving objects,"In a typical image synthesis where multiple differently exposed images are captured for processing, it is important to design an anti-ghost algorithm so as to prevent ghosting artifacts from appearing in the final image. An anti-ghost algorithm is usually composed of a detection module and a correction module. In this paper, a new detection module is proposed to detect non-consistent pixels of all input images without predefining any initial reference image. The proposed module is suitable when an interactive mode is desired. In addition, a bidirectional approach is introduced to correct the non-consistent pixels in the correction module. Compared with existing unidirectional correction methods, the proposed bidirectional correction approach uses information from two adjacent images of a detected image to correct its non-consistent pixels. This leads to a quality improvement in the final image.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6467467,no
Automation System for Validation of Configuration and Security Compliance in Managed Cloud Services,"Validation of configuration and security compliance at the time of creating new service is an important part of service management process and governance in most IT delivery organizations. It is performed to ensure that security risks, governance controls and vulnerabilities are proactively managed through the lifecycle of the services, and to guarantee that all discovered problems and issues are addressed and remediated for quality assurance before the services are delivered to customers. The validation process is complex and is typically carried out by following a checklist with questions and answers through manual steps that are time consuming and error prone. This lengthy process is particularly troublesome when providing managed cloud services to enterprise customers with a pre-specified request fulfillment time in SLA. In order to improve the timeliness and quality of cloud services, we have introduced an automation system to orchestrate the validation process with executable scripts to be executed against the services. We will describe a novel policy mechanism to capture exception rules for eliminating possible interference in security configuration contained in the scripts. We will explain how our system is designed and implemented to fulfill the needs of large enterprises from both the service provider's and the service consumer's vantage points.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6468252,no
Efficient Data Tagging for Managing Privacy in the Internet of Things,"The Internet of Things creates an environment where software systems are influenced and controlled by phenomena in the physical world. The goal is invisible and natural interactions with technology. However, if such systems are to provide a high-quality personalised service to individuals, they must by necessity gather information about those individuals. This leads to potential privacy invasion. Using techniques from Information Flow Control, data representing phenomena can be tagged with their privacy properties, allowing a trusted computing base to control access based on sensitivity and the system to reason about the flows of private data. For this to work well, tags must be assigned as soon as possible after phenomena are detected. Tagging within resource-constrained sensors raises worries that computing the tags may be too expensive and that useful tags are too large in relation to the data's size and the data's sensitivity. This paper assuages these worries, giving code templates for two small micro controllers (PIC and AVR) that effect meaningful tagging.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6468320,no
Towards a Fault-Tolerant Wireless Sensor Network Using Fault Injection Mechanisms: A Parking Lot Monitoring Case,"A Wireless Sensor Network (WSN) requires a high level of robust and fault tolerant sensing and actuating capabilities, specially when the application aims to gather delicate and urgent data with reasonable latency. Hence, verifying the behavior properties under the presence of faults remains an important step in developing an application over a WSN. A comprehensive study on characterization and understanding of all the possible faults is required in order to generate and inject 'any' known error to the system. In order to ensure appearance of all the faults and possible bugs in the system, conception and developing a fault injector which generates and injects any requested fault to the system is promising. This becomes more important and critical when the fault happens very rarely, while due to Murphy's law it happens certainly along the network life. Considering that occurrence of faults depends heavily on the specifications of the use case, in this paper we concentrate on a sensor network which aims to detect the presence of vehicles on parking lots. We try to categorize and characterize the faults driven by this system as the first step of developing a fault injector.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6468409,no
Design and modeling of a non-blocking checkpointing system,"As the capability and component count of systems increase, the MTBF decreases. Typically, applications tolerate failures with checkpoint/restart to a parallel file system (PFS). While simple, this approach can suffer from contention for PFS resources. Multi-level checkpointing is a promising solution. However, while multi-level checkpointing is successful on today's machines, it is not expected to be sufficient for exascale class machines, which are predicted to have orders of magnitude larger memory sizes and failure rates. Our solution combines the benefits of non-blocking and multi-level checkpointing. In this paper, we present the design of our system and model its performance. Our experiments show that our system can improve efficiency by 1.1 to 2.0x on future machines. Additionally, applications using our checkpointing system can achieve high efficiency even when using a PFS with lower bandwidth.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6468461,no
Cost- and deadline-constrained provisioning for scientific workflow ensembles in IaaS clouds,"Large-scale applications expressed as scientific workflows are often grouped into ensembles of inter-related workflows. In this paper, we address a new and important problem concerning the efficient management of such ensembles under budget and deadline constraints on Infrastructure- as-aService (IaaS) clouds. We discuss, develop, and assess algorithms based on static and dynamic strategies for both task scheduling and resource provisioning. We perform the evaluation via simulation using a set of scientific workflow ensembles with a broad range of budget and deadline parameters, taking into account uncertainties in task runtime estimations, provisioning delays, and failures. We find that the key factor determining the performance of an algorithm is its ability to decide which workflows in an ensemble to admit or reject for execution. Our results show that an admission procedure based on workflow structure and estimates of task runtimes can significantly improve the quality of solutions.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6468463,no
A data-driven model for software reliability prediction,"In the actual software development, failure data is rarely pure linear or nonlinear. It is usually formed by the linear and nonlinear patterns at the same time. These models can be divided into two main categories: analytical model and data-driven model. Analytical SRMs are proposed based on underlying assumptions about the nature of software faults, the stochastic behavior of the software processes and the development environments. On the contrary, the so-called data-driven models, borrowing heavily from artificial intelligence techniques, rely directly on the collected data describing input and output characteristics. Compared to analytical SRMs, data-driven models have much less unpractical assumptions and are much abler to make abstractions and generalizations of the software failure process. It has been recognized that the auto regression integrated moving average (ARIMA) and the support vector machine (SVM) perform fairly well in predicting linear and nonlinear time series data. Therefore, we propose a hybrid approach to software reliability forecasting using both ARIMA and SVM models.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6468581,no
High performance automatic number plate recognition in video streams,"We present a range of image and video analysis techniques that we have developed in connection with license plate recognition. Our methods focus on two areas - efficient image preprocessing to improve low-quality detection rate and combining the detection results from multiple frames to improve the accuracy of the recognized license plates. To evaluate our algorithms, we have implemented a complete ANPR system that detects and reads license plates. The system can process up to 110 frames per second on single CPU core and scales well to at least 4 cores. The recognition rate varies depending on the quality of video streams (amount of motion blur, resolution), but approaches 100% for clear, sharp license plate input data. The software is currently marketed commercially as CarID<sup>1</sup>. Some of our methods are more general and may have applications outside of the ANPR domain.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6469554,no
Quality of Estimations - How to Assess Reliability of Cost Predictions,"Software Project Cost Prediction is one of the unresolved problems of mankind. While today's civil engineering work is more or less under control, software projects are not. Cost overruns are so frequent that it is wise never trusting any initial cost estimate but take precaution for higher cost. Nevertheless, finance managers need reliable estimates in order to be able to fund software and ICT projects without running risks. Estimates are usually readily available - for instance based on functional size and benchmarking. However, the question how reliable these estimations are is often left out, or answered in a purely statistical manner that gives no clue to practitioners what these overall statistical variations means for them. This paper explains how to make use of Six Sigma's transfer functions that map cost defined by a committee of GUFPI-ISMA onto project cost. Transfer functions reverse the process of estimation: they show how much a project costs under suitable assumptions for the cost drivers. If cost drivers can be measured, and transfer functions can be determined with known accuracy, not only project cost can be predicted but also the range and probability for such cost to occur.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6472561,no
Measuring and Evaluating a DotNet Application System to Better Predict Maintenance Effort,"The ISO standard 9126 defines the basic quality criteria for evaluating a software product and suggests a suite of metrics for measuring them, however it remains for the user of the standard to apply those metrics to his particular situation. This paper describes how the metrics were extended to assess the static quality criteria as well as the complexity of a large Dot Net application. In addition, the size of the software was measured to be able to compare it with similar systems of the same type. The result was a comparative evaluation to aide the owners of that system in planning further maintenance and evolution activities. Besides that cost estimations were made for maintenance and further development. The measurement project described here is a practical example of how metrics can be applied to assess existing software systems.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6472564,no
Metrics Based Software Supplier Selection - Best Practice Used in the Largest Dutch Telecom Company,"This article provides insight into a 'best practice' used for the selection of software suppliers at the largest Dutch telecom operator, KPN[1]. It explains the metrics rationale applied by KPN when selecting only one preferred supplier (system integrator) per domain instead of the various suppliers that were previously active in each domain. Presently (Q2 2012) the selection and contracting process is entering its final phase. In this paper, the model that was built and used to assess the productivity of the various suppliers and the results of the supplier selection process are discussed. In addition, a number of lessons learned and recommendations are shared.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6472571,no
"Incremental Sampling Process for Actual Function Points Validation in a Contract, An Empirical Experiment","Customer verification of functional size measures provided by the supplier in the acceptance phase is a critical task for the correctness of contract execution. A lack of control by the customer, both in depth and in scope, can lead to relevant deviations of the actual unitary price if compared to that accepted in the bid assignment process, with potential consequences in terms of unfairness or, in some cases, illegality. In this paper we summarize an efficient and well defined approach to validate supplier's functional size measurements in order to present the validation experiment. This approach was extensively presented at SMEF 2012. The approach, although statistically based, is rigorous, since it defines clear and unambiguous game roles, and efficient, in order to spend the adequate effort to achieve the expected confidence about supplier's functional size measurement capabilities. The approach consists in applying a variation of the Incremental Sampling Method that allows the customer tuning the validation effort on the quality level of size measures provided by the supplier, detected by the gap among these measures and the ones checked and validated on a sampled base. An empirical validation experiment, which is the focus of the present paper, is presented to illustrate the advantages of the approach.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6472576,no
Verification of Spatio-Temporal Role Based Access Control using Timed Automata,"The verification of Spatio-Temporal Role Based Access Control policies (STRBAC) during the early development life cycle improves the security of the software. It helps to identify inconsistencies in the Access Control policies before proceeding to other phases where the cost of fixing defects is augmented. This paper proposes a formal method for an automatic analysis of STRBAC policies. It ensures that the policies are consistent and conflict-free. The method proposed in this paper makes the use of Timed Automata to verify the STRBAC policies. This is done by translating the STRBAC model into Timed Automata, and then the produced Timed Automata is implemented and verified using the model checker UPPAAL. This paper presents a security query expressed using TCTL to detect inconsistency caused due to the interaction between STRBAC policies. With the help of an example, this paper shows how we convert STRBAC model to Timed Automata models and verify the resulting models using the UPPAAL to identify an erroneous design.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6474023,no
Studying volatility predictors in open source software,"Volatile software modules, for the purposes of this work, are defined as those that are significantly more change-prone than other modules in the same system or subsystem. There is significant literature investigating models for predicting which modules in a system will become volatile, and/or are defect-prone. Much of this work focuses on using source code-related characteristics (e.g., complexity metrics) and simple change metrics (e.g., number of past changes) as inputs to the predictive models. Our work attempts to broaden the array of factors considered in such prediction approaches. To this end, we collected data directly from development personnel about the factors they rely on to foresee what parts of a system are going to become volatile. In this paper, we describe a focus group study conducted with the development team of a small but active open source project, in which we asked this very question. The results of the focus group indicate, among other things, that a period of volatility in a particular area of the system is often predicted by a pattern characterized by inactivity in a certain area (resulting in that area becoming less mature than others), increased communication between developers regarding opportunities for improvement in that area, and then the emergence of a champion who takes the initiative to start working on those improvements. The initial changes lead to more changes (both to extend the improvements already made and to fix problems introduced), thus leading to volatility.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6475416,no
Predicting defect numbers based on defect state transition models,"During software maintenance, a large number of defects could be discovered and reported. A defect can enter many states during its lifecycle, such as NEW, ASSIGNED, and RESOLVED. The ability to predict the number of defects at each state can help project teams better evaluate and plan maintenance activities. In this paper, we present BugStates, a method for predicting defect numbers at each state based on defect state transition models. In our method, we first construct defect state transition models using historical data. We then derive a stability metric from the transition models to measure a project's defect-fixing performance. For projects with stable defect-fixing performance, we show that we can apply Markovian method to predict the number of defects at each state in future based on the state transition model. We evaluate the effectiveness of BugStates using six open source projects and the results are promising. For example, when predicting defect numbers at each state in December 2010 using data from July 2009 to June 2010, the absolute errors for all projects are less than 28. In general, BugStates also outperforms other related methods.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6475417,no
How many individuals to use in a QA task with fixed total effort?,"Increasing the number of persons working on quality assurance (QA) tasks, e.g., reviews and testing, increases the number of defects detected - but it also increases the total effort unless effort is controlled with fixed effort budgets. Our research investigates how QA tasks should be configured regarding two parameters, i.e., time and number of people. We define an optimization problem to answer this question. As a core element of the optimization problem we discuss and describe how defect detection probability should be modeled as a function of time. We apply the formulas used in the definition of the optimization problem to empirical defect data of an experiment previously conducted with university students. The results show that the optimal choice of the number of persons depends on the actual defect detection probabilities of the individual defects over time, but also on the size of the effort budget. Future work will focus on generalizing the optimization problem to a larger set of parameters, including not only task time and number of persons but also experience and knowledge of the personnel involved, and methods and tools applied when performing a QA task.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6475432,no
A comparison of database fault detection capabilities using mutation testing,"Mutation testing involves systematically generating and introducing faults into an application to improve testing. A quasi-experimental study is reported comparing the fault-detection capabilities of realworld database application test suites to those of an SQL vendor test suite (NIST SQL) based on mutation scores. The higher the mutation score the more successful the test suite will be at detecting faults. The SQLMutation tool was used to generate query mutants from beginner-level sample schemas obtained from three popular real-world database test suite vendors - MySQL, SQL Server, and Oracle. Four SQLMutation operators were applied to both realworld and NIST SQL vendor compliance test suites - SQL Clause (SC), Operator Replacement (OR), NULL (NL) and Identifier Replacement (IR). Two mutation operators, SC and NL generated significantly lower mutation scores in real-world test suites than for those in the vendor test suite. The IR operator generated significantly higher mutation scores in real-world test suites than for those in the vendor test suite. The OR operator produced roughly the same mutation scores in both the real-world and vendor test suites.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6475435,no
Methodology to assess the influence of PV systems as a distributed generation technology,"This paper shows a simplified methodology for assess the impacts of photovoltaic systems connected to a supply of low voltage distribution power system, analyzing voltage variations outside the limits according to the NTC 1340, the chargeability and losses through a development software implemented in DIgSILENT. The development includes the characterization of solar resource and ambient temperature to estimate the generation of the photovoltaic system, with this procedure it can find an optimum rate of penetration PV on the electric network under two scenarios: first, a circuit located for residential loads and second, a commercial circuit, both in the city of Bogota, Colombia.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6478903,no
Investigating Automatic Static Analysis Results to Identify Quality Problems: An Inductive Study,"Background: Automatic static analysis (ASA) tools examine source code to discover """"issues"""", i.e. code patterns that are symptoms of bad programming practices and that can lead to defective behavior. Studies in the literature have shown that these tools find defects earlier than other verification activities, but they produce a substantial number of false positive warnings. For this reason, an alternative approach is to use the set of ASA issues to identify defect prone files and components rather than focusing on the individual issues. Aim: We conducted an exploratory study to investigate whether ASA issues can be used as early indicators of faulty files and components and, for the first time, whether they point to a decay of specific software quality attributes, such as maintainability or functionality. Our aim is to understand the critical parameters and feasibility of such an approach to feed into future research on more specific quality and defect prediction models. Method: We analyzed an industrial C# web application using the Resharper ASA tool and explored if significant correlations exist in such a data set. Results: We found promising results when predicting defect-prone files. A set of specific Resharper categories are better indicators of faulty files than common software metrics or the collection of issues of all issue categories, and these categories correlate to different software quality attributes. Conclusions: Our advice for future research is to perform analysis on file rather component level and to evaluate the generalizability of categories. We also recommend using larger datasets as we learned that data sparseness can lead to challenges in the proposed analysis process.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6479799,no
Model-Driven Development of Secure Service Applications,"The development of a secure service application is a difficult task and designed protocols are very error-prone. To develop a secure SOA application, application-independent protocols (e.g. TLS or Web service security protocols) are used. These protocols guarantee standard security properties like integrity or confidentiality but the critical properties are application-specific (e.g. 'a ticket can not be used twice'). For that, security has to be integrated in the whole development process and application-specific security properties have to be guaranteed. This paper illustrates the modeling of a security-critical service application with UML. The modeling is part of an integrated software engineering approach that encompasses model-driven development. Using the approach, an application based on service-oriented architectures (SOA) is modeled with UML. From this model executable code as well as a formal specification to prove the security of the application is generated automatically. Our approach, called SecureMDD, supports the development of security-critical applications and integrates formal methods to guarantee the security of the system. The modeling guidelines are demonstrated with an online banking example.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6479803,no
Three dimension Time-Frequency approach for diagnosing eccentricity faults in Switched Reluctance motor,"This paper presents an analysis of effects of dynamic air-gap eccentricity on the performances of a 6/4 Switched Reluctance Machine (SRM) through finite element analysis (FEA) based on a FEMM package associated to MATLAB/SIMULINK software. Among the various Time-Frequency methods used for detection of defects, the Time-Frequency Representation (TFR) is an appropriate tool to detect the mechanical failures through the torque analysis by allowing a better representation independent from the type of fault. Simulation results of healthy and faulty cases are discussed and illustrate the effectiveness of the proposed approach.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6482007,no
Applying evolution programming Search Based Software Engineering (SBSE) in selecting the best open source software maintainability metrics,"The nature of an Open Source Software development paradigm forces individual practitioners and organization to adopt software through trial and error approach. This leads to the problems of coming across software and then abandoning it after realizing its lack of important qualities to suit their requirements or facing negative challenges in maintaining the software. These contributed by lack of recognizing guidelines to lead the practitioners in selecting out of the dozens available metrics, the best metric(s) to measure quality OSS. In this study, the novel results provide the guidelines that lead to the development of metrics model that can select the best metric(s) to predict maintainability of Open Source Software.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6482071,no
Simulation study of a simple flux saturation controller for high-frequency transformer link full-bridge DC-DC converters,"High-Frequency Transformer Link Full-bridge DC-DC converter systems such as those used in plasma cutting applications are prone to transformer flux saturation. It can cause unit shutdown due to over-current protection or even catastrophic failure under extreme situations [1]. This is especially unacceptable in large plasma cutting applications where unexpected production stoppages can lead to severe economic loss to the customer. A simple flux control method that overcomes the disadvantages of current flux saturation control methods has been presented in [2]. Here, the transformer flux is controlled without affecting the dynamics of the main control loop. The proposed method is verified by simulating a 16 kW DC-DC full-bridge converter circuit model in ORCAD-PSPICE software. Results of this exercise show a 50% improvement in dynamic response, a 25% reduction in transformer size and weight, and improvements in system reliability and efficiency when compared with the conventional approach. It is seen that the proposed method can be retro-fitted on an existing power supply whether voltage or current controlled, with minimal change to its circuitry. In addition, it can also be extended to converter topologies like the push-pull as well.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6484342,no
Opportunities and challenges of static code analysis of IEC 61131-3 programs,"Static code analysis techniques analyze programs by examining the source code without actually executing them. The main benefits lie in improving software quality by detecting potential defects and problematic code constructs in early development stages. Today, static code analysis is widely used and numerous tools are available for established programming languages like C/C++, Java, C# and others. However, in the domain of PLC programming, static code analysis tools are still rare. In this paper we present an approach and tool support for static code analysis of PLC programs. The paper discusses opportunities static code analysis can offer for PLC programming, it reviews techniques for static analysis, and it describes our tool that implements a rule-based analysis approach for IEC 61131-3 programs.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6489535,no
A status protocol for system-operation in a fault-tolerant system  Verification and testing with SPIN,This paper presents a status protocol for a fault-tolerant distributed real-time system. The protocol aims to give all nodes a consistent view of the status of processing operations during one communication cycle; despite the occurrence of asymmetric omission failures. The system consists of nodes interconnected with a time-triggered network. A part of the protocol is performed only on-demand i.e. when failure is detected and can thus make use of event-triggered messages in e.g. FlexRay. The protocol is studied in several configurations of nodes and processes. Model checking with SPIN shows that it is not possible to guarantee a consistent decision when more than one failure occurs. SPIN is then used to enumerate the success-ratio (at least 90%) of the protocol in failure scenarios for a number of configurations of the protocol.,2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6489648,no
Reliability correlation between physical and virtual cores at the ISA level,"The proliferation of highly-configurable FPGA technology has allowed the implementation of dedicated systems of diverse configurations and fueled the software to hardware migration paradigm. This work demonstrates how the hardware implementation of virtualization technology affects the system reliability at several levels of abstraction. By correlating faults between the physical and virtual, the reliability impact of hardware-assisted virtualization is shown, as well as how runtime faults are capable of breaching virtualization. ISA profiling is used to assess reliability at early design stages and how its use can serve as a robustness guideline for hardware and software designers is explained.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6489725,no
Semantic design and integration of simulation models in the industrial automation area,"Simulations are software tools approximating and predicting the behavior of real industrial plants. Unlike real plants, the utilization of simulations cannot cause damages and it saves time and costs during series of experiments. A shortcoming of current simulation models is the complicated runtime integration into legacy industrial systems and platforms, as well as ad-hoc design phase, introducing manual and error-prone work. This paper contributes to improve the efficiency of simulation model design and integration. It utilizes a semantic knowledge base, implemented by ontologies and their mappings. The integration uses the Automation Service Bus and the paper explains how to configure the runtime integration level semantically. The main contributions are the concept of semantic configuration of the service bus and the workflows of simulation design and integration.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6489781,no
An autonomous recovery software module for protecting embedded OS and application software,"Embedded systems have been widespread for novel technologies which bring people more convenience and hence become more relevant to our life. When embedded systems are utilized on safety-critical applications, their availability and reliability issues must be addressed and systems must be protected by effective techniques. One primary cause of the embedded system crash is the data corruption error. In this study, the embedded system crashes caused by data corruption errors are resolved by an autonomous recovery software methodology (ARSM). ARSM is composed by system monitor, bad block salvage, autonomous recovery mechanism and OS initial backup. ARSM performs all-operation system monitoring. Once any application software and operation system crash is detected, the autonomous recovery mechanism will be activated to recover the embedded system back to normal operation. For verification of the ARSM, we adopt a car event data recorder to be the case demonstration, and generate data corruption errors to validate the efficiency of the ARSM.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6490149,no
Reliability asssessment of SIPS based on a Safety Integrity Level and Spurious Trip Level,"As the number and complexity of System Integrity Protection Schemes (SIPS) in operation increases very rapidly, it must be ensured that their performance meets the reliability requirements of electrical utilities, in terms of dependability and security. A procedure based on Markov Modeling and Fault Tree Analysis is proposed for assessing SIPS reliability. Many operators tend to have SIPS permanently in service; this reduces the probability the arming software or the human operator will fail to arm the SIPS. Whilst this can decrease the probability of dependability-based misoperation, it may increase the probability of security-based misoperation. Therefore, the impact of having SIPS always armed is examined and compared with the impact of arming the schemes only when required. In addition, two reliability indices are introduced for quantifying the level of SIPS reliability: Safety Integrity Level and Spurious Trip Level. The proposed method is illustrated using the South of Lugo N-2 SIPS, which is part of the South California Edison grid.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6493132,no
QoS and performance optimization with VM provisioning approach in Cloud computing environment,"Cloud computing is the computing paradigm which delivers IT resources as a service, hence user are free from setting up the infrastructure and managing hardware etc. Cloud Computing provides dynamic provisioned resources and presented as one or more integrated computing resources based on constraints. The process of provisioning in Clouds requires the application provisioner to compute the best software and hardware configuration so as to ensure that Quality of Services (QoS) target of application services are achieved, without compromising efficiency and utilization of whole system. This paper presents a dynamic provisioning technique, adapting to peak-to-peak workload changes related to applications to offer end-users guaranteed Quality of Services (QoS) in highly dynamic environments. Behavior and performance of applications and Cloud-based IT resources are modeled to adaptively serve end-user requests. Analytical performance (queuing network system model) and workload information are used to supply intelligent input about the physical infrastructure which causes improvement in efficiency. VM provisioning technique detects changes in workload intensity that occurs over time and makes appropriate changes in allocations of multiple virtualized IT resources to achieve application QoS targets.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6493187,no
To what extent could we detect field defects? an empirical study of false negatives in static bug finding tools,"Software defects can cause much loss. Static bug-finding tools are believed to help detect and remove defects. These tools are designed to find programming errors; but, do they in fact help prevent actual defects that occur in the field and reported by users? If these tools had been used, would they have detected these field defects, and generated warnings that would direct programmers to fix them? To answer these questions, we perform an empirical study that investigates the effectiveness of state-of-the-art static bug finding tools on hundreds of reported and fixed defects extracted from three open source programs: Lucene, Rhino, and AspectJ. Our study addresses the question: To what extent could field defects be found and detected by state-of-the-art static bug-finding tools? Different from past studies that are concerned with the numbers of false positives produced by such tools, we address an orthogonal issue on the numbers of false negatives. We find that although many field defects could be detected by static bug finding tools, a substantial proportion of defects could not be flagged. We also analyze the types of tool warnings that are more effective in finding field defects and characterize the types of missed defects.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6494905,no
An automated approach to forecasting QoS attributes based on linear and non-linear time series modeling,"Predicting future values of Quality of Service (QoS) attributes can assist in the control of software intensive systems by preventing QoS violations before they happen. Currently, many approaches prefer Autoregressive Integrated Moving Average (ARIMA) models for this task, and assume the QoS attributes' behavior can be linearly modeled. However, the analysis of real QoS datasets shows that they are characterized by a highly dynamic and mostly nonlinear behavior to the extent that existing ARIMA models cannot guarantee accurate QoS forecasting, which can introduce crucial problems such as proactively triggering unrequired adaptations and thus leading to follow-up failures and increased costs. To address this limitation, we propose an automated forecasting approach that integrates linear and nonlinear time series models and automatically, without human intervention, selects and constructs the best suitable forecasting model to fit the QoS attributes' dynamic behavior. Using real-world QoS datasets of 800 web services we evaluate the applicability, accuracy, and performance aspects of the proposed approach, and results show that the approach outperforms the popular existing ARIMA models and improves the forecasting accuracy by on average 35.4%.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6494913,no
Predicting recurring crash stacks,"Software crash is one of the most severe bug manifestations and developers want to fix crash bugs quickly and efficiently. The Crash Reporting System (CRS) is widely deployed for this purpose. Even with the help of CRS, fixes are largely by manual effort, which is error-prone and results in recurring crashes even after the fixes. Our empirical study reveals that 48% of fixed crashes in Firefox CRS are recurring mostly due to incomplete or missing fixes. It is desirable to automatically check if a crash fix misses some reported crash traces at the time of the first fix. This paper proposes an automatic technique to predict recurring crash traces. We first extract stack traces and then compare them with bug fix locations to predict recurring crash traces. Evaluation using the real Firefox crash data shows that the approach yields reasonable accuracy in prediction of recurring crashes. Had our technique been deployed earlier, more than 2,225 crashes in Firefox 3.6 could have been avoided.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6494917,no
Code patterns for automatically validating requirements-to-code traces,"Traces between requirements and code reveal where requirements are implemented. Such traces are essential for code understanding and change management. Unfortunately, traces are known to be error prone. This paper introduces a novel approach for validating requirements-to-code traces through calling relationships within the code. As input, the approach requires an executable system, the corresponding requirements, and the requirements-to-code traces that need validating. As output, the approach identifies likely incorrect or missing traces by investigating patterns of traces with calling relationships. The empirical evaluation of four case study systems covering 150 KLOC and 59 requirements demonstrates that the approach detects most errors with 85-95% precision and 82-96% recall and is able to handle traces of varying levels of correctness and completeness. The approach is fully automated, tool supported, and scalable.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6494919,no
Can I clone this piece of code here?,"While code cloning is a convenient way for developers to reuse existing code, it may potentially lead to negative impacts, such as degrading code quality or increasing maintenance costs. Actually, some cloned code pieces are viewed as harmless since they evolve independently, while some other cloned code pieces are viewed as harmful since they need to be changed consistently, thus incurring extra maintenance costs. Recent studies demonstrate that neither the percentage of harmful code clones nor that of harmless code clones is negligible. To assist developers in leveraging the benefits of harmless code cloning and/or in avoiding the negative impacts of harmful code cloning, we propose a novel approach that automatically predicts the harmfulness of a code cloning operation at the point of performing copy-and-paste. Our insight is that the potential harmfulness of a code cloning operation may relate to some characteristics of the code to be cloned and the characteristics of its context. Based on a number of features extracted from the cloned code and the context of the code cloning operation, we use Bayesian Networks, a machine-learning technique, to predict the harmfulness of an intended code cloning operation. We evaluated our approach on two large-scale industrial software projects under two usage scenarios: 1) approving only cloning operations predicted to be very likely of no harm, and 2) blocking only cloning operations predicted to be very likely of harm. In the first scenario, our approach is able to approve more than 50% cloning operations with a precision higher than 94.9% in both subjects. In the second scenario, our approach is able to avoid more than 48% of the harmful cloning operations by blocking only 15% of the cloning operations for the first subject, and avoid more than 67% of the cloning operations by blocking only 34% of the cloning operations for the second subject.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6494924,no
Using GUI ripping for automated testing of Android applications,"We present AndroidRipper, an automated technique that tests Android apps via their Graphical User Interface (GUI). AndroidRipper is based on a user-interface driven ripper that automatically explores the app's GUI with the aim of exercising the application in a structured manner. We evaluate AndroidRipper on an open-source Android app. Our results show that our GUI-based test cases are able to detect severe, previously unknown, faults in the underlying code, and the structured exploration outperforms a random approach.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6494930,no
Detection of embedded code smells in dynamic web applications,"In dynamic Web applications, there often exists a type of code smells, called embedded code smells, that violate important principles in software development such as software modularity and separation of concerns, resulting in much maintenance effort. Detecting and fixing those code smells is crucial yet challenging since the code with smells is embedded and generated from the server-side code. We introduce WebScent, a tool to detect such embedded code smells. WebScent first detects the smells in the generated code, and then locates them in the server-side code using the mapping between client-side code fragments and their embedding locations in the server program, which is captured during the generation of those fragments. Our empirical evaluation on real-world Web applications shows that 34%-81% of the tested server files contain embedded code smells. We also found that the source files with more embedded code smells are likely to have more defects and scattered changes, thus potentially require more maintenance effort.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6494936,no
Predicting common web application vulnerabilities from input validation and sanitization code patterns,"Software defect prediction studies have shown that defect predictors built from static code attributes are useful and effective. On the other hand, to mitigate the threats posed by common web application vulnerabilities, many vulnerability detection approaches have been proposed. However, finding alternative solutions to address these risks remains an important research problem. As web applications generally adopt input validation and sanitization routines to prevent web security risks, in this paper, we propose a set of static code attributes that represent the characteristics of these routines for predicting the two most common web application vulnerabilities-SQL injection and cross site scripting. In our experiments, vulnerability predictors built from the proposed attributes detected more than 80% of the vulnerabilities in the test subjects at low false alarm rates.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6494943,no
Software defect prediction using semi-supervised learning with dimension reduction,"Accurate detection of fault prone modules offers the path to high quality software products while minimizing non essential assurance expenditures. This type of quality modeling requires the availability of software modules with known fault content developed in similar environment. Establishing whether a module contains a fault or not can be expensive. The basic idea behind semi-supervised learning is to learn from a small number of software modules with known fault content and supplement model training with modules for which the fault information is not available. In this study, we investigate the performance of semi-supervised learning for software fault prediction. A preprocessing strategy, multidimensional scaling, is embedded in the approach to reduce the dimensional complexity of software metrics. Our results show that the semi-supervised learning algorithm with dimension-reduction preforms significantly better than one of the best performing supervised learning algorithms, random forest, in situations when few modules with known fault content are available for training.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6494944,no
Healing online service systems via mining historical issue repositories,"Online service systems have been increasingly popular and important nowadays, with an increasing demand on the availability of services provided by these systems, while significant efforts have been made to strive for keeping services up continuously. Therefore, reducing the MTTR (Mean Time to Restore) of a service remains the most important step to assure the user-perceived availability of the service. To reduce the MTTR, a common practice is to restore the service by identifying and applying an appropriate healing action (i.e., a temporary workaround action such as rebooting a SQL machine). However, manually identifying an appropriate healing action for a given new issue (such as service down) is typically time consuming and error prone. To address this challenge, in this paper, we present an automated mining-based approach for suggesting an appropriate healing action for a given new issue. Our approach generates signatures of an issue from its corresponding transaction logs and then retrieves historical issues from a historical issue repository. Finally, our approach suggests an appropriate healing action by adapting healing actions for the retrieved historical issues. We have implemented a healing suggestion system for our approach and applied it to a real-world product online service that serves millions of online customers globally. The studies on 77 incidents (severe issues) over 3 months showed that our approach can effectively provide appropriate healing actions to reduce the MTTR of the service.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6494945,no
MaramaAI: tool support for capturing and managing consistency of multi-lingual requirements,"Requirements captured by Requirements Engineers are commonly inconsistent with their client's intended requirements and are often error prone especially if the requirements are written in multiple languages. We demonstrate the use of our automated inconsistency-checking tool MaramaAI to capture and manage the consistency of multi-lingual requirements in both the English and Malay languages for requirements engineers and clients using a round-trip, rapid prototyping approach.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6494947,no
GZoltar: an eclipse plug-in for testing and debugging,"Testing and debugging is the most expensive, error-prone phase in the software development life cycle. Automated testing and diagnosis of software faults can drastically improve the efficiency of this phase, this way improving the overall quality of the software. In this paper we present a toolset for automatic testing and fault localization, dubbed GZoltar, which hosts techniques for (regression) test suite minimization and automatic fault diagnosis (namely, spectrum-based fault localization). The toolset provides the infrastructure to automatically instrument the source code of software programs to produce runtime data. Subsequently the data was analyzed to both minimize the test suite and return a ranked list of diagnosis candidates. The toolset is a plug-and-play plug-in for the Eclipse IDE to ease world-wide adoption.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6494960,no
Software Defect Prediction Scheme Based on Feature Selection,"Predicting defect-prone software modules accurately and effectively are important ways to control the quality of a software system during software development. Feature selection can highly improve the accuracy and efficiency of the software defect prediction model. The main purpose of this paper is to discuss the best size of feature subset for building a prediction model and prove that feature selection method is useful for establishing software defect prediction model. Mutual information is an outstanding indicator of relevance between variables, and it has been used as a measurement in our feature selection algorithm. We also introduce a nonlinear factor to our evaluation function for feature selection to improve its performance. The results of our feature selection algorithm are validated by different machine learning methods. The experiment results show that all the classifiers achieve higher accuracy by using the feature subset provided by our algorithm.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6495391,no
EasyBuild: Building Software with Ease,"Maintaining a collection of software installations for a diverse user base can be a tedious, repetitive, error-prone and time-consuming task. Because most end-user software packages for an HPC environment are not readily available in existing OS package managers, they require significant extra effort from the user support team. Reducing this effort would free up a large amount of time for tackling more urgent tasks. In this work, we present EasyBuild, a software installation framework written in Python that aims to support the various installation procedures used by the vast collection of software packages that are typically installed in an HPC environment - catering to widely different user profiles. It is built on top of existing tools, and provides support for well-established installation procedures. Supporting customised installation procedures requires little effort, and sharing implementations of installation procedures becomes very easy. Installing software packages that are supported can be done by issuing a single command, even if dependencies are not available yet. Hence, it simplifies the task of HPC site support teams, and even allows end-users to keep their software installations consistent and up to date.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6495863,no
"Abstract: cTuning.org: Novel Extensible Methodology, Framework and Public Repository to Collaboratively Address Exascale Challenges","Innovation in science and technology is vital for our society and requires faster, more power efficient and reliable computer systems. However, designing and optimizing such systems has become intolerably complex, ad-hoc, costly and error prone due to ever increasing number of available design and optimization choices combined with complex interactions between all software and hardware components, multiple strict requirements placed on characteristics of new computer systems, and a large number of ever-changing and often incompatible analysis and optimization tools. Auto-tuning, run-time adaptation and machine learning based approaches have been demonstrating good promise to address above challenges for more than a decade but are still far from the widespread production use due to unbearably long exploration and training times, lack of a common experimental methodology, and lack of public repositories for unified data collection, analysis and mining.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6495999,no
"Poster: Collective Tuning: Novel Extensible Methodology, Framework and Public Repository to Collaboratively Address Exascale Challenges","Designing and optimizing novel computing systems became intolerably complex, ad-hoc, costly and error prone due to an unprecedented number of available tuning choices, and complex interactions between all software and hardware components. I present a novel holistic methodology, extensible infrastructure and public repository (cTuning.org and Collective Mind) to overcome the rising complexity of computer systems by distributing their characterization and optimization among multiple users. This technology effectively combines online auto-tuning, run-time adaptation, data mining and predictive modeling to collaboratively analyze thousands of codelets and datasets, explore large optimization spaces and detect abnormal behavior. It then extrapolates collected knowledge to suggest program optimizations, run-time adaptation scenarios or architecture designs to balance performance, power consumption and other characteristics. This technology has been recently successfully validated and extended in several academic and industrial projects with NCAR, Intel Exascale Lab, IBM and CAPS Entreprise, and we believe that it will be vital for developing future Exascale systems.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6496000,no
Comparison of genome-scale reconstructions using ModeRator,"The Computational Intelligence is one of the main tools in biochemical network modeling that help predict or optimize engineering means of objectives to achieve. For this purpose, reconstructions (which tend to increase in number and size) of different genome-scale metabolic networks can be used. Consequently, realizing different tasks it is necessary to evaluate alternative models and to assess the quality, similarity and usefulness of the combination. This article provides an in depth look into reconstruction comparison software tool ModeRator for detection of inconsistencies, duplicate reactions and visualization of comparison results. A case study shows comparison results of two representative genome-scale metabolic network reconstructions containing 600 and 747 reactions. The obtained results show how using various options the threshold of comparison strictness can be lowered to reveal similar or probably equal reactions. The application, user manual and sample reconstructions can be downloaded from http://biosystems.lv/moderator2/. The ModeRator2 is implemented in Python and is freely available.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6496737,no
Market-Awareness in Service-Based Systems,"Service-based systems are applications built by composing pre-existing services. During design time and according to the specifications, a set of services is selected. Both, service providers and consumers exist in a service market that is constantly changing. Service providers continuously change their quality of services (QoS), and service consumers can update their specifications according to what the market is offering. Therefore, during runtime, the services are periodically and manually checked to verify if they still satisfy the specifications. Unfortunately, humans are overwhelmed with the degree of changes exhibited by the service market. Consequently, verification of the compliance specification and execution of the corresponding adaptations when deviations are detected cannot be carried out in a manual fashion. In this work, we propose a framework to enable online awareness of changes in the service market in both consumers and providers by representing them as active software agents. At runtime, consumer agents concretize QoS specifications according to the available market knowledge. Services agents are collectively aware of themselves and of the consumers' requests. Moreover, they can create and maintain virtual organizations to react actively to demands that come from the market. In this paper we show preliminary results that allow us to conclude that the creation and adaptation of service-based systems can be carried out by a self-organized service market system.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6498398,no
Visual field monitoring of road defect and modeling of pavement road vibration from moving truck traffic,"A number of flexible pavement structures experience deterioration due to high traffic volume and growing weights. Thus, there is a need to model pavement responses due to various types of overweight truck traffic by taking into account axle loads, configuration and traffic operations in order to provide a comprehensive understanding and to assess the existing pavement performance and expected service life. The data used are from 16-hour traffic volume at these stations taken twice a year for seven years by the Highway Planning Unit (HPU) using manual counting are used to investigate the relationship between volume of heavy vehicles and the damages that occur. The empirical modeling was performed to ascertain the relationships among three parameters which are vertical vibration of ground borne vibration, vehicular speed and traffic noise generated by the movement of vehicles. As a tool to assess the performance of the pavement based on vibration index level, new approach procedure has been set up to determine the relationship between vibration index with speed, noise and pavement defect by developing three multiple linear regression model using advanced statistical software.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6504428,no
Application DANP with MCDM model to explore smartphone software,"To understand the behavior of smartphone online application software will be helpful to predict whether the software application would be adopted by the users and to guide the providers to enhance the functions of the software. A wide range of criteria are used to assess smartphone software quality, but most of these criteria have interdependent or interactive characteristics, which can make it difficult to effectively analyze and improve smartphone use intention. The purpose of this study is to address this issue using a hybrid MCDM (multiple criteria decision-making) approach that includes the DEMATEL (decision-making trial and evaluation laboratory), DANP (the DEMATEL-based analytic network process) methods to achieve an optimal solution. By exploring the influential interrelationships between criteria related to mobile communication industry's and related value-added service content providers' reference in the respect of operation. This approach can be used to solve interdependence and feedback problems, allowing for greater satisfaction of the actual needs of mobile communication industries.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6505023,no
A process for clouds services procurement based on model and QoS,"A relevant challenge for cloud computing is related to quality control of the services available. Cloud providers sometimes just deliver services, but do not clearly define quality of services guarantees. In addition, each provider uses a particular process to provide services. In this way, aiming to define a service procurement process to clouds, this paper proposes an approach based on a cloud environment model, considering service quality preservation. The proposed process will use an environment model containing all relevant information to create a virtual workspace, taking into account requirements of hardware and software and quality parameters, all of them specified by users. From this model, it will be possible to automatically provide platform and infrastructure as a service. The agreement negotiation happens during the service acquisition process from automated agents, creating the services and monitoring their quality attributes, generating an environment less error-prone, increasing the customer level satisfaction.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6508155,no
Research on risk control system in regional power grid,"Today's society increasingly high demand on the supply level of service, and quality service based on the safe and stable operation of power grids, regional power grid face the majority of users directly, while the active power itself can be modulated by a relatively small grid operation security risk is always there, in order to effectively control the risk of grid when it runs and improve ability to resist risks, therefor it is very important to build the system of risk control system of regional power grid. Based on the characteristics of regional power grid itself, described the contents of the regional power grid risks tube control system's design principle, the system software and hardware architecture, which is mainly introduced the regional power grid risk management and control systems: historical data read, grid risk identification and assessment, grid risk comprehensive assessment, grid risk controlling and decision-making, intelligent early warning function, and realize the function of this five respects the need to study the content and technical policy. Regional power grid can effectively analyse, induce and recognise the various potential or inherent risk factors in the regional grid security and stability through the establishment of risk management and control system. at the same time the probability of occurrence of various risk factors and the severity of the impact on the regional power grid are quantified, with the formation of different risk indicators and unified the various indicators of risk, it determines the level of risk ultimately, power companies establish appropriate contingency plans and emergency response programs according to the level of risk, which reduce the risk of the grid operation and improve the supply service levels and the enterprise economic efficiency.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6508468,no
Intelligent online monitoring of power capacitor complete equipment in substation,"The general structure of capacitor complete equipment inside the substation and internal structure of single capacitor are introduced and analyzed in this paper. Based on this, a mathematical model is established to simulate several possible damages of internal component of single capacitor by using computing software, and then some applicable conclusions are drawn. In accordance with China's overall trend of developing smart grid, a system of intelligent online monitoring applicable to the substation capacitor complete equipment is figured out, and a pre-detection of fault symptom of the power capacitor is able to detected, and the fault could be analyzed and located.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6508601,no
Modular based multiple test case prioritization,"Cost and time effective reliable test case prioritization technique is the need for present software industries. The test case prioritization for the entire program consumes more time and the selection of test case for entire software is also affecting the test performance. In order to alleviate the above problem a new methodology using modular based test case prioritization is proposed for regression testing. In this method the program is divided into multiple modules. The test cases corresponding to each module is prioritized first. In the second stage, the individual modular based prioritized test suites are combined together and further prioritized for the whole program. This method is verified for fault coverage and compared with overall program test case prioritization method. The proposed method is assessed using three standard applications namely University Students Monitoring System, Hospital Management System, and Industrial Process Operation System. The empirical studies show that the proposed algorithm is significantly performed well. The superiority of the proposed method is also highlighted.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6510205,no
Predicting fault-prone software modules using feature selection and classification through data mining algorithms,"Software defect detection has been an important topic of research in the field of software engineering for more than a decade. This research work aims to evaluate the performance of supervised machine learning techniques on predicting defective software through data mining algorithms. This paper places emphasis on the performance of classification algorithms in categorizing seven datasets (CM1, JM1, MW1, KC3, PC1, PC2, PC3 and PC4) under two classes namely Defective and Normal. In this study, publicly available data sets from different organizations are used. This permitted us to explore the impact of data from different sources on different processes for finding appropriate classification models. We propose a computational framework using data mining techniques to detect the existence of defects in software components. The framework comprises of data pre-processing, data classification and classifier evaluation. In this paper; we report the performance of twenty classification algorithms on seven publicly available datasets from the NASA MDP Repository. Random Tree Classification algorithm produced 100 percent accuracy in classifying the datasets and hence the features selected by this technique were considered to be the most significant features. The results were validated with suitable test data.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6510294,no
"Quality Metrics in optical modulation analysis: EVM and its relation to Q-factor, OSNR, and BER","The quality of optical signals is a very important parameter in optical communications. Several metrics are in common use, like optical signal-to-noise power ratio (OSNR), Q-factor, error vector magnitude (EVM) and bit error ratio (BER). A measured raw BER is not necessarily useful to predict the final BER after soft-decision forward error correction (FEC), if the statistics of the noise leading to errors is unknown. In this respect the EVM is superior, as it allows an estimation of the error statistics. We compare various metrics analytically, by simulation, and through experiments. We employ six quadrature amplitude modulation (QAM) formats at symbol rates of 20 GBd and 25 GBd. The signals were generated by a software-defined transmitter. We conclude that for optical channels with additive Gaussian noise the EVM metric is a reliable quality measure. For nondata-aided QAM reception, BER in the range 10<sup>-6</sup>...10<sup>-2</sup> can be reliably estimated from measured EVM.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6510607,no
Finding focused itemsets from software defect data,"Software product measures have been widely used to predict software defects. Though these measures help develop good classification models, studies propose that relationship between software measures and defects still needs to be investigated. This paper investigates the relationship between software measures and the defect prone modules by studying associations between the two. The paper identifies the critical ranges of the software measures that are strongly associated with defects across five datasets of PROMISE repository. The paper also identifies the ranges of the measures that do not necessarily contribute towards defects. These results are supported by information gain based ranking of software measures.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6511437,no
Lessons Learnt in the Implementation of CMMI Maturity Level 5,"CMMI<sup></sup> has proven benefits in software process improvement. Typically, organisations that achieve a CMMI level rating improve their performance. However, CMMI implementation is not trivial, in particular for high maturity levels, and not all organisations achieve the expected results. Certain CMMI implementation problems may remain undetected by SCAMPISM since only a sample of the organisation is analysed during the appraisal and assessing the quality of implementation of some practices may be difficult. In this paper we present the case of three CMMI level 5 organisations. From the lessons learnt and based on an extensive bibliographic research, we identify a set of problems and difficulties that organisations willing to implement CMMI should be aware of and provide a set of recommendations to help avoid them. As future research we will develop a framework to help to evaluate the quality of implementation of CMMI practices.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6511781,no
A Metamodel-Based Approach for Customizing and Assessing Agile Methods,"In today's dynamic market environments, producing high quality software rapidly and efficiently is crucial. In order to allow fast and reliable development processes, several agile methodologies have been designed and are now quite popular. Although existing agile methodologies are abundant, companies are increasingly interested in the construction of their own customized methods to fit their specific environment. In this paper, we investigate how agile methods can be constructed in-house to address specific software process needs. First, we examine a case study focusing on the tailoring of two agile methodologies, XP and Scrum. Then, we focus on the high-level scope of any agile method customization and investigate an approach based on the Situational Method Engineering (SME) paradigm that includes measurement concepts for constructing context specific agile methodologies. We also examine several existing metamodels proposed for use in SME. Finally, we introduce an agile metamodel designed to support the construction of agile methods and relying on measurements to provide guidance to agile methodologists during the construction phase and throughout the development process itself.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6511783,no
Using Association Rules to Identify Similarities between Software Datasets,"A number of V&V datasets are publicly available. These datasets have software measurements and defectiveness information regarding the software modules. To facilitate V&V, numerous defect prediction studies have used these datasets and have detected defective modules effectively. Software developers and managers can benefit from the existing studies to avoid analogous defects and mistakes if they are able to find similarity between their software and the software represented by the public datasets. This paper identifies the similar datasets by comparing association patterns in the datasets. The proposed approach finds association rules from each dataset and identifies the overlapping rules from the 100 strongest rules from each of the two datasets being compared. Afterwards, average support and average confidence of the overlap is calculated to determine the strength of the similarity between the datasets. This study compares eight public datasets and results show that KC2 and PC2 have the highest similarity 83% with 97% support and 100% confidence. Datasets with similar attributes and almost same number of attributes have shown higher similarity than the other datasets.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6511790,no
Developing a Process Assessment Model for Technological and Business Competencies on Software Development,"This article describes the design, development, validation and results of a Process Assessment Model for assessing Technological and Business Competencies on Software Development. The model follows the ISO/IEC 15504 (SPICE) requirements for Process Assessment Models. The development of this model follows the PRO2PI Method Framework for Engineering Process Models as a methodology. In the first phases the concept to be assessed was defined in terms of technological and business competencies on software development. In order to identify these competencies the processes used to develop the software systems are identified and analyzed. Model's versions have been used in thirteen software intensive organizations.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6511793,no
Formally Specifying Requirements with RSL-IL,"To mediate and properly support the interplay between the domains of business stakeholders and the development team, requirements must be documented and maintained in a rigorous manner. Furthermore, to effectively communicate the viewpoints of different stakeholders, it is of the utmost importance to provide complementary views that support a better understanding of the intended software system's requirements. However, the quality of requirements specifications and related artifacts strongly depends on the expertise of whoever performs these human-intensive and error-prone activities. This paper introduces RSL-IL, a domain-specific language that can be used to formally specify the requirements of software systems. The formal semantics of RSL-IL constructs enable further computations on its requirements representations, such as the automatic verification and generation of complementary views that support stakeholders during requirements validation.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6511812,no
A Quality Model for Spreadsheets,"In this paper we present a quality model for spreadsheets based on the ISO/IEC 9126 standard that defines a generic quality model for software. To each of the software characteristics defined in the ISO/IEC 9126, we associate an equivalent spreadsheet characteristic. Then, we propose a set of spreadsheet specific metrics to assess the quality of a spreadsheet in each of the defined characteristics. To obtain the normal distribution of expected values for a spreadsheet in each of the proposed metrics, we have executed them in the widely used EUSES spreadsheet corpus. Then, we quantify each characteristic of our quality model after computing the values of our metrics, and we define quality scores for the different ranges of values. Finally, to automate the quality assessment of a given spreadsheet, according to our quality model, we have integrated the computation of the metrics it includes in both a batch and a web-based tool.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6511816,no
Structuring and Verifying Requirement Specifications through Activity Diagrams to Support the Semi-automated Generation of Functional Test Procedures,"The higher the quality of the specification document, the lower the effort of its translation into design models and testing plans. Besides, an adequate level of abstraction to promote such translations must be described. Therefore, to ensure the quality of requirements specifications it is strategic to develop high quality software applications. So, in this paper a model-based approach to support the correctness, structuring, and translation of functional requirements specifications is described. This approach consists of facilities to build and inspect requirements specifications based on activity diagrams (capturing use cases), and derive functional tests from them. A tool to model and check the activity diagram, a checklist-based inspection technique and a test procedure generation tool form it. This approach was assessed in experimental studies that indicated its feasibility in specification time and a significant reduction of defects in the specified use cases when compared to ad-hoc approaches.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6511818,no
Usability Evaluation of Domain-Specific Languages,"Domain-Specific Languages (DSLs) are claimed to bring important productivity improvements to developers, when compared to General-Purpose Languages (GPLs). The increased Usability is regarded as one of the key benefits of DSLs when compared to GPLs, and has an important impact on the achieved productivity of the DSL users. So, it is essential to build in good usability while developing the DSL. The purpose of this proposal is to contribute to the systematic activity of Software Language Engineering by focusing on the issue of the Usability evaluation of DSLs. Usability evaluation is often skipped, relaxed, or at least omitted from papers reporting development of DSLs. We argue that a systematic approach based on User Interface experimental validation techniques should be used to assess the impact of new DSLs. For that purpose, we propose to merge common Usability evaluation processes with the DSL development process. In order to provide reliable metrics and tools we should reuse and identify good practices that exist in Human-Computer Interaction community.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6511840,no
A Software Framework for Supporting Ubiquitous Business Processes: An ANSI/ISA-95 Approach,"Nowadays, organizations to survive competitively they need to be, innovative and efficient. The way the Internet has been expanding along with other technological changes is leading us to a future in which all the objects that surround us will be seamlessly integrated into information networks. The possibility to implement concepts related with the ubiquitous computing in the business process-level will influence how they are designed, structured, monitored, and managed. One of the most remarkable possibilities of ubiquitous computing can be the real-time monitoring of a particular business process: it should be possible to analyze the flow of materials and information, identify possible points of failure or improve energetic efficiency with a small delay on they occur in reality. Currently, there is no direct and automated link between ubiquitous business processes descriptions and their physical executions which, frequently, promotes the occurrence of a discrepancy between the planned modes of operation and the executed ones. The ubiquitous business processes will enable a narrowing between the real (objects) and virtual (models) world and the possibility to create adaptive business processes that can predict failures, adapting themselves to changes in the environment is an attractive challenge. In this PhD thesis, we will propose a new software framework to monitor real-time executions of ubiquitous industrial business processes.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6511843,no
"Modeling Organizational Information System Architecture Using """"Complex Networks"""" Concepts","Organizations live in a world where interdependence, self-organization and emergence are factors for agility, adaptability and flexibility plunged into networks. Software-based information systems go into a service oriented architecture direction and the same goes to Infrastructures where services are become structures available in networks. Inspired into empirical studies of networked systems such as Internet, social networks, and biological networks, researchers have in recent years developed a variety of techniques and models to help us structurally understand or predict the behavior of these systems. Those findings are characterized by been supported on the """"complex networks"""" concepts. On this PhD research we present the use of the concepts of complex networks from physics to develop organizational information system architectural models, as requirements modeling technique. The research is about the structure and function of networks and its use for modeling organizational information systems architectures by using a combination of empirical methods, analysis, and computer simulations.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6511844,no
Human perceived quality-of-service for multimedia applications,"Usage of multimedia applications is increasing day by day in human social life, medical, military and businesses. Providing the expected quality of service (QoS) to end users is a challenging work for multimedia experts and companies in high technology environment. Delivering the good quality of services (QoS) to end users in distributed systems is based on multiple reliable components of software and hardware. Furthermore, human perceived quality of services are also depended on multiple factors such as multimedia application platform, middleware components and quality of service component architectures. This work addresses the quality of service components architecture particularly assessing the quality of service as perceived by end users. This paper discussed major components of QoS architecture in sense of perceiving the quality to end user and described the role of objective and subjective methods. For testing the quality-of-service for multimedia application, this paper evaluated subjective and objective methods and selected the successfully subjective method for assessing the video quality by end users. Furthermore, described the experiment results and data which collected for assessing perceived quality of multimedia by end users.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6512435,no
Aspect-oriented software for testability,"AOP is recently popular for effective technique in modularizing the crosscutting concerns such as exception handling, fault tolerance, error handling and reusability. Modularizing crosscutting concerns has a great impact on testability of software. Testability of software is the degree to facilitate testing in a given test context and ease reveling of faults. Controllability and observability are the important measures of testing non-functional requirements of software. To test software requires controlling the input and observing the output. Controllability provides a concept of probability to handle the software's input (the internal state) while observability is to observe the output for certain input. This paper presents an overview of the use of aspect-oriented programming (AOP) for facilitating controllability to ease testability of object-oriented software, and simulation of well-mixed biochemical systems.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6512950,no
A Study on the Validation of Histogram Equalization as a Contrast Enhancement Technique,"Our study uncovers that histogram equalization (HE) - in a striking contrast to it's claim - is not related to enhancement of contrast. To understand this view, we start with real world images which have varying degree of image quality that almost invariably require processing to improve image contrast. For this purpose, histogram equalization including its variants is a frequently relied upon technique. HE processes image by calculating pixel density of its constituent gray levels. This mathematical model, described by HE, is neither linked to contrast nor is contrast directly included in HE equations. Therefore, the study aims to find out the factual nature of transformation functions used by HE. To understand these mathematical calculations thoroughly, the paper dismantles HE into it's building blocks. These blocks are, then, critically analyzed to understand the true relationship between HE fundamentals and contrast. This analysis' determines that HE manipulates density - not contrast - which, in turn, achieves density changes but no contrast enhancement. Hence the study concludes that HE is not a valid contrast enhancement technique.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6516361,no
Automatic Generation of On-Line Test Programs through a Cooperation Scheme,"Test programs for Software-based Self-Test (SBST) can be exploited during the mission phase of microprocessor-based systems to periodically assess hardware integrity. However, several additional constraints must be imposed due to the coexistence of test programs with the mission application. This paper proposes a method for the generation of SBST on-line test programs for embedded RISC processors, systems where the impact of on-line constraints is significant. The proposed strategy exploits an evolutionary optimizer that is able to create a complete test set of programs relying on a new cooperative scheme. Experimental results showed high fault coverage values on two different modules of a MIPS-like processor core. These two case studies demonstrate the effectiveness of the technique and the low human effort required for its implementation.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6519728,no
Software testing suite prioritization using multi-criteria fitness function,"Regression testing is the process of validating modifications introduced in a system during software maintenance. It is an expensive, yet an important process. As the test suite size is very large, system retesting consumes large amount of time and computing resources. Unfortunately, there may be insufficient resources to allow for the re-execution of all test cases during regression testing. Testcase prioritization techniques aim to improve the effectiveness of regression testing, by ordering the testcases so that the most beneficial are executed first with higher priority. The objective of test case prioritization is to detect faults as early as possible. An approach for automating the test case prioritization process using genetic algorithm with Multi-Criteria Fitness function is presented. It uses multiple control flow coverage metrics. These metrics measure the degree of coverage of conditions, multiple conditions and statements that the test case covers. Theses metrics are weighted by the number of faults revealed and their severity. The proposed Multi-criteria technique showed superior results compared to similar work.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6523563,no
Bug fix-time prediction model using naive Bayes classifier,"Predicting bug fix-time is an important issue in order to assess the software quality or to estimate the time and effort needed during the bug triaging. Previous work has proposed several bug fix-time prediction models that had taken into consideration various bug report attributes (e.g. severity, number of developers, dependencies) in order to know which bug to fix first and how long it will take to fix it. Our aim is to distinguish the very fast and the very slow bugs in order to prioritize which bugs to start with and which to exclude at the mean time respectively. We used the data of four systems taken from three large open source projects Mozilla, Eclipse, Gnome. We used naive Bayes classifier to compute our prediction model.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6523564,no
Updated Schneidewind model with single change-point by geometrical method,"Many software reliability growth models assume that all faults have equal probability of being detected during the software testing process and the rate remains constant over the intervals between faults occurrences. But in fact, the fault detection rate may depend on the fault discovery efficiency, the fault detection density, the testing-effort, the inspection rate and so on, and may change with the software requirement and testing team. Change due to variations in resource allocation, defect density, running environment and testing strategy is called change-point. In this paper, we propose a updated Schneidewind model with a single change-point by geometrical method, and the experiment has been done to prove the method can be improve the reliability precision to some degree. At the same time, in this paper, we elaborate on the merits and limitations.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6526166,no
CloudAssoc: A pipeline for imputation based genome wide association study on cloud,"Genome wide association study (GWAS) has been proved to be an efficient approach to identify susceptibility genes for complex diseases. In order to increase the power for detecting the disease causal variants, imputation has been used to predict genotype dosages of untyped variants on the basis of linkage disequilibrium evaluated by public data. However, as the volume of data grows, time-consuming of imputation based association study becomes extremely large. We developed a cloud based pipeline to implement data format conversion, imputation, quality control and association study based on Map/Reduce framework which can aid biologists to accelerate the identification and evaluation of susceptibility genes for complex diseases and make it easier to combine GWAS data from worldwide for meta analysis.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6526190,no
A solution for detecting the congestion in networks,"Congestion in networks is one of the main factors adversely affect the quality of communication among networks, detecting congestion is the prerequisite to solve it. In this paper, a type of software sensor which has been designed and implemented can detect the levels of congestion, monitor the current network conditions. Meanwhile, it's very flexible because the key algorithm can be transplanted in a bid to provide a basis for wide application. Finally, simulation results support the conclusion that the software sensor has precise detection performance and future prospects.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6526265,no
Bayesian network-based exception handling for web service composition,"Nowadays, there are more and more web services published on the internet and the new application systems can be constructed by the composition of such web services. It's a trend and novel way for enterprises to build dynamic needs in modern markets, which has won a lot of increasing attentions in all fields. However, the application system composed by web services is easy to cause various failures due to the uncontrollable web services and the network instability. In order to guarantee the quality of the web service composition, this paper presents a method of exception handling based on the improved Bayesian network. The relationships between web services and Bayesian network is constructed as the topological structure during the process of web services composition. For getting better performance, the traditional Bayesian network algorithm is improved by modifying parameter setting for determining the prior probability of service nodes and the conditional probability of service output node. The detail of Bayesian Network-based exception handling for web service composition is given and analyzed. An experiment is also performed and the results show the proposed method is feasible and effective.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6526354,no
Applicability and benefits of mutation analysis as an aid for unit testing,"Unit testing is a highly popular and widely practiced measure for assuring software quality. Nevertheless, writing good unit tests requires experience in test design and in applying the testing frameworks. Hence, existing unit test suites often suffer from issues that limit their defect detection capability or that impact the understandability and maintainability of the implemented tests. Several methods and techniques have been proposed to aid the developer in creating good unit tests. Mutation analysis is one of the most promising techniques to assess the quality of a test suite. Over the last years it has caught increasing attention by researchers and practitioners and a variety of tools have been developed to support this technique. In this work, mutation analysis is studied for its practical applicability and the potential benefits in providing guidance for unit testing. Five mutation analysis tools are investigated on four test suites representing different levels of test quality. The results show that the applied tools have reached an acceptable level of maturity although practical application still uncovers technical limitations. Furthermore, the study results indicate that the implemented mutation operators allow a good approximation of the actual quality of a test suite and an advantage over conventional code coverage measures when a comprehensive set of mutation operators has been implemented.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6530466,no
An approach to predict software project success by cascading clustering and classification,"Generation of successful project is the core challenge of the day. Prediction of software project success is therefore one of the vital activities of software engineering community. Data mining techniques enable one to predict the success of the company by estimating the degree of success of their projects. This paper presents an empirical study of several projects developed at various software industries in order to comprehend the effectiveness of data mining technique for efficient project management. The paper provides K-means clustering approach for grouping of projects based on project success as one of the parameters. Subsequently, different classification algorithms are trained on the result set to build the classifier model based on K-fold cross validation. The best accuracy for the given dataset is achieved in Random Forest algorithm compared to other classifiers. This mode of project management using effective data mining techniques on empirical projects ensures accurate prediction of project success rate of the company. It further reflects process maturity leading towards implementation of strategies for improved productivity and sustainability of the company in the industrial market.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6549301,no
CMMI for educational institutions,"We all know that Education constitutes one of the important foundations of any society. There are various levels of teaching institutes from the Ivy League to ordinary colleges. This gradation is a very informal and there are no formal mechanisms to classify (assess) and improve the standards in an institution. There are bodies like Washington Accord (International Engineering Alliance, www.washingtonaccord.org) which consists of six international agreements governing mutual recognition of engineering qualifications and professional competence. There are other National bodies like NBA (National Board of Accreditation - India) set up in September 1994, for the purpose of assessment of Quality and Accreditation of Technical programmes in India. But these bodies are more like certification agencies. What we want to propose in this paper is an assessment and improvement framework for all Professional institutions like the CMMI for SW industry. In fact we propose to use the same levels, with appropriate process areas, relevant specific and common goals, work products etc. We propose to produce a complete, consistent framework along with the needed software which will be useful for any academic institution to assess themselves where they are and produce a roadmap (project plan) to go to the subsequent levels. We used some of these processes during our preparation for NBA accreditation.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6549305,no
Design and implementation of scalable DAQ software for a high-resolution PET camera,"This paper describes a new data acquisition (DAQ) program for a breast dedicated high-resolution positron emission tomography (PET) camera employing 4608 position-sensitive avalanche photodiodes (PSAPDs). The DAQ program is designed to be highly scalable to match the needs of evolving implementation as the system is built up from 1 to 4608 PSAPDs. The program features real time data quality monitoring capabilities. Energy and flood histograms of each PSAPD are monitored in real time, as well as the charge histogram for each channel. This allows the user to detect any hardware problems and configuration errors prior to the completion of experiment. In order to view flood histograms collected data needs to be corrected for pedestals. The program employs a pedestal estimation algorithm by fitting a Landau function to the histograms. It is also possible to use this algorithm to detect changes in pedestal values during data acquisition. A circular buffer supports real time data monitoring. Changes in the display settings, like photopeak windowing, require refilling the histograms. In order to do this as fast as possible, a number of events are stored in a circular buffer with data split into separate groups, one for each PSAPD. When a histogram of a certain PSAPD needs to be updated, the buffer pulls out the data of that specific PSAPD. This approach reduces the time it takes to update a histogram as opposed to reading data from disk or waiting for new events. It also prevents unnecessary data transfer between different parts of the program. Using one of the data acquisition boards developed, we tested the throughput capabilities of the program. The maximum throughput is limited by the hardware to 64Mbits/s. This throughput corresponds to roughly 228000 events per second. Due to processing overhead, real time online monitoring ability of the program is limited to 120000 events per second. During the test none of the events were dropped due to processing and plotting tas- s.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6551579,no
"Optimization of energy window and multiple event acceptance policy for PETbox4, a high sensitivity preclinical imaging tomograph","PETBox4 is a preclinical system dedicated to imaging mice. This system is composed by four detector panels, each made by a 24  50 array of 1.825  1.825  7 mm BGO crystals. The face to face crystal separation of the detectors is 5 cm, generating a 4.5  4.5  9.4 cm field of view (FOV). The result is a tomograph with a large detection solid angle, which in combination with a wide open energy window achieves high peak detection efficiency (~17%). Despite the small size of the typical imaged subjects, these design features also increase the fraction of accepted crystal and object scattered events, which reduce the overall image signal to noise ratio. In this work, we investigated the system acquisition configuration settings necessary to optimize the NEC (Noise equivalent Counts) and image quality. A Monte Carlo simulation software package was used (GATE) to investigate the different types of events detected as a function of energy window settings and multiple event acceptance policy. This was done for a realistic distribution of activity and attenuation coefficients in the PETBox4 FOV, based on emission data from an in-vivo preclinical PET image from an average sized mouse (30g). Based on the simulations, the NEC rate was optimized for a dual energy window that accepts both low (50-175 keY) and high (410-650 keY) energy events. This indicates that low energy events that are composed mostly from single crystal scatter contribute useful image information, while events in the middle of the energy spectrum (175keV-410keV), tend to include large fractions of crystal backscatter as well as object scatter and do not contribute significantly in data signal to noise ratio. As a result of the same simulations, a new policy for the acceptance of multiple events was introduced, implementing a """"KiIlAIl"""" multiple policy, further improving the NEe. Overall, these two optimization parameters improved the NEC rate by 56%, providing a signifi- ant advantage in image signal to noise ratio.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6551610,no
Polyenergetic CT sinogram generator,"Energy-sensitive X-ray detection devices operating in photon counting mode are getting growing interest since the last decade. By offering a promise of lower dosage requirements and spectroscopic analysis capabilities, they might redefine the paradigm of clinical X-ray measurements in a near future. A simulation software reproducing the data collection scheme of such detection devices was implemented. Without trying to reproduce the in-depth electronic mechanism of those devices, it is rather oriented toward the overall quality of the measured data in terms of simple detection characteristics. Build upon rugged and proven software components, the generator includes realistic material definitions with respect to energy-dependent attenuation. Projections are measured and features such as energy resolution, number of detected energy levels, counting noise statistics and data weighting schemes are taken into account. Using the proposed method, iterative image reconstructions show that classic beam-hardening related artefacts can successfully be reproduced. The proposed method is intended to be used as a tool aimed at predicting the imaging capabilities of these forthcoming energy-sensitive detection devices and to help in the design of their specifications. Being an easily parameterizable analytical tool, it will also be useful to validate the behavior of new dedicated polyenergetic reconstruction algorithms.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6551616,no
Evaluation of a novel wafer-scale CMOS APS X-ray detector for use in mammography,"The most important factors that affect the image quality are contrast, spatial resolution and noise. These factors and their relationship are quantitatively described by the Contrast-to-Noise Ratio (CNR), Signal-to-Noise Ratio (SNR), Modulation Transfer Function (MTF), Noise Power Spectrum (NPS) and Detective Quantum Efficiency (DQE) parameters. The combination of SNR, MTF and NPS determines the DQE, which represents the ability to visualize object details of a certain size and contrast at a given dose. In this study the performance of a novel large area Complementary Metal-Oxide-Semiconductor (CMOS) Active Pixel Sensor (APS) X-ray detector, called DynAMITe (Dynamic range Adjustable for Medical Imaging Technology), was investigated and compared to other three digital mammography systems (namely a) Large Area Sensor (LAS), b) Hamamatsu C9732DK, and c) Anrad SMAM), in terms of physical characteristics and evaluation of the image quality. DynAMITe detector consists of two geometrically superimposed grids: a) 2560  2624 pixels at 50 11m pitch, named Sub-Pixels (SP camera) and b) 1280  1312 pixels at 100 11m pitch, named Pixels (P camera). The X-ray performance evaluation of DynAMITe SP detector demonstrated high DQE results (0.58 to 0.64 at 0.5 Ip/mm). Image simulation based on the X-ray performance of the detectors was used to predict and compare the mammographic image quality using ideal software phantoms: a) one representing two three dimensional (3-D) breasts of various thickness and glandularity to estimate the CNR between simulated microcalcifications and the background, and b) the CDMAM 3.4 test tool for a contrast-detail analysis of small thickness and low contrast objects. The results show that DynAMITe SP detector results in high CNR and contrast-detail performance.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6551742,no
Development and assessment of statistical iterative image reconstruction for CT on a small animal SPECT/CT dual-modality system,"We developed a statistical iterative CT image reconstruction software for a newly constructed high-resolution small animal SPECT/CT dual-modality system, and assessed its performance at different radiation exposure levels. The objective of this work was to preserve or improve reconstructed image quality at either the same or reduced animal x-ray radiation exposure. The SPECT/CT system used a single detector for both the CT and SPECT modalities that consists of a micro-columnar CsI(TI) phosphor, a light image intensifier (LII) and a CCD sensor. The CT reconstruction software was based on the ordered-subset-convex (OSC) algorithm, and the system matrix was calculated through a ray-driven approach. A self-calibration method was implemented to calculate the offset of the axis of rotation (AOR), an important geometry parameter of the system. An endovascular stent was imaged to evaluate the high resolution performance of the statistical reconstructed image. A sacrificed mouse was scanned at different exposure levels to assess the effect of statistical noise on the image. The mouse studies were reconstructed with both the statistical reconstruction software and a filtered back-projection (FBP) program. The images were assessed and compared by contrast to noise ratio (CNR) in the region of interest. The images yielded by the statistical reconstruction software were artifact free and show superior noise performance to those from FBP reconstruction at different radiation exposure levels. The statistical reconstructed images with reduced exposure showed obviously higher image quality than those from FBP reconstruction at full exposure.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6551776,no
Quality metrics for optical transmission,"The quality of optical signals is a very important parameter in optical communications. Several metrics are in common use, like optical signal-to-noise power ratio (OSNR), Qfactor, error vector magnitude (EVM) and bit error ratio (BER). While the BER is the final determinant for a system, a measured raw BER is not necessarily useful to predict the final BER after soft-decision forward error correction (FEC), if the statistics of the noise leading to errors remains unknown. In this respect the EVM is superior, as it allows an estimation of the error statistics. The accuracy of the BER estimate from a measured Q-factor is impaired by the basic requirement that the relevant noise must be Gaussian. Already for plain on-off keying (OOK) signals this assumption is violated if an optical pre-amplifier is employed, and the estimated BER becomes even worse if phase modulation formats are involved. We compare various metrics analytically, by simulation, and through experiments. We employ six quadrature amplitude modulation (QAM) formats at symbol rates of 20 GBd and 25 GBd. The signals were generated by a software-defined transmitter. We conclude that for optical channels with additive Gaussian noise the EVM metric is a reliable quality measure. For nondata-aided QAM reception, BER in the range 10<sup>-6</sup>...10<sup>-2</sup> can be reliably estimated from measured EVM.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6608238,no
New method on the relays protective coordination due to presence of distributed generation,"The main purpose of this paper is to serve as a guideline for assessing the impact of distributed generation (DG) on the protection coordination of the distribution network. In particular, the paper details and presents a generalized assessment procedure for determining the impact of the integration of DG on the protection practices of distribution systems. To ensure the relevancy and the compatibility with the existing utility practices, demonstrations are carried out using ETAP, which are the commonly used software for distribution system protection planning.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6612449,no
The application of fault location system for 220kV overhead power line in China,"Reliable power supply is considered more and more important for today's infrastructure. If power line faults occur, they could give damage to the facility that fail the power supply quality, so that the fault point must be located, investigated and repaired quickly as possible. Since the length of overhead transmission line is long, say several decades to hundreds kilometers, fault location system has been employed and utilized. J-Power Systems (JPS) has provided the fault location system for overhead power transmission line since early 1980s, which locates the fault point based on the distribution pattern of the induced current detected by the current sensors on the ground wire. By applying the combination of EMTP (Electro Magnetic Transients Program) simulation and the neural network pattern recognizing technology, the located fault point is achieved narrower than the sensor to sensor section. Typical fault location system consists of one Master Station (MS) which has a personal computer with fault location software and communication device to communicate with Local Stations (LS), several LS installed on tower which are driven by solar panel and battery, incorporate processing circuit boards and communication device to communicate with MS, and several current sensors which are installed on ground wire and whose signal cables are connected to LS. Except for few cases JPS has not provided the fault location system in overseas. Recently the authors achieved to apply the fault location system to Chinese power supply network, by studying and considering to make the system improved to adapt the local material, local telecommunication network, local installation procedure, and so on. This paper describes the fault location system for 220kV overhead power line in China, designed, manufactured and implemented by authors.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6615038,no
Multi-sensor information fusion for unmanned cars using radar map,"Safety is the foremost quality to unmanned cars. In order to ensure the safety, unmanned cars must percept the surrounding environment precisely and exhaustively. To achieve this, various sensors including camera, lidar, and radar are equipped with unmanned cars. While the environment perception algorithms are relatively mature, there is no general solution for the multi-sensor information fusion for unmanned cars. In this article, we present a solution for multi-sensor information fusion for unmanned cars using radar map. With this solution, different environment information detected by various sensors can fuse naturally in radar map. Besides, the radar map is essentially a matrix and can be easily stored in memory. Experiment results show that radar map works well in all road conditions. And software development practices for unmanned cars also show that radar map can provide well support to decision-making, path planning and other subsequent sections.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6664567,no
T2M and M2T Transformations between Software Processes and Software Process Models,"Formalizing software processes allows process engineers to analyze, simulate, evolve and manipulate them with different purposes. Eclipse Process Framework Composer is a free tool for formalizing SPEM 2.0 processes, it provides primitives for process definition and allows to graphically visualize processes. This user friendliness is crucial for practitioners to both follow the designed process and to validate it. For the last two years we have been working in tailoring software process models, i.e. configuring general processes to particular contexts in order to make them more productive. However, it is hard for end users to validate the results and follow the process if it is in its model format, i.e., xmi. Moreover, manual translation is hard and error prone. In this paper we present a set of projectors: an injector that transforms the xml description of the process into xmi, and a extractor that transforms the xmi description of the configured process into the xml epresentation needed for its visualization.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6694071,no
Evaluating a Methodology to Establish Usability Heuristics,"Assessing usability in any software product may be a key factor for predicting its success or fail. Heuristic evaluation is the most commonly used usability evaluation method. It uses a set of recognized usability design principles (heuristics). Until now, the Nielsen's ten usability heuristics have been widely used. However, such heuristics are too general and currently it is necessary to provide new sets of heuristics for evaluating specific kinds of applications. Through a survey, applied to a group of researchers, it was possible to analyze the pertinence of formalizing a methodology for establishing specific usability heuristics that could improve usability assessments.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6694073,no
An effective error concealment method based on abrupt scene change detection algorithm,"Compressed video bit-streams are extremely sensitive to packet loss over error-prone channels. Error concealment (EC) has been considered as one of error control techniques to improve the reconstructed picture quality against transmission errors. However, EC methods show poor image reconstruction performance due to unavailable scene change information of the video sequence, especially when an abrupt scene change occurs. In this paper, we propose an effective EC method based on scene change detection algorithm (SCDA), which provides information to decide whether spatial or temporal EC is better to be used for intra and inter frames respectively. The simulation results show that the proposed method highly improves the subjective quality of incorrectly decoded frames and obtains an average gain of 0.7dB compared with the H.264/AVC Joint Model reference software.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6755830,no
Integrative software design for reliability: Beyond models and defect prediction,"The prevalence of software as an integral part of almost anything is commonplace today among the products, systems, applications, services and/or solutions with which we all interact. Software reliability directly impacts both the user experience and field operating costs. A key challenge is for the product developer to accurately predict and improve software field reliability in terms of attributes such as outage frequency and duration, prior to software delivery. Unfortunately, the actual field performance data does not often correlate with the predictions. Markov analysis and software reliability growth models are necessary key steps, but are insufficient to fully address customer-perceived reliability issues. Can we make better use of field performance data to improve our predictive models and tune our test strategies from one release to the next? In this paper, we describe an integrative software reliability framework that attempts to close the gap between expectations, predictions, and the actual behavior of systems by leveraging key best-in-class practices, tools, and techniques for software reliability analysis, modeling and predicting software field performance in terms of outage frequency/duration; tracking and monitoring test defect data and defects not fixed in subsequent releases over the software lifecycle, and validating prediction results with the actual field data. The proposed approach is designed to achieve customer expectations for software reliability and help assure the delivery of highly reliable software products.  2012 Alcatel-Lucent.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6770186,no
Intelligent systems based in hospital database malfunction scenarios,"Databases are indispensable for everyday tasks in organizations, particularly in healthcare units. Databases archive important and confidential information about patient's clinical status. Therefore, they must always be available, reliable and at high performance level. In healthcare units, fault tolerant systems ensure the availability, reliability and disaster recovery of data. However, these mechanisms do not allow taking preventive actions in order to avoid fault occurrence. In this context, it emerges the necessity of developing a fault prevention system. It can predict database malfunction in advance and provides early decision taken to solve problems. The objectives of this paper are: monitoring database performance and adapt a forecasting model used in medicine (MEWS) to the database context. Based on mathematical tools it was created a scale that assesses the severity of abnormal situations. In this way, it is possible to define the scenarios where database symptoms must trigger alerts and assistance request.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6837859,no
Poster abstract: Direct multi-hop time synchronization with constructive interference,"Multi-hop time synchronization in wireless sensor networks (WSNs) is often time-consuming and error-prone due to random time-stamp delays for MAC layer access and unstable clocks of intermediate nodes. Constructive interference (CI), a recently discovered physical layer phenomenon, allows multiple nodes transmit and forward an identical packet simultaneously. By leveraging CI, we propose direct multihop (DMH) time synchronization by directly utilizing the time-stamps from the sink node instead of intermediate n-odes, which avoids the error caused by the unstable clock of intermediate nodes. DMH doesn't need decode the flooding time synchronization beacons. Moreover, DMH explores the linear regression technique in CI based time synchronization to counterbalance the clock drifts due to clock skews.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6920981,no
Modeling software failures using neural network,"Failure-free software is a major concern for delivering high-quality system. High reliable software system requires robust modeling techniques to estimate the probability of the software failures over a period of time. In this paper, we have proposed a neural network based approach for predicting software failures. This paper presents the use of Feedforward neural network, mostly adopted by many researchers for reliability prediction [12][13][14] and Elman neural network. This experiment was conducted on real software failure dataset for three different applications. We have used various error metrics such as MSE, RMSE, NRMSE, MAE, MAPE, MMRE and BRE for the performance analysis of Feedforward (Universally adopted) and Elman neural network. Experimental results show that Elman neural network has good predictive capability. Mean Square Error (MSE), Mean absolute Error (MAE) and Mean Absolute Percentage Error (MAPE) are both reduced significantly for Elman Neural Network in comparison to Feedforward network.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7087794,no
High Speed Format Converter with Intelligent Quality Checker for File-Based System,"NHK broadcasting is shifting to file-based systems for its TV production and playout systems including VTRs and editing machines. A variety of codecs and Material eXchange Format (MXF) formats for broadcast equipment have been adopted. These include MPEG-2/AVC and OP1a/OP-Atom. Video files need to be converted into the selected codec and format to operate efficiently. The quality of video and audio must be checked during this conversion process because degradation and noise may occur.  This paper describes equipment that can quickly convert files to multiple formats as well as intelligently check the quality of video and audio during the conversion. The equipment automatically adjusts thresholds to detect anomalies in the video quality check, depending on the type of codec and the spatial frequency of each area. This can be done in less time than the actual video duration by optimizing the processing software.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7269456,no
Planning and Managing the Cost of Compromise for AV Retention and Access,"Long-term retention and access to audiovisual (AV) assets as part of a preservation strategy inevitably involve some form of compromise in order to achieve acceptable levels of cost, throughput, quality, and many other parameters. Examples include quality control and throughput in media transfer chains; data safety and accessibility in digital storage systems; and service levels for ingest and access for archive functions delivered as services. We present new software tools and frameworks developed in the PrestoPRIME project that allow these compromises to be quantitatively assessed, planned, and managed for file-based AV assets. Our focus is how to give an archive an assurance that when they design and operate a preservation strategy as a set of services, it will function as expected and will cope with the inevitable and often unpredictable variations that happen in operation. This includes being able to do cost projections, sensitivity analysis, simulation of disaster scenarios, and to govern preservation services using service-level agreements and policies.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7311109,no
On the optimized generation of Software-Based Self-Test programs for VLIW processors,"Software-Based Self-Test (SBST) approaches have shown to be an effective solution to detect permanent faults, both at the end of the production process, and during the operational phase. However, when Very Long Instruction Word (VLIW) processors are addressed these techniques require some optimization steps in order to properly exploit the parallelism intrinsic in these architectures. In this paper we present a new method that, starting from previously known algorithms, automatically generates an effective test program able to still reach high fault coverage on the VLIW processor under test, while reducing the test duration and the test code size. The method consists of three parametric phases and can deal with different VLIW processor models. The main goal of the proposed method is to automatically obtain a test program able to effectively reduce the test time and the required resources. Experimental results gathered on a case study show the effectiveness of the proposed approach.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7332089,no
Case Study in Computer Design,This chapter contains sections titled: <br> Design Principles <br> Design Decisions <br> Identification of System Elements <br> Architectural Design <br> Test Strategies <br> Fault Detection and Correction <br> CRC <br> Sequence Analysis <br> Sequence Probability and Sequence Response Time Predictions and Analysis <br> Sequence Failure Rate <br> Reliability <br> Detailed Design <br> Summary <br> References,2012,http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6169019.pdf&bkn=6168884&pdfType=chapter,no
Network Reliability and Availability Metrics,This chapter contains sections titled: <br> Introduction <br> Model Development <br> Probability of Failure Analysis Results <br> Fault and Failure Correction Analysis Results <br> Remaining Failures Analysis Results <br> Reliability Analysis Results <br> Availability Analysis Results <br> Another Perspective on Probability of Failure <br> Measuring Prediction Accuracy <br> Methods for Improving Reliability <br> Summary of Results <br> Summary <br> References,2012,http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6169025.pdf&bkn=6168884&pdfType=chapter,no
Reflections on the NASA MDP data sets,"The NASA metrics data program (MDP) data sets have been heavily used in software defect prediction research. Aim: To highlight the data quality issues present in these data sets, and the problems that can arise when they are used in a binary classification context. Method: A thorough exploration of all 13 original NASA data sets, followed by various experiments demonstrating the potential impact of duplicate data points when data mining. Conclusions: Firstly researchers need to analyse the data that forms the basis of their findings in the context of how it will be used. Secondly, the bulk of defect prediction experiments based on the NASA MDP data sets may have led to erroneous findings. This is mainly because of repeated/duplicate data points potentially causing substantial amounts of training and testing data to be identical.",2012,http://ieeexplore.ieee.org/document/6353339/,yes
Transfer learning for cross-company software defect prediction,"Context: Software defect prediction studies usually built models using within-company data, but very few focused on the prediction models trained with cross-company data. It is difficult to employ these models which are built on the within-company data in practice, because of the lack of these local data repositories. Recently, transfer learning has attracted more and more attention for building classifier in target domain using the data from related source domain. It is very useful in cases when distributions of training and test instances differ, but is it appropriate for cross-company software defect prediction? Objective: In this paper, we consider the cross-company defect prediction scenario where source and target data are drawn from different companies. In order to harness cross company data, we try to exploit the transfer learning method to build faster and highly effective prediction model. Method: Unlike the prior works selecting training data which are similar from the test data, we proposed a novel algorithm called Transfer Naive Bayes (TNB), by using the information of all the proper features in training data. Our solution estimates the distribution of the test data, and transfers cross-company data information into the weights of the training data. On these weighted data, the defect prediction model is built. Results: This article presents a theoretical analysis for the comparative methods, and shows the experiment results on the data sets from different organizations. It indicates that TNB is more accurate in terms of AUC (The area under the receiver operating characteristic curve), within less runtime than the state of the art methods. Conclusion: It is concluded that when there are too few local training data to train good classifiers, the useful knowledge from different-distribution training data on feature level may help. We are optimistic that our transfer learning method can guide optimal resource allocation strategies, which may reduce software testing cost and increase effectiveness of software testing process.",2012,https://pdfs.semanticscholar.org/dafc/c5d8fdc84d76b771c3b7ad07cd6c0b981d54.pdf,yes
User preferences based software defect detection algorithms selection using MCDM,"A variety of classification algorithms for software defect detection have been developed over the years. How to select an appropriate classifier for a given task is an important issue in Data mining and knowledge discovery (DMKD). Many studies have compared different types of classification algorithms and the performances of these algorithms may vary using different performance measures and under different circumstances. Since the algorithm selection task needs to examine several criteria, such as accuracy, computational time, and misclassification rate, it can be modeled as a multiple criteria decision making (MCDM) problem. The goal of this paper is to use a set of MCDM methods to rank classification algorithms, with empirical results based on the software defect detection datasets. Since the preferences of the decision maker (DM) play an important role in algorithm evaluation and selection, this paper involved the DM during the ranking procedure by assigning user weights to the performance measures. Four MCDM methods are examined using 38 classification algorithms and 13 evaluation criteria over 10 public-domain software defect datasets. The results indicate that the boosting of CART and the boosting of C4.5 decision tree are ranked as the most appropriate algorithms for software defect datasets. Though the MCDM methods provide some conflicting results for the selected software defect datasets, they agree on most top-ranked classification algorithms.",2012,http://www.sciencedirect.com/science/article/pii/S0020025510001751,yes
Effective Software Fault Localization Using an RBF Neural Network,"We propose the application of a modified radial basis function neural network in the context of software fault localization, to assist programmers in locating bugs effectively. This neural network is trained to learn the relationship between the statement coverage information of a test case and its corresponding execution result, success or failure. The trained network is then given as input a set of virtual test cases, each covering a single statement. The output of the network, for each virtual test case, is considered to be the suspiciousness of the corresponding covered statement. A statement with a higher suspiciousness has a higher likelihood of containing a bug, and thus statements can be ranked in descending order of their suspiciousness. The ranking can then be examined one by one, starting from the top, until a bug is located. Case studies on 15 different programs were conducted, and the results clearly show that our proposed technique is more effective than several other popular, state of the art fault localization techniques. Further studies investigate the robustness of the proposed technique, and illustrate how it can easily be applied to programs with multiple bugs as well.",2012,http://ieeexplore.ieee.org/document/6058639/?arnumber=6058639,yes
Software fault prediction based on grey neural network,"Considering determining the number of software fault is an uncertain non-linear problem with only small sample, a novel software fault prediction method based on grey neural network is put forward. Firstly, constructing the grey neural network topological structure according the small sample sequence is necessary, and then the network learning algorithm is discussed. Finally, the grey neural network prediction model based on the grey theory and artificial neural network is proposed. The sample fault sequences of some software project are used to verify the precision of this method. Comparison with GM(1,1), the proposed model can reduce the prediction relative error effectively.",2012,http://ieeexplore.ieee.org/document/6234505/?arnumber=6234505,yes
Antecedence Graph Approach to Checkpointing for Fault Tolerance in Mobile Agent Systems,"The flexibility offered by mobile agents is quite noticeable in distributed computing environments. However, the greater flexibility of the mobile agent paradigm compared to the client/server computing paradigm comes at an additional threats since agent systems are prone to failures originating from bad communication, security attacks, agent server crashes, system resources unavailability, network congestion, or even deadlock situations. In such events, mobile agents either get lost or damaged (partially or totally) during execution. In this paper, we propose parallel checkpointing approach based on the use of antecedence graphs for providing fault tolerance in mobile agent systems. During normal computation message transmission, the dependency information among mobile agents is recorded in the form of antecedence graphs by participating mobile agents of mobile agent group. When a checkpointing procedure begins, the initiator concurrently informs relevant mobile agents, which minimizes the identifying time. The proposed scheme utilizes the checkpointed information for fault tolerance which is stored in form of antecedence graphs. In case of failures, using checkpointed information, the antecedence graphs and message logs are regenerated for recovery and then normal operation continued. Moreover, compared with the existing schemes, our algorithm involves the minimum number of mobile agents during the identifying and checkpoiting procedure, which leads to the improvement of the system performance. In addition, the proposed algorithm is a domino-free checkpointing algorithm, which is especially desirable for mobile agent systems. Quantitative analysis and experimental simulation show that our algorithm outperforms other coordinated checkpointing schemes in terms of the identifying time and the number of blocked mobile agents and then can provide a better system performance. The main contribution of the proposed checkpointing scheme is the enhancement of graph-based ap- roach in terms of considerable improvement by reducing message overhead, execution, and recovery times.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6109235,no
Empirical Principles and an Industrial Case Study in Retrieving Equivalent Requirements via Natural Language Processing Techniques,"Though very important in software engineering, linking artifacts of the same type (clone detection) or different types (traceability recovery) is extremely tedious, error-prone, and effort-intensive. Past research focused on supporting analysts with techniques based on Natural Language Processing (NLP) to identify candidate links. Because many NLP techniques exist and their performance varies according to context, it is crucial to define and use reliable evaluation procedures. The aim of this paper is to propose a set of seven principles for evaluating the performance of NLP techniques in identifying equivalent requirements. In this paper, we conjecture, and verify, that NLP techniques perform on a given dataset according to both ability and the odds of identifying equivalent requirements correctly. For instance, when the odds of identifying equivalent requirements are very high, then it is reasonable to expect that NLP techniques will result in good performance. Our key idea is to measure this random factor of the specific dataset(s) in use and then adjust the observed performance accordingly. To support the application of the principles we report their practical application to a case study that evaluates the performance of a large number of NLP techniques for identifying equivalent requirements in the context of an Italian company in the defense and aerospace domain. The current application context is the evaluation of NLP techniques to identify equivalent requirements. However, most of the proposed principles seem applicable to evaluating any estimation technique aimed at supporting a binary decision (e.g., equivalent/nonequivalent), with the estimate in the range [0,1] (e.g., the similarity provided by the NLP), when the dataset(s) is used as a benchmark (i.e., testbed), independently of the type of estimator (i.e., requirements text) and of the estimation method (e.g., NLP).",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6112783,no
On Fault Representativeness of Software Fault Injection,"The injection of software faults in software components to assess the impact of these faults on other components or on the system as a whole, allowing the evaluation of fault tolerance, is relatively new compared to decades of research on hardware fault injection. This paper presents an extensive experimental study (more than 3.8 million individual experiments in three real systems) to evaluate the representativeness of faults injected by a state-of-the-art approach (G-SWFIT). Results show that a significant share (up to 72 percent) of injected faults cannot be considered representative of residual software faults as they are consistently detected by regression tests, and that the representativeness of injected faults is affected by the fault location within the system, resulting in different distributions of representative/nonrepresentative faults across files and functions. Therefore, we propose a new approach to refine the faultload by removing faults that are not representative of residual software faults. This filtering is essential to assure meaningful results and to reduce the cost (in terms of number of faults) of software fault injection campaigns in complex software. The proposed approach is based on classification algorithms, is fully automatic, and can be used for improving fault representativeness of existing software fault injection approaches.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6122035,no
Multilevel Diskless Checkpointing,"Extreme scale systems available before the end of this decade are expected to have 100 million to 1 billion CPU cores. The probability that a failure occurs during an application execution is expected to be much higher than today's systems. Counteracting this higher failure rate may require a combination of disk-based checkpointing, diskless checkpointing, and algorithmic fault tolerance. Diskless checkpointing is an efficient technique to tolerate a small number of process failures in large parallel and distributed systems. In the literature, a simultaneous failure of no more than N processes is often tolerated by using a one-level Reed-Solomon checkpointing scheme for N simultaneous process failures, whose overhead often increases quickly as N increases. We introduce an N-level diskless checkpointing scheme that reduces the overhead for tolerating a simultaneous failure of up to N processes. Each level is a diskless checkpointing scheme for a simultaneous failure of i processes, where i = 1, 2,..., N. Simulation results indicate the proposed N-level diskless checkpointing scheme achieves lower fault tolerance overhead than the one-level Reed-Solomon checkpointing scheme for N simultaneous processor failures.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6127862,no
Systematic Elaboration of Scalability Requirements through Goal-Obstacle Analysis,"Scalability is a critical concern for many software systems. Despite the recognized importance of considering scalability from the earliest stages of development, there is currently little support for reasoning about scalability at the requirements level. This paper presents a goal-oriented approach for eliciting, modeling, and reasoning about scalability requirements. The approach consists of systematically identifying scalability-related obstacles to the satisfaction of goals, assessing the likelihood and severity of these obstacles, and generating new goals to deal with them. The result is a consolidated set of requirements in which important scalability concerns are anticipated through the precise, quantified specification of scaling assumptions and scalability goals. The paper presents results from applying the approach to a complex, large-scale financial fraud detection system.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6152130,no
Automated Behavioral Testing of Refactoring Engines,"Refactoring is a transformation that preserves the external behavior of a program and improves its internal quality. Usually, compilation errors and behavioral changes are avoided by preconditions determined for each refactoring transformation. However, to formally define these preconditions and transfer them to program checks is a rather complex task. In practice, refactoring engine developers commonly implement refactorings in an ad hoc manner since no guidelines are available for evaluating the correctness of refactoring implementations. As a result, even mainstream refactoring engines contain critical bugs. We present a technique to test Java refactoring engines. It automates test input generation by using a Java program generator that exhaustively generates programs for a given scope of Java declarations. The refactoring under test is applied to each generated program. The technique uses SafeRefactor, a tool for detecting behavioral changes, as an oracle to evaluate the correctness of these transformations. Finally, the technique classifies the failing transformations by the kind of behavioral change or compilation error introduced by them. We have evaluated this technique by testing 29 refactorings in Eclipse JDT, NetBeans, and the JastAdd Refactoring Tools. We analyzed 153,444 transformations, and identified 57 bugs related to compilation errors, and 63 bugs related to behavioral changes.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6175911,no
Toward Comprehensible Software Fault Prediction Models Using Bayesian Network Classifiers,"Software testing is a crucial activity during software development and fault prediction models assist practitioners herein by providing an upfront identification of faulty software code by drawing upon the machine learning literature. While especially the Naive Bayes classifier is often applied in this regard, citing predictive performance and comprehensibility as its major strengths, a number of alternative Bayesian algorithms that boost the possibility of constructing simpler networks with fewer nodes and arcs remain unexplored. This study contributes to the literature by considering 15 different Bayesian Network (BN) classifiers and comparing them to other popular machine learning techniques. Furthermore, the applicability of the Markov blanket principle for feature selection, which is a natural extension to BN theory, is investigated. The results, both in terms of the AUC and the recently introduced H-measure, are rigorously tested using the statistical framework of Demsar. It is concluded that simple and comprehensible networks with less nodes can be constructed using BN classifiers other than the Naive Bayes classifier. Furthermore, it is found that the aspects of comprehensibility and predictive performance need to be balanced out, and also the development context is an item which should be taken into account during model selection.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6175912,yes
Simulating Service-Oriented Systems: A Survey and the Services-Aware Simulation Framework,"The service-oriented architecture style supports desirable qualities, including distributed, loosely coupled systems spanning organizational boundaries. Such systems and their configurations are challenging to understand, reason about, and test. Improved understanding of these systems will support activities such as autonomic runtime configuration, application deployment, and development/testing. Simulation is one way to understand and test service systems. This paper describes a literature survey of simulation frameworks for service-oriented systems, examining simulation software, systems, approaches, and frameworks used to simulate service-oriented systems. We identify a set of dimensions for describing the various approaches, considering their modeling methodology, their functionalities, their underlying infrastructure, and their evaluation. We then introduce the services-aware simulation framework (SASF), a simulation framework for predicting the behavior of service-oriented systems under different configurations and loads, and discuss the unique features that distinguish it from other systems in the literature. We demonstrate its use in simulating two service-oriented systems.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200256,no
Locating Need-to-Externalize Constant Strings for Software Internationalization with Generalized String-Taint Analysis,"Nowadays, a software product usually faces a global market. To meet the requirements of different local users, the software product must be internationalized. In an internationalized software product, user-visible hard-coded constant strings are externalized to resource files so that local versions can be generated by translating the resource files. In many cases, a software product is not internationalized at the beginning of the software development process. To internationalize an existing product, the developers must locate the user-visible constant strings that should be externalized. This locating process is tedious and error-prone due to 1) the large number of both user-visible and non-user-visible constant strings and 2) the complex data flows from constant strings to the Graphical User Interface (GUI). In this paper, we propose an automatic approach to locating need-to-externalize constant strings in the source code of a software product. Given a list of precollected API methods that output values of their string argument variables to the GUI and the source code of the software product under analysis, our approach traces from the invocation sites (within the source code) of these methods back to the need-to-externalize constant strings using generalized string-taint analysis. In our empirical evaluation, we used our approach to locate need-to-externalize constant strings in the uninternationalized versions of seven real-world open source software products. The results of our evaluation demonstrate that our approach is able to effectively locate need-to-externalize constant strings in uninternationalized software products. Furthermore, to help developers understand why a constant string requires translation and properly translate the need-to-externalize strings, we provide visual representation of the string dependencies related to the need-to-externalize strings.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6216383,no
A Second Replicated Quantitative Analysis of Fault Distributions in Complex Software Systems,"Background: Software engineering is searching for general principles that apply across contexts, for example, to help guide software quality assurance. Fenton and Ohlsson presented such observations on fault distributions, which have been replicated once. Objectives: We aimed to replicate their study again to assess the robustness of the findings in a new environment, five years later. Method: We conducted a literal replication, collecting defect data from five consecutive releases of a large software system in the telecommunications domain, and conducted the same analysis as in the original study. Results: The replication confirms results on unevenly distributed faults over modules, and that fault proneness distributions persist over test phases. Size measures are not useful as predictors of fault proneness, while fault densities are of the same order of magnitude across releases and contexts. Conclusions: This replication confirms that the uneven distribution of defects motivates uneven distribution of quality assurance efforts, although predictors for such distribution of efforts are not sufficiently precise.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6235962,no
Defect-Density Assessment in Evolutionary Product Development: A Case Study in Medical Imaging,"Defect density is the ratio between the number of defects and software size. Properly assessing defect density in evolutionary product development requires a strong tool and rigid process support that enables defects to be traced to the offending source code. In addition, it requires waiting for field defects after the product is deployed. To ease the calculation in practice, a proposed method approximates the lifetime number of defects against the software by the number of defects reported in a development period even if the defects are reported against previous product releases. The method uses aggregated code churn to measure the software size. It was applied to two development projects in medical imaging that involved three geographical locations (sites) with about 30 software engineers and 1.354 million lines of code in the released products. The results suggest the approach has some merits and validity, which the authors discuss in the distributed development context. The method is simple and operable and can be used by others with situations similar to ours.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6253198,no
Parametric Design and Performance Analysis of a Decoupled Service-Oriented Prediction Framework Based on Embedded Numerical Software,"In modern utility computing infrastructures, like grids and clouds, one of the significant actions of a service provider is to predict the resources needed by the services included in its platform in an automated fashion for service provisioning optimization. Furthermore, a variety of software toolkits exist that implement an extended set of algorithms applicable to workload forecasting. However, their automated use as services in the distributed computing paradigm includes a number of design and implementation challenges. In this paper, a decoupled framework is presented, for taking advantage of software like GNU Octave in the process of creating and using prediction models during the service life cycle of a SOI. A performance analysis of the framework is also conducted. In this context, a methodology for creating parametric or gearbox services with multiple modes of operations based on the execution conditions is portrayed and is applied to transform the aforementioned service framework to optimize service performance. A new estimation algorithm is introduced, that creates performance rules of applications as black boxes, through the creation and usage of genetically optimized artificial neural networks. Through this combination, the critical parameters of the networks are decided through an evolutionary iterative process.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6264050,no
GPS Near-Real-Time Coseismic Displacements for the Great Tohoku-oki Earthquake,"Here, we present the application to the great Tohoku-oki (Japan) earthquake (United States Geological Survey <i>M</i> = 9.0, March 11, 2011, 05:46:24 Coordinated Universal Time) of a novel approach, named Variometric Approach for Displacements Analysis Stand-Alone Engine, able to estimate accurate coseismic displacements and waveforms in real time, in the global reference frame, just using the standard broadcast products (orbits and clocks) and the high-rate (1 Hz or more) carrier phase observations continuously collected by a stand-alone global-positioning-system receiver. We processed separately the data collected at MIZU (Mizusawa, 140 km from the epicenter) and USUD (Usuda, 430 km from the epicenter) International Global Navigation Satellite System Service sites. A total horizontal displacement of about 2.4 m east-southeast was estimated for the MIZU, with a maximum horizontal oscillation amplitude of about 3.4 m along the same direction. Generally, an overall accuracy better than 10 cm for all the components (east, north, and up) and an average accuracy around 5 cm were assessed over an interval shorter than 5 min, with respect to independent solutions obtained with two different scientific software. The threshold of 5-cm accuracy has been recently indicated as sufficient for real-time fault determination for near-field tsunami forecasting for a major earthquake, like the 2011 Tohoku-oki one.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6265361,no
Master Failure Detection Protocol in Internal Synchronization Environment,"During the last decades, the wide advance in the networking technologies has allowed the development of distributed monitoring and control systems. These systems show advantages compared with centralized solutions: heterogeneous nodes can be easily integrated, new nodes can be easily added to the system, and no single point of failure. For these reasons, distributed systems have been adopted in different fields, such as industrial automation and telecommunication systems. Recently, due to technology improvements, distributed systems are also adopted in the control of power-grid and transport systems, i.e., the so-called large-scale complex critical infrastructures. Given the strict safety, security, reliability, and real-time requirements, using distributed systems for controlling such critical infrastructure demands that adequate mechanisms have to be established to share the same notion of time among the nodes. For this class of systems, a synchronization protocol, such as the IEEE 1588 standard, can be adopted. This type of synchronization protocol was designed to achieve very precise clock synchronization, but it may not be sufficient to ensure safety of the entire system. For example, instability of the local oscillator of a reference node, due to a failure of the node itself or to malicious attacks, could influence the quality of synchronization of all nodes. In recent years, a new software clock, the reliable and self-aware clock (R&SAClock), which is designed to estimate the quality of synchronization through statistical analysis, was developed and tested. This statistical instrument can be used to identify any anomalous conditions with respect to normal behavior. A careful analysis and classification of the main points of failure of IEEE 1588 standard suggests that the reference node, which is called master, is the weak point of the system. For this reason, this paper deals with the detection of faults of the reference node(s) of an of IEEE 1588 setup- This paper describes and evaluates the design of a protocol for timing failure detection for internal synchronization based on a revised version of the R&SAClock software suitably modified to cross-exploit the information on the quality of synchronization among all the nodes of the system. The experimental evaluation of this approach confirms the capability of the synchronization uncertainty, which is provided by R&SAClock, to reveal the anomalous behaviors either of the local node or of the reference node. In fact, it is shown that, through a proper configuration of the parameters of the protocol, the system is able to detect all the failures injected on the master in different experimental conditions and to correctly identify failures on slaves with a probability of 87%.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6269083,no
Self-Organized Cooperation Policy Setting in P2P Systems Based on Reinforcement Learning,"In this paper, we have developed a self-organized approach to cooperation policy setting in a system of rational peers that have only partial views of the whole system in order to improve the overall welfare as a system-wide performance metric. The proposed approach is based on distributed reinforcement learning and sets cooperation policies of the peers through their self-organized interactions. We have analyzed this approach to demonstrate that it results in Pareto optimality in the system by disseminating the local value functions of the peers among the neighbors. We have also experimentally verified that this approach outperforms the other commonly used approaches in the literature, in terms of the performance of the system.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6280692,no
Low-Cost Electronic Tongue System and Its Application to Explosive Detection,"The use of biomimetic measurement systems as electronic tongues and noses has considerably increased in the last years. This paper presents the design of a low-cost electronic tongue system that includes a software application that runs on a personal computer and electronic equipment based on a 16-b microcontroller. The designed system is able to implement voltammetry and impedance spectroscopy measurements with different-electrode configurations. The data obtained from the electrochemical measurements can be used to build statistical models able to assess physicochemical properties of the samples. The designed system has been applied to the detection and quantification of trinitrotoluene (TNT), which is one of the most common explosive materials. Pulse voltammetry measurements were carried out on TNT samples with different concentration levels. The principal component analysis of the obtained results shows that the electronic tongue is able to detect TNT in acetonitrile samples. Prediction models were built with partial least squares regression, and a good correlation was observed between the pulse voltammetry measurements and the TNT concentration levels. In this experience, a new voltammetric data compression algorithm based on polynomial approximations has been tested with good results. The electronic tongue has also been applied to the prediction of water quality parameters in wastewater and to the evaluation of different-pulse array designs for pulse voltammetry experiences.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6301715,no
Robust Dynamic Service Composition in Sensor Networks,"Service modeling and service composition are software architecture paradigms that have been used extensively in web services where there is an abundance of resources. They mainly capture the idea that advanced functionality can be realized by combining a set of primitive services provided by the system. Many efforts in web services domain focused on detecting the initial composition, which is then followed for the rest of service operation. In sensor networks, however, communication among nodes is error-prone and unreliable, while sensor nodes have constrained resources. This dynamic environment requires a continuous adaptation of the composition of a complex service. In this paper, we first propose a graph-based formulation for modeling sensor services that maps to the operational model of sensor networks and is amenable to analysis. Based on this model, we formulate the process of sensor service composition as a cost-optimization problem and show that it is NP-complete. Two heuristic methods are proposed to solve the composition problem: the top-down and the bottom-up approaches. We discuss centralized and distributed implementations of these methods. Finally, using ns-2 simulations, we evaluate the performance and overhead of our proposed methods.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6331486,no
A Remainder-Based Contention-Avoidance Scheme for Saturated Wireless CSMA Networks,"The traditional truncated-binary-exponential-backoff contention-resolution scheme can address the collision problem of carrier-sense multiple-access networks when the number of users is limited. However, its performance significantly worsens when many users join the contention or when the network is saturated. To address this issue, we propose a demand-assigned multiaccess (DAMA) time-division multiple-access (TDMA) system with a remainder-based contention-avoidance scheme. In our scheme, the wireless users are divided into several groups according to the available transmission opportunities in each time frame, and each user sends its bandwidth request during the time window for its group. According to the average collision probability and the utilization of the transmission opportunities in the last two rounds of contention resolution, the size of available transmission opportunities and the number of wireless users who issue bandwidth requests are adaptively controlled by the base station and the wireless users, respectively. Our analysis and simulation show that the proposed contention-avoidance scheme can significantly improve the performance of the entire network.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6341122,no
A large-scale empirical study of just-in-time quality assurance,"Defect prediction models are a well-known technique for identifying defect-prone files or packages such that practitioners can allocate their quality assurance efforts (e.g., testing and code reviews). However, once the critical files or packages have been identified, developers still need to spend considerable time drilling down to the functions or even code snippets that should be reviewed or tested. This makes the approach too time consuming and impractical for large software systems. Instead, we consider defect prediction models that focus on identifying defect-prone (risky) software changes instead of files or packages. We refer to this type of quality assurance activity as Just-In-Time Quality Assurance, because developers can review and test these risky changes while they are still fresh in their minds (i.e., at check-in time). To build a change risk model, we use a wide range of factors based on the characteristics of a software change, such as the number of added lines, and developer experience. A large-scale study of six open source and five commercial projects from multiple domains shows that our models can predict whether or not a change will lead to a defect with an average accuracy of 68 percent and an average recall of 64 percent. Furthermore, when considering the effort needed to review changes, we find that using only 20 percent of the effort it would take to inspect all changes, we can identify 35 percent of all defect-inducing changes. Our findings indicate that Just-In-Time Quality Assurance may provide an effort-reducing way to focus on the most risky changes and thus reduce the costs of developing high-quality software.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6341763,no
Making a Completely Blind Image Quality Analyzer,"An important aim of research on the blind image quality assessment (IQA) problem is to devise perceptual models that can predict the quality of distorted images with as little prior knowledge of the images or their distortions as possible. Current state-of-the-art general purpose no reference (NR) IQA algorithms require knowledge about anticipated distortions in the form of training examples and corresponding human opinion scores. However we have recently derived a blind IQA model that only makes use of measurable deviations from statistical regularities observed in natural images, without training on human-rated distorted images, and, indeed without any exposure to distorted images. Thus, it is completely blind. The new IQA model, which we call the Natural Image Quality Evaluator (NIQE) is based on the construction of a quality aware collection of statistical features based on a simple and successful space domain natural scene statistic (NSS) model. These features are derived from a corpus of natural, undistorted images. Experimental results show that the new index delivers performance comparable to top performing NR IQA models that require training on large databases of human opinions of distorted images. A software release is available at http://live.ece.utexas.edu/research/quality/niqe_release.zip.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6353522,no
Overcurrent and Overload Protection of Directly Voltage-Controlled Distributed Resources in a Microgrid,"This paper presents two add-on features for the voltage control scheme of directly voltage-controlled distributed energy resource units (VC-DERs) of an islanded microgrid to provide overcurrent and overload protection. The overcurrent protection scheme detects the fault, limits the output current magnitude of the DER unit, and restores the microgrid to its normal operating conditions subsequent to fault clearance. The overload protection scheme limits the output power of the VC-DER unit. Off-line digital time-domain simulation studies, in the EMTDC/PSCAD software environment, demonstrate the feasibility and desirable performance of the proposed features. Real-time case studies based on an RTDS system verifies performance of the hardware-implemented overload and overcurrent protection schemes in a hardware-in-the-loop environment.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6359912,no
Predicting Architectural Vulnerability on Multithreaded Processors under Resource Contention and Sharing,"Architectural vulnerability factor (AVF) characterizes a processor's vulnerability to soft errors. Interthread resource contention and sharing on a multithreaded processor (e.g., SMT, CMP) shows nonuniform impact on a program's AVF when it is co-scheduled with different programs. However, measuring the AVF is extremely expensive in terms of hardware and computation. This paper proposes a scalable two-level predictive mechanism capable of predicting a program's AVF on a SMT/CMP architecture from easily measured metrics. Essentially, the first-level model correlates the AVF in a contention-free environment with important performance metrics and the processor configuration, while the second-level model captures the interthread resource contention and sharing via processor structures' occupancies. By utilizing the proposed scheme, we can accurately estimate any unseen program's soft error vulnerability under resource contention and sharing with any other program(s), on an arbitrarily configured multithreaded processor. In practice, the proposed model can be used to find soft error resilient thread-to-core scheduling for multithreaded processors.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6363442,no
Trends in the Quality of Human-Centric Software Engineering Experiments--A Quasi-Experiment,"Context: Several text books and papers published between 2000 and 2002 have attempted to introduce experimental design and statistical methods to software engineers undertaking empirical studies. Objective: This paper investigates whether there has been an increase in the quality of human-centric experimental and quasi-experimental journal papers over the time period 1993 to 2010. Method: Seventy experimental and quasi-experimental papers published in four general software engineering journals in the years 1992-2002 and 2006-2010 were each assessed for quality by three empirical software engineering researchers using two quality assessment methods (a questionnaire-based method and a subjective overall assessment). Regression analysis was used to assess the relationship between paper quality and the year of publication, publication date group (before 2003 and after 2005), source journal, average coauthor experience, citation of statistical text books and papers, and paper length. The results were validated both by removing papers for which the quality score appeared unreliable and using an alternative quality measure. Results: Paper quality was significantly associated with year, citing general statistical texts, and paper length (p <; 0.05). Paper length did not reach significance when quality was measured using an overall subjective assessment. Conclusions: The quality of experimental and quasi-experimental software engineering papers appears to have improved gradually since 1993.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6374196,no
Proactive and Reactive Runtime Service Discovery: A Framework and Its Evaluation,"The identification of services during the execution of service-based applications to replace services in them that are no longer available and/or fail to satisfy certain requirements is an important issue. In this paper, we present a framework to support runtime service discovery. This framework can execute service discovery queries in pull and push mode. In pull mode, it executes queries when a need for finding a replacement service arises. In push mode, queries are subscribed to the framework to be executed proactively and, in parallel with the operation of the application, to identify adequate services that could be used if the need for replacing a service arises. Hence, the proactive (push) mode of query execution makes it more likely to avoid interruptions in the operation of service-based applications when a service in them needs to be replaced at runtime. In both modes of query execution, the identification of services relies on distance-based matching of structural, behavioral, quality, and contextual characteristics of services and applications. A prototype implementation of the framework has been developed and an evaluation was carried out to assess the performance of the framework. This evaluation has shown positive results, which are discussed in the paper.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6375710,no
C-MART: Benchmarking the Cloud,"Cloud computing environments provide on-demand resource provisioning, allowing applications to elastically scale. However, application benchmarks currently being used to test cloud management systems are not designed for this purpose. This results in resource underprovisioning and quality-of-service (QoS) violations when systems tested using these benchmarks are deployed in production environments. We present C-MART, a benchmark designed to emulate a modern web application running in a cloud computing environment. It is designed using the cloud computing paradigm of elastic scalability at every application tier and utilizes modern web-based technologies such as HTML5, AJAX, jQuery, and SQLite. C-MART consists of a web application, client emulator, deployment server, and scaling API. The deployment server automatically deploys and configures the test environment in orders of magnitude less time than current benchmarks. The scaling API allows users to define and provision their own customized datacenter. The client emulator generates the web workload for the application by emulating complex and varied client behaviors, including decisions based on page content and prior history. We show that C-MART can detect problems in management systems that previous benchmarks fail to identify, such as an increase from 4.4 to 50 percent error in predicting server CPU utilization and resource underprovisioning in 22 percent of QoS measurements.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6381404,no
Design and Implementation of an Approximate Communication System for Wireless Media Applications,"All practical wireless communication systems are prone to errors. At the symbol level, such wireless errors have a well-defined structure: When a receiver decodes a symbol erroneously, it is more likely that the decoded symbol is a good approximation of the transmitted symbol than a randomly chosen symbol among all possible transmitted symbols. Based on this property, we define approximate communication, a method that exploits this error structure to natively provide unequal error protection to data bits. Unlike traditional [forward error correction (FEC)-based] mechanisms of unequal error protection that consume additional network and spectrum resources to encode redundant data, the approximate communication technique achieves this property at the PHY layer without consuming any additional network or spectrum resources (apart from a minimal signaling overhead). Approximate communication is particularly useful to media delivery applications that can benefit significantly from unequal error protection of data bits. We show the usefulness of this method to such applications by designing and implementing an end-to-end media delivery system, called Apex. Our Software Defined Radio (SDR)-based experiments reveal that Apex can improve video quality by 5-20 dB [peak signal-to-noise ratio (PSNR)] across a diverse set of wireless conditions when compared to traditional approaches. We believe that mechanisms such as Apex can be a cornerstone in designing future wireless media delivery systems under any error-prone channel condition.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6381500,no
Modeling of Second Generation HTS Cables for Grid Fault Analysis Applied to Power System Simulation,"HTS power cable systems are an emerging technology aimed at competing with XLPE cable systems. Knowledge on the thermal operating conditions of HTS power devices is needed to estimate their availability when connected to a power system, because the HTS material must remain below its critical temperature to transport current. In this work, a simple finite difference method is used to assess the temperature distribution at certain cross-section of a second-generation coaxial HTS cable. This method has been implemented in MATLAB and its proper functioning has been verified with the software package FLUX. This method is a tool to establish temperature distributions among HTS cable layers under normal operating conditions. Additionally, the aim of this work is to serve as basis for future simulations including heat generation changes within the cable layers typically caused by grid fault events.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6395813,no
Monitor-Based Instant Software Refactoring,"Software refactoring is an effective method for improvement of software quality while software external behavior remains unchanged. To facilitate software refactoring, a number of tools have been proposed for code smell detection and/or for automatic or semi-automatic refactoring. However, these tools are passive and human driven, thus making software refactoring dependent on developers' spontaneity. As a result, software engineers with little experience in software refactoring might miss a number of potential refactorings or may conduct refactorings later than expected. Few refactorings might result in poor software quality, and delayed refactorings may incur higher refactoring cost. To this end, we propose a monitor-based instant refactoring framework to drive inexperienced software engineers to conduct more refactorings promptly. Changes in the source code are instantly analyzed by a monitor running in the background. If these changes have the potential to introduce code smells, i.e., signs of potential problems in the code that might require refactorings, the monitor invokes corresponding smell detection tools and warns developers to resolve detected smells promptly. Feedback from developers, i.e., whether detected smells have been acknowledged and resolved, is consequently used to optimize smell detection algorithms. The proposed framework has been implemented, evaluated, and compared with the traditional human-driven refactoring tools. Evaluation results suggest that the proposed framework could drive inexperienced engineers to resolve more code smells (by an increase of 140 percent) promptly. The average lifespan of resolved smells was reduced by 92 percent. Results also suggest that the proposed framework could help developers to avoid similar code smells through timely warnings at the early stages of software development, thus reducing the total number of code smells by 51 percent.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6409360,no
Modular Modeling for the Diagnostic of Complex Discrete-Event Systems,"For the complex systems, the development of a methodology of fault diagnosis is important. Indeed, for such systems, an efficient diagnosis contributes to the improvement of the availability, the growth of production, and, of course, the reduction of maintenance costs. It is a key action in the improvement of performance of industrial feature. This paper proposes a new approach to diagnose complex systems modeled by communicating timed automata. Each component has been modeled separately by a timed automaton integrating various operating modes while the communication between the various components is carried out by the control module. Starting from each module of the complex system, a single deterministic automaton, called a diagnoser, is constructed that uses observable events to detect the occurrence of a failure. This modeling formalism provides means for formal verification of the complex system model and its diagnoser. The model-checking methods are used to check correctness properties. The steps of the method are described by an algorithm and illustrated through a batch neutralization process. The implementation of the algorithm is also discussed.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6416097,no
Automated Fault Diagnosis for an Autonomous Underwater Vehicle,"This paper reports our results in using a discrete fault diagnosis system Livingstone 2 (L2), onboard an autonomous underwater vehicle (AUV) Autosub 6000. Due to the difficulty of communicating between an AUV and its operators, AUVs can benefit particularly from increased autonomy, of which fault diagnosis is a part. However, they are also restricted in their power consumption. We show that a discrete diagnosis system can detect and identify a number of faults that would threaten the health of an AUV, while also being sufficiently lightweight computationally to be deployed onboard the vehicle. Since AUVs also often have their missions designed just before deployment in response to data from previous missions, a diagnosis system that monitors the software as well as the hardware of the system is also very useful. We show how a software diagnosis model can be built automatically that can be integrated with the hardware model to diagnose the complete system. We show empirically that on Autosub 6000 this allows us to diagnose real vehicle faults that could potentially lead to the loss of the vehicle.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6423214,no
Code Coverage of Adaptive Random Testing,"Random testing is a basic software testing technique that can be used to assess the software reliability as well as to detect software failures. Adaptive random testing has been proposed to enhance the failure-detection capability of random testing. Previous studies have shown that adaptive random testing can use fewer test cases than random testing to detect the first software failure. In this paper, we evaluate and compare the performance of adaptive random testing and random testing from another perspective, that of code coverage. As shown in various investigations, a higher code coverage not only brings a higher failure-detection capability, but also improves the effectiveness of software reliability estimation. We conduct a series of experiments based on two categories of code coverage criteria: structure-based coverage, and fault-based coverage. Adaptive random testing can achieve higher code coverage than random testing with the same number of test cases. Our experimental results imply that, in addition to having a better failure-detection capability than random testing, adaptive random testing also delivers a higher effectiveness in assessing software reliability, and a higher confidence in the reliability of the software under test even when no failure is detected.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6449335,no
Languages for software-defined networks,"Modern computer networks perform a bewildering array of tasks, from routing and traffic monitoring, to access control and server load balancing. However, managing these networks is unnecessarily complicated and error-prone, due to a heterogeneous mix of devices (e.g., routers, switches, firewalls, and middleboxes) with closed and proprietary configuration interfaces. Softwaredefined networks are poised to change this by offering a clean and open interface between networking devices and the software that controls them. In particular, many commercial switches support the OpenFlow protocol, and a number of campus, data center, and backbone networks have deployed the new technology. However, while SDNs make it possible to program the network, they does not make it easy. Today's OpenFlow controllers offer low-level APIs that mimic the underlying switch hardware. To reach SDNs full potential, we need to identify the right higher-level abstractions for creating (and composing) applications. In the Frenetic project, we are designing simple and intuitive abstractions for programming the three main stages of network management: monitoring network traffic, specifying and composing packet forwarding policies, and updating policies in a consistent way. Overall, these abstractions make it dramatically easier for programmers to write and reason about SDN applications.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6461197,no
"ECG signal processing: Lossless compression, transmission via GSM network and feature extraction using Hilbert transform","Software based new, efficient and reliable lossless ECG data compression, transmission and feature extraction scheme is proposed here. The compression and reconstruction algorithm is implemented on C-platform. The compression scheme is such that the compressed file contains only ASCII characters. These characters are transmitted using internet based Short Message Service (SMS) system and at the receiving end, original ECG signal is brought back using just the reverse logic of compression. Reconstructed ECG signal is de-noised and R peaks are detected using Lagrange Five Point Interpolation formula and Hilbert transform. ECG baseline modulation correction is done and Q, S, QRS onset-offset points are identified. The whole module has been applied to various ECG data of all the 12 leads taken from PTB diagnostic ECG database (PTB-DB). It is observed that the compression module gives a moderate to high compression ratio (CR=7.18), an excellent Quality Score (QS=312.17) and the difference between original and reconstructed ECG signal is negligible (PRD=0.023%). Also the feature extraction module offers a good level of Sensitivity and Positive Predictivity (99.91%) of R peak detection. Measurement errors in extracted ECG features are also calculated.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6461290,no
Data Quality: Some Comments on the NASA Software Defect Datasets,"Background--Self-evidently empirical analyses rely upon the quality of their data. Likewise, replications rely upon accurate reporting and using the same rather than similar versions of datasets. In recent years, there has been much interest in using machine learners to classify software modules into defect-prone and not defect-prone categories. The publicly available NASA datasets have been extensively used as part of this research. Objective--This short note investigates the extent to which published analyses based on the NASA defect datasets are meaningful and comparable. Method--We analyze the five studies published in the IEEE Transactions on Software Engineering since 2007 that have utilized these datasets and compare the two versions of the datasets currently in use. Results--We find important differences between the two versions of the datasets, implausible values in one dataset and generally insufficient detail documented on dataset preprocessing. Conclusions--It is recommended that researchers 1) indicate the provenance of the datasets they use, 2) report any preprocessing in sufficient detail to enable meaningful replication, and 3) invest effort in understanding the data prior to applying machine learners.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6464273,yes
Software cost estimation using Particle Swarm Optimization in the light of Quality Function Deployment technique,"Although software industry has seen a tremendous growth and expansion since its birth, it is continuously facing problems in its evolution. The major challenge for this industry is to produce quality software which is timely designed and build with proper cost estimates. Thus the techniques for controlling the quality and predicting cost of software are in the center of attention for many software firms. In this paper, we have tried to propose a cost estimation model based on Multi-objective Particle Swarm Optimization (MPSO) to tune the parameters of the famous COstructive COst MOdel (COCOMO). This cost estimation model is integrated with Quality Function Deployment (QFD) methodology to assist decision making in software designing and development processes for improving the quality. This unique combination will help the project managers to efficiently plan the overall software development life cycle of the software product.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6466263,no
Validating Software Reliability Early through Statistical Model Checking,"Conventional software reliability assessment validates a system's reliability only at the end of development, resulting in costly defect correction. A proposed framework employs statistical model checking (SMC) to validate reliability at an early stage. SMC computes the probability that a target system will satisfy functional-safety requirements. The framework compares the allocated reliability goal with the calculated reliability using the probabilities and relative weight values for the functional-safety requirements. Early validation can prevent the propagation of reliability allocation errors and design errors at later stages, thereby achieving safer, cheaper, and faster development of safety-critical systems.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6468034,no
Tailoring a large-sized software process using process slicing and case-based reasoning technique,"As the process tailoring is an inevitable and costly activity in software development projects, it is important to reduce the effort for process tailoring. It is critical to a large-sized software process. A large-sized software process usually contains hundreds of elements and relationships between the elements. Manually identifying the elements to be tailored is laborious and error-prone. To overcome this problem, the authors proposed an approach to process tailoring using process slicing (PS) in a large-sized software process. PS operates on a software process that includes various sub-processes and utilises past experience by case-based reasoning (CBR) technique to increase its effectiveness. The authors validated that PS can help a project manager to identify the elements to be tailored with less effort. It has also been illustrated that the CBR technique is helpful in reducing errors and increasing the performance of PS.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6471308,no
Patterns of Knowledge in API Reference Documentation,"Reading reference documentation is an important part of programming with application programming interfaces (APIs). Reference documentation complements the API by providing information not obvious from the API syntax. To improve the quality of reference documentation and the efficiency with which the relevant information it contains can be accessed, we must first understand its content. We report on a study of the nature and organization of knowledge contained in the reference documentation of the hundreds of APIs provided as a part of two major technology platforms: Java SDK 6 and .NET 4.0. Our study involved the development of a taxonomy of knowledge types based on grounded methods and independent empirical validation. Seventeen trained coders used the taxonomy to rate a total of 5,574 randomly sampled documentation units to assess the knowledge they contain. Our results provide a comprehensive perspective on the patterns of knowledge in API documentation: observations about the types of knowledge it contains and how this knowledge is distributed throughout the documentation. The taxonomy and patterns of knowledge we present in this paper can be used to help practitioners evaluate the content of their API documentation, better organize their documentation, and limit the amount of low-value content. They also provide a vocabulary that can help structure and facilitate discussions about the content of APIs.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6473801,no
Tool Use within NASA Software Quality Assurance,"As space mission software systems become larger and more complex, it is increasingly important for the software assurance effort to have the ability to effectively assess both the artifacts produced during software system development and the development process itself. Conceptually, assurance is a straightforward idea -- it is the result of activities carried out by an organization independent of the software developers to better inform project management of potential technical and programmatic risks, and thus increase management's confidence in the decisions they ultimately make. In practice, effective assurance for large, complex systems often entails assessing large, complex software artifacts (e.g., requirements specifications, architectural descriptions) as well as substantial amounts of unstructured information (e.g., anomaly reports resulting from testing activities during development). In such an environment, assurance engineers can benefit greatly from appropriate tool support. In order to do so, an assurance organization will need accurate and timely information on the tool support available for various types of assurance activities. In this paper, we investigate the current use of tool support for assurance organizations within NASA, and describe on-going work at JPL for providing assurance organizations with the information about tools they need to use them effectively.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6480440,no
HETA: Hybrid Error-Detection Technique Using Assertions,"This paper presents HETA, a hybrid technique based on assertions and a non-intrusive enhanced watchdog module to detect SEE faults in microprocessors. These types of faults have a major influence in the microprocessor's control flow, causing incorrect jumps in the program's execution flow. In order to protect the system, a non-intrusive hardware module is implemented in order to monitor the data exchanged between the microprocessor and its memory. Since the hardware itself is not capable of detecting all control flow errors, it is enhanced to support a new software-based technique. Also, previous techniques are used to reach higher detection rates. A fault injection campaign is performed using a MIPS microprocessor. Simulation results show high detection rates with a small amount of performance degradation and area overhead.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6482684,no
Automatic Arabic pronunciation scoring for computer aided language learning,"Automatic articulation scoring makes the computer able to give feedback on the quality of pronunciation and eventually detect some phonemes on miss-pronunciation. Computer-assisted language learning has evolved from simple interactive software that access the learner's knowledge in grammar and vocabulary to more advanced systems that accept speech input as a result of the recent development of speech recognition. Therefore many computer based self teaching systems have been developed for several languages such as English, Deutsch and Chinese, however for Arabic; the research is still in its infancy. This study is part of the Arabic Pronunciation improvement system for Malaysian Teachers of the Arabic language project which aims at developing computer based systems for standard Arabic language learning for Malaysian teachers of the Arabic language. The system aims to help teachers to learn the Arabic language quickly by focusing on the listening and speaking comprehension (receptive skills) to improve their pronunciation. In this paper we addressed the problem of giving marks for Arabic pronunciation by using a Automatic Speech Recognizer (ASR) based on Hidden Markov Models (HMM). Therefore, our methodology for pronunciation assessment is based on the HMM log-likelihood probability, however our main contribution was to train the system using both native and non native speakers. This resulted on improving the system's accuracy from 87.61% to 89.69%.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6487246,no
Relyzer: Application Resiliency Analyzer for Transient Faults,"Future microprocessors need low-cost solutions for reliable operation in the presence of failure-prone devices. A promising approach is to detect hardware faults by deploying low-cost software-level symptom monitors. However, there remains a nonnegligible risk that several faults might escape these detectors to produce silent data corruptions (SDCs). Evaluating and bounding SDCs is, therefore, crucial for low-cost resiliency solutions. The authors present Relyzer, an approach that can systematically analyze all application fault sites and identify virtually all SDC-causing program locations. Instead of performing fault injections on all possible application-level fault sites, which is impractical, Relyzer carefully picks a small subset. It employs novel fault-pruning techniques that reduce the number of fault sites by either predicting their outcomes or showing them equivalent to others. Results show that 99.78 percent of faults are pruned across 12 studied workloads, reducing the complete application resiliency evaluation time by 2 to 6 orders of magnitude. Relyzer, for the first time, achieves the capability to list virtually all SDC-vulnerable program locations, which is critical in designing low-cost application-centric resiliency solutions. Relyzer also opens new avenues of research in designing error-resilient programming models as well as even faster (and simpler) evaluation methodologies.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6487478,no
Fault Detection in Nonlinear Stable Systems Over Lossy Networks,"This paper addresses the problem of fault detection (FD) in nonlinear stable systems, which are monitored via communications networks. An FD based on the system data provided by the communications network is called networked fault detection (NFD) or over network FD in the literature. A residual signal is generated, which gives a satisfactory estimation of the fault. A sufficient condition is derived, which minimizes the estimation error in the presence of packet drops, quantization error, and unwanted exogenous inputs such as disturbance and noise. A linear matrix inequality is obtained for the design of the FD filter parameters. In order to produce appropriate fault alarms, two widely used residual signal evaluation methodologies, based on the signals' peak and average values, are presented and compared together. Finally, the effectiveness of the proposed NFD technique is extensively assessed by using an experimental testbed that was built for performance evaluation of such systems with the use of IEEE 802.15.4 wireless sensor networks (WSNs) technology. In particular, this paper describes the issue of floating point calculus when connecting the WSNs to the engineering design softwares, such as MATLAB, and a possible solution is presented.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6488783,no
Web based testing  An optimal solution to handle peak load,"Software Testing is a difficult task and testing web applications may be even more difficult due to peculiarities of such applications. One way to assess IT infrastructure performance is through load testing, which lets you assess how your Web site supports its expected workload by running a specified set of scripts that emulate customer behavior at different load levels. This paper describe the QoS factors load testing addresses, how to conduct load testing, and how it addresses business needs at several requirement levels and presents the efficiency of web based applications in terms of QoS, throughput and Response Time.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6496439,no
Reliable code coverage technique in software testing,"E-Learning has become a major field of interest in recent year, and multiple approaches and solutions have been developed. Testing in E-Iearning software is the most important way of assuring the quality of the application. The E-Learning software contains miscommunication or no communication, software complexity, programming errors, time pressures and changing requirements, there are too many unrealistic software which results in bugs. In order to remove or defuse the bugs that cause a lot of project failures at the final stage of the delivery., this paper focuses on adducing a Reliable code coverage technique in software testing, which will ensure a bug free delivery of the software development. Software testing aims at detecting error-prone areas. This helps in the detection and correction of errors. It can be applied at the unit of integration and system levels of the software testing process, and it is usually done at the unit level. This method of test design uncovered many errors or problems. Experimental results show that, the increase in software performance rating and software quality assurance increases the testing level in performance.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6496465,no
HELIOLIB/MIDL: An example of code reuse over mission lifetime,"This paper presents an overview of using a single semantic data model (HELIOLIB) and science data analysis code base (MIDL) throughout all phases of a spacecraft mission from integration and testing to archive submission. By using a single code base to create both ground support software and analysis software, we reduce costs and increase the integrity of the final science data product. The detailed, error-prone task of decoding telemetry is coded only once, and is tested early in the instrument development cycle. Not only does this result in better science telemetry processing later on, but can also reveal instrument or data design issues while the instrument is still on the ground. The telemetry processing code is then encapsulated within a data model that allows the code to be used as a pluggable reader module within science analysis tools. The daily use of these tools (MIDL) by instrument scientists helps validate the code. Final archive products can then be created with the same code base (same jar file even), ensuring that the quality of the archive products is as good as the data used routinely by the instrument team.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6496973,no
Comprehensive visual field test & diagnosis system in support of astronaut health and performance,"Long duration spaceflight, permanent human presence on the Moon, and future human missions to Mars will require autonomous medical care to address both expected and unexpected risks. An integrated non-invasive visual field test & diagnosis system is presented for the identification, characterization, and automated classification of visual field defects caused by the spaceflight environment. This system will support the onboard medical provider and astronauts on space missions with an innovative, non-invasive, accurate, sensitive, and fast visual field test. It includes a database for examination data, and a software package for automated visual field analysis and diagnosis. The system will be used to detect and diagnose conditions affecting the visual field, while in space and on Earth, permitting the timely application of therapeutic countermeasures before astronaut health or performance are impaired. State-of-the-art perimetry devices are bulky, thereby precluding application in a spaceflight setting. In contrast, the visual field test & diagnosis system requires only a touchscreen-equipped computer or touchpad device, which may already be in use for other purposes (i.e., no additional payload), and custom software. The system has application in routine astronaut assessment (Clinical Status Exam), pre-, in-, and post-flight monitoring, and astronaut selection. It is deployable in operational space environments, such as aboard the International Space Station or during future missions to or permanent presence on the Moon and Mars.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6497367,no
Assessing the Cost Effectiveness of Fault Prediction in Acceptance Testing,"Until now, various techniques for predicting fault-prone modules have been proposed and evaluated in terms of their prediction performance; however, their actual contribution to business objectives such as quality improvement and cost reduction has rarely been assessed. This paper proposes using a simulation model of software testing to assess the cost effectiveness of test effort allocation strategies based on fault prediction results. The simulation model estimates the number of discoverable faults with respect to the given test resources, the resource allocation strategy, a set of modules to be tested, and the fault prediction results. In a case study applying fault prediction of a small system to acceptance testing in the telecommunication industry, results from our simulation model showed that the best strategy was to let the test effort be proportional to """"the number of expected faults in a module  log(module size)."""" By using this strategy with our best fault prediction model, the test effort could be reduced by 25 percent while still detecting as many faults as were normally discovered in testing, although the company required about 6 percent of the test effort for metrics collection, data cleansing, and modeling. The simulation results also indicate that the lower bound of acceptable prediction accuracy is around 0.78 in terms of an effort-aware measure, Norm(P<sub>opt</sub>). The results indicate that reduction of the test effort can be achieved by fault prediction only if the appropriate test strategy is employed with high enough fault prediction accuracy. Based on these preliminary results, we expect further research to assess their general validity with larger systems.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6497441,no
Impacts of information and communication failures on optimal power system operation,"This paper focuses on recognizing the ways in which information and communication network failures cause a loss of control over a power system's operation. Using numerical evidence, it also assesses the specific impacts of such failures on the optimal operation of a power system. Optimal power flow (OPF) is the most prominent method for implementing optimal operation. In OPF, it is assumed that all power appliances are accessible through the communication and information network, and all power devices are set as the output of OPF; nevertheless, the loss of control and operation of the power system's apparatuses may seriously impact the real-time operation of the bulk power system. The control and operation of the power system is dedicated to a modern communication network, in that intelligent electronic devices (IEDs) are connected to apparatuses of the power network. Data communication among IEDs enables both automatic and remote manual control of the power system. Although such a network offers new advantages and possibilities not previously achievable, it intrinsically has its own source of failures, such as the failure of physical components, loss of integrity, software failures and data communication faults.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6497860,no
On the Relationship between Program Evolution and Fault-Proneness: An Empirical Study,"Over the years, many researchers have studied the evolution and maintenance of object-oriented source code in order to understand the possibly costly erosion of the software. However, many studies thus far did not link the evolution of classes to faults. Since (1) some classes evolve independently, other classes have to do it together with others (co-evolution), and (2) not all classes are meant to last forever, but some are meant for experimentation or to try out an idea that was then dropped or modified. In this paper, we group classes based on their evolution to infer their lifetime models and coevolution trends. Then, we link each group's evolution to faults. We create phylogenetic trees showing the evolutionary history of programs and we use such trees to facilitate spotting the program code decay. We perform an empirical study, on three open-source programs: ArgoUML, JFreechart, and XercesJ, to examine the relation between the evolution of object-oriented source code at class level and fault-proneness. Our results indicate that (1) classes having a specific lifetime model are significantly less fault-prone than other classes and (2) faults fixed by maintaining co-evolved classes are significantly more frequent than faults fixed using not co-evolved classes.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6498451,no
An Empirical Analysis of Bug Reports and Bug Fixing in Open Source Android Apps,"Smartphone platforms and applications (apps) have gained tremendous popularity recently. Due to the novelty of the smartphone platform and tools, and the low barrier to entry for app distribution, apps are prone to errors, which affects user experience and requires frequent bug fixes. An essential step towards correcting this situation is understanding the nature of the bugs and bug-fixing processes associated with smartphone platforms and apps. However, prior empirical bug studies have focused mostly on desktop and server applications. Therefore, in this paper, we perform an in-depth empirical study on bugs in the Google Android smartphone platform and 24 widely-used open-source Android apps from diverse categories such as communication, tools, and media. Our analysis has three main thrusts. First, we define several metrics to understand the quality of bug reports and analyze the bug-fix process, including developer involvement. Second, we show how differences in bug life-cycles can affect the bug-fix process. Third, as Android devices carry significant amounts of security-sensitive information, we perform a study of Android security bugs. We found that, although contributor activity in these projects is generally high, developer involvement decreases in some projects, similarly, while bug-report quality is high, bug triaging is still a problem. Finally, we observe that in Android apps, security bug reports are of higher quality but get fixed slower than non-security bugs. We believe that the findings of our study could potentially benefit both developers and users of Android apps.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6498462,no
A Study on the Relation between Antipatterns and the Cost of Class Unit Testing,"Antipatterns are known as recurring, poor design choices, recent and past studies indicated that they negatively affect software systems in terms of understand ability and maintainability, also increasing change-and defect-proneness. For this reason, refactoring actions are often suggested. In this paper, we investigate a different side-effect of antipatterns, which is their effect on testability and on testing cost in particular. We consider as (upper bound) indicator of testing cost the number of test cases that satisfy the minimal data member usage matrix (MaDUM) criterion proposed by Bashir and Goel. A study-carried out on four Java programs, Ant 1.8.3, ArgoUML 0.20, Check Style 4.0, and JFreeChart 1.0.13-supports the evidence that, on the one hand, antipatterns unit testing requires, on average, a number of test cases substantially higher than unit testing for non-antipattern classes. On the other hand, antipattern classes must be carefully tested because they are more defect-prone than other classes. Finally, we illustrate how specific refactoring actions-applied to classes participating in antipatterns-could reduce testing cost.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6498465,no
Relating Clusterization Measures and Software Quality,"Empirical studies have shown that dependence clusters are both prevalent in source code and detrimental to many activities related to software, including maintenance, testing and comprehension. Based on such observations, it would be worthwhile to try to give a more precise characterization of the connection between dependence clusters and software quality. Such attempts are hindered by a number of difficulties: there are problems in assessing the quality of software, measuring the degree of clusterization of software and finding the means to exhibit the connection (or lack of it) between the two. In this paper we present our approach to establish a connection between software quality and clusterization. Software quality models comprise of low- and high-level quality attributes, in addition we defined new clusterization metrics that give a concise characterization of the clusters contained in programs. Apart from calculating correlation coefficients, we used mutual information to quantify the relationship between clusterization and quality. Results show that a connection can be demonstrated between the two, and that mutual information combined with correlation can be a better indicator to conduct deeper examinations in the area.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6498485,no
"Adoption of Software Testing in Open Source Projects--A Preliminary Study on 50,000 Projects","In software engineering, testing is a crucial activity that is designed to ensure the quality of program code. For this activity, development teams spend substantial resources constructing test cases to thoroughly assess the correctness of software functionality. What is however the proportion of open source projects that include test cases? What kind of projects are more likely to include test cases? In this study, we explore 50,000 projects and investigate the correlation between the presence of test cases and various project development characteristics, including the lines of code and the size of development teams.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6498487,no
Supplying Compiler's Static Compatibility Checks by the Analysis of Third-Party Libraries,"Statically typed languages and their compile time checks prevent a lot of runtime errors thanks to type mismatches detection, namely calls of incompatible methods. Since current applications typically include tens of already compiled third-party libraries, the compile checks are powerless to detect their mutual dependencies. However, the calls among third-party library methods are not less error prone due to bugs or wrong library usage. These all lead to runtime failures. In this paper, we describe a partial solution to this problem based on the static analysis of third-party libraries and verification of their dependencies. This verification is invoked as the application is compiled and assembled, essentially supplying the compiler detecting errors before the application runs. This approach promises improved error detection of complex applications on the static type checking level.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6498492,no
A Pilot Study on Software Quality Practices in Belgian Industry,"In the context of an ERDF-funded project portfolio, we have carried out a survey to assess the state-of-the-practice in software quality in Belgian companies. With this survey, we wish to find out what are the most common industry practices (processes, techniques and tools) with respect to software quality, and how these practices vary across companies. Companies could use the results of this study to improve upon their current software quality practices compared to other companies. Researchers could use it to develop better techniques and tools for aspects that have not found sufficient take-up by industry. Teachers may use it to adapt their courses to become more directly relevant to industry practices.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6498496,no
Facts and Fallacies of Reuse in Practice,"Despite the positive effects of reuse claimed in a significant amount of research, anecdotal evidence indicates that industry is not yet experiencing the expected benefit. This dissertation proposal aims to investigate these indicators and therefore addresses reuse from an industrial point of view. As a first step, it empirically assesses the general state of reuse in practice. This is achieved via a large-scale online questionnaire distributed to multiple companies. Complementing the questionnaire, extensive interviews are being scheduled with developers and project managers of the respective companies. The goal is to interview approximately 10 employees per company. Three companies have already committed to the interview phase and contact with seven further companies is currently being established. In a second step, the findings of the study will be used to extract the context and type of reuse, as well as success factors and hindrances. This information forms the basis for an analytical assessment model for internal code reuse, which is developed in a third step. It will capture a range of different aspects of reuse in practice and will be combined with a process to evaluate the adequacy of reuse. The planned result is a larger assessment framework for evaluating the reuse (management) process within a project as well as a multi-project context. As a result, guidelines for code organization should be developed and tested for their effects in improving reuse in one or more of the industrial partners projects.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6498504,no
Quality Assessment in the Cloud: Is It Worthwhile?,"As software systems become increasingly complex, the aspects of quality that we want to assess become increasingly diverse, requiring the usage of a significant number of tools. Therefore, the installation and proper configuration of these various analysis tools, as well as running them on local computers for large-scale systems becomes more and more a significant investment both in terms of time and computing power. In this paper we present how the infrastructure and services that are developed within the HOST project could be employed to facilitate the extensive use of quality assessment tools. We present the HOST service functionality infrastructure by showing how INFUSION, a popular tool for detecting design flaws, has been integrated and used there. The paper presents the performance improvements due to running INFUSION within a cloud infrastructure and discusses the trade-offs of moving software analysis tools in the cloud.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6498509,no
Vulnerability Scrying Method for Software Vulnerability Discovery Prediction Without a Vulnerability Database,"Predicting software vulnerability discovery trends can help improve secure deployment of software applications and facilitate backup provisioning, disaster recovery, diversity planning, and maintenance scheduling. Vulnerability discovery models (VDMs) have been studied in the literature as a means to capture the underlying stochastic process. Based on the VDMs, a few vulnerability prediction schemes have been proposed. Unfortunately, all these schemes suffer from the same weaknesses: they require a large amount of historical vulnerability data from a database (hence they are not applicable to a newly released software application), their precision depends on the amount of training data, and they have significant amount of error in their estimates. In this work, we propose vulnerability scrying, a new paradigm for vulnerability discovery prediction based on code properties. Using compiler-based static analysis of a codebase, we extract code properties such as code complexity (cyclomatic complexity), and more importantly code quality (compliance with secure coding rules), from the source code of a software application. Then we propose a stochastic model which uses code properties as its parameters to predict vulnerability discovery. We have studied the impact of code properties on the vulnerability discovery trends by performing static analysis on the source code of four real-world software applications. We have used our scheme to predict vulnerability discovery in three other software applications. The results show that even though we use no historical data in our prediction, vulnerability scrying can predict vulnerability discovery with better precision and less divergence over time.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6502762,no
Evaluation of Stretcher Alignment in Radiotherapy using Computed Tomography,"Currently, a wider and more frequent use of Computed Tomography (CT) volumes are used for planning the radiotherapy treatment of oncological patients. These image volumes are instrumental in positioning the patient and the precise incidence of radiation fields. Any error in such a process will adversely affect the result of the treatment and, consequently, the health of the patient. Therefore, quality control is mandatory throughout this process. As regards image acquisition, the controls are typically executed manually, implying a tedious repetitive task. This problem spurred the need to develop an algorithm that allows eliminating this issue while it evaluates and corrects the main concern as well. Particularly, an algorithm for automatic control of the entire positioning process is proposed here which prevents errors from deviation on both the longitudinal and transversal axes of the arrangement. The alignment will vary according to the anatomical part to be treated and the type of stretcher support, among other aspects. The application was developed by resorting to free software tools (GLP) and the libraries for processing, segmenting and recording of medical images (ITK). The algorithm detects these deviations on both axes by applying threshold methods, morphologic operations and Hough's Transform. This algorithm is currently operative in the Medical Image Processing Server of the Nuclear Medicine School Foundation (FUESMEN) of Mendoza, Argentina.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6502803,no
Educational Software for Power Quality Analysis,"This paper presents educational software that allows users to generate, detect and classify electrical power disturbance signals using a Wavelet Transform and Neural Networks based algorithm. This software includes four main modules: a) Signal Acquisition Module that allows the incorporation of waveforms stored in a data base; b) Generation Module which permits the generation of diverse disturbed waveforms; c) Detections Module provides tools to analyze different disturbance detection algorithms and d) Classification Module that determines the disturbance type using different pattern classification methods.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6502849,no
An integrated health management process for automotive cyber-physical systems,"Automobile is one of the most widely distributed cyber-physical systems. Over the last few years, the electronic explosion in automotive vehicles has significantly increased the complexity, heterogeneity and interconnectedness of embedded systems. Although designed to sustain long life, systems degrade in performance due to gradual development of anomalies eventually leading to faults. In addition, system usage and operating conditions (e.g., weather, road surfaces, and environment) may lead to different failure modes that can affect the performance of vehicles. Advanced diagnosis and prognosis technologies are needed to quickly detect and isolate faults in network-embedded automotive systems so that proactive corrective maintenance actions can be taken to avoid failures and improve vehicle availability. This paper discusses an integrated diagnostic and prognostic framework, and applies it to two automotive systems, viz., a Regenerative Braking System (RBS) in hybrid electric vehicles and an Electric Power Generation and Storage (EPGS) system.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6504058,no
Multiple fault diagnosis on a synchronous 2 pole generator using shaft and flux probe signals,A method for diagnosis of multiple incipient faults on a 2-pole synchronous generator is presented. Simulation of the generator on a finite element analysis (FEA) software package is used to predict the effects of these faults. Experimental analysis of the generator under fault conditions is then conducted and confirms the predicted behaviour. The investigation utilises shaft brushes as a non-invasive condition monitoring tool and search coils are used to validate findings from the shaft signal analysis. Results of the investigation indicate definitive relationships between the faults and specific harmonics of the output signals from the condition monitoring tools.,2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6505699,no
Fault diagnosis of voltage sensor in grid-connected 3-phase voltage source converters,"This paper proposes a fault diagnosis method of the line-to-line voltage sensors in grid-connected 3-phase voltage source converters. The line-to-line voltage sensors are an essential device to obtain the information of the grid side voltages for controlling the converters. If there are problems in the voltage sensors by some faults, the controller obtains wrong information of the grid voltage. It causes the unbalance 3 phase currents and the pulsation of the DC-link voltage even though the power grid is healthy. Therefore fault diagnosis methods are required to detect the failure and to avoid the abnormal operation. This proposed diagnosis method identifies whether the fault is at the voltage sensor or the power grid when the abnormal values are measured from the line-to-line voltage sensors. Then the fault tolerance control is possible in case of the voltage sensor fault. The proposed method can improve the system reliability by just adding some software algorithm without additional hardware circuits. The usefulness of this paper is verified through the computer simulation and experiment.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6505725,no
Robust and integrated diagnostics for safety systems in the industrial domain,"The development of robust, safety critical systems with effective diagnostics is increasingly difficult, since hardware is getting more complex, code size is constantly increasing and soft-errors (transient errors) are becoming a dominating factor. It is difficult to reach the required safety integrity in future systems without improving the way diagnostic functions are handled today. Diagnostics are integral part of both hardware and software and it is crucial to design architectures with cross-connected and smart functions being able to detect dangerous errors in the system. While adequate safety is required by EU directives, the end customers require also high availability (uptime). This paper introduces a robust architecture that covers the requirements in order to build fault-tolerant and highly available systems for industrial devices.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6505874,no
MedMon: Securing Medical Devices Through Wireless Monitoring and Anomaly Detection,"Rapid advances in personal healthcare systems based on implantable and wearable medical devices promise to greatly improve the quality of diagnosis and treatment for a range of medical conditions. However, the increasing programmability and wireless connectivity of medical devices also open up opportunities for malicious attackers. Unfortunately, implantable/wearable medical devices come with extreme size and power constraints, and unique usage models, making it infeasible to simply borrow conventional security solutions such as cryptography. We propose a general framework for securing medical devices based on wireless channel monitoring and anomaly detection. Our proposal is based on a medical security monitor (MedMon) that snoops on all the radio-frequency wireless communications to/from medical devices and uses multi-layered anomaly detection to identify potentially malicious transactions. Upon detection of a malicious transaction, MedMon takes appropriate response actions, which could range from passive (notifying the user) to active (jamming the packets so that they do not reach the medical device). A key benefit of MedMon is that it is applicable to existing medical devices that are in use by patients, with no hardware or software modifications to them. Consequently, it also leads to zero power overheads on these devices. We demonstrate the feasibility of our proposal by developing a prototype implementation for an insulin delivery system using off-the-shelf components (USRP software-defined radio). We evaluate its effectiveness under several attack scenarios. Our results show that MedMon can detect virtually all naive attacks and a large fraction of more sophisticated attacks, suggesting that it is an effective approach to enhancing the security of medical devices.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6507636,no
Implementation of an integrated FPGA based automatic test equipment and test generation for digital circuits,"The VLSI circuit manufacturers cannot guarantee defect-free integrated circuits (IC's). Circuit complexity, IC defect anomalies, and economic considerations prevent complete validation of VLSI circuits. The aim is to present the integrated automatic test equipment/generation System for digital circuits. The test generation is developed using device behavior and its based on Behavior-Based Automatic Test Generation (BBATG) technique. A behavior of a device is a set of functions with timing relations on its in/out pins. Automatic test Equipment which is the vital part of electronics test scene today provides the complete set of test executing software and test supporting hardware for the ATE which can use the BBATG generated test data directly to detect behavior faults and diagnose faults at the device level for digital circuits. The low cost, versatile and reconfigurable FPGA-based ATE is implemented called FATE to support in ASIC development phase. This provides the ideal solution for engineers to develop test programs and perform device tests and yield analysis on their desktop and then transfer the test program directly to production. Thus it could able to execute a preliminary digital test, using just a Laptop and an FPGA- board.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6508284,no
Software defect prediction using software metrics - A survey,"Traditionally software metrics have been used to define the complexity of the program, to estimate programming time. Extensive research has also been carried out to predict the number of defects in a module using software metrics. If the metric values are to be used in mathematical equations designed to represent a model of the software process, metrics associated with a ratio scale may be preferred, since ratio scale data allow most mathematical operations to meaningfully apply. Work on the mechanics of implementing metrics programs. The goal of this research is to help developers identify defects based on existing software metrics using data mining techniques and thereby improve software quality which ultimately leads to reducing the software development cost in the development and maintenance phase. This research focuses in identifying defective modules and hence the scope of software that needs to be examined for defects can be prioritized. This allows the developer to run test cases in the predicted modules using test cases. The proposed methodology helps in identifying modules that require immediate attention and hence the reliability of the software can be improved faster as higher priority defects can be handled first. Our goal in this research focuses to improve the classification accuracy of the Data mining algorithm. To initiate this process we initially propose to evaluate the existing classification algorithms and based on its weakness we propose a novel Neural network algorithm with a degree of fuzziness in the hidden layer to improve the classification accuracy.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6508369,no
Developer Dashboards: The Need for Qualitative Analytics,"Prominent technology companies including IBM, Microsoft, and Google have embraced an analytics-driven culture to help improve their decision making. Analytics aim to help practitioners answer questions critical to their projects, such as """"Are we on track to deliver the next release on schedule?"""" and """"Of the recent features added, which are the most prone to defects?"""" by providing fact-based views about projects. Analytic results are often quantitative in nature, presenting data as graphical dashboards with reports and charts. Although current dashboards are often geared toward project managers, they aren't well suited to help individual developers. Mozilla developer interviews show that developers face challenges maintaining a global understanding of the tasks they're working on and that they desire improved support for situational awareness, a form of qualitative analytics that's difficult to achieve with current quantitative tools. This article motivates the need for qualitative dashboards designed to improve developers' situational awareness by providing task tracking and prioritizing capabilities, presenting insights on the workloads of others, listing individual actions, and providing custom views to help manage workload while performing day-to-day development tasks.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6509380,no
Using Class Imbalance Learning for Software Defect Prediction,"To facilitate software testing, and save testing costs, a wide range of machine learning methods have been studied to predict defects in software modules. Unfortunately, the imbalanced nature of this type of data increases the learning difficulty of such a task. Class imbalance learning specializes in tackling classification problems with imbalanced distributions, which could be helpful for defect prediction, but has not been investigated in depth so far. In this paper, we study the issue of if and how class imbalance learning methods can benefit software defect prediction with the aim of finding better solutions. We investigate different types of class imbalance learning methods, including resampling techniques, threshold moving, and ensemble algorithms. Among those methods we studied, AdaBoost.NC shows the best overall performance in terms of the measures including balance, G-mean, and Area Under the Curve (AUC). To further improve the performance of the algorithm, and facilitate its use in software defect prediction, we propose a dynamic version of AdaBoost.NC, which adjusts its parameter automatically during training. Without the need to pre-define any parameters, it is shown to be more effective and efficient than the original AdaBoost.NC.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6509481,yes
Design and verification tools for continuous fluid flow-based microfluidic devices,"This paper describes an integrated design, verification, and simulation environment for programmable microfluidic devices called laboratories-on-chip (LoCs). Today's LoCs are architected and laid out by hand, which is time-consuming, tedious, and error-prone. To increase designer productivity, this paper introduces a Microfluidic Hardware Design Language (MHDL) for LoC specification, along with software tools to assist LoC designers verify the correctness of their specifications and estimate their performance.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6509599,no
VISA synthesis: Variation-aware Instruction Set Architecture synthesis,"We present VISA: a novel Variation-aware Instruction Set Architecture synthesis approach that makes effective use of process variation from both software and hardware points of view. To achieve an efficient speedup, VISA selects custom instructions based on statistical static timing analysis (SSTA) for aggressive clocking. Furthermore, with minimum performance overhead, VISA dynamically detects and corrects timing faults resulting from aggressive clocking of the underlying processor. This hybrid software/hardware approach generates significant speedup without degrading the yield. Our experimental results on commonly used ISA synthesis benchmarks demonstrate that VISA achieves significant performance improvement compared with a traditional deterministic worst case-based approach (up to 78.0%) and an existing SSTA-based approach (up to 49.4%).",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6509603,no
An Efficient and Experimentally Tuned Software-Based Hardening Strategy for Matrix Multiplication on GPUs,"Neutron radiation experiment results on matrix multiplication on graphic processing units (GPUs) show that multiple errors are detected at the output in more than 50% of the cases. In the presence of multiple errors, the available hardening strategies may become ineffective or inefficient. Analyzing radiation-induced error distributions, we developed an optimized and experimentally tuned software-based hardening strategy for GPUs. With fault-injection simulations, we compare the performance and correcting capabilities of the proposed technique with the available ones.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6510503,no
Scalable fault localization for SystemC TLM designs,"SystemC and Transaction Level Modeling (TLM) have become the de-facto standard for Electronic System Level (ESL) design. For the costly task of verification at ESL, simulation is the most widely used and scalable approach. Besides the Design Under Test (DUT), the TLM verification environment typically consists of stimuli generators and checkers where the latter are responsible for detecting errors. However, in case of an error, the subsequent debugging process is still very timeconsuming.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6513468,no
Extracting useful computation from error-prone processors for streaming applications,"As semiconductor fabrics scale closer to fundamental physical limits, their reliability is decreasing due to process variation, noise margin effects, aging effects, and increased susceptibility to soft errors. Reliability can be regained through redundancy, error checking with recovery, voltage scaling and other means, but these techniques impose area/energy costs. Since some applications (e.g. media) can tolerate limited computation errors and still provide useful results, error-tolerant computation models have been explored, with both the application and computation fabric having stochastic characteristics. Stochastic computation has, however, largely focused on application-specific hardware solutions, and is not general enough to handle arbitrary bit errors that impact memory addressing or control in processors. In response, this paper addresses requirements for error-tolerant execution by proposing and evaluating techniques for running error-tolerant software on a general-purpose processor built from an unreliable fabric. We study the minimum error-protection required, from a microarchitecture perspective, to still produce useful results at the application output. Even with random errors as frequent as every 250s, our proposed design allows JPEG and MP3 benchmarks to sustain good output quality14dB and 7dB respectively. Overall, this work establishes the potential for error-tolerant single-threaded execution, and details its required hardware/system support.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6513501,no
Reliability analysis reloaded: How will we survive?,"In safety related applications and in products with long lifetimes reliability is a must. Moreover, facing future technology nodes of integrated circuit device level reliability may decrease, i.e., counter-measures have to be taken to ensure product level reliability. But assessing the reliability of a large system is not a trivial task. This paper revisits the state-of-the-art in reliability evaluation starting from the physical device level, to the software system level, all the way up to the product level. Relevant standards and future trends are discussed.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6513530,no
"Fault detection, real-time error recovery, and experimental demonstration for digital microfluidic biochips","Advances in digital microfluidics and integrated sensing hold promise for a new generation of droplet-based biochips that can perform multiplexed assays to determine the identity of target molecules. Despite these benefits, defects and erroneous fluidic operations remain a major barrier to the adoption and deployment of these devices. We describe the first integrated demonstration of cyberphysical coupling in digital microfluidics, whereby errors in droplet transportation on the digital microfluidic platform are detected using capacitive sensors, the test outcome is interpreted by control hardware, and software-based error recovery is accomplished using dynamic reconfiguration. The hardware/software interface is realized through seamless interaction between control software, an off-the-shelf microcontroller and a frequency divider implemented on an FPGA. Experimental results are reported for a fabricated silicon device and links to videos are provided for the first-ever experimental demonstration of cyberphysical coupling and dynamic error recovery in digital microfluidic biochips.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6513570,no
On-line testing of permanent radiation effects in reconfigurable systems,"Partially reconfigurable systems are more and more employed in many application fields, including aerospace. SRAM-based FPGAs represent an extremely interesting hardware platform for this kind of systems, because they offer flexibility as well as processing power. In this paper we report about the ongoing development of a software flow for the generation of hard macros for on-line testing and diagnosing of permanent faults due to radiation in SRAM-FPGAs used in space missions. Once faults have been detected and diagnosed the flow allows to generate fine-grained patch hard macros that can be used to mask out the discovered faulty resources, allowing partially faulty regions of the FPGA to be available for further use.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6513600,no
Data mining MPSoC simulation traces to identify concurrent memory access patterns,"Due to a growing need for flexibility, massively parallel Multiprocessor SoC (MPSoC) architectures are currently being developed. This leads to the need for parallel software, but poses the problem of the efficient deployment of the software on these architectures. To address this problem, the execution of the parallel program with software traces enabled on the platform and the visualization of these traces to detect irregular timing behavior is the rule. This is error prone as it relies on software logs and human analysis, and requires an existing platform. To overcome these issues and automate the process, we propose the conjoint use of a virtual platform logging at hardware level the memory accesses and of a data-mining approach to automatically report unexpected instructions timings, and the context of occurrence of these instructions. We demonstrate the approach on a multiprocessor platform running a video decoding application.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6513607,no
Efficient software-based fault tolerance approach on multicore platforms,This paper describes a low overhead software-based fault tolerance approach for shared memory multicore systems. The scheme is implemented at user-space level and requires almost no changes to the original application. Redundant multithreaded processes are used to detect soft errors and recover from them. Our scheme makes sure that the execution of the redundant processes is identical even in the presence of non-determinism due to shared memory accesses. It provides a very low overhead mechanism to achieve this. Moreover it implements a fast error detection and recovery mechanism. The overhead incurred by our approach ranges from 0% to 18% for selected benchmarks. This is lower than comparable systems published in literature.,2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6513640,no
Low cost permanent fault detection using ultra-reduced instruction set co-processors,"In this paper, we propose a new, low hardware overhead solution for permanent fault detection at the micro-architecture/instruction level. The proposed technique is based on an ultra-reduced instruction set co-processor (URISC) that, in its simplest form, executes only one Turing complete instruction  the subleq instruction. Thus, any instruction on the main core can be redundantly executed on the URISC using a sequence of subleq instructions, and the results can be compared, also on the URISC, to detect faults. A number of novel software and hardware techniques are proposed to decrease the performance overhead of online fault detection while keeping the error detection latency bounded including: (i) URISC routines and hardware support to check both control and data flow instructions; (ii) checking only a subset of instructions in the code based on a novel check window criterion; and (iii) URISC instruction set extensions. Our experimental results, based on FPGA synthesis and RTL simulations, illustrate the benefits of the proposed techniques.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6513642,no
Improving fault tolerance utilizing hardware-software-co-synthesis,"Embedded systems consist of hardware and software and are ubiquitous in safety-critical and mission-critical fields. The increasing integration density of modern, digital circuits causes an increasing vulnerability of embedded systems to transient faults. Techniques to improve the fault tolerance are often either implemented in hardware or in software. In this paper, we focus on synthesis techniques to improve the fault tolerance of embedded systems considering hardware and software. A greedy algorithm is presented which iteratively assesses the fault tolerance of a processor-based system and decides which components of the system have to be hardened choosing from a set of existing techniques. We evaluate the algorithm in a simple case study using a Traffic Collision Avoidance System (TCAS).",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6513643,no
Object oriented approach for building extraction from high resolution satellite images,"In this paper, an object oriented approach for automatic building extraction from high resolution satellite image is developed. Firstly, Single Feature Classification is applied on the high resolution satellite image. After that, the high resolution image is segmented by using the split and merge segmentation so that the pixels that are grouped as raster objects have probability attributes associated with them. Then different filters are applied on the image to remove the objects which are not of our interest. After filtering the segments, the output raster image is converted into vector image. After converting the raster image into vector image, the building objects are extracted on the basis of area. The cleanup methods are applied to smoothen the extracted buildings and also to increase the accuracy of extraction of buildings. Imagine Objective tool of ERDAS 2011 has been used. The approach is applied on three different satellite images. The extracted buildings are compared with the manually digitized buildings. For one satellite image it has picked up all the buildings with a slight change in the area of footprints of buildings. Only one patch of road is extracted as a building. For the other two satellite images, the overall accuracy is low as compared to the first satellite image. Some patches of road and ground are also extracted as buildings. The branching factor, miss factor, building detection percentage and quality percentage were also calculated for accuracy assessment. Nonetheless, the overall accuracy of building extraction with respect to area was found to be 85.38% in a set of 66 buildings, 73.81% in a set of 94 buildings and 70.64% in a set of 102 buildings.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6514416,no
Predicting Design Quality of Object-Oriented Software using UML diagrams,"Assessment of Object Oriented Software Design Quality has been an important issue among researchers in Software Engineering discipline. In this paper, we propose an approach for determining the design quality of Object Oriented Software System. The approach makes use of a set of UML diagrams created during the design phase of the development process. Design metrics are fetched from the UML diagrams using a parser developed by us and design quality is assessed using a Hierarchical Model of Software Design Quality. To validate the design quality, we compute the product quality for the same software that corresponds to the UML design diagrams using available tools METRIC 1.3.4, JHAWK and Team In a Box. The objective is to establish a correspondence between design quality and product quality of Object Oriented Software. For this purpose, we have chosen priory known three software of Low, Medium and High quality. This is a work under progress, though; the substantial task has already been completed.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6514442,no
Fault Injection for Software Certification,"As software becomes more pervasive and complex, it's increasingly important to ensure that a system will be safe even in the presence of residual software faults (or bugs). Software fault injection consists of the deliberate introduction of software faults for assessing the impact of faulty software on a system and improving its fault tolerance. SFI has been included as a recommended practice in recent safety standards and has therefore gained interest among practitioners, but it's still unclear how it can be effectively used for certification purposes. In this article, the authors discuss the adoption of SFI in the context of safety certification, present a tool for the injection of realistic software faults, and show the usage of that tool in evaluating and improving the robustness of an operating system used in the avionic domain.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6517431,no
Development of a stereo vision measurement architecture for an underwater robot,"Underwater robotics tasks are considered very critical, mainly because of the hazardous environment. The embedded systems for this kind of robots should be robust and fault-tolerant. This paper describes the development of a system for embedded stereo vision in real-time, using a hardware/software co-design approach. The system is capable to detect an object and measure the distance between the object and the cameras. The platform uses two CMOS cameras, a development board with a low-cost FPGA, and a display for visualizing images. Each camera provides a pixel-clock, which are used to synchronize the processing architectures inside the FPGA. For each camera a hardware architecture has been implemented for detecting objects, using a background subtraction algorithm. Whenever an object is detected, its center of mass is calculated in both images, using another hardware architecture to do that. The coordinates of the object center in each image are sent to a soft-processor, which computes the disparity and determines the distance from the object to the cameras. A calibration procedure gives to the soft-processor the capability of computing both disparities and distances. The synthesis tool used (Altera Quartus II) estimates that the system consumes 115.25mW and achieves a throughput of 26.56 frames per second (800480 pixels). These synthesis and the operation results have shown that the implemented system is useful to real-time distance measurements achieving a good precision and an adequate throughput, being suitable for real-time critical operation.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6519001,no
Test suite prioritisation using trace events technique,"The size of the test suite and the duration of time determines the time taken by the regression testing. Conversely, the testers can prioritise the test cases by the use of a competent prioritisation technique to obtain an increased rate of fault detection in the system, allowing for earlier corrections, and getting higher overall confidence that the software has been tested suitably. A prioritised test suite is more likely to be more effective during that time period than would have been achieved via a random ordering if execution needs to be suspended after some time. An enhanced test case ordering may be probable if the desired implementation time to run the test cases is proven earlier. This research work's main intention is to prioritise the regressiontesting test cases. In order to prioritise the test cases some factors are considered here. These factors are employed in the prioritisation algorithm. The trace events are one of the important factors, used to find the most significant test cases in the projects. The requirement factor value is calculated and subsequently a weightage is calculated and assigned to each test case in the software based on these factors by using a thresholding technique. Later, the test cases are prioritised according to the weightage allocated to them. Executing the test cases based on the prioritisation will greatly decreases the computation cost and time. The proposed technique is efficient in prioritising the regression test cases. The new prioritised subsequences of the given unit test suites are executed on Java programs after the completion of prioritisation. Average of the percentage of faults detected is an evaluation metric used for evaluating the 'superiority' of these orderings.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6519507,no
Validating dimension hierarchy metrics for the understandability of multidimensional models for data warehouse,"Structural properties including hierarchies have been recognised as important factors influencing quality of a software product. Metrics based on structural properties (structural complexity metrics) have been popularly used to assess the quality attributes like understandability, maintainability, fault-proneness etc. of a software artefact. Although few researchers have considered metrics based on dimension hierarchies to assess the quality of multidimensional models for data warehouse, there are certain aspects of dimension hierarchies like those related to multiple hierarchies, shared dimension hierarchies among various dimensions etc. which have not been considered in the earlier works. In the authors' previous work, they identified the metrics based on these aspects which may contribute towards the structural complexity and in turn the quality of multidimensional models for data warehouse. However, the work lacks theoretical and empirical validation of the proposed metrics and any metric proposal is acceptable in practice, if it is theoretically and empirically valid. In this study, the authors provide thorough validation of the metrics considered in their previous work. The metrics have been validated theoretically on the basis of Briand's framework - a property-based framework and empirically on the basis of controlled experiment using statistical techniques like correlation and linear regression. The results of these validations indicate that these metrics are either size or length measure and hence, contribute significantly towards structural complexity of multidimensional models and have considerable impact on understandability of these models.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6519508,no
Early performance assessment in component-based software systems,"Most techniques used to assess the qualitative characteristics of software are done in testing phase of software development. Assessment of performance in the early software development process is particularly important to risk management. Software architecture, as the first product, plays an important role in the development of the complex software systems. Using software architecture, quality attributes (such as performance, reliability and security) can be evaluated at the early stages of the software development. In this study, the authors present a framework for taking the advantages of architectural description to evaluate software performance. To do so, the authors describe static structure and architectural behaviour of a software system as the sequence diagram and the component diagram of the Unified Modelling Language (UML), respectively; then, the described model is automatically converted into the 'interface automata', which provides the formal foundation for the evaluation. Finally, the evaluation of architectural performance is performed using 'queuing theory'. The proposed framework can help the software architect to choose an appropriate architecture in terms of quality or remind him/her of making necessary changes in the selected architecture. The main difference among the proposed method and other methods is that the proposed method benefits the informal description methods, such as UML, to describe the architecture of software systems; it also enjoys a formal and lightweight language, called 'interface automata' to provide the infrastructure for verification and evaluation.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6519510,no
Classification and diagnosis of broken rotor bar faults in induction motor using spectral analysis and SVM,"In this paper, we propose to detect and localize the broken bar faults in multi-winding induction motor using Motor current signature (MCSA) combined to Support Vector Machine (SVM). The analysis of stator currents in the frequency domain is the most commonly used method, because induction machine faults often generates particular frequency components in the stator current spectrum. In order to obtain a more robust diagnosis, we propose to classify the feature vectors extracted from the magnitude of spectral analysis using multi-class SVM to discriminate the state of the motor. Finally, in order to validate our proposed approach, we simulated the multi-winding induction motor under Matlab software. Promising results were obtained, which confirms the validity of the proposed approach.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6521554,no
Cooperative sensor anomaly detection using global information,"Sensor networks are deployed in many application areas nowadays ranging from environment monitoring, industrial monitoring, and agriculture monitoring to military battlefield sensing. The accuracy of sensor readings is without a doubt one of the most important measures to evaluate the quality of a sensor and its network. Therefore, this work is motivated to propose approaches that can detect and repair erroneous (i.e., dirty) data caused by inevitable system problems involving various hardware and software components of sensor networks. As information about a single event of interest in a sensor network is usually reflected in multiple measurement points, the inconsistency among multiple sensor measurements serves as an indicator for data quality problem. The focus of this paper is thus to study methods that can effectively detect and identify erroneous data among inconsistent observations based on the inherent structure of various sensor measurement series from a group of sensors. Particularly, we present three models to characterize the inherent data structures among sensor measurement traces and then apply these models individually to guide the error detection of a sensor network. First, we propose a multivariate Gaussian model which explores the correlated data changes of a group of sensors. Second, we present a Principal Component Analysis (PCA) model which captures the sparse geometric relationship among sensors in a network. The PCA model is motivated by the fact that not all sensor networks have clustered sensor deployment and clear data correlation structure. Further, if the sensor data show non-linear characteristic, a traditional PCA model can not capture the data attributes properly. Therefore, we propose a third model which utilizes kernel functions to map the original data into a high dimensional feature space and then apply PCA model on the mapped linearized data. All these three models serve the purpose of capturing the underlying phenomenon of a se- sor network from its global view, and then guide the error detection to discover any anomaly observations. We conducted simulations for each of the proposed models, and evaluated the performance by deriving the Receiver Operating Characteristic (ROC) curves.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6522580,no
Increasing the security level of analog IPs by using a dedicated vulnerability analysis methodology,"With the increasing diffusion of multi-purpose systems such as smart phones and set-top boxes, security requirements are becoming as important as power consumption and silicon area constraints in SoCs and ASICs conception. In the same time, the complexity of IPs and the new technology nodes make the security evaluation more difficult. Indeed, predicting how a circuit behaves when pushed beyond its specifications limits is now a harder task. While security concerns in software development and digital hardware design are very well known, analog hardware security issues are not really studied. This paper first introduces the security concerns for analog and mixed circuits and then presents a vulnerability analysis methodology dedicated to them. Using this methodology, the security level of AMS SoC and Analog IP is increased by evaluating objectively its vulnerabilities and selecting appropriated countermeasure in the earliest design steps.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6523662,no
Commissioning and periodic maintenance of microprocessor-based protection relays at industrial facilities,"Microprocessor-based protective relays are being used throughout industrial facilities and offer the benefits of extensive metering and monitoring, which include sequence components and waveform capturing. There are two types of relay testing which is performed on microprocessor-based protective relays: (1) commission testing and; (2) routine or periodic testing. Commission testing is extensive and exhaustive and its role is to completely test the design and installation of the protective system. Routine or periodic testing is used to validate that a protective system will perform its task by verifying the relay is measuring correctly, set correctly and that it will operate its output contacts for a fault or alarm condition. This paper will first review the differences and functions of commission testing and routine/periodic testing. Secondly, the paper will review methods to use the smarts of the microprocessor-based protective relay to detect issues during startup or during normal operation. These methods include protective relay setting comparison, minimal negative sequence current and voltage, verification/recognition of contact inputs, manual operation of contact outputs, complete control circuitry (trip, close, start, stop functions), lack of device self-test alarms, device date & time, and phasor diagrams provided by protective relay. Examples will be reviewed on the methods including an overview of symmetrical components. The paper will discuss options of installing test switches for AC current & AC voltage isolation and use of spare relay case or chassis for bench tests/verifications. In addition, the paper will discuss the periodic tests that should be performed on protective relay spares that are stored in an industrial facility's warehouse.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6525284,no
Dependability Prediction of WS-BPEL Service Compositions Using Petri Net and Time Series Models,"Web services are emerging as a major technology for deploying automated interactions between distributed and heterogeneous applications. To predict dependability of composite service processes allows service users to decide whether service process meets quantitative trustworthiness requirement. Existing contributions for dependability prediction simply trust QoS information published in Service-Level-Agreement (SLA) or assume QoS of service activities to follow certain assumed distributions. These information and distributions are used as static model inputs into stochastic process models to obtain analytical results. Instead, we consider QoS of service activities to be fluctuating and introduce a dynamic framework to predict runtime dependability of service compositions built on WS-BPEL, employing the Autoregressive-Moving-Average Model (ARMA) time series model and general stochastic Petri net model. In the case study of a real-world service composition sample, a comparison between existing approaches and our one is presented and results suggest that our approach achieves higher prediction accuracy and a better curve-fitting.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6525522,no
A User-Oriented Trust Model for Web Services,"Trust is one of the most critical quality factors for service requestors when they select the services from a large pool of Web services. However, existing web service trust models either do not focus on satisfying user's preferences for different quality of service (QoS) attributes, or do not pay enough attention to the impact of malicious ratings on trust evaluation. To address these gaps, a user-oriented trust model considering user's preferences and false ratings is proposed in this paper. The model introduces an approach to automatically mine user's preferences from their requirements, the preferences are used to determine the weights of each QoS attribute when integrating local trust into the multi-dimensional QoS attributes. The local trust on a service for the user is derived by combining the trust on QoS attributes and the trust on user's ratings. In this model, the users are classified into different groups according to their preferences, the honesty of each group is assessed by filtering out dishonest users using a hybrid approach which combines rating consistency clustering and an average method. To calculate the global trustworthiness of a service for the users group, the weight of ratings is dynamically adjusted according to the results of honesty assessment. The simulation results indicate that the model works well on personalized evaluation of trust, and it can effectively dilute the influence of malicious ratings.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6525525,no
On the design of Trojan tolerant finite field multipliers,"In this paper we analyze the process variation in different multiplier circuits and describe techniques to design error correcting circuits. Integrated circuits have reached such a level of integration that the length transistors is limited to 10s of nanometres. The increasing difficulty to fabricate millions of transistors of the same parameters specified in the integrated circuit design have lead to variation in the performance of the integrated circuit, for instance the thickness of the gate oxide, the length and width of the of the transistor, the doping concentration in the N well substrate, gate threshold voltage and so on. This process variation can be misused for Trojan attacks. Trojan attacks are based on injecting some fault in to the cryptosystem and observing any leak of information by analyzing the erroneous results due to the additional Trojan circuitry. In order to avoid such fault-based attacks, the cryptosystem can be used to detect errors and correct computations, thereby not producing any erroneous results as output. In this paper we further discuss about the error correcting finite field multiplier, as on-line error correction is done it results in more robust hardware modules. The Trojan circuitry can be added even after the error correction stage and hence we have designed a new technique such that error detection and correction is done irrespective of the position of the Trojan in the multiplier.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6526453,no
Simulation Model of IBM-Watson Intelligent System for Early Software Development,"IBM-Watson is among the leading intelligent systems available today. It is extremely complex, employing multiple processors, peripherals, interconnects, along with specialized software and applications. Developing software for this architecture, in absence of target platform, is an extremely error-prone affair. Bringup of software once the hardware is available detects large amount of bugs, throwing the project cost and schedule out of control. This paper introduces a methodology based on IBM-Watson system-level simulation, developed using high-level simulation models of all the components of the target architecture. This methodology helps to debug, verify and fine-tune the IBM-Watson software much before the availability of the target hardware. Use of this methodology enables detecting the bugs much earlier in the development cycle. Majority of the defects are removed much earlier and software bringup-time on actual hardware has reduced from months to days !",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6527438,no
Mathematical Function of a Signal Generator for Voltage Dips Analysis,"This paper presents a mathematical model for a complex voltage dip signal generator, that can be used to generate waveforms similar to those recorded. The voltage dip signal generator is designed to shape the voltage waveform for normal operating conditions, during the voltage dip stage and the transition between these two situations. The advantage of using a generator instead of real data measurements, is that the dip parameters are known, and un this way, it can be detected if the specific algorithms used for voltage dips analysis give the same results. The mathematical model can be implemented in different software used for voltage dip analysis. To demonstrate the complexity of the voltage dip generator, the corresponding mathematical model was implemented in MAPLE and use to generate some voltage dips with particular characteristics.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6527481,no
Novel Automated Fault Isolation System on Low Voltage Distribution System,"This novel automated fault isolation system has been developed and integrated into a new customer side distribution system of 415/240V. The distribution system is based on the Tenaga Nasional Berhad (TNB), the Malaysia's power utility company especially the distribution system. Supervisory Control and Data Acquisition (SCADA), Remote Terminal Unit (RTUs) and power line communication (PLC) system have been used and developed for detecting, fault locating, fault isolating, fault segregating and power restoration in terms of hardware and software. Open loop distribution system is the distribution configuration system used as TNB distribution system. It is the first distribution automation system (DAS) based on fault management research work on customer side substation for operating and controlling betweenthe consumer side system and the substation.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6527485,no
Service Isolation vs. Consolidation: Implications for IaaS Cloud Application Deployment,"Service isolation, achieved by deploying components of multi-tier applications using separate virtual machines (VMs), is a common """"best"""" practice. Various advantages cited include simpler deployment architectures, easier resource scalability for supporting dynamic application throughput requirements, and support for component-level fault tolerance. This paper presents results from an empirical study which investigates the performance implications of component placement for deployments of multi-tier applications to Infrastructure-as-a-Service (IaaS) clouds. Relationships between performance and resource utilization (CPU, disk, network) are investigated to better understand the implications which result from how applications are deployed. All possible deployments for two variants of a multi-tier application were tested, one computationally bound by the model, the other bound by a geospatial database. The best performing deployments required as few as 2 VMs, half the number required for service isolation, demonstrating potential cost savings with service consolidation. Resource use (CPU time, disk I/O, and network I/O) varied based on component placement and VM memory allocation. Using separate VMs to host each application component resulted in performance overhead of ~1-2%. Relationships between resource utilization and performance were harnessed to build a multiple linear regression model to predict performance of component deployments. CPU time, disk sector reads, and disk sector writes are identified as the most powerful performance predictors for component deployments.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6529264,no
A Differential Approach for Configuration Fault Localization in Cloud Environments,"Configuration fault localization is the process of identifying fault in the configuration of component(s) that is the source of failure given a set of observed failure conditions. Configuration faults are harder to detect than on/off failures as it involves analysis of the parameters that constitute the configuration. While distributed systems become more complex and interconnected, the requirements on configuration fault localization have changed. In this paper we present a new, simple but effective approach to configuration fault localization, which utilizes the difference in configuration parameters of components that share a resource. We establish a Reference Configuration State (RCS) by determining a set of non-faulty probing components for each faulty component with respect to shared resources. Performing difference in configuration of reference state with that of the faulty components localizes faulty configuration parameter. Experiments through simulations demonstrate that our approach is effective in identifying configuration faults with reduced time and increased accuracy. Our algorithm gracefully handles the complexity of the problem as the system size grows.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6529291,no
A method for evaluating project management competency acquired from role-play training,"The information technology industry in Japan has required universities to provide project management education. In Tokyo University of Technology, role-play training has been carried out as part of project management education. The role-play scenarios necessary to run role-play exercises have been created in accordance with the ADDIE (Analysis, Design, Development, Implementation, Evaluation) model. This paper describes a method for evaluating project management competency that learners gain through role-play training conducted using the scenarios. Competency in project management is assessed from a learner's behavior characteristics in taking an appropriate action when needed. We first examined the quality of the role-play scenarios by using a design checklist based on Goal-Based Scenarios (GBS). In addition, we analyzed the behavior of each learner during a role-play exercise by using rubrics based on how the user behaved. A high correlation was found between the acquired skill with which learners generally played the role assigned to them in role-play training and the level of quality of the role-play scenario. Based on the analysis results, we will propose a method for helping learners to be able to take effective action by providing appropriate advice from a software agent and feedback from a teacher, along with use of the GBS checklist.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6530101,no
iLight: Device-Free Passive Tracking Using Wireless Sensor Networks,"In this paper, we study indoor passive tracking problem using wireless sensor networks, in which we assume that a target being tracked is clean, i.e., there is no any equipment carried by the target, hence, the tracking procedure is considered to be passive. We first show that received signal strength indicator and link quality indicator are not as effective as expected for passive detection (tracking) through our extensive testbed studies, and further propose to utilize light to track targets using WSNs. To the best of our knowledge, this is the first work, which studies the passive tracking problem in WSNs using light sensors and general light sources. We present a number of probability-based algorithms to study the moving patterns (properties) of targets being tracked. We design and implement our tracking system named as iLight consisting of 40 wireless sensor nodes and one base station. Through extensive experimental results, we show that iLight can track both single target and multiple targets efficiently.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6530604,no
Sequoll: A framework for model checking binaries,"Multi-criticality real-time systems require protected-mode operating systems with bounded interrupt latencies and guaranteed isolation between components. A tight WCET analysis of such systems requires trustworthy information about loop bounds and infeasible paths. We propose sequoll, a framework for employing model checking of binary code to determine loop counts and infeasible paths, as well as validating manual infeasible path annotations which are often error-prone. We show that sequoll automatically determines many of the loop counts in the Malardalen WCET benchmarks. We also show that sequoll computes loop bounds and validates several infeasible path annotations used to reduce the computed WCET bound of seL4, a high-assurance protected microkernel for multi-criticality systems.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6531083,no
Adaptive luminance coding-based scene-change detection for frame rate up-conversion,"This paper presents a new scene-change detection method that uses adaptive luminance coding for frame rate up-conversion. The proposed scene-change detection method splits a frame into several blocks and converts the gray levels of pixels in each block to bit codes. Then, it computes the difference between the bit codes in previous and current blocks. In addition, directional distribution analysis is applied to correct the areas falsely detected as a scene change. The experimental results show that the average F<sub>1</sub> score of the proposed method was up to 0.479 higher than those of the benchmark methods (a 108.53% improvement). The proposed method also reduced the average computation time per pixel by up to 5.572 s compared to the benchmark methods (a 73.14% reduction).",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6531119,no
Verifying Cyber-Physical Interactions in Safety-Critical Systems,"Safety-compromising bugs in software-controlled systems are often hard to detect. In a 2007 DARPA Urban Challenge vehicle, such a defect remained hidden during more than 300 miles of test-driving, manifesting for the first time during the competition. With this incident as an example, the authors discuss formalisms and techniques available for safety analysis of cyber-physical systems.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6531612,no
Efficient Near-Optimal Dynamic Content Adaptation Applied to JPEG Slides Presentations in Mobile Web Conferencing,"In the context of mobile Web conferencing, slide documents are generally transcoded into JPEG format and wrapped into a Web page prior to delivery. Given the diversity of these devices and their networks, dynamically identifying the optimal transcoding parameters is very challenging, as the number of transcoding parameters combinations could be very high. Current solutions use the resolution of the target mobile device and a fixed quality factor as transcoding parameters. However, this technique allows no control over the resulting file size, which, if too large, might increase the delivery time and negatively affect users' experience. Another solution (content selection) which leads to better quality consists in creating several versions and, at delivery time, selecting the best one. However, such a solution is computationally expensive. In this paper, we propose a prediction-based framework which computes near-optimal transcoding parameters dynamically with far less computations. We propose five methods based on this framework. The first predicts near-optimal transcoding parameters, while the others improve their accuracy. From the set of documents tested, two of the proposed methods reach optimality 14% and 30% of the time, respectively. Moreover, the average deviation from optimality for the proposed methods varies from 6% to 3%, with a complexity varying from 1 to 5 transcoding operations.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6531826,no
Internet Metaobject Protocol (IMOP): Weaving the Global Program Grid,"Software applications are increasingly relying on networks to function, but making programs to interact over the network is still tedious and error-prone. Conventional technologies such as CORBA and the WS-* stack are complicated to use, whereas Restful style operations rely on costly ad-hoc developments on a per-service basis. We believe the problem lies in the lack of a network protocol that can solely and sufficiently address interoperability needs. In light of this, we developed Internet Metaobject Protocol (IMOP), a remote method invocation protocol for object-based resource representations. IMOP thoroughly defines operations required to facilitate interactions, from reflecting a resource's definition to invoking its methods. It also rigorously defines the types of data passed between systems, including primitive types, composite value types, and reference types. All of these are programming language neutral.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6531848,no
Privacy preservation and enhanced utility in search log publishing using improved zealous algorithm,"Search log records can enhance the quality and delivery of internet information services to the end user. Analysing and exploring the search log can explore the user's behaviour. When these search logs are published it must ensure privacy of the users at the same time it should exhibit better utility. The existing ZEALOUS algorithm uses a two threshold framework to provide probabilistic differential privacy. In the course of providing this level of privacy the search log looses it's utility, as it publishes only frequent items. So an algorithm is proposed to enhance the utility of search log by qualifying the infrequent items while publishing at the same time preserving the stronger level of privacy.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6533936,no
Robustness Evaluation of Controllers in Self-Adaptive Software Systems,"An increasingly important requirement for software-intensive systems is the ability to self-manage by adapting their structure and behavior at run-time in an autonomous way as a response to a variety of changes that may occur to the system, its environment, or its goals. In particular, self-adaptive (or autonomic) systems incorporate complex software components that act as controllers of a target system by executing actions through effectors, based on information monitored by probes. However, although these controllers are becoming critical in many application domains, it is still difficult to assess their robustness. The proposed approach for evaluating the robustness of controllers for self-adaptive software systems, is aimed at the effective identification of design faults. To achieve this objective, our proposal is based on a set of robustness tests that include the provision of mutated inputs to the interfaces between the controller and the target system (i.e., probes). The feasibility of the approach is evaluated in the context of Znn.com, a case study implemented using the Rainbow framework for architecture-based self-adaptation.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6542600,no
Reliability Analysis of Software Architecture Evolution,"Software engineers and practitioners regard software architecture as an important artifact, providing the means to model the structure and behavior of systems and to support early decisions on dependability and other quality attributes. Since systems are most often subject to evolution, the software architecture can be used as an early indicator on the impact of the planned evolution on quality attributes. We propose an automated approach to evaluate the impact on reliability of architecture evolution. Our approach provides relevant information for architects to predict the impact of component reliabilities, usage profile and system structure on the overall reliability. We translate a system's architectural description written in an Architecture Description Language (ADL) to a stochastic model suitable for performing a thorough analysis on the possible architectural modifications. We applied our method to a case study widely used in research in which we identified the reliability bottlenecks and performed structural modifications to obtain an improved architecture regarding its reliability.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6542601,no
A Model-Driven Approach for Runtime Reliability Analysis,"Runtime reliability analysis has proven to be a valuable technique to enhance the overall reliability of safety-critical systems. It has the potential to close the dependability gap that has been identified by Laprie. However, existing approaches suffer from either too complex and therefore error-prone input languages or from long execution time due to the state space explosion of the underlying analysis techniques. In this paper, we present an approach for runtime reliability analysis, which handles both problems. It provides a compact metamodel that can be used to describe all necessary information. Moreover, it provides analysis algorithms that can be automatically parameterized by code generation. These algorithms are runtime efficient so that they can be executed even on low-end computers, e.g., safety-critical embedded systems, to adapt the system to changing environmental conditions.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6542602,no
The Time Dimension in Predicting Failures: A Case Study,"Online Failure Prediction is a cutting-edge technique for improving the dependability of software systems. It makes extensive use of machine learning techniques applied to variables monitored from the system at regular intervals of time (e.g. mutexes/s, paged bytes/s, etc.). The goal of this work is to assess the impact of considering the time dimension in failure prediction, through the use of sliding windows. The state-of-the-art SVM (Support Vector Machine) classifier is used to support the study, predicting failure events occurring in a Windows XP machine. An extensive comparative analysis is carried out, in particular using a software fault injection technique to speed up the failure data generation process.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6542609,no
Assessing the Impact of Virtualization on the Generation of Failure Prediction Data,"Fault injection has been successfully used in the past to support the generation of realistic failure data for offline training of failure prediction algorithms. However, runtime computer systems evolution requires the online generation of training data. The problem is that using fault injection in a production environment is unacceptable. Virtualization is a cheap sand boxing solution that may be used to run multiple copies of a system, over which fault injection can be safely applied. Nevertheless, there is no guarantee that the data generated in the virtualized environment can be used for training the algorithms that will run in the original system. In this work we study the similarity of failure data obtained in the two scenarios, considering different virtualized environments. Results show that the data share key characteristics, suggesting virtualization as a viable solution to be further researched.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6542610,no
"Declarative, Temporal, and Practical Programming with Capabilities","New operating systems, such as the Capsicum capability system, allow a programmer to write an application that satisfies strong security properties by invoking security-specific system calls at a few key points in the program. However, rewriting an application to invoke such system calls correctly is an error-prone process: even the Capsicum developers have reported difficulties in rewriting programs to correctly invoke system calls. This paper describes capweave, a tool that takes as input (i) an LLVM program, and (ii) a declarative policy of the possibly-changing capabilities that a program must hold during its execution, and rewrites the program to use Capsicum system calls to enforce the policy. Our experiments demonstrate that capweave can be applied to rewrite security-critical UNIX utilities to satisfy practical security policies. capweave itself works quickly, and the runtime overhead incurred in the programs that capweave produces is generally low for practical workloads.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6547099,no
Differential proteome of the striatum from A30P -Synuclein transgenic mouse model of parkinson's disease,"Parkinson's disease (PD) is a multifactorial, neurodegenerative disease where etiopathogenesis are not fully understood. Mutations in -Synuclein (-Syn) were the first genetic defect linked to PD. They are deposited in Lewy bodies (LBs) characteristic for PD. Some experiments had showed that A30P mutant a-Syn have high toxicity than wide-type -Syn. Here we used A30P-Syn transgenic mice model to analysed proteome changes of the striatum 11 months after the birth. Striata were removed and after digesting the proteins we used isotope labelling method to mark different group of peptides. Strong-cation exchange (SCX) liquid chromatography (LC) was integrated with peptide separation as the first dimension of the two-dimensional LC tandem mass spectrometry workflow. In this work, electrospray ionization (ESI) quadrupole time-of-flight (QTOF) mass spectrometer was explored as a means of detecting the Ms/Ms spectrogram. Agilent Spectrum Mill software was used to analysed the results. A total of 660 proteins were quantified. 280 proteins were down-regulated and 77 proteins were up-regulated.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6548335,no
Innovative practices session 5C: Cloud atlas  Unreliability through massive connectivity,"The rapid pace of integration, emergence of low power, low cost computing elements, and ubiquitous and ever-increasing bandwidth of connectivity have given rise to data center and cloud infrastructures. These infrastructures are beginning to be used on a massive scale across vast geographic boundaries to provide commercial services to businesses such as banking, enterprise computing, online sales, and data mining and processing for targeted marketing to name a few. Such an infrastructure comprises of thousands of compute and storage nodes that are interconnected by massive network fabrics, each of them having their own hardware and firmware stacks, with layers of software stacks for operating systems, network protocols, schedulers and application programs. The scale of such an infrastructure has made possible service that has been unimaginable only a few years ago, but has the downside of severe losses in case of failure. A system of such scale and risk necessitates methods to (a) proactively anticipate and protect against impending failures, (b) efficiently, transparently and quickly detect, diagnose and correct failures in any software or hardware layer, and (c) be able to automatically adapt itself based on prior failures to prevent future occurrences. Addressing the above reliability challenges is inherently different from the traditional reliability techniques. First, there is a great amount of redundant resources available in the cloud from networking to computing and storage nodes, which opens up many reliability approaches by harvesting these available redundancies. Second, due to the large scale of the system, techniques with high overheads, especially in power, are not acceptable. Consequently, cross layer approaches to optimize the availability and power have gained traction recently. This session will address these challenges in maintaining reliable service with solutions across the hardware/software stacks. The currently available commercial data-cente- and cloud infrastructures will be reviewed and the relative occurrences of different causalities of failures, the level to which they are anticipated and diagnosed in practice, and their impact on the quality of service and infrastructure design will be discussed. A study on real-time analytics to proactively address failures in a private, secure cloud engaged in domain-specific computations, with streaming inputs received from embedded computing platforms (such as airborne image sources, data streams, or sensors) will be presented next. The session concludes with a discussion on the increased relevance of resiliency features built inside individual systems and components (private cloud) and how the macro public cloud absorbs innovations from this realm.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6548907,no
Performance modelling and analysis of the delay aware routing metric in Cognitive Radio Ad Hoc networks,Cognitive Radio Networks have been proposed to solve the problem of overcrowded unlicensed spectrum by using the cognitive ability built in software radios to utilise the underutilised licensed channel when the licensed users are not using it. Successful results from the research community have led to its application to wireless technologies like Ad Hoc networks due to their extensive advantages. Cognitive Radio Ad Hoc networks are a novel technology that will provide a solution to many communication challenges. This paper investigates the end-to-end performance modelling of a link using quality of service parameters; delay vs. link capacity while considering the factors of spectrum management and node mobility of two nodes in tandem representing a hop in Cognitive Radio Ad Hoc networks. We modelled spectrum management and node mobility using the pre-emptive resume priority M/G/1 queuing model and the gated node model respectively. We considered delay aware routing schemes; shortest queue and random probability routing and compared them with the analytical link-capacity for analysis. The study shows that already existing mathematical models can be used as close approximations to analyse the queuing models proposed for Cognitive Radio Ad Hoc Networks.,2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6549030,no
Fault location in combined overhead line and underground cable distribution networks using fault transient based mother wavelets,"This paper presents an optimized fault location approach in combined overhead line and underground cable distribution networks. Continuous wavelet transform (CWT) is employed for analyzing fault originated travelling waves. The transient voltage waveform is recorded at a measuring point and then analyzed using both standard and fault transient inferred mother wavelets. This approach rely on the relationship between typical frequencies of CWT signal energies and certain paths in the network passed by travelling waves produced by faults. In order to identify characteristic frequencies directly related to the previously mentioned paths, the continuous frequency spectrum of fault transients must be determined. Fault location is then detected using this frequency domain data. The frequency domain data along with the theoretically obtained characteristic frequencies specify the fault position. In order to verify this procedure, the IEEE 34-bus test distribution network is modeled by EMTP-RV software and the relevant transient signal analyses are executed in MATLAB programming environment.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6549586,no
Safety analysis integration in a SysML-based complex system design process,"Model-based system engineering is an efficient approach to specifying, designing, simulating and validating complex systems. This approach allows errors to be detected as soon as possible in the design process, and thus reduces the overall cost of the product. Uniformity in a system engineering project, which is by definition multidisciplinary, is achieved by expressing the models in a common modeling language such as SysML. This paper presents an approach to integrate safety analysis in SysML at early stages in the design process of safety-critical systems. Qualitative analysis is performed through functional as well as behavioral safety analysis and strengthened by formal verification method. This approach is applied to a real-life avionic system and contributes to the integration of formal models in the overall safety and systems engineering design process of complex systems.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6549861,no
EVM as new quality metric for optical modulation analysis,"The quality of optical signals is a very important parameter in optical communications. Several metrics are in common use, like optical signal-to-noise power ratio (OSNR), Q-factor, error vector magnitude (EVM) and bit error ratio (BER). A measured raw BER is not necessarily useful to predict the final BER after soft-decision forward error correction (FEC), if the statistics of the noise leading to errors is unknown. In this respect the EVM is superior, as it allows an estimation of the error statistics. We compare various metrics analytically, by simulation, and through experiments. We employ six quadrature amplitude modulation (QAM) formats at symbol rates of 20 GBd and 25 GBd. The signals were generated by a software-defined transmitter. We conclude that for optical channels with additive Gaussian noise the EVM metric is a reliable quality measure. For nondata-aided QAM reception, BER in the range 10<sup>-6</sup>-10<sup>-2</sup> can be reliably estimated from measured EVM.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6551002,no
Special section on advanced tuneable/reconfigurable and multi-function RF/microwave filtering devices,"Modern trends towards the design of highly-flexible next-generation multi-purpose RF transceivers for emerging applications, such as software-defined radio and radar systems, have reactivated the interest into reconfigurable high-frequency electronics. Microwave tuneable filters are among the most challenging components to carry out the adaptive frequency-band selection demanded by such systems; more still taking into account some critical factors to be considered in their development depending on the application; e.g., power-handling capability, non-linear/noise behavior, switching speed or quality (Q) factor over covered tuning range. On the other hand, a lot of attention has lately been detected in the development of RF/microwave multi-function circuits. This means sophisticated devices simultaneously showing multiple functionalities in the same electrical network. Benefits of this multi-function approach are compact size and more efficient implementations by means of the co-synthesis of different high-frequency components. Within this trend, a considerable effort has been dedicated to integrate the filtering function in other types of RF/microwave circuits, such as power dividers/combiners, antennas, amplifiers or baluns. As a consequence, completely new families of multi-operation filtering devices are being conceived.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6552355,no
Photogrammetric Bundle Adjustment With Self-Calibration of the PrimeSense 3D Camera Technology: Microsoft Kinect,"The Kinect system is arguably the most popular 3-D camera technology currently on the market. Its application domain is vast and has been deployed in scenarios where accurate geometric measurements are needed. Regarding the PrimeSense technology, a limited amount of work has been devoted to calibrating the Kinect, especially the depth data. The Kinect is, however, inevitably prone to distortions, as independently confirmed by numerous users. An effective method for improving the quality of the Kinect system is by modeling the sensor's systematic errors using bundle adjustment. In this paper, a method for modeling the intrinsic and extrinsic parameters of the infrared and colour cameras, and more importantly the distortions in the depth image, is presented. Through an integrated marker-and feature-based self-calibration, two Kinects were calibrated. A novel approach for modeling the depth systematic errors as a function of lens distortion and relative orientation parameters is shown to be effective. The results show improvements in geometric accuracy up to 53% compared with uncalibrated point clouds captured using the popular software RGBDemo. Systematic depth discontinuities were also reduced and in the check-plane analysis the noise of the Kinect point cloud was reduced by 17%.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6552953,no
Distributed Integrated Development Environment for Mobile Platforms,"It is believed that future technologies related to smart devices could add more towards making life easy while saving on time for a person on the go. Already mobile devices have added value to our everyday tasks. However, programmers, so far, seem to be denied the use of such facilities with these smart devices. Distributed Integrated Development Environment for Mobile Platforms (DIMP) is directed towards them with an innovative way to write software programs on the go. Using a mobile device such as a mobile phone or a tablet computer, DIMP is capable of writing source codes and compiling. DIMP consists of a mobile application, a central server and a set of compilation servers, while an administrative web console supports the administrative functions. Together, they comprise DIMP. The mobile application is an android application and provides a rich source code editor integrated to the software. It allows compiling and running of source codes where users can write programs in a selected language. If the source code is error free, a user can expect a worthwhile output whereas an error prone source code would reveal the relevant error message with useful hints for debugging. A further benefit from DIMP is that it allows a user to maintain online work space as well as an offline workspace. Source codes can be shared with other interested users.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6553932,no
An empirical study on the importance of quality among offshore outsourced software development firms in Sri Lanka,"Offshore outsourcing of software development has become an increasingly popular trend in recent years. Sri Lanka has emerged as a favourable destination for outsourcing and it is currently catering to many offshore projects globally. However it is also observed that many other potential destinations are emerging globally. Due to this factor Sri Lankan software development firms would eventually face global competition in time to come. Therefore in this research paper, we would carry out an empirical study to assess & study the current quality measurements that software development companies in Sri Lanka have taken and further discuss on the importance and future benefits that companies would attain by performing these quality practices consistently. Our research adopts both quantitative and qualitative methods to solidify our results. The final outcome of this result would facilitate the software development industry to gain more understanding on the importance of adopting feasible quality measures into their software development life cycle.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6553972,no
Resolving context conflicts using Association Rules (RCCAR) to improve quality of context-aware systems,Context-aware systems (CASs) face many challenges to keep high quality performance. One challenge faces CASs is conflicted values come from different sensors because of different reasons. These conflicts affect the quality of context (QoC) and as a result the quality of service as a whole. This paper conducts a novel approach called RCCAR resolves the context conflicts and so contributes in improving QoC for CASso RCCAR approach resolve context conflicts by exploiting the previous context using Association Rules (AR) to predict the valid values among different conflicted ones. RCCAR introduces an equation that evaluates the strength of prediction for different conflicted context elements values. The approach RCCAR has been implemented using Weka 3.7.7 and results show the success of the solution for different experiments applied to different scenarios designed to examine the solution according to different possible conditions.,2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6554154,no
A collaborative filtering recommendation algorithm based on user clustering and Slope One scheme,"Recommendation system has been widely used in electronic commerce, news, web2.0, E-Iearning and other fields. Collaborative filtering is one of the most important algorithms. But as scale of recommendation system continues to expand, more and more problems appear. Data sparsity and poor prediction are main problems that recommendation system has to face. To improve the quality and performance, a new collaborative filtering recommendation algorithm combining user-clustering and Slope One algorithm is proposed. In our algorithm, users were clustered into several classes based on users' rating on items; therefore the useless information was filtered. Then the slope-one scheme was applied to predict the object rating. The experiments were applied to the MovieLens dataset to exploit the benefits of our detector and the experiment results show that the accuracy of our algorithm is in advance of previous research.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6554158,no
Joint source-channel coding for delay-constrained iterative resource allocation algorithms,"Before achieving convergence, iterative resource allocation algorithms may result in degraded link quality that presents a challenge for the transmission of delay constrained multimedia traffic because of the impossibility for retransmission of frames received with errors. In this paper, a novel approach to solve this problem is presented by proposing the use of a source and channel coding scheme matched to the performance of the algorithm during iterations. The solution uses a JSCC scheme based on incremental redundancy and single feedback for transmission under strict delay constraints. The presented results, based on three widely representative iterative algorithms, show that the novel approach notably reduces the probability of transmissions with excessive distortion. The results show an increase of 12 % in relative values for the probability of links achieving target end-to-end distortion values, reduce the negative effects of degraded channels when performing iterations and also effectively absorb the effect of channel changes over time. These characteristics are specially useful for cognitive radio learning algorithms to mitigate the distortion increase during the exploratory phase.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6554668,no
QoS optimization in ad hoc wireless networks through adaptive control of marginal utility,"Applications consisting of messaging, voice, and video are used to provide situational awareness to decision makers and emergency responders in high criticality crisis scenarios such as disaster management. Here, ad hoc wireless networks are often quickly provisioned to provide the necessary connectivity to support these applications. Applications ill prepared to deal with the constant fluctuation of available bandwidth will stall or fail and contribute to mission failure. Our algorithm, D-Q-RAM (Distributed Quality of Service (QoS) Resource Allocation Model) allows applications to satisfy their specific QoS expectations in dynamically fluctuating networked environments by incorporating a distributed optimization heuristic that results in near optimal adaptation without the need to know, estimate, or predict available bandwidth at any moment in time. This paper describes our approach for managing that optimization heuristic in a manner that is decentralized, that is network routers are unaware of the semantics of the applications, and the applications can arbitrate among competing signals from numerous network routers and select an appropriate QoS level which results in an improved overall global utility of available network bandwidth.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6554733,no
An intelligent ophthalmic regional anesthesia training system based on capacitive sensing,"Safe administration of regional anesthesia in the eye involves insertion of syringe needle into the intra-orbital space at the correct position and angle to avoid injury to ocular structures. A training manikin which emulates human ocular anatomy and provides feedback on the quality of anesthetic procedure would considerably help to reduce the risks involved in real life procedures. This paper presents an anatomically accurate training manikin that has been developed employing rapid prototyping techniques. The system detects and alarms the trainee when a needle is in close proximity of the ocular muscles to avoid injury. Additionally it also apprises the trainee on whether the muscles have been touched by the needle. Proximity of needle is detected by a capacitive sensing scheme. A Virtual Instrument developed measures output from capacitive sensing electrodes and presents it through an intuitive graphical user interface. The proposed touch and proximity detection schemes have been validated by tests performed on a prototype training manikin developed, thus demonstrating its use for practical purposes.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6555543,no
Software reliability prediction model based on ICA algorithm and MLP neural network,"To achieve the high performance system without any failure, we should provide the high reliability level of software. Soft computing models for software reliability prediction suffer from low accuracy during predicting the number of faults. Moreover, the models have some problems like no solid mathematical foundation for analysis, being trapped in local minima, and convergence problem. This paper introduces Imperialist Competitive Algorithm (ICA) to overcome the weaknesses of previous models and improve the efficiency of training process of Multi-Layer Perceptron (MLP) neural network. Therefore, the network can predict the number of faults precisely. The results show that the proposed predicting model is more efficient than the existing techniques in prediction performance.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6556733,no
A statistical machine learning based modeling and exploration framework for run-time cross-stack energy optimization,"As the complexity of many-core processors grow, meeting performance, energy, temperature, reliability, and noise requirements under dynamically changing operating conditions requires run-time optimization of all parts of the computing stack - architecture, system software, and applications. Unfortunately, the combination of design parameters for the entire computing stack results in an operating space of millions of points that must be explored and evaluated at run-time. In this paper, we present a statistical machine learning (SML) based modeling framework that can be used to rapidly explore such vast operating spaces. We construct a multivariate adaptive regression spline (MARS) based model that uses a number of architecture and application parameters as predictor variables to predict performance and power. We then use a Pareto-front exploring evolutionary algorithm to determine operating points for optimal power and performance. The operating points constituting the Pareto front are stored in look-up tables for runtime use. The proposed framework is applied to an 264 video encoding application executing on a quad core processor. The microarchitectural predictor variables include core and cache parameters. The application predictor variables include the video resolution, and visual quality determined by the choice of the motion estimation algorithm. The model outputs the average frames per second (FPS) and the average power consumption. The MARS model has an R<sup>2</sup> of 0.9657 and 0.9467 respectively for FPS and power. For a video frame resolution of 480x320, and FPS of 20, a power saving of 55% can be obtained by jointly tuning the microarchitectural parameters and the visual quality.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6557161,no
A new model for software defect prediction using Particle Swarm Optimization and support vector machine,"Software defect prediction could improve the reliability of software and reduce development costs. Traditional prediction models usually have a lower prediction accuracy. In order to solve this problem, a new model for software defect prediction using Particle Swarm Optimization (PSO) and Support Vector Machine (SVM) named P-SVM model is proposed in this paper, which takes advantage of non-linear computing capability of SVM and parameters optimization capability of PSO. Firstly, P-SVM model uses PSO algorithm to calculate the best parameters of SVM, and then it adopts the optimized SVM model to predict software defect. P-SVM model and other three different prediction models are used to predict the software defects in JM1 data set as an experiment, the results show that P-SVM model has a higher prediction accuracy than BP Neural Network model, SVM model, GA-SVM model.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6561670,no
Method study on fault-tolerant dispatch of the control system of the aero-engine,"Time-limited dispatch (TLD) allows the degraded redundancy dispatch of aircraft. The aero-engine fitted with full authority digital electronic control (FADEC) system with known faults can be dispatched with a deferred fault by applying TLD, and the time of deferred fault should be determined. Under fleet average loss of thrust control (LOTC) rate being achieved, faults are classified by rate of instantaneous LOTC caused by single faults of FADEC system internal redundant components, and components with single faults which leave the FADEC system in an acceptable dispatch configuration are determined.Building the reduced-state open loop Markov calculation model and computing the rate of LOTC in assumed time of deferred fault by MATLAB software. Determining the longest time of limited dispatch without exceedence of LOTC rate meeting the necessary airworthiness requirements.The results indicate that the method is an effective approach for type certification and developing master minimum equipment list (MMEL)and maintenance review board report(MRBR) of aircraft.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6561671,no
The research and design of visual fault tree modeling analysis,"According to the situation that creating fault tree manually is inefficient, error-prone, existing software of visual fault tree modeling is not perfect, not easy to transplant, research visual modeling and analysis of the fault tree. First, this paper discusses the visual building method combines dynamic contribution with stratified achievements, fault event editing functions, separation of fault tree with failure information method, then discusses the fault qualitative analysis in visual modeling process, final design the visual fault tree modeling platform, realize the rapid modeling and visual analysis of the fault tree, and presents an application examples to illustrate. The actual instructions that the system can meet the needs of visual fault tree modeling.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6561793,no
Improving error detection with selective redundancy in software-based techniques,"This paper presents an analysis of the impact of selective software-based techniques to detect faults in microprocessor systems. A set of algorithms is implemented, compiled to a microprocessor and selected variables of the code are hardened with software-based techniques. Seven different methods that choose which variables are hardened are introduced and compared. The system is implemented over a miniMIPS microprocessor and a fault injection campaign is performed in order to verify the feasibility and effectiveness of each selective fault tolerance approach. Results can lead designers to choose more wisely which variables of the code should be hardened considering detection rates and hardening overheads.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6562659,no
Assessment of diagnostic test for automated bug localization,Statistical simulation based design error debug approaches strongly rely on quality of the diagnostic test. At the same time there exists no dedicated technique to perform its quality assessment and engineers are forced to rely on subjective figures such as verification test quality metrics or just the size of the diagnostic test. This paper has proposed two new approaches for assessing diagnostic capability of diagnostic tests for automated bug localization. The first approach relies on probabilistic simulation of diagnostic experiments. The second assessment method is based on calculating Hamming distances of the individual sub-tests in the diagnostic test set. The methods are computationally cheap and they provide for a measure of confidence in the localization results and allow estimating impact of the diagnostic test enhancement. The approach is implemented as a part of an open-source hardware design and debugging framework zamiaCAD. Experimental results with an industrial processor design and a set of documented bugs demonstrate feasibility and effectiveness of the proposed approach.,2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6562665,no
Towards an automatic generation of diagnostic in-field SBST for processor components,"This paper deals with a diagnostic software-based self-test program for multiplexer based components in a processor. These are in particular the read ports of a multi-ported register file and the bypass structures of an instruction pipeline. Based on the detailed analysis of both multiplexer structures, first a manually coded diagnostic test program is presented. This test program can detect all single and multiple stuck-at data- and address faults in a multiplexer structure. But it does not fully cover the control-logic of the bypass. By further refinements a 100% fault coverage for single stuck-at faults, including the control logic, is finally obtained. Based on these results, an ATPG-assisted method for the generation of such a diagnostic test program is described for arbitrary processor components. This method is finally applied to the multiplexer structures for which the manually coded test program is available. The test length and test coverage of the generated test program and of the hand-coded test program are compared.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6562676,no
Supporting the adaptation of open-source database applications through extracting data lifecycles,"The adaptation of open-source database applications is common in the industry. Most open-source database applications are incomplete. During adaptation, users usually have to implement additional data maintenance. Hence, the completeness of an application is an important concern for the adaptation as a key factor to indicate how much additional effort is required before using a system. From our study of database applications with complete functionalities, we observe that data in a database has common patterns of lifecycles. Anomaly in data lifecycles provides a good indicator on the completeness of database applications. In this paper, we propose a novel approach to automatically extract the data lifecycles out of the source code of database applications through inter-procedural static program analysis. This representative information can benefit the adaptation of database applications specifically for selection, maintenance and extension. We have developed a tool to implement the proposed approach for PHP (Hypertext Preprocessor)-based database applications. Case studies have shown that the proposed approach is useful in assisting adaptation and detecting faults of open-source database applications.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6562944,no
About diagnosis of circuit breakers,"On-line monitoring of electrical equipment and their diagnosis is a field which has special attention because it can detect some faults in their incipient phase and thus prevent serious failure of the equipment and also prevent financial and materials related losses. This paper presents a system for monitoring and diagnosis of electrical equipment of medium voltage installations. Also are presented the evolutions and values of some parameters considered important for knowledge of technical condition of a circuit breaker, in case of abnormal operating conditions, and comparing them with similar records considered of reference. Advanced processing and data analysis was performed by using a software application developed in LabVIEW programming environment.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6563368,no
ClimaWin: An intelligent window for optimal ventilation and minimum thermal loss,"In this paper the ClimaWin concept is introduced. The ClimaWin project's main goals are to improve both indoor air quality and the energy efficiency of new and refurbished buildings, through the use of novel green smart windows. Generally, in order to improve windows' energy efficiency better insulation materials are used in windows frames and glasses. However, this approach leads to a severe deterioration of indoor air quality (IAQ) especially in buildings that are not equipped with heating, ventilation and air conditioning (HVAQ) systems. The Climawin windows do not require wires neither for power nor for communications. The window is powered through a battery (for blind operation) and a solar panel, which makes it an ideal solution for retrofitting. In order to achieve the energy efficiency requirements, the Climawin system hardware, the microcontroller software architecture and the radio communication strategy were designed for low power consumption. Furthermore, all the information about the system status can be monitored and actuated using intuitive graphical applications developed for PCs and Android OS smartphones. A remote database keeps all the relevant information about the system, making it easy to detect any anomaly or even to adjust the control algorithm parameters from a remote location. A full-set of web services are also provided in order to simplify the communication with home automation systems.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6563790,no
"Conservative Bounds for the pfd of a 1-out-of-2 Software-Based System Based on an Assessor's Subjective Probability of """"Not Worse Than Independence""""","We consider the problem of assessing the reliability of a 1-out-of-2 software-based system, in which failures of the two channels cannot be assumed to be independent with certainty. An informal approach to this problem assesses the channel probabilities of failure on demand (pfds) conservatively, and then multiplies these together in the hope that the conservatism will be sufficient to overcome any possible dependence between the channel failures. Our intention here is to place this kind of reasoning on a formal footing. We introduce a notion of """"not worse than independence""""' and assume that an assessor has a prior belief about this, expressed as a probability. We obtain a conservative prior system pfd, and show how a conservative posterior system pfd can be obtained following the observation of a number of demands without system failure. We present some illustrative numerical examples, discuss some of the difficulties involved in this way of reasoning, and suggest some avenues of future research.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6564279,no
Adaptive Mho type distance relaying scheme with fault resistance compensation,"This paper describes an adaptive distance relaying scheme which can eliminate the effect of fault resistance on distance relay zone reach. Distance relay is commonly used as main protection to protect transmission line from any type of fault. For a stand-alone distance relay, fault resistance can make Mho type distance relay to be under reached and thus the fault will be isolated at a longer time. In this scheme, the relay detects the fault location using a two-terminal algorithm. By knowing fault location, fault voltage at the fault point can be calculated by using equivalent sequence network connection as seen from local terminal. Then, fault resistance is calculated by using simple equation considering contribution from remote terminal current. Finally, the compensation of fault resistance is done onto calculated apparent resistance as seen at relaying point. The modeling and simulation was carried out using Matlab/Simulink software. Several cases were carried out and the results show the validity of the scheme.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6564545,no
Autokite experimental use of a low cost autonomous kite plane for aerial photography and reconnaissance,"An experimental kite-plane capable of autonomous aerial imaging is introduced as a viable low-cost small-scale civilian UAV imaging platform ideal for field use. The AUTOKITE fulfills a need currently unmet by other fully automated Unmanned Aerial Vehicles (UAVs), resulting from ease of operation, extended flight time, and overall reliability. The AUTOKITE is outfitted with an off-the-shelf autopilot system, and has demonstrated fully autonomous flight in field deployments while collecting high-resolution (~12 cm/pixel) images. The AUTOKITE has been used to map regions historically prone to earthquakes along the Southern San Andreas Fault in California. Comparative image methods enabled by photogrammetric software, like Agisoft's PhotoScan, are then used to discern Structure-from-Motion (SfM) from a multitude of aerial images taken by AUTOKITE [8]. Processing SfM data from overlapping images results in the creation of Digital Elevation Models (DEMs) and Orthophotos for geographic areas of interest. In addition to sample data sets illustrating the SfM process, The AUTOKITE is compared with three alternative UAV systems, and payload integration/automation details are discussed.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6564692,no
A New Modeling Based on Urban Trenches to Improve GNSS Positioning Quality of Service in Cities,"Digital maps with 3D data proved to make it possible the determination of Non-Line-Of-Sight (NLOS) satellites in real time, whilst moving, and obtain significant benefit in terms of navigation accuracy. However, such data are difficult to handle with Geographical Information System (GIS) embedded software in real time. The idea developed in this article consists is proposing a method, light in terms of information contents and computation throughput, for taking into account the knowledge of the 3D environment of a vehicle in a city, where multipath phenomena can cause severe errors in positioning solution. This method makes use of a digital map where homogeneous sections of streets have been identified, and classified among different types of urban trenches. This classification is so called: """"Urban Trench Model"""". Not only NLOS satellites can be detected, but also, if needed, the corresponding measurements can be corrected and further used in the positioning solver. The paper presents in details the method and its results on several real test sites, with a demonstration of the gain obtained on the final position accuracy. The benefit of the Urban Trench Model, i.e. the reduction of positioning errors as compared to conventional solver considering all satellites, gets up to an amount between 30% and as much as 70% e.g. in Paris.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6565516,no
Investigation on transient stability of an industrial network and relevant impact on over-current protection performance,"System protection performance and transient stability of the electrical network are significantly affected by each other. The larger the time delay in which protection detects and clears the fault, the more likely loss of synchronism will be, especially in the networks with internal generation. Over current protection schemes inherently operates with considerable delay. Moreover, system dynamic oscillations discernibly aggravate their performance. Therefore utilizing them as main protection is controversial and even abortive in order to maintain system stability. In this paper, transient stability of a real industrial network is studied. The study is investigated using critical clearing time (CCT) criterion for different network configuration. Equipments such as generators and motors are modeled and simulated by DIgSILENT software. In addition, the operation of over current relays adjusted by conventional methods is investigated dynamically and its performance is examined under different network configurations.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6565971,no
Approaching reliable realtime communications? A novel system design and implementation for roadway safety oriented vehicular communications,"Though there exist ready-made DSRC/WiFi/3G/4G cellular systems for roadway communications, there are common defects in these systems for roadway safety oriented applications and the corresponding challenges remain unsolved for years, i.e., WiFi cannot work well in vehicular networks due to the high probability of packet loss caused by burst communications, which is a common phenomenon in roadway networks; 3G/4G cannot well support real-time communications due to the nature of their designs; DSRC lacks the support to roadway safety oriented applications with hard realtime and reliability requirements [1]. To solve the conflict between the capability limitations of existing systems and the ever-growing demands of roadway safety oriented communication applications, we propose a novel system design and implementation for realtime reliable roadway communications, aiming at providing safety messages to users in a realtime and reliable manner. In our extensive experimental study, the latency is well controlled within the hard realtime requirement (100ms) for roadway safety applications given by NHTSA [2], and the reliability is proved to be improved by two orders of magnitude compared with existing experimental results [1]. Our experiments show that the proposed system for roadway safety communications can provide guaranteed highly reliable packet delivery ratio (PDR) of 99% within the hard realtime requirement 100ms under various scenarios, e.g., highways, city areas, rural areas, tunnels, bridges. Our design can be widely applied for roadway communications and facilitate the current research in both hardware and software design and further provide an opportunity to consolidate the existing work on a practical and easy-configurable low-cost roadway communication platform.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6566746,no
A software-based self test of CUDA Fermi GPUs,"Nowadays, Graphical Processing Units (GPUs) have become increasingly popular due to their high computational power and low prices. This makes them particularly suitable for high-performance computing applications, like data elaboration and financial computation. In these fields, high efficient test methodologies are mandatory. One of the most effective ways to detect and localize hardware faults in GPUs is a Software-Based-Self-Test methodology (SBST). In this paper a fully comprehensive SBST and fault localization methodology for GPUs is presented. This novel approach exploits different custom test strategies for each component inside the GPU architecture. Such strategies guarantee both permanent fault detection and accurate fault localization.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569353,no
Transitioning Manual System Test Suites to Automated Testing: An Industrial Case Study,"Visual GUI testing (VGT) is an emerging technique that provides software companies with the capability to automate previously time-consuming, tedious, and fault prone manual system and acceptance tests. Previous work on VGT has shown that the technique is industrially applicable, but has not addressed the real-world applicability of the technique when used by practitioners on industrial grade systems. This paper presents a case study performed during an industrial project with the goal to transition from manual to automated system testing using VGT. Results of the study show that the VGT transition was successful and that VGT could be applied in the industrial context when performed by practitioners but that there were several problems that first had to be solved, e.g. testing of a distributed system, tool volatility. These problems and solutions have been presented together with qualitative, and quantitative, data about the benefits of the technique compared to manual testing, e.g. greatly improved execution speed, feasible transition and maintenance costs, improved bug finding ability. The study thereby provides valuable, and previously missing, contributions about VGT to both practitioners and researchers.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569716,no
Efficient JavaScript Mutation Testing,"Mutation testing is an effective test adequacy assessment technique. However, it suffers from two main issues. First, there is a high computational cost in executing the test suite against a potentially large pool of generated mutants. Second, there is much effort involved in filtering out equivalent mutants, which are syntactically different but semantically identical to the original program. Prior work has mainly focused on detecting equivalent mutants after the mutation generation phase, which is computationally expensive and has limited efficiency. In this paper, we propose a technique that leverages static and dynamic program analysis to guide the mutation generation process a-priori towards parts of the code that are error-prone or likely to influence the program's output. Further, we focus on the JavaScript language, and propose a set of mutation operators that are specific to web applications. We implement our approach in a tool called MUTANDIS. We empirically evaluate MUTANDIS on a number of web applications to assess the efficacy of the approach.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569719,no
CHECK-THEN-ACT Misuse of Java Concurrent Collections,"Concurrent collections provide thread-safe, highly-scalable operations, and are widely used in practice. However, programmers can misuse these concurrent collections when composing two operations where a check on the collection (such as non-emptiness) precedes an action (such as removing an entry). Unless the whole composition is atomic, the program contains an atomicity violation bug. In this paper we present the first empirical study of CHECK-THEN-ACT idioms of Java concurrent collections in a large corpus of open-source applications. We catalog nine commonly misused CHECK-THEN-ACT idioms and show the correct usage. We quantitatively and qualitatively analyze 28 widely-used open source Java projects that use Java concurrency collections - comprising 6.4M lines of code. We classify the commonly used idioms, the ones that are the most error-prone, and the evolution of the programs with respect to misused idioms. We implemented a tool, CTADetector, to detect and correct misused CHECK-THEN-ACT idioms. Using CTADetector we found 282 buggy instances. We reported 155 to the developers, who examined 90 of them. The developers confirmed 60 as new bugs and accepted our patch. This shows that CHECK-THEN-ACT idioms are commonly misused in practice, and correcting them is important.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569728,no
Assessing Quality and Effort of Applying Aspect State Machines for Robustness Testing: A Controlled Experiment,"Aspect-Oriented Modeling (AOM) has been the subject of intense research over the last decade and aims to provide numerous benefits to modeling, such as enhanced modularization, easier evolution, higher quality as well as reduced modeling effort. However, these benefits can only be obtained at the cost of learning and applying new modeling approaches. Studying their applicability is therefore important to assess whether they are worth using in practice. In this paper, we report a controlled experiment to assess the applicability of AOM, focusing on a recently published UML profile (AspectSM). This profile was originally designed to support model-based robustness testing in an industrial context but is applicable to the behavioral modeling of other crosscutting concerns. This experiment assesses the applicability of AspectSM from two aspects: the quality of derived state machines and the effort required to build them. With AspectSM, a crosscutting behavior is modeled using an aspect state machine. The applicability of aspect state machines is evaluated by comparing them with standard UML state machines that directly model the entire system behavior, including crosscutting concerns. The quality of both aspect and standard UML state machines derived by subjects is measured by comparing them against predefined reference state machines. Results show that aspect state machines derived with AspectSM are significantly more complete and correct though AspectSM took significantly more time than the standard approach.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569733,no
Multi-objective Cross-Project Defect Prediction,"Cross-project defect prediction is very appealing because (i) it allows predicting defects in projects for which the availability of data is limited, and (ii) it allows producing generalizable prediction models. However, existing research suggests that cross-project prediction is particularly challenging and, due to heterogeneity of projects, prediction accuracy is not always very good. This paper proposes a novel, multi-objective approach for cross-project defect prediction, based on a multi-objective logistic regression model built using a genetic algorithm. Instead of providing the software engineer with a single predictive model, the multi-objective approach allows software engineers to choose predictors achieving a compromise between number of likely defect-prone artifacts (effectiveness) and LOC to be analyzed/tested (which can be considered as a proxy of the cost of code inspection). Results of an empirical evaluation on 10 datasets from the Promise repository indicate the superiority and the usefulness of the multi-objective approach with respect to single-objective predictors. Also, the proposed approach outperforms an alternative approach for cross-project prediction, based on local prediction upon clusters of similar classes.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569737,no
Analysis and Prediction of Mandelbugs in an Industrial Software System,"Mandelbugs are faults that are triggered by complex conditions, such as interaction with hardware and other software, and timing or ordering of events. These faults are considerably difficult to detect with traditional testing techniques, since it can be challenging to control their complex triggering conditions in a testing environment. Therefore, it is necessary to adopt specific verification and/or fault-tolerance strategies for dealing with them in a cost-effective way. In this paper, we investigate how to predict the location of Mandelbugs in complex software systems, in order to focus V&V activities and fault tolerance mechanisms in those modules where Mandelbugs are most likely present. In the context of an industrial complex software system, we empirically analyze Mandelbugs, and investigate an approach for Mandelbug prediction based on a set of novel software complexity metrics. Results show that Mandelbugs account for a noticeable share of faults, and that the proposed approach can predict Mandelbug-prone modules with greater accuracy than the sole adoption of traditional software metrics.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569738,no
Estimating Fault Numbers Remaining After Testing,"Testing is an essential component of the software development process, but also one which is exceptionally difficult to manage and control. For example, it is well understood that testing techniques are not guaranteed to detect all faults, but more frustrating is that after the application of a testing technique the tester has little or no knowledge of how many faults might still be left undiscovered. This paper investigates the performance of a range of capture-recapture models to determine the accuracy with which they predict the number of defects remaining after testing. The models are evaluated with data from two empirical testing-related studies and from one larger publicly available project and the factors affecting the accuracy of the models are analysed. The paper also considers how additional information (such as structural coverage data) may be used to improve the accuracy of the estimates. The results demonstrate that diverse sets of faults resulting from different testers using different techniques tend to produce the most accurate results, and also illustrate the sensitivity of the estimators to the patterns of fault data.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569739,no
Oracle-based Regression Test Selection,"Regression test selection (RTS) techniques attempt to reduce regression testing costs by selecting a subset of a software system's test cases for use in testing changes made to that system. In practice, RTS techniques may select inordinately large sets of test cases, particularly when applied to industrial systems such as those developed at ABB, where code changes may have far-reaching impact. In this paper, we present a new RTS technique that addresses this problem by focusing on specific classes of faults that can be detected by internal oracles - oracles (rules) that enforce constraints on system states during system execution. Our technique uses program chopping to identify code changes that are relevant to internal oracles, and selects test cases that cover these changes. We present the results of an empirical study that show that our technique is more effective and efficient than other RTS techniques, relative to the classes of faults targeted by the internal oracles.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569741,no
JAutomate: A Tool for System- and Acceptance-test Automation,"System- and acceptance-testing are primarily performed with manual practices in current software industry. However, these practices have several issues, e.g. they are tedious, error prone and time consuming with costs up towards 40 percent of the total development cost. Automated test techniques have been proposed as a solution to mitigate these issues, but they generally approach testing from a lower level of system abstraction, leaving a gap for a flexible, high system-level test automation technique/tool. In this paper we present JAutomate, a Visual GUI Testing (VGT) tool that fills this gap by combining image recognition with record and replay functionality for high system-level test automation performed through the system under test's graphical user interface. We present the tool, its benefits compared to other similar techniques and manual testing. In addition, we compare JAutomate with two other VGT tools based on their static properties. Finally, we present the results from a survey with industrial practitioners that identifies test-related problems that industry is currently facing and discuss how JAutomate can solve or mitigate these problems.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569758,no
AURORA: AUtomatic RObustness coveRage Analysis Tool,"Code coverage is usually used as a measurement of testing quality and as adequacy criterion. Unfortunately, code coverage is very sensitive to modifications of the code structure, and, therefore, we can achieve the same degree of coverage with different testing effort by writing the same program in syntactically different ways. For this reason, code coverage can provide the tester with misleading information. In order to understand how a testing criterion is affected by code structure modifications, we have introduced a way to measure the sensitivity of coverage to code changes by means of code-to-code transformations. However the manual execution of the robustness analysis is tedious, time consuming and error prone. In order to solve these issues we present AURORA, a tool that automates the robustness analysis process and leverages the capabilities offered from several existing tools. AURORA has an extendible architecture that concretely supports the tester in the execution of the robustness analysis. Due to this extendible architecture, each user can personalize the robustness analysis to his/her needs. AURORA allows the user to add new transformations by using TXL, which is a programming language specifically designed to support source transformation tasks. It performs the coverage evaluation by using existing code coverage tools and is based on the use of the JUnit framework.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569761,no
A Toolchain for Designing and Testing XACML Policies,"In modern pervasive application domains, such as Service Oriented Architectures (SOAs) and Peer-to-Peer (P2P) systems, security aspects are critical. Justified confidence in the security mechanisms that are implemented for assuring proper data access is a key point. In the last years XACML has become the de facto standard for specifying policies for access control decisions in many application domains. Briefly, an XACML policy defines the constraints and conditions that a subject needs to comply with for accessing a resource and doing an action in a given environment. Due to the complexity of the language, XACML policy specification is a difficult and error prone process that requires specific knowledge and a high effort to be properly managed.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569771,no
GUIdiff -- A Regression Testing Tool for Graphical User Interfaces,"Due to the rise of tablets and smart phones and their impact on everyday life, robust and high-quality Graphical User Interfaces (GUIs) are becoming more and more important. Unfortunately, testing these GUIs still remains a big challenge with the current industrial tools, which only cater to manual testing practices and provide limited oracle functionalities such as screenshot comparison. These tools often result in large amounts of manual labor and thus increase cost. We propose a new GUI regression testing tool called GUIdiff, which works similar to diff tools for text data. It executes two different versions of a System Under Test (SUT) side by side, compares the GUI states against each other and presents the list of the detected deviations to the tester. The tool is semi-automatic in the sense that it finds the differences completely automatic and that the tester labels them as faults or false positives.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569773,no
Identification of Anomalies in Processes of Database Alteration,"Data, especially in large item sets, hide a wealth of information on the processes that have created and modified them. Often, a data-field or a set of data-fields are not modified only through well-defined processes, but also through latent processes; without the knowledge of the second type of processes, testing cannot be considered exhaustive. As a matter of fact, changes in the data deriving from unknown processes can cause anomalies not detectable by testing, which focuses on known data variation rules. History of data variations can yield information about the nature of the changes. In my work I focus on the elicitation of an evolution profile of data: the values data may assume, the change frequencies, the temporal variation of a piece of data in relation to other data, or other constraints that are directly connected to the reference domain. The profile of evolution is then used to detect anomalies in the database state evolution. Detecting anomalies in the database state evolution could strengthen the quality of a system, since a data anomaly could be the signal of a defect in the applications populating the database.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569780,no
CDCGM Track Report,"The Convergence of distributed clouds, grids and their management conference track focuses on virtualization and cloud computing as they enjoy wider acceptance. A recent IDC report predicts that by 2016, $1 of every $5 will be spent on cloud-based software and infrastructure. Three papers address key issues in cloud computing such as resource optimization and scaling to address changing workloads and energy management. In addition, the DIME network architecture proposed in WETICE2010 is discussed in two papers in this conference, both showing its usefulness in addressing fault, configuration, accounting, performance and security of service transactions with in the service oriented architecture implementation and also spanning across multiple clouds. While virtualization has brought resource elasticity and application agility to the services infrastructure management, the resulting layers of orchestration and the lack of end-to-end service visibility and control spanning across multiple service provider infrastructure have added an alarming degree of complexity. Hopefully, reducing the complexity in the next generation datacenters will be a major research topic in this conference.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6570598,no
Mutation Operators for the Atlas Transformation Language,"Faults in model transformations will result in defective models, and eventually defective code. Correction of defects at the code level is considered very late and is often expensive. Uncorrected defects in the models will propagate to other artifacts; thus, adversely affecting the quality of the end product. Moreover, defect propagation may result in a system that does not meet the stakeholders' requirements. Therefore, model transformations must be thoroughly tested to maintain product quality, while keeping development cost at reasonable levels. Existing literature on model transformation verification and validation has considered coverage based techniques. Mutation testing is a popular technique that has been extensively studied in the literature, and shown to perform better than coverage based techniques. To support the mutation testing of model transformations, this paper proposes a suite of mutation operators for the Atlas Transformation language (ATL). The effectiveness of the proposed operators is evaluated using a model transformation program, implemented in ATL, to transform Use Case Maps models to UML Activity Diagrams. The results show that the proposed operators can successfully detect inadequacies in an example test suite.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6571607,no
Efficient Mutation Analysis of Relational Database Structure Using Mutant Schemata and Parallelisation,"Mutation analysis is an effective way to assess the quality of input values and test oracles. Yet, since this technique requires the generation and execution of many mutants, it often incurs a substantial computational cost. In the context of program mutation, the use of mutant schemata and parallelisation can reduce the costs of mutation analysis. This paper is the first to apply these approaches to the mutation analysis of a relational database schema, arguably one of the most important artefacts in a database application. Using a representative set of case studies that vary in both their purpose and structure, this paper empirically compares an unoptimised method to four database structure mutation techniques that intelligently employ both mutant schemata and parallelisation. The results of the experimental study highlight the performance trade-offs that depend on the type of database management system (DBMS), underscoring the fact that every DBMS does not support all types of efficient mutation analysis. However, the experiments also identify a method that yields a one to ten times reduction in the cost of mutation analysis for relational schemas hosted by both the Postgres and SQLite DBMSs.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6571609,no
Conditional-Based Refactorings and Fault-Proneness: An Empirical Study,"Recent empirical work has shown that some of the most frequently applied Java-based refactorings relate to the manipulation of code conditionals and flags. The logic of such code is often complex and difficult to test regressively. One open research issue thus relates to the fault-proneness profiles of classes where these refactorings have been applied, vis-a-vis refactorings on other classes. In this paper, we explore six releases of three Eclipse projects and the faults in the refactored classes of those releases. We explore four specific conditional-based refactorings and the supposition that: classes where these four refactorings have been applied will tend to have relatively higher fault incidences because of the inherent complexity of the embedded logic given by the constructs they operate on. Results showed that one of the four refactorings in particular had been applied to classes with higher fault profiles - the `Replace Nested Conditional with Guard Clauses' refactoring. Some evidence that the `Remove Control Flag' refactoring had also been applied to relatively highly fault-prone classes was found. Relative to other types of refactoring, the result thus suggests that these two effectively signpost fault-prone classes.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6571612,no
A Process for Assessing Data Quality,"This industrial contribution describes a tool support approach to assessing the quality of relational databases. The approach combines two separate audits - an audit of the database structure as described in the schema and an audit of the database content at a given point in time. The audit of the database schema checks for design weaknesses, data rule violations and deviations from the original data model. It also measures the size, complexity and structural quality of the database. The audit of the database content compares the state of selected data attributes to identify incorrect data and checks for missing and redundant records. The purpose is to initiate a data clean-up process to ensure or restore the quality of the data.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6571617,no
Model-Based Test Suite Generation for Function Block Diagrams Using the UPPAAL Model Checker,"A method for model-based test generation of safety-critical embedded applications using Programmable Logic Controllers and implemented in a programming language such as Function Block Diagram (FBD) is described. The FBD component model is based on the IEC 1131 standard and it is used primarily for embedded systems, in which timeliness is an important property to be tested. Our method involves the transformation of FBD programs with timed annotations into timed automata models which are used to automatically generate test suites. Specifically we demonstrate how to use model transformation for formalization and model-checking of FBD programs using the UPPAAL tool. Many benefits emerge from this method, including the ability to automatically generate test suites from a formal model in order to ensure compliance to strict quality requirements including unit testing and specific coverage measurements. The approach is experimentally assessed on a train control system in terms of consumed resources.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6571626,no
Evaluation of t-wise Approach for Testing Logical Expressions in Software,"Pair-wise and, more generally, t-wise testing are the most common and powerful combinatorial testing approaches. This paper investigates the effectiveness of the t-wise approach for testing logical expressions in software in terms of its fault detecting capabilities. Effectiveness is evaluated experimentally using special software tools for generating logical expressions and t-wise test cases, simulating faults in expressions, testing faulty expressions, and evaluating effectiveness of the testing. T-wise testing effectiveness is measured in its totality and for specific types of faults; it is then compared with random testing. A detailed analysis of the experimental results is also provided.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6571640,no
On Use of Coverage Metrics in Assessing Effectiveness of Combinatorial Test Designs,"Combinatorial test suite design is a test generation technique, popular in part due to its ability to achieve coverage and defect finding power approximating that of exhaustive testing while keeping test suite sizes constrained. In recent years, there have been numerous advances in combinatorial test design techniques, in terms of efficiency and usability of methods used to create them as well as in understanding of their benefits and limitations when applied to real world software. Numerous case studies have appeared presenting practical applications of the combinatorial test suite design techniques, often comparing them with manually-created, random, or exhaustive suites. These comparisons are done either in terms of defects found or by applying some code coverage metric. Since many different and valid combinatorial test suites of strength t can be created for a given test domain, the question whether they all have the same coverage properties is a pertinent one. In this paper we explore the stability of size and coverage of combinatorial test suites. We find that in general coverage levels increase and coverage variability decreases with increasing order of combinations t; however we also find exceptions with implications for practitioners. In addition, we explore cases where coverage achieved by combinatorial test suites of order t applied to the same program is not different from test suites of order t-1. Lastly, we discuss these findings in context of the ongoing practice of applying code coverage metrics to measure effectiveness of combinatorial test suites.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6571641,no
Identifying Failure-Inducing Combinations Using Tuple Relationship,"Combinatorial testing (CT) aims at detecting interaction failures between parameters in a system. Identifying the failure-inducing combinations of a failing test configuration can help developers find the cause of this failure. However, most studies in CT focus on detecting the failures rather than identifying failure-inducing combinations. In this paper, we propose the notion of a tuple relationship tree (TRT) to describe the relationships among all the candidate parameter interactions. TRT reduces additional test configurations that need to be generated in the fault localization process, and it also provides a clear view of all possible candidate interactions. As a result, our approach will not omit any possible interaction that could be the cause of a failure. In particular, we can identify multiple failure-inducing combinations that overlap with each other. Moreover, we extend our approach to handle the case where additional failure-inducing combinations may be introduced by newly generated test configurations.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6571643,no
Combinatorial Interaction Testing with Multi-perspective Feature Models,"Testing product lines and similar software involves the important task of testing feature interactions. The challenge is to test all those feature interactions that result in testing of all variations across all dimensions of variation. In this context, we propose the use of combinatorial test generation, with Multi-Perspective Feature Models (MPFM) as the input model. MPFMs are a set of feature models created to achieve Separation of Concerns within the model. We believe that the MPFM is useful as an input model for combinatorial testing and it is easy to create and understand. This approach helps achieve a better coverage of variability in the product line. Results from an experiment on a real-life case show that up to 37% of the test effort could be reduced and up to 79% defects from the live system could be detected.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6571649,no
Applying Combinatorial Testing to the Siemens Suite,"Combinatorial testing has attracted a lot of attention from both industry and academia. A number of reports suggest that combinatorial testing can be effective for practical applications. However, there are few systematic, controlled studies on the effectiveness of combinatorial testing. In particular, input parameter modeling is a key step in the combinatorial testing process. But most studies do not report the details of the modeling process. In this paper, we report an experiment that applies combinatorial testing to the Siemens suite. The Siemens suite has been used as a benchmark to evaluate the effectiveness of many testing techniques. Each program in the suite has a number of faulty versions. The effectiveness of combinatorial testing is measured in terms of the number of faulty versions that are detected. The experimental results show that combinatorial testing is effective in terms of detecting most of the faulty versions with a small number of tests. In addition, we report the details of our modeling process, which we hope to shed some lights on this critical, yet often ignored step, in the combinatorial testing process.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6571654,no
On Adequacy of Assertions in Automated Test Suites: An Empirical Investigation,"An integral part of test case is the verification phase (also called `test oracle'), which verifies program's state, output or behavior. In automated testing, the verification phase is often implemented using test assertions which are usually developed manually by testers. More precisely, assertions are used for checking the unit or the system's behavior (or output) which is reflected by the changes in the data fields of the class under test, or the output of the function under test. Originated from human (testers') error, test suites are prone to having inadequate assertions. The paper reports an empirical study on the Inadequate-Assertion (IA) problem in the context of automated test suites developed for open-source projects. In this study, test suites of three active open-source projects have been chosen. To investigate IA problem occurrence among the sampled test suites, we performed mutation analysis and coverage analysis. The results indicate that: (1) the IA problem is common among the sampled open-source projects, and the occurrence varies from project to project and from package to package, and (2) the occurrence rate of the IA problem is positively co-related with the complexity of test code.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6571656,no
Search-Based Propagation of Regression Faults in Automated Regression Testing,"Over the lifetime of software programs, developers make changes by adding, removing, enhancing functionality or by refactoring code. These changes can sometimes result in undesired side effects in the original functionality of the software, better known as regression faults. To detect these, developers either have to rely on an existing set of test cases, or have to create new tests that exercise the changes. However, simply executing the changed code does not guarantee that a regression fault manifests in a state change, or that this state change propagates to an observable output where it could be detected by a test case. To address this propagation aspect, we present EVOSUITER, an extension of the EVOSUITE unit test generation tool. Our approach generates tests that propagate regression faults to an observable difference using a search-based approach, and captures this observable difference with test assertions. We illustrate on an example program that EVOSUITER can be effective in revealing regression errors in cases where alternative approaches may fail, and motivate further research in this direction.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6571658,no
The Forth International Workshop on Security Testing (SECTEST 2013),"To improve software security, several techniques, including vulnerability modelling and security testing, have been developed but the problem remains unsolved. On one hand, the SECTEST workshop tries to answer how vulnerability modelling can help users understand the occurrence of vulnerabilities so to avoid them, and what the advantages and drawbacks of the existing models are to represent vulnerabilities. At the same time, the workshop tries to understand how to solve the challenging security testing problem given that testing the mere functionality of a system alone is already a fundamentally critical task, how security testing is different from and related to classical functional testing, and how to assess the quality of security testing. The objective of this workshop is to share ideas, methods, techniques, and tools about vulnerability modelling and security testing to improve the state of the art.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6571666,no
An Empirical Study on Data Retrievability in Decentralized Erasure Code Based Distributed Storage Systems,"Erasure codes are applied in distributed storage systems to provide data robustness against server failures by storing data redundancy among many storage servers. A (n, k) erasure code encodes a data object, which is represented as k elements, into a codeword of n elements such that any k out of these n codeword elements can recover the data object back. Decentralized erasure codes are proposed for distributed storage systems without a central authority. The characteristic of decentralization makes resulting storage systems more scalable and suitable for loosely-organized networking environments. However, different from conventional erasure codes, decentralized erasure codes trade some probability of a successful data retrieval for decentralization. Although theoretical lower bounds on the probability are overwhelming from a theoretical aspect, it is essential to know what the data retrievability is in real applications from a practical aspect. We focus on decentralized erasure code based storage systems and investigate data retrievability from both theoretical and practical aspects. We conduct simulation for random processes of storage systems to evaluate data retrievability. Then we compare simulation results and analytical values from theoretical bounds. By our comparison, we find that data retrievability is underestimated by those bounds. Data retrievability is over 99% in most cases in our simulations, where the order of the used finite field is an 8-bit prime. Data retrievability can be enlarged by using a larger finite field. We believe that data retrievability of decentralized erasure code based storage systems is acceptable for real applications.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6571693,no
Improving Service Diagnosis through Increased Monitoring Granularity,"Due to their loose coupling and highly dynamic nature, service-oriented systems offer many benefits for realizing fault tolerance and supporting trustworthy computing. They enable automatic system reconfiguration in case that a faulty service is detected. Spectrum-based fault localization (SFL) is a statistics-based diagnosis technique that can effectively be applied to pinpoint problematic services. It works by monitoring service usage in system transactions and comparing service coverage with pass/fail observations. SFL exhibits poor performance in diagnosing faulty services in cases when services are tightly coupled. In this paper, we study how and to which extent an increase in monitoring granularity can help to improve correct diagnosis of tightly coupled faulty services. We apply SFL in a real service-based system, for which we show that 100% correct identification of faulty services can be achieved through an increase in the monitoring granularity.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6571703,no
The Determination Method for Software Reliability Qualitative Indices,"The determination of software reliability indices is the primary task in the software reliability engineering. The indices are taken as not only the basis for the software reliability design and the constraints during the software development process, but also the foundation of the software's acceptance. Software reliability indices are usually divided into quantitative indices and qualitative indices. Quantitative indices are quantified software reliability parameters' values, such as software reliability is quantitatively defined as the probability of failure-free operation of a software program for a specified time under specified conditions. However, having a number, even with the appropriate accompanying evidence, is not generally sufficient to convince customers or even the system/software suppliers that the software satisfies its requirements. Thus, qualitative indices such as software reliability is also qualitatively defined as a set of attributes that bear on the capability of software to maintain its level of performance under stated conditions for a stated period of time. Attributes that relate to implementation of fault tolerance design, use of best engineering practices, application of specialized methods and techniques for ensuring reliability-critical requirements, and procedural methods to ensure mistake-proof loading and/or operation also provide evidence that improves the confidence that the software will not cause a system failure. So the qualitative indices can be regarded as the requirements for software reliability activities throughout the development process. Unfortunately, currently there is no systematic theory and approach for software reliability indices' determination. This paper proposes a method for determining the software reliability qualitative indices based on the two standards of SAE-JAI 003 and RTCA DO-I 78B which are widely used by the airworthiness and industrial sectors, as well as the best practices and- management experiences of software reliability engineering. This paper proposes the method's principle, which determines the software reliability qualitative indices according to the profile formed by all stages of the software life cycle and the environment requirements, technique requirements, validation requirements and management requirements. Combined with the software's criticality levels, this paper also proposes a generic framework which recommends a variety of tailoring mechanisms and building guidelines to help users develop their demanded indices.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6571714,no
Quality perception in 3D interactive environments,"In this paper we investigate how configuration and visualization parameters influence the quality of experience in a 3D interactive environment, more specifically in a motion parallax setup. In order to do so, we designed a dedicated testing room and conducted subjective experiments with a team of evaluators. The tests considered parameters such as different disparities, amount of parallax, monitor sizes and visualization angles. Factors such as visual comfort, sense of immersion and the 3D experience as a whole have been assessed. The users were also asked to execute a task in the 3D motion parallax environment assessing the difficulty to complete the task for different parameter configurations. Obtained results suggest that user experience in an immersive environment is not as critically influenced by configuration parameters such as disparity and amount of parallax as initially thought. They also indicate that a better understanding of how this experience is influenced still requires the design and conduction of more comprehensive testing procedures.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6571769,no
TURNUS: A design exploration framework for dataflow system design,"While research on the design of heterogeneous concurrent systems has a long and rich history, a unified design methodology and tool support has not emerged so far, and thus the creation of such systems remains a difficult, time-consuming and error-prone process. The absence of principled support for system evaluation and optimization at high abstraction levels makes the quality of the resulting implementation highly dependent on the experience or prejudices of the designer. This is particularly critical when the combinatorial explosion of design parameters overwhelms available optimization tools. In this work we address these matters by presenting a unified design exploration framework suitable for a wide range of different target platforms. The design is unified and implemented at high level by using a standard dataflow language, while the target platform is described using the IP-XACT standard. This facilitates different design space heuristics that guide the designer during validation and optimization stages without requiring low-level implementations of parts of the application. Our framework currently yields exploration and optimization results in terms of application throughput and buffer size dimensioning, although other co-exploration and optimization heuristics are available.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6571927,no
Pattern generation for Mutation Analysis using Genetic Algorithms,"Mutation Analysis (MA) is a fault-based simulation technique that is used to measure the quality of testbenches for mutant detections where mutants are simple syntactical changes in the designs. A mutant is said living if its error effect cannot be observed at the primary outputs. Previous works mainly focused on the cost reduction in the process of MA, because the MA is a computation intensive process in the commercial tool. For the living mutants, to the best of our knowledge, the commercial tool has not addressed the pattern generation issue yet. Thus, this paper presents a Genetic Algorithm to generate patterns for detecting living mutants such that the quality of the verification environment is improved. The experimental results show that more living mutants can be detected after adding the generated patterns in the testbench.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6572397,no
A collaborative software development model based on formal concept analysis and stable matching,"As the world shrinks into a global village, software development processes seek cooperation from multiple teams that are spread across the globe and possess their own unique capabilities and skills. Studies indicate that the Collaborative Software Development model has several advantages such as increased productivity and cost efficiency. However, it also poses the challenge of coordination and task assignment between dispersed and heterogeneous teams. In this paper, we propose a Formal Concept Analysis based model for skills oriented mapping between disparate teams and a set of software development tasks within a distributed and collaborative software development environment in a manner that is efficient, economical and fault-tolerant. Concepts extracted in the form of teams exhibiting common skills and software development tasks requiring specific skills are matched by using an extended version of the Stable Marriage Problem. The stable pairs so obtained are subsequently pruned with the objective of either minimizing the cost of allocation or maximizing the continuity of tasks. We also assess the redundancy for each task. Experimental results demonstrate the efficacy of our approach.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6572676,no
Identifying the root cause of failures in IT changes: Novel strategies and trade-offs,"Despite the Change and Problem Management have received significant attention from the academic community in recent years, the developed solutions do not identify the root cause of failures in IT Changes and, in some cases, only detect software failures. To address this, in this paper, we introduce four strategies to identify root cause of problems based on an interactive approach, in which the Diagnosis System questions a human operator. The strategies introduced and evaluated in this paper are built upon a system we have developed previously, but whose root cause identification was more rudimentary. A case study that uses the improved solution is conducted for the purpose of analyzing the diagnostics generated. Thus, it was possible to compare the diagnostics generated by each strategy, identifying any trends.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6572977,no
PReSET: A toolset for the evaluation of network resilience strategies,"Computer networks support many of the services that our society relies on. Therefore, ensuring their resilience to faults and challenges, such as attacks, is critical. To do this can require the execution of resilience strategies that perform dynamic reconfiguration of networks, including resilience-specific functionality. It is important that resilience strategies are evaluated prior to their execution, for example, to ensure they will not exacerbate an on-going problem. To facilitate this activity, we have developed a toolset that supports the evaluation of resilience strategies that are specified as event-driven policies. The toolset couples the Ponder2 policy-based management framework and the OMNeT++ simulation environment. In this paper, we discuss the network resilience problem and motivate simulation as a suitable way to evaluate resilience strategies. We describe the toolset we have developed, including its architecture and the implementation of a number of resilience mechanisms, and its application to evaluating strategies that detect and mitigate Internet worm behaviour.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6572987,no
Detecting software aging in a cloud computing framework by comparing development versions,"Software aging, i.e. degradation of software performance or functionality caused by resource depletion is usually discovered only in the production scenario. This incurs large costs and delays of defect removal and requires provisional solutions such as rejuvenation (controlled restarts). We propose a method for detecting aging problems shortly after their introduction by runtime comparisons of different development versions of the same software. Possible aging issues are discovered by analyzing the differences in runtime traces of selected metrics. The required comparisons are workload-independent which minimizes the additional effort of dedicated stress tests. Consequently, the method requires only minimal changes to the traditional development and testing process. This paves the way to detecting such problems before public releases, greatly reducing the cost of defect fixing. Our study focuses on the memory leaks of Eucalyptus, a popular open source framework for managing cloud computing environments.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6573106,no
Universal Script Wrapper  An innovative solution to manage endpoints in large and heterogeneous environment,"Endpoint management is a key function for data center management and cloud management. Today's practice to manage endpoints for the enterprise is labor intensive, tedious and error prone. In this paper, we present Universal Script Wrapper, an innovative solution which provides a unique solution to allow users to manage a group of endpoints as if logged in to one endpoint. It harvests proven scripts and makes them available to automate management tasks on endpoints without modification. To reduce risk, it provides a mechanism to guard against intrusive commands and scripts that could cause massive damage to the infrastructure.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6573111,no
Pattern detection in unstructured data: An experience for a virtualized IT infrastructure,"Data-agnostic management of today's virtualized and cloud IT infrastructures motivates statistical inference from unstructured or semi-structured data. We introduce a universal approach to the determination of statistically relevant patterns in unstructured data, and then showcase its application to log data of a Virtual Center (VMware's virtualization management software). The premise of this study is that the unstructured data can be converted into events, where an event is defined by time, source, and a series of attributes. Every event can have any number of attributes but all must have a time stamp and optionally a source of origination (be it a server, a location, a business process, etc.) The statistical relevance of the data can then be made clear via determining the joint and prior probabilities of events using a discrete probability computation. From this we construct a Directed Virtual Graph with nodes representing events and the branches representing the conditional probabilities between two events. Employing information-theoretic measures the graphs are reduced to a subset of relevant nodes and connections. Moreover, the information contained in the unstructured data set is extracted from these graphs by detecting particular patterns of interest.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6573128,no
Video quality monitoring based on precomputed frame distortions,"In the past decade, video streaming has taken over a large part of the current Internet traffic and more and more TV broadcasters and network providers extend their portfolio of video streaming services. With the growing expectations of video consumers with respect to the service quality, monitoring is an important aspect for network providers to detect possible performance problems or high network load. In parallel, emerging technologies like software defined networking or network virtualization introduce support for specialized networks which allow enhanced functionality in the network. This development enables more sophisticated monitoring techniques in the specialized networks which use knowledge about the video content to better predict the service quality at consumers. In this work, we present a SSIM-based monitoring technique and compare it with the current state-of-the-art which infers the service quality from the monitored packet loss. We further show how network conditions like packet loss or bursts influence the two different monitoring techniques.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6573182,no
System synthesis from UML/MARTE models: The PHARAON approach,"Model-Driven Engineering (MDE) based on UML is a mature methodology for software development. However, its application to HW/SW embedded system specification and design requires specific features not covered by the language. For this reason, the MARTE profile for Real-Time and Embedded systems was defined. It has proven to be powerful enough to support holistic system modeling under different views. This single-source model is able to capture the required information, enabling the automatic generation of executable and configurable models for fast performance analysis without requiring additional engineering effort. As a result of this performance analysis suitable system architecture can be decided. At this point, the SW stack to be executed by each processing node in the selected heterogeneous platform has to be generated. In the general case this is a tedious and error-prone process with little assistance from available tools. Current practices oblige the SW engineer to develop the code for each node of the heterogeneous multi-core platform by hand. The code has to be written specifically for the selected architecture and architectural mapping, thus reducing reusability. In order to overcome this limitation, the FP7 PHARAON project aims to develop tools able to automatically generate the code to be executed in each node from the initial system model. This affects not only the application code, the static and run-time libraries (e.g. OpenMP/OpenCL), the middleware and communication functions, but also the OS and the driver calls in each node.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6573222,no
A Petri-Net-Based Approach to Reliability Determination of Ontology-Based Service Compositions,"Ontology Web Language for Services (OWL-S), one of the most significant semantic Web service ontologies proposed to date, provides a core ontological framework and guidelines for describing the properties and capabilities of services in an unambiguous computer-interpretable form. Analysis of the quality of service of composite service processes specified in OWL-S enables service users to decide whether the process meets nonfunctional requirements. In this paper, we propose a probabilistic approach for reliability analysis of OWL-S processes, employing the non-Markovian stochastic Petri net (NMSPN) as the fundamental model. Based on the NMSPN representations of the OWL-S elements, we introduce an analytical method for the calculation of the process-normal-completion probability as the reliability estimate. This method takes the probabilistic parameters of service invocations and messages as model inputs. To validate the feasibility and accuracy of our approach, we obtain runtime experimental data and conduct a confidence interval analysis in a case study. A sensitivity analysis is also performed to determine the impact of model parameters on reliability and to help identify the reliability bottlenecks.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6573405,no
Improving the Trustworthiness of Medical Device Software with Formal Verification Methods,"Wearable and implantable medical devices are commonly used for diagnosing, monitoring, and treating various medical conditions. Increasingly complex software and wireless connectivity have enabled great improvements in the quality of care and convenience for users of such devices. However, an unfortunate side-effect of these trends has been the emergence of security concerns. In this letter, we propose the use of formal verification techniques to verify temporal safety properties and improve the trustworthiness of medical device software. We demonstrate how to bridge the gap between traditional formal verification and the needs of medical device software. We apply the proposed approach to cardiac pacemaker software and demonstrate its ability to detect a range of software vulnerabilities that compromise security and safety.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6574212,no
A Learning-Based Framework for Engineering Feature-Oriented Self-Adaptive Software Systems,"Self-adaptive software systems are capable of adjusting their behavior at runtime to achieve certain functional or quality-of-service goals. Often a representation that reflects the internal structure of the managed system is used to reason about its characteristics and make the appropriate adaptation decisions. However, runtime conditions can radically change the internal structure in ways that were not accounted for during their design. As a result, unanticipated changes at runtime that violate the assumptions made about the internal structure of the system could degrade the accuracy of the adaptation decisions. We present an approach for engineering self-adaptive software systems that brings about two innovations: 1) a feature-oriented approach for representing engineers' knowledge of adaptation choices that are deemed practical, and 2) an online learning-based approach for assessing and reasoning about adaptation decisions that does not require an explicit representation of the internal structure of the managed software system. Engineers' knowledge, represented in feature-models, adds structure to learning, which in turn makes online learning feasible. We present an empirical evaluation of the framework using a real-world self-adaptive software system. Results demonstrate the framework's ability to accurately learn the changing dynamics of the system while achieving efficient analysis and adaptation.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6574860,no
"Conservative Reasoning about the Probability of Failure on Demand of a 1-out-of-2 Software-Based System in Which One Channel Is """"Possibly Perfect""""","In earlier work, [11] (henceforth LR), an analysis was presented of a 1-out-of-2 software-based system in which one channel was possibly perfect. It was shown that, at the aleatory level, the system pfd (probability of failure on demand) could be bounded above by the product of the pfd of channel A and the pnp (probability of nonperfection) of channel B. This result was presented as a way of avoiding the well-known difficulty that for two certainly-fallible channels, failures of the two will be dependent, i.e., the system pfd cannot be expressed simply as a product of the channel pfds. A price paid in this new approach for avoiding the issue of failure dependence is that the result is conservative. Furthermore, a complete analysis requires that account be taken of epistemic uncertainty-here concerning the numeric values of the two parameters pfd<sub>A</sub> and pnp<sub>B</sub>. Unfortunately this introduces a different difficult problem of dependence: estimating the dependence between an assessor's beliefs about the parameters. The work reported here avoids this problem by obtaining results that require only an assessor's marginal beliefs about the individual channels, i.e., they do not require knowledge of the dependence between these beliefs. The price paid is further conservatism in the results.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6574864,no
Mars science laboratory frame manager for centralized frame tree database and target pointing,"The FM (Frame Manager) flight software module is responsible for maintaining the frame tree database containing coordinate transforms between frames. The frame tree is a proper tree structure of directed links, consisting of surface and rover subtrees. Actual frame transforms are updated by their owner. FM updates site and saved frames for the surface tree. As the rover drives to a new area, a new site frame with an incremented site index can be created. Several clients including ARM and RSM (Remote Sensing Mast) update their related rover frames that they own. Through the onboard centralized FM frame tree database, client modules can query transforms between any two frames. Important applications include target image pointing for RSM-mounted cameras and frame-referenced arm moves. The use of frame tree eliminates cumbersome, error-prone calculations of coordinate entries for commands and thus simplifies flight operations significantly.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6575252,no
Hector: Detecting Resource-Release Omission Faults in error-handling code for systems software,"Omitting resource-release operations in systems error handling code can lead to memory leaks, crashes, and deadlocks. Finding omission faults is challenging due to the difficulty of reproducing system errors, the diversity of system resources, and the lack of appropriate abstractions in the C language. To address these issues, numerous approaches have been proposed that globally scan a code base for common resource-release operations. Such macroscopic approaches are notorious for their many false positives, while also leaving many faults undetected. We propose a novel microscopic approach to finding resource-release omission faults in systems software. Rather than generalizing from the entire source code, our approach focuses on the error-handling code of each function. Using our tool, Hector, we have found over 370 faults in six systems software projects, including Linux, with a 23% false positive rate. Some of these faults allow an unprivileged malicious user to crash the entire system.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6575307,no
An algorithmic approach to error localization and partial recomputation for low-overhead fault tolerance,"The increasing size and complexity of massively parallel systems (e.g. HPC systems) is making it increasingly likely that individual circuits will produce erroneous results. For this reason, novel fault tolerance approaches are increasingly needed. Prior fault tolerance approaches often rely on checkpoint-rollback based schemes. Unfortunately, such schemes are primarily limited to rare error event scenarios as the overheads of such schemes become prohibitive if faults are common. In this paper, we propose a novel approach for algorithmic correction of faulty application outputs. The key insight for this approach is that even under high error scenarios, even if the result of an algorithm is erroneous, most of it is correct. Instead of simply rolling back to the most recent checkpoint and repeating the entire segment of computation, our novel resilience approach uses algorithmic error localization and partial recomputation to efficiently correct the corrupted results. We evaluate our approach in the specific algorithmic scenario of linear algebra operations, focusing on matrix-vector multiplication (MVM) and iterative linear solvers. We develop a novel technique for localizing errors in MVM and show how to achieve partial recomputation within this algorithm, and demonstrate that this approach both improves the performance of the Conjugate Gradient solver in high error scenarios by 3x-4x and increases the probability that it completes successfully by up to 60% with parallel experiments up to 100 nodes.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6575309,no
A view on the past and future of fault injection,"Fault injection is a well-known technology that enables assessing dependability attributes of computer systems. Many works on fault injection have been developed in the past, and fault injection has been used in different application domains. This fast abstract briefly revises previous applications of fault injection, especially for embedded systems, and puts forward ideas on its future use, both in terms of application areas and business markets.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6575332,no
Error detector placement for soft computation,"The scaling of Silicon devices has exacerbated the unreliability of modern computer systems, and power constraints have necessitated the involvement of software in hardware error detection. At the same time, emerging workloads in the form of soft computing applications, (e.g., multimedia applications) can tolerate most hardware errors as long as the erroneous outputs do not deviate significantly from error-free outcomes. We term outcomes that deviate significantly from the error-free outcomes as Egregious Data Corruptions (EDCs). In this study, we propose a technique to place detectors for selectively detecting EDC causing errors in an application. We performed an initial study to formulate heuristics that identify EDC causing data. Based on these heuristics, we developed an algorithm that identifies program locations for placing high coverage detectors for EDCs using static analysis.We evaluate our technique on six benchmarks to measure the EDC coverage under given performance overhead bounds. Our technique achieves an average EDC coverage of 82%, under performance overheads of 10%, while detecting 10% of the Non-EDC and benign faults.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6575353,no
Mobile app development and usability research to help dementia and Alzheimer patients,"Caregiver anecdotes attest that music and photographs play an important role for family members diagnosed with Alzheimer's disease (AD), even those with severe AD. Tablets and iPads, which are prevalent, can be utilized with dementia patients in portraying favorite music and family photographs via apps developed in close partnership with geriatric facilities. Anecdotal research has shown that non-verbal late-stage dementia patients have become stimulated when iPods played their beloved tunes. There is an unmet need in geriatric facilities for stimulating dementia patients, as well providing hard-core data for proving increased cognitive abilities with technology. Technology can help bridge the gap between patients and staff to improve the quality of life for the cognitively impaired. This study addresses cognitive functioning and quality of life for people diagnosed with dementia via technology. In recent times, the influx of older adults suffering from Alzheimer's or dementia related illness has impacted the U.S. Healthcare system. Cognition significantly declines in older adults with AD or dementia over the course of the disease, causing most to be dependent on caregivers, thus routinely institutionalized. Caregivers are often required to focus their attention on addressing the agitation or discomfort of the AD or dementia patient. Research has shown that technology instruments such as iPods, help stimulate those with dementia. This study focuses on innovative devices such as iPads and tablets, which are mainstream and easy to use, cannot only help determine stage of dementia, but also provide stimulation to improve cognitive functioning. It is hoped that this research will analyze that specially created apps and existing assistive software can be used to decrease the symptoms and improve cognition of older adults suffering from AD or other dementia related diseases. Via service-learning courses, students developed an easy-to-use application for tablets to help- older adults with disabilities more readily use the technology. Student programmers produced apps and performed usability tests with the dementia patients, as well as met with geriatric facility personnel to produce application software that meets the patients, family, and caregiver needs and expectations. For example, a student term project produced an application entitled Candoo that utilizes Google's voice recognition and synthesis engine to navigate the web, provide the weather, and supply pill reminder alerts. Another application example included one that allows families to electronically send photographs, video clips, and favorite music from anywhere to loved ones for enjoyment. Furthermore, older adults were assessed by nursing students for cognitive functioning before, and after the semester's intervention. Such mobile apps could allow dementia persons to become less agitated and stay in their homes longer, while also providing awareness and positive change of attitude by those of another generation towards the elderly. This research will discuss student developed mobile applications in the scope of helping improve the quality of life of patients with AD or dementia.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6578252,no
OCR-independent and segmentation-free word-spotting in handwritten Arabic Archive documents,"In this paper, a word-spotting approach is presented that can help in reading handwritten Arabic Archive Documents. Because of the low quality of these documents, the proposed approach is free segmentation, independent of OCR, using a global transformation of word images. It is a based learning approach which employs Generalized Hough Transform (GHT) technique. It detects words, described by their models, in documents images by finding the model's position in the image. With the GHT, the problem of finding the model's position is transformed to a problem of finding the transformation's parameter that maps the model into the image. Parameters such as Hough threshold and distance between voting points are considered for a better location and recognition of words. We tested our system on registers from the 19th century onwards, held in the National Archives of Tunisia. Our first experiments reach an average of 94% of well-spotted words.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6578363,no
Induction motor mechanical fault identification using Park's vector approach,"In this work we have shown that the extended Park's vector spectrum is rich in harmonics characteristics of mechanical defects (air-gap eccentricity and outer raceway bearing fault). About the use of Park's Lissajou's curves to identify mechanical defects, we have demonstrated that this type of index can only detect the occurrence of a fault, but it cannot identify.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6578381,no
"Current sensors faults detection, isolation and control reconfiguration for PMSM drives","This paper deals with a new method current sensors faults detection isolation (FDI) and reconfiguration of the control loops of a permanent magnet synchronous motor (PMSM) drives. The stator currents are measured as well as observed. During fault free operation, the measured signals are used for the PMSM control. In the case of current sensors faults, the faulty measurements are detected and isolated using the new FDI algorithm. This algorithm uses an augmented PMSM model and a bank of adaptive observers to generate residuals. The resulting residuals are used for sensor fault detection. A logic algorithm is built in such a way to isolate and identify the faulty sensor for a stator phase current fault after detecting the fault occurrence. After sensor fault detection and isolation, the control is reconfigured using the healthy observer's outputs. The validity of the proposed method is verified by simulation tests.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6578414,no
A real-time open phase faults detection for IPMSM drives based on Discrete Fourier Transform phase analysis,"Permanent Magnet Synchronous Motors (PMSM) are many used to high performance applications. Accurate faults detection can significantly improve system availability and reliability. This paper investigates the experimental implementation and detection of open phase faults in interior permanent magnet synchronous motor (IPMSM). The proposed method of open phase fault detection is based only on stator current measurement. The objective of this paper is to develop a new detection method for the open phase fault in IPMSM drives. The main idea consists in minimizing the number of sensors allowing the open stator phase fault of the system to study. This paper proposes the fault diagnosis for open-phase faults of IPMSM drives using a Discrete Fourier Transform phase. The current waveform patterns for various modes of open phase winding are investigated. Discrete Fourier Transform is used for the phases (<sub></sub>, <sub></sub>) calculation. Experimental results show that the method is able to detect the open-phase faults in IPMSM drive. The experimental implementation is carried out on powerful dSpace DS1103 controller board based on the digital signal processor (DSP) TMS320F240. Experimental results obtained confirm the aforementioned study.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6578436,no
Neural SKCS for efficient noise reduction and content preserving,"Images are often corrupted by random variations in intensity, illumination or have poor contrast and can't be used directly. Several studies have expressed the need to reduce noise and to improve the visual quality of the image. For this purpose, several mathematical tools have been developed such as image filtering by a convolution filter, such as the kernel with compact support (KCS) which has been recently proposed by Remaki and Cheriet [1] and it's version separable (SKCS) 10]. The effectiveness of the SKCS filter in the smoothing operation depends on the value of the scale parameter. Moreover, if the scale parameter is increased, the image is blurred and details and borders are removed. This disadvantage is related to the static nature of the KCS kernel. In this paper we propose a dynamic and adaptive SKCS filter based on neural networks. The scale parameters involved in the filtering process are calculated in real time and supervised by the neural network. The filter scale varies continuously in order to detect and clean noisy areas of the image. To assess the developed theory, an application of filtering noisy images is presented, including a qualitative comparison between the result obtained by the static SKCS and the adaptive SKCS kernel proposed.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6578449,no
Detection of brushless exciter rotating diodes failures by spectral analysis of main output voltage,"Rotating rectifier is a basic part of synchronous generators. Inappropriate operation of this component can prove costly for the machine's owner. This paper presents theoretical analysis and experimental validation for detecting failure of brushless exciter rotating diodes that can fail either open circuit or short circuit. Harmonic analysis of the alternator output voltage waveform is performed when machine is unloaded as well as when it runs around its rated load. Apparition of characteristic frequencies can be useful to distinguish the rotating diodes state. By considering the relative amplitudes at specific harmonics, it is possible to discriminate short circuit diode failure case from open circuit diode breakdown.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6578469,no
"Single, multiple and simultaneous current sensors FDI based on an adaptive observer for PMSM drives","This paper deals with a new method single, multiple and simultaneous current sensors faults detection isolation (FDI) and identification for permanent magnet synchronous motor (PMSM) drives. A new state variable is introduced so that an augmented system can be constructed to treat PMSM sensor faults as actuator faults. This method uses the PMSM model and a bank of adaptive observers to generate residuals. The resulting residuals are used for sensor fault detection. A logic algorithm is built in such a way to isolate and identify the faulty sensor for a stator phase current fault after detecting the fault occurrence. The validity of the proposed method is verified by simulation tests.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6578471,no
Assessing the quality of bioforensic signatures,"We present a mathematical framework for assessing the quality of signature systems in terms of fidelity, risk, cost, other attributes, and utility-a method we call Signature Quality Metrics (SQM). We demonstrate the SQM approach by assessing the quality of a signature system designed to predict the culture medium used to grow a microorganism. The system consists of four chemical assays and a Bayesian network that estimates the probabilities the microorganism was grown using one of eleven culture media. We evaluated fifteen combinations of the signature system by removing one or more of the assays from the Bayes net. We show how SQM can be used to compare the various combinations while accounting for the tradeoffs among three attributes of interest: fidelity, cost, and the amount of sample material consumed by the assays.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6578856,no
Applying Scheduling Algorithms with QoS in the Cloud Computing,"Cloud computing is the model to use existing computing resources that are delivered as a form of service over a network. These services can be divided into three parts with software, platform, and infrastructure. It is important to evaluate cloud computing environment to predict valid cost to manage the cloud computing system. SimJava and GridSim is well-known simulation tools but they do not support the virtualization of cloud computing. CloudSim is only tool which can evaluate the performance of this environment and it is based on SimJava and GridSim. It is suitable to simulate the situation with large amount of devices and data in cloud computing. Also, it can simulate the virtualization of computing nodes, network devices, and storage units. Service provider has to guarantee quality of service to provide stable related services. For this, we can use the scheduling algorithms. However, there is no consideration of data priority in CloudSim. It is important to support QoS to keep the service level agreement. Thus, it is needed to research a scheduling algorithm to support QoS. In this paper, we propose the way to support various scheduling algorithms in CloudSim.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6579336,no
Machine Learning-Based Software Quality Prediction Models: State of the Art,"Quantification of parameters affecting the software quality is one of the important aspects of research in the field of software engineering. In this paper, we present a comprehensive literature survey of prominent quality molding studies. The survey addresses two views: (1) quantification of parameters affecting the software quality; and (2) using machine learning techniques in predicting the software quality. The paper concludes that, model transparency is a common shortcoming to all the surveyed studies.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6579473,no
Indentifying Fault-Prone Object in the Web Service,"The faults in web services are very variant. They are occurred by software complexity. This paper focuses on identifying the fault-prone objects in the Web service that are very complex and different. At first define software complexity metrics for the web service. The technique is successful in classifying objects with relatively low error rate. This procedure shows very useful method in the detection of objects, which occur the fault of web services with high potential.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6579478,no
Automatic enhanced CDFG generation based on runtime instrumentation,"Control and Data Flow Graph (CDFG) is a universal description of program behavior, which is widely used in the co-design of software and hardware. The derivation of CDFG has been done mostly by manually or automatically analyzing corresponding source code, which makes this process time-consuming, error-prone and incomplete. In this paper, we proposed an automated design flow based on runtime instrumentation to generate Enhanced CDFG (ECDFG) with additional runtime information. Though the approach of runtime instrumentation is widely used in software debugging to analyze the program with the accurate runtime information, it is rarely used in software and hardware co-design due to the huge trace data and processing overhead. To overcome the bottle neck of the runtime instrumentation approach, Parallel Background Event Logger is proposed to compress and save the huge amount of trace data. Hierarchical loop structures are detected by intersecting reachable set and backward reachable set. Precise data dependency information is deducted by an address based analytical method named Shower Line Algorithm. With these algorithms and techniques, a set of automatic design tools are implemented to collect runtime events, identify nested and implicit loops and deduct data dependance between modules. Exemplar results demonstrated that Enhanced CDFGs for various programs can be generated correctly with acceptable overhead.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6580945,no
Semi-Automatic Generation of Device Drivers for Rapid Embedded Platform Development,"IP core integration into an embedded platform implies the implementation of a customized device driver complying with both the IP communication protocol and the CPU organization (single processor, SMP, AMP). Such a close dependence between driver and platform organization makes reuse of already existing device drivers very hard. Designers are forced to manually customize the driver code to any different organization of the target platform. This results in a very time-consuming and error-prone task. In this paper, we propose a methodology to semi-automatically generate customized device drivers, thus allowing a more rapid embedded platform development. The methodology exploits the testbench provided with the RTL IP module for extracting the formal model of the IP communication protocol. Then, a taxonomy of device drivers based on the CPU organization allows the system to determine the characteristics of the target platform and to obtain a template of the device driver code. This requires some manual support to identify the target architecture and to generate the desired device driver functionality. The template is used then to automatically generate drivers compliant with 1) the CPU organization, 2) the use in a simulated or in a real platform, 3) the interrupt support, 4) the operating system, 5) the I/O architecture, and 6) possible parallel execution. The proposed methodology has been successfully tested on a family of embedded platforms with different CPU organizations.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6582616,no
Assessing QoS trade-offs for real-time video,"Demand for real-time video in law enforcement, emergency and first responder situations has been in rapid growth. Further, different users will have different requirements which, depending on their needs, may change over time. At some times, a user may require high frame rate to detect motion, while at other times the user may be more concerned with resolution for object recognition. In this paper we describe our model for quantifying Quality of Experience (QoE) and managing Quality of Service (QoS) that incorporates end user or mission needs. We describe our distributed utility-based QoS optimization technique, D-Q-RAM and show how it can be used to make QoS trade-offs in response to mission needs to maximize QoE. We experimentally demonstrate QoS optimized trade-offs as user preferences shift between resolution and frame rate in a live 802.11 ad hoc wireless network. The results show the ability to meet all individual user needs, changing or not, while minimizing the impact on other users.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6583504,no
Optimizing agent placement for flow reconstruction of DDoS attacks,"The Internet today continues to be vulnerable to distributed denial of service (DDoS) attacks. We consider the design of a scalable agent-based system for collecting information about the structure and dynamics of DDoS attacks. Our system requires placement of agents on inter-autonomous system (AS) links in the Internet. The agents implement a self-organizing and totally decentralized mechanism capable of reconstructing topological information about the spatial and temporal structure of attacks. The system is effective at recovering DDoS attack structure, even at moderate levels of deployment. In this paper, we demonstrate how careful placement of agents within the system can improve the system's effectiveness and provide better tradeoffs between system parameters and the quality of structural information the system generates. We introduced two agent placement algorithms for our agent-based DDoS system. The first attempts to maximize the percentage of attack flows detected, while the second tries to maximize the extent to which we are able to trace back detected flows to their sources. We show, somewhat surprisingly, these two objectives are concomitant. Placement of agents in a manner which optimizes in the first criterion tends also to optimize with respect to the second criterion, and vice versa. Both placement schemes show a marked improvement over a system in which agents are placed randomly, and thus provide a concrete design process by which to instrument a DDoS flow reconstruction system that is effective at recovering attack structure in large networks at moderate levels of deployment.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6583539,no
New techniques for testing and operational support of AESA radars,"The Active Antennas (AESA) technology has dramatically increased the operational capability of modern radars. Nevertheless, minimize production costs and cost of ownership of these systems is also a major industrial objective. The paper is organized in two parts. The first one deals with improvements achieved so far for reducing the costs and the complexity of industrial testing. In the second part, a data mining method, based on Bayesian Networks, is presented. It aims at processing all data issued from the Built-In-Test (B.I.T.) in order to accurately detect some defects impossible to catch with current methods, such as transient failures or initiation of youth's defects. Originally planned for testing in production, this new performing method could replace, in the future, the current B.I.T. processing, which is used in operational support.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6586042,no
Introducing tool-supported architecture review into software design education,"While modularity is highly regarded as an important quality of software, it poses an educational dilemma: the true value of modularity is realized only as software evolves, but student homework, assignments and labs, once completed, seldom evolve. In addition, students seldom receive feedback regarding the modularity and evolvability of their designs. Prior work has shown that it is extremely easy for students and junior developers to introduce extra dependencies in their programs. In this paper, we report on a first experiment applying a tool-supported architecture review process in a software design class. To scientifically address this education problem, our first objective is to advance our understanding of why students make these modularity mistakes, and how the mistakes can be corrected. We propose tool-guided architecture review so that modularity problems in students' implementation can be revealed and their consequences can be assessed against possible change scenarios. Our pilot study shows that even students who understand the importance of modularity and have excellent programming skills may introduce additional harmful dependencies in their implementations. Furthermore, it is hard for them to detect the existence of these dependencies on their own. Our pilot study also showed that students need more formal training in architectural review to effectively detect and analyze these problems.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6595238,no
An empirical study of the effects of personality on software testing,"The effectiveness of testing is a major determinant of software quality. It is believed that individual testers vary in their effectiveness, but so far the factors contributing to this variation have not been well studied. In this study, we examined whether personality traits, as described by the five-factor model, affect performance on a software testing task. ICT students were given a small software testing task at which their effectiveness was assessed using several different criteria, including bug location rate, weighted fault density, and bug report quality. Their personality was assessed using the NEO PI-3 personality questionnaire. We then compared testing performance according to individual and aggregate measures against different five-factor personality traits. Several weak correlations between two of these personality traits, extraversion and conscientiousness, and testing effectiveness were found.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6595255,no
Foreword,"The purpose of this workshop is to study and advance the effective use of models in the engineering of software systems. In particular, we are interested in the exchange of experiences, challenges and promising technologies related to modeling. The goals of the software modeling community are to improve the productivity of software developers and to improve the quality of the resulting software products. Models are useful in all phases and activities surrounding software development and deployment. Thus, workshop topics range from requirements modeling, to runtime models, to models for assessing software quality, and to the pragmatics of how to manage large collections of models. This year, we received 23 submissions. Of these, the program committee accepted 11 papers for long presentations and 3 papers papers for shorter presentations, for an acceptance rate of 61%. These papers form the basis of workshop sessions, each of which starts with short presentations of 2-3 papers, followed by discussions of issues and research opportunities raised by the papers and by the session topic in general. The program also includes two keynotes, a panel discussion, and a poster/demo session.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6595287,no
Complementing model-driven development for the detection of software architecture erosion,"Detecting software architecture erosion is an important task during the development and maintenance of software systems. Even in model-driven approaches in which consistency between artifacts can partially be established by construction and consistency issues have been intensively investigated, the intended architecture and its realization may diverge with negative effects on software quality. In this article, we describe an approach to flexible architecture erosion detection for model-driven development approaches. Consistency constraints expressed by architectural aspects called architectural rules are specified as formulas on a common ontology, and models are mapped to instances of that ontology. A knowledge representation and reasoning system is then utilized to check whether these architectural rules are satisfied for a given set of models. We describe three case studies in which this approach has been used to detect architecture erosion flexibly and argue that the negative effects of architecture erosion can be minimized effectively.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6595292,no
Do external feedback loops improve the design of self-adaptive systems? A controlled experiment,"Providing high-quality software in the face of uncertainties, such as dealing with new user needs, changing availability of resources, and faults that are difficult to predict, raises fundamental challenges to software engineers. These challenges have motivated the need for self-adaptive systems. One of the primary claimed benefits of self-adaptation is that a design with external feedback loops provide a more effective engineering solution for self-adaptation compared to a design with internal mechanisms. While many efforts indicate the validity of this claim, to the best of our knowledge, no controlled experiments have been performed that provide scientifically founded evidence for it. Such experiments are crucial for researchers and engineers to underpin their claims and improve research. In this paper, we report the results of a controlled experiment performed with 24 final-year students of a Master in Software Engineering program in which designs based on external feedback loops are compared with designs based on internal mechanisms. The results show that applying external feedback loops can reduce control flow complexity and fault density, and improve productivity. We found no evidence for a reduction of activity complexity.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6595487,no
QoS-aware fully decentralized service assembly,"Large distributed software systems are increasingly common in today geographically distributed IT infrastructures. A key challenge for the software engineering community is how to efficiently and effectively manage such complex systems. Extending software services with autonomic capabilities has been suggested as a possible way to address this challenge. Ideally, self-management capabilities should be based on fully distributed, peer-to-peer (P2P) architectures in order to try to overcome the scalability and robustness problems of centralized solutions. Within this context, we propose an approach for the adaptive self-assembly of distributed services, based on a simple epidemic protocol. Our approach is based on the three-layer reference model for adaptive systems, and is centered on the use of a gossip protocol to achieve decentralized information dissemination and decision making. The goal of our system is to build and maintain an assembly of services that, besides functional requirements, is able to fulfill global quality of service (QoS) and structural requirements. A set of simulation experiments is used to assess the effectiveness of our approach in terms of convergence speed towards the optimal solution, and resilience to failures.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6595492,no
An automated tool selection method based on model transformation: OPNET and NS-3 case study,"Errors in telecom service (TS) design may be expensive to correct by telecommunication enterprises, especially if they are discovered late and after the equipments and software are deployed. Verifying complex architectures of TS designs is a daunting task and subject to human errors. Thus, we aim to provide supportive tools that helps during the TS creation activity. Network simulators play an important role in detecting design errors and predicting performance quality violations in the TS domain due to the measurements that they can produce. The metrics associated with performance requirements are numerous, and it is difficult to find a unique tool that can handle the prediction of their values. In this paper, we tackle the tool selection challenge for non domain-expert designers taking into consideration the differences between tools and the large number of metrics that they can measure. Thus, by applying model transformation techniques, we propose a method to select the proper tool(s) to obtain the measurements needed during verification activity. Therefore, we present our contributions on the modeling language level, and the tool selection algorithm with its implementation. Reusability, complexity, and customized measurements are taken into account. We illustrate our approach with a video conference and customized measurement example using OPNET and NS-3 simulators.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6595735,no
Functional SOA testing based on constraints,"In the fierce competition on today's software market, Service-Oriented Architectures (SOAs) are an established design paradigm. Essential concepts like modularization, reuse, and the corresponding IP core business are inherently supported in the development and operation of SOAs that offer flexibility in many aspects and thus optimal conditions also for heterogeneous system developments. The intrinsics of large and complex SOA enterprises, however, require us to adopt and evolve our verification technology, in order to achieve expected software quality levels. In this paper, we contribute to this challenge by proposing a constraint based testing approach for SOAs. In our work, we augment a SOA's BPEL business model with pre- and postcondition contracts defining essential component traits, and derive a suite of feasible test cases to be executed after assessing its quality via corresponding coverage criteria. We illustrate our approach's viability via a running example as well as experimental results, and discuss current and envisioned automation levels in the context of a test and diagnosis workflow.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6595788,no
Did we test our changes? Assessing alignment between tests and development in practice,"Testing and development are increasingly performed by different organizations, often in different countries and time zones. Since their distance complicates communication, close alignment between development and testing becomes increasingly challenging. Unfortunately, poor alignment between the two threatens to decrease test effectiveness or increases costs. In this paper, we propose a conceptually simple approach to assess test alignment by uncovering methods that were changed but never executed during testing. The paper's contribution is a large industrial case study that analyzes development changes, test service activity and field faults of an industrial business information system over 14 months. It demonstrates that the approach is suitable to produce meaningful data and supports test alignment in practice.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6595800,no
Automatic test generation for mutation testing on database applications,"To assure high quality of database applications, testing database applications remains the most popularly used approach. In testing database applications, tests consist of both program inputs and database states. Assessing the adequacy of tests allows targeted generation of new tests for improving their adequacy (e.g., fault-detection capabilities). Comparing to code coverage criteria, mutation testing has been a stronger criterion for assessing the adequacy of tests. Mutation testing would produce a set of mutants (each being the software under test systematically seeded with a small fault) and then measure how high percentage of these mutants are killed (i.e., detected) by the tests under assessment. However, existing test-generation approaches for database applications do not provide sufficient support for killing mutants in database applications (in either program code or its embedded or resulted SQL queries). To address such issues, in this paper, we propose an approach called MutaGen that conducts test generation for mutation testing on database applications. In our approach, we first apply an existing approach that correlates various constraints within a database application through constructing synthesized database interactions and transforming the constraints from SQL queries into normal program code. Based on the transformed code, we generate program-code mutants and SQL-query mutants, and then derive and incorporate query-mutant-killing constraints into the transformed code. Then, we generate tests to satisfy query-mutant-killing constraints. Evaluation results show that MutaGen can effectively kill mutants in database applications, and MutaGen outperforms existing test-generation approaches for database applications in terms of strong mutant killing.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6595801,no
An industry proof-of-concept demonstration of automated combinatorial test,"Studies have found that the largest single cost and schedule component of safety-critical, embedded system development is software rework: locating and fixing software defects found during test. In many such systems these defects are the result of interactions among no more than 6 variables, suggesting that 6-way combinatorial testing would be sufficient to trigger and detect them. The National Institute of Standards and Technology developed an approach to automatically generating, executing, and analyzing such tests. This paper describes an industry proof-of-concept demonstration of automated unit and integration testing using this approach. The goal was to see if it might cost-effectively reduce rework by reducing the number of software defects escaping into system test - if it was adequately accurate, scalable, mature, easy to learn, and easy to use and still was able to achieve the required level of structural coverage. Results were positive - e.g., 2775 test input vectors were generated in 6 seconds, expected outputs were generated in 60 minutes, and executing and analyzing them took 8 minutes. Tests detected all seeded defects and in the proof-of-concept demonstration achieved nearly 100% structural coverage.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6595802,no
ReFit: A Fit test maintenance plug-in for the Eclipse refactoring plug-in,"The Fit framework is a widely established tool for automated acceptance test-driven development (ATDD). Fit stores the test specification separate from the test fixture code in an easily human readable and editable tabular form in HTML format. Additional tools like the FitPro plugin or FitN esse support the writing of test specifications and test fixtures from within the Eclipse IDE or the Web. With the increasing popularity of agile test-driven software development, maintenance of the evolving and growing test base has become an important issue. However, there has been no support yet for automated refactoring of Fit test cases. In a recent research project, we developed the Eclipse plugin ReFit for automated refactoring of Fit test cases. Fit test refactoring can occur due to changing requirements or changing Java code, which in either case means a cross-language refactoring to keep test specification and test fixture in sync. In this paper the concept for the development of the ReFit Eclipse Plugin is described, which significantly reduces the effort for Fit test maintenance and makes refactoring less error prone. Besides a tight integration into the existing Eclipse refactoring plugin, major goals of the plugin were to make it easy extensible for additional refactorings, new fixture types and further test specification file formats. Challenges faced when adding new and modifying existing Eclipse refactoring behavior are described and are due to the strong dependency on the Eclipse JDK and LTK features, and the solutions developed are presented.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6597187,no
A novel fuzzy classification to enhance software regression testing,"An effective system regression testing for consecutive releases of very large software systems, such as modern telecommunications systems, depends considerably on the selection of test cases for execution. Classification models can classify, early in the test planning phase, those test cases that are likely to detect faults in the upcoming regression test. Due to the high uncertainties in regression test, classification models based on fuzzy logic are very useful. Recently, methods have been proposed for automatically generating fuzzy if-then rules by applying complicated rule generation procedures to numerical data. In this research, we introduce and demonstrate a new rule-based fuzzy classification (RBFC) modeling approach as a method for identifying high effective test cases. The modeling approach, based on test case metrics and the proposed rule generation technique, is applied to extracting fuzzy rules from numerical data. In addition, it also provides a convenient way to modify rules according to the costs of different misclassification errors. We illustrate our modeling technique with a case study of large-scale industrial software systems and the results showed that test effectiveness and efficiency was significantly improved.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6597217,no
Discovering signature patterns from event logs,"More and more information about processes is recorded in the form of so-called event logs. High-tech systems such as X-ray machines and high-end copiers provide their manufacturers and services organizations with detailed event data. Larger organizations record relevant business events for process improvement, auditing, and fraud detection. Traces in such event logs can be classified as desirable or undesirable (e.g., faulty or fraudulent behavior). In this paper, we present a comprehensive framework for discovering signatures that can be used to explain or predict the class of seen or unseen traces. These signatures are characteristic patterns that can be used to discriminate between desirable and undesirable behavior. As shown, these patterns can, for example, be used to predict remotely whether a particular component in an X-ray machine is broken or not. Moreover, the signatures also help to improve systems and organizational processes. Our framework for signature discovery is fully implemented in ProM and supports class labeling, feature extraction and selection, pattern discovery, pattern evaluation and cross-validation, reporting, and visualization. A real-life case study is used to demonstrate the applicability and scalability of the approach.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6597225,no
A Programming Language Approach to Fault Tolerance for Fork-Join Parallelism,"When running big parallel computations on thousands of processors, the probability that an individual processor will fail during the execution cannot be ignored. Computations should be replicated, or else failures should be detected at runtime and failed subcomputations reexecuted. We follow the latter approach and propose a high-level operational semantics that detects computation failures, and allows failed computations to be restarted from the point of failure. We implement this high-level semantics with a lower-level operational semantics that provides a more accurate account of processor failures, and prove in Coq the correspondence between the high- and low-level semantics.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6597884,no
Requirements-Driven Self-Repairing against Environmental Failures,"Self-repairing approaches have been proposed to alleviate the runtime requirements satisfaction problem by switching to appropriate alternative solutions according to the feedback monitored. However, little has been done formally on analyzing the relations between specific environmental failures and corresponding repairing decisions, making it a challenge to derive a set of alternative solutions to withstand possible environmental failures at runtime. To address these challenges, we propose a requirements-driven self-repairing approach against environmental failures, which combines both development-time and runtime techniques. At the development phase, in a stepwise manner, we formally analyze the issue of self-repairing against environmental failures with the support of the model checking technique, and then design a sufficient and necessary set of alternative solutions to withstand possible environmental failures. The runtime part is a runtime self-repairing mechanism that monitors the operating environment for unsatisfiable situations, and makes self-repairing decisions among alternative solutions in response to the detected environmental failures.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6597904,no
EnHTM: Exploiting Hardware Transaction Memory for Achieving Low-Cost Fault Tolerance,"Fault-tolerance has become an essential concern for processor designers due to increasing transient fault rates, even for the processors used in the mainstream computing. As the mainstream commodity market accepts only low-cost fault tolerance solutions, traditional high-end solutions are unacceptable due to their expensive overheads. This paper presents EnHTM, a hybrid software/hardware implemented low-cost fault tolerance solution for the serial programs running on commodity systems. EnHTM employs light-weight symptom-based mechanism to detect faults and recovers from faults using a minimally-modified Hardware Transactional Memory (HTM) which features lazy conflict detection, lazy data versioning. Compile-time analysis approach is also exploited to support larger transaction size, so that transient faults detected within long latency can be recovered. The evaluation experiment result shows that EnHTM can recover from 89.4%of catastrophic failures caused by transient faults, with a performance overhead of 2.6% in error-free executions on average.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6598051,no
Regression Testing Prioritization Based on Model Checking for Safety-Crucial Embedded Systems,"The order in which test-cases are executed has an influence on the rate at which faults can be detected. In this paper we demonstrate how test-case prioritization can be performed with the use of model-checkers. For this, different well known prioritization techniques are adapted for model-based use. New property based prioritization techniques are introduced. In addition it is shown that prioritization can be done at test-case generation time, thus removing the need for test-suite post-processing. Several experiments for safety-crucial embedded systems are used to show the validity of these ideas.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6598153,no
A New Multi-threaded Code Synthesis Methodology and Tool for Correct-by-Construction Synthesis from Polychronous Specifications,"Embedded software systems respond to multiple events coming from various sources - some temporally regular (ex: periodic sampling of continuous time signals) and some intermittent (ex: interrupts, exception events etc.). Timely response to such events while executing complex computation, might require multi-threaded implementation. For example, overlapping I/O of various types of events, and computation on such events may be delegated to different threads. However, manual programming of multi-threaded programs is error-prone, and proving correctness is computationally expensive. In order to guarantee safety of such implementations, we believe that a correct-by-construction synthesis of multi-threaded software from formal specification is required. It is also imperative that the multiple threads are capable of making progress asynchronous to each other, only synchronizing when shared data is involved or information requires to be passed from one thread to other. Especially on a multi-core platform, lesser the synchronization between threads, better will be the performance. Also, the ability of the threads to make asynchronous progress, rather than barrier synchronize too often, would allow better real-time schedulability. In this work, we describe our technique for multi-threaded code synthesis from a variant of the polychronous programming language SIGNAL, namely MRICDF. Through a series of experimental benchmarks we show the efficacy of our synthesis technique. Our tool EmCodeSyn which was built originally for sequential code synthesis from MRICDF models has been now extended with multi-threaded code synthesis capability. Our technique first checks the concurrent implementability of the given MRICDF model. For implementable models, we further compute the execution schedule and generate multi-threaded code with appropriate synchronization constructs so that the behavior of the implementation is latency equivalent to that of the original MRICDF model.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6598337,no
Modeling Stock Analysts' Decision Making: An Intelligent Decision Support System,"It is well known that security analysis is a time-consuming and error-prone process. However, it can be improved or enhanced considerably by automated reasoning. Efforts to reduce the inaccuracy and incorrectness of analyses and to enhance the confidence levels of stock selection have led to the development of an intelligent decision support system called Trade Expert, which assists, not replaces, portfolio managers. Trade Expert assumes the role of a hypothetical securities analyst capable of analyzing stocks, calling market turns, and making recommendations. It has a knowledge base of stock trading expertise, and a case base of past episodes and consequences of decisions. By combining knowledge-based problem solving with case-based reasoning and fuzzy inference, Trade Expert demonstrates forms of intelligent behavior not yet observed in traditional decision support systems and expert systems. The novelty of this research lies in its application to analogical reasoning, fuzzy reasoning, and knowledge-based decision making.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6598441,no
A Comparison of Some Predictive Models for Modeling Abortion Rate in Russia,"Predictive modeling techniques are popular methods for building models to predict a target of interest. In many modeling problems, however, the focus is to identify possible factors that have significant association with the target. For this type of problem, it is very easy to stretch the interpretation of an association relationship to a causation relationship. Practitioners must pay special attention to such a misinterpretation when data are observational data. In addition, the process of data collection and cleansing are critical in order to produce quality data for modeling. In this article, an observational study is conducted to illustrate the issues about data quality and model building to identify potential important factors associated with abortion rate using data collected in Russia from year 2000 to 2009. Some pitfalls and cautions of applying predictive modeling techniques are discussed.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6598454,no
Water Distribution System Monitoring and Decision Support Using a Wireless Sensor Network,"Water distribution systems comprise labyrinthine networks of pipes, often in poor states of repair, that are buried beneath our city streets and relatively inaccessible. Engineers who manage these systems need reliable data to understand and detect water losses due to leaks or burst events, anomalies in the control of water quality and the impacts of operational activities (such as pipe isolation, maintenance or repair) on water supply to customers. Water Wise is a platform that manages and analyses data from a network of wireless sensor nodes, continuously monitoring hydraulic, acoustic and water quality parameters. Water Wise supports many applications including rolling predictions of water demand and hydraulic state, online detection of events such as pipe bursts, and data mining for identification of longer-term trends. This paper illustrates the advantage of the Water Wise platform in resolving operational decisions.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6598533,no
A new failure analysis approach to predict and localize defects and weakness areas in trough-glass-vias for a multifunctional package level camera,"In this paper, we provide a novel approach to identify failures and defects that occur in the glass interposer of a system-on-package technology-based miniaturized multifunctional camera. First, we use simulations to validate the proposed defect prediction and/or weakness identification techniques. Then, we confirm the predictions using non-destructive failure analysis techniques. Finally, we use the physical analysis techniques to confirm the software failure mode assumptions.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6599196,no
The Web Services Composition Testing Based on Extended Finite State Machine and UML Model,"Web services are designed as software building blocks for Service Oriented Architecture (SOA). It provides an approach to software development that system and application can be constructed by assembling reusable software building blocks, called services. The industries have adopted web services composition to generate new business applications or mission critical services. One of the most popular integration languages for web services composition is Web Services Business Process Execution Language (WS-BPEL). Although the individual service is usually functional correctly, however, several unexpected faults may occur during execution of composite web service. It is difficult to detect the original failure service because the faults may propagate, accumulate and spread. In this paper, we present a technique of Model-Based Testing (MBT) to enhance testing of interactions among the web services. The technique combines Extended Finite State Machine (EFSM) and UML sequence diagram to generate a test model, called EFSM-SeTM. We also defined various coverage criteria to generate valid test paths from EFSM-SeTM model for a better test coverage of all possible scenarios.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6599388,no
Exact determination of a winding disk radial deformation location considering tank effect using an analytical method,"Power transformers are one of the most expensive components of the power system. Timely detection of fault arose in the transformer can be used to prevent unwanted outage of transformer and repair costs. Using electromagnetic waves has recently been proposed for on-line monitoring of the transformer. The presence of the tank in the transformer structure causes problem for analyzing the electromagnetic waves. In this paper, a new analytical method based on locus of the objects in the space is proposed to detect the radial deformation location of a disk winding considering tank effect. The proposed experimental setup for this method has been modeled using CST (Computer Simulation Technology) software. In this paper, Vivaldi antennas suitable for measurements in environments with multi-path routing are used and the analysis is performed in the time domain. The simulation results show that exact determination of radial deformation location can be detected with good accuracy using this method.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6599629,no
A new traveling wave fault location algorithm in series compensated transmission line,"Series capacitors (SCs) are installed on long transmission lines to reduce the inductive reactance of lines. This makes it appear electrically shorter and increases the power transfer capability. Series capacitors and their associated over-voltage protection devices (typically Metal Oxide Varistors (MOVs), and/or air gaps) create several problems for protection relays and fault locators including voltage and/or current inversion, sub-harmonic oscillations, transients caused by the air-gap flashover and sudden changes in the operating reach. In this paper, an accurate fault location algorithm for series compensated power transmission lines is presented. With using voltage and current traveling waves and placement of a fault locator in the middle of transmission line near the SCs, location of faults is calculated with high accuracy also proposed algorithm needs no communication link and uses only local signals and because of using of traveling wave polarity have no problem for detecting of reflected waves and therefore it solves problems caused by one end traveling wave based fault location methods. A simple power system containing a compensated transmission line is simulated on PSCAD/EMTDC software and fault location algorithm is implemented on MATLAB environment using wavelet transformer.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6599888,no
Productive Development of Dynamic Program Analysis Tools with DiSL,"Dynamic program analysis tools serve many important software engineering tasks such as profiling, debugging, testing, program comprehension, and reverse engineering. Many dynamic analysis tools rely on program instrumentation and are implemented using low-level instrumentation libraries, resulting in tedious and error-prone tool development. The recently released Domain-Specific Language for Instrumentation (DiSL) was designed to boost the productivity of tool developers targeting the Java Virtual Machine, without impairing the performance of the resulting tools. DiSL offers high-level programming abstractions especially designed for development of instrumentation-based dynamic analysis tools. In this paper, we present a controlled experiment aimed at quantifying the impact of the DiSL programming model and high-level abstractions on the development of dynamic program analysis instrumentations. The experiment results show that compared with a prevailing, state-of-the-art instrumentation library, the DiSL users were able to complete instrumentation development tasks faster, and with more correct results.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6601288,no
"Rule-Based Behaviour Engineering: Integrated, Intuitive Formal Rule Modelling","Requirement engineering is a difficult task which has a critical impact on software quality. Errors related to requirements are considered the most expensive types of software errors. They are the major cause of project delays and cost overruns. Software developers need to cooperate with multiple stakeholders with different backgrounds and concerns. The developers need to investigate an unfamiliar problem space and make the transition from the informal problem space to the formal solution space. The requirement engineering process should use systematic methods which are constructive, incremental, and rigorous. The methods also need to be easy to use and understand so that they can be used for communication among different stakeholders. Is it possible to invent a human intuitive modelling methodology which systematically translates the informal requirements into a formally defined model? Behaviour Engineering has arguably solved many problems. However, the size and low level of the final Behavior Tree makes it hard to match with the original requirements. Here, we propose a new requirement modelling approach called Rule-Based Behaviour Engineering. We separate two concerns, rules and procedural behaviours, right at the beginning of the requirement modelling process. We combine the Behavior Tree notation for procedural behaviour modelling with a non-monotonic logic called Clausal Defeasible Logic for rule modelling. In a systematic way, the target model is constructed incrementally in four well-defined steps. Both the representations of rules and procedural flows are humanly readable and intuitive. The result is an effective mechanism for formally modelling requirements, detecting requirement defects, and providing a set of tools for communication among stakeholders.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6601289,no
Development of Robust Traceability Benchmarks,"Traceability benchmarks are essential for the evaluation of traceability recovery techniques. This includes the validation of an individual trace ability technique itself and the objective comparison of the technique with other traceability techniques. However, it is generally acknowledged that it is a real challenge for researchers to obtain or build meaningful and robust benchmarks. This is because of the difficulty of obtaining or creating suitable benchmarks. In this paper, we describe an approach to enable researchers to establish affordable and robust benchmarks. We have designed rigorous manual identification and verification strategies to determine whether or not a link is correct. We have developed a formula to calculate the probability of errors in benchmarks. Analysis of error probability results shows that our approach can produce high quality benchmarks, and our strategies significantly reduce error probability in them.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6601302,no
Predicting Fault-Prone Software Modules with Rank Sum Classification,"The detection and correction of defects remains among the most time consuming and expensive aspects of software development. Extensive automated testing and code inspections may mitigate their effect, but some code fragments are necessarily more likely to be faulty than others, and automated identification of fault prone modules helps to focus testing and inspections, thus limiting wasted effort and potentially improving detection rates. However, software metrics data is often extremely noisy, with enormous imbalances in the size of the positive and negative classes. In this work, we present a new approach to predictive modelling of fault proneness in software modules, introducing a new feature representation to overcome some of these issues. This rank sum representation offers improved or at worst comparable performance to earlier approaches for standard data sets, and readily allows the user to choose an appropriate trade-off between precision and recall to optimise inspection effort to suit different testing environments. The method is evaluated using the NASA Metrics Data Program (MDP) data sets, and performance is compared with existing studies based on the Support Vector Machine (SVM) and Naive Bayes (NB) Classifiers, and with our own comprehensive evaluation of these methods.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6601309,no
Fast-Tracking GENI Experiments Using HyperNets,"Although the underlying network resources needed to support virtualized networks are rapidly becoming available, the tools and abstractions needed to effectively make use of these virtual networks is severely lacking. Although networks like GENI are now available to experimenters, creating an experimental network can still be a daunting and error-prone task. While virtual networks enable experimenters to build tailored networks from the """"ground up"""", starting from scratch is rarely what an experimenter wants to do. Moreover, the challenges of incorporating real-world users into GENI experiments make it difficult to benefit real users or obtain realistic traffic. In this paper we describe a new service designed to simplify the process of setting up and running GENI experiments while at the same time adding support for real-world users to join GENI experiments. Our approach is based on a network hypervisor service used to deploy """"HyperNets"""": pre-defined experimental environments that can be quickly and easily created by experimenters. To illustrate the utility and simplicity of our approach, we describe two example HyperNets, and show how our network hypervisor service is able to automatically deploy them on GENI. We then present some initial performance results from our implentation on GENI. Because our network hypervisor is itself a client of GENI (i.e., it calls the GENI AM APIs to create HyperNets), we briefly discuss our experience using GENI and the challenges we encountered mapping HyperNets onto the GENI framework.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6601407,no
Automated Analysis of Reliability Architectures,"The development of complex and critical systems calls for a rigorous and thorough evaluation of reliability aspects. Over the years, several methodologies have been introduced in order to aid the verification and analysis of such systems. Despite this fact, current technologies are still limited to specific architectures, without providing a generic evaluation of redundant system definitions. In this paper we present a novel approach able to assess the reliability of an arbitrary combinatorial redundant system. We rely on an expressive modeling language to represent a wide class of architectural solutions to be assessed. On such models, we provide a portfolio of automatic analysis techniques: we can produce a fault tree, that represents the conditions under which the system fails to produce a correct output, based on it, we can provide a function over the components reliability, which represents the failure probability of the system. At its core, the approach relies on the logical formalism of equality and uninterpreted functions, it relies on automated reasoning techniques, in particular Satisfiability Modulo Theories decision procedures, to achieve efficiency. We carried out an extensive experimental evaluation of the proposed approach on a wide class of multi-stage redundant systems. On the one hand, we are able to automatically obtain all the results that are manually obtained in [1], on the other, we provide results for a much wider class of architectures, including the cases of non-uniform probabilities and of two voters per stage.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6601824,no
Game-Based Monitors for Scenario-Based Specification,"Run-time verification techniques based on monitors have become the basic means of detecting software failures in dynamic and open environments. One challenging problem is how the monitor can provide sufficient indications before the real failures, so that the system has enough time to act before the failures cause serious harm. To this end, this paper proposes the main idea on how to generate monitors from a scenario-based specification called property sequence chart based on game theory. The monitors are interpreted in multivalued semantics: satisfied, infinitely controllable, system finitely controllable, system urgently controllable, environment finitely controllable, environment urgently controllable, violated. Through the multi-valued semantics definition, the monitors can provide enough information to help the system to take measures for failure prevention or recovery.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6601834,no
Educational Collaborative Virtual Environments: Evaluation Model,In this paper we propose a model for assessing the quality of educational collaborative virtual environments. The objective is to establish a theoretical model that highlights a number of relevant sets of requirements in relation to the quality in educational collaborative virtual environments. It is intended to apply the model during the lifecycle of product development and in the selection of the environment in order to support the learning/teaching process.,2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6601909,no
Towards Efficient Probabilistic Scheduling Guarantees for Real-Time Systems Subject to Random Errors and Random Bursts of Errors,"Real-time computing and communication systems are often required to operate with prespecified levels of reliability in harsh environments, which may lead to the exposure of the system to random errors and random bursts of errors. The classical fault-tolerant schedulability analysis in such cases assumes a pseudo-periodic arrival of errors, and does not effectively capture any underlying randomness or burst characteristics. More modern approaches employ much richer stochastic error models to capture these behaviors, but this is at the expense of greatly increased complexity. In this paper, we develop a quantile-based approach to probabilistic schedulability analysis in a bid to improve efficiency whilst still retaining a rich stochastic error model capturing random errors and random bursts of errors. Our principal contribution is the derivation of a simple closed-form expression that tightly bounds the number of errors that a system must be able to tolerate at any time subsequent to its critical instant in order to achieve a specified level of reliability. We apply this technique to develop an efficient 'one-shot' schedulability analysis for a simple fault-tolerant EDF scheduler. The paper concludes that the proposed method is capable of giving efficient probabilistic scheduling guarantees, and may easily be coupled with more representative higher-level job failure models, giving rise to efficient analysis procedures for safety-critical fault-tolerant real-time systems.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6602106,no
PIE: A lightweight control scheme to address the bufferbloat problem,"Bufferbloat is a phenomenon where excess buffers in the network cause high latency and jitter. As more and more interactive applications (e.g. voice over IP, real time video conferencing and financial transactions) run in the Internet, high latency and jitter degrade application performance. There is a pressing need to design intelligent queue management schemes that can control latency and jitter; and hence provide desirable quality of service to users. We present here a lightweight design, PIE (Proportional Integral controller Enhanced), that can effectively control the average queueing latency to a reference value. The design does not require per-packet extra processing, so it incurs very small overhead and is simple to implement in both hardware and software. In addition, the design parameters are self-tuning, and hence PIE is robust and optimized for various network scenarios. Simulation results, theoretical analysis and Linux testbed results show that PIE can ensure low latency and achieve high link utilization under various congestion situations.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6602305,no
Transformation operators for easier engineering of medical process models,"The need for high-quality models is increasingly recognized for driving and documenting complex medical processes such as cancer therapies. A medical environment for such processes has to deal with a great multiplicity of dimensions such as different pathologies, different hospital departments, different agents with different concerns and expertise, different resources with a wide spectrum of capabilities, and so forth. The variety of needs along those multiple dimensions calls for multiple, complementary and consistent facets of the composite process model, each addressing a specific dimension. Building multi-dimensional process models is in our experience hard and error-prone. The paper describes various operators for composing process model facets in a coherent way or, conversely, for decomposing process models into specific facets that abstract from details irrelevant to a specific dimension. These operators are grounded on the formal trace semantics provided by our process language and its supporting analysis toolset. The paper shows how these operators may help modeling, analyzing, documenting and enacting complex processes. Their use is illustrated on simplified examples taken from real cancer therapies.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6602476,no
RSL-PL: A linguistic pattern language for documenting software requirements,"Software requirements are traditionally documented in natural language (NL). However, despite being easy to understand and having high expressivity, this approach often leads to well-known requirements quality problems. In turn, dealing with these problems warrants a significant amount of human effort, causing requirements development activities to be error-prone and time-consuming. This paper introduces RSL-PL, a language that enables the definition of linguistic patterns typically found in well-formed individual NL requirements, according to the field's best practices. The linguistic features encoded within RSL-PL patterns enable the usage of information extraction techniques to automatically perform the linguistic analysis of NL requirements. Thus, in this paper we argue that RSL-PL can improve the quality of requirements specifications, as well as the productivity of requirements engineers, by mitigating the continuous effort that is often required to ensure requirements quality criteria, such as clearness, consistency, and completeness.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6602667,no
Uncovering product line variability from early requirement documents,"Mass production of customer-specific software application through software product lines has been gaining great attention in the past years. A software product line supports fast production of customized software applications by the composition of variable requirements, namely variability. Practitioners and researchers suggest that the efficient construction of software product lines depends on the ability of domain engineers to early identify potential variability. Controversially, uncovering product line variability from elicited requirements remains one of the main challenges in domain engineering. The current practice is an ad-hoc, tacit and consequently error-prone identification of variable requirements by domain experts while reviewing different versions of specification documents for similar products. Therefore, variability uncovering could represent an adoption barrier for many companies that should otherwise benefit. To cope with this challenge on product line requirement engineering, we propose in this paper a novel technique for uncovering variability from early requirement documents, specially, from existing Language Extended Lexicons (LEL). The technique suggests the analysis of LEL following a set of heuristics, which therefore, supports the precise grouping, identification and relation of potential variable requirements. In this paper we also illustrate the proposed technique through examples for the meeting scheduler domain.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6602670,no
Development of a binocular eye tracking system for quality assessment of S3D representations,"It is intended to develop a high-precision binocular eye tracking system to assess the eye behavior whilst watching stereo 3D content. Up to now, available eye tracking systems are not providing necessary high precision for issues concerning stereo 3D perception. Hence, an up to now not available detailed specification of the recordings of the eye movements as well as the interpretation of binocular data is investigated. Therefore, a binocular prototype is developed as well as specification software leading to optimized data interpretation by software modules computing against uncertainties and physical thresholds. This intention including planned software modules as well as the therefore used basic eye tracking system are presented within the QUALINET industry forum.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6603222,no
Parametric classification over multiple samples,"This pattern was originally designed to classify sequences of events in log files by error-proneness. Sequences of events trace application use in real contexts. As such, identifying error-prone sequences helps understand and predict application use. The classification problem we describe is typical in supervised machine learning, but the composite pattern we propose investigates it with several techniques to control for data brittleness. Data pre-processing, feature selection, parametric classification, and cross-validation are the major instruments that enable a good degree of control over this classification problem. In particular, the pattern includes a solution for typical problems that occurs when data comes from several samples of different populations and with different degree of sparcity.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6603805,no
Personalising Multi Video Streams and Camera Views for Live Events,"An innovative Internet streaming video player, we call ePlayer, that supports personalised camera switching for live events has been researched, developed and evaluated. The main novelty of the system is that in dispensing the available bandwidth amongst multiple input video streams it can automatically coordinate and preserve the video quality across multiple live video streams. An additional novelty of the system is that it supports automatic camera switching based upon an individual user's preferences. The experimental results indicate that the system is able to effectively allocate a contested bandwidth resource amongst multiple streams and is able to infer a user's camera switching preferences via dynamically predicting a user's switching intervals amongst multiple cameras.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6603877,no
A Hybrid Method of User Privacy Protection for Location Based Services,"Recently, highly accurate positioning devices enables us to provide various types of location based services (LBS). On the other hand, because such positioning data include deeply personal information, the protection of location privacy is one of the most significant problems in LBS. Lots of different techniques for securing the location privacy have been proposed, for instance the concept of Silent period, the concept of Dummy node, and the concept of Cloaking-region. However, many of these researches have a problem that quality of the LBS (QoS) decreased when anonymity is improved, and anonymity falls down when QoS is improved. In this paper, we present a node density-based location privacy scheme which can provide location privacy by utilizing hybrid concept of Dummy node and Cloaking-region. Simulation results show that the probability of tracking of a target node by an adversary is reduced and the QoS of LBS is also improved.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6603928,no
Poster abstract: Formal analysis of fresenius infusion pump (FIP),"Summary form only given. Today's medical devices are based on embedded architecture, with software used to control the underlying hardware. They are highly critical since errors in the software can endanger end users such as patients and medics. Medical devices should be designed and manufactured in such a way that when used, they perform as intended and they ensure a high level of safety. Current industrial practices are based on testing processes to check if the software meets the specifications and if it fulfills its purpose. However, testing does have several disadvantages that limit the reliability of this verification and validation process. Testing cannot guarantee that a device will function properly under all conditions and bugs can never be completely identified withing a program. Several attempts have already been made to provide standards for the formal verification of safety properties of medical devices, initiated by the Generic Infusion Pump project [2]. Our work is a collaboration between Objet Direct R&D and Fresenius [1]. Fresenius is a leading international health care group which produces and markets pharmaceuticals and medical devices. We aim to investigate innovative methods for software development, validation and verification. We study existing results provided amongst others by [3, 4] which we intend to extend by analyzing the Fresenius Infusion Pump (FIP) software. FIP automatizes the delivery process of fluid medical solution into patient's body. Its design is based on three layers. The highest level is the user interface and consists of three components, the administration protocol, the application system and the power management. The middle level consists of the pumping control components and the lowest level contains driver components such as Door, Watchdog, Optical Disk, Motor. FIP is modeled in UML (a total of 100 state machines) and the requirements are written in natural language. The implementation of the model is done in C- + with automatic code generation. For the V&V process, software testing checks if the implementation meets the requirements using fault scenarios written in UML. The main objective of this project is to use model-based design for migrating from software testing to formal based solution for verifying the Fresenius Infusion Pump. The goal is to use model checking technologies in order to verify requirements and eliminate bugs during the design process. Several faulty design patterns have already been identified to be caused by deadlocks, lost signal events, stack overflow, violation of real-time properties, incoherent behavior of UML state machines. We present and analyze the case study of the FIP's Motor component, a driver component of the lowest level. Its interest lies on the fact that while the Motor Control is stopped, the Motor Driver is still running. This faulty behavior was detected during the test checks and bug was partially corrected in code review.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6604031,no
Highly-reliable integer matrix multiplication via numerical packing,"The generic matrix multiply (GEMM) routine comprises the compute and memory-intensive part of many information retrieval, relevance ranking and object recognition systems. Because of the prevalence of GEMM in these applications, ensuring its robustness to transient hardware faults is of paramount importance for highly-efficientlhighly-reliable systems. This is currently accomplished via error control coding (ECC) or via dual modular redundancy (DMR) approaches that produce a separate set of parity results to allow for fault detection in GEMM. We introduce a third family of methods for fault detection in integer matrix products based on the concept of numerical packing. The key difference of the new approach against ECC and DMR approaches is the production of redundant results within the numerical representation of the inputs rather than as a separate set of parity results. In this way, high reliability is ensured within integer matrix products while allowing for: (i) in-place storage; (ii) usage of any off-the-shelf 64-bit floating-point GEMM routine; (iii) computational overhead that is independent of the GEMM inner dimension. The only detriment against a conventional (i.e. fault-intolerant) integer matrix multiplication based on 32-bit floating-point GEMM is the sacrifice of approximately 30.6% of the bitwidth of the numerical representation. However, unlike ECC methods that can reliably detect only up to a few faults per GEMM computation (typically two), the proposed method attains more than 12 nines reliability, i.e. it will only fail to detect 1 fault out of more than 1 trillion arbitrary faults in the GEMM operations. As such, it achieves reliability that approaches that of DMR, at a very small fraction of its cost. Specifically, a single-threaded software realization of our proposal on an Intel i7-3632QM 2.2GHz processor (Ivy Bridge architecture with AVX support) incurs, on average, only 19% increase of execution time agai- st an optimized, fault-intolerant, 32-bit GEMM routine over a range of matrix sizes and it remains more than 80% more efficient than a DMR-based GEMM.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6604045,no
Exploiting the debug interface to support on-line test of control flow errors,"Detecting the effects of transient faults is a key point in many safety-critical applications. This paper explores the possibility of using for this purpose the debug interface existing today in several processors/controllers on the market. In this way one can achieve a good detection capability with respect to control flow errors with very small latency, while the cost for adopting the proposed technique is rather limited and does not involve any change either in the processor hardware or in the application software. The method works even if the processor uses caches. Experimental results are reported, showing both the advantages and the costs of the method.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6604058,no
Experimental evaluation of GPUs radiation sensitivity and algorithm-based fault tolerance efficiency,"Experimental results demonstrate that Graphic Processing Units are very prone to be corrupted by neutrons. We have performed several experimental campaigns at ISIS, UK and at LANSCE, Los Alamos, NM, USA accessing the sensitivity of the GPU internal resources as well as the error rate of common parallel algorithms. Experiments highlight output error patterns and radiation responses that can be fruitfully used to design optimized Algorithm-Based Fault Tolerance strategies and provide pragmatic programming guidelines to increase the code reliability with low computational overhead.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6604091,no
A Feedback-Based Approach to Validate SWRL Rules for Developing Situation-Aware Software,"Recently, the Web Ontology Language (OWL) and Semantic Web Rule Language (SWRL) have been widely used to construct situation-aware environments. However, incorrect situations can be inferred, and these decrease the quality of situation-aware services. SWRL rules are one of the main causes of incorrectly inferred situations. Therefore, in this paper, we propose an approach to validate SWRL rules by applying feedback, a key concept used in software cybernetics research. We propose a feedback-based approach that consists of preparation, structural analysis, contextual analysis, and SWRL rule adaptation. Using the proposed approach, we can systematically detect errors and adapt the SWRL rules accordingly. Furthermore, our method can be used as a base model to validate SWRL rules for situation-aware software.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6605761,no
A Case Study of Adaptive Combinatorial Testing,"The ability of Combinatorial Testing (CT) to detect and locate the interaction triggered failure has been well studied. But CT still suffers from many challenges, such as modeling for CT, sampling mechanisms for test generation, applicability and effectiveness. To overcome these issues of CT, adaptive combinatorial testing (ACT) is proposed in this paper, which improves the traditional CT with a well established adaptive testing method as the counter part of adaptive control and aims to make CT more flexible and practical. ACT can significantly enhance testing quality, software reliability and support testing strategy adjustment dynamically. To support further investigation, a preliminary form of concrete strategy for ACT is given as a heuristic guideline, and a case study is presented to illustrate its operations.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6605762,no
Privacy-Aware Community Sensing Using Randomized Response,"Community sensing is an emerging system which allows the increasing number of mobile phone users to share effectively minute statistical information collected by themselves. This system relies on participants' active contribution including intentional input data through mobile phone's applications, e.g. Facebook, Twitter and Linkdin. However, a number of privacy concerns will hinder the spread of community sensing applications. It is difficult for resource-constrained mobile phones to rely on complicated encryption scheme. We should prepare a privacy-preserving community sensing scheme with less computational-complexity. Moreover, an environment that is reassuring for participants to conduct community sensing is strongly required because the quality of the statistical data is depending on general users' active contribution. In this article, we suggest a privacy-preserving community sensing scheme for human-centric data such as profile information by using the combination of negative surveys and randomized response techniques. By using our method described in this paper, the server can reconstruct the probability distributions of the original distributions of sensed values without violating the privacy of users. Especially, we can protect sensitive information from malicious tracking attacks. We evaluated how this scheme can preserve the privacy while keeping the integrity of aggregated information.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6605777,no
Using a Trust Model to Reduce False Positives of SIP Flooding Attack Detection in IMS,"The IP Multimedia Subsystem (IMS) is constantly evolving to meet the growth of mobile services and Internet applications. One major security problem of the IMS is flooding attacks. There are many works that have been proposed to detect such attacks. However, generally, the detection systems trigger many alarms and most of them are false positives. These false alarms impact the quality of the detection. In this paper, we first present a method to improve the detection accuracy of SIP flooding detection in IMS by using a trust model. The trust value is calculated by a communication activity between a caller and a callee. By this algorithm, the trust value of an attacker is lower than a legitimate user because it does not have real human activities. To evaluate the proposed method, we integrate the trust model with three SIP flooding attack detection algorithms: Cumulative sum, Hellinger distance, and Tanimoto distance. The system is evaluated by using a comprehensive traffic dataset that consists of varying legitimate and malicious traffic patterns. The experimental results show that the trust integration method can reduce false alarms and improve the accuracy of the flooding attack detection algorithms.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6605798,no
Analyzing and Predicting Software Quality Trends Using Financial Patterns,"The financial community assesses and analyzes fundamental qualities of stocks to predict their future performance. During the analysis different external and internal factors are considered which can affect the stock price. Financial analysts use indicators and analysis patterns, such as such as Moving Averages, Crossover patterns, and M-Top/W-Bottom patterns to determine stock price trends and potential trading opportunities. Similar to the stock market, also qualities of software systems are part of larger ecosystems which are affected by internal and external factors. Our research provides a cross disciplinary approach which takes advantages of these financial indicators and analysis patterns and re-applies them for the analysis and prediction of evolvability qualities in software system. We conducted several case studies to illustrate the applicability of our approach.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6605837,no
Multi-constrained Routing Algorithm: A Networking Evaluation,"IP networks may face issues to support the offered workload due to the increasing number of Internet users, the steady influx of new Internet applications, which require stringent QoS, and applications needing big data transmission. QoS routing can be viewed as an attractive approach to tackle this issue. However, most of the QoS routing solutions are not evaluated in a realistic framework. In this paper we propose a networking evaluation of multi-constrained routing to assess the potential benefit of QoS routing protocol. To do this, we converted a multi-constrained routing algorithm into a protocol, and implemented it in the simulator NS2. Our results indicate that if the monitoring tool of a network can not sustain frequent link-state announcements, the benefits coming from implementing a QoS routing are quite low. On the other hand, if the network is equipped with an adequate measurement tool, then QoS routing can be worth implementing, and the routing based only the available bandwidth at each link arises as the best option (no need to consider the end-to-end delay constraint, nor the loss rate constraint).",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6605878,no
A Study on the Efficiency Aspect of Data Race Detection: A Compiler Optimization Level Perspective,"Dynamically detecting data races in multithreaded programs incurs significant slowdown and memory overheads. Many existing techniques have been put forward to improve the performance slowdown through different dimensions such as sampling, detection precision, and data structures to track the happened-before relations among events in execution traces. Compiling the program source code with different compiler optimization options, such as reducing the object code size as the selected optimization objective, may produce different versions of the object code. Does optimizing the object code with a standard optimization option help improve the performance of the precise online race detection? To study this question and a family of related questions, this paper reports a pilot study based on four benchmarks from the PARSEC 3.0 suite compiled with six GCC compiler optimization options. We observe from the empirical data that in terms of performance slowdown, the standard optimization options behave comparably to the optimization options for speed and code size, but behave quite different from the baseline option. Moreover, in terms of memory cost, the standard optimization options incur similar memory costs as the baseline option and the option for speed, and consume less memory than the option for code size.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6605907,no
Bayesian Probabilistic Monitor: A New and Efficient Probabilistic Monitoring Approach Based on Bayesian Statistics,"Modern software systems deal with increasing dependability requirements which specify non-functional aspect of a system correct operation. Usually, probabilistic properties are used to formulate dependability requirements like performance, reliability, safety, and availability. Probabilistic monitoring techniques, as an important assurance measure, has drawn more and more interest. Despite currently several approaches has been proposed to monitor probabilistic properties, it still lacks of a general and efficient monitoring approach for monitoring probabilistic properties. This paper puts forward a novel probabilistic monitoring approach based on Bayesian statistics, called Bayesian Probabilistic Monitor (BaProMon). By calculating Bayesian Factor, the approach can check whether the runtime information can provide sufficient evidences to support the null or alternative hypothesis. We give the corresponding algorithms and validate them via simulated-based experiments. The experimental results show that BaProMon can effectively monitor QoS properties. The results also indicate that our approach is superior to other approaches.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6605908,no
Evaluating Web Service Quality Using Finite State Models,"This paper addresses the problem of evaluating the Web service quality using Finite State Machines. The most popular metrics for estimating such quality and user perception are Quality of Service (QoS) and Quality of Experience (QoE), which represent objective and subjective assessments, correspondingly. In this paper, we show how QoS can be estimated for Web services and their composition using finite state models. We also discuss how different machine learning algorithms can be applied for evaluating QoE of Web services based on known QoS parameter values.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6605913,no
A Comparison of Mutation Analysis Tools for Java,"Mutation analysis allows software developers to evaluate the quality of a test suite. The quality is measured as the ability of the test suite to detect faults injected into the program under tests. A fault is detected if at least one test case gives different results on the original program and the fault injected one. Mutation tools aim at automating and speeding both the generation of fault injected variants, called mutants, and the execution of the test suite on those mutants. In this paper, we aim at offering meaningful elements of comparison between mutation tools for Java for different usage profiles.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6605925,no
ColFinder Collaborative Concurrency Bug Detection,"Many concurrency bugs are extremely difficult to be detected by random test due to huge input space and huge interleaving space. The multicore technology trend worsens this problem. We propose an innovative, collaborative approach called ColFinder to detect concurrency bugs effectively and efficiently. ColFinder uses static analysis to identify potential buggy statements. With respect to these statements, ColFinder uses program slicing to cut the original programs into smaller programs. Finally, it uses dynamic active test to verify whether the potential buggy statements will trigger real bugs. We implement a prototype of ColFinder, and evaluate it with several real-world programs. It significantly improves the probability of bug manifestation, from 0.75% to 89%. Additionally, ColFinder makes the time of bug manifestation obviously reduced by program slicing, with an average of 33%.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6605929,no
Similarity-Based Search for Model Checking: A Pilot Study with Java PathFinder,"When a model checker cannot explore the entire state space because of limited resources, model checking becomes a kind of testing with an attempt to find a failure (violation of properties) quickly. We consider two state sequences in model checking: (i) the sequence in which new states are generated, and (ii) the sequence in which the states generated in sequence (i) are checked for property violation. We observe that neighboring states in sequence (i) often have similarities in certain ways. Based on this observation we propose a search strategy, which generates sequence (ii) in such a way that similar states are evenly spread over the sequence. As a result, neighboring states in sequence (ii) can have a higher diversity. A pilot empirical study with Java Path Finder suggests that the proposed strategy can outperform random search in terms of creating equal or smaller number of states to detect a failure.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6605933,no
Leveraging a Constraint Solver for Minimizing Test Suites,"Software (regression) testing is performed to detect errors as early as possible and guarantee that changes did not affect the system negatively. As test suites tend to grow over time, (re-)executing the entire suite becomes prohibitive. We propose an approach, RZoltar, addressing this issue: it encodes the relation between a test case and its testing requirements (code statements in this paper) in a so-called coverage matrix, maps this matrix into a set of constraints, and computes a collection of optimal minimal sets (maintaining the same coverage as the original suite) by leveraging a fast constraint solver. We show that RZoltar efficiently (0.95 seconds on average) finds a collection of test suites that significantly reduce the size (64.88% on average) maintaining the same fault detection (as initial test suite), while the well-known greedy approach needs 11.23 seconds on average to find just one solution.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6605935,no
Taming Deadlocks in Multithreaded Programs,"Many real-world multithreaded programs contain deadlock bugs. These bugs should be detected and corrected. Many existing detection strategies are not consistently scalable to handle large-scale applications. Many existing dynamic confirmation strategies may not reveal detectable deadlocks with high probability. And many existing runtime deadlock-tolerant strategies may incur high runtime overhead and may not prevent the same deadlock from re-occurring. This paper presents the current progress of our project on dynamic deadlock detection, confirmation, and resolution. It also describes a test harness framework developed to support our proposed approach.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6605938,no
Adaptive Combinatorial Testing,"Combinatorial Testing (CT) has been proven to be effective in detecting and locating the interaction triggered failure in the last 20 years. But CT still suffers from many challenges, such as modeling for CT, sampling mechanisms for test generation, applicability and effectiveness. To overcome these issues of CT, adaptive combinatorial testing (ACT) is proposed in this paper, which improves the traditional CT with a well established adaptive testing method as the counter part of adaptive control and aims to make CT more flexible and practical. ACT can significantly enhance testing quality, software reliability and support testing strategy adjustment dynamically. To support further investigation, a preliminary form of concrete strategy for ACT is given as a heuristic guideline.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6605940,no
A Theoretical Study: The Impact of Cloning Failed Test Cases on the Effectiveness of Fault Localization,"Statistical fault localization techniques analyze the dynamic program information provided by executing a large number of test cases to predict fault positions in faulty programs. Related studies show that the extent of imbalance between the number of passed test cases and that of failed test cases may reduce the effectiveness of such techniques, while failed test cases can frequently be less than passed test cases in practice. In this study, we propose a strategy to generate balanced test suite by cloning the failed test cases for suitable number of times to catch up with the number of passed test cases. We further give an analysis to show that by carrying out the cloning the effectiveness of two representative fault localization techniques can be improved under certain conditions and impaired at no time.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6605941,no
A Low-Cost Fault Tolerance Technique in Multi-media Applications through Configurability,"As chip densities and clock rates increases, processors are becoming more susceptible to transient faults that affect program correctness. Therefore, fault tolerance becomes increasingly important in computing system. Two major concerns of fault tolerance techniques are: a) improving system reliability by detecting transient errors and b) reducing performance overhead. In this study, we propose a configurable fault tolerance technique targeting both high reliability and low performance overhead for multi-media applications. The basic principle is applying different levels of fault tolerance configurability, which means that different degrees of fault tolerance are applied to different parts of the source codes in multi-media applications. First, a primary analysis is performed on the source code level to classify the critical statements. Second, a fault injection process combined with a statistical analysis is used to assure the partition with regards to a confidence degree. Finally, checksum-based fault tolerance and instruction duplication are applied to critical statements, while no fault tolerance mechanism is applied to non-critical parts. Performance experiment results demonstrate that our configurable fault tolerance technique can lead to significant performance gains compared with duplicating all instructions. The fault coverage of this scheme is also evaluated. Fault injection results show that about 90% of outputs are application-level correctness with just 20% of runtime overhead.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6605943,no
An Approach to Reliable Software Architectures Evolution,"In recent years, reliability is becoming a more and more important concern for software architectures. There exist many reliability model to predict the software reliability at architecture level, but few of them give the formal description of the software architecture. Although many formal approaches have been proposed to specify the software architecture, unfortunately, few of them pay attention to the important non-functional characteristic, namely reliability here. In this paper, we try to bridge the gap between software reliability model and software architecture description. Our work expands such idea in four directions. First, we propose a reliable hypergraph grammar by extending hyperedge. Then we describe the architecture structure by using our reliable hypergraph grammar. Meanwhile, through this reliable hypergraph grammar, architecture evolution is achieved by applying predefined transformation rules. At last, we use a case study to illustrate how our approach works.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6605944,no
An Approach for Fault Localization Based on Program Slicing and Bayesian,"The key issue of reducing software cost and improving software reliability is locating defective codes precisely and efficiently. In this paper, we propose a fault localization method which combines program slicing and Bayesian method. First, we perform dynamic program slicing according to the slicing criteria. Then, we calculate the posterior probability according to Bayesian Theory. Finally, we take the posterior probability as the suspicion degree of the statement and rank the statements in the descending order based on suspicion degree. We apply our approach to six open-source programs. The results of the experiments show that the method we propose can improve the precision of fault localization to some extent.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6605947,no
Abstraction Based Domain Ontology Extraction for Idea Creation,"Idea creation is a complicated process in which it requires plenty of knowledge as support for specific domain. Creativity becomes a more and more important feature for idea nowadays when information in various domains is on an explosion. Likewise, it raises the requirement for the amount of domain knowledge as background. Many previous traditional approaches to domain specific idea creation are performed by domain experts based on their personal knowledge storage and manually information research, which are considered to be time consuming, uncreative and out of date prone processes. As a creative knowledge combination activity, idea creation requires a great number of knowledge covers from research to industry in the application domains. The integration of domain ontology and idea creation is a trend in creative computing research area. The introduction of domain ontology based approach into idea creation can bridge the gap between knowledge collection and mental thought, and improve the efficiency and creativity of idea creation. In this paper, we propose an abstraction method to support one of the essential parts in this field - domain ontology extraction. Abstraction techniques are explored, classified, selected and integrated while elements of domain ontology are defined including concepts and relations. Also, a framework and approach is specified for apply the method into domain ontology extraction with designed abstraction rules to support its automation. A case study on idea creation scenario particularly is represented to validate the feasibility and reusability of our proposed method. Furthermore, the mapping rules for transformation from abstracted results to domain ontology are discussed as an initial idea and further work.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6605949,no
Supporting Reliability Modeling and Analysis for Component-Based Software Architecture: An XML-Based Approach,"With recent development of Component-Based Software Engineering (CBSE), the importance of predicting the non-functional properties, such as performance and reliability, has been widely acknowledged. A special problem in CBSE stems from its specific development process: Software components should be specified and implemented independently from their later context to enable reuse. Thus, non-functional properties of components need to be specified in the abstract level of architecture. In this paper, we explore the possibility of supporting reliability modeling and analysis for component-based software architecture simultaneously by an XML-based approach. The contribution of this paper is twofold: first we present an extension of xADL 3.0 that enables the support for reliability modeling of software architectures, based on this extension, we propose a method for generation of analysis-oriented models for reliability prediction. We demonstrate the applicability of our approach by modeling an example and conducting reliability prediction.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6605957,no
Micro defect detection in solar cell wafer based on hybrid illumination and near-infrared optics,"In this paper, an defect detection system based on hybrid illumination and near-infrared optics, is developed for solar cell wafer. It consists of geometrical camera optics, hybrid illumination device(HID), near-infrared(NIR) camera optics, machinery and control system and algorithm of defect detection and software. Especially, illumination conditions in HID is determined for reliable defect detection. Optimum illumination conditions in the HID are found with contrast analysis of RGB LED image, based on design of experiment. As a result, various surface micro defects are accurately detected. It is shown that the developed defect detection system can accurately detect micro defects of solar cell wafer.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606013,no
Automatic synthesis of modular connectors via composition of protocol mediation patterns,"Ubiquitous and pervasive computing promotes the creation of an environment where Networked Systems (NSs) eternally provide connectivity and services without requiring explicit awareness of the underlying communications and computing technologies. In this context, achieving interoperability among heterogeneous NSs represents an important issue. In order to mediate the NSs interaction protocol and solve possible mismatches, connectors are often built. However, connector development is a never-ending and error-prone task and prevents the eternality of NSs. For this reason, in the literature, many approaches propose the automatic synthesis of connectors. However, solving the connector synthesis problem in general is hard and, when possible, it results in a monolithic connector hence preventing its evolution. In this paper, we define a method for the automatic synthesis of modular connectors, each of them expressed as the composition of independent mediators. A modular connector, as synthesized by our method, supports connector evolution and performs correct mediation.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606546,no
Drag-and-drop refactoring: Intuitive and efficient program transformation,"Refactoring is a disciplined technique for restructuring code to improve its readability and maintainability. Almost all modern integrated development environments (IDEs) offer built-in support for automated refactoring tools. However, the user interface for refactoring tools has remained largely unchanged from the menu and dialog approach introduced in the Smalltalk Refactoring Browser, the first automated refactoring tool, more than a decade ago. As the number of supported refactorings and their options increase, invoking and configuring these tools through the traditional methods have become increasingly unintuitive and inefficient. The contribution of this paper is a novel approach that eliminates the use of menus and dialogs altogether. We streamline the invocation and configuration process through direct manipulation of program elements via drag-and-drop. We implemented and evaluated this approach in our tool, Drag-and-Drop Refactoring (DNDRefactoring), which supports up to 12 of 23 refactorings in the Eclipse IDE. Empirical evaluation through surveys and controlled user studies demonstrates that our approach is intuitive, more efficient, and less error-prone compared to traditional methods available in IDEs today. Our results bolster the need for researchers and tool developers to rethink the design of future refactoring tools.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606548,no
Observable modified condition/decision coverage,"In many critical systems domains, test suite adequacy is currently measured using structural coverage metrics over the source code. Of particular interest is the modified condition/decision coverage (MC/DC) criterion required for, e.g., critical avionics systems. In previous investigations we have found that the efficacy of such test suites is highly dependent on the structure of the program under test and the choice of variables monitored by the oracle. MC/DC adequate tests would frequently exercise faulty code, but the effects of the faults would not propagate to the monitored oracle variables. In this report, we combine the MC/DC coverage metric with a notion of observability that helps ensure that the result of a fault encountered when covering a structural obligation propagates to a monitored variable; we term this new coverage criterion Observable MC/DC (OMC/DC). We hypothesize this path requirement will make structural coverage metrics 1.) more effective at revealing faults, 2.) more robust to changes in program structure, and 3.) more robust to the choice of variables monitored. We assess the efficacy and sensitivity to program structure of OMC/DC as compared to masking MC/DC using four subsystems from the civil avionics domain and the control logic of a microwave. We have found that test suites satisfying OMC/DC are significantly more effective than test suites satisfying MC/DC, revealing up to 88% more faults, and are less sensitive to program structure and the choice of monitored variables.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606556,no
What good are strong specifications?,"Experience with lightweight formal methods suggests that programmers are willing to write specification if it brings tangible benefits to their usual development activities. This paper considers stronger specifications and studies whether they can be deployed as an incremental practice that brings additional benefits without being unacceptably expensive. We introduce a methodology that extends Design by Contract to write strong specifications of functional properties in the form of preconditions, postconditions, and invariants. The methodology aims at being palatable to developers who are not fluent in formal techniques but are comfortable with writing simple specifications. We evaluate the cost and the benefits of using strong specifications by applying the methodology to testing data structure implementations written in Eiffel and C#. In our extensive experiments, testing against strong specifications detects twice as many bugs as standard contracts, with a reasonable overhead in terms of annotation burden and run-time performance while testing. In the wide spectrum of formal techniques for software quality, testing against strong specifications lies in a sweet spot with a favorable benefit to effort ratio.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606572,no
Data clone detection and visualization in spreadsheets,"Spreadsheets are widely used in industry: it is estimated that end-user programmers outnumber programmers by a factor 5. However, spreadsheets are error-prone, numerous companies have lost money because of spreadsheet errors. One of the causes for spreadsheet problems is the prevalence of copy-pasting. In this paper, we study this cloning in spreadsheets. Based on existing text-based clone detection algorithms, we have developed an algorithm to detect data clones in spreadsheets: formulas whose values are copied as plain text in a different location. To evaluate the usefulness of the proposed approach, we conducted two evaluations. A quantitative evaluation in which we analyzed the EUSES corpus and a qualitative evaluation consisting of two case studies. The results of the evaluation clearly indicate that 1) data clones are common, 2) data clones pose threats to spreadsheet quality and 3) our approach supports users in finding and resolving data clones.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606575,no
"How, and why, process metrics are better","Defect prediction techniques could potentially help us to focus quality-assurance efforts on the most defect-prone files. Modern statistical tools make it very easy to quickly build and deploy prediction models. Software metrics are at the heart of prediction models; understanding how and especially why different types of metrics are effective is very important for successful model deployment. In this paper we analyze the applicability and efficacy of process and code metrics from several different perspectives. We build many prediction models across 85 releases of 12 large open source projects to address the performance, stability, portability and stasis of different sets of metrics. Our results suggest that code metrics, despite widespread use in the defect prediction literature, are generally less useful than process metrics for prediction. Second, we find that code metrics have high stasis; they don't change very much from release to release. This leads to stagnation in the prediction models, leading to the same files being repeatedly predicted as defective; unfortunately, these recurringly defective files turn out to be comparatively less defect-dense.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606589,no
Lase: Locating and applying systematic edits by learning from examples,"Adding features and fixing bugs often require systematic edits that make similar, but not identical, changes to many code locations. Finding all the relevant locations and making the correct edits is a tedious and error-prone process for developers. This paper addresses both problems using edit scripts learned from multiple examples. We design and implement a tool called LASE that (1) creates a context-aware edit script from two or more examples, and uses the script to (2) automatically identify edit locations and to (3) transform the code. We evaluate LASE on an oracle test suite of systematic edits from Eclipse JDT and SWT. LASE finds edit locations with 99% precision and 89% recall, and transforms them with 91% accuracy. We also evaluate LASE on 37 example systematic edits from other open source programs and find LASE is accurate and effective. Furthermore, we confirmed with developers that LASE found edit locations which they missed. Our novel algorithm that learns from multiple examples is critical to achieving high precision and recall; edit scripts created from only one example produce too many false positives, false negatives, or both. Our results indicate that LASE should help developers in automating systematic editing. Whereas most prior work either suggests edit locations or performs simple edits, LASE is the first to do both for nontrivial program edits.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606596,no
Mining SQL injection and cross site scripting vulnerabilities using hybrid program analysis,"In previous work, we proposed a set of static attributes that characterize input validation and input sanitization code patterns. We showed that some of the proposed static attributes are significant predictors of SQL injection and cross site scripting vulnerabilities. Static attributes have the advantage of reflecting general properties of a program. Yet, dynamic attributes collected from execution traces may reflect more specific code characteristics that are complementary to static attributes. Hence, to improve our initial work, in this paper, we propose the use of dynamic attributes to complement static attributes in vulnerability prediction. Furthermore, since existing work relies on supervised learning, it is dependent on the availability of training data labeled with known vulnerabilities. This paper presents prediction models that are based on both classification and clustering in order to predict vulnerabilities, working in the presence or absence of labeled training data, respectively. In our experiments across six applications, our new supervised vulnerability predictors based on hybrid (static and dynamic) attributes achieved, on average, 90% recall and 85% precision, that is a sharp increase in recall when compared to static analysis-based predictions. Though not nearly as accurate, our unsupervised predictors based on clustering achieved, on average, 76% recall and 39% precision, thus suggesting they can be useful in the absence of labeled training data.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606610,no
Exploring the impact of inter-smell relations on software maintainability: An empirical study,"Code smells are indicators of issues with source code quality that may hinder evolution. While previous studies mainly focused on the effects of individual code smells on maintainability, we conjecture that not only the individual code smells but also the interactions between code smells affect maintenance. We empirically investigate the interactions amongst 12 code smells and analyze how those interactions relate to maintenance problems. Professional developers were hired for a period of four weeks to implement change requests on four medium-sized Java systems with known smells. On a daily basis, we recorded what specific problems they faced and which artifacts were associated with them. Code smells were automatically detected in the pre-maintenance versions of the systems and analyzed using Principal Component Analysis (PCA) to identify patterns of co-located code smells. Analysis of these factors with the observed maintenance problems revealed how smells that were co-located in the same artifact interacted with each other, and affected maintainability. Moreover, we found that code smell interactions occurred across coupled artifacts, with comparable negative effects as same-artifact co-location. We argue that future studies into the effects of code smells on maintainability should integrate dependency analysis in their process so that they can obtain a more complete understanding by including such coupled interactions.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606614,no
X-PERT: Accurate identification of cross-browser issues in web applications,"Due to the increasing popularity of web applications, and the number of browsers and platforms on which such applications can be executed, cross-browser incompatibilities (XBIs) are becoming a serious concern for organizations that develop web-based software. Most of the techniques for XBI detection developed to date are either manual, and thus costly and error-prone, or partial and imprecise, and thus prone to generating both false positives and false negatives. To address these limitations of existing techniques, we developed X-PERT, a new automated, precise, and comprehensive approach for XBI detection. X-PERT combines several new and existing differencing techniques and is based on our findings from an extensive study of XBIs in real-world web applications. The key strength of our approach is that it handles each aspects of a web application using the differencing technique that is best suited to accurately detect XBIs related to that aspect. Our empirical evaluation shows that X-PERT is effective in detecting real-world XBIs, improves on the state of the art, and can provide useful support to developers for the diagnosis and (eventually) elimination of XBIs.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606616,no
Measuring architecture quality by structure plus history analysis,"This case study combines known software structure and revision history analysis techniques, in known and new ways, to predict bug-related change frequency, and uncover architecture-related risks in an agile industrial software development project. We applied a suite of structure and history measures and statistically analyzed the correlations between them. We detected architecture issues by identifying outliers in the distributions of measured values and investigating the architectural significance of the associated classes. We used a clustering method to identify sets of files that often change together without being structurally close together, investigating whether architecture issues were among the root causes. The development team confirmed that the identified clusters reflected significant architectural violations, unstable key interfaces, and important undocumented assumptions shared between modules. The combined structure diagrams and history data justified a refactoring proposal that was accepted by the project manager and implemented.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606638,no
MIDAS: A design quality assessment method for industrial software,"Siemens Corporate Development Center Asia Australia (CT DC AA) develops and maintains software applications for the Industry, Energy, Healthcare, and Infrastructure & Cities sectors of Siemens. The critical nature of these applications necessitates a high level of software design quality. A survey of software architects indicated a low level of satisfaction with existing design assessment practices in CT DC AA and highlighted several shortcomings of existing practices. To address this, we have developed a design assessment method called MIDAS (Method for Intensive Design ASsessments). MIDAS is an expert-based method wherein manual assessment of design quality by experts is directed by the systematic application of design analysis tools through the use of a three view-model consisting of design principles, project-specific constraints, and an ility-based quality model. In this paper, we describe the motivation for MIDAS, its design, and its application to three projects in CT DC AA. We believe that the insights from our MIDAS experience not only provide useful pointers to other organizations and practitioners looking to assess and improve software design quality but also suggest research questions for the software engineering community to explore.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606640,no
Evaluating usefulness of software metrics: An industrial experience report,"A wide range of software metrics targeting various abstraction levels and quality attributes have been proposed by the research community. For many of these metrics the evaluation consists of verifying the mathematical properties of the metric, investigating the behavior of the metric for a number of open-source systems or comparing the value of the metric against other metrics quantifying related quality attributes. Unfortunately, a structural analysis of the usefulness of metrics in a real-world evaluation setting is often missing. Such an evaluation is important to understand the situations in which a metric can be applied, to identify areas of possible improvements, to explore general problems detected by the metrics and to define generally applicable solution strategies. In this paper we execute such an analysis for two architecture level metrics, Component Balance and Dependency Profiles, by analyzing the challenges involved in applying these metrics in an industrial setting. In addition, we explore the usefulness of the metrics by conducting semi-structured interviews with experienced assessors. We document the lessons learned both for the application of these specific metrics, as well as for the method of evaluating metrics in practice.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606641,no
User involvement in software evolution practice: A case study,"User involvement in software engineering has been researched over the last three decades. However, existing studies concentrate mainly on early phases of user-centered design projects, while little is known about how professionals work with post-deployment end-user feedback. In this paper we report on an empirical case study that explores the current practice of user involvement during software evolution. We found that user feedback contains important information for developers, helps to improve software quality and to identify missing features. In order to assess its relevance and potential impact, developers need to analyze the gathered feedback, which is mostly accomplished manually and consequently requires high effort. Overall, our results show the need for tool support to consolidate, structure, analyze, and track user feedback, particularly when feedback volume is high. Our findings call for a hypothesis-driven analysis of user feedback to establish the foundations for future user feedback tools.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606645,no
Predicting bug-fixing time: An empirical study of commercial software projects,"For a large and evolving software system, the project team could receive many bug reports over a long period of time. It is important to achieve a quantitative understanding of bug-fixing time. The ability to predict bug-fixing time can help a project team better estimate software maintenance efforts and better manage software projects. In this paper, we perform an empirical study of bug-fixing time for three CA Technologies projects. We propose a Markov-based method for predicting the number of bugs that will be fixed in future. For a given number of defects, we propose a method for estimating the total amount of time required to fix them based on the empirical distribution of bug-fixing time derived from historical data. For a given bug report, we can also construct a classification model to predict slow or quick fix (e.g., below or above a time threshold). We evaluate our methods using real maintenance data from three CA Technologies projects. The results show that the proposed methods are effective.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606654,no
Towards automated testing and fixing of re-engineered Feature Models,"Mass customization of software products requires their efficient tailoring performed through combination of features. Such features and the constraints linking them can be represented by Feature Models (FMs), allowing formal analysis, derivation of specific variants and interactive configuration. Since they are seldom present in existing systems, techniques to re-engineer FMs have been proposed. There are nevertheless error-prone and require human intervention. This paper introduces an automated search-based process to test and fix FMs so that they adequately represent actual products. Preliminary evaluation on the Linux kernel FM exhibit erroneous FM constraints and significant reduction of the inconsistencies.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606689,no
LambdaFicator: From imperative to functional programming through automated refactoring,"Java 8 introduces two functional features: lambda expressions and functional operations like map or filter that apply a lambda expression over the elements of a Collection. Refactoring existing code to use these new features enables explicit but unobtrusive parallelism and makes the code more succinct. However, refactoring is tedious (it requires changing many lines of code) and error-prone (the programmer must reason about the control-flow, data-flow, and side-effects). Fortunately, these refactorings can be automated. We present LambdaFicator, a tool which automates two refactorings. The first refactoring converts anonymous inner classes to lambda expressions. The second refactoring converts for loops that iterate over Collections to functional operations that use lambda expressions. In 9 open-source projects we have applied these two refactorings 1263 and 1595 times, respectively. The results show that LambdaFicator is useful. A video highlighting the main features can be found at: http://www.youtube.com/watch?v=EIyAflgHVpU.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606699,no
Query quality prediction and reformulation for source code search: The Refoqus tool,"Developers search source code frequently during their daily tasks, to find pieces of code to reuse, to find where to implement changes, etc. Code search based on text retrieval (TR) techniques has been widely used in the software engineering community during the past decade. The accuracy of the TR-based search results depends largely on the quality of the query used. We introduce Refoqus, an Eclipse plugin which is able to automatically detect the quality of a text retrieval query and to propose reformulations for it, when needed, in order to improve the results of TR-based code search. A video of Refoqus is found online at http://www.youtube.com/watch?v=UQlWGiauyk4.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606704,no
LASE: An example-based program transformation tool for locating and applying systematic edits,"Adding features and fixing bugs in software often require systematic edits which are similar, but not identical, changes to many code locations. Finding all edit locations and editing them correctly is tedious and error-prone. In this paper, we demonstrate an Eclipse plug-in called Lase that (1) creates context-aware edit scripts from two or more examples, and uses these scripts to (2) automatically identify edit locations and (3) transform the code. In Lase, users can view syntactic edit operations and corresponding context for each input example. They can also choose a different subset of the examples to adjust the abstraction level of inferred edits. When Lase locates target methods matching the inferred edit context and suggests customized edits, users can review and correct LASE's edit suggestion. These features can reduce developers' burden in repetitively applying similar edits to different methods. The tool's video demonstration is available at https://www.youtube.com/ watch?v=npDqMVP2e9Q.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606707,no
An observable and controllable testing framework for modern systems,"Modern computer systems are prone to various classes of runtime faults due to their reliance on features such as concurrency and peripheral devices such as sensors. Testing remains a common method for uncovering faults in these systems. However, commonly used testing techniques that execute the program with test inputs and inspect program outputs to detect failures are often ineffective. To test for concurrency and temporal faults, test engineers need to be able to observe faults as they occur instead of relying on observable incorrect outputs. Furthermore, they need to be able to control thread or process interleavings so that they are deterministic. This research will provide a framework that allows engineers to effectively test for subtle and intermittent faults in modern systems by providing them with greater observability and controllability.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606721,no
Fault comprehension for concurrent programs,"Concurrency bugs are difficult to find because they occur with specific memory-access orderings between threads. Traditional bug-finding techniques for concurrent programs have focused on detecting raw-memory accesses representing the bugs, and they do not identify memory accesses that are responsible for the same bug. To address these limitations, we present an approach that uses memory-access patterns and their suspicious-ness scores, which indicate how likely they are to be buggy, and clusters the patterns responsible for the same bug. The evaluation on our prototype shows that our approach is effective in handling multiple concurrency bugs and in clustering patterns for the same bugs, which improves understanding of the bugs.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606739,no
Studying the effect of co-change dispersion on software quality,"Software change history plays an important role in measuring software quality and predicting defects. Co-change metrics such as number of files changed together has been used as a predictor of bugs. In this study, we further investigate the impact of specific characteristics of co-change dispersion on software quality. Using statistical regression models we show that co-changes that include files from different subsystems result in more bugs than co-changes that include files only from the same subsystem. This can be used to improve bug prediction models based on co-changes.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606741,no
Changeset based developer communication to detect software failures,"As software systems get more complex, the companies developing them consist of larger teams and therefore results in more complex communication artifacts. As these software systems grow, so does the impact of every action to the product. To prevent software failure created by this growth and complexity, companies need to find more efficient and effective ways to communicate. The method used in this paper presents developer communication in the form of social networks of which have properties that can be used to detect software failures.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606747,no
Data science for software engineering,"Target audience: Software practitioners and researchers wanting to understand the state of the art in using data science for software engineering (SE). Content: In the age of big data, data science (the knowledge of deriving meaningful outcomes from data) is an essential skill that should be equipped by software engineers. It can be used to predict useful information on new projects based on completed projects. This tutorial offers core insights about the state-of-the-art in this important field. What participants will learn: Before data science: this tutorial discusses the tasks needed to deploy machine-learning algorithms to organizations (Part 1: Organization Issues). During data science: from discretization to clustering to dichotomization and statistical analysis. And the rest: When local data is scarce, we show how to adapt data from other organizations to local problems. When privacy concerns block access, we show how to privatize data while still being able to mine it. When working with data of dubious quality, we show how to prune spurious information. When data or models seem too complex, we show how to simplify data mining results. When data is too scarce to support intricate models, we show methods for generating predictions. When the world changes, and old models need to be updated, we show how to handle those updates. When the effect is too complex for one model, we show how to reason across ensembles of models. Pre-requisites: This tutorial makes minimal use of maths of advanced algorithms and would be understandable by developers and technical managers.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606752,no
4th International workshop on managing technical debt (MTD 2013),"Although now 20 years old, only recently has the concept of technical debt gained some momentum and credibility in the software engineering community. The goal of this fourth workshop on managing technical debt is to engage researchers and practitioners in exchanging ideas on viable research directions and on how to put the concept to actual use, beyond its usage as a rhetorical instrument to discuss the fate and ailments of software development projects. The workshop participants presented and discussed approaches to detect, analyze, visualize, and manage technical debt, in its various forms, on large software-intensive system developments.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606774,no
Adding automatic dependency processing to Makefile-based build systems with amake,"This paper explains how to improve the quality of an existing Makefile-based build system, using a new variant of Make. Ordinary file-oriented dependencies are detected, recorded, and monitored automatically. Checksums are compared, rather than timestamps. Other important dependencies are also processed automatically. This provides an accurate, compact, and low-maintenance build system. Experiences with the Linux kernel/driver build system are described.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6607687,no
Kanbanize the release engineering process,"Release management process must be adapted when IT organizations scale up to avoid discontinuity at the release flow and to preserve the software quality. This paper reports on means to improve the release process in a large-scale project. It discusses the rationale behind adopting Kanban principles for release management, how to implement these principles within a transitional approach, and what are the benefits. The paper discusses the post-transitional product to assess the status of the outcomes.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6607689,no
An analytical model to evaluate reliability of cloud computing systems in the presence of QoS requirements,"Cloud computing is widely referred as the next generation of computing systems. Reliability is a key metric for assessing performance in such systems. Redundancy and diversity are prevalent approaches to enhance reliability in Cloud Computing Systems (CCS). Proper resource allocation is an alternative approach to reliability improvement in such systems. In contrast to redundancy, appropriate resource allocation can improve system reliability without imposing extra cost. On the other hand, contemplating reliability irrespective of Quality of Service (QoS) requirements may be undesirable in most of CCSs. In this paper, we focus on resource allocation approach and introduce an analytical model in order to analyze system reliability besides considering application and resource constraints. Task precedence structure and QoS are taken into account as the application constraints. Memory and storage limitation of each server as well as maximum communication load on each link are considered as the principle resource constraints. In addition, effect of network topology on system reliability is discussed in detail and the model is extended to cover various network topologies.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6607860,no
Modular AUV system for Sea Water Quality Monitoring and Management,"The sustained and cost-effective monitoring of the water quality within European coastal areas is of growing importance in view of the upcoming European marine and maritime directives, i.e. the increased industrial use of the marine environment. Such monitoring needs mechanisms/systems to detect the water quality in a large sea area at different depths in real time. This paper presents a system for the automated detection and analysis of water quality parameters using an autonomous underwater vehicle. The analysis of discharge of nitrate into Norwegian fjords near aqua farms is one of the main application fields of this AUV system. As carrier platform the AUV CWolf from the Fraunhofer IOSB-AST will be used, which is perfectly suited through its modular payload concept. The mission task and the integration of the payload unit which includes the sensor module, the scientific and measurement computer in the AUV carrier platform will be described. Few practice oriented information about the software and interface concept, the function of the several software modules and the test platform with the several test levels to test every module will be discussed.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6608086,no
Statistical fault localization in decision support system based on probability distribution criterion,"Finding the location of a fault in code is an important research and practical problem, which often requires much time and manual effort. To automate this time consuming task, a class of predicate-based statistical fault localization techniques have been proposed, which test the similarity of dynamic predicate spectra between non-failed runs and failed runs and suggest suspicious predicates to the programmers to facilitate the identification of faults. However, with the existence of coincidental correctness, how to efficiently and effectively compare the difference of predicate spectra distribution has become a crucial problem to be solved. In this paper, we make use of probability distribution criterion in developing a new statistical fault localization algorithm. Instead of using geometry distance, it calculates the overlapping of dynamic predicate spectra in two communities (non-failed runs and failed runs) to evaluate the difference. Empirical results show that our technique outperforms some representative predicate-based fault localization techniques for localizing faults in most subject programs of the Siemens suite and space program. To facilitate the debugging process and provide visual help to the debugger, we also designed a system software prototype, which integrates many recent fault localization algorithms, including the one proposed in this paper.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6608516,no
Feature interaction testing of variability intensive systems,Testing variability intensive systems is a formidable task due to the combinatorial explosion of feature interactions that result from all variations. We developed and validated an approach of combinatorial test generation using Multi-Perspective Feature Models (MPFM). MPFMs are a set of feature models created to achieve Separation of Concerns within the model. This approach improves test coverage of variability. Results from an experiment on a real-life case show that up to 37% of the test effort could be reduced and up to 79% defects from the live system could be detected. We discuss the learning from this experiment and further research potential in testing variability intensive systems.,2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6608666,no
Managing technical debt: An industrial case study,"Technical debt is the consequence of trade-offs made during software development to ensure speedy releases. The research community lacks rigorously evaluated guidelines to help practitioners characterize, manage and prioritize debt. This paper describes a study conducted with an industrial partner during their implementation of Agile development practices for a large software development division within the company. The report contains our initial findings based on ethnographic observations and semi-structured interviews. The goal is to identify the best practices regarding managing technical debt so that the researchers and the practitioners can further evaluate these practices to extend their knowledge of the technical debt metaphor. We determined that the developers considered their own taxonomy of technical debt based on the type of work they were assigned and their personal understanding of the term. Despite management's high-level categories, the developers mostly considered design debt, testing debt and defect debt. In addition to developers having their own taxonomy, assigning dedicated teams for technical debt reduction and allowing other teams about 20% of time per sprint for debt reduction are good initiatives towards lowering technical debt. While technical debt has become a well-regarded concept in the Agile community, further empirical evaluation is needed to assess how to properly apply the concept for various development organizations.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6608672,no
Mapping architectural decay instances to dependency models,"The architectures of software systems tend to drift or erode as they are maintained and evolved. These systems often develop architectural decay instances, which are instances of design decisions that negatively impact a system's lifecycle properties and are the analog to code-level decay instances that are potential targets for refactoring. While code-level decay instances are based on source-level constructs, architectural decay instances are based on higher levels of abstractions, such as components and connectors, and related concepts, such as concerns. Unlike code-level decay instances, architectural decay usually has more significant consequences. Not being able to detect or address architectural decay in time incurs architecture debt that may result in a higher penalty in terms of quality and maintainability (interest) over time. To facilitate architecture debt detection, in this paper, we demonstrate the possibility of transforming architectural models and concerns into an extended augmented constraint network (EACN), which can uniformly model the constraints among design decisions and environmental conditions. From an ACN, a pairwise-dependency relation (PWDR) can be derived, which, in turn, can be used to automatically and uniformly detect architectural decay instances.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6608677,no
Preliminary results of ON/OFF detection using an integrated system for Parkinson's disease monitoring,"This paper describes the experimental set up of a system composed by a set of wearable sensors devices for the recording of the motion signals and software algorithms for the signal analysis. This system is able to automatically detect and assess the severity of bradykinesia, tremor, dyskinesia and akinesia motor symptoms. Based on the assessment of the akinesia, the ON-OFF status of the patient is determined for each moment. The assessment performed through the automatic evaluation of the akinesia is compared with the status reported by the patients in their diaries. Preliminary results with a total recording period of 32 hours with two PD patients are presented, where a good correspondence (88.2 +/- 3.7 %) was observed. Best (93.7%) and worst (87%) correlation results are illustrated, together with the analysis of the automatic assessment of the akinesia symptom leading to the status determination. The results obtained are promising, and if confirmed with further data, this automatic assessment of PD motor symptoms will lead to a better adjustment of medication dosages and timing, cost savings and an improved quality of life of the patients.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6609657,no
A laboratory instrument for characterizing multiple microelectrodes,"The task of chronic monitoring and characterizing a large number of microelectrodes can be tedious and error prone, especially if needed to be done in vivo. This paper presents a lab instrument that automates the measurement and data processing, allowing for large numbers of electrodes to be characterized within a short time period. A version 1.0 of the Electrode Analyser System (EAS 1.0) has already been used in various neural engineering laboratories, as well by one electrode array manufacturer. The goal of the current work is to implement the EAS 2.0 system that provides improved performance beyond that of the 1.0 system, as well as reducing size and cost.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6609811,no
A new device for the care of Congenital Central Hypoventilation Syndrome patients during sleep,"Congenital Central Hypoventilation Syndrome (CCHS) is a genetic disease that causes an autonomous nervous system dysregulation. Patients are unable to have a correct ventilation, especially during sleep, facing risk of death. Therefore, most of them are mechanically ventilated during night and their blood oxygenation is monitored, while a supervisor keeps watch over them. If low oxygen levels are detected by the pulse-oximeter, an alarm fires; the supervisor deals with the situation and, if there is neither a technical problem nor a false alarm, wakes the subject, as CCHS patients usually recover from hypoxia when roused from sleep. During a single night multiple alarms may occur, causing fractioned sleep for the subject and a lasting state of anxiety for supervisors. In this work we introduce a novel device that can: acquire realtime data from a pulse-oximeter; provide a multisensory stimulation (e.g. by means of an air fan, a vibrating pillow, and a buzzer), if saturation falls under a threshold; stop the stimulation if oxygenation recovers; wake up the patient or the supervisor if the suffering state lasts beyond a safe interval. The main aim of this work is to lessen the number of awakenings, improving the quality of sleep and life for patients and their supervisors, and to increase young and adult CCHS patients autonomy. Initial testing of the device on a CCHS patient and his supervisor has provided encouraging preliminary results.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6610034,no
Saccadic Vector Optokinetic Perimetry (SVOP): A novel technique for automated static perimetry in children using eye tracking,"Perimetry is essential for identifying visual field defects due to disorders of the eye and brain. However, young children are often unable to reliably perform the preferred method of visual field assessment known as automated static perimetry (ASP). This paper introduces a novel method of ASP specifically developed for children called Saccadic Vector Optokinetic Perimetry (SVOP). SVOP uses eye tracking to detect the natural saccadic eye response of gaze orientation towards visual field stimuli if they are seen. In this paper, the direction and magnitude of a sample of subject gaze responses to visual field stimuli is used to construct a software decision algorithm for use in SVOP. SVOP was clinically evaluated in a group of 24 subjects, comprising children and adults, with and without visual field defects, by comparison with an equivalent test on the Humphrey Field Analyser (HFA). SVOP provides promising visual field test results when compared with the reference HFA test, and has proven extremely useful in detecting visual field defects in children unable to perform traditional ASP.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6610218,no
A fast recognition algorithm for detecting common broadcasting faults,"The video before it is been broadcast should be detected to find out whether it has broadcasting faults. Broadcasting fault mainly refers to black field, static frame, color bar, mute, volume excess. In this paper, we proposed a fast recognition algorithm based on MATLAB to detect common broadcasting faults. We used M language to complete a software platform which can identify the static frame, black fields, color bar in the video sequence, and the mute, volume excess in the audio sequence. Experimental evaluation shows that our approach significantly performs well in the automatic broadcasting faults detection.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6611470,no
Avoiding state explosion problem of generated AJAX web application state machine using BDD,"There is growing tendency of users to use web application in place of desktop application because of technological advancement such as AJAX. AJAX is used to build single page web application because content and structure can be changed using AJAX features like Asynchronous communication and run time DOM manipulation. To understand and analyze extreme dynamism of web application, we implemented a tool which is used to generate state machine model of dynamic behavior of user session. In this research paper, we validated and evaluated efficiency of the generated model to detect faults embedded area. However, the state machine can be huge and unbounded and may hit by state explosion problem for large number of user session traces and for extreme dynamism. In this paper to avoid this problem, we used binary decision diagram, a model checking technique to reduce state space at the time of state machine generation. Finally, we are able to control the size of generated state machine without affecting faults embedded area.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6612224,no
Runtime verification and reflection for wireless sensor networks,"The paper proposes to re-visit a light-weight verification technique called runtime verification in the context of wireless sensor networks. The authors believe that especially an extension of runtime verification which is called runtime reflection and which is not only able to detect faults, but diagnose and even repair them, can be an important step towards robust, self-organizing and self-healing WSNs. They present the basic idea of runtime reflection and possible applications.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6612263,no
Sens4U: Wireless sensor network applications for environment monitoring made easy,"The development of wireless sensor network (WSN) or cyber physical systems (CPS) applications is a complex and error prone task. This is due to the huge number of possible combinations of protocols and other software modules, to choose from. Additionally, testing of the chosen configuration and the individual software modules is not a trivial task, especially in case where they are all implemented from scratch. The aim of the Sens4U methodology we present in this paper is to simplify and possibly automate the process of building a WSN application and to simplify its testing. The main idea of our approach is to exploit the modularity of the available libraries in order to speed-up application development done by non-WSN-experts and to solve the real-life problems. The proposed abstraction is very powerful-the modules provide specific functionalities via defined interfaces and can be connected using these according to the application requirements, to create the desired and minimum target configuration. The modularity improves the testability and reuse of components and thus, their reliability and, as a result, the reliability of the target configurations. Further, the Sens4U approach goes beyond pure software generation and supports creating software and hardware configurations. We are currently focusing on environment monitoring scenarios in order to analyze this problem area in the semi-automatic computer aided application logic generalization process. This paper presents the general concept as well as the tool chain that supports the application development done by non-WSN-experts.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6612264,no
How much really changes? A case study of firefox version evolution using a clone detector,"This paper focuses on the applicability of clone detectors for system evolution understanding. Specifically, it is a case study of Firefox for which the development release cycle changed from a slow release cycle to a fast release cycle two years ago. Since the transition of the release cycle, three times more versions of the software were deployed. To understand whether or not the changes between the newer versions are as significant as the changes in the older versions, we measured the similarity between consecutive versions.We analyzed 82MLOC of C/C++ code to compute the overall change distribution between all existing major versions of Firefox. The results indicate a significant decrease in the overall difference between many versions in the fast release cycle. We discuss the results and highlight how differently the versions have evolved in their respective release cycle. We also relate our results with other results assessing potential changes in the quality of Firefox. We conclude the paper by raising questions on the impact of a fast release cycle.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6613048,no
Improving program comprehension by answering questions (keynote),"My Natural Programming Project is working on making software development easier to learn, more effective, and less error prone. An important focus over the last few years has been to discover what are the hard-to-answer questions that developers ask while they are trying to comprehend their programs, and then to develop tools to help answer those questions. For example, when studying programmers working on everyday bugs, we found that they continuously ask Why and Why Not questions as they try to comprehend what happened. We developed the Whyline debugging tool, which allows programmers to directly ask these questions of their programs and get a visualization of the answers. In a small lab study, Whyline increased productivity by a factor of about two. We studied professional programmers trying to understand unfamiliar code and identified over 100 questions they identified as hard-to-answer. In particular, we saw that programmers frequently had specific questions about the feasible execution paths, so we developed a new visualization tool to directly present this information. When trying to use unfamiliar APIs, such as the Java SDK and the SAP eSOA APIs, we discovered some common patterns that make programmers up to 10 times slower in finding and understanding how to use the appropriate methods, so we developed new tools to assist them. This talk will provide an overview of our studies and resulting tools that address program comprehension issues.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6613827,no
SArF map: Visualizing software architecture from feature and layer viewpoints,"To facilitate understanding the architecture of a software system, we developed SArF Map technique that visualizes software architecture from feature and layer viewpoints using a city metaphor. SArF Map visualizes implicit software features using our previous study, SArF dependency-based software clustering algorithm. Since features are high-level abstraction units of software, a generated map can be directly used for high-level decision making such as reuse and also for communications between developers and non-developer stakeholders. In SArF Map, each feature is visualized as a city block, and classes in the feature are laid out as buildings reflecting their software layer. Relevance between features is represented as streets. Dependency links are visualized lucidly. Through open source and industrial case studies, we show that the architecture of the target systems can be easily overviewed and that the quality of their packaging designs can be quickly assessed.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6613832,no
Quality analysis of source code comments,"A significant amount of source code in software systems consists of comments, i. e., parts of the code which are ignored by the compiler. Comments in code represent a main source for system documentation and are hence key for source code understanding with respect to development and maintenance. Although many software developers consider comments to be crucial for program understanding, existing approaches for software quality analysis ignore system commenting or make only quantitative claims. Hence, current quality analyzes do not take a significant part of the software into account. In this work, we present a first detailed approach for quality analysis and assessment of code comments. The approach provides a model for comment quality which is based on different comment categories. To categorize comments, we use machine learning on Java and C/C++ programs. The model comprises different quality aspects: by providing metrics tailored to suit specific categories, we show how quality aspects of the model can be assessed. The validity of the metrics is evaluated with a survey among 16 experienced software developers, a case study demonstrates the relevance of the metrics in practice.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6613836,no
QoS support in routing protocols for MANET,"This paper deals with the characteristics of mobile ad-hoc networks and the issue of quality of service assurance in these networks. The work is focused mainly on the routing protocol AODV and its ability to provide different processing probabilities for different types of network traffic. Firstly, the development of the MANET simulation model and implementation of AODV protocol are described. The Network Simulator 3 was used as the key software tool for this purpose. The analysis of the efficiency of QoS mechanism implemented into AODV protocol and the final evaluation are posted at the end of this paper.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6613903,no
Copy-move forgery detection in images via 2D-Fourier Transform,"Digital images have been widely used in many applications. However, digital image forgery has already become a serious problem due to the rapid development of powerful image editing software. One of the most commonly used forgery techniques is Copy-move forgery that copies a region of an image and pastes it on the other region in the same image. In recent years, most techniques aim to detect such tampering. Different feature extraction methods have been used to improve the capability of the detection algorithm. In this work, we used two dimensional Fourier Transform (2D-FT) to extract some features from the blocks. Predetermined number of Fourier coefficients hold information about the blocks. At the final stage, the similarity search between the adjacent feature vectors is performed to determine the forgery. Experimental results show that proposed method can detect the duplicated regions with high accuracy rate even if the image is distorted with blurring mask or it is compressed with different JPEG quality factors. The dimension of feature vector is also lower than the other methods in the literature. Thus, the method ensures the lower feature vector with high accuracy rates. The proposed method also detects multiple copy move forgery as shown in the results.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6614051,no
Range Analyzer: An Automatic Tool for Arithmetic Overflow Detection in Model-Based Development,"Airborne software is considered safety critical, since a defect in its execution can lead to economic consequences and the loss of human lives. In order to increase the correctness of an embedded software implementing system functions, compliance to the guidelines DO-178C and DO-331 is used to demonstrate that the software was developed according to requirements. Software verification is one of the processes to be performed during software development life cycle, analyzing the files generated during the development process looking for defects that could have been introduced. Absence of arithmetic overflow in one of the variables is a situation to be proved by the verification team because, when there is an overflow, the software calculations could not be trusted anymore. In order to detect this situation, some tools may be used to check source codes or to perform such analysis in model-based software design. The aim of this paper is to present an overview of the airborne software approval process, focusing on the model-based development, and to introduce a preliminary version of the development of the Range Analyzer, a tool with the capability to detect arithmetic overflow occurrences in a model within a SCADE Suite project. This proposed tool is an implementation of a range propagation algorithm, modified for the software analysis needs.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6614318,no
The Test Path Generation from State-Based Polymorphic Interaction Graph for Object-Oriented Software,"Successful integration of classes makes functionalities work correctly in software. The individual class usually functions correctly, but when the classes are integrated several unexpected faults may occur. In Object-Oriented software it is particularly hard to detect faults when classes are integrated because of inheritance, polymorphism and dynamic binding. Software designers use Unified Modeling Language (UML) to create an abstract system scenario and to visualize the system's architecture. A lot of research reveals that UML is not only for software design, but also for software testing. More and more researchers have realized UML models can be a source for Object-Oriented software testing. This paper proposes an intermediate test model called the Polymorphism State SEquence Test Model (PSSETM), which is generated from sequence diagram, class diagram and state-charts for integration testing. The example of Bookstore System shows the PSSETM test model is able to exhibit the possible state of object and the polymorphic information of class. Based on the PSSETM test model, various coverage criteria are defined to generate valid test paths to enhance testing on interaction among classes and the polymorphism of class.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6614329,no
Towards Test Focus Selection for Integration Testing Using Method Level Software Metrics,"The aim of integration testing is to uncover errors in the interactions between system modules. However, it is generally impossible to test all the interactions between modules because of time and cost constraints. Thus, it is important to focus the testing on the connections presumed to be more error-prone. The goal of this research is to guide quality assurance team wherein a software system to focus when they perform integration testing to save time and resources. In this work, we use method level metrics that capture both dependencies and internal complexity of methods. In addition, we build a tool that calculates the metrics automatically. We also propose an approach to select the test focus in integration testing. The main goal is to reduce the number of test cases needed while still detecting at least 80% of integration errors. We conducted an experimental study on several Java applications taken from different domains. Error seeding technique have been used for evaluation. The experimental results showed that our proposed approach is very effective for selecting the test focus in integration testing. It reduces considerably the number of required test cases while at the same time detects at least 80% of integration errors.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6614332,no
Automated Test Data Generation for Coupling Based Integration Testing of Object Oriented Programs Using Evolutionary Approaches,"Software testing is one of the most important phases in development of software. Software testing detects faults in software and ensures quality. Software testing can be performed at unit, integration, or system level. Integration testing tests the interactions of different components, when they are integrated together in specific application, for the smooth functionality of software system. Coupling based testing is an integration testing approach that is based upon coupling relationships that exist among different variables across different call sites in functions. Different types of coupling exist between variables across different call sites. Up until now, test data generation approaches deal only unit level testing. There is no work for test data generation for coupling based integration testing. In this paper, we have proposed a novel approach for automated test data generation for coupling based integration testing of object oriented programs using genetic algorithm. Our approach takes the coupling path as input, containing different sub paths, and generates the test data using genetic algorithm. We have implemented a prototype tool E-Coup in Java and successfully performed different experiments for the generation of test data. In experiments with this tool, our approach has much better results as compared to random testing.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6614336,no
Collaborative bug triaging using textual similarities and change set analysis,"Bug triaging assigns a bug report, which is also known as a work item, an issue, a task or simply a bug, to the most appropriate software developer for fixing or implementing it. However, this task is tedious, time-consuming and error-prone if not supported by effective means. Current techniques either use information retrieval and machine learning to find the most similar bugs already fixed and recommend expert developers, or they analyze change information stemming from source code to propose expert bug solvers. Neither technique combines textual similarity with change set analysis and thereby exploits the potential of the interlinking between bug reports and change sets. In this paper, we present our approach to identify potential experts by identifying similar bug reports and analyzing the associated change sets. Studies have shown that effective bug triaging is done collaboratively in a meeting, as it requires the coordination of multiple individuals, the understanding of the project context and the understanding of the specific work practices. Therefore, we implemented our approach on a multi-touch table to allow multiple stakeholders to interact simultaneously in the bug triaging and to foster their collaboration. In the current stage of our experiments we have experienced that the expert recommendations are more specific and useful when the rationale behind the expert selection is also presented to the users.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6614727,no
What is social debt in software engineering?,"Social debt in software engineering informally refers to unforeseen project cost connected to a suboptimal development community. The causes of suboptimal development communities can be many, ranging from global distance to organisational barriers to wrong or uninformed socio-technical decisions (i.e., decisions that influence both social and technical aspects of software development). Much like technical debt, social debt impacts heavily on software development success. We argue that, to ensure quality software engineering, practitioners should be provided with mechanisms to detect and manage the social debt connected to their development communities. This paper defines and elaborates on social debt, pointing out relevant research paths. We illustrate social debt by comparison with technical debt and discuss common real-life scenarios that exhibit sub-optimal development communities.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6614739,no
Meeting intensity as an indicator for project pressure: Exploring meeting profiles,"Meetings are hot spots of communication and collaboration in software development teams. Both distributed and co-located teams need to meet for coordination, communication, and collaboration. It is difficult to assess the quality of these three crucial aspects, or the social effectiveness and impact of a meeting: Personalities, psychological and professional aspects interact. It is, therefore, challenging to identify emerging communication problems or to improve collaboration by studying a wealth of interrelated details of project meetings. However, it is relatively easy to count meetings, and to measure when and how long they took place. This is objective information, does not violate privacy of participants, and the data might even be retrieved from project calendars automatically. In an exploratory study, we observed 14 student teams working on comparable four-month projects. Among many other aspects, we counted and measured meetings. In this contribution, we compare the meeting profiles qualitatively, and derive a number of hypotheses relevant for software projects.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6614754,no
Addressing the QoS drift in specification models of self-adaptive service-based systems,"Analysts elaborate precise and verifiable specification models, using as inputs non-functional requirements and assumptions drawn from the current environment studied at design time. As most real world applications exist in dynamic environments, recently there has been research efforts towards the design of software systems that use their specification models during runtime. The main idea is that software systems should endeavor to keep their requirements satisfied by adapting their architectural configurations when appropriated. Unfortunately, such specifications models use specific numbers (i.e. values) to specify non-functional constraints (NFCs) and may become rapidly obsolete during runtime given the drastic changes that operational environments can go through. The above may create circumstances when software systems are unaware that their requirements have been violated. To mitigate the obsolescence of specification models we have already proposed to use computing with words (CWW) to represent the NFCs with linguistic values instead of numbers. The numerical meanings of these linguistic values are computed from the measurements of non-functional properties (NFPs) gathered by a monitoring infrastructure. This article introduces the concept of QoS-drift to represent a significant degree of change in the numerical meanings of the linguistic values of the NFPs in the service market. We add to our former proposal a QoS-drift's vigilance unit to update linguistic values only when a QoS-drift is detected. Therefore, the new models are proactive and automatically maintained, what results in a more efficient assessment of run-time requirements' compliance under non-stationary environments. We validate the effectiveness of our approach using (1) a service market of 1500 services with two NFPs, (2) a synthetical QoS-drift and, (3) five systems built by different service compositions. We have detected that four of t- e five systems experienced requirements violations that would not have been detected without the use of our approach.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6615201,no
The study of urban drainage network information system space framework data standards in kunming based on GIS,"Geographic data are the basis of the urban drainage network management information system, in order to ensure the consistency and validity of the data, provide reliable data sharing and exchange for the tube network information system, research of data standardization is very important and must be carried out. This article based on detected data, completing the following standardized work: 1.Divide the drainage network layer for ordering layer structure in logic; 2.Formulate the rules of data encoding for providing all the data in a standardized format; 3.Developing data attribute table, so to elaborate content rules for each type of tube wells based on unified coding; 4. At last, propose requirements of validity of the data to ensure the maintenance of the system data and new data update.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6615266,no
Testing Central Processing Unit scheduling algorithms using Metamorphic Testing,"Central Processing Unit (CPU) scheduling is used to allocate CPU for multiple processes. CPU is one of the most important resources in the computer system, and its scheduling is vital and influential in operating systems. Thus, it is necessary to ensure the correctness of the CPU scheduling program. However, testing the correctness of a scheduling program is difficult because it is hard to verify the correctness of its output, which is known as the test oracle problem in software testing. Metamorphic Testing (MT) which has been recently proposed to alleviate the test oracle problem, is applied to test the CPU scheduling program. In this paper, we use MT to test the Highest Response Ratio Next (HRRN) scheduling algorithm. Two simulators of HRRN scheduler are used in the evaluation of our method. Surprisingly, some real life faults in one open source simulator are detected by MT. Further experiments are performed based on mutants, and the experimental results show that MT is an effective strategy to test CPU scheduler.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6615365,no
The model-based service fault diagnosis with probability analysis,"With the development of web service, it's important to localize the fault activity and explain the faulty reason. In this paper, we propose a model-based diagnosis method of web service fault. In our method, we firstly built service model using Petri nets, then rank the diagnosed activities by computing the faulty probability of all activities, finally use the defined diagnosis rules to find out the faulty activity in the given diagnosis sequence. Our method not only improves the diagnostic accuracy, but also raises the diagnostic efficiency. Our experimental results show our method is effective and outperforms the existing model-based method.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6615417,no
Fault detection method based on file naming algorithm for ocean observing network,"Ocean observing network is foundational facility for marine disaster prevention. Base on the file naming algorithm of ocean observing data, a fault detection method for ocean observing network is proposed in this paper. By analyzing file naming and storing rules of ocean observing data, the file naming algorithm is designed. Further, the data receiving exception can be detected and located to the data source node by executing file naming algorithm. Last, combining with network circuit detecting result, the fault type can be distinguished and the faulty object can be located precisely. For illustration, a fault detection example with minutely data files is utilized to show the effect. Empirical results show that the fault detection method based on file naming algorithm can locate the faulty object in 10 minutes, and can provide operation protecting for ocean observing network.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6615457,no
Reducing service failures by failure and workload aware load balancing in SaaS clouds,"SLA violations are typically viewed as service failures. If service fails once, it will fail again unless remedial action is taken. In a virtualized environment, a common remedial action is to restart or reboot a virtual machine (VM). In this paper we present, a VM live-migration policy that is aware of SLA threshold violations of workload response time, physical machine (PM) and VM utilization as well as availability violations at the PM and VM. In the migration policy we take into account PM failures and VM (software) failures as well as workload features such as burstiness (coefficient of variation or CoV >1) which calls for caution during the selection of target PM when migrating these workloads. The proposed policy also considers migration of a VM when the utilization of the physical machine hosting the VM approaches its utilization threshold. We propose an algorithm that detects proactive triggers for remedial action, selects a VM (for migration) and also suggests a possible target PM. We show the efficacy of our proposed approach by plotting the decrease in the number of SLA violations in a system using our approach over existing approaches that do not trigger migration in response to non-availability related SLA violations, via discrete event simulation of a relevant case study.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6615511,no
Predicting job completion times using system logs in supercomputing clusters,"Most large systems such as HPC/cloud computing clusters and data centers are built from commercial off-the-shelf components. System logs are usually the main source of choice to gain insights into the system issues. Therefore, mining logs to diagnose anomalies has been an active research area. Due to the lack of organization and semantic consistency in commodity PC clusters' logs, what constitutes a fault or an error is subjective and thus building an automatic failure prediction model from log messages is hard. In this paper we sidestep the difficulty by asking a different question: Given the concomitant system log messages of a running job, can we predict the job's remaining time? We adopt Hidden Markov Model (HMM) coupled with frequency analysis to achieve this. Our HMM approach can predict 75% of jobs' remaining times with an error of less than 200 seconds.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6615513,no
Detecting and tolerating data corruptions due to device driver defects,"Critical systems widely depend on operating systems to perform their mission. Device drivers are a critical and defect-prone part of operating systems. Software defects in device drivers often cause corruption of data that may lead to data losses, that are a significant source of costs for large enterprise systems. This paper describes an ongoing research that aims at mitigating the impact of data corruption due to device driver defects on the availability and the integrity of data. We discuss a methodology for run-time detection and the tolerance of protocol violations in device drivers and then we present a preliminary activity that we are currently performing.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6615522,no
The KARYON project: Predictable and safe coordination in cooperative vehicular systems,"KARYON, a kernel-based architecture for safety-critical control, is a European project that proposes a new perspective to improve performance of smart vehicle coordination. The key objective of KARYON is to provide system solutions for predictable and safe coordination of smart vehicles that autonomously cooperate and interact in an open and inherently uncertain environment. One of the main challenges is to ensure high performance levels of vehicular functionality in the presence of uncertainties and failures. This paper describes some of the steps being taken in KARYON to address this challenge, from the definition of a suitable architectural pattern to the development of proof-of-concept prototypes intended to show the applicability of the KARYON solutions. The project proposes a safety architecture that exploits the concept of architectural hybridization to define systems in which a small local safety kernel can be built for guaranteeing functional safety along a set of safety rules. KARYON is also developing a fault model and fault semantics for distributed, continuous-valued sensor systems, which allows abstracting specific sensor faults and facilitates the definition of safety rules in terms of quality of perception. Solutions for improved communication predictability are proposed, ranging from network inaccessibility control at lower communication levels to protocols for assessment of cooperation state at the process level. KARYON contributions include improved simulation and fault-injection tools for evaluating safety assurance according to the ISO 26262 safety standard. The results will be assessed using selected use cases in the automotive and avionic domains.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6615530,no
Smart sensing and smart material for smart automotive damping,"Vehicle suspensions represent one of the most interesting applications of dampening, where the introduction of smart damping technologies and control policies lead hardware and software designers to overcome new measurement challenges. Advanced sensing techniques have been proposed and tested for real-time execution by an automotive ECU based on a DSP-microcontroller. Smart sensing is able both to improve noise filtering of the signals provided by low-cost sensors and detect specific motorcycle dynamics which most influence the road holding and comfort. Finally, an ANN has been modeled, which can be effectively adopted as a benchmark (in terms of false alarms and correctly detected faults) in the development of fault detection strategies (i.e., threshold identification) directed to the sensor validation of the rear suspension stroke.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6616288,no
The Time/State-based Software-Intensive Systems Failure Mode Researches,"Nowadays the application status of Software-Intensive Systems(SISs) introduces a category of system failure caused by unforeseen operation or environment change. Generally speaking this kind of failure can be observed as system emergent behavior or degraded running. Because it relates to both the running time and state, it is called Time/State(TS)-based SISs failure. Moreover it is one of the significant sources of SISs failure. However the related researches are few. This paper presents the life cycle of software-related failure of SISs firstly. Secondly it analyzes the TS-based SISs failure mechanism and establishes the corresponding model. Moreover it introduces the traditional verification methods of SISs. Furthermore it presents the definition, classification and ontology representation of TS-based SISs failure mode. The instance validation shows the existence of TS-based SISs failure and feasibility of detecting the failure by using combined test method primarily. Finally this paper analyzes the problems and prospects the future researches.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6616333,no
Early diagnostic value of circulating MiRNA-21 in lung cancer: A meta-analysis,"To evaluate the early diagnostic value of circulating miRNA-21 in diagnosis of lung cancer, databases such as Wan Fang, VIP, PubMed, and Elsevier were systematically searched from 2005 to 2013 to collect relevant references in which the diagnostic value had been evaluated. The statistics were consolidated and the qualities of the studies were classified. The data were analyzed using Meta Disc1.4 software. The diagnostic value of circulating miRNA-21 in lung cancer was assessed by pooling sensitivity, specificity, the likelihood ratio, and the Summary Receiver Operating Characteristic (SROC) curve. Publication biases of the studies involved were analyzed using Stata 11.0 software. A total of 143 papers were collected of which 8 were included, which contained 600 cases and 440 controls. A heterogeneity test proved the existence of homogeneity in this study. Upon analysis using random effects models, the weighted sensitivity was 0.68, the specificity 0.77, the positive likelihood ratio 2.84, the negative likelihood ratio 0.40, and the SROC Area Under the Curve (AUC) was 0.8133. Further analysis by subgroup showed that the 5 indicators mentioned above were 0.72, 0.84, 4.50, 0.27, and 0.8987, respectively, for the serum group and 0.63, 0.70, 1.95, 0.53, and 0.7318, respectively, for the plasma group. We conclude that circulating miRNA-21 can be regarded a valuable reference in diagnosis of lung cancer. This research showed that in lung cancer the early diagnostic value of miRNA-21 in serum was better than that in plasma.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6616517,no
Three dimensional multifunctional nanomechanical photonic crystal sensor with high sensitivity by using pillar-inserted aslant nanocavity resonator,"We propose a method to detect nanomechanical variations in the three dimensional space with a shoulder-coupled pillar-inserted aslant photonic crystal nanocavity resonator. FEM and 3D-FDTD simulation software are employed to investigate the sensing characteristics. With high quality factor of the aslant nanocavity and the optimized structure, high sensitivity of nanomechanical sensing can be achieved in three dimensions and the limitation of the smallest detectable variations is ultra small. For its ultra small size and ultra high sensitivity in every dimension, this versatile nanomechanical sensor can be widely used in MEMS.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6617196,no
Methods with low complexity for evaluating cloud service reliability,"The critical significance of cloud computing lies in its ability to deliver high performance and reliable calculation on demand to external customers over the Internet. Because of heterogeneous software/hardware components and complicated interactions among them, the probability of failures improves. The services reliability arouses more attention. Cloud reliability analysis and modeling are very critical but hard because of the complexity and large scale of the system. The connectivity of subtasks and data-resources can affect the system reliability. This paper proposes the analysis methods of cloud service reliability based on a simple manner on two conditions: independent failures and correlated failures, which uses Graph Theory, Bayesian networks and Markov models. Simulation results show that time complexity of our proposed method has been greatly improved than traditional algorithms. Our new methods ensure the precision of reliability calculation.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6618577,no
Jigsaw: Scalable software-defined caches,"Shared last-level caches, widely used in chip-multi-processors (CMPs), face two fundamental limitations. First, the latency and energy of shared caches degrade as the system scales up. Second, when multiple workloads share the CMP, they suffer from interference in shared cache accesses. Unfortunately, prior research addressing one issue either ignores or worsens the other: NUCA techniques reduce access latency but are prone to hotspots and interference, and cache partitioning techniques only provide isolation but do not reduce access latency.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6618818,no
A study of the community structure of a complex software network,"This paper presents a case study of a large software system, Netbeans 6.0, made of independent subsystems, which are analyzed as complex software networks. Starting from the source code we built the associated software graphs, where classes represent graph nodes and inter-class relationships represent graph edges. We computed various metrics for the software systems and found interdependences with various quantities computed by mean of the complex network analysis. In particular we found that the number of communities in which the software networks can be partitioned and their modularity, average path length and mean degree can be related to the amount of bugs detected in the system. This result can be useful to provide indications about the fault proneness of software clusters in terms of quantities related to the associated graph structure.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6619331,no
Metrics for modularization assessment of Scala and C# systems,"Modularization of a software system leads to software that is more understandable and maintainable. Hence it is important to assess the modularization quality of a given system. In this paper, we define metrics for quantifying the level of modularization in Scala and C# systems. We propose metrics for Scala systems, measuring modularization with respect to concepts like referential transparency, functional purity, first order functions etc., which are present in modern functional programming languages. We also propose modularity metrics for C# systems in addition to the Object Oriented metrics that are existing in the literature. We validated our metrics, by applying them to popular open-source Scala Systems - Lift, Play, Akka and C# systems - ProcessHacker and Cosmos.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6619334,no
Towards indicators of instabilities in software product lines: An empirical evaluation of metrics,"A Software Product Line (SPL) is a set of software systems (products) that share common functionalities, so-called features. The success of a SPL design is largely dependent on its stability; otherwise, a single implementation change will cause ripple effects in several products. Therefore, there is a growing concern in identifying means to either indicate or predict design instabilities in the SPL source code. However, existing studies up to now rely on conventional metrics as indicators of SPL instability. These conventional metrics, typically used in standalone systems, are not able to capture the properties of SPL features in the source code, which in turn might neglect frequent causes of SPL instabilities. On the other hand, there is a small set of emerging software metrics that take into account specific properties of SPL features. The problem is that there is a lack of empirical validation of the effectiveness of metrics in indicating quality attributes in the context of SPLs. This paper presents an empirical investigation through two set of metrics regarding their power of indicating instabilities in evolving SPLs. A set of conventional metrics was confronted with a set of metrics we instantiated to capture important properties of SPLs. The software evolution history of two SPLs were analysed in our studies. These SPLs are implemented using two different programming techniques and all together they encompass 30 different versions under analysis. Our analysis confirmed that conventional metrics are not good indicators of instabilities in the context of evolving SPLs. The set of employed feature dependency metrics presented a high correlation with instabilities proving its value as indicator of SPL instabilities.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6619339,no
TDDHQ: Achieving Higher Quality Testing in Test Driven Development,"Test driven development (TDD) appears not to be immune to positive test bias effects, as we observed in several empirical studies. In these studies, developers created a significantly larger set of positive tests, but at the same time the number of defects detected with negative tests is significantly higher than those detected by positive ones. In this paper we propose the concept of TDDHQ which is aimed at achieving higher quality of testing in TDD by augmenting the standard TDD with suitable test design techniques. To exemplify this concept, we present combining equivalence partitioning test design technique together with the TDD, for the purpose of improving design of test cases. Initial evaluation of this approach showed a noticeable improvement in the quality of test cases created by developers utilising TDDHQ approach.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6619485,no
A Toolchain for Home Automation Controller Development,"Home Automation systems provide a large number of devices to control diverse appliances. Taking advantage of this diversity to create efficient and intelligent environments requires well designed, validated, and implemented controllers. However, designing and deploying such controllers is a complex and error prone process. This paper presents a tool chain that transforms a design in the form of communicating state machines to an executable controller that interfaces to appliances through a service oriented middleware. Design and validation is supported by integrated model checking and simulation facilities. This is extendable to controller synthesis. This tool chain is implemented, and we provide different examples to show its usability.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6619499,no
Towards Translational Execution of Action Language for Foundational UML,"Model-driven engineering has prominently gained consideration as effective substitute of error-prone code-centric development approaches especially for its capability of abstracting the problem through models and then manipulating them to automatically generate target code. Nowadays, thanks to powerful modelling languages, a system can be designed by means of well-specified models that capture both structural as well as behavioural aspects. From them, target implementation is meant to be automatically generated. An example of well-established general purpose modelling language is the UML, recently enhanced with the introduction of an action language denominated ALF, both proposed by the OMG. In this work we focus on enabling the execution of models defined in UML-ALF and more specifically on the translational execution of ALF towards non-UML target platforms.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6619504,no
Accuracy of Contemporary Parametric Software Estimation Models: A Comparative Analysis,"Predicting the effort, duration and cost required to develop and maintain a software system is crucial in IT project management. Although an accurate estimation is invaluable for the success of an IT development project, it often proves difficult to attain. This paper presents an empirical evaluation of four parametric software estimation models, namely COCOMO II, SEER-SEM, SLIM, and True Planning, in terms of their project effort and duration prediction accuracy. Using real project data from 51 software development projects, we evaluated the capabilities of the models by comparing the predictions with the actual effort and duration values. The study showed that the estimation capabilities of the models investigated are on a par in accuracy, while there is still significant room for improvement in order to better address the prediction challenges faced in practice.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6619527,no
Identifying Implicit Architectural Dependencies Using Measures of Source Code Change Waves,"The principles of Agile software development are increasingly used in large software development projects, e.g. using Scrum of Scrums or combining Agile and Lean development methods. When large software products are developed by self-organized, usually feature-oriented teams, there is a risk that architectural dependencies between software components become uncontrolled. In particular there is a risk that the prescriptive architecture models in form of diagrams are outdated and implicit architectural dependencies may become more frequent than the explicit ones. In this paper we present a method for automated discovery of potential dependencies between software components based on analyzing revision history of software repositories. The result of this method is a map of implicit dependencies which is used by architects in decisions on the evolution of the architecture. The software architects can assess the validity of the dependencies and can prevent unwanted component couplings and design erosion hence minimizing the risk of post-release quality problems. Our method was evaluated in a case study at one large product at Saab Electronic Defense Systems (Saab EDS) and one large software product at Ericsson AB.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6619529,no
LiRCUP: Linear Regression Based CPU Usage Prediction Algorithm for Live Migration of Virtual Machines in Data Centers,"Virtualization is a vital technology of cloud computing which enables the partition of a physical host into several Virtual Machines (VMs). The number of active hosts can be reduced according to the resources requirements using live migration in order to minimize the power consumption in this technology. However, the Service Level Agreement (SLA) is essential for maintaining reliable quality of service between data centers and their users in the cloud environment. Therefore, reduction of the SLA violation level and power costs are considered as two objectives in this paper. We present a CPU usage prediction method based on the linear regression technique. The proposed approach approximates the short-time future CPU utilization based on the history of usage in each host. It is employed in the live migration process to predict over-loaded and under-loaded hosts. When a host becomes over-loaded, some VMs migrate to other hosts to avoid SLA violation. Moreover, first all VMs migrate from a host while it becomes under-loaded. Then, the host switches to the sleep mode for reducing power consumption. Experimental results on the real workload traces from more than a thousand Planet Lab VMs show that the proposed technique can significantly reduce the energy consumption and SLA violation rates.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6619533,no
Facilitating Scientific Workflow Configuration with Parameterized Workflow Skeletons,"This paper describes an operator for configuring scientific workflows that facilitates the process of assigning workflow activities to cloud resources. In general, modeling and configuring scientific workflows is complex and error-prone, because workflows are built of highly parallel patterns comprising huge numbers of tasks. Reusing tested patterns as building blocks avoids repeating errors. Workflow skeletons are parametrizable building blocks describing such patterns. Hence, scientists have a means to reuse validated parallel constructs for rapidly defining their in-silico experiments. Often, configurations of data parallel patterns are generated automatically. However, for many task parallel patterns each task needs to be configured manually. In frameworks like MapReduce, scientists have no control of how tasks are assigned to cloud resources. What is the strength of such patterns, may lead to unnecessary data transfers in other patterns. Workflow Skeletons facilitate the configuration by providing an operator that accepts parameters, this allows for scalable configurations saving time and cost by allocating cloud resources just in time. In addition, this configuration operator helps to define configurations that avoid unnecessary data transfers.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6619536,no
Simulation design of duffing system based on singlechip microcomputer,"In order to solve the parameters' adjusting difficulty of the continuous Duffing chaotic system, a method of using single-chip microcomputer to realize Duffing chaotic system is proposed in this paper. It is proven that the discrete Duffing chaotic system as well as the continuous system has the ability of signal detection. The simulation results based on singlechip microcomputer under MATLAB environment show that this method is feasible to adjust the parameters of the discrete Duffing chaotic system by software, and Duffing chaotic system based on single-chip microcomputer can detect the multi-frequency sine signals.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6619647,no
Detection limitation of high frequency signal travelling along underground power cable,"High frequency pulse injection is part of many diagnostics techniques, e.g. cable fault location. An injected pulse along an underground power cable will reflect at an impedance impurity, for instance a cable joint. These joints can be identified based on pulse reflections only if sufficiently high-frequency components can be detected. Signals with higher frequency components can provide more accurate spatial resolution but experience stronger attenuation and dispersion. This paper discusses whether pulse reflection from a cable joint can be better distinguished if detection is focused on high frequency signal content. Two aspects, considering effects of noise level and applied equipment, are discussed in detail: averaging and highpass filtering. It is shown that in presence of noise, averaging can improve signal to noise ratio also for signals below the quantization error of digital detection equipment. High-pass filtering is realized in hardware but similar results can be achieved by software implementation. However, simulation study shows that a high-pass filter itself hardly improves reflection recognition.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6619701,no
Towards feature-aware retrieval of refinement traces,"Requirements traceability supports practitioners in reaching higher project maturity and better product quality. To gain this support, traces between various artifacts of the software development process are required. Depending on the number of existing artifacts, establishing traces can be a time-consuming and error-prone task. Additionally, the manual creation of traces frequently interrupts the software development process. In order to overcome those problems, practitioners are asking for techniques that support the creation of traces (see Grand Challenge: Ubiquitous (GC-U)). In this paper, we propose the usage of a graph clustering algorithm to support the retrieval of refinement traces. Refinement traces are traces that exist between artifacts created in different phases of a development project, e.g., between features and use cases. We assessed the effectiveness of our approach in several TraceLab experiments. These experiments employ three standard datasets containing differing types of refinement traces. Results show that graph clustering can improve the retrieval of refinement traces and is a step towards the overall goal of ubiquitous traceability.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6620163,no
GPUburn: A system to test and mitigate GPU hardware failures,"Due to many factors such as, high transistor density, high frequency, and low voltage, today's processors are more than ever subject to hardware failures. These errors have various impacts depending on the location of the error and the type of processor. Because of the hierarchical structure of the compute units and work scheduling, the hardware failure on GPUs affect only part of the application. In this paper we present a new methodology to characterize the hardware failures of Nvidia GPUs based on a software micro-benchmarking platform implemented in OpenCL. We also present which hardware part of TESLA architecture is more sensitive to intermittent errors, which usually appears when the processor is aging. We obtained these results by accelerating the aging process by running the processors at high temperature. We show that on GPUs, intermittent errors impact is limited to a localized architecture tile. Finally, we propose a methodology to detect, record location of defective units in order to avoid them to ensure the program correctness on such architectures, improving the GPU fault-tolerance capability and lifespan.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6621133,no
Prototype test insertion co-processor for agile development in multi-threaded embedded environments,"Agile methodologies have been shown useful in constructing Enterprise applications with a reduced level of defects in the released product. Movement of Agile processes into the embedded world is hindered by the lack of suitable tool support. For example, software instrumented test insertion methods to detect race condition in multithreaded programs have the potential to increase code size beyond the limited embedded system memory, and degrade performance to an extent that would impair the real-time characteristics of the system. We propose a FPGA-based, hardware assisted, test insertion co-processor for embedded systems which introduces low additional system overhead and incurs minimal code size increase. In this preliminary study, we compare the ideal characteristics of a FPGA-based test insertion co-processor with our initial prototype and other proposed hardware assisted test insertion approaches.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6621224,no
Security Prognostics: Cyber meets PHM,"In this paper we cast a vision for Security Prognostics (SP) for critical systems, promoting the view that security related protections would be well served to integrate fully with Monitoring and Diagnostics (M&D) systems that assess the health of complex assets and systems. To detect complex Cyber threats we propose combining system parameters already in use by M&D systems for Prognostics and Health Monitoring (PHM) with security parameters. Combining system parameters used by M&D to detect non-malicious faults with the system parameters used by security schemes to detect complex Cyber threats will improve: (a) accuracy of PHM (b) security of M&D, and (c) availability and safety of critical systems. We also introduce the notion of Remaining Secure Life (RSL), assessed based on the propagation of security damage, to create the prospect for Security Prognostics. RSL will assist in the selection of appropriate response(s), based on breach or compromise to security component's and potential impact on system operation. An example of M&D data is provided which is normally associated with non-malicious faults providing input to detect Malware execution through time series monitoring.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6621448,no
Towards systems level prognostics in the Cloud,"Many application systems are transforming from device centric architectures to cloud based systems that leverage shared compute resources to reduce cost and maximize reach. These systems require new paradigms to assure availability and quality of service. In this paper, we discuss the challenges in assuring Availability and Quality of Service in a Cloud Based Application System. We propose machine learning techniques for monitoring systems logs to assess the health of the system. A web services data set is employed to show that variety of services can be clustered to different service classes using a k-means clustering scheme. Reliability, Availability, and Serviceability (RAS) logs and Job logs dataset from high performance computing system is employed to show that impending fatal errors in the system can be predicted from the logs using an SVM classifier. These approaches illustrate the feasibility of methods to monitor the systems health and performance of compute resources and hence can be used to manage these systems for high availability and quality of service for critical tasks such as health care monitoring in the cloud.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6621449,no
Method and RIKDEDIN software package for interpretation of remote sensing data,"The creation of the computer program package RIKDEDIN with the realization of all five preceding items will permit to develop and construct a specialized multi-processor on-board computer, which will automatically decode remote observation data, i.e. recognize objects of environment by remote (satellite, aircraft and ground-based) sensing in real time [1, 2].",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6622051,no
CI-LQD: A software tool for modeling and decision making with Low Quality Data,"The software tool CI-LQD (Computational Intelligence for Low Quality Data) is introduced in this paper. CI-LQD is an ongoing project that includes a lightweight open source software that has been designed with scientific and teaching purposes in mind. The main usefulness of the software is to automate the calculations involved in the statistical comparisons of different algorithms, with both numerical and graphical techniques, when the available information is interval-valued, fuzzy, incomplete or otherwise vague. A growing catalog of evolutionary algorithms for learning classifiers, models and association rules, along with their corresponding data conditioning and preprocessing techniques, is included. A demonstrative example of the tool is described that illustrates the capabilities of the software.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6622418,no
Hardware Trojan Horses in Cryptographic IP Cores,"Detecting hardware trojans is a difficult task in general. In this article we study hardware trojan horses insertion and detection in cryptographic intellectual property (IP) blocks. The context is that of a fabless design house that sells IP blocks as GDSII hard macros, and wants to check that final products have not been infected by trojans during the foundry stage. First, we show the efficiency of a medium cost hardware trojans detection method if the placement or the routing have been redone by the foundry. It consists in the comparison between optical microscopic pictures of the silicon product and the original view from a GDSII layout database reader. Second, we analyze the ability of an attacker to introduce a hardware trojan horse without changing neither the placement nor the routing of the cryptographic IP logic. On the example of an AES engine, we show that if the placement density is beyond 80%, the insertion is basically impossible. Therefore, this settles a simple design guidance to avoid trojan horses insertion in cryptographic IP blocks: have the design be compact enough, so that any functionally discreet trojan necessarily requires a complete replace and re-route, which is detected by mere optical imaging (and not complete chip reverse-engineering).",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6623552,no
Self-healing Performance Anomalies in Web-based Applications,"In this paper, we describe the SHoWA framework and evaluate its ability to recover from performance anomalies in Web-based applications. SHoWA is meant to automatically detect and recover from performance anomalies, without calling for human intervention. It does not require manual changes to the application source code or previous knowledge about its implementation details. The application is monitored at runtime and the anomalies are detected and pinpointed by means of correlation analysis. A recovery procedure is performed every time an anomaly is detected. An experimental study was conducted to evaluate the recovery process included in the SHoWA framework. The experimental environment considers a benchmarking application, installed in a high-availability system. The results show that SHoWA is able to detect and recover from different anomaly scenarios, before any visible error, higher-latency or work-in-progress loss is observed. It proved to be efficient in terms of time of repair. The performance impact induced on the managed system was low: the response time penalty per request varied between 0 and 2.21 milliseconds, the throughput was affected in less than 1%.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6623645,no
Hybrid Cloud Management to Comply Efficiently with SLA Availability Guarantees,"SLAs are common means to define specifications and requirements of cloud computing services, where the guaranteed availability is one of the most important parameters. Fulfilling the stipulated availability may be expensive, due to the cost of failure recovery software, and the amount of physical equipment needed to deploy the cloud services. Therefore, a relevant question for cloud providers is: How to guarantee the SLA availability in a cost efficient way? This paper studies different fault tolerance techniques available in the market, and it proposes the use of an hybrid management to have full control over the SLA risk, using only the necessary resources in order to keep a cost efficient operation. This paper shows how to model the probability distribution of the accumulated downtime, and how this can be used in the design of hybrid policies. Using specific case studies, this paper illustrates how to implement the proposed hybrid policies, and it shows the obtained cost saving by using them. This paper takes advantage of the cloud computing flexibility, and it opens the door to the use of dynamic management policies to reach specific performance objectives in ICT systems.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6623652,no
Composition-Safe re-parametrization in Distributed Component-based WSN Applications,"Contemporary Wireless Sensor Networks like Smart Offices and Smart Cities are evolving to become multi-purpose application hosting platforms. These WSN platforms can simultaneously support multiple applications which may be managed by multiple actors. Reconfigurable component models have been shown to be viable solutions to reducing the complexity of managing and developing these applications while promoting software re-use. However, implicit parameter dependencies between components make reconfiguration complex and error-prone. Our approach achieves automatic composition-safe re-parametrization of distributed component compositions. To achieve this, we propose the use of language annotations that allow component developers to make these dependencies explicit and constraint-aware network protocols to ensure constraint propagation and enforcement.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6623656,no
Answering questions about unanswered questions of Stack Overflow,"Community-based question answering services accumulate large volumes of knowledge through the voluntary services of people across the globe. Stack Overflow is an example of such a service that targets developers and software engineers. In general, questions in Stack Overflow are answered in a very short time. However, we found that the number of unanswered questions has increased significantly in the past two years. Understanding why questions remain unanswered can help information seekers improve the quality of their questions, increase their chances of getting answers, and better decide when to use Stack Overflow services. In this paper, we mine data on unanswered questions from Stack Overflow. We then conduct a qualitative study to categorize unanswered questions, which reveals characteristics that would be difficult to find otherwise. Finally, we conduct an experiment to determine whether we can predict how long a question will remain unanswered in Stack Overflow.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624015,no
Search-based duplicate defect detection: An industrial experience,"Duplicate defects put extra overheads on software organizations, as the cost and effort of managing duplicate defects are mainly redundant. Due to the use of natural language and various ways to describe a defect, it is usually hard to investigate duplicate defects automatically. This problem is more severe in large software organizations with huge defect repositories and massive number of defect reporters. Ideally, an efficient tool should prevent duplicate reports from reaching developers by automatically detecting and/or filtering duplicates. It also should be able to offer defect triagers a list of top-N similar bug reports and allow them to compare the similarity of incoming bug reports with the suggested duplicates. This demand has motivated us to design and develop a search-based duplicate bug detection framework at BlackBerry. The approach follows a generalized process model to evaluate and tune the performance of the system in a systematic way. We have applied the framework on software projects at BlackBerry, in addition to the Mozilla defect repository. The experimental results exhibit the performance of the developed framework and highlight the high impact of parameter tuning on its performance.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624025,no
A contextual approach towards more accurate duplicate bug report detection,"Bug-tracking and issue-tracking systems tend to be populated with bugs, issues, or tickets written by a wide variety of bug reporters, with different levels of training and knowledge about the system being discussed. Many bug reporters lack the skills, vocabulary, knowledge, or time to efficiently search the issue tracker for similar issues. As a result, issue trackers are often full of duplicate issues and bugs, and bug triaging is time consuming and error prone. Many researchers have approached the bug-deduplication problem using off-the-shelf information-retrieval tools, such as BM25F used by Sun et al. In our work, we extend the state of the art by investigating how contextual information, relying on our prior knowledge of software quality, software architecture, and system-development (LDA) topics, can be exploited to improve bug-deduplication. We demonstrate the effectiveness of our contextual bug-deduplication method on the bug repository of the Android ecosystem. Based on this experience, we conclude that researchers should not ignore the context of software engineering when using IR tools for deduplication.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624026,no
The Eclipse and Mozilla defect tracking dataset: A genuine dataset for mining bug information,"The analysis of bug reports is an important subfield within the mining software repositories community. It explores the rich data available in defect tracking systems to uncover interesting and actionable information about the bug triaging process. While bug data is readily accessible from systems like Bugzilla and JIRA, a common database schema and a curated dataset could significantly enhance future research because it allows for easier replication. Consequently, in this paper we propose the Eclipse and Mozilla Defect Tracking Dataset, a representative database of bug data, filtered to contain only genuine defects (i.e., no feature requests) and designed to cover the whole bug-triage life cycle (i.e., store all intermediate actions). We have used this dataset ourselves for predicting bug severity, for studying bug-fixing time and for identifying erroneously assigned components. Sharing these data with the rest of the community will allow for reproducibility, validation and comparison of the results obtained in bug-report analyses and experiments.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624028,no
Do software categories impact coupling metrics?,"Software metrics is a valuable mechanism to assess the quality of software systems. Metrics can help the automated analysis of the growing data available in software repositories. Coupling metrics is a kind of software metrics that have been extensively used since the seventies to evaluate several software properties related to maintenance, evolution and reuse tasks. For example, several works have shown that we can use coupling metrics to assess the reusability of software artifacts available in repositories. However, thresholds for software metrics to indicate adequate coupling levels are still a matter of discussion. In this paper, we investigate the impact of software categories on the coupling level of software systems. We have found that different categories may have different levels of coupling, suggesting that we need special attention when comparing software systems in different categories and when using predefined thresholds already available in the literature.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624030,no
"Discovering, reporting, and fixing performance bugs","Software performance is critical for how users perceive the quality of software products. Performance bugs - programming errors that cause significant performance degradation - lead to poor user experience and low system throughput. Designing effective techniques to address performance bugs requires a deep understanding of how performance bugs are discovered, reported, and fixed. In this paper, we study how performance bugs are discovered, reported to developers, and fixed by developers, and compare the results with those for non-performance bugs. We study performance and non-performance bugs from three popular code bases: Eclipse JDT, Eclipse SWT, and Mozilla. First, we find little evidence that fixing performance bugs has a higher chance to introduce new functional bugs than fixing non-performance bugs, which implies that developers may not need to be over-concerned about fixing performance bugs. Second, although fixing performance bugs is about as error-prone as fixing nonperformance bugs, fixing performance bugs is more difficult than fixing non-performance bugs, indicating that developers need better tool support for fixing performance bugs and testing performance bug patches. Third, unlike many non-performance bugs, a large percentage of performance bugs are discovered through code reasoning, not through users observing the negative effects of the bugs (e.g., performance degradation) or through profiling. The result suggests that techniques to help developers reason about performance, better test oracles, and better profiling techniques are needed for discovering performance bugs.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624035,no
Better cross company defect prediction,"How can we find data for quality prediction? Early in the life cycle, projects may lack the data needed to build such predictors. Prior work assumed that relevant training data was found nearest to the local project. But is this the best approach? This paper introduces the Peters filter which is based on the following conjecture: When local data is scarce, more information exists in other projects. Accordingly, this filter selects training data via the structure of other projects. To assess the performance of the Peters filter, we compare it with two other approaches for quality prediction. Within-company learning and cross-company learning with the Burak filter (the state-of-the-art relevancy filter). This paper finds that: 1) within-company predictors are weak for small data-sets; 2) the Peters filter+cross-company builds better predictors than both within-company and the Burak filter+cross-company; and 3) the Peters filter builds 64% more useful predictors than both within-company and the Burak filter+cross-company approaches. Hence, we recommend the Peters filter for cross-company learning.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624057,no
Using citation influence to predict software defects,"The software dependency network reflects structure and the developer contribution network reflects process. Previous studies have used social network properties over these networks to predict whether a software component is defect-prone. However, these studies do not consider the strengths of the dependencies in the networks. In our approach, we use a citation influence topic model to determine dependency strengths among components and developers, analyze weak and strong dependencies separately, and apply social network properties to predict defect-prone components. In experiments on Eclipse and NetBeans, our approach has higher accuracy than prior work.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624058,no
Error model and the reliability of arithmetic operations,"Error detecting and correcting codes are widely used in data transmission, storage systems and also for data processing. In logical circuits like arithmetic operations, arbitrary faults can cause errors in the result. However in safety critical applications, it is important to avoid those errors which would lead to system failures. Several approaches are known to protect the result of operations during software processing. In the same way like transmission systems, coded processing uses codes for fault detection. But in contrast to transmission systems, there is no adequate channel model available which makes it possible to evaluate the residue error probability of an arithmetic operation in an analytical way. This paper tries to close the gap of arithmetic error models by the development of a model for an ordinary addition in a computer system. Thus, the reliability of an addition's result can be analytically evaluated.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6625047,no
Modeling of 25 kV electric railway system for power quality studies,"25 kV, 50 Hz single-phase AC supply has been widely adopted in the long-distance electrified railway systems in many countries. Electrical locomotives generate harmonic currents in railway power supply systems. Single-phase traction loads also inject large unbalance currents to the transmission system and cause voltage unbalance subsequently. As the amount of rail traffic increases, the issue of power quality distortion becomes more critical. Harmonic currents and unbalanced voltages may cause negative effects on the components of the power system such as overheating, vibration and torque reduction of rotating machines, additional losses of lines and transformers, interference with communication systems, malfunctions of protection relays, measuring instrument error, etc. Therefore, the harmonic current flow must be assessed exactly in the designing and planning stage of the electric railway system (ERS). Harmonic current flow through the contact line system has to be accurately modeled to analyze and assess the harmonic effect on the transmission system. This paper describes the influence of electric railway system on power quality in 110 kV transmission system. Locomotives with diode rectifiers were analyzed. Electric railway system was modeled using EMTP-RV software. Currents and voltages were calculated in 110 kV and 25 kV network. Power quality measurements were performed on 110 kV level in 110/35/25 kV substation and analyzed according to IEC 61000-3-6.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6625081,no
Fail-safe and fail-operational systems safeguarded with coded processing,"Safety has the highest priority because it helps contribute to customer confidence and thereby ensures further growth of the new markets, like electromobility. Therefore in series production redundant hardware concepts like dual core microcontrollers running in lock-step-mode are used to reach for example ASIL D safety requirements given from the ISO 26262. Coded processing is capable of reducing redundancy in hardware by adding diverse redundancy in software, e.g. by specific coding of data and instructions. A system with two coded processing channels is considered. Both channels are active. When one channel fails, the service can be continued with the other channel. It is imaginable that the two channels with implemented coded processing are running with time redundancy on a single core or on a multi core system where for example different ASIL levels are partitioned on different cores. In this paper a redundancy concept based on coded processing will be taken into account. The improvement of the Mean Time To Failure by safeguarding the system with coded processing will be computed for fail-safe as well as for fail-operational systems. The use of the coded processing approach in safeguarding failsafe systems is proved.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6625234,no
An enhanced Jacobi method for lattice-reduction-aided MIMO detection,"Lattice reduction aided decoding has been successfully used for signal detection in multiinput and multioutput (MIMO) systems and many other wireless communication applications. In this paper, we propose a novel enhanced Jacobi (short as EJacobi) method for lattice basis reduction. To assess the performance of the new EJacobi method, we compared it with the LLL algorithm, a widely used algorithm in wireless communications. Our experimental results show that the EJacobi method is more efficient and produces better results measured by both orthogonality defect and condition number than the LLL algorithm.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6625293,no
Forensics of blurred images based on no-reference image quality assessment,"The inexpensive hardware and sophisticated image editing software tools have been widely used, which makes it easy to create and manipulate digital images. The detection of forgery images has attracted academic researches in recent years. In this paper, we proposed a forensic method to detect globally or locally blurred images using no-reference image quality assessment. The features are extracted from mean subtracted contrast normalized (MSCN) coefficients and fed to SVM, which can distinguish the tampered regions from the original ones and can quantify the tampered regions. Experimental results show that this method can detect the edges of tampered regions efficiently.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6625377,no
Optimal threshold characteristics of call admission control by considering cooperative behavior of users (loss model),"Call admission control (CAC) is an important technology for maintaining Quality of Service (QoS) in Software-Defined Networking (SDN). The most popular type of CACs is trunk reservation (TR) control, and many TR control methods had been proposed for improving evaluation values such as call blocking probability or the resource utilization rate. In general, some users under these TR control methods may behave cooperatively. However, conventional TR methods do not take into account a user's cooperative behavior when they begin to communicate. In this paper, we propose a novel TR-type CAC method by considering the cooperative behavior of some users. This proposed method is presented using the loss model of queueing theory for the call-level analysis of a single link. We also analyze the characteristics of the optimal control parameter.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6625468,no
Performance reliability simulation analysis for the complex mechanical system,"Large sample failure data for traditional probability statistics methods usually cannot be obtained from over-sized and complex mechanisms, which makes its reliability prediction very difficult. In this paper, the physics of failure technology is considered for the failure of products thoroughly. With computer simulation, the kinematic performance reliability of an aircraft cabin-door-lock mechanism is researched in this article. The example shows that the method combining physics of failure and software simulation method can solve the reliability problem of complex mechanical products effectively, which will bring guiding significance to engineering practice.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6625578,no
Implementing Nataf transformation in a spreadsheet environment and application in reliability analysis,"This paper presents a methodology for implementing Nataf transformation in a spreadsheet environment which is a widely used tool for generating correlated samples in reliability analysis. While the practicability of Nataf transformation has been demonstrated by numerous studies, this work emphasizes on the development of a systematic and user-friendly EXCEL Add-In. Combining with the recently developed subset simulation Add-In, it can be easily applied for reliability analysis problems with correlated random variables. A simple example is used to illustrate the performance of the proposed methodology.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6625628,no
Notice of Retraction<BR>Analysis of multi-body systems with revolute joint wear,"Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR>Clearances exist in all joints in multi-body system, but traditional dynamic analysis method often treats joints as ideal or perfect, ignoring joint clearance, which leads to a large error in the results. Joint clearance can cause structure vibration, bring some peaks to dynamic response, which is bad to the reliability of the system. And clearance also led to joint wear, it makes the clearance get larger with the increasing of working time. In a word, joint clearance will degenerate the performance of the multi-body system or even cause system failure. In this paper, a slider-crank mechanism is used for example, apply multi-body dynamics software to establish the mechanism simulation model, the joint with clearance was replaced by a contact unit, after calculate the contact force and the relative sliding velocity of the contact unit, applying Archard's Wear model to calculate wear depth of each segment in bushing circumference, and then update the geometry profile of bushing in the multi-body model. Through repeating the above process, the joint wear prediction and the dynamic response of the multi-body system after predetermined cycles can be obtained.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6625704,no
Fatigue life prediction for wind turbine main shaft bearings,"Taking the wind turbine main shaft bearing as the research object, the actual working status of the main shaft bearing under radial load and axial load is considered. Using the ANSYS software, the contact stress under different working conditions are analyzed, the dangerous position of main shaft bearing and the stress analysis results of dangerous position are determined. Based on the results of stress analysis, the main shaft bearing's S-N curve is obtained by changing the material's S-N curve. Considering the influence of average stress to fatigue damage, the average stress is corrected using the Goodman formula. According to the nominal stress approach and the fatigue damage cumulative rule, the fatigue life of the wind turbine main shaft bearing is predicted under combined action of different working conditions. The prediction result is 24.07 years, which meets with the requirement of 20-year design life of wind turbines.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6625711,no
Design of safety-critical software reliability demonstration test based on Bayesian theory,"The original software reliability demonstration test (SRDT) takes no consideration of prior knowledge and priori distribution adequately, which costs a lot of time and resource. A new improved Bayesian based SRDT method was proposed. First, a framework for SRDT scheme was constructed. According to the framework the decreasing function was employed to construct the priori distribution density functions for discrete and continuous safety-critical software respectively, and then discrete Bayesian software demonstration function (DBSDF) scheme and continuous Bayesian software demonstration function (CBSDF) scheme were presented. A set of comparative experiments have been carried out with the classic demonstration testing scheme on several published data sets. The experimental results reveal that both DBSDF scheme and CBSDF scheme are more efficient and applicable especially for the safety-critical software with high reliability requirements.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6625734,no
Malicious circuitry detection using transient power analysis for IC security,"Malicious modification of integrated circuits (ICs) in untrusted foundry, referred to as Hardware Trojan, has emerged as a serious security threat. Since it is extremely difficult to detect the presence of such Trojan circuits using conventional testing strategies, side-channel analysis has been considered as an alternative. In this paper, we proposed a non-destructive side-channel approach that characterizes and compares transient power signature using principle component analysis to achieve the hardware Trojan detection. The approach is validated with hardware measurement results using an FPGA-based test setup for large design including a 128-bit AES cipher. Experimental results show that this approach can discover small (<;1.1% area) Trojans under large noise and variations.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6625774,no
A method for software reliability test case design based on Markov chain usage model,"Test case design is a key factor to improve software reliability level. A new method for software reliability test case design based on Markov chain usage model is presented. Construction steps of software Markov chain usage Model based on UML are introduced. Markov chain usage model is described with directed graph. Auto generation arithmetic of reliability test case is proposed to ease the software reliability test in practice. Based on the method, an ATM software reliability test case is designed and demonstrated. The result proves the method in this paper is practical and efficient in engineering practices.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6625785,no
Research on safety assessment of gas environment in ammunition warehouse,"Because of the accumulation of harmful gas in the ammunition warehouse, the health of the privates and the long-term safety of ammunition storage would be affected. In order to assess the safety of the gas environment in ammunition warehouse, the software Fluent was applied to simulate the distribution rule of the harmful gas, and the area where the concentration of harmful gas is the highest in the warehouse was found. Then, a gas analysis system composed of sensor array and BP neural network was established to detect the harmful gas in the area where the concentration of harmful gas is the highest in the warehouse. If the concentration of harmful gas was greater than the extreme, the measures would be taken to guarantee the safety of the warehouse.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6625830,no
Notice of Retraction<BR>Assessment model for battle damage location process based on diagram entropy method,"Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR>The quality of battle damage location process (BDLP) is very important for the efficiency of damage location. The person's location ability can also be reacted by it. However, measuring complexity of BDLP is a rather new area of research with only a small number of contributions. So the complexity of BDLP is put forward to assess the BDLP's quality. The diagram entropy model which applied on the program maintenance complexity in software engineering is introduced to quantify the complexity of BDLP. Both the method of generating information structure diagram (ISG) and action control diagram (ACG) are given. To validate the relationship between the complexity value and the locating time which is an important factor of BDLP's quality, a non-linear cure fitting is introduced, and they are consistent correlative proved by the result. An instance is given to explain this method.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6625890,no
The application of the MODHMS in contaminant transport simulation,"With the development of the economy and rapid population growth, the range of contaminated groundwater is getting bigger and bigger. Groundwater pollution has a slow process, and it is not easy to find and difficult to control. Once polluted, it takes more than ten years, sometimes even decades to recover. Groundwater pollution has a close relationship with people's health. This paper mainly discusses the range of contaminant transport in different time of groundwater in the MODHMS model. MODHMS is a fully integrated surface/groundwater flow code. The Guishui River is selected as the study area, and the major involved research methods and key technologies are discussed briefly. Analysis shows that the MODHMS is an effective tool to simulate the contaminant transport of groundwater; it provides a more feasible idea to predict and control the groundwater quality.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6626050,no
DC technology for stable and reliable grids,"The expansion of cities often leads to enlarged load concentration and carries extra challenges to managing the electrical grid while insuring power availability to critical loads. The developing Shanghai power grid faces many challenges in the areas of generation, transmission, and distribution of the rapidly growing amounts of electrical energy in demand. Its development should address issues of congestion, voltage-stability, power restriction, permits, and scarcity of land or right of way. In this paper, an equivalent model of Shanghai power grid is implemented using the ABB power transient software analyzer (SIMPOW) where detailed modeling of the generators including exciters and governors is performed. Solutions including Fault Current Limiters (FCL), Static Var Compensators Light (SVC-Light) and High Voltage Direct Current (HVDC) systems were analyzed to assess their effectiveness in addressing the grid issues stated above. The best solution was obtained when the individual AC subsystems were decoupled so a fault in a given subsystem is not propagated to another subsystem and short-circuit currents are limited only by local generation capacity. This solution was obtained using a DC ring where power sources and loads are connected to a common DC bus through Voltage Source Converters (VSC).",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6626393,no
Software Modification Aided Transient Error Tolerance for Embedded Systems,"Commercial off-the-shelf (COTS) components are increasingly being employed in embedded systems due to their high performance at low cost. With emerging reliability requirements, design of these components using traditional hardware redundancy incur large overheads, time-demanding re-design and validation. To reduce the design time with shorter time-to-market requirements, software-only reliable design techniques can provide with an effective and low-cost alternative. This paper presents a novel, architecture-independent software modification tool, SMART (Software Modification Aided transient eRror Tolerance) for effective error detection and tolerance. To detect transient errors in processor data path, control flow and memory at reasonable system overheads, the tool incorporates selective and non-intrusive data duplication and dynamic signature comparison. Also, to mitigate the impact of the detected errors, it facilitates further software modification implementing software-based check-pointing. Due to automatic software based source-to-source modification tailored to a given reliability requirement, the tool requires no re-design effort, hardware- or compiler-level intervention. We evaluate the effectiveness of the tool using a Xentium processor based system as a case study of COTS based systems. Using various benchmark applications with single-event upset (SEUs) based error model, we show that up to 91% of the errors can be detected or masked with reasonable performance, energy and memory footprint overheads.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6628280,no
Data Flow Analysis of Software Executed by Unreliable Hardware,"The data flow is a crucial part of software execution in recent applications. It depends on the concrete implementation of the realized algorithm and it influences the correctness of a result in case of hardware faults during the calculation. In logical circuits, like arithmetic operations in a processor system, arbitrary faults become a more tremendous aspect in future. With modern manufacturing processes, the probability of such faults will increase and the result of a software's data flow will be more vulnerable. This paper shows a principle evaluation method for the reliability of a software's data flow with arbitrary soft errors also with the concept of fault compensation. This evaluation is discussed by means of a simple example based on an addition.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6628283,no
Automatic Hard Block Inference on FPGAs,"Modern FPGAs often provide a number of highly optimized hard IP blocks with certain functionalities. However, manually instantiating these blocks is both time-consuming and error-prone, in particular, if only a part of the functionality of the IP block is used. To solve this problem, we developed an algorithm to automatically replace a selected combinational subset of a hardware design with a correct instantiation of a given IP block. Both the IP block and the part of the hardware circuit to be replaced are specified using arithmetic and Boolean operators. Our method is based on higher-order E-unification with an equational theory of arithmetic and Boolean laws. To demonstrate the effectiveness and efficiency of our approach, we present preliminary experiments with various circuits.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6628326,no
Power and Thermal Fault Effect Exploration Framework for Reader/Smart Card Designs,"Power consumption and thermal behavior are important characteristics that need to be explored and evaluated during a product's development cycle. If not handled properly, the consequences are, for example, increased mean-time-to-failure and fatal timing variations of the critical path. In the field of contactlessly powered reader/smart card systems, a magnetic field strength exceeding the allowed maximum threshold may harm the smart card's hardware. Thus, secure smart cards must be designed to cope with faults provoked by power oversupply and thermal stress. Proper fault detection and fault handling are imperative tasks to protect internal secrets. However, state-of-the-art design exploration tools cover these smart card specific power and thermal stress issues only to some extent. Here we present an innovative high level simulation approach used for exploring and simulating secure reader/smart card systems, focusing on magnetic field oversupply and thermal stress evaluations. Gate-level-based power models are used besides RF-channel models, thermal models, and thermal effect models. Furthermore, fault injection techniques are featured to evaluate the fault resistance of a smart card system's software implementation. This framework grants software and hardware designers a novel opportunity to detect functional, power, thermal, and security issues during the design time. We demonstrate the usage of our exploration framework and show an innovative hardware design approach to prolong the lifetime of smart card electronics, which are exposed to high magnetic field strengths.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6628374,no
Real Time Camera Phone Guidance for Compliant Document Image Acquisition without Sight,"Here we present an evaluation of an ideal document acquisition guidance system. Guidance is provided to help someone take a picture of a document capable of Optical Character Recognition (OCR). Our method infers the pose of the camera by detecting a pattern of fiduciary markers on a printed page. The guidance system offers a corrective trajectory based on the current pose, by optimizing the requirements for complete OCR. We evaluate the effectiveness of our software by measuring the quality of the image captured when we vary the experimental setting. After completing a user study with eight participants, we found that our guidance system is effective at helping the user position the phone in such a way that a compliant image is captured. This is based on an evaluation of a one way analysis of variance comparing the percentage of successful trials in each experimental setting. Negative Helmert Contrast is applied in order to tolerate only one ordering of experimental settings: no guidance (control), just confirmation, and full guidance.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6628654,no
Document Authentication Using Printing Technique Features and Unsupervised Anomaly Detection,"Automatically identifying that a certain page in a set of documents is printed with a different printer than the rest of the documents can give an important clue for a possible forgery attempt. Different printers vary in their produced printing quality, which is especially noticeable at the edges of printed characters. In this paper, a system using the difference in edge roughness to distinguish laser printed ages from inkjet printed pages is presented. Several feature extraction methods have been developed and evaluated for that purpose. In contrast to previous work, this system uses unsupervised anomaly detection to detect documents printed by a different printing technique than the majority of the documents among a set. This approach has the advantage that no prior training using genuine documents has to be done. Furthermore, we created a dataset featuring 1200 document images from different domains (invoices, contracts, scientific papers) printed by 7 different inkjet and 13 laser printers. Results show that the presented feature extraction method achieves the best outlier rank score in comparison to state-of-the-art features.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6628667,no
Detecting OOV Names in Arabic Handwritten Data,"This paper presents a novel approach to detect Arabic OOV names from OCR'ed handwritten documents. In our approach, OOV names are searched for using approximate string match on character consensus networks (cnets). The retrieved regions are re-ranked using novel features representing the quality of the match and the likelihood of the detected region to be an OOV name. Our features that encode word boundary information into the approximate match algorithm significantly improve mean average precision (MAP) by 12.2% (absolute gains) for rank cut-off 100 (48.2% vs. 36.0%) and 11.9% for cut-off 1000 (47.0% vs. 35.1%) over the baseline system. Discriminative reranking based on maximum entropy classification using novel features, such as the probability of a retrieved region being an OOV name (called OOV name probability) from a conditional random field model, further improve MAP by 2.3% (absolute gains) for cut-off 100 and 3.0% for cut-off 1000. The improvements are consistent in DET (Detection Error Tradeoff) curves. Our results show that character cnet based OOV name search benefits clearly from the approximate match using word boundary information and the reranking algorithm. Our experiments also show that OOV name probability is very useful for reranking.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6628765,no
A New Method for Discriminating Printers Based on Contours Qualities of Printed Characters Using Wavelet Decomposition,"This article described a new method for discriminating models of laser printers by means of their printed characters, in particular details in contours of characters. A method we proposed was based on evaluating qualities in contours of printed characters using wavelet decomposition. Recently most of characters printed by laser printers were originally stored in printers or computers as vector outline such as Bezier or Spline. Raster Image Processor (RIP) implemented as hardware or software in printers or computers rasterized the outline into a pattern which was composed of subtle vast dots. There was a variety of types in contours of printed characters which were printed by each printer model. In Japan, stalkers typed their threatening letters using common used fonts such as MS Mincho in Japanese and Times or Century in English. Even though the kinds of fonts were known, there was not evidence since these fonts were equipped in almost all computers in Japan. Therefore a new method to discriminate models of printers was desired. Even though same too common fonts were used in threatening letters, subtle differences among contours of printed characters were observed since there was a variety of methods which each maker adopted in rendering and screening which converted from outline to pattern of dots. In order to detect the subtle differences among contours of printed documents, the article utilized wavelet decomposition and a high resolution i.e., 5400dpi flat bed image scanner. The article also used a simple method to analyze results of wavelet decomposition which was counting numbers of zero-crossing points at each scale of decomposition. The results of the experiment showed that the method we proposed was able to detect differences among models of laser printers even though using same too common fonts both Japanese and English.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6628787,no
Fault detection for vehicular ad-hoc wireless networks,"An increasing number of intelligent transportation applications require robust and reliable wireless communication. To achieve the required quality of service it is necessary to implement redundancy in the critical path which includes the radio software and hardware. In a real-world application there are many things that can cause the communication between two vehicles to degrade or stop completely. This paper describes a novel technique for detecting degradation or failure of communication links by comparing the performance of the radios to a probabilistic model built using data collected in the field. The results show that this techinique can successfully detect when there is partial or complete failure to communicate due to damage to the external components such as antennas, connectors and cables.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6629485,no
"An FPGA-based coded excitation system for ultrasonic imaging using a second-order, one-bit sigma-delta modulator","Coded excitation and pulse compression techniques have been used to improve the echo signal-to-noise ratio (eSNR) in ultrasonic imaging. However, most hardware use phase modulated codes over frequency modulated codes because of ease of implementation. In this study, a technique that converts non-binary frequency modulated codes into binary frequency modulated codes is evaluated. To convert from a non-binary to a binary code, a second-order, one-bit sigma delta modulator is used. This sigma-delta modulated code is generated in MATLAB which is then stored in a double data rate synchronous dynamic random access memory. A field programmable gate array, which has access to the memory device, transmits the binary waveform and is recorded using an oscilloscope. The recorded data is then filtered using the pulse-echo transducer model of a linear array with a center frequency of 8.4 MHz and a fractional bandwidth of 100% at -6 dB. Pulse compression was then performed using a matched filter, a mismatched filter, and a Wiener filter. Image quality metrics, such as modulation transfer function and sidelobe-to-mainlobe ratio, were used to assess compression performance. Overall, echoes compressed when the excitation is the sigma-delta modulated coded waveform resulted in no measurable difference in axial resolution.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6632667,no
An application of improved synchronous reference frame-based voltage sag detection in voltage sag compensation system,"This paper proposes an application of improved synchronous reference frame (ISRF)-based voltage sag detection in voltage sag compensation system. The ISRF-based voltage sag detection presents the fast detection time that is suitable to use in voltage sag compensation system. The proposed voltage sag compensation system consists of ISRF-based voltage sag detector, static transfer switches (STSs), and alternative voltage source using the inverter. In a normal incident, the load fed the power from main voltage source via the main STS. When the voltage sag occurs and it is detected by the voltage sag detector, the alternative STS connects the load to an alternative voltage source instead of the main STS. The computer software simulation and the experiment were made to investigate and verify the operation of proposed voltage sag compensation system. It can be seen from the results that a short voltage sag detection time is obtained and voltage sag can be compensated with the proposed system.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634734,no
RECOG: A Sensing-Based Cognitive Radio System with Real-Time Application Support,"While conventional cognitive radio (CR) system is striving at providing best possible protections for the usage of primary users (PU), little attention has been given to ensure the quality of service (QoS) of applications of secondary users (SU). When loading real-time applications over such a CR system, we have found that existing spectrum sensing schemes create a major hurdle for real-time traffic delivery of SU. For example, energy detection based sensing, a widely used technique, requires possibly more than 100 ms to detect a PU with weak signals. The delay is intolerable for real-time applications with stringent QoS requirements, such as voice over internet protocol (VoIP) or live video chat. This delay, along with other delays caused by backup channel searching, channel switching, and possible buffer overflow due to the insertion of sensing periods, makes supporting real-time applications over CR system very difficult if not impossible. In this paper, we present the design and implementation of a sensing-based CR system - RECOG, which is able to support realtime communications among SUs. We first redesign the conventional sensing scheme. Without increasing the complexity or trading off the detection performance, we break down a long sensing period into a series of shorter blocks, turning a disruptive long delay into negligible short delays. To enhance the sensing capability as well as better protect the QoS of SU traffic, we also incorporate an on-demand sensing scheme based on MAC layer information. In addition, to ensure a fast and reliable switching when PU returns, we integrate an efficient backup channel scanning and searching component in our system. Finally, to overcome a potential buffer overflow, we propose a CR-aware QoS manager. Our extensive experimental evaluations validate that RECOG can not only support realtime traffic among SUs with high quality, but also improve protections for PUs.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6635262,no
Simulation on quantitative analysis of crack inspection by using eddy current stimulated thermography,"Eddy current (EC) stimulated thermography has been proven to be an emerging integrative nondestructive approach for detecting and characterizing surface and subsurface cracks. In this paper, numerical simulation study has been conducted to understand EC stimulated thermography for defect inspection on metallic sample. It has been investigated that transient EC distribution and heating propagation for cracks with different lengths and depths. The simulations are carried out by using AC/DC module of COMSOL mul-tiphysics software. Image processing technique is proposed to analyze the thermal images obtained during the heating and cooling period of the inspection process. The proposed approach is proved to be capable of tracking the heat diffusion by processing the images sequentially. Understanding of transient EC distribution and heating propagation is the fundamental of quantitative nondestructive evaluation of crack inspection with EC stimulated thermography.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6635529,no
Plate components ultrasonic guide wave detection based on the transducer array,"Aluminum plate components has been widely used in the practical industrial applications, because of the plate's defects, a lot of major accidents occurred and caused significant economic loss. Therefore, based on the fundamental theory of the guide wave that propagating in the aluminum plate, and by solving and analyzing the guide wave's dispersion, the guide wave's inspiring method have been obtained from the wave-mode conversion. Using the wavelet to process the testing signal, and introducing the ellipse localization imaging algorithm to identify the defect's orientation, the defect's localization and orientation can be detected accurately. Based on the theory and technique previous statement, a multi-channel ultrasonic transducer array detecting system including hardware and software has been established, and a series of experiments have been done. The results shows that: the multi-channel ultrasonic detecting system established has a high detection precision in defect's localization and orientation detecting, which is up to 98%.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6635535,no
Discrimination of stator winding turn fault and unbalanced supply voltage in permanent magnet synchronous motor using ANN,"Permanent magnet synchronous motor (PMSM) is currently the most attractive application electric machine for several industrial applications. It has obtained widespread application in motor drives in recent time. However, different types of faults are unavoidable in such motors. This paper focuses on stator winding faults diagnosis. This paper proposes the ratio of third harmonic to fundamental FFT magnitude component of the three-phase stator line current and supply voltage as a parameter for detecting stator winding turn faults under different load conditions and using artificial neural network (ANN). Discrimination among unbalancing of supply voltage conditions and stator turn short circuit poses a challenge that is addressed in this paper. The presented approach yields a high degree of accuracy in fault detection and diagnosis between the effects of stator winding turn fault and those due to unbalanced supply voltages using artificial neural network. All simulations in this paper are conducted using finite element analysis software.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6635722,no
Towards QoS prediction based on composition structure analysis and probabilistic environment models,"Complex software systems are usually built by composing numerous components, including external services. The quality of service (QoS) is essential for determining the usability of such systems, and depends both on the structure of the composition and on the QoS of its components. Since the QoS of each component is usually determined with uncertainty and varies from one invocation to another, the composite system also exhibits stochastic QoS behavior. We propose an approach for computing probability distributions of the composite system QoS attributes based on known probability distributions of the component QoS attributes and the composition structure. The approach is experimentally evaluated using a prototype analyzer tool and a real-world service-based example, by comparing the predicted probability distributions for the composition QoS with the actual distribution of QoS values from repeated actual executions.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6635972,no
Framework for evaluating reusability of Component-as-a-Service (CaaS),"As a form of service, Component-as-a-Service (CaaS) provides a reusable functionality which is subscribed by and integrated into service-based applications. Hence, the reusability of CaaS is a key factor for its value. This paper proposes a comprehensive reusability evaluation framework for CaaS. We derive a set of CaaS reusability attributes by applying a logical and objective process, and define metrics for key attributes with the focuses on theoretical soundness and practical applicability. The proposed reusability evaluation suite is assessed with a case study.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6635976,no
Research on QoS Reliability Prediction in Web Service Composition,"Because of the flexibility of Web Services and dynamical network, QoS is difficult to be assured of reliability, often causing the Web Service selected and invoked by users are often not working properly, not result in high performance Web service composition. In the composition of services for service selection, in order to improve the reliability and performance of composite services need to consider the services of non-functional factors, the paper apply of knowledge of probability and statistics predicting Web service's dynamic QoS property values, propose a objective evaluation of the credibility of Web Service and a improved K-MCSP QoS global optimization algorithm of service composition for improving the reliability of service composition.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6636410,no
A Conceptual Approach for Assessing SOA Design Defects' Impact on Quality Attributes,"This research proposes an approach for assessing the impacts of SOA design defects on SOA quality attributes. Eleven items were selected to measure SOA Design Defects, fourteen items were selected to measure SOA Design Attributes, seventeen items were selected to measure SOA Quality Attributes and eleven items were selected to measure SOA Quality Metrics. This work is an integrated part to previous studies in the field.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6636421,no
Comparison of different materials for manufacturing of antialiasing LP filter,"This paper deals with design, simulation, manufacturing and experimental testing of antialiasing low pass (LP) filter for I - Q demodulator that is a part of Ultra Wide-Band (UWB) sensor system. It focuses on various technological possibilities (mechanical by CNC drilling and chemical by etching) of production of printed circuit boards (PCB) for antialiasing filters that are used in the branch of UWB area. Paper demonstrates realization of LP filter designed in the Filter Solution 2011 software from Nuhertz as well as its electromagnetic (EM) simulation made by Ansoft Designer software from ANSYS Company. It assesses the suitability of hydrocarbon ceramic laminate RO4003C and epoxy-glass laminate FR4 for production of LP filter suitable for high frequency (HF) area. Paper refers to the needs for focusing on the design and construction of passive filters used in I - Q demodulator systems. Emphasis lay on issues of quality of transmitted signals in the HF range. It examines the potential manufacturing possibility of such filter based on Low Temperature Ceramics (LTCC) technology. There are presented simulated and measured results of insertion loss (S<sub>21</sub>) and return loss (S<sub>11</sub>) of LP filter for I - Q demodulator made from RO4003C and FR4 substrate. The presented filters should be used as an antialiasing LP filter mean for I - Q demodulator presented in [1] which is a part of evaluated UWB sensor system.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6636522,no
Can requirements dependency network be used as early indicator of software integration bugs?,"Complexity cohesion and coupling have been recognized as prominent indicators for software quality. One characterization of software complexity is the existence of dependency relationship. Moreover, degree of dependency reflects the cohesion and coupling between software elements. Dependencies on design and implementation phase have been proven as important predictors for software bugs. We empirically investigated how requirements dependencies correlate with and predict software integration bugs, which can provide early estimate regarding software quality, therefore facilitate decision making early in the software lifecycle. We conducted network analysis on requirements dependency networks of two commercial software projects. We then performed correlation analysis between network measures (e.g., degree, closeness) and number of bugs. Afterwards, bug prediction models were built using these network measures. Significant correlation is observed between most of our network measures and number of bugs. These network measures can predict the number of bugs with high accuracy and sensitivity. We further identified the significant predictors for bug prediction. Besides, the indication effect of network measures on bug number varies among different types of requirements dependency. These observations show that requirements dependency network can be used as an early indicator of software Integration bugs.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6636718,no
An empirical study on project-specific traceability strategies,"Effective requirements traceability supports practitioners in reaching higher project maturity and better product quality. Researchers argue that effective traceability barely happens by chance or through ad-hoc efforts and that traceability should be explicitly defined upfront. However, in a previous study we found that practitioners rarely follow explicit traceability strategies. We were interested in the reason for this discrepancy. Are practitioners able to reach effective traceability without an explicit definition? More specifically, how suitable is requirements traceability that is not strategically planned in supporting a project's development process. Our interview study involved practitioners from 17 companies. These practitioners were familiar with the development process, the existing traceability and the goals of the project they reported about. For each project, we first modeled a traceability strategy based on the gathered information. Second, we examined and modeled the applied software engineering processes of each project. Thereby, we focused on executed tasks, involved actors, and pursued goals. Finally, we analyzed the quality and suitability of a project's traceability strategy. We report common problems across the analyzed traceability strategies and their possible causes. The overall quality and mismatch of analyzed traceability suggests that an upfront-defined traceability strategy is indeed required. Furthermore, we show that the decision for or against traceability relations between artifacts requires a detailed understanding of the project's engineering process and goals; emphasizing the need for a goal-oriented procedure to assess existing and define new traceability strategies.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6636719,no
Improving recovery probability of mobile hosts using secure checkpointing,"In this work, we have proposed a mobility based secure checkpointing and log based rollback recovery technique to provide fault tolerance to mobile hosts in infrastructured wireless/mobile computing system, like, wireless cellular network. Mobility based checkpointing limits number of scattered checkpoint or logs in different mobile support stations that remain due to movement of mobile hosts. Secure checkpointing using low overhead elliptic curve cryptography ensures protection of checkpoint against security attack in both nodes and links and restricts access to checkpoint content only to the mobile host that is owner of the checkpoint. Log based rollback recovery ensures optimized recovery from last event using determinants saved in logs. Security attack to checkpoints leads to unsuccessful recovery. In case of security attack to checkpoint, recovery probability of a failed mobile host using secure checkpointing technique is 1 whereas that in checkpointing without security technique is <;1.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6637310,no
Performance evaluation of ip wireless networks using two way active measurement protocol,"With the advent of different kinds of wireless networks and smart phones, Cellular network users are provided with various data connectivity options by Network Service Providers (ISPs) abiding to Service Level Agreement, i.e. regarding to Quality of Service (QoS) of network deployed. Network Performance Metrics (NPMs) are needed to measure the network performance and guarantee the QoS Parameters like Availability, delivery, latency, bandwidth, etc. Two way active measurement protocol (TWAMP) is widely prevalent active measurement approach to measure two-way metrics of networks. In this work, software tool is developed, that enables network user to assess the network performance. There is dearth of tools, which can measure the network performance of wireless networks like Wi-Fi, 3G, etc., Therefore proprietary TWAMP implementation for IPv6 wireless networks on Android platform and indigenous driver development to obtain send/receive timestamps of packets, is proposed, to obtain metrics namely Round-trip delay, Two-way packet Loss, Jitter, Packet Reordering, Packet Duplication and Loss-patterns etc. Analysis of aforementioned metrics indicate QoS of the wireless network under concern and give hints to applications of varying QoS profiles like VOIP, video streaming, etc. to be run at that instant of time or not.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6637471,no
An investigation into aliasing in images recaptured from an LCD monitor using a digital camera,"With current technology, high quality recaptured images can be created from soft displays, such as an LCD monitor, using a digital still camera and professional image editing software. The task of verifying the ownership and past history of an image is, consequently, more difficult. One approach to detecting an image that has been recaptured from an LCD monitor is to search for the presence of aliasing due to the sampling of the monitor pixel grid. To validate this approach, an investigation into the aliasing introduced in a digitally recaptured image is conducted. An anti-forensic method for recapturing images that are free from aliasing is developed using a model of the image acquisition process. This is supported by a simulation of the acquisition process and illustrated with examples of recaptured images that are free from aliasing.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6638053,no
Feedwater heater system fault diagnosis during dynamic transient process based on two-stage neural networks,"At present, researches on power plant fault diagnosis are mostly for steady-state work conditions and can not well adapt to the load-changing dynamic process, which greatly limits the practical application of a fault diagnosis system. Thus, a transient fault diagnosis approach based on two-stage neural networks was put forward for power plant thermal system fault diagnosis. An Elman recurrent neural network with time-delay inputs was applied to predict the expected normal values of the fault feature variables, and a BP neural network was used to identify the fault types. To improve the diagnostic effect for faults of varying severity under transient conditions, fault symptom zoom optimization technique was also used. Taking the high-pressure feedwater heater system of a 600MW supercritical power unit as the object investigated, the predictive model was built, trained and validated with large amount of historical operating data. The BP network fault diagnosis model was trained with the fault fuzzy knowledge library including typical fault samples. The real-time fault diagnosis program was then developed with MATLAB software. By communicating with the power plant simulator, intensive fault diagnosis tests were carried out. It was shown the suggested method can achieve good diagnosis results for the power plant thermal system under load-varying transient process.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6640515,no
An automated ontology generation technique for an emergent behavior detection system,"Due to the lack of central control in distributed systems, design and implementation of such systems is a challenging task. Interaction of multiple autonomous components can easily result in unwanted behavior in the system. Therefore it is vital to carefully review the design of distributed systems. Manual review of software documents is too inefficient and error prone. It would therefore be beneficial to have a systematic methodology to automatically analyze software requirements and design documents. However automating the process of software analysis is a challenging task because besides the design know-how, each software system requires its own domain knowledge. Existing approaches often require a great deal of input from system engineers familiar with the domain. Such information needs to be interpreted by the designer which is a time-consuming and error prone process. This research suggests the use of a scenario-based approach to represent system requirements. Scenarios are often depicted using message sequence charts (MSCs). Due to their formal notation, MSCs can be used to analyze software requirements in a systematic manner. In an earlier paper, it was demonstrated that ontologies can be used to effectively automate the construction of domain knowledge for the system. However the construction of ontologies remained a challenging task. This paper describes a process which infers ontology from the provided message sequence charts. Furthermore this paper introduces a software tool which automates the process of domain ontology construction. This methodology is demonstrated using a case study of a fleet-management software system.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6642496,no
Multi-operator Image Retargeting Based on Automatic Quality Assessment,"Image retargeting aims to avoid visual distortion while retaining important image content in resizing. However, no single image retargeting method can handle all cases. In this paper, we propose a novel multi-operator image retargeting approach, which utilizes an efficient and human perception based automatic quality assessment in operator selection. First, we calculate the importance map and distortion map for quality assessment. Then, we construct the resizing space and assess the performance of each operator in iterative width and/or height reduction. Finally, we select the optimal operator sequence by dynamic programming and generate the target image. Experiments demonstrate the effectiveness of the proposed approach.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6643710,no
Ontology of architectural decisions supporting ATAM based assessment of SOA architectures,"Nowadays, Service Oriented Architecture (SOA) might be treated as a state of the art approach to the design and implementation of enterprise software. Contemporary software developed according to SOA paradigm is a complex structure, often integrating various platforms, technologies, products and design patterns. Hence, it arises a problem of early evaluation of a software architecture to detect design flaws that might compromise expected system qualities. Such assessment requires extensive knowledge gathering information on various types of architectural decisions, their relations and influences on quality attributes. In this paper we describe SOAROAD (SOA Related Ontology of Architectural Decisions), which was developed to support the evaluation of architectures of information systems using SOA technologies. The main goal of the ontology is to provide constructs for documenting SOA. However, it is designed to support future reasoning about architecture quality and for building a common knowledge base. When building the ontology we focused on the requirements of Architecture Tradeoff Analysis Method (ATAM) which was chosen as a reference methodology of architecture evaluation.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6644014,no
Object-oriented approach to Timed Colored Petri Net simulation,This paper presents object-oriented design of library meant for modeling and simulating Timed Colored Petri Net models. The approach is prepared to integrate TCPN models with crucial parts of larger applications implemented in object-oriented languages. The formal models can be tightly joined with applications allowing the latter to interpret states of the formal model in their domain of responsibility. This approach allows less error-prone and more pervasive use of formal methods to improve quality of software created with imperative languages.,2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6644200,no
Safety analysis of Autonomous Ground Vehicle optical systems: Bayesian belief networks approach,"Autonomous Ground Vehicles (AGV) require diverse sensor systems to support the navigation and sense-and-avoid tasks. Two of these systems are discussed in the paper: dual camera-based computer vision (CV) and laser-based detection and ranging (LIDAR). Reliable operation of these optical systems is critical to safety since potential faults or failures could result in mishaps leading to loss of life and property. The paper identifies basic hazards and, using fault tree analysis, the causes and effects of these hazards as related to LIDAR and CV systems. A Bayesian Belief Network approach (BN) supported by automated tool is subsequently used to obtain quantitative probabilistic estimation of system safety.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6644203,no
Design and implementation of a frequency-aware wireless video communication system,"In an orthogonal frequency division multiplexing (OFDM) communication system, data bits carried by each subcarrier are not delivered at an equal error probability due to the effect of multipath fading. The effect can be exploited to provide unequal error protections (UEP) to wireless data by carefully mapping bits into subcarriers. Previous works have shown that this frequency-aware approach can improve the throughput of wireless data delivery significantly over conventional frequency-oblivious approaches. We are inspired to explore the frequency-aware approach to improve the quality of wireless streaming, where video frames are naturally not of equal importance. In this work, we present FAVICS, a Frequency-Aware Video Communication System. In particular, we propose three techniques in FAVICS to harvest the frequency-diversity gain. First, FAVICS employs a searching algorithm to identify and provide reliable subcarrier information from a receiver to the transmitter. It effectively reduces the channel feedback overhead and decreases the network latency. Second, FAVICS uses a series of special bit manipulations at the MAC layer to counter the effects that alter the bits-to-subcarrier mapping at the PHY layer. In this way, FAVICS does not require any modifications to wireless PHY and can benefit existing wireless systems immediately. Third, FAVICS adopts a greedy algorithm to jointly deal with channel dynamics and frequency diversity, and thus can further improve the system performance. We prototype an end-to-end system on a software defined radio (SDR) platform that can stream video real-time over wireless medium. Our extensive experiments across a range of wireless scenarios demonstrate that FAVICS can improve the PSNR of video streaming by 5~10 dB.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6645012,no
Sub-carrier Switch Off in OFDM-based wireless local area networks,"OFDM based wireless communication systems split the available frequency band into so-called sub-carriers, and data is transmitted on each of these sub-carriers in parallel. With frequency selective fading, sub-carriers may experience different channel qualities. Thus, choosing a different modulation and coding scheme (MCS) per sub-carrier improves performance. However, this comes at an increase in transceiver complexity and no current wireless system adapts the MCS at such a fine granularity. Some OFDMA based systems such as LTE allow to adapt the MCS per user, whereas wireless local area networks as specified by IEEE 802.11 use the same MCS on every sub-carrier. The performance of such wireless systems that use a single MCS in a frequency selective fading channel can be significantly improved through Sub-Carrier Switch Off (SSO), a simple but powerful alternative to adaptive MCS. SSO deactivates weak sub-carriers that excessively raise the error probability to improve the overall throughput. In this paper, we implement and test SSO in a software-defined radio testbed based on the Wireless Open Access Research Platform (WARP). We present a novel light-weight method for selecting the sub-carriers to be switched off based on the per-sub-carrier channel quality. The results we obtain from our measurements indicate that throughput increases of up to 250% are possible and thus SSO is a highly promising and very low complexity mechanism for future wireless local area networks.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6645015,no
Using cloud computing to enhance automatic test equipment testing and maintenance capabilities,"The purpose of this paper is to present a conceptual approach and to make practical recommendations on how to improve the current Automatic Test Equipment (ATE) testing and maintenance capabilities by utilizing the existing cloud computing model to build a globally linked ATE maintenance system. The basic tenet of the ATE community is to support a multi-tiered maintenance concept which, in general, is a three tiered system that is composed of organizational maintenance (O-level), intermediate maintenance (I-level), and depot maintenance (D-level) organizations. The goal of the ATE is to (1) quickly and accurately detect and isolate each fault, (2) provide software tools for analyzing historical data, and (3) gather, manage, and distribute accurate and reliable maintenance information for the failed Unit Under Test (UUT). The ATE system should provide services that will (1) maintain a repository of information that will improve fault detection and isolation, allow for off-platform assessments, document failures, and help quantify corrective actions, (2) reduce false UUT pulls, and (3) reduce repair time by prompting repair procedures. Furthermore, the ATE system should provide additional services that will help optimize the time to diagnose problems by using collected failure information and by recommending entry points into the Test Program Set (TPS) software. It should also present information to the ATE maintainer to aid in informed repair decisions which could be in the form of pilot debrief results, platform Built In Test (BIT) results, O-level test outcomes and corrective actions, and maintenance and usage history of the platform and UUT. So, based on this definition of ATE maintenance the use of cloud computing can be used to provide services to improve the overall ATE testing throughput which will result in bottom line improvements to ATE life cycle costs. By using cloud computing, which is defined to be a model for enabling ubiquitous, convenient, - n-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction, users can develop cloud computing models that will provide access to application software and databases that can be used to build a globally linked ATE maintenance system. This paper will discuss the essential characteristics of the cloud computing models and define the various flavors of cloud offerings available to designers today. This paper will also analyze the cloud computing model to arrive at a conceptual approach that can be used to enhance the current ATE Testing and Maintenance capabilities. Practical recommendations will be discussed on how to transform the current ATE Testing and Maintenance capabilities into the specific cloud computing model offerings in order to help configure a globally linked ATE maintenance system.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6645048,no
What are we able to do with test data or using LabVIEW to hone in on actual cause of failures,"As systems/circuits degrade or high failure performance trends occur over time, there is an increased probability of predicting with reasonable confidence, when a given assembly or component is likely to experience an insipient fault or a cause a mission failure. Also, performance trends can be applied to algorithms to enhance the testing process. However, profound predictions can be made accurately which are based on true test data. Using LabVIEW is an excellent way to process actual failure history and to display the results.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6645054,no
Testability verification based on sequential probability ratio test method,"Testability plays an important role in the readiness of equipment as a good design for testability (DFT) can greatly decrease the fault detection and isolation time, which will accelerate the maintenance actions. Testability verification is a procedure to check that whether the testability indexes such as fault detection rate (FDR) and fault isolation rate (FIR) meet the requirement in the contract. Currently, standards and statistical methods used in testability verification have the problems such as large sample, long period and so on. Sequential probability ratio test (SPRT) method can decrease the test sample size with almost a same operation characteristic as classical method based on binomial distribution. SPRT method and its truncated rules are introduced and the spectrum of expected test number is proposed. Then, the sample size allocation method and failure mode selection method based on failure rate used in sequential testability verification are illustrated. Testability verification of a control system is implemented with the given method and steps. Software named testability demonstration and evaluation system (TDES) which can calculate the decision criteria, plot decision chart, select failure mode and make judgment is used in the test as assistance. The result shows that the test sample size is remarkably decreased while comparing with the classical method.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6645066,no
Reducing test program costs through ATML-based requirements conversion and code generation,"Most military and aerospace organizations maintain their test requirements as paper-like forms stored electronically. When test programs need to be created or modified, these documents are often manually referenced, which can be an inefficient and error-prone process. Additionally, because modifications to test program code are sometimes made without updating the corresponding requirements, implementation and documentation tend to diverge as projects evolve, which has an adverse effect on the long-term maintainability of Test Program Sets (TPSs). In the past, the lack of an industry-standard data format for test requirements has imposed limitations on the traceability between test results and test specifications. Previous attempts at automating the conversion of analog and mixed-signal test requirements into test programs produced proprietary solutions with limited adoption. In this paper, we describe an innovative process in which multiple software applications interact through a standard XML format that conforms to IEEE Std 1671.1 Automatic Test Markup Language (ATML) Test Description. The process uses automated test data conversion and code generation to facilitate the initial creation and long-term maintenance of test programs.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6645068,no
Multi-stratum resource integration for OpenFlow-based data center interconnect [invited],"Nowadays, most service providers offer their services and support their applications through federated sets of data centers that need to be interconnected using high-capacity telecom transport networks. To provide such high-capacity network channels, data center interconnection is typically based on IP and optical transport networks that ensure certain end-to-end connectivity performance guarantees. However, in the current mode of operation, the control of IP networks, optical networks, and data centers is separately deployed. Enabling even a limited interworking among these separated control systems requires the adoption of complex and inelastic interfaces among the various networks, and this solution is not efficient enough to provide the required quality of service. In this paper, we propose a multi-stratum resource integration (MSRI) architecture for OpenFlow-based data center interconnection using IP and optical transport networks. The control of the architecture is implemented through multiple OpenFlow controllers' cooperation. By exchanging information among multiple controllers, the MSRI can effectively overcome the interworking limitations of a multi-stratum architecture, enable joint optimization of data center and network resources, and enhance the data center responsiveness to end-to-end service demands. Additionally, a service-aware flow estimation strategy for MSRI is introduced based on the proposed architecture. The overall feasibility and efficiency of the proposed architecture are experimentally demonstrated on our optical as a service testbed in terms of blocking probability, resource occupation rate, and path provisioning latency.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6645118,no
A generic framework for executable gestural interaction models,"Integrating new input devices and their associated interaction techniques into interactive applications has always been challenging and time-consuming, due to the learning curve and technical complexity involved. Modeling devices, interactions and applications helps reducing the accidental complexity. Visual modeling languages can hide an important part of the technical aspects involved in the development process, thus allowing a faster and less error-prone development process. However, even with the help of modeling, a gap remains to be bridged in order to go from models to the actual implementation of the interactive application. In this paper we use ICO, a visual formalism based on high-level Petri nets, to develop a generic layered framework for specifying executable models of interaction using gestural input devices. By way of the CASE tool Petshop we demonstrate the framework's feasibility to handle the Kinect and gesture-based interaction techniques. We validate the approach through two case studies that illustrate how to use executable, reusable and extensible ICO models to develop gesture-based applications.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6645240,no
Visualization of fine-grained code change history,"Conventional version control systems save code changes at each check-in. Recently, some development environments retain more fine-grain changes. However, providing tools for developers to use those histories is not a trivial task, due to the difficulties in visualizing the history. We present two visualizations of fine-grained code change history, which actively interact with the code editor: a timeline visualization, and a code history diff view. Our timeline and filtering options allow developers to navigate through the history and easily focus on the information they need. The code history diff view shows the history of any particular code fragment, allowing developers to move through the history simply by dragging the marker back and forth through the timeline to instantly see the code that was in the snippet at any point in the past. We augment the usefulness of these visualizations with richer editor commands including selective undo and search, which are all implemented in an Eclipse plug-in called Azurite. Azurite helps developers with answering common questions developers ask about the code change history that have been identified by prior research. In addition, many of users' backtracking tasks can be achieved using Azurite, which would be tedious or error-prone otherwise.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6645254,no
A resource-efficient probabilistic fault simulator,"The reduction of CMOS structures into the nanometer regime, as well as the high demand for low-power applications, animating to further reduce the supply voltages towards the threshold, results in an increased susceptibility of integrated circuits to soft errors. Hence, circuit reliability has become a major concern in today's VLSI design process. A new approach to further support these trends is to relax the reliability requirements of a circuit, while ensuring that the functionality of the circuit remains unaffected, or effects remain unnoticed by the user. To realize such an approach it is necessary to determine the probability of an error at the output of a circuit, given an error probability distribution at the circuits' elements. Purely software-based simulation approaches are unsuitable due to the large simulation times. Hardware-accelerated approaches exist, but lack the ability to inject errors based on probabilities, are slow or have a large area overhead. In this paper we propose a novel approach for FPGA-based, probabilistic, circuit fault simulation. The proposed system is a mainly hardware-based, which makes the simulation fast, but also keeps the hardware overhead on the FPGA low by exploiting FPGA specific features.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6645581,no
A probabilistic verification framework of SysML activity diagrams,"SysML activity diagrams are OMG/INCOSE standard used for modeling and analyzing probabilistic systems. In this paper, we propose a formal verification framework that is based on PRISM probabilistic symbolic model checker to verify the correctness of these diagrams. To this end, we present an efficient algorithm that transforms a composition of SysML activity diagrams to an equivalent probabilistic automata encoded in PRISM input language. To clarify the quality of our verification framework, we formalize both SysML activity diagrams and PRISM input language. Finally, we demonstrate the effectiveness of our approach by presenting a case study.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6645657,no
A persistent data storage design for real-time interactive applications,"Real-time Online Interactive Applications (ROIA) like multiplayer online games usually work in a persistent environment (also called virtual world) which continues to exist and evolve also while the user is offline and away from the application. This paper deals with storing persistent data of real-time interactive applications in modern relational databases. We describe a preliminary design of the Entity Persistence Module (EPM) middleware which liberates the application developer from writing and maintaining complex and error-prone, application-specific code for persistent data management.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6645659,no
Inter-domain QoS in dynamic circuit network,"A dynamic circuit network (DCN) provides an advance bandwidth reservation (ABR) service across multiple administrative domains. The On-demand Secure Circuits and Advance Reservation System (OSCARS) is a DCN controller deployed in many networks, for example, the Energy Sciences Network (ESnet) and Internet2 in USA, Rede Nacional de Ensino e Pesquisa (RNP) in Brazil, and JGN-X (a new generation network testbed) in Japan. This paper proposes inter-domain QoS scenarios and an extension of a new path computation element (PCE) with Attribute-Mapping for quality of services (QoS) differentiation in OSCARS. Our modified OSCARS performs a QoS mechanism in which network resources are reserved for high priority requests to ensure low request blocking probabilities (RBPs). Our proposal differentiates among requests for both intra- and inter-domain communications.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6645842,no
Analysis and control of DC voltage ripple for modular multilevel converters under single line to ground fault,"This paper deals with DC voltage ripple suppression of the modular multilevel converter (MMC) under single-line-to-ground (SLG) fault condition. First, the instantaneous power of a phase unit is derived theoretically according to the equivalent circuit model of the MMC under unbalanced condition, providing a mathematical explanation of the double-line frequency ripple contained in the dc voltage. Moreover, different characteristics of phase current during three possible SLG faults are analyzed and compared. Based on the derivation and analysis, a quasi-PR controller is proposed to suppress the dc voltage ripple. The proposed controller, combining with the negative and/or zero sequence current controllers, could enhance the overall fault-tolerant capability of the MMC under different types of SLG faults. In addition, no extra cost will be introduced given that only DC voltage is required to be detected. Simulation results from a three-phase MMC based rectifier system generated with the Matlab/Simulink software are provided to support the theoretical considerations.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6647247,no
Criticality of defects in cyclic dependent components,"(Background) Software defects that most likely will turn into system and/or business failures are termed critical by most stakeholders. Thus, having some warnings of the most probable location of such critical defects in a software system is crucial. Software complexity (e.g. coupling) has long been established to be associated with the number of defects. However, what is really challenging is not in the number but identifying the most severe defects that impact reliability. (Research Goal) Do cyclic related components account for a clear majority of the critical defects in software systems? (Approach) We have empirically evaluated two non-trivial systems. One commercial Smart Grid system developed with C# and an open source messaging and integrated pattern server developed with Java. By using cycle metrics, we mined the components into cyclic-related and non-cyclic related groups. Lastly, we evaluated the statistical significance of critical defects and severe defect-prone components (SDCs) in both groups. (Results) In these two systems, results demonstrated convincingly, that components in cyclic relationships account for a significant and the most critical defects and SDCs. (Discussion and Conclusion) We further identified a segment of a system with cyclic complexity that consist almost all of the critical defects and SDCs that impact on system's reliability. Such critical defects and the affected components should be focused for increased testing and refactoring possibilities.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6648180,no
Proteum/FL: A tool for localizing faults using mutation analysis,"Fault diagnosis is the process of analyzing programs with the aim of identifying the code fragments that are faulty. It has been identified as one of the most expensive and time consuming tasks of software development. Even worst, this activity is usually accomplished based on manual analysis. To this end, automatic or semi-automatic fault diagnosis approaches are useful in assisting software developers. Hence, they can play an essential role in decreasing the overall development cost. This paper presents Proteum/FL, a mutation analysis tool for diagnosing previously detected faults. Given an ANSI-C program and a set of test cases, Proteum/FL returns a list of program statements ranked according to their likelihood of being faulty. The tool differs from the rest of the mutation analysis and fault diagnosis tools by employing mutation analysis as a means of diagnosing program faults. It therefore demonstrates the effective use of mutation in supporting both testing and debugging activities.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6648189,no
Fix-it: An extensible code auto-fix component in Review Bot,"Coding standard violations, defect patterns and non-conformance to best practices are abundant in checked-in source code. This often leads to unmaintainable code and potential bugs in later stages of software life cycle. It is important to detect and correct these issues early in the development cycle, when it is less expensive to fix. Even though static analysis techniques such as tool-assisted code review are effective in addressing this problem, there is significant amount of human effort involved in identifying the source code issues and fixing it. Review Bot is a tool designed to reduce the human effort and improve the quality in code reviews by generating automatic reviews using static analysis output. In this paper, we propose an extension to Review Bot- addition of a component called Fix-it for the auto-correction of various source code issues using Abstract Syntax Tree (AST) transformations. Fix-it uses built-in fixes to automatically fix various issues reported by the auto-reviewer component in Review Bot, thereby reducing the human effort to greater extent. Fix-it is designed to be highly extensible-users can add support for the detection of new defect patterns using XPath or XQuery and provide fixes for it based on AST transformations written in a high-level programming language. It allows the user to treat the AST as a DOM tree and run XQuery UPDATE expressions to perform AST transformations as part of a fix. Fix-it also includes a designer application which enables Review Bot administrators to design new defect patterns and fixes. The developer feedback on a stand-alone prototype indicates the possibility of significant human effort reduction in code reviews using Fix-it.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6648198,no
Differential Debugging,"Phillip G. Armour responds to """"Differential Debugging"""" in the Tools of the Trade column September/October issue of IEEE Software to discuss the process of predicting defects.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6648591,no
Service Matching under Consideration of Explicitly Specified Service Variants,"One of the main ideas of Service-Oriented Computing (SOC) is the delivery of flexibly composable services provided on world-wide markets. For a successful service discovery, service requests have to be matched with the available service offers. However, in a situation in which no service that completely matches the request can be discovered, the customer may tolerate slight discrepancies between request and offer. Some existing fuzzy matching approaches are able to detect such service variants, but they do not allow to explicitly specify which parts of a request are not mandatory. In this paper, we improve an existing service matching approach based on Visual Contracts leveraging our preliminary work of design pattern detection. Thereby, we support explicit specifications of service variants and realize gradual matching results that can be ranked in order to discover the service offer that matches a customer's request best.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6649635,no
Reliable Service Composition via Automatic QoS Prediction,"Service composition has received considerable attention nowadays as a key technology to deliver desired business logics by directly aggregating existing Web services. Considering the dynamic and autonomous nature of Web services, building high-quality software systems by composing third-party services faces novel challenges. As a solution, new techniques have been recently developed to automatically predict the QoS of services in a future time and the prediction result will facilitate in selecting individual services. Nonetheless, limited effort has been devoted to QoS prediction for service composition. To fill out this technical gap, we propose a novel model in this paper that integrates QoS prediction with service composition. The integrated model will lead to a composition result that is not only able to fulfill user requirement during the composition time but also expected to maintain the desired QoS in future. As user requirement is expected to be satisfied by the composition result for a long period of time, significant effort can be reduced for re-composing newly selected services, which usually incurs high cost. We conduct experiments on both real and synthetic QoS datasets to demonstrate the effectiveness of the proposed approach.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6649696,no
AESON: A Model-Driven and Fault Tolerant Composite Deployment Runtime for IaaS Clouds,"Infrastructure-as-a-Service (IaaS) cloud environments expose to users the infrastructure of a data center while relieving them from the burden and costs associated with its management and maintenance. IaaS clouds provide an interface by means of which users can create, configure, and control a set of virtual machines that will typically host a composite software service. Given the increasing popularity of this computing paradigm, previous work has focused on modeling composite software services to automate their deployment in IaaS clouds. This work is concerned with the runtime state of composite services during and after deployment. We propose AESON, a deployment runtime that automatically detects node (virtual machine) failures and eventually brings the composite service to the desired deployment state by using information describing relationships between the service components. We have designed AESON as a decentralized peer-to-peer publish/subscribe system leveraging IBM's Bulletin Board (BB), a topic-based distributed shared memory service built on top of an overlay network.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6649743,no
Generalized Logit Regression-Based Software Reliability Modeling with Metrics Data,"It is well known that multifactor software reliability modeling with software metrics data is useful to predict the software reliability with higher accuracy, because it utilizes not only software fault count data but also software testing metrics data observed in the development process. In this paper we extend the existing logit regression-based software reliability model by introducing more generalized logistic type functions and improve the goodness-of-fit and predictive performances. In numerical examples with real software development project data, it is shown that our generalized models can outperform the existing logit regression-based model and the Cox regression-based model significantly.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6649828,no
Expose: Discovering Potential Binary Code Re-use,"The use of third-party libraries in deployed applications can potentially put an organization's intellectual property at risk due to licensing restrictions requiring disclosure or distribution of the resulting software. Binary applications that are statically linked to buggy version(s) of a library can also provide malware with entry points into an organization. While many organizations have policies to restrict the use of third-party software in applications, determining whether an application uses a restricted library can be difficult when it is distributed as binary code. Compiler optimizations, function inlining, and lack of symbols in binary code make the task challenging for automated techniques. On the other hand, semantic analysis techniques are relatively slow. Given a library and a set of binary applications, we propose Expose, a tool that combines symbolic execution using a theorem prover, and function-level syntactic matching techniques to achieve both performance and high quality rankings of applications. Higher rankings indicate a higher likelihood of re-using the library's code. Expose ranked applications that used two libraries at or near the top, out of 2,927 and 128 applications respectively. Expose detected one application that was not detected by another scanner to use some functions in one of the libraries. In addition, Expose ranked applications correctly for different versions of a library, and when different compiler options were used. Expose analyzed 97.68% and 99.48% of the applications within five and 10 minutes respectively.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6649873,no
On the Gain of Measuring Test Case Prioritization,"Test case prioritization (TCP) techniques aim to schedule the order of regression test suite to maximize some properties, such as early fault detection. In order to measure the abilities of different TCP techniques for early fault detection, a metric named average percentage of faults detected (APFD) is widely adopted. In this paper, we analyze the metric APFD and explore the gain of measuring TCP techniques from a control theory viewpoint. Based on that, we propose a generalized metric for TCP. This new metric focuses on the gain of defining early fault detection and measuring TCP techniques for various needs in different evaluation scenarios. By adopting this new metric, not only flexibility can be guaranteed, but also explicit physical significance for the metric will be provided before evaluation.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6649891,no
Isolating and Understanding Program Errors Using Probabilistic Dispute Model,"Automated software debugging can have a signifi-cant impact on the cost and quality of software development and maintenance. In recent years, researchers have invested a considerable amount of effort in developing automated techniques, and have demonstrated their effectiveness in helping developers in certain debugging tasks by pinpointing faulty statements. But there is still a gap between examining a faulty statement and understanding root causes of the cor-responding bug. As a step in this direction, we believe good developers have defensive programming in minds and software debugging is a process in search of arguments about why a statement is faulty. Therefore, a fault localization problem is rephrased as a dispute game between statements involved in successful runs and failing runs. A statement is OK if it can always provide arguments against other's blames, whereas a less defensive statement is thought to be faulty. In doing so, we propose a probabilistic dispute graph which is built upon dynamic dependencies between statements and statistics of program runs. Using such a graph, we put executed statements in dispute, compute acceptable statements, and thus figure out faulty statements if they have not strong arguments about their correctness. For empirical purpose, we carry out experiments on the well-known Siemens benchmark, and conclude that our approach not only casts new light on the causes of bugs in various cases, but also is statistically more effective in fault localization than competitors like Tarantula, SOBER, CT and PPDG.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6649892,no
SQAF-DS: A Software Quality Assessment Framework for Dependable Systems,"This paper proposes a software quality assessment framework for dependable systems (SQAF-DS), providing a systematic way to assess software quality through test cases, indirectly. SQAF-DS intends to reduce the time and cost for dependability assessment thorough using test cases as a means of the assessment. Test cases are developed in the process of software development and used to test target system, while dependability requirements are derived from dependability analysis, such as FTA (Fault Tree Analysis). SQAF-DS formally checks inclusion relation between dependability requirements and test cases. If the formal checking succeeds, then we can assure that the dependability requirements are well implemented in the software system.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6649907,no
Empirical Effectiveness Evaluation of Spectra-Based Fault Localization on Automated Program Repair,"Researchers have proposed many spectra-based fault localization (SBFL) techniques in the past decades. Existing studies evaluate the effectiveness of these techniques from the viewpoint of developers, and have drawn some important conclusions through either empirical study or theoretical analysis. In this paper, we present the first study on the effectiveness of SBFL techniques from the viewpoint of fully automated debugging including the program repair of automation, for which the activity of automated fault localization is necessary. We assess the accuracy of fault localization according to the repair effectiveness in the automated repair process guided by the localization technique. Our experiment on 14 popular SBFL techniques with 11 subject programs shipping with real-life field failures presents the evidence that some conclusions drawn in prior studies do not hold in our experiment. Based on experimental results, we suggest that Jaccard should be used with high priority before some more effective SBFL techniques specially proposed for automated program repair occur in the future.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6649928,no
Using HTML5 visualizations in software fault localization,"Testing and debugging is the most expensive, error-prone phase in the software development life cycle. Automated software fault localization can drastically improve the efficiency of this phase, thus improving the overall quality of the software. Amongst the most well-known techniques, due to its efficiency and effectiveness, is spectrum-based fault localization. In this paper, we propose three dynamic graphical forms using HTML5 to display the diagnostic reports yielded by spectrum-based fault localization. The visualizations proposed, namely Sunburst, Vertical Partition, and Bubble Hierarchy, have been implemented within the GZOLTAR toolset, replacing previous and less-intuitive OpenGL-based visualizations. The GZOLTAR toolset is a plug-and-play plugin for the Eclipse IDE to ease world-wide adoption. Finally, we performed an user study with GZOLTAR and confirmed that the visualizations help to drastically reduce the time needed in debugging (e.g., all participants using the visualizations were able to pinpoint the fault, whereas of those using traditional methods only 35% found the fault). The group that used the visualizations took on average 9 minutes and 17 seconds less than the group that did not use them.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6650539,no
Model Checking Stencil Computations Written in a Partitioned Global Address Space Language,"This paper proposes an approach to software model checking of stencil computations written in partitioned global address space (PGAS)languages. Although a stencil computation offers a simple and powerful programming style, it becomes error prone when considering optimization and parallelization. In the proposed approach, the state explosion problem associated with model checking (that is, where the number of states to be explored increases dramatically) is avoided by introducing abstractions suitable for stencil computation. In addition, this paper also describes XMP-SPIN, our model checker for XcalableMP (XMP), a PGAS language that provides support for implementing parallelized stencil computations. One distinguishing feature of XMP-SPIN is that users are able to define their own abstractions in a simple and flexible way. The proposed abstractions are implemented as user-defined abstractions. This paper also presents experimental results for model checking stencil computations using XMP-SPIN. The results demonstrate the effectiveness and practicality of the proposed approach and XMP-SPIN.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6650908,no
Dauphin: A new statistical signal processing language,"Many software packages support scientific research by means of numerical calculations and specialised library calls, but very few support specific application domains such as signal processing at the symbolic level or at problem formulation. Translation from the natural domain-specific structure of problem description to the computer formulation is often a time consuming and error-prone exercise. As signal processing becomes more sophisticated, there is a need to codify its basic tools, thus allowing the researcher to spend more time on the challenges specific to a particular application. In this paper, we describe the design of Dauphin, a domain-specific programming language. Dauphin ultimately aims to extend the power of signal processing researchers by allowing them to focus on their research problems while simplifying the process of implementing their ideas. In Dauphin, the basic algorithms of signal processing become the standard function calls and are expressed naturally in terms of predefined signal processing primitives such as random variables and probability distributions.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6652033,no
An efficient and intelligent model to control driving offenses by using cloud computing concepts based on road transportation situation in Malaysia,"Information Technology (IT) has had undeniable effects on various industries in the recent years whereby road transportation industry and control services over vehicles and drivers are not apart from these effects. One of the most potential and new IT technologies that has not been considered and focused significantly in road transportation industry is cloud computing. This paper proposes an efficient and intelligence model for control driving offenses by using three main technologies in IT industry: Image Processing, Artificial Intelligence, and Cloud Computing. In the proposed model, Vertical-Edge Detection Algorithm (VEDA) was used for car license plate detection process in highways to provide an efficient image processing process with low quality images that were taken from installed cameras. Furthermore, two intelligence cloud-based Software-as-a-Service applications were used for car license plate detection, matching violations detected numbers with entrance detected numbers, and identification of possible exit routes for further processes. In addition, the suggested model contains a cloud server for storing databases and violation records which make them always accessible according to cloud computing concepts. The theoretical analysis of the proposed model was done according to three main parameters: efficiency, intelligence, and compatibility, and showed that Cloud-based Driving Offenses Control (CDOC) algorithm might be effective for providing an efficient method to control driving offenses and decreasing the rate of violations at highways.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6653268,no
CFEDR: Control-flow error detection and recovery using encoded signatures monitoring,"The incorporation of error detection and recovery mechanisms becomes mandatory as the probability of the occurrence of transient faults increases. The detection of control flow errors has been extensively investigated in literature. However, only few works have been conducted towards recovery from control-flow errors. Generally, a program is re-executed after error detection. Although re-execution prevents faults from corrupting data, it does not allow the application to run to completion correctly in the presence of an error. Moreover, the overhead of re-execution increases prominently. The current study presents a pure-software method based on encoded signatures to recover from control-flow errors. Unlike general signature monitoring techniques, the proposed method targets not only interblock transitions, but also intrablock and inter-function transitions. After detecting the illegal transition, the program flow transfers back to the block where the error occurred, and the data errors caused by the error propagation are recovered. Fault injection and performance overhead experiments are performed to evaluate the proposed method. The experimental results show that most control flow errors can be recovered with relatively low performance overhead.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6653578,no
DaemonGuard: O/S-assisted selective software-based Self-Testing for multi-core systems,"As technology scales deep into the sub-micron regime, transistors become less reliable. Future systems are widely predicted to suffer from considerable aging and wear-out effects. This ominous threat has urged system designers to develop effective run-time testing methodologies that can monitor and assess the system's health. In this work, we investigate the potential of online software-based functional testing at the granularity of individual microprocessor core components in multi-core systems. While existing techniques monolithically test the entire core, our approach aims to reduce testing time by avoiding the over-testing of under-utilized units. To facilitate fine-grained testing, we introduce DaemonGuard, a framework that enables the real-time observation of individual sub-core modules and performs on-demand selective testing of only the modules that have recently been stressed. The monitoring and test-initiation process is orchestrated by a transparent, minimally-intrusive, and lightweight operating system process that observes the utilization of individual datapath components at run-time. We perform a series of experiments using a full-system, execution-driven simulation framework running a commodity operating system, real multi-threaded workloads, and test programs. Our results indicate that operating-system-assisted selective testing at the sub-core level leads to substantial savings in testing time and very low impact on system performance.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6653581,no
Approximate simulation of circuits with probabilistic behavior,"Various emerging technologies promise advantages with respect to integration density, performance or power consumption, at the cost of approximate or probabilistic behavior. Approximate computing, where limited computational inaccuracies are tolerated at the system or application level is therefore of increasing interest. This paper investigates the use of stochastic computing (SC) as a tool for approximate simulation of probabilistic behavior. SC has the advantage of processing probabilities directly at very low hardware cost. It also allows accuracy to be traded for run-time in a natural way (progressive precision). AS a target technology to be simulated, we choose quantum computing circuits, whose behavior is inherently probabilistic and cannot be efficiently simulated by conventional (classical) means. We show how complex operations such as superposition and entanglement can be handled by SC. Finally, we report experimental results on software-based simulation of representative quantum circuits, both stand-alone and FPGA-supported. The results show that the SC implementations are orders of magnitude more compact than those based on classical circuits. Accurate results may require very long simulation runs, but run-times can be reduced by exploiting SC's progressive precision property.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6653589,no
Shielding heterogeneous MPSoCs from untrustworthy 3PIPs through security-driven task scheduling,"Outsourcing of the various aspects of IC design and fabrication flow strongly questions the classic assumption that hardware is trustworthy. Multiprocessor System-on-Chip (MPSoC) platforms face some of the most demanding security concerns, as they process, store, and communicate sensitive information using third-party intellectual property (3PIP) cores that may be untrustworthy. The complexity of an MPSoC makes it expensive and time consuming to fully analyze and test it during the design stage. Consequently, the trustworthiness of the 3PIP components cannot be ensured. To protect MPSoCs against malicious modifications, we propose to incorporate trojan toleration into MPSoC platforms by revising the task scheduling step of the MPSoC design process. We impose a set of security-driven diversity constraints into the scheduling process, enabling the system to detect the presence of malicious modifications or to mute their effects during application execution. Furthermore, we pose the security-constrained MPSoC task scheduling as a multi-dimensional optimization problem, and propose a set of heuristics to ensure that the introduced security constraints can be fulfilled with minimum performance and hardware overhead.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6653590,no
Exploiting error control approaches for Hardware Trojans on Network-on-Chip links,"We exploit transient and permanent error control methods to address Hardware Trojan (HT) issues in Network-on-Chip (NoC) links. The use of hardware-efficient error control methods on NoC links has the potential to reduce the overall hardware cost for security protection, with respect to cryptographic-based rerouting algorithms. An error control coding method for transient errors is used to detect the HT-induced link errors. Regarding the faulty links as permanently failed interconnects, we propose to reshuffle the links and isolate the HT-controlled link wires. Rather than rerouting packets via alternative paths, the proposed method resumes the utilization of partially failed links to improve the bandwidth and the average latency of NoCs. Simulation results show that our method improves the average latency by up to 44.7% over the rerouting approach. The reduction on latency varies from 20% to 41% for three traffic patterns on a 55 mesh NoC. The impact of different HT locations on NoC links was examined, as well. Our method is not sensitive to HT locations and can improve the effective bandwidth by up to 29 bits per cycle with minor overhead.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6653617,no
A smart Trojan circuit and smart attack method in AES encryption circuits,"The increased utilization of outsourcing services for designing and manufacturing LSIs can reduce the reliability of LSIs. Trojan circuits are malicious circuits that can leak secret information. In this paper, we propose a Trojan circuit whose detection is difficult in AES circuits. To make it difficult to detect the proposed Trojan circuit, we propose two methods. In one method, one of test mode signal lines not used in normal operation is included in the activation conditions on the trigger unit. In the other, the payload unit does not directly leak the cipher key of an AES circuit but instead leaks information related to the cipher key. We also propose a procedure to obtain the secret key from the information. We demonstrate that it is difficult to detect the proposed Trojan circuit by using existing approaches. We show results to implement and to estimate the area and power of AES circuits with and without the proposed Trojan circuit.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6653619,no
A novel approach to effective detection and analysis of code clones,"Code clones are found in most of the software systems. They play a major role in the field of software engineering. The presence of clones in a particular module will either improve or degrade the quality of the overall software system. Poor quality software indirectly leads to strenuous software maintenance. Detecting the code clones will also pave way for analyzing them. The existing approaches detect the clones efficiently. Though some of the tools analyze the clones, accuracy is still missing. In this paper, a novel method is proposed, which exhibits the use of an efficient data mining technique in the phase of analysis. Based on the outcome of the analysis, the clones are either removed or retained in the software system.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6653701,no
Sensor data quality assessment for building simulation model calibration based on automatic differentiation,"Building simulation models play a vital role in optimal building climate control, energy audit, fault detection and diagnosis, continuous commissioning, and planning. Real system parameters are often unknown or partially unknown and need to be identified through historical data, which are currently acquired by heuristically designed experiments. Without quality sensor data, model calibration is prone to fail, even if the calibration algorithm is appropriate. In this paper, we propose a Fisher-information-matrix (FIM)-based metric to examine the sensor data measurements and how their quality is related to the model calibration quality. It aims to provide quantitative guidance in the calibration cycle of a whole building model that takes as many variables as possible into consideration for the sake of accuracy. Our concerned model is based on well-known physical laws and tries to avoid simplification, thereby leading to a highly discontinuous system with model switches due to the seasonal or daily variation and other reasons. Such a model is implemented in the form of a software package. Hence, no explicit mathematical expression can be given. A key technical challenge is that the complexity of the model prohibits the analytical derivation of FIM, while the numeric calculation is sensitive to sensor noise and model switches. We, hence, propose to adopt an automatic differentiation method, which exploits the operator overload feature of object oriented programming language, for robust numerical FIM calculation.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6654061,no
A comprehensive QoS determination model for Infrastructure-as-a-Service clouds,"Cloud computing is a recently developed new technology for complex systems with massive service sharing, which is different from the resource sharing of the grid computing systems. In a cloud environment, service requests from users go through numerous provider specific steps from the instant it is submitted to when the service is fully delivered. Quality modeling and analysis of clouds are not easy tasks because of the complexity of the provisioning mechanism and the dynamic cloud environment. This study proposes an analytical model-based approach for quality evaluation of Infrastructure-as-a-Service cloud and consider expected request completion time, rejection probability, and system overhead rate as key QoS metrics. It also features with the modeling of different warming and cooling strategies of machines and the ability to identify the optimal balance between system overhead and performance.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6654070,no
Sensor health state estimation for target tracking with binary sensor networks,"We consider the problem of target (event source) tracking using a binary Wireless Sensor Network (WSN). For this problem, a WSN consisting of sensors that can detect the presence of a target in an area around them, should fuse the information received by the individual sensors in order to localize and track the target. This is a challenging problem particularly when sensors may fail either due to hardware and/or software malfunctions, energy depletion or adversary attacks. Using information from failed sensors during target tracking may lead to high estimation errors. Since failure of individual sensors is unavoidable, there is a need to estimate the health state of each sensor in order to ignore those sensors that are considered as faulty. The contribution of this work is the investigation of three different algorithms for estimating the sensors' health state simultaneously with target tracking.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6654795,no
DSVM: A buffer management strategy for video transmission in opportunistic networks,"In recent years, more and more people have begun to focus on data delivery among mobile users through opportunistic networks, and in most cases, such as emergency, traffic accident and disaster, we need to transmit video data to other users by the means of opportunistic contacts between mobile users. Meanwhile, the buffers of mobile wireless devices are usually limited, then some messages which have different importance on video recovery, will be dropped during transmission. Therefore, it's imperative to design efficient buffer management strategy to improve the video delivery quality. Several policies have been presented, but they are just designed for general data and not suitable for video transmission. In this paper, we comprehensively take the temporal correlation of video data and the diffusivity of messages into account, and propose DSVM, a novel buffer management strategy. Extensive simulations validate its performance, and up to about 3dB Peak Signal-to-Noise Ratio (PSNR) gain can be achieved over the state-of-the-art buffer management policies.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6654998,no
Efficient Formal Verification in Banking Processes,"Model checking is a very useful method to verify concurrent and distributed systems which is traditionally applied to computer system design. We examine the applicability of model checking to validation of Business Processes that are mapped through the systems of Workflow Management. The use of model checking in business domain is affected by the state explosion problem, which says that the state space grows exponentially in the number of concurrent processes. In this paper we consider a property-based methodology developed to combat the state explosion problem. Our focus is two fold; firstly we show how model checking can be applied in the context of business modelling and analysis and secondly we evaluate and test the methodology using as a case study a real-world banking workflow of a loan origination process. Our investigations suggest that the business community, especially in the banking field, can benefit from this efficient methodology developed in formal methods since it can detect errors that were missed by traditional verification techniques, and being cost-efficient, it can be adopted as a standard quality assurance procedure. We show and discuss the experimental results obtained.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6655717,no
"Improving frequency and ROCOF accuracy during faults, for P class Phasor Measurement Units","Many aspects of Phasor Measurement Unit (PMU) performance are tested using the existing (and evolving) IEEE C37.118 standard. However, at present the reaction of PMUs to power network faults is not assessed under C37.118. Nevertheless, the behaviour of PMUs under such conditions may be important when the entire closed loop of power system measurement, control and response is considered. This paper presents ways in which P class PMU algorithms may be augmented with software which reduces peak frequency excursions during unbalanced faults by factors of typically between 2.5 and 6 with no additional effect on response time, delay or latency. Peak ROCOF excursions are also reduced. In addition, extra filtering which still allows P class response requirements to be met can further reduce excursions, in particular ROCOF. Further improvement of triggering by using midpoint taps of the P class filter, and adaptive filtering, allows peak excursions to be reduced by total factors of between 8 and 40 (or up to 180 for ROCOF), compared to the C37.118 reference device. Steady-state frequency and ROCOF errors during sustained faults or unbalanced operation, particularly under unbalanced conditions, can be reduced by factors of hundreds or thousands compared to the C37.118 reference device.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6656233,no
Rigorous Performance Evaluation of Self-Stabilization Using Probabilistic Model Checking,"We propose a new metric for effectively and accurately evaluating the performance of self-stabilizing algorithms. Self-stabilization is a versatile category of fault-tolerance that guarantees system recovery to normal behavior within a finite number of steps, when the state of the system is perturbed by transient faults (or equally, the initial state of the system can be some arbitrary state). The performance of self-stabilizing algorithms is conventionally characterized in the literature by asymptotic computation complexity. We argue that such characterization of performance is too abstract and does not reflect accurately the realities of deploying a distributed algorithm in practice. Our new metric for characterizing the performance of self-stabilizing algorithms is the expected mean value of recovery time. Our metric has several crucial features. Firstly, it encodes accurate average case speed of recovery. Secondly, we show that our evaluation method can effectively incorporate several other parameters that are of importance in practice and have no place in asymptotic computation complexity. Examples include the type of distributed scheduler, likelihood of occurrence of faults, the impact of faults on speed of recovery, and network topology. We utilize a deep analysis technique, namely, probabilistic model checking to rigorously compute our proposed metric. All our claims are backed by detailed case studies and experiments.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6656271,no
Adaptive Anomaly Identification by Exploring Metric Subspace in Cloud Computing Infrastructures,"Cloud computing has become increasingly popular by obviating the need for users to own and maintain complex computing infrastructures. However, due to their inherent complexity and large scale, production cloud computing systems are prone to various runtime problems caused by hardware and software faults and environmental factors. Autonomic anomaly detection is a crucial technique for understanding emergent, cloud-wide phenomena and self-managing cloud resources for system-level dependability assurance. To detect anomalous cloud behaviors, we need to monitor the cloud execution and collect runtime cloud performance data. These data consist of values of performance metrics for different types of failures, which display different correlations with the performance metrics. In this paper, we present an adaptive anomaly identification mechanism that explores the most relevant principal components of different failure types in cloud computing infrastructures. It integrates the cloud performance metric analysis with filtering techniques to achieve automated, efficient, and accurate anomaly identification. The proposed mechanism adapts itself by recursively learning from the newly verified detection results to refine future detections. We have implemented a prototype of the anomaly identification system and conducted experiments in an on-campus cloud computing environment and by using the Google data center traces. Our experimental results show that our mechanism can achieve more efficient and accurate anomaly detection than other existing schemes.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6656276,no
Cross Domain Assessment of Document to HTML Conversion Tools to Quantify Text and Structural Loss during Document Analysis,"During forensic text analysis, the automation of the process is key when working with large quantities of documents. As documents often come in a wide variety of different file types, this creates the need for tailored tools to be developed to analyze each document type to correctly identify and extract text elements for analysis without loss. These text extraction tools often omit sections of text that are unreadable from documents leaving drastic inconsistencies during the forensic text analysis process. As a solution to this a single output format, HTML, was chosen as a unified analysis format. Document to HTML/CSS extraction tools each with varying techniques to convert common document formats to rich HTML/CSS counterparts were tested. This approach can reduce the amount of analysis tools needed during forensic text analysis by utilizing a single document format. Two tests were designed, a 10 point document overview test and a 48 point detailed document analysis test to assess and quantify the level of loss, rate of error and overall quality of outputted HTML structures. This study concluded that tools that utilize a number of different approaches and have an understanding of the document structure yield the best results with the least amount of loss.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6657132,no
A Classifier of Malicious Android Applications,"Malware for smart phones is rapidly spreading out. This paper proposes a method for detecting malware based on three metrics, which evaluate: the occurrences of a specific subset of system calls, a weighted sum of a subset of permissions that the application required, and a set of combinations of permissions. The experimentation carried out suggests that these metrics are promising in detecting malware, but further improvements are needed to increase the quality of detection.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6657296,no
Measuring the Portability of Executable Service-Oriented Processes,"A key promise of process languages based on open standards, such as the Web Services Business Process Execution Language, is the avoidance of vendor lock-in through the portability of process definitions among runtime environments. Despite the fact that today, various runtimes claim to support this language, every runtime implements a different subset, thus hampering portability and locking in their users. In this paper, we intend to improve this situation by enabling the measurement of the degree of portability of process definitions. This helps developers to assess their process definitions and to decide if it is feasible to invest in the effort of porting a process definition to another runtime. We define several software quality metrics that quantify the degree of portability a process definition provides from different viewpoints. We validate these metrics theoretically with two validation frameworks and empirically with a large set of process definitions coming from several process libraries.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6658270,no
Detecting Software Aging in safety-critical infrastuctures,"In this paper we investigate the application of Software Aging and Rejuvenation in the context of Critical Infrastructures and Systems-of-Systems. Explained are the characteristics of Systems-of-Systems and classes of Critical Systems, attributes which define their dependability as a high priority requirement. In addition we survey Software Aging and Rejuvenation establishing founding research and recent research in this field. The work presented is on-going, however that discusses the challenges pertinent in the field of critical infrastructures, how we intend to investigate and propose new methods for applying context-sensitive fault-forecasting in a variety of complex systems that are associated with both domains and why context is so important.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6661720,no
Non-destructive testing of wood defects for Korean pine in northeast China based on ultrasonic technology,"The wood samples were tested by the technique of ultrasonic, and the testing results were analyzed by using the statistic software of SPSS. The results showed that the length, density and knots of wood, the sizes of holes and numbers of holes have significant influence on propagation parameters and dynamic modulus of elasticity. If there are holes in the propagation path, the propagation time will be longer, and the propagation velocity and wood modulus will decrease accordingly. The studying results of this paper will provide a sound background for the application of ultrasonic technique in detecting the inner defects of wood products and other wooden structures, and also offer important reference for testing the inner defects of old trees and ancient buildings.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6664057,no
Method for visualizing information from large-scale carrier networks,"With the increase in services, such as telephone, video on demand, and internet connection, networks now consist of various elements, such as routers, switches, and a wide variety of servers. The structure of a network has become more complicated. Therefore, failuare diagnosis and the affected area by using many alarms tends to be more difficult and the time required detecting the causal point of failure also becomes longer. However, to improve quality of services, reducing diagnosis time is essential. Alarm browsers and graphs are used to display the collected data from a networkto determine the network's status. An operator manages a network by envisioning the network structure. However, the larger the network becomes, the more difficult it is for operators to do this. Therefore, a topology view with geographical information and a topology view with hierarchical information of equipment are used. However, these views degrade if the scale of the network is even larger and more complex. We propose a method for visualizing network information on space and time axes. This method can support network operators to recognize causal points of failure and affected areas. We also explain a prototype software implementation of this visualization method.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6665290,no
A Mechanism of Maintaining the Survivability of Streaming Media Service in Overlay Networks,"With the quick development of service-oriented network, the services of streaming media and their traffic has become a rather important part in the circumstance of virtual network. However, there hasn't been a unified control mechanism to maintain the qualities of these services. The QoS of streaming media is a great concern of current network, and it'll be vital in future network because of the rapid growth of internet users and traffics. This paper describes a mechanism of maintaining the survivalbility of streaming media service in the circumstance of overlay network. We propose a method to calculate the health value of each path and service. Through this mechanism we can evaluate the current quality of services and provide information for further decisions. As a proof of concept we implement an experimental scenario to assess the functionality and the availability of this mechanism. It has managed effectively in the evaluation of streaming services.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6665348,no
Substation grounding transfer of potential case studies,"Industrial substation grounding studies usually assume a simple isolated substation, follow the routine IEEE 80 design and analysis procedures using the software tools bundled with the power systems analysis suite of choice, and carry on with the rest of the project. However, there can be complications to substation grounding designs. This paper discusses two case studies where a ground fault in one substation generates a voltage which is transferred to other areas. The issues are assessed using common software tools supplemented with simple spreadsheet calculations. Some common limitations of the standard software tools, when applied to these more complex problems, are discussed in this paper, along with appropriate work-arounds. Mitigation methods are discussed.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6666039,no
"Design and implementation of an intelligent system to detect quality state of temperature defects in hot rolled strips: At Siderurgica del Orinoco """"Alfredo Maneiro""""","This work show the design and implementation of an on-line intelligent system to determine the quality status of temperature defects in the hot rolled strips manufactured by SIDOR. The proposed system is based on a combination of expert systems, the standard automation platform of the company, and signal processing techniques. The rules of the expert system were proposed by quality assurance experts, who hold a huge expertise identifying temperature defects in the hot rolled strips, which indirectly determine the mechanical properties of the material. The entire architecture of the system was designed according to software engineering practices. The results shows the system is successful identifying and applying the quality status of each strip manufactured by the mill, with an initial performance of 34.5% of retained coils, and 12% of released coils.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6670621,no
Diagnosability Behaviour over faulty concurrent systems,"Complex systems often exhibit unexpected faults that are difficult to handle. It is desirable that such systems are diagnosable, i.e. faults are automatically detected as they occur (or shortly afterwards), enabling the system to handle the fault or recover. Formally, a system is diagnosable if it is possible to detect every fault, in a finite time after they occurred, by only observing available information from the system. Complex systems are usually built from simpler subsystems running concurrently. In order to model different communication and synchronization methods, the interactions between subsystems may be specified in various ways. In this work we present an analysis of the di-agnosability problem in concurrent systems under such different interaction strategies, with arbitrary faults occurring freely in subsystems. We rigorously define diagnosability in this setting, and formally prove in which cases diagnosability is preserved under composition. We illustrate our approach with several examples, and present a tool that implements our analysis.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6670624,no
"The first decade of GUI ripping: Extensions, applications, and broader impacts","This paper provides a retrospective examination of GUI Ripping - reverse engineering a workflow model of the graphical user interface of a software application - born a decade ago out of recognition of the severe need for improving the then largely manual state-of-the-practice of functional GUI testing. In these last 10 years, GUI ripping has turned out to be an enabler for much research, both within our group at Maryland and other groups. Researchers have found new and unique applications of GUI ripping, ranging from measuring human performance to re-engineering legacy user interfaces. GUI ripping has also enabled large-scale experimentation involving millions of test cases, thereby helping to understand the nature of GUI faults and characteristics of test cases to detect them. It has resulted in large multi-institutional Government-sponsored research projects on test automation and benchmarking. GUI ripping tools have been ported to many platforms, including Java AWT and Swing, iOS, Android, UNO, Microsoft Windows, and web. In essence, the technology has transformed the way researchers and practitioners think about the nature of GUI testing, no longer considered a manual activity; rather, thanks largely to GUI Ripping, automation has become the primary focus of current GUI testing techniques.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6671275,no
Distilling useful clones by contextual differencing,"Clone detectors find similar code fragments and report large numbers of them for large systems. Textually similar clones may perform different computations, depending on the program context in which clones occur. Understanding these contextual differences is essential to distill useful clones for a specific maintenance task, such as refactoring. Manual analysis of contextual differences is time consuming and error-prone. To mitigate this problem, we present an automated approach to helping developers find and analyze contextual differences of clones. Our approach represents context of clones as program dependence graphs, and applies a graph differencing technique to identify required contextual differences of clones. We implemented a tool called CloneDifferentiator that identifies contextual differences of clones and allows developers to formulate queries to distill candidate clones that are useful for a given refactoring task. Two empirical studies show that CloneDifferentiator can reduce the efforts of post-detection analysis of clones for refactorings.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6671285,no
The influence of non-technical factors on code review,"When submitting a patch, the primary concerns of individual developers are How can I maximize the chances of my patch being approved, and minimize the time it takes for this to happen? In principle, code review is a transparent process that aims to assess qualities of the patch by their technical merits and in a timely manner; however, in practice the execution of this process can be affected by a variety of factors, some of which are external to the technical content of the patch itself. In this paper, we describe an empirical study of the code review process for WebKit, a large, open source project; we replicate the impact of previously studied factors - such as patch size, priority, and component and extend these studies by investigating organizational (the company) and personal dimensions (reviewer load and activity, patch writer experience) on code review response time and outcome. Our approach uses a reverse engineered model of the patch submission process and extracts key information from the issue tracking and code review systems. Our findings suggest that these nontechnical factors can significantly impact code review outcomes.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6671287,no
A model-driven graph-matching approach for design pattern detection,"In this paper an approach to automatically detect Design Patterns (DPs) in Object Oriented systems is presented. It allows to link system's source code components to the roles they play in each pattern. DPs are modelled by high level structural properties (e.g. inheritance, dependency, invocation, delegation, type nesting and membership relationships) that are checked against the system structure and components. The proposed metamodel also allows to define DP variants, overriding the structural properties of existing DP models, to improve detection quality. The approach was validated on an open benchmark containing several open-source systems of increasing sizes. Moreover, for other two systems, the results have been compared with the ones from a similar approach existing in literature. The results obtained on the analyzed systems, the identified variants and the efficiency and effectiveness of the approach are thoroughly presented and discussed.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6671292,no
Automatic discovery of function mappings between similar libraries,"Library migration is the process of replacing a third-party library in favor of a competing one during software maintenance. The process of transforming a software source code to become compliant with a new library is cumbersome and error-prone. Indeed, developers have to understand a new Application Programming Interface (API) and search for the right replacements for the functions they use from the old library. As the two libraries are independent, the functions may have totally different structures and names, making the search of mappings very difficult. To assist the developers in this difficult task, we introduce an approach that analyzes source code changes from software projects that already underwent a given library migration to extract mappings between functions. We demonstrate the applicability of our approach on several library migrations performed on the Java open source software projects.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6671294,no
Heuristics for discovering architectural violations,"Software architecture conformance is a key software quality control activity that aims to reveal the progressive gap normally observed between concrete and planned software architectures. In this paper, we present ArchLint, a lightweight approach for architecture conformance based on a combination of static and historical source code analysis. For this purpose, ArchLint relies on four heuristics for detecting both absences and divergences in source code based architectures. We applied ArchLint in an industrial-strength system and as a result we detected 119 architectural violations, with an overall precision of 46.7% and a recall of 96.2%, for divergences. We also evaluated ArchLint with four open-source systems, used in an independent study on reflexion models. In this second study, ArchLint achieved precision results ranging from 57.1% to 89.4%.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6671297,no
Circe: A grammar-based oracle for testing Cross-site scripting in web applications,"Security is a crucial concern, especially for those applications, like web-based programs, that are constantly exposed to potentially malicious environments. Security testing aims at verifying the presence of security related defects. Security tests consist of two major parts, input values to run the application and the decision if the actual output matches the expected output, the latter is known as the oracle. In this paper, we present a process to build a security oracle for testing Cross-site scripting vulnerabilities in web applications. In the learning phase, we analyze web pages generated in safe conditions to learn a model of their syntactic structure. Then, in the testing phase, the model is used to classify new test cases either as safe tests or as successful attacks. This approach has been implemented in a tool, called Circe, and empirically assessed in classifying security test cases for two real world open source web applications.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6671301,no
Improving SOA antipatterns detection in Service Based Systems by mining execution traces,"Service Based Systems (SBSs), like other software systems, evolve due to changes in both user requirements and execution contexts. Continuous evolution could easily deteriorate the design and reduce the Quality of Service (QoS) of SBSs and may result in poor design solutions, commonly known as SOA antipatterns. SOA antipatterns lead to a reduced maintainability and reusability of SBSs. It is therefore important to first detect and then remove them. However, techniques for SOA antipattern detection are still in their infancy, and there are hardly any tools for their automatic detection. In this paper, we propose a new and innovative approach for SOA antipattern detection called SOMAD (Service Oriented Mining for Antipattern Detection) which is an evolution of the previously published SODA (Service Oriented Detection For Antpatterns) tool. SOMAD improves SOA antipattern detection by mining execution traces: It detects strong associations between sequences of service/method calls and further filters them using a suite of dedicated metrics. We first present the underlying association mining model and introduce the SBS-oriented rule metrics. We then describe a validating application of SOMAD to two independently developed SBSs. A comparison of our new tool with SODA reveals superiority of the former: Its precision is better by a margin ranging from 2.6% to 16.67% while the recall remains optimal at 100% and the speed is significantly reduces (2.5+ times on the same test subjects).",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6671307,no
Mining system specific rules from change patterns,"A significant percentage of warnings reported by tools to detect coding standard violations are false positives. Thus, there are some works dedicated to provide better rules by mining them from source code history, analyzing bug-fixes or changes between system releases. However, software evolves over time, and during development not only bugs are fixed, but also features are added, and code is refactored. In such cases, changes must be consistently applied in source code to avoid maintenance problems. In this paper, we propose to extract system specific rules by mining systematic changes over source code history, i.e., not just from bug-fixes or system releases, to ensure that changes are consistently applied over source code. We focus on structural changes done to support API modification or evolution with the goal of providing better rules to developers. Also, rules are mined from predefined rule patterns that ensure their quality. In order to assess the precision of such specific rules to detect real violations, we compare them with generic rules provided by tools to detect coding standard violations on four real world systems covering two programming languages. The results show that specific rules are more precise in identifying real violations in source code than generic ones, and thus can complement them.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6671308,no
Mining the relationship between anti-patterns dependencies and fault-proneness,"Anti-patterns describe poor solutions to design and implementation problems which are claimed to make object oriented systems hard to maintain. Anti-patterns indicate weaknesses in design that may slow down development or increase the risk of faults or failures in the future. Classes in anti-patterns have some dependencies, such as static relationships, that may propagate potential problems to other classes. To the best of our knowledge, the relationship between anti-patterns dependencies (with non anti-patterns classes) and faults has yet to be studied in details. This paper presents the results of an empirical study aimed at analysing anti-patterns dependencies in three open source software systems, namely ArgoUML, JFreeChart, and XerecesJ. We show that, in almost all releases of the three systems, classes having dependencies with anti-patterns are more fault-prone than others. We also report other observations about these dependencies such as their impact on fault prediction. Software organizations could make use of these knowledge about anti-patterns dependencies to better focus their testing and reviews activities toward the most risky classes, e.g., classes with fault-prone dependencies with anti-patterns.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6671310,no
Assessing the complexity of upgrading software modules,"Modern software development frequently involves developing multiple codelines simultaneously. Improvements to one codeline should often be applied to other codelines as well, which is typically a time consuming and error-prone process. In order to reduce this (manual) effort, changes are applied to the system's modules and those affected modules are upgraded on the target system. This is a more coarse-grained approach than upgrading the affected files only. However, when a module is upgraded, one must make sure that all its dependencies are still satisfied. This paper proposes an approach to assess the ease of upgrading a software system. An algorithm was developed to compute the smallest set of upgrade dependencies, given the current version of a module and the version it has to be upgraded to. Furthermore, a visualization has been designed to explain why upgrading one module requires upgrading many additional modules. A case study has been performed at ASML to study the ease of upgrading the TwinScan software. The analysis shows that removing elements from interfaces leads to many additional upgrade dependencies. Moreover, based on our analysis we have formulated a number improvement suggestions such as a clear separation between the test code and the production code as well as an introduction of a structured process of symbols deprecation and removal.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6671319,no
Analyzing PL/1 legacy ecosystems: An experience report,This paper presents a case study of analyzing a legacy PL/1 ecosystem that has grown for 40 years to support the business needs of a large banking company. In order to support the stakeholders in analyzing it we developed St1-PL/1 - a tool that parses the code for association data and computes structural metrics which it then visualizes using top-down interactive exploration. Before building the tool and after demonstrating it to stakeholders we conducted several interviews to learn about legacy ecosystem analysis requirements. We briefly introduce the tool and then present results of analysing the case study. We show that although the vision for the future is to have an ecosystem architecture in which systems are as decoupled as possible the current state of the ecosystem is still removed from this. We also present some of the lessons learned during our experience discussions with stakeholders which include their interests in automatically assessing the quality of the legacy code.,2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6671320,no
Detecting dependencies in Enterprise JavaBeans with SQuAVisiT,"We present recent extensions to SQuAVisiT, Software Quality Assessment and Visualization Toolset. While SQuAVisiT has been designed with traditional software and traditional caller-callee dependencies in mind, recent popularity of Enterprise JavaBeans (EJB) required extensions that enable analysis of additional forms of dependencies: EJB dependency injections, object-relational (persistence) mappings and Web service mappings. In this paper we discuss the implementation of these extensions in SQuAVisiT and the application of SQuAVisiT to an open-source software system.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6671330,no
Disjoint paths pair computation procedure for SDH/SONET networks,"The increasing demand for bandwidth and the error-prone, costly, and long-lasting service provisioning process force carriers to find new ways of automatic service provisioning. Despite the introduction of numerous path computation procedures, suitable for WDM-technology-based networks, very few studies exist that discusses the SDH/SONET specific multiplexing requirements. Furthermore, the existing disjoint path computation procedures do not always find the best possible path between two nodes. In this paper, we propose a path computation procedure capable of addressing the SDH/SONET multiplexing requirements and the shortcomings of the existing disjoint path computation procedures. Our disjoint paths pair computation procedure is applied to the topology of the NSF.net network. The simulation results, obtained with Matlab, suggest that the proposed procedure outperforms existing path computation procedures. The proposed procedure covers the demand for bandwidth with fewer resources. Furthermore, it finds paths considering the capacity units of SDH/SONET. It is also observed that the time complexity is tolerable.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6671866,no
End-to-end QoS management across LTE networks,"In order to effectively deliver traffic from different applications, providing end-to-end Quality of Service (QoS) is critical in Long Term Evolution (LTE) networks. Mobility requires special handling of QoS enforcement rules and methods. The LTE QoS Signaling (LQSIG) protocol presented in this paper will allow ensuring resource reservation before using a data path. Operation of the proposed protocol in different mobility scenarios is also explained. The key features of the protocol includes LTE QoS model mapping to QSPEC objects used in reservation and interworking with mobility protocols in the LTE protocol stack, especially with Radio Resource Control (RRC). The basics of an analytical model is proposed in order to determine the blocking probability at the bottlenecks of LTE network.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6671871,no
Energy-efficient and low blocking probability differentiated quality of protection scheme for dynamic elastic optical networks,We proposed a differentiated quality of protection scheme to improve the energy efficiency and to reduce the blocking probability in dynamic elastic optical core networks. Simulation results show significant energy efficiency improvements and a notably lower blocking ratio of this novel scheme compared to 1+1 dedicated protection.,2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6671872,no
Traveling-wave-based line fault location in star-connected multiterminal HVDC systems,"Summary form only given. This paper presents a novel algorithm to determine the location of dc line faults in an HVDC system with multiple terminals connected to a common point, using only the measurements taken at the converter stations. The algorithm relies on the traveling-wave principle, and requires the fault-generated surge arrival times at the converter terminals. With accurate surge arrival times obtained from time-synchronized measurements, the proposed algorithm can accurately predict the faulty segment as well as the exact fault location. Continuous wavelet transform coefficients of the input signal are used to determine the precise time of arrival of traveling waves at the dc line terminals. Performance of the proposed fault-location scheme is analyzed through detailed simulations carried out using the electromagnetic transient simulation software PSCAD. The algorithm does not use reflected waves for its calculations and therefore it is more robust compared to fault location algorithms previously proposed for teed transmission lines. Furthermore, the algorithm can be generalized to handle any number of line segments connected to the star point.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6672083,no
Unambiguous power system dynamic modeling and simulation using modelica tools,"Dynamic modeling and time-domain simulation for power systems is inconsistent across different simulation platforms, which makes it difficult for engineers to consistently exchange models and assess model quality. Therefore, there is a clear need for unambiguous dynamic model exchange. In this article, a possible solution is proposed by using open modeling equation-based Modelica tools. The nature of the Modelica modeling language supports model exchange at the equation-level, this allows for unambiguous model exchange between different Modelica-based simulation tools without loss of information about the model. An example of power system dynamic model exchange between two Modelica-based software Scilab/Xcos and Dymola is presented. In addition, common issues related to simulation, including the extended modeling of complex controls, the capabilities of the DAE solvers and initialization problems are discussed. In order to integrate power system Modelica models into other simulation tools (Matlab/Simulink), the utilization of the FMI Toolbox is investigated as well.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6672476,no
Voltage unbalance emission assessment in radial power systems,"Voltage unbalance (VU) emission assessment is an integral part in the VU management process where loads are allocated a portion of the unbalance absorption capacity of the power system. The International Electrotechnical Commission Report IEC/TR 61000-3-13:2008 prescribes a VU emission allocation methodology establishing the fact that the VU can arise at the point of common connection (PCC) due to both upstream network unbalance and load unbalance. Although this is the case for emission allocation, approaches for post connection emission assessment do not exist except for cases where the load is the only contributor to the VU at the PCC. Such assessment methods require separation of the post connection VU emission level into its constituent parts. In developing suitable methodologies for this purpose, the pre- and post-connection data requirements need to be given due consideration to ensure that such data can be easily established. This paper presents systematic, theoretical bases which can be used to assess the individual VU emission contributions made by the upstream source, asymmetrical line and the load for a radial power system. The methodology covers different load configurations including induction motors. Assessments obtained employing the theoretical bases on the study system were verified by using unbalanced load flow analysis in MATLAB and using DIgSILENT PowerFactory software.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6672490,no
Synthesis of clock trees for Sampled-Data Analog IC blocks,"This paper describes a methodology for automated design of clock trees in Sampled-Data Analog Circuits (SDACs). The current practice in the industry and academia for clock tree design of SDACs is a manual process, which is time-consuming and error-prone. Clock tree design in digital domain, however, is fully automated and is carried out by what we call Clock Tree Synthesis (CTS) software. In spite of some critical differences, SDAC clock tree design problem has fundamental similarities with its digital counterpart. As a result, we were able to construct a methodology for SDACs around a commercial digital CTS software and a set of Perl & Tcl scripts. We will explain our methodology using a 10-bit 180 MHz 2-stage ADC as a test circuit.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6673154,no
On the development of diagnostic test programs for VLIW processors,"Software-Based Self-Test (SBST) approaches have shown to be an effective solution to detect permanent faults, both at the end of the production process, and during the operational phase. When partial reconfiguration is adopted to deal with permanent faults, we also need to identify the faulty module, which is then substituted with a spare one. Software-based Diagnosis techniques can be exploited for this purpose, too. When Very Long Instruction Word (VLIW) processors are addressed, these techniques can effectively exploit the parallelism intrinsic in these architectures. In this paper we propose a new approach that starting from existing detection-oriented programs generates a diagnosis-oriented test program which in most cases is able to identify the faulty module. Experimental results gathered on a case study show the effectiveness of the proposed approach.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6673255,no
Scalable fragile watermarking for image authentication,"Semi-fragile watermarks are used to detect unauthorised changes to an image, whereas tolerating allowed changes such as compression. Most semi-fragile algorithms that tolerate compression assume that because compression only removes the less visually significant data from an image, tampering with any data that would normally be removed by compression cannot affect a meaningful change to the image. Scalable compression allows a single compressed image to produce a variety of reduced resolution or reduced quality images, termed subimages, to suit the different display or bandwidth requirements of each user. However, highly scaled subimages remove a substantial fraction of the data in the original image, so the assumption used by most semi-fragile algorithms breaks down, as tampering with this data allows meaningful changes to the image content. The authors propose a scalable fragile watermarking algorithm for authentication of scalable JPEG2000 compressed images. It tolerates the loss of large amounts of image data because of resolution or quality scaling, producing no false alarms. Yet, it also protects that data from tampering, detecting even minor manipulations other than scaling, and is secure against mark transfer and collage attacks. Experimental results demonstrate this for scaling down to 1/1024th the area of the original or to 1/100th the file size.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6673861,no
Extending IP-XACT to embedded system HW/SW integration,"Typical MPSoC FPGA product design is a rigid waterfall process proceeding one-way from HW to SW design. Any changes to HW trigger the SW project re-creation from the beginning. When several product variations or speculative development time exploration is required, the disk bloats easily with hundreds of Board Support Package (BSP), configuration and SW project files. In this paper, we present an IP-XACT based design flow that solves the problems by agile re-use of HW and SW components, automation and single golden reference source for information. We also present new extensions to IP-XACT since the standard lacks SW related features. Three use cases demonstrate how the BSP is changed, an application is moved to another processor and a function is moved from SW implementation to a HW accelerator. Our flow reduces the design time to one third compared to the conventional FPGA flow, the number of automated design phases is doubled and any manual error prone data transfer between HW and SW tools is completely avoided.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6675264,no
Reliability comparison of various regenerating codes for cloud services,"In this paper, we consider the reliability comparison of various regenerating codes for cloud services. By simulations, we compare the performance of such regenerating codes as MBR codes, MSR codes, local reconstruction codes, and LT regenerating codes, in terms of storage overhead, repair read cost, and repair failure probability. We give some discussions on what must be done in the near future.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6675444,no
Evaluating IEEE 802.11s mesh channel switching using open source solution,"To avoid interference from a detected radar signal, or to reassign mesh station (STA) channels to ensure the connectivity, IEEE 802.11s defines a procedure on how to propagate the channel switch attempt throughout the mesh network, known as mesh Basic Service Set (MBSS) channel switch. Wireless Mesh Network (WMN) that utilizes single-radio nodes with omni-directional and directional antennas is ideally suited for rural area where cost and simplicity take precedence over service quality. However, single-channel single-radio mesh network is easily affected by the interference from its neighborhood, especially from co-located Wi-Fi deployment or other devices operating in the same frequency channel. Thus, the ability to switch to a new channel for self healing is indeed appealing to single-channel single-radio mesh network. The implementation of 802.11s in Linux kernel is available since the year 2007, but the MBSS channel switching has yet to be implemented. This paper describes the MBSS channel switching in details and also discusses our efforts to implement this in the Linux wireless subsystem. Our implementation are verified and evaluated in our experimental testbed.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676371,no
Constraint-Based Autonomic Reconfiguration,"Declarative, object-oriented configuration management systems are widely used by system administrators. Recently, logical constraints have been added to such systems to facilitate the automatic generation of configurations. However, there is no facility for reasoning about subsequent reconfigurations, such as those needed in an autonomic configuration system. In this paper we develop a number of language primitives, which facilitate not only one-off configuration tasks, but also subsequent reconfigurations in which the previous state of the system is taken into account. We show how it can be directly integrated into a declarative language, and assess its impact on performance.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676497,no
FESAS: Towards a Framework for Engineering Self-Adaptive Systems,"The complexity and size of information systems are growing, resulting in an increasing effort for maintenance. Self-adaptive systems (SAS) that autonomously adapt to changes in the environment or in the system itself (e.g. disfunction of components) can be a solution. So far, the development of SAS is frequently tailored to specific use case requirements. The creation of frameworks with reusable process elements and system components is often neglected. However, with such a framework developing SAS would become faster and less error prone. This work addresses this gap by providing a framework for engineering SAS.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676516,no
QoS-Aware VM Placement in Multi-domain Service Level Agreements Scenarios,"Virtualization technologies of Infrastructure-as-a- Service enable the live migration of running Virtual Machines (VMs) to achieve load balancing, fault-tolerance and hardware consolidation in data centers. However, the downtime/service unavailability due to live migration may be substantial with relevance to the customers' expectations on responsiveness, as the latter are declared in established Service Level Agreements (SLAs). Moreover, it may cause significant (potentially exponential) SLA violation penalties to its associated higher- level domains (Platform-as-a-Service and Software-as-a-Service). Therefore, VM live migration should be managed carefully. In this paper, we present the OpenStack version of the Generic SLA Manager, alongside its strategies for VM selection and allocation during live migration of VMs. We simulate a use case where IaaS (OpenStack-SLAM) and PaaS (OpenShift) are combined, and assess performance and efficiency of the aforementioned VM placement strategies, when a multi-domain SLA pricing & penalty model is involved. We find that our proposal is efficient in managing trade-offs between the operational objectives of service providers (including financial considerations) and the customers' expected QoS requirements.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676754,no
An Empirical Study of API Stability and Adoption in the Android Ecosystem,"When APIs evolve, clients make corresponding changes to their applications to utilize new or updated APIs. Despite the benefits of new or updated APIs, developers are often slow to adopt the new APIs. As a first step toward understanding the impact of API evolution on software ecosystems, we conduct an in-depth case study of the co-evolution behavior of Android API and dependent applications using the version history data found in github. Our study confirms that Android is evolving fast at a rate of 115 API updates per month on average. Client adoption, however, is not catching up with the pace of API evolution. About 28% of API references in client applications are outdated with a median lagging time of 16 months. 22% of outdated API usages eventually upgrade to use newer API versions, but the propagation time is about 14 months, much slower than the average API release interval (3 months). Fast evolving APIs are used more by clients than slow evolving APIs but the average time taken to adopt new versions is longer for fast evolving APIs. Further, API usage adaptation code is more defect prone than the one without API usage adaptation. This may indicate that developers avoid API instability.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676878,no
"How We Design Interfaces, and How to Assess It","Interfaces are widely used in Java applications as central design elements for modular programming to increase program reusability and to ease maintainability of software systems. Despite the importance of interfaces and a considerable research effort that has investigated code quality and concrete classes' design, few works have investigated interfaces' design. In this paper, we empirically study interfaces' design and its impact on the design quality of implementing classes (i.e., class cohesion) analyzing twelve Java object-oriented applications. In this study we propose the """"Interface-Implementations Model"""" that we use to adapt class cohesion metrics to assess the cohesion of interfaces based on their implementations. Moreover, we use other metrics that evaluate the conformance of interfaces to the well-known design principles """"Program to an Interface, not an implementation"""" and """"Interface Segregation Principle"""". The results show that software developers abide well by the interface design principles cited above, but they neglect the cohesion property. The results also show that such design practices of interfaces lead to a degraded cohesion of implementing classes, where these latter would be characterized by a worse cohesion than other classes.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676879,no
DRONE: Predicting Priority of Reported Bugs by Multi-factor Analysis,"Bugs are prevalent. To improve software quality, developers often allow users to report bugs that they found using a bug tracking system such as Bugzilla. Users would specify among other things, a description of the bug, the component that is affected by the bug, and the severity of the bug. Based on this information, bug triagers would then assign a priority level to the reported bug. As resources are limited, bug reports would be investigated based on their priority levels. This priority assignment process however is a manual one. Could we do better? In this paper, we propose an automated approach based on machine learning that would recommend a priority level based on information available in bug reports. Our approach considers multiple factors, temporal, textual, author, related-report, severity, and product, that potentially affect the priority level of a bug report. These factors are extracted as features which are then used to train a discriminative model via a new classification algorithm that handles ordinal class labels and imbalanced data. Experiments on more than a hundred thousands bug reports from Eclipse show that we can outperform baseline approaches in terms of average F-measure by a relative improvement of 58.61%.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676891,no
Predicting Bugs Using Antipatterns,"Bug prediction models are often used to help allocate software quality assurance efforts. Software metrics (e.g., process metrics and product metrics) are at the heart of bug prediction models. However, some of these metrics like churn are not actionable, on the contrary, antipatterns which refer to specific design and implementation styles can tell the developers whether a design choice is """"poor"""" or not. Poor designs can be fixed by refactoring. Therefore in this paper, we explore the use of antipatterns for bug prediction, and strive to improve the accuracy of bug prediction models by proposing various metrics based on antipatterns. An additional feature to our proposed metrics is that they take into account the history of antipatterns in files from their inception into the system. Through a case study on multiple versions of Eclipse and ArgoUML, we observe that (i) files participating in antipatterns have higher bug density than other files, (ii) our proposed antipattern based metrics can provide additional explanatory power over traditional metrics, and (iii) improve the F-measure of cross-system bug prediction models by 12.5% in average. Managers and quality assurance personnel can use our proposed metrics to better improve their bug prediction models and better focus testing activities and the allocation of support resources.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676898,no
Will Fault Localization Work for These Failures? An Automated Approach to Predict Effectiveness of Fault Localization Tools,"Debugging is a crucial yet expensive activity to improve the reliability of software systems. To reduce debugging cost, various fault localization tools have been proposed. A spectrum-based fault localization tool often outputs an ordered list of program elements sorted based on their likelihood to be the root cause of a set of failures (i.e., their suspiciousness scores). Despite the many studies on fault localization, unfortunately, however, for many bugs, the root causes are often low in the ordered list. This potentially causes developers to distrust fault localization tools. Recently, Parnin and Orso highlight in their user study that many debuggers do not find fault localization useful if they do not find the root cause early in the list. To alleviate the above issue, we build an oracle that could predict whether the output of a fault localization tool can be trusted or not. If the output is not likely to be trusted, developers do not need to spend time going through the list of most suspicious program elements one by one. Rather, other conventional means of debugging could be performed. To construct the oracle, we extract the values of a number of features that are potentially related to the effectiveness of fault localization. Building upon advances in machine learning, we process these feature values to learn a discriminative model that is able to predict the effectiveness of a fault localization tool output. In this preliminary work, we consider an output of a fault localization tool to be effective if the root cause appears in the top 10 most suspicious program elements. We have experimented our proposed oracle on 200 faulty programs from Space, NanoXML, XML-Security, and the 7 programs in Siemens test suite. Our experiments demonstrate that we could predict the effectiveness of fault localization tool with a precision, recall, and F-measure (harmonic mean of precision and recall) of 54.36%, 95.29%, and 69.23%. The numbers indicate that many ineffective - ault localization instances are identified correctly, while only very few effective ones are identified wrongly.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676902,no
Supporting and Accelerating Reproducible Research in Software Maintenance Using TraceLab Component Library,"Research studies in software maintenance are notoriously hard to reproduce due to lack of datasets, tools, implementation details (e.g., parameter values, environmental settings) and other factors. The progress in the field is hindered by the challenge of comparing new techniques against existing ones, as researchers have to devote a lot of their resources to the tedious and error-prone process of reproducing previously introduced approaches. In this paper, we address the problem of experiment reproducibility in software maintenance and provide a long term solution towards ensuring that future experiments will be reproducible and extensible. We conducted a mapping study of a number of representative maintenance techniques and approaches and implemented them as a library of experiments and components that we make publicly available with TraceLab, called the Component Library. The goal of these experiments and components is to create a body of actionable knowledge that would (i) facilitate future research and would (ii) allow the research community to contribute to it as well. In addition, to illustrate the process of using and adapting these techniques, we present an example of creating new techniques based on existing ones, which produce improved results.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676904,no
How Does Context Affect the Distribution of Software Maintainability Metrics?,"Software metrics have many uses, e.g., defect prediction, effort estimation, and benchmarking an organization against peers and industry standards. In all these cases, metrics may depend on the context, such as the programming language. Here we aim to investigate if the distributions of commonly used metrics do, in fact, vary with six context factors: application domain, programming language, age, lifespan, the number of changes, and the number of downloads. For this preliminary study we select 320 nontrivial software systems from Source Forge. These software systems are randomly sampled from nine popular application domains of Source Forge. We calculate 39 metrics commonly used to assess software maintainability for each software system and use Kruskal Wallis test and Mann-Whitney U test to determine if there are significant differences among the distributions with respect to each of the six context factors. We use Cliff's delta to measure the magnitude of the differences and find that all six context factors affect the distribution of 20 metrics and the programming language factor affects 35 metrics. We also briefly discuss how each context factor may affect the distribution of metric values. We expect our results to help software benchmarking and other software engineering methods that rely on these commonly used metrics to be tailored to a particular context.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676906,no
Can Refactoring Cyclic Dependent Components Reduce Defect-Proneness?,"Previous studies have shown that dependency cycles contain significant number of defects, defect-prone components and account for the most critical defects. Thereby, demonstrating the impacts of cycles on software reliability. This preliminary study investigates the variables in a cyclic dependency graph that relate most with the number of defect-prone components in such graphs so as to motivate and guide decisions for possible system refactoring. By using network analysis and statistical methods on cyclic graphs of Eclipse and Apache-Active MQ, we have examined the relationships between the size and distance measures of cyclic dependency graphs. The size of the cyclic graphs consistently correlates more with the defect-proneness of components in these systems than other measures. Showing that adding new components to and/or creating new dependencies within an existing cyclic dependency structures are stronger in increasing the likelihood of defect-proneness. Our next study will investigate whether there is a cause and effect between refactoring (breaking) cyclic dependencies and defect-proneness of affected components.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676922,no
"Determining """"Grim Reaper"""" Policies to Prevent Languishing Bugs","Long-lived software products commonly have a large number of reported defects, some of which may not be fixed for a lengthy period of time, if ever. These so-called languishing bugs can incur various costs to project teams, such as wasted time in release planning and in defect analysis and inspection. They also result in an unrealistic view of the number of bugs still to be fixed at a given time. The goal of this work is to help software practitioners mitigate their costs from languishing bugs by providing a technique to predict and pre-emptively close them. We analyze defect fix times from an ABB program and the Apache HTTP server, and find that both contain a substantial number of languishing bugs. We also train decision tree classification models to predict whether a given bug will be fixed within a desired time period. We propose that an organization could use such a model to form a """"grim reaper"""" policy, whereby bugs that are predicted to become languishing will be pre-emptively closed. However, initial results are mixed, with models for the ABB program achieving F-scores of 63-95%, while the Apache program has Fscores of 21-59%.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676926,no
Towards a Scalable Cloud Platform for Search-Based Probabilistic Testing,"Probabilistic testing techniques that sample input data at random from a probability distribution can be more effective at detecting faults than deterministic techniques. However, if overly large (and therefore expensive) test sets are to be avoided, the probability distribution from which the input data is sampled must be optimised to the particular software-under-test. Such an optimisation process is often resource-intensive. In this paper, we present a prototypical cloud platform-and architecture-that permits the optimisation of such probability distributions in a scalable, distributed and robust manner, and thereby enables cost-effective probabilistic testing.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676937,no
TRINITY: An IDE for the Matrix,"Digital forensics software often has to be changed to cope with new variants and versions of file formats. Developers reverse engineer the actual files, and then change the source code of the analysis tools. This process is error-prone and time consuming because the relation between the newly encountered data and how the source code must be changed is implicit. TRINITY is an integrated debugging environment which makes this relation explicit using the DERRIC DSL for describing file formats. TRINITY consists of three simultaneous views: 1) the runtime state of an analysis, 2) a hex view of the actual data, and 3) the file format description. Cross-view trace ability links allow developers to better understand how the file format description should be modified. TRINITY aims to make the process of adapting digital forensics software more effective and efficient.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676947,no
Improving Statistical Approach for Memory Leak Detection Using Machine Learning,"Memory leaks are major problems in all kinds of applications, depleting their performance, even if they run on platforms with automatic memory management, such as Java Virtual Machine. In addition, memory leaks contribute to software aging, increasing the complexity of software maintenance. So far memory leak detection was considered to be a part of development process, rather than part of software maintenance. To detect slow memory leaks as a part of quality assurance process or in production environments statistical approach for memory leak detection was implemented and deployed in a commercial tool called Plumbr. It showed promising results in terms of leak detection precision and recall, however, even better detection quality was desired. To achieve this improvement goal, classification algorithms were applied to the statistical data, which was gathered from customer environments where Plumbr was deployed. This paper presents the challenges which had to be solved, method that was used to generate features for supervised learning and the results of the corresponding experiments.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676953,no
Implementation of semi-virtual Multiple-Master/Multiple-Slave system,"Building an experimental robotic setup can be a very tedious, prone to hardware faults and expensive process. A common way to circumvent some of this problems is to model a part or entire system in software. Moreover, virtual environments can be the only option to model hazardous or inaccessible sites. However, implementation of teleoperated robotic systems with force feedback exhibits additional problems. The human-robot interfaces should exist in hardware and this in turn requires simulated system to work in a proximity with a real-time. In this paper we describe a successful implementation of such a system in Gazebo simulator and Robot Operating System. We built an experimental Multiple-Master/Multiple-Slave setup in virtual environment for peg-in-hole task that consists of two 7-DOF Schunk LWA-3 robots and a common object to manipulate. To display forces to operators two SensAble PHANToM devices were utilized.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6677356,no
Evaluating Neutron Induced SEE in SRAM-Based FPGA Protected by Hardware- and Software-Based Fault Tolerant Techniques,"This paper presents an approach to detect SEEs in SRAM-based FPGAs by using software-based techniques combined with a nonintrusive hardware module. We implemented a MIPS-based soft-core processor in a Virtex5 FPGA and hardened it with software- and hardware-based fault tolerance techniques. First fault injection in the configuration memory bitstream was performed in order to verify the feasibility of the proposed approach, detection rates and diagnosis. Furthermore a neutron radiation experiment was performed at LANSCE. Results demonstrate the possibility of employing more flexible fault tolerant techniques to SRAM-based FPGAs with a high detection rate. Comparisons between bitstream fault injection and radiation test is also presented.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6678297,no
Use of hydroxyl-modified carbon nanotubes for detecting SF<sub>6</sub> decomposition products under partial discharge in gas insulated switchgear,"Gas-insulated switchgear (GIS) has inherent internal defects that may result in partial discharge (PD) and the eventual development of equipment faults. PD in GIS can lead to the generation of multiple decomposition products of SF<sub>6</sub>, and the detection and analysis of these decomposition products is important for fault diagnosis. In this paper, a molecular dynamics simulation software package, Materials Studio (MS), is used to model accurately the processes by which single-walled carbon nanotubes modified by hydroxyl (SWNT-OH) adsorb the main decomposition products of SF<sub>6</sub> (SOF<sub>2</sub>, SO<sub>2</sub>F<sub>2</sub>, SO<sub>2</sub> and CF<sub>4</sub>) generated by PD. In addition, experimental studies are performed to validate the predicted gas-sensing characteristics. The theoretical calculations and experimental results both indicate that, of the four gases, SWNT-OH showed the fastest response time and highest sensitivity to SO<sub>2</sub>. The sensitivities of SWNT-OH to the other gases were low, and response times long. We conclude that SWNT-OH shows good sensitivity and selectivity to SO<sub>2</sub>.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6678876,no
Integrating a market-based model in trust-based service systems,"The reputation-based trust mechanism is a way to assess the trustworthiness of offered services, based on the feedback obtained from their users. In the absence of appropriate safeguards, service users can still manipulate this feedback. Auction mechanisms have already addressed the problem of manipulation by markettrading participants. When auction mechanisms are applied to trust systems, their interaction with the trust systems and associated overhead need to be quantitatively evaluated. This paper proposes two distributed architectures based on centralized and hybrid computing for integrating an auction mechanism with the trust systems. The empirical evaluation demonstrates how the architectures help to discourage users from giving untruthful feedback and reduce the overhead costs of the auction mechanisms.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6678901,no
Control of Multiple Packet Schedulers for Improving QoS on OpenFlow/SDN Networking,"Packet scheduling is essential to properly support applications on Software-Defined Networking (SDN) model. However, on OpenFlow/SDN, QoS is only performed with bandwidth guarantees and by a well-known FIFO scheduling. Facing this limitation, this paper presents the QoSFlow proposal, which controls multiple packet schedulers of Linux kernel and improve the flexibility of QoS control. The paper assesses QoSFlow performance, by analysing response time of packet scheduler operations running on datapath level, maximum bandwidth capacity, hardware resource utilization rate, bandwidth isolation and QoE. Our outcomes show an increase more than 48% on PSNR value of QoE by using SFQ scheduling.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6680563,no
Machine learning approaches for predicting software maintainability: a fuzzy-based transparent model,"Software quality is one of the most important factors for assessing the global competitive position of any software company. Thus, the quantification of the quality parameters and integrating them into the quality models is very essential.Many attempts have been made to precisely quantify the software quality parameters using various models such as Boehm's Model, McCall's Model and ISO/IEC 9126 Quality Model. A major challenge, although, is that effective quality models should consider two types of knowledge: imprecise linguistic knowledge from the experts and precise numerical knowledge from historical data.Incorporating the experts' knowledge poses a constraint on the quality model; the model has to be transparent.In this study, the authorspropose a process for developing fuzzy logic-based transparent quality prediction models.They applied the process to a case study where Mamdani fuzzy inference engine is used to predict software maintainability.Theycompared the Mamdani-based model with other machine learning approaches.The resultsshow that the Mamdani-based model is superior to all.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6680574,no
"Algorithm Parallelization Using Software Design Patterns, an Embedded Case Study Approach","Multicore embedded systems introduce new opportunities and challenges. Scaling of computational power is one of the main reasons for transition to a multicore environment. In most cases parallelization of existing algorithms is time consuming and error prone, dealing with low-level constructs. Migrating principles of object-oriented design patterns to parallel embedded software avoids this. We propose a top-down approach for refactoring existing sequential to parallel algorithms in an intuitive way, avoiding the usage of locking mechanisms. We illustrate the approach on the well known Fast Fourier Transformation algorithm. Parallel design patterns, such as Map Reduce, Divide-and-Conquer and Task Parallelism assist to derive a parallel approach for calculating the Fast Fourier Transform. By combining these design patterns, a robust and better performing application is obtained.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6681274,no
An Empirical Study of Client-Side JavaScript Bugs,"Context: Client-side JavaScript is widely used in web applications to improve user-interactivity and minimize client-server communications. Unfortunately, web applications are prone to JavaScript faults. While prior studies have demonstrated the prevalence of these faults, no attempts have been made to determine their root causes and consequences. Objective: The goal of our study is to understand the root causes and impact of JavaScript faults and how the results can impact JavaScript programmers, testers and tool developers. Method: We perform an empirical study of 317 bug reports from 12 bug repositories. The bug reports are thoroughly examined to classify and extract information about the fault's cause (the error) and consequence (the failure and impact). Result: The majority (65%) of JavaScript faults are DOM-related, meaning they are caused by faulty interactions of the JavaScript code with the Document Object Model (DOM). Further, 80% of the highest impact JavaScript faults are DOM-related. Finally, most JavaScript faults originate from programmer mistakes committed in the JavaScript code itself, as opposed to other web application components such as the server-side or HTML code. Conclusion: Given the prevalence of DOM-related faults, JavaScript programmers need development tools that can help them reason about the DOM. Also, testers should prioritize detection of DOM-related faults as most high impact faults belong to this category. Finally, developers can use the error patterns we found to design more powerful static analysis tools for JavaScript.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6681338,no
Evaluating Software Product Metrics with Synthetic Defect Data,"Source code metrics have been used in past research to predict software quality and focus tasks such as code inspection. A large number of metrics have been proposed and implemented in consumer metric software, however, a smaller, more manageable subset of these metrics may be just as suitable for accomplishing specific tasks as the whole. In this research, we introduce a mathematical model for software defect counts conditioned on product metrics, along with a method for generating synthetic defect data that chooses parameters for this model to match statistics observed in empirical bug datasets. We then show how these synthetic datasets, when combined with measurements from actual software systems, can be used to demonstrate how sets of metrics perform in various scenarios. Our preliminary results suggest that a small number of source code metrics conveys similar information as a larger set, while providing evidence for the independence of traditional software metric classifications such as size and coupling.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6681361,no
Tools to Support Systematic Literature Reviews in Software Engineering: A Mapping Study,Background: Systematic literature reviews (SLRs) have become an established methodology in software engineering (SE) research however they can be very time consuming and error prone. Aim: The aims of this study are to identify and classify tools that can help to automate part or all of the SLR process within the SE domain. Method: A mapping study was performed using an automated search strategy plus snowballing to locate relevant papers. A set of known papers was used to validate the search string. Results: 14 papers were accepted into the final set. Eight presented text mining tools and six discussed the use of visualisation techniques. The stage most commonly targeted was study selection. Only two papers reported an independent evaluation of the tool presented. The majority were evaluated through small experiments and examples of their use. Conclusions: A variety of tools are available to support the SLR process although many are in the early stages of development and usage.,2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6681371,no
Cost Effectiveness of Unit Testing: A Case Study in a Financial Institution,"This paper presents a case study on the cost effectiveness of unit testing in the context of a financial institution in Costa Rica. The study comprises four main steps: choosing a software application, implementing unit tests for this application, identifying prevented defects, and performing a cost and savings analysis. The impact of unit testing on the quality of software is assessed in terms of early defect detection, and the impact on the overall cost of software is evaluated based on the cost of developing the unit tests and the savings derived from the reduction of defects in later phases of the application development lifecycle. Our results indicate that while unit testing could help early defect detection, the monetary cost associated to unit testing would be higher than the monetary savings, in the particular context of the financial software studied, and under the limitations of our cost-savings model.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6681377,no
Constructing Defect Predictors and Communicating the Outcomes to Practitioners,"Background: An alternative to expert-based decisions is to take data-driven decisions and software analytics is the key enabler for this evidence-based management approach. Defect prediction is one popular application area of software analytics, however with serious challenges to deploy into practice. Goal: We aim at developing and deploying a defect prediction model for guiding practitioners to focus their activities on the most problematic parts of the software and improve the efficiency of the testing process. Method: We present a pilot study, where we developed a defect prediction model and different modes of information representation of the data and the model outcomes, namely: commit hotness ranking, error probability mapping to the source and visualization of interactions among teams through errors. We also share the challenges and lessons learned in the process. Result: In terms of standard performance measures, the constructed defect prediction model performs similar to those reported in earlier studies, e.g. 80% of errors can be detected by inspecting 30% of the source. However, the feedback from practitioners indicates that such performance figures are not useful to have an impact in their daily work. Pointing out most problematic source files, even isolating error-prone sections within files are regarded as stating the obvious by the practitioners, though the latter is found to be helpful for activities such as refactoring. On the other hand, visualizing the interactions among teams, based on the errors introduced and fixed, turns out to be the most helpful representation as it helps pinpointing communication related issues within and across teams. Conclusion: The constructed predictor can give accurate information about the most error prone parts. Creating practical representations from this data is possible, but takes effort. The error prediction research done in Elektrobit Wireless Ltd is concluded to be useful and we will further improve the present- tions made from the error prediction data.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6681379,no
FChain: Toward Black-Box Online Fault Localization for Cloud Systems,"Distributed applications running inside cloud systems are prone to performance anomalies due to various reasons such as resource contentions, software bugs, and hardware failures. One big challenge for diagnosing an abnormal distributed application is to pinpoint the faulty components. In this paper, we present a black-box online fault localization system called FChain that can pinpoint faulty components immediately after a performance anomaly is detected. FChain first discovers the onset time of abnormal behaviors at different components by distinguishing the abnormal change point from many change points caused by normal workload fluctuations. Faulty components are then pinpointed based on the abnormal change propagation patterns and inter-component dependency relationships. FChain performs runtime validation to further filter out false alarms. We have implemented FChain on top of the Xen platform and tested it using several benchmark applications (RUBiS, Hadoop, and IBM System S). Our experimental results show that FChain can quickly pinpoint the faulty components with high accuracy within a few seconds. FChain can achieve up to 90% higher precision and 20% higher recall than existing schemes. FChain is non-intrusive and light-weight, which imposes less than 1% overhead to the cloud system.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6681572,no
Improvement of Peach Platform to Support GUI-Based Protocol State Modeling,"This Article describes how to improve model-based testing on the Peach platform, and it could make protocol security experts and testers describe network protocol state machine models and carry on a model-based testing much easier than ever before. This paper describes:(1) the graphical user interface of protocol state machine and the process of modeling, (2) the algorithm converting protocol state machine in graphic format to SCXML format, (3) the algorithm converting protocol state machine in SCXML format to the Pit File format. The Pit File generated could be loads into Peach platform directly to test target software. The contribution of our work could ease protocol security experts from tedious and error-prone testing work: creative research work completed by them, and tedious Pit File syntax learning and debugging is accomplished by the computer. Therefore, it is possible to focus on the description of the protocol state machine, rather than the tedious Pit File syntax details, and improve their work efficiency. Besides, this method applies the SCXML as intermediate files between graphical user interface and Pit File, so that has a high flexibility.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6682202,no
Fuzzy-neuro Health Monitoring System for HVAC system variable-air-volume unit,"For Indoor Smart Grids (ISG), the proper operation of building environmental systems is essential to energy efficiency, so automatic detection and classification of abnormal conditions is important. The application of computational intelligence tools to a building's environmental systems that include the Building Automation System (BAS) and Heating Ventilating and Air Conditioning (HVAC) loads, is used to develop Automatic Building Diagnostic Software (ABDS) Tools for health monitoring, fault detection, and diagnostics. A novel Health Monitoring System (HMS) for a Variable Air Volume Unit is developed using fuzzy logic to detect abnormal operating conditions and to generate fault signatures for various fault types. Artificial Neural Network software is applied to fault signatures to classify the fault type. The HMS is tested with simulated data and actual BAS data. The system created was demonstrated to recognize faults and to accurately classify the various fault signatures for test faults of interest.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6682499,no
A simple predictive method to estimate flicker,"This paper presents a method for predicting the flicker level generated by high power load in a distribution system. Through the development of the Flickermeter that met the IEC requirements, a reference curve is obtained based on the variation of voltage and the amount of changes per minute of the load. The purpose of this prediction is to determine if the load operation will produce flicker before being connected to the electrical system. This analysis cannot be developed by power system software due to the Flickermeter complexity and amount of data required for processing.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6682605,no
Finding assignable cause in medium voltage network by statistical process control,"The current of outgoing feeders are very important data transmitted over SCADA system. Monitoring of these currents can help dispatching engineers to detect abnormality in energy consumption trend and minor faults in distribution network. Statistical process control (SPC) is one of the capable approaches which can be used for this purpose. Statistical process control is based on categorizing variations into assignable causes and random causes. In current paper we described the methods which were used for finding assignable causes in load trend and short time load variation in Alborz province power distribution company pilot project. Although this approach is not developed completely and some theoretical and practical challenges should be met before extending this project to all feeders, we hope completing this study can help engineers to developing more capable network monitoring softwares.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6683210,no
Calculation and analysis of customer dissatisfaction index for reliability studies in gilan electric distribution network,"Reliable electric energy is one of the most important necessities for customers and there are high correlation between customer based reliability indices and customer satisfaction. Electric power interruptions have not only economics problems but also it makes social and mental difficulties. However customers' sensitive is different against interruption. Culture and living stiles of customers have significant effects on their satisfaction from utilities. For consideration of customer view point against interruptions, it seems that Customer Dissatisfaction Index (CDI) should define and enter as a reliability index. Based on this index, reliability enhancement strategies can be planned for maximizing customer satisfaction index. In this paper for assessing customer dissatisfaction index, questionnaires are designed. As case study these questionnaires have been filled by domestic customers in Rasht, a big and costal city in north of Iran. Gathered data has entered in SPSS software and customers' reply stochastic indices have obtained and analyzed. According to results, sharp threshold values for customer satisfied regarding reliability of supply were found. Results show customers' sensitive for number and duration of outages and transient-fault depends on time a day or seasons, in this paper time based Customer Dissatisfaction Index has been analyzed.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6683555,no
Quality Assessment of Software as a Service on Cloud Using Fuzzy Logic,"Cloud computing is a business model which provides on demand services on the pay-per-use premise. Software as a Service (SaaS) is one of the delivery models for cloud computing, where software ownership by the SaaS provider is isolated from its use by the SaaS customer. The notion of quality is central to any service provision. Also, it is important to evaluate the quality of SaaS in order to be able to improve it. Traditional software engineering quality models are not effusively suitable for this purpose due to difference in the nature of software and service. In the past, a few approaches to service quality estimation have been proposed. Some of these approaches extend quality characteristics from existing quality models and even devise SaaS quality metrics, while others discuss quality around Service Level Agreement (SLA) and Quality of Service (QoS) parameters. In this paper, some representative quality factors have been identified by analyzing literature and a model based on fuzzy logic has been proposed to assess SaaS quality. Such a model of quality criteria may provide a ground for more comprehensive quality model which may assist a SaaS customer to choose a higher quality service from available services on cloud; the quality model may also serve as a guideline to SaaS provider to improve the quality of service provided.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6684439,no
The Erlang approach to concurrent system development,"The prevalence of multi-core processors means application developers can no longer ignore concurrency and its attendant problems of data races, deadlock, safety, and liveness. Imperative languages such as Java and C, based on shared, mutable state, have added locks, semaphores and condition variables to address these problems; unfortunately, these locking approaches are notoriously error-prone. Functional (""""single assignment"""") languages with immutable state have been promoted as tools to mitigate these problems. In particular, Erlang, a functional language with roots in Prolog, has been used by Erickson, Ltd., to develop robust, concurrent, fault-tolerant, communications switches (31ms downtime per year). This workshop will introduce Erlang to educators interested in the language per se as well as those focusing on concurrent system development. The goal is to encourage the use of both imperative and functional languages in teaching about concurrency. Participants will install the Erlang system on their notebooks so as to engage in activities along with the organizer. Both sequential and concurrent systems - small but complete - will be developed in conjunction with the presentations. Time is allocated at the end of the workshop to discuss the pedagogical issues involved in adopting Erlang or similar technology.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6684776,no
Improving Modular Reasoning on Preprocessor-Based Systems,"Preprocessors are often used to implement the variability of a Software Product Line (SPL). Despite their widespread use, they have several drawbacks like code pollution, no separation of concerns, and error-prone. Virtual Separationof Concerns (VSoC) has been used to address some of thesepreprocessor problems by allowing developers to hide featurecode not relevant to the current maintenance task. However, different features eventually share the same variables and methods, so VSoC does not modularize features, since developers do not know anything about hidden features. Thus, the maintenance of one feature might break another. Emergent Interfaces (EI) capture dependencies between a feature maintenance point and parts of other feature implementation, but they do not provide an overall feature interface considering all parts in an integrated way. Thus, we still have the feature modularization problem. To address that, we propose Emergent Feature Interfaces (EFI) that complement EI by treating feature as a module in order to improve modular reasoning on preprocessor-based systems. EFI capture dependencies among entire features, with the potential of improving productivity. Our proposal, implemented in an opensource tool called Emergo, is evaluated with preprocessor-based systems. The results of our study suggest the feasibility and usefulness of the proposed approach.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6685786,no
A Reference Architecture Based on Reflection for Self-Adaptive Software,"Self-adaptive Software (SaS) presents specific characteristics compared to traditional ones, as it makes possible adaptations to be incorporated at runtime. These adaptations, when manually performed, normally become an onerous, error-prone activity. In this scenario, automated approaches have been proposed to support such adaptations; however, the development of SaS is not a trivial task. In parallel, reference architectures are reusable artifacts that aggregate the knowledge of architectures of software systems in specific domains. They have facilitated the development, standardization, and evolution of systems of those domains. In spite of their relevance, in the SaS domain, reference architectures that could support a more systematic development of SaS are not found yet. Considering this context, the main contribution of this paper is to present a reference architecture based on reflection for SaS, named RA4SaS (Reference Architecture for SaS). Its main purpose is to support the development of SaS that presents adaptations at runtime. To show the viability of this reference architecture, a case study is presented. As result, it has been observed that RA4SaS has presented good perspective to efficiently contribute to the area of SaS.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6685798,no
Polynomial Models Identification Using Real Data Acquisition Applied to Didactic System,"Models of real systems are of fundamental importance for its analysis, making it possible to simulate or predict its behavior. Additionally, advanced techniques for controller design, optimization, monitoring, fault detection and diagnosis components are also based on process models. One of the most used techniques to model a system is by identification. System identification or process identification is the field of mathematical modeling of systems, in which the parameters are obtained from test or experimental data. Given the importance of obtaining a model able to represent the dynamics of real processes, we developed a software that aggregates identification algorithms using Least squares (LS), Least squares Extended (ELS), Generalized Least Squares (GLS) Recursive least squares with Compensator Polarization (BCRLS). This identification package is used in this paper to identify an educational level plant. Its actual data was inserted in the package and thus, results from different identification techniques implemented in the algorithm were compared. All steps necessary to carry out the identification and analysis of the autocorrelation of the output data for the definition of the sampling period, the design of excitation signals and data collection were taken in consideration. The conclusions reached are that the software provides consistency and the implemented algorithms return a model capable of representing the linear part of the system's dynamics.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6686105,no
An Architecture for Justified Assessments of Service Provider Reputation,"In a service-oriented system, an accurate assessment of reputation is essential for selecting between alternative providers. In many cases, providers have differing characteristics that must be considered alongside reliability, including their cost, experience, quality, and use of sub-providers, etc. Existing methods for reputation assessment are limited in terms of the extent to which the full interaction history and context is considered. While factors such as cost and quality might be considered, the assessment of reputation is typically based only on a combination of direct experience and recommendations from third parties, without considering the wider context. Furthermore, reputation is typically expressed as a simple numerical score or probability estimate with no rationale for the reasoning behind it, and there is no opportunity for the user to interrogate the assessment. Existing approaches exclude from consideration a wide range of information, about the context of providers' previous actions, that could give useful information to a user in selecting a service provider. For example, there may have been mitigating circumstances for past failures, or a provider may have changed their organisational affiliation. In this paper we argue that provenance records are a rich source of information on which a more nuanced reputation mechanism can be based. Specifically, the paper makes two main contributions: (i) we provide an analysis of the challenges and open research questions that must be addressed in achieving a rich provenance-based reputation mechanism, and (ii) we define an architecture in which the results of these challenges fit together with existing technologies to enable provenance-based reputation.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6686286,no
Invited talk: How the Fundamental Assurance Question pervades certification,"Assurance Cases are promoted as a means by which to present an argument for why a system is sufficiently dependable (alternate terms for the same concept include Dependability Cases, Safety Cases when the concern is safety, Security Cases, etc.). The purpose of such an argument is typically to inform a decision maker, often in the context of a key certification decision, so he/she will be better able to make that decision. Examples of such decisions include whether to deploy a system, whether to make an upgrade to an existing system, whether to advance a system to the next phase in its development. Assurance Cases are widely practiced in Europe, and are receiving growing attention in North America. For software systems in particular, an assurance-case-based approach is often contrasted to a standards-based approach, the latter being characterized as more prescriptive in specifying the process and techniques to be applied to sufficiently assure software. The proponents of an assurance-case-based approach point out that the need to construct a sufficiently convincing Assurance Case puts the onus on the provider of the software to present the argument for its dependability, as compared to putting the onus on the regulator to have described in advance a sufficient process to be followed by the provider in their development of software. The distinction is not as clear-cut as it might at first seem. Both approaches have the need to assess by how much the outcomes of assurance activities (e.g., testing; code review; fault tree analysis; model-checking) raise confidence in decisions made about the system. For a standards-based approach, how is it possible to determine whether the required standard practice can be relaxed or waived entirely, when an alternate approach can be substituted, when additional activities are warranted? These determinations hinge on an understanding of the role of assurance act- vities, and the information conveyed by their outcome. These questions will arise more often and become more urgent to answer in the evolving world mentioned in the Call for Papers. For an assurance-case-based approach the outcome of an assurance activity will be evidence located within the assurance case, which makes it easier to see the role it plays in the overall assurance argument, but the same question arises  what is its information contribution to confidence? Distilling these gives the Fundamental Assurance Question, namely how much do assurance activities contribute to raising decision confidence about system qualities, such as safety? These questions  and an intriguing start at answering them  will be the focus of this talk.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6688831,no
Improving reliability of data protection software with integrated multilayered fault Injection testing,"Application involved in data protection for enterprises are responsible to ensure data integrity on backup target as well as remote site designed for disaster recovery (DR). By nature, backup applications needs to operate under very infrastructure which are prone to multiple failure right from physical to application layers. If applications are not designed to consider its operating environment effectively they may not respond to fault in operating environment and may result in data loss and data unavailability scenario. They could potential lead into false reporting which later can become issue with data integrity. We at EMC applied multilayered fault injection test strategy for backup product where we identified different layers of product operating environments. The interface between two layers was targeted to inject appropriate fault based on role and functionality of these layers. The response of application and its impact to product behavior was monitored and analyzed. This has helped improving various exception handling, product agility to fault operating environment and improving usability by providing better picture on failure in product. This session can help audience on understanding how an application operating environments plays key role in designing test strategy. This leads into improving product reliability and better customer experience about application.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6688865,no
Improving manual analysis of automated code inspection results: Need and effectiveness,"Automated code inspection using static analysis tools has been found to be useful and cost-effective over manual code reviews. This is due to ability of these tools to detect programming bugs (or defects) early in the software development cycle without running the code. Further, using sound static analysis tools, even large industry applications can be certified to be free of certain types of the programming bugs such as Division by Zero, Null/Illegal Dereference of a Pointer, Memory Leaks, and so on. In spite of these merits, as per various surveys, the static analysis tools are used infrequently and inconsistently in practice to ensure software quality. Large number of false alarms generated and the efforts required to manually analyze them are the primary reasons for this. Similar has been the experience of our team with the usage of these tools.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6688867,no
Using capture-recapture models to make objective post-inspection decisions,"Problem Definition: Project managers manage the development process by enabling software developers to perform inspection of early software artifacts. However, an inspection can only detect the presence of defects; it cannot certify the absence of defects or indicate how many defects remain post inspection. Managers need objective information to help them decide when they can safely stop the inspection process. A reliable estimate of the number of defects remaining in software can aid mangers in determining whether there is a need for additional inspections.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6688868,no
Design of a dependable peer-to-peer system for numerical optimization,"Summary form only given. Numerical Optimization is an integral part of most engineering, scientific work and is a computationally intensive job. Most optimization frameworks developed so far executes numerical algorithms in a single processor or in a dedicated cluster of machines. A single system based optimizer is plagued by the resources and a dedicated high performance computational cluster is extremely cost prohibitive. Further with the increase in dimensions of the decision / objective space variables / functions, it is difficult to foresee and plan a computation cluster ahead of time. A peer-to-peer system provides a viable alternative to this problem. A peer-to-peer (P2P) system has no central co-ordination and is generally a loose union of a set of non-dedicated machines glued via a logical network for fast dissemination of information. The advantage to cost-effectiveness and elasticity with a P2P system however comes with a price. A P2P system lacks trust and malicious nodes can jeopardize the application to a significant extent. The nodes/communication links are prone to failure of various types such a fail-stop, omission, timing (value) and response (value). As a result there is no guarantee of completion of an optimization job. Furthermore, if a certain section of nodes are susceptible to Byzantine faults, it could lead to a misleading front in the objective space where there is absolute un-certainty of reaching a global minimum. Redundancy, failure detection and recovery are an essential part in the design of such a system. In essence, since in a large scale distributed system Failure is not an exception but a norm, dependability in design of the system is not just a choice but an absolute requirement. In this presentation, we would like to put forth the challenges of designing such a P2P system together with the algorithms that has been used, designed and developed by us in creating a P2P optimization framework. The presentation is - ivided into three sections: firstly in identifying the challenges, secondly, the solutions to mitigate the challenges and thirdly the results that we have obtained by applying the solutions to the problem sets.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6688870,no
Predicting multi-platform release quality,"One difficulty in characterizing the quality of a major feature release is that many releases are implemented on several platforms, with each platform using a different subset of the new features. Also, these platforms can have substantially different performance expectations and results. In order to characterize the entire release adequately in predictive models, we need a robust customer experience metric that is capable of representing many disparate platforms. Several multi-platform SWDPMH (software defects per million usage hours per month) variants have been developed in an attempt to anticipate a release's overall field quality. In addition to predicting the overall release quality, it is critical that we provide guidance to business units concerning remediation of releases predicted to not achieve adequate quality, and also provide guidance regarding how to modify practices so subsequent releases achieve adequate quality. Models have been developed to both predict MP-SWDPMH and to identify specific in-process drivers that likely influence MP-SWDPMH. At this time, these modeling results can be available as early as five or six months prior to release to the customers.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6688874,no
A statistical approach for software resource leak detection and prediction,"Summary form only given. Resource leaks are a common type of software fault. Accruing with time, resource leaks can lead to performance degradation and/or service failures. However, there are few effective general methods and tools to detect and especially predict resource leaks. We propose a lightweight statistical approach to tackling this problem. Without complex resource management and modification to the original application code, the proposed approach simply monitors the target's resource usage periodically, and exploits some statistical analysis methods to extract the useful information behind the usage data. The decomposition method from the field of time series analysis is adopted to identify the different components (trend, seasonal, and random) of resource usage. The Mann-Kendall test method is then applied to the decomposed trend component to identify whether a significant consistent upward trend exists (and thus a leak). Furthermore, we establish a prediction procedure based on the decomposition. The basic idea is to estimate the three different components separately (using such statistical methods as curve fitting and confidence limit), and then add them together to predict the total usage. Several experimental studies that take memory as an example resource demonstrate that our proposed approach is effective to detect leaks and predict relevant leak index of interest (e.g., time to exhaustion, time to crossing some dangerous threshold), and has a very low runtime overhead.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6688880,no
Diagnosing development software release to predict field failures,"With the advancement of analytical engines for big data, the healthcare industry has taken a big leap to minimize escalations on healthcare expenditure, while providing a reliably working solution for the customers based on the slice and dice of the collected information. The research and development (R & D) departments of the healthcare players are providing more focus on the stability and the usage of the system in the field. The field studies have created a reliability based feedback loop that has helped R & D provide hotfixes and service packs in shrinking time lines to better answer the customized needs of the user. Given the variety of possible optimizations in the actual usage, the software-hardware product combine such as the Philips Magnetic Resonance (MR) modality has to ensure that the business critical workflows are ever stable. In a nutshell, fault prediction becomes an important aspect for the R & D department because it helps address the situation in an effective and timely fashion, for both the end-user and the manufacturer to alleviate process hiccups and delays in addressing the fault. Reliability growth plot using the Weibull probability plots helps to predict failures that guide reliability centric maintenance strategies <sup>[1]</sup>; however, this will be a passive application of prediction for the new software yet to be released for market. This paper tries to address the case where a fault/failure at the customer-end can be better predicted for software-under-development with the help of analysis of field data. The terms failures and faults are interchangeably used in the paper to represent error events that can occur at an installed base.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6688882,no
Predicting field experience of releases on specific platforms,"Since 2009, Software Defects Per Million Hours (SWDPMH) has been the primary customer experience metric used at Cisco, and is goaled on a yearly basis for about 100 product families. A key reason SWDPMH is considered to be of critical importance is that we see a high correlation between SWDPMH and Software Customer Satisfaction (SW CSAT) over a wide spectrum of products and feature releases. Therefore, it is important to try to anticipate SWDPMH for new releases before the software is released to customers, for several reasons:  Early warning that a major feature release is likely to experience substantial quality problems in the field may allow for remediation of the release during, or even prior to, function and system testing  Prediction of SWDPMH enables better planning for subsequent maintenance releases and rollout strategies  Calculating the tradeoffs between SWDPMH and feature volume can provide guidance concerning acceptable feature content, test effort, release cycle timing, and other key parameters affecting subsequent feature releases. Our efforts over the past year have been to enhance our ability to predict SWDPMH in the field. Toward this end, we have developed predictive models, tested the models with major feature releases for strategic products, and provided guidance to development, test, and release management teams on how to improve the chances of achieving best-in-class levels of SWDPMH. This work is ongoing, but several models are currently used in a production mode for five product families, with good results. We plan to achieve production capability with an additional several dozen product families over the next year.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6688885,no
An extended notation of FTA for risk assessment of software-intensive medical devices.: Recognition of the risk class before and after the risk control measure,It is difficult to assess the risk of software-intensive medical devices. An extended notation of FTA recognizes the risk class before and after the risk control measure and the software in the system affects the top event of FTA.,2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6688900,no
Safety assessment of software-intensive medical devices: Introducing a safety quality model approach,"Argumentation-based safety assurance is a promising approach for the development of safe software-intensive medical devices. However, one challenge is safety assessment by an independent authority. This article presents an approach that enables argumentation-based safety development on the one hand, while providing means for assessing the product's safety afterwards on the other hand. We combine a generic safety case with an engineering model, which results in specific quality questions for assessors and provides a generic argumentation structure for manufacturers.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6688901,no
Testing distortion estimations in Retinal Prostheses,Retinal Prosthesis device has been approved by FDA for treatment of vision impairment caused by RP. Validating the visual distortion estimation algorithms used in prosthesis is crucial for the safe use of prosthesis. An approach based on metamorphic testing was described to validate a prosthesis distortion estimation algorithm. Four metamorphic relations including two necessary conditions for the correct functioning of the estimation algorithm were identified. Violations in two metamorphic relations were detected showing different estimation behavior of prosthetic vs. regular images and those having high distortions.,2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6688902,no
On the effectiveness of Mann-Kendall test for detection of software aging,"Software aging (i.e. progressive performance degradation of long-running software systems) is difficult to detect due to the long latency until it manifests during program execution. Fast and accurate detection of aging is important for eliminating the underlying defects already during software development and testing. Also in a deployment scenario, aging detection is needed to plan mitigation methods like software rejuvenation. The goal of this paper is to evaluate whether the Mann-Kendall test is an effective approach for detecting software aging from traces of computer system metrics. This technique tests for existence of monotonic trends in time series, and studies of software aging often consider existence of trends in certain metrics as indication of software aging. Through an experimental study we show that the Mann-Kendall test is highly vulnerable to creating false positives in context of aging detection. By increasing the amount of data considered in the test, the false positive rate can be reduced; however, time to detect aging increases considerably. Our findings indicate that aging detection using the Mann-Kendall test alone is in general unreliable, or may require long measurement times.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6688905,no
Software rejuvenation impacts on a phased-mission system for Mars exploration,"When software contains aging-related faults and the system has a long mission period, phased-mission systems consisting of several software components can suffer from software aging, which is a progressive degradation of the software execution environment. Failures caused by software aging might impact on the mission success probability. In this paper, we present a model for a phased-mission system with software rejuvenation, and analyze the impacts of software rejuvenation on the success probability and completion time distribution of the mission. The mission of Mars exploration rover is considered as an example of phased-mission system. The analysis results show that the mission success probability is improved by software rejuvenation at the cost of the mission completion time.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6688906,no
Comparing four case studies on Bohr-Mandel characteristics using ODC,"This paper uses four case studies to examine the difference in properties of Bohr-Mandel bugs. The mechanism used to differentiate Bohr versus Mandel bugs are the ODC Triggers that was developed in a previous study on this subject. In this study, the method is extended to reflect on two additional dimensions. First, on the customer perceived impact. And, second, on how these change between their manifestation in production or the field usage versus late stage development or quality assurance testing. This paper: ; Compares Bohr and Mandel bugs rates between customer/field usage and pre-release system testing. ; Finds that Mandel bugs predominantly have a Reliability-Availability-Serviceability (RAS) Impact. ; Finds that Mandel bugs, rarely, if ever have a Functional Impact. ; Finds these studies predict Mandel bug rates consistent with other studies. ; Finds that pre-release testing found very few Mandel bugs (<;10%).",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6688908,no
Identifying silent failures of SaaS services using finite state machine based invariant analysis,"Field failure analysis is usually driven by a characterization of the different time related properties of failure. This characterization does not help the production support team in understanding the root cause. In order to pinpoint the root cause of failure, one of the most effective techniques used is checking for violations of the system invariants which are the consistent, time invariant correlations that exist in the system. Understanding when and where these violations happen helps in detecting the root cause of the failure. Silent failures, on the other hand are characterized by no evidence of failures either in the console or in the field failure logs. They are unearthed at moments of crisis, either with a customer complaint or other cascading failures. These failures often result in data loss or data corruption, creating many latent errors. Accumulation of these errors over time results in degraded system performance. This represents the problem of software aging and restoration of the system, i.e. its rejuvenation becomes a critical need. Subsequent to the restoration, a rigorous failure detection mechanism is needed to detect them early. What we describe in the paper is a novel method that could be used to detect silent failures using a combination of invariant violation checking and finite state machine based analysis of the system. We use the audit-trail logs of system to extract information about the state and transitions for FSM representation. Currently our research work was limited to proving its efficiency. We applied this approach to our SaaS platform and were able to detect 36 silent failures over a period of 9 months. As next steps, we will implement this as a part of automated failure detection in the operational SaaS platforms.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6688909,no
D-Script : Dependable scripting with DEOS process,"This paper presents our idea and design on script-based framework for dynamic fault management in distributed open systems. Today's distributed systems face unexpected faults and error propagations that are hard to predict at the design time. A key idea behind our D-Script is the dependability through assuredness with a scripting solution to add fault detection and its recovery at the operation time. To realize our vision, we have developed several key tools, including Assure-It authoring tool, D-Shell dependable shell, and REC runtime evidence collector.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6688915,no
Empirical evaluation of an early understandability measurement method,"Usability is a quality factor which increasingly attracts the attention of Human Computer Interaction (HCI) developers. It consists on measuring the usability aspects of a user interface and identifying specific problems. It was usually evaluated based on user's perception. The development costs are the main limitation of methods which target the usability measurement. However, the appearance of the Model Driven Engineering (MDE) allows migrating to a new challenge: early usability evaluation. In an MDE method, the conceptual model represents an abstraction of the application code. Hence, measuring the usability since the conceptual model can be a promising method to predict the usability of the application code. This paper proposes that certain usability attributes, especially understandability attributes, can be measured from the conceptual model. An empirical study is carried out in order to evaluate our proposal. The goal is to evaluate the coherence between values obtained using our proposal and those perceived by the end user.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6689587,no
Sensor and actuator fault detection and isolation based on artificial neural networks and fuzzy logic applicated on induction motor,"This paper presents a scheme for fault detection and isolation (FDI). It deals with sensors and actuator fault of an induction machine. This scheme is established with artificial intelligent techniques in order to resolve two big troubles. The first is the detection problem. It is resolved with the neural network and the second is the isolation difficulty, it solved using the fuzzy logic. The proposed FDI approach is implemented on Matlab/Simulink software and tested under three types of fault (current, speed sensor fault and inverter fault). The obtained results improving the importance of this method. Then, the actuator and sensor fault are detected and isolated successfully.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6689665,no
Detection of Process Antipatterns: A BPEL Perspective,"With the increasing significance of the service-oriented paradigm for implementing business solutions, assessing and analyzing such solutions also becomes an essential task to ensure and improve their quality of design. One way to develop such solutions, a.k.a., Service-Based systems (SBSs) is to generate BPEL (Business Process Execution Language) processes via orchestrating Web services. Development of large business processes (BPs) involves design decisions. Improper and wrong design decisions in software engineering are commonly known as antipatterns, i.e., poor solutions that might affect the quality of design. The detection of antipatterns is thus important to ensure and improve the quality of BPs. However, although BP antipatterns have been defined in the literature, no effort was given to detect such antipatterns within BPEL processes. With the aim of improving the design and quality of BPEL processes, we propose the first rule-based approach to specify and detect BP antipatterns. We specify 7 BP antipatterns from the literature and perform the detection for 4 of them in an initial experiment with 3 BPEL processes.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6690549,no
Automatic concolic test generation with virtual prototypes for post-silicon validation,"Post-silicon validation is a crucial stage in the system development cycle. To accelerate post-silicon validation, high-quality tests should be ready before the first silicon prototype becomes available. In this paper, we present a concolic testing approach to generation of post-silicon tests with virtual prototypes. We identify device states under test from concrete executions of a virtual prototype based on the concept of device transaction, symbolically execute the virtual prototype from these device states to generate tests, and issue the generated tests concretely to the silicon device. We have applied this approach to virtual prototypes of three network adapters to generate their tests. The generated test cases have been issued to both virtual prototypes and silicon devices. We observed significant coverage improvement with generated test cases. Furthermore, we detected 20 inconsistencies between virtual prototypes and silicon devices, each of which reveals a virtual prototype or silicon device defect.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6691136,no
A reliability prediction model for complex systems using data flow dependency,Research on software reliability prediction is of great practical importance. Failure characteristic of large and complex software depends on the operations of individual components and their architecture. Complexity of the components as well as their dependency provides a greater impact on overall reliability of the software. Reliability prediction in the early stage of component based software requires the knowledge of interconnection between the components as well as propagation of errors between the components. In this paper we propose a reliability prediction model which not only considers the control flow of the component it also considers data sharing between the components and their deployment details. We propose a new graphical structure Data Flow Dependency Graph to estimate the effective reliability of the data processed by the components and then use the operational profile to predict the reliability.,2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6691412,no
Big data solutions for predicting risk-of-readmission for congestive heart failure patients,"Developing holistic predictive modeling solutions for risk prediction is extremely challenging in healthcare informatics. Risk prediction involves integration of clinical factors with socio-demographic factors, health conditions, disease parameters, hospital care quality parameters, and a variety of variables specific to each health care provider making the task increasingly complex. Unsurprisingly, many of such factors need to be extracted independently from different sources, and integrated back to improve the quality of predictive modeling. Such sources are typically voluminous, diverse, and vary significantly over the time. Therefore, distributed and parallel computing tools collectively termed big data have to be developed. In this work, we study big data driven solutions to predict the 30-day risk of readmission for congestive heart failure (CHF) incidents. First, we extract useful factors from National Inpatient Dataset (NIS) and augment it with our patient dataset from Multicare Health System (MHS). Then, we develop scalable data mining models to predict risk of readmission using the integrated dataset. We demonstrate the effectiveness and efficiency of the open-source predictive modeling framework we used, describe the results from various modeling algorithms we tested, and compare the performance against baseline non-distributed, non-parallel, non-integrated small data results previously published to demonstrate comparable accuracy over millions of records.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6691760,no
Entropy-based test generation for improved fault localization,"Spectrum-based Bayesian reasoning can effectively rank candidate fault locations based on passing/failing test cases, but the diagnostic quality highly depends on the size and diversity of the underlying test suite. As test suites in practice often do not exhibit the necessary properties, we present a technique to extend existing test suites with new test cases that optimize the diagnostic quality. We apply probability theory concepts to guide test case generation using entropy, such that the amount of uncertainty in the diagnostic ranking is minimized. Our ENTBUG prototype extends the search-based test generation tool EVOSUITE to use entropy in the fitness function of its underlying genetic algorithm, and we applied it to seven real faults. Empirical results show that our approach reduces the entropy of the diagnostic ranking by 49% on average (compared to using the original test suite), leading to a 91% average reduction of diagnosis candidates needed to inspect to find the true faulty one.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693085,no
Detecting bad smells in source code using change history information,"Code smells represent symptoms of poor implementation choices. Previous studies found that these smells make source code more difficult to maintain, possibly also increasing its fault-proneness. There are several approaches that identify smells based on code analysis techniques. However, we observe that many code smells are intrinsically characterized by how code elements change over time. Thus, relying solely on structural information may not be sufficient to detect all the smells accurately. We propose an approach to detect five different code smells, namely Divergent Change, Shotgun Surgery, Parallel Inheritance, Blob, and Feature Envy, by exploiting change history information mined from versioning systems. We applied approach, coined as HIST (Historical Information for Smell deTection), to eight software projects written in Java, and wherever possible compared with existing state-of-the-art smell detectors based on source code analysis. The results indicate that HIST's precision ranges between 61% and 80%, and its recall ranges between 61% and 100%. More importantly, the results confirm that HIST is able to identify code smells that cannot be identified through approaches solely based on code analysis.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693086,no
Personalized defect prediction,"Many defect prediction techniques have been proposed. While they often take the author of the code into consideration, none of these techniques build a separate prediction model for each developer. Different developers have different coding styles, commit frequencies, and experience levels, causing different defect patterns. When the defects of different developers are combined, such differences are obscured, hurting prediction performance. This paper proposes personalized defect prediction-building a separate prediction model for each developer to predict software defects. As a proof of concept, we apply our personalized defect prediction to classify defects at the file change level. We evaluate our personalized change classification technique on six large software projects written in C and Java-the Linux kernel, PostgreSQL, Xorg, Eclipse, Lucene and Jackrabbit. Our personalized approach can discover up to 155 more bugs than the traditional change classification (210 versus 55) if developers inspect the top 20% lines of code that are predicted buggy. In addition, our approach improves the F1-score by 0.01-0.06 compared to the traditional change classification.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693087,no
Automatically partition software into least privilege components using dynamic data dependency analysis,"The principle of least privilege requires that software components should be granted only necessary privileges, so that compromising one component does not lead to compromising others. However, writing privilege separated software is difficult and as a result, a large number of software is monolithic, i.e., it runs as a whole without separation. Manually rewriting monolithic software into privilege separated software requires significant effort and can be error prone. We propose ProgramCutter, a novel approach to automatically partitioning monolithic software using dynamic data dependency analysis. ProgramCutter works by constructing a data dependency graph whose nodes are functions and edges are data dependencies between functions. The graph is then partitioned into subgraphs where each subgraph represents a least privilege component. The privilege separated software runs each component in a separated process with confined system privileges. We evaluate it by applying it on four open source software. We can reduce the privileged part of the program from 100% to below 22%, while having a reasonable execution time overhead. Since ProgramCutter does not require any expert knowledge of the software, it not only can be used by its developers for software refactoring, but also by end users or system administrators. Our contributions are threefold: (i) we define a quantitative measure of the security and performance of privilege separation; (ii) we propose a graph-based approach to compute the optimal separation based on dynamic information flow analysis; and (iii) the separation process is automatic and does not require expert knowledge of the software.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693091,no
Finding architectural flaws using constraints,"During Architectural Risk Analysis (ARA), security architects use a runtime architecture to look for security vulnerabilities that are architectural flaws rather than coding defects. The current ARA process, however, is mostly informal and manual. In this paper, we propose Scoria, a semi-automated approach for finding architectural flaws. Scoria uses a sound, hierarchical object graph with abstract objects and dataflow edges, where edges can refer to nodes in the graph. The architects can augment the object graph with security properties, which can express security information unavailable in code. Scoria allows architects to write queries on the graph in terms of the hierarchy, reachability, and provenance of a dataflow object. Based on the query results, the architects enhance their knowledge of the system security and write expressive constraints. The expressiveness is richer than previous approaches that check only for the presence or absence of communication or do not track a dataflow as an object. To evaluate Scoria, we apply these constraints to several extended examples adapted from the CERT standard for Java to confirm that Scoria can detect injected architectural flaws. Next, we write constraints to enforce an Android security policy and find one architectural flaw in one Android application.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693092,no
Characterizing and detecting resource leaks in Android applications,"Android phones come with a host of hardware components embedded in them, such as Camera, Media Player and Sensor. Most of these components are exclusive resources or resources consuming more memory/energy than general. And they should be explicitly released by developers. Missing release operations of these resources might cause serious problems such as performance degradation or system crash. These kinds of defects are called resource leaks. This paper focuses on resource leak problems in Android apps, and presents our lightweight static analysis tool called Relda, which can automatically analyze an application's resource operations and locate the resource leaks. We propose an automatic method for detecting resource leaks based on a modified Function Call Graph, which handles the features of event-driven mobile programming by analyzing the callbacks defined in Android framework. Our experimental data shows that Relda is effective in detecting resource leaks in real Android apps.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693097,no
Dangling references in multi-configuration and dynamic PHP-based Web applications,"PHP is a dynamic language popularly used in Web development for writing server-side code to dynamically create multiple versions of client-side pages at run time for different configurations. A PHP program contains code to be executed or produced for multiple configurations/versions. That dynamism and multi-configuration nature leads to dangling references. Specifically, in the execution for a configuration, a reference to a variable or a call to a function is dangling if its corresponding declaration cannot be found. We conducted an exploratory study to confirm the existence of such dangling reference errors including dangling cross-language and embedded references in the client-side HTML/JavaScript code and in data-accessing SQL code that are embedded in scattered PHP code. Dangling references have caused run-time fatal failures and security vulnerabilities. We developed DRC, a static analysis method to detect such dangling references. DRC uses symbolic execution to collect PHP declarations/references and to approximate all versions of the generated output, and then extracts embedded declarations/references. It associates each detected declaration/reference with a conditional constraint that represents the execution paths (i.e. configurations/versions) containing that declaration/reference. It then validates references against declarations via a novel dangling reference detection algorithm. Our empirical evaluation shows that DRC detects dangling references with high accuracy. It revealed 83 yet undiscovered defects caused by dangling references.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693098,no
Environment rematching: Toward dependability improvement for self-adaptive applications,"Self-adaptive applications can easily contain faults. Existing approaches detect faults, but can still leave some undetected and manifesting into failures at runtime. In this paper, we study the correlation between occurrences of application failure and those of consistency failure. We propose fixing consistency failure to reduce application failure at runtime. We name this environment rematching, which can systematically reconnect a self-adaptive application to its environment in a consistent way. We also propose enforcing atomicity for application semantics during the rematching to avoid its side effect. We evaluated our approach using 12 self-adaptive robot-car applications by both simulated and real experiments. The experimental results confirmed our approach's effectiveness in improving dependability for all applications by 12.5-52.5%.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693118,no
PYTHIA: Generating test cases with oracles for JavaScript applications,Web developers often write test cases manually using testing frameworks such as Selenium. Testing JavaScript-based applications is challenging as manually exploring various execution paths of the application is difficult. Also JavaScript's highly dynamic nature as well as its complex interaction with the DOM make it difficult for the tester to achieve high coverage. We present a framework to automatically generate unit test cases for individual JavaScript functions. These test cases are strengthened by automatically generated test oracles capable of detecting faults in JavaScript code. Our approach is implemented in a tool called Pythia. Our preliminary evaluation results point to the efficacy of the approach in achieving high coverage and detecting faults.,2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693121,no
Class level fault prediction using software clustering,"Defect prediction approaches use software metrics and fault data to learn which software properties associate with faults in classes. Existing techniques predict fault-prone classes in the same release (intra) or in a subsequent releases (inter) of a subject software system. We propose an intra-release fault prediction technique, which learns from clusters of related classes, rather than from the entire system. Classes are clustered using structural information and fault prediction models are built using the properties of the classes in each cluster. We present an empirical investigation on data from 29 releases of eight open source software systems from the PROMISE repository, with predictors built using multivariate linear regression. The results indicate that the prediction models built on clusters outperform those built on all the classes of the system.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693126,no
Context-aware task allocation for distributed agile team,"The philosophy of Agile software development advocates the spirit of open discussion and coordination among team members to adapt to incremental changes encountered during the process. Based on our observations from 20 agile student development teams over an 8-week study in Beihang University, China, we found that the task allocation strategy as a result of following the Agile process heavily depends on the experience of the users, and cannot be guaranteed to result in efficient utilization of team resources. In this research, we propose a context-aware task allocation decision support system that balances the considerations for quality and timeliness to improve the overall utility derived from an agile software development project.We formulate the agile process as a distributed constraint optimization problem, and propose a technology framework that assesses individual developers' situations based on data collected from a Scrum-based agile process, and helps individual developers make situation-aware decisions on which tasks from the backlog to select in real-time. Preliminary analysis and simulation results show that it can achieve close to optimally efficient utilization of the developers' collective capacity. We plan to build the framework into a computer-supported collaborative development platform and refine the method through more realistic projects.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693151,no
"Preventing erosion of architectural tactics through their strategic implementation, preservation, and visualization","Nowadays, a successful software production is increasingly dependent on how the final deployed system addresses customers' and users' quality concerns such as security, reliability, availability, interoperability, performance and many other types of such requirements. In order to satisfy such quality concerns, software architects are accountable for devising and comparing various alternate solutions, assessing the trade-offs, and finally adopting strategic design decisions which optimize the degree to which each of the quality concerns is satisfied. Although designing and implementing a good architecture is necessary, it is not usually enough. Even a good architecture can deteriorate in subsequent releases and then fail to address those concerns for which it was initially designed. In this work, we present a novel traceability approach for automating the construction of traceabilty links for architectural tactics and utilizing those links to implement a change impact analysis infrastructure to mitigate the problem of architecture degradation. Our approach utilizes machine learning methods to detect tactic-related classes. The detected tactic-related classes are then mapped to a Tactic Traceability Pattern. We train our trace algorithm using code extracted from fifty performance-centric and safety-critical open source software systems and then evaluate it against a real case study.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693152,no
Towards the Development of a Defect Detection Tool for COSMIC Functional Size Measurement,"Reliability of functional size measurement is very crucial since software management activities such as cost and budget estimations, process benchmarking and project control depend on software size measurements. In order to improve the reliability of functional size measurements, they should be controlled and reviewed at the end of the measurement process. However, manual inspection for detecting defects and errors of measurements is time and effort consuming and there is always a possibility of missing a defect. To overcome such problems we developed a tool, for detecting defects of COSMIC functional size measurements automatically. In this study we presented the process of developing the tool, R-COVER, and the results of the case studies conducted for analyzing the efficiency of the tool in terms of correctness and accuracy.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693216,no
AM-QuICk: A Measurement-Based Framework for Agile Methods Customisation,"Software development practitioners are increasingly interested in adopting agile methods and generally recommend customisation so that the adopted method can fit the organisational reality. Many studies from the literature report agile adoption and customisation experiences but most of them are hardly generalisable and few are metric-based. They therefore cannot provide quantitative evidence of the suitability of the customised agile method, neither assess the organisation readability to adopt it, nor help in decision-making concerning the organisation transformation strategy. In this paper, we first describe the Agile Methods Quality-Integrated Customisation framework (AM-QuICk) that relies on measurements and aims to continuously assist agile methodologists throughout the agile adoption and customisation process, i.e., during the initial organisation adoption, the method design and throughout the working development process. Then, we present a case study using AM-QuICk within an organisation. With this study, we aim to analyse the current development process and its level of agility and identify the initial risk factors. The data were collected using preliminary interviews with the different team members and two questionnaires. The results reveal that though most respondents are enthusiastic towards agile principles, a progressive transformation strategy would be beneficial.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693225,no
Experiences from an Initial Study on Risk Probability Estimation Based on Expert Opinion,"Background: Determining the factor probability in risk estimation requires detailed knowledge about the software product and the development process. Basing estimates on expert opinion may be a viable approach if no other data is available. Objective: In this paper we analyze initial results from estimating the risk probability based on expert opinion to answer the questions (1) Are expert opinions consistent? (2) Do expert opinions reflect the actual situation? (3) How can the results be improved? Approach: An industry project serves as case for our study. In this project six members provided initial risk estimates for the components of a software system. The resulting estimates are compared to each other to reveal the agreement between experts and they are compared to the actual risk probabilities derived in an ex-post analysis from the released version. Results: We found a moderate agreement between the rations of the individual experts. We found a significant accuracy when compared to the risk probabilities computed from the actual defects. We identified a number of lessons learned useful for improving the simple initial estimation approach applied in the studied project. Conclusions: Risk estimates have successfully been derived from subjective expert opinions. However, additional measures should be applied to triangulate and improve expert estimates.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693227,no
Noise in Bug Report Data and the Impact on Defect Prediction Results,"The potential benefits of defect prediction have created widespread interest in research and generated a considerable number of empirical studies. Applications with real-world data revealed a central problem: Real-world data is """"dirty"""" and often of poor quality. Noise in bug report data is a particular problem for defect prediction since it effects the correct classification of software modules. Is the module actually defective or not? In this paper we examine different causes of noise encountered when predicting defects in an industrial software system and we provide an overview of commonly reported causes in related work. Furthermore we conduct an experiment to explore the impact of class noise on the predictions performance. The experiment shows that the prediction results for the studied system remain reliable even at a noise level of 20% probability of incorrect links between bug reports and modules.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693237,no
A Comparison of Different Defect Measures to Identify Defect-Prone Components,"(Background) Defect distribution in software systems has been shown to follow the Pareto rule of 20-80. This motivates the prioritization of components with the majority of defects for testing activities. (Research goal) Are there significant variations between defective components and architectural hotspots identified by other defect measures? (Approach) We have performed a study using post-release data of an industrial Smart Grid application with a well-maintained defect tracking system. Using the Pareto principle, we identify and compare defect-prone and hotspots components based on four defect metrics. Furthermore, we validated the quantitative results against qualitative data from the developers. (Results) Our results show that at the top 25% of the measures 1) significant variations exist between the defective components identified by the different defect metrics and that some of the components persist as defective across releases 2) the top defective components based on number of defects could only identify about 40% of critical components in this system 3) other defect metrics identify about 30% additional critical components 4) additional quality challenges of a component could be identified by considering the pair wise intersection of the defect metrics. (Discussion and Conclusion) Since a set of critical components in the system is missed by using largest-first or smallest-first prioritization approaches, this study, therefore, makes a case for an all-inclusive metrics during defect model construction such as number of defects, defect density, defect severity and defect correction effort to make us better understand what comprises defect-prone components and architectural hotspots, especially in critical applications.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693238,no
Comparing between Maximum Likelihood Estimator and Non-linear Regression Estimation Procedures for NHPP Software Reliability Growth Modelling,"Software Reliability Growth Models (SRGMs) have been used by engineers and managers for tracking and managing the reliability change of software to ensure required standard of quality is achieved before the software is released to the customer. SRGMs can be used during the project to help make testing resource allocation decisions and/ or it can be used after the testing phase to determine the latent faults prediction to assess the maturity of software artifact. A number of SRGMs have been proposed and to apply a given reliability model, defect inflow data is fitted to model equations. Two of the widely known and recommended techniques for parameter estimation are maximum likelihood and method of least squares. In this paper we compare between the two estimation procedures for their applicability in context of NHPP SRGMs. We also highlight a couple of practical considerations, reliability practitioners must be aware of when applying SRGMs.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693241,no
Assessing Organizational Learning in IT Organizations: An Experience Report from Industry,"With the increase in demand for higher-quality and more capable IT services, IT organizations in order to obtain competitive advantage require extensive knowledge that needs to be shared and reused among different entities within the organization. The existing IT Service Management (ITSM) mechanisms mention the importance of organizational learning (OL) and knowledge management (KM) for IT organizations. However, they do not explicitly address how OL capabilities of an IT organization can be assessed. This paper, by using an OL assessment model developed for software organizations, namely AiOLoS, shows that with the proper adjustment, the application of the model to IT organizations is feasible. We report the results of applying the model in four functional teams in an IT organization from private sector.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693248,no
FinancialCloud: Open Cloud Framework of Derivative Pricing,"Predicting prices and risk measures of assets and derivatives and rating of financial products have been studied and widely used by financial institutions and individual investors. In contrast to the centralized and oligopoly nature of the existing financial information services, in this paper, we advocate the notion of a Financial Cloud, i.e., an open distributed framework based cloud computing architecture to host modularize financial services such that these modularized financial services may easily be integrated flexibly and dynamically to meet users' needs on demand. This new cloud based architecture of modularized financial services provides several advantages. We may have different types of service providers in the ecosystem on top of the framework. For example, market data resellers may collect and sell long-term historical market data. Statistical analyses of macroeconomic indices, interest rates, and correlation of a set of assets may also be purchased online. Some agencies might be interested in providing services based on rating or pricing values of financial products. Traders may use the statistically estimated parameters to fine-tune their trading algorithm to maximize the profit of their clients. Providers of each service module may focus on effectiveness, performance, robustness, and security of their innovative products. On the other hand, a user pays for exactly what one uses to optimally manage their assets. A user may also acquire services through an online agent who is an expert in assessing the structural model and quality of existing products and thus assembles service modules matching users risk taking behavior. In this paper, we will also present a survey of related existing technologies and a prototype we developed so far.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693414,no
Fault tolerant approach for verified software: Case of natural gas purification simulator,"Well logically verified and tested software may fail because of undesired physical phenomena provoking transient faults during its execution. While being the most frequent kind of faults, transient faults are difficult to localize because they have a very short life, but they may cause the failure of software. A fault tolerant method against transient faults under the hypothesis of statically verified software is presented. In order to ensure the right experimental environment, first the specification of the application is validated by Alloy analyzer, second a JML annotated Java code is statically verified. The proposed approach is based on some rules transforming basic Java statements like assignments, conditional and iterative statements into equivalent fault tolerant ones. The current research has exhibited some natural redundancy in any code, and the corrective power of repetitive statements. It also proved that the proposed method makes more efficient fault tolerant versions compared with natural error recovery, i.e. without inserting any additional code for detecting or repairing the damaged state. Illustrated by Gas purification simulator, one can see the natural error recovery in case of fault injection in the code, and how fault tolerant rules recover more errors in less time compared to the natural recovery. The proposed approach is preventive because it avoids the propagation of errors at early stages by repeating low level statements until some stability of their behavior.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693902,no
Hierarchical diagnosis for an overactuated autonomous vehicle,"The paper presents a new strategy based on hierarchical diagnosis for an autonomous four-wheel steering four-wheel driving (4WS4WD) electrical vehicle. It is known that the lateral stability of the vehicle may be lost in specific faulty scenarios (due, for instance, to the front wheels steering mechanism faults, wheels blocking or drop of pressure). We propose a hierarchical diagnosis to ensure the stability of the vehicle when isolating precisely the component fault. When the vehicle lateral error exceeds a threshold, a dynamic reference generator for rear wheels steering actuator is activated in order to guarantee the vehicle's lateral stability. Simultaneously, an active diagnosis based on the rear wheels steering mathematical model is used to identify the tire-road interface, a vital information for detecting and isolating faults when using analytical redundancy based residuals. The strategy proposed is tested and validated on a realistic dynamic vehicle model simulated using CarSim and Matlab-Simulink softwares.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693956,no
A method of illumination effect transfer between images using color transfer and gradient fusion,"Illumination plays a crucial role to determine the quality of an image especially in photography. However, illumination alteration is quite difficult to achieve with existing image composition techniques. This paper proposes an unsupervised illumination-transfer approach for altering the illumination effects of an image by transferring illumination effects from another. Our approach consists of three phases. Phase-one layers the target image to three luminosity-variant layers by a series of pre-processing and alpha matting; meanwhile the source image is layered accordingly. Then the layers of the source image are recolored respectively by casting the colors from the corresponding layers of the target image. In phase-two, the recolored source image is edited to seamlessly transit at the boundaries between the layers using gradient fusion technique. Finally, phase-three recolors the fused source image again to produce a similar illuminating image with the target image. Our approach is tested on a number of different scenarios and the experimental results show that our method works well to transfer illumination effects between images.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6694220,no
Secret sharing mechanism with cheater detection,"Cheater detection is essential for a secret sharing approach which allows the involved participants to detect cheaters during the secret retrieval process. In this article, we propose a verifiable secret sharing mechanism that can not only resist dishonest participants but can also satisfy the requirements of larger secret payload and camouflage. The new approach conceals the shadows into a pixel pair of the cover image based on the adaptive pixel pair matching. Consequently, the embedding alteration can be reduced to preserve the fidelity of the shadow image. The experimental results exhibit that the proposed scheme can share a large secret capacity and retain superior quality.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6694288,no
On effects of tokens in source code to accuracy of fault-prone module prediction,"In the software development, defects affect quality and cost in an adverse way. Therefore, various studies have been proposed defect prediction techniques. Most of current defect prediction approaches use past project data for building prediction models. That is, these approaches are difficult to apply new development projects without past data. In this study, we use 28 versions of 8 projects to conduct experiments using the fault-prone filtering technique. Fault-prone filtering is a method that predicts faults using tokens from source code modules. Since the classes of tokens have impact to the accuracy of fault-proneness, we conduct an experiment to find appropriate token sets for prediction. From the results of experiments, we found that using tokens extracted from all parts of modules is the best way to predict faults and using tokens extracted from code part of modules shows better precision.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6694761,no
Enhancing the control of IP tactical networks via measurements,"Measurements in an IP communication system should serve network planning, allow the follow-up of Service Level Agreements and contribute in protecting the security of the network by detecting denial of service attacks as well as threats to the exterior routing protocol. In a black network, they will be best realised using active methods which rely on specific test flows. In a tactical system, they can be performed by software probes which will preserve the compactness of network nodes. All the components of a comprehensive measurement architecture are available off the shelf today, but some precautions must be taken to avoid a number of pitfalls which could lead to erroneous measurement results or make measurement overhead unacceptable. These precautions include an appropriate use of statistical laws and steps to compensate for some errors inherent in sampling methods. The conclusions of this paper are valid for deployable tactical networks, but not necessarily for highly mobile ones. Measurements in a MANET would require more theoretical and experimental work.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6695531,no
Model-based generation of safety test-cases for Onboard systems,"As a core subsystem in CTCS-3, the Onboard subsystem is a typical safety-critical system, in which any fault can lead to huge human injury or wealth losing. It is important to guarantee the safety of train control system. Safety testing is an effective method to detect the safety holes and bugs in the system. However, because of the special characters of train control system like diversification, structural complexity and multiplicity of interfaces, most safety testing for train control system are manually executed based on specialistic experience, which leads to a huge testing workload. Besides, manual generation will easily cause the problem of missing test cases. In this paper, a model-based safety test method is introduced. We select a core function of onboard system as the representative to study the method. This function was analyzed by Fault Tree Analysis (FTA) to get the bottom events, which are used to turn to fault models being injected into the whole system model, affected system safety, and a set of timed automata network model of the core function is built using the tools of UPPAAL. Then COVER, the real-time test case generation tool, is used to generate the safety test cases from the system model (included fault models) automatically, and states transition criteria is customized based on preferences to achieve user-defined test, the test accuracy and efficiency is improved.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6696292,no
A hybrid algorithm for coverage path planning with imperfect sensors,"We are interested in the coverage path planning problem with imperfect sensors, within the context of robotics for mine countermeasures. In the studied problem, an autonomous underwater vehicle (AUV) equipped with sonar surveys the bottom of the ocean searching for mines. We use a cellular decomposition to represent the ocean floor by a grid of uniform square cells. The robot scans a fixed number of cells sideways with a varying probability of detection as a function of distance and of seabed type. The goal is to plan a path that achieves the minimal required coverage in each cell while minimizing the total traveled distance and the total number of turns. We propose an off-line hybrid algorithm based on dynamic programming and on a traveling salesman problem reduction. We present experimental results and show that our algorithm's performance is superior to published results in terms of path quality and computational time, which makes it possible to implement the algorithm in an AUV.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6697225,no
Automatic TFT-LCD mura detection based on image reconstruction and processing,"Automatic inspection of Mura defects is a challenging task in thin-film transistor liquid crystal display (TFT-LCD) defect detection, which is critical for LCD manufacturers to guarantee high standard quality control. In this paper, we propose a set of automatic procedures to detect mura defects by using image processing and computer vision techniques. Singular Value Decomposition (SVD) and Discrete Cosine Transformation(DCT) techniques are employed to conduct image reconstruction, based on which we are able to obtain the differential image of LCD Cells. In order to detect different types of mura defects accurately, we then design a method that employs different detection modules adaptively, which can overcome the disadvantage of simply using a single threshold value. Finally, we provide the experimental results to validate the effectiveness of the proposed method in mura detection.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6698053,no
MATCASC: A tool to analyse cascading line outages in power grids,"Blackouts in power grids typically result from cascading failures. The key importance of the electric power grid to society encourages further research into sustaining power system reliability and developing new methods to manage the risks of cascading blackouts. Adequate software tools are required to better analyse, understand, and assess the consequences of the cascading failures. This paper presents MATCASC, an open source MATLAB based tool to analyse cascading failures in power grids. Cascading effects due to line overload outages are considered. The applicability of the MATCASC tool is demonstrated by assessing the robustness of IEEE test systems and real-world power grids with respect to cascading failures.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6698576,no
Adaptive protection schemes for feeders with the penetration of SEIG based wind farm,"Due to the increasing penetration of Distributed Generation (DG), conventional distribution overhead feeder protection schemes are prone to potential threats. In order to cope up with this, adaptive feeder protection schemes are required. This work involves the development and evaluation of an adaptive Overcurrent feeder protection scheme and an adaptive Recloser and Sectionalizers feeder protection scheme to vanquish the impacts of wind based DG which is highly intermittent in nature. PSCAD is used to carry out simulations. MATLAB based software package for distribution system conductor sizing and protection coordination studies are also presented.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6698725,no
Model-based testing of NASA's OSAL API  An experience report,"We present a case study that evaluates the applicability and effectiveness of model-based testing in detecting bugs in real-world, mission-critical systems. NASA's Operating System ion Layer (OSAL) is the subject system of this paper. The OSAL is a reusable framework that wraps several operating systems (OS) and is used extensively in NASA's flight software missions. We developed a suite of behavioral models, represented as hierarchical finite state machines (FSMs), of the core file system API and generated a large number of test cases automatically. We then automatically executed these test cases against the OSAL. The results show that the OSAL is a high quality product. Naturally, due to the systematic and rigorous nature of MBT, we detected a few previously unknown corner-case bugs and issues, which escaped traditional manual testing and code reviews. We discuss the MBT architecture, the detected bugs, the code coverage of generated tests, as well as threats to validity of the study.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6698883,no
"Help, help, i'm being suppressed! The significance of suppressors in software testing","Test features are basic compositional units used to describe what a test does (and does not) involve. For example, in API-based testing, the most obvious features are function calls; in grammar-based testing, the obvious features are the elements of the grammar. The relationship between features as abstractions of tests and produced behaviors of the tested program is surprisingly poorly understood. This paper shows how large-scale random testing modified to use diverse feature sets can uncover causal relationships between what a test contains and what the program being tested does. We introduce a general notion of observable behaviors as targets, where a target can be a detected fault, an executed branch or statement, or a complex coverage entity such as a state, predicate-valuation, or program path. While it is obvious that targets have triggers - features without which they cannot be hit by a test - the notion of suppressors - features which make a test less likely to hit a target - has received little attention despite having important implications for automated test generation and program understanding. For a set of subjects including C compilers, a flash file system, and JavaScript engines, we show that suppression is both common and important.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6698892,no
An empirical comparison of the fault-detection capabilities of internal oracles,"Modern computer systems are prone to various classes of runtime faults due to their reliance on features such as concurrency and peripheral devices such as sensors. Testing remains a common method for uncovering faults in these systems, but many runtime faults are difficult to detect using typical testing oracles that monitor only program output. In this work we empirically investigate the use of internal test oracles: oracles that detect faults by monitoring aspects of internal program and system states. We compare these internal oracles to each other and to output-based oracles for relative effectiveness and examine tradeoffs between oracles involving incorrect reports about faults (false positives and false negatives). Our results reveal several implications that test engineers and researchers should consider when testing for runtime faults.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6698900,no
Towards fast OS rejuvenation: An experimental evaluation of fast OS reboot techniques,"Continuous or high availability is a key requirement for many modern IT systems. Computer operating systems play an important role in IT systems availability. Due to the complexity of their architecture, they are prone to suffer failures due to several types of software faults. Software aging causes a nonnegligible fraction of these failures. It leads to an accumulation of errors with time, increasing the system failure rate. This phenomenon can be accompanied by performance degradation and eventually system hang or even crash. As a countermeasure, software rejuvenation entails stopping the system, cleaning its internal state, and resuming its operation. This process usually incurs downtime. For an operating system, the downtime impacts any application running on top of it. Several solutions have been developed to speed up the boot time of operating systems in order to reduce the downtime overhead. We present a study of two fast OS reboot techniques for rejuvenation of Linux-based operating systems, namely Kexec and Phase-based reboot. The study measures the performance penalty they introduce and the gain in reduction of downtime overhead. The results reveal that the Kexec and Phase-based reboot have no statistically significant impact in terms of performance penalty from the user perspective. However, they may require extra resource (e.g., CPU) usage. The downtime overhead reduction, compared with normal Linux and VM reboots, is 77% and 79% in Kexec and Phase-based reboot, respectively.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6698905,no
Predicting defects using change genealogies,"When analyzing version histories, researchers traditionally focused on single events: e.g. the change that causes a bug, the fix that resolves an issue. Sometimes however, there are indirect effects that count: Changing a module may lead to plenty of follow-up modifications in other places, making the initial change having an impact on those later changes. To this end, we group changes into change genealogies, graphs of changes reflecting their mutual dependencies and influences and develop new metrics to capture the spatial and temporal influence of changes. In this paper, we show that change genealogies offer good classification models when identifying defective source files: With a median precision of 73% and a median recall of 76%, change genealogy defect prediction models not only show better classification accuracies as models based on code complexity, but can also outperform classification models based on code dependency network metrics.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6698911,no
Predicting risk of pre-release code changes with Checkinmentor,"Code defects introduced during the development of the software system can result in failures after its release. Such post-release failures are costly to fix and have negative impact on the reputation of the released software. In this paper we propose a methodology for early detection of faulty code changes. We describe code changes with metrics and then use a statistical model that discriminates between faulty and non-faulty changes. The predictions are done not at a file or binary level but at the change level thereby assessing the impact of each change. We also study the impact of code branches on collecting code metrics and on the accuracy of the model. The model has shown high accuracy and was developed into a tool called CheckinMentor. CheckinMentor was deployed to predict risk for the Windows Phone software. However, our methodology is versatile and can be used to predict risk in a variety of large complex software systems.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6698912,no
Fault localization based on failure-inducing combinations,"Combinatorial testing has been shown to be a very effective testing strategy. After a failure is detected, the next task is to identify the fault that causes the failure. In this paper, we present an approach to fault localization that leverages the result of combinatorial testing. Our approach is based on a notion called failure-inducing combinations. A combination is failure-inducing if it causes any test in which it appears to fail. Given a failure-inducing combination, our approach derives a group of tests that are likely to exercise similar traces but produce different outcomes. These tests are then analyzed to locate the faults. We conducted an experiment in which our approach was applied to the Siemens suite as well as the grep program from the SIR repository that has 10068 lines of code. The experimental results show that our approach can effectively and efficiently localize the faults in these programs.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6698916,no
Evaluating long-term predictive power of standard reliability growth models on automotive systems,"Software is today an integral part of providing improved functionality and innovative features in the automotive industry. Safety and reliability are important requirements for automotive software and software testing is still the main source of ensuring dependability of the software artifacts. Software Reliability Growth Models (SRGMs) have been long used to assess the reliability of software systems; they are also used for predicting the defect inflow in order to allocate maintenance resources. Although a number of models have been proposed and evaluated, much of the assessment of their predictive ability is studied for short term (e.g. last 10% of data). But in practice (in industry) the usefulness of SRGMs with respect to optimal resource allocation depends heavily on the long term predictive power of SRGMs i.e. much before the project is close to completion. The ability to reasonably predict the expected defect inflow provides important insight that can help project and quality managers to take necessary actions related to testing resource allocation on time to ensure high quality software at the release. In this paper we evaluate the long-term predictive power of commonly used SRGMs on four software projects from the automotive sector. The results indicate that Gompertz and Logistic model performs best among the tested models on all fit criterias as well as on predictive power, although these models are not reliable for long-term prediction with partial data.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6698922,no
Quality assessment of row crop plants by using a machine vision system,"This paper reports research results on developing a machine vision system to assess the quality of row crop plants. Comparing to the prevalent machine vision system employed in agricultural industry for weed-crops classification as well as plant density evaluation, the proposed machine vision system is able to detect the location of plants (weed / crops) and calculate the leaves' area for plant quality assessment, even if the leaves are overlapped with each other. The developed machine vision system involves a camera system and an image processing system. The camera system uses a coaxial camera constructed by a RGB sensor and near infrared (NIR) sensor, which cooperate with a white front lighting and NIR front lighting respectively. Plants are firstly captured by the coaxial camera. The plants are segmented from background on RGB image; the overlapping edges of leaves are detected on NIR image. Afterwards the overlapping leaves are separated and assigned to the assessed stem position of plants. At last, based on the assigned leaves, the plants are separated, and the area of plant canopy is calculated. A set of experiments have been made to prove the feasibility of the proposed machine vision system.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6699518,no
Semi-automated deployment of Simulation-aided Building Controls,"The deployment of Simulation-aided Building Controls is a complex process due to the uniqueness of each building along with an increasing complexity of building systems. Typically the deployment tasks are performed manually by highly specialized personnel which results in a poorly documented and extremely scattered deployment process. This paper introduces a workflow for the deployment of a Simulation-aided Building Control service suitable for supporting the operation phase of a building. Some tasks in the deployment process may benefit from machine support, especially data intensive, repetitive and error prone tasks. These may be fully or semi-automated. The proposed approach reduces the complexity of the setup procedure, decreases problems related to the uniqueness of the infrastructure and supports the documentation of the deployment process. Specially large facility management service providers may profit from this deployment process.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6700076,no
A customized design framework for the model-based development of engine control systems,"In the model-based design of complex technical systems, many design data artifacts are generated, such as models in different formalisms and design-related documents, which include specifications, test results, and design decisions. The consistent treatment and integration of these design artifacts is a challenge that is as of yet unsolved in industrial practice. This paper illustrates the industrial applicability of a software-based Design Framework (DF) [1] for the model-based design of an engine control system that was developed recently within the European research project MULTIFORM [2]. The goal of the Design Framework is to reduce the design effort, and thus the cost, while improving the quality of the designed system by consistently integrating the artifacts and tools that arise in model-based design processes. To ensure that design inconsistencies and errors are detected as early as possible (i.e. when it is relatively cheap to correct them), the framework provides structured data and model management as well as automated design consistency checking and design parameter propagation.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6700279,no
Recent progress in thin wafer processing,"The ability to process thin wafers with thicknesses of 20-50um on front- and backside is a key technology for 3D IC. The most obvious reason for thin wafers is the reduced form factor, which is especially important for handheld devices. However, probably even more important is that thinner wafers enable significant cost reduction for TSVs. The silicon real estate consumed by the TSVs has to be minimized in order that the final device provides a performance advantage compared to traditional 2D devices. The only way to reduce area consumption by the TSVs is to reduce their diameter. For a given wafer thickness the reduction of TSV diameter increases the TSV aspect ratio. Consensus has developed on the use of Temporary Bonding / Debonding Technology as the solution of choice for reliably handling thin wafers through backside processing steps. While the majority of the device manufacturing steps on the front side of the wafer will be completed with the wafer still at full thickness, it will be temporarily mounted onto a carrier before thinning and processing of the features on its backside. Once the wafer reaches the temporary bonding step, it already represents a significant value, as it has already gone through numerous processing steps. For this reason, inspection of wafers prior to non-reworkable process steps is of great interest. Within the context of Temporary Bonding this consideration calls for inline metrology that allows for detection of excursions of the temporary bonding process in terms of adhesive thickness, thickness uniformity as well as bonding voids prior to thinning of the product wafer. This paper introduces a novel metrology solution capable of detecting all quality relevant parameters of temporarily bonded stacks in a single measurement cycle using an Infrared (IR) based measurement principle. Thanks to the IR based measurement principle, the metrology solution is compatible with both silicon and glass carriers. The system design has been develop- d with the inline metrology task in mind. This has led to a unique system design concept that enables scanning of wafers at a throughput rate sufficient to enable 100% inspection of all bonded wafers inline in the Temporary Bonding system. Both, current generation temporary bonding system throughputs and future high volume production system throughputs as required by the industry for cost effective manufacturing of 3D stacked devices were taken into account as basic specifications for the newly developed metrology solution. Sophisticated software algorithms allow for making pass/ fail decisions for the bonded stacks and triggering further inspection, processing and / or rework. Actual metrology results achieved with this novel system will be presented and discussed. In terms of adhesive total thickness variation (TTV) of bonded wafers, currently achieved performance values for postbond TTV will be reviewed in light of roadmaps as required by high volume production customers.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6702341,no
OpenFlow Rules Interactions: Definition and Detection,"Software Defined Networking (SDN) is a promising architecture for computer networks that allows the development of complex and revolutionary applications, without breaking the backward compatibility with legacy networks. Programmability of the control-plane is one of the most interesting features of SDN, since it provides a higher degree of flexibility in network management: network operations are driven by ad-hoc written programs that substitute the classical combination of firewalls, routers and switches configurations performed in traditional networks. A successful SDN implementation is provided by the OpenFlow standard, that defines a rule-based programming model for the network. The development process of OpenFlow applications is currently a low-level, error prone programming exercise, mainly performed manually in both the implementation and verification phases. In this paper we provide a first formal classification of OpenFlow rules interactions into a single OpenFlow switch, and an algorithm to detect such interactions in order to aid the OpenFlow applications development. Moreover, we briefly present a performance evaluation of our prototype and how it has been used in a real-word application.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6702547,no
Easily Rendering Token-Ring Algorithms of Distributed and Parallel Applications Fault Tolerant,"We propose in this paper a new algorithm that, when called by existing token ring-based algorithms of parallel and distributed applications, easily renders the token tolerant to losses in presence of node crashes. At most k consecutive node crashes are tolerated in the ring. Our algorithm scales very well since a node monitors the liveness of at most k other nodes and neither a global election algorithm nor broadcast primitives are used to regenerate a new token. It is thus very effective in terms of latency cost. Finally, a study of the probability of having at most k consecutive node crashes in the presence of f failures and a discussion of how to extend our algorithm to other logical topologies are also presented.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6702599,no
A Model to Assess the Usability of Enterprise Architecture Frameworks,"Since the advent of Enterprise Architecture (EA), several EA frameworks have been proposed. Each EA framework has strong and weak points which can be assessed qualitatively to determine the best EA for an organization. One of the qualitative characteristics is usability of the EA framework. However, currently there is a lack of well-defined criteria to measure EA framework usability. In this paper a model is proposed to evaluate and measure the usability of EA frameworks.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6702778,no
An Empirical Analysis of a Testability Model,"Testability modeling has been performed for many years. Unfortunately, the modeling of a design for testability is often performed after the design is complete. This limits the functional use of the testability model to determining what level of test coverage is available in the design. This information may be useful to help assess whether a product meets a requirement to achieve a desired level of test coverage, but has little pro-active effect on making the design more testable. This paper investigates and presents a number of approaches for tackling this problem. Approaches are surveyed, achievements and main issues of each approach are considered. Investigation of that classification will help researchers who are working on model testability to deliver more applicable solutions.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6702784,no
An Empirical Study into Model Testability,"Testability modeling has been performed for many years. Unfortunately, the modeling of a design for testability is often performed after the design is complete. This limits the functional use of the testability model to determining what level of test coverage is available in the design. This information may be useful to help assess whether a product meets a requirement to achieve a desired level of test coverage, but has little pro-active effect on making the design more testable. This paper investigates and presents a number of approaches for tackling this problem. Approaches are surveyed, achievements and main issues of each approach are considered. Investigation of that classification will help researchers who are working on model testability to deliver more applicable solutions.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6702788,no
Measuring and visualising the quality of models,"The quality of graphical software or business process models is influenced by several aspects such as correctness of the formal syntax, understandability or compliance to existing rules. Motivated by a standardised software quality model, we discuss characteristics and subcharacteristics of model quality and sugest measures for those quality (sub)characteristics. Also, we extended SonarQube, a well-known tool for aggregating and visualising different measures for software quality such that it can now be used with repositories of business process models as well. This allows assessing the quality of a collection of models in the same way that is already well-established for assessing the quality of software code. Given the fact that models are early software development artifacts (and can even be executable and thus become a part of a software product), such a quality control can lead to the detection of possible problems in the early phases of the software development process.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6703084,no
Multi_level data pre_processing for software defect prediction,"Early detection of defective software components enables verification experts give much time and allocate scare resources to the problem areas of the system under development. This is the usefulness of defect prediction; defect prediction streamline testing efforts and reduce the development cost of software when as stated above it is detected at the early stages. An important step to building effective predictive models is to apply one or more sampling techniques. A model is claimed to be effective if it is able to correctly classify defective and non-defective modules as accurately as possible. In this paper we considered the outcome of data preprocessing by filtering and compared the performance with non-pre-processing original dataset. We compared the performance of the four different K-Nearest Neighbor(KNN-LWL, Kstar, IBK, IB1 classifiers) with Non Nested Generalized Exemplars (NNGE), Random Tree and Random Forest. We observed that our Multi-level data pre-processing; which includes double attribute selection and tripartite instance filtering enhanced the defect prediction results. We also observed that these two filtering methods improved performance of the prediction results independently; by using attribute selection only and resampling filtering. The excellent performance achieved could be attributed to the removal of irrelevant attributes by dimension reduction and Resampling also handled the problem of class imbalanced. These together led to the improved performance competences of the classifiers considered. NNGE as its name implies avoided generalization of some of the datasets; those with instances above 2,000; (JM1=10,885 and KC1=2,109) using pre-processing, this may be due to conflicting instances. We also used Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) measures to check the effectiveness of our model.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6703111,no
"A cognitive system for future-proof wireless microphones: Concept, implementation and results","Since several years, `cognitive radio' is given much attention in research and is seen as a future technology in communications. In this paper a cognitive radio system for Program Making and Special Event (PMSE) devices (e.g. wireless microphones) is presented and the results of a field test are discussed. First, we outline the challenges which the PMSE industry has to cope with and how changes in regulatory and the digital dividend in the TV UHF band has impact on the operation reliability of PMSE devices. Afterwards, a system is presented that uses cognitive radio techniques to ensure the high audio quality requirements and interference free operation of PMSE devices under the previously mentioned circumstances. Beside a general overview over the developed system we focus on a distributed spectrum sensing network as part of the cognitive information acquisition to monitor the current spectrum situation. The data is used by the cognitive engine to detect potential interferers and to trigger operation parameter changes (e.g. frequency or transmit power) of the PMSE devices to avoid link quality degradation. We present the built-up sensor nodes for the spectrum measurement and the developed software to control the sensor nodes and to preprocess the measured data. Finally, the results, collected with the field test platform in the fair-ground of Berlin, show convincingly how the developed cognitive system makes PMSE devices future-proof.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6704934,no
Agreement assessment of biochemical pathway models by structural analysis of their intersection,"In case of model development, it would be an advantage to assess the quality of available models looking for the best one or to find suitable parts of a published model to build a new one. The differences or contradictions in reconstructions can indicate the level of agreement between different authors about the topic of interest. The intersecting part of models can reveal also the differences in the scope of the models. Two pairs of models from BioCyc database were analyzed: 1) the Escherichia coli models ecol199310cyc and ecol316407cyc and 2) the Saccharomyces cerevisiae models iND750 and iLL672. The ModeRator software tool is used to compare models and generate their intersection model. The structural parameters of models are analyzed by the software BINESA. The study reveals very different parameters of the intersections of the pairs of the E. coli and the S. cerevisiae models. The models built by the same group of authors like in the case of E. coli is selected as an example of a high agreement between models and can be interpreted as a consensus part of two initial models. The intersection of the S. cerevisiae models demonstrates very different structural properties and the intersection model would not be able to function even after significant improvement. The structural analysis of the pairs of original models and their intersections is performed to determine which structural parameters can be used to determine a poor agreement between the pairs of models. It is concluded that an application of the automated comparison and intersection generation of two models can give a fast insight in the similarity of the models to find out the consensus level in modelling of metabolism of a particular organism. This approach can be used also to find similarities between the models of different organisms. Automation of intersection creation and structural analysis are enabling technologies of this approach.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6705232,no
A Simulink model of an active island detection technique for inverter-based distributed generation,"With the increased involvement of Distributed Power Generation Systems (DPGSs) into the conventional power system, the structure has evolved and therefore has brought in various challenges albeit improving flexibility and smartness of the system. This paper addresses modeling one of these challenges where a Simulink model for inverter-based distributed generation (IBDG) active islanding detection technique is introduced. Out of the various types of active islanding detection methods, the modeling of the general electric islanding detection method which uses the positive feedback of the voltage or frequency at the point of common coupling (PCC) for the detection of an island is presented. This methodology is modeled and applied for an IBDG connected to a low voltage distribution network. The simulation results are presented for a 20kW, three-phase IBDG showing that the system is able to detect islanding and cease the current flow from the IBDG even under critical operating condition of a close matching between the power delivered by the inverter and the load demand (zero nondetection zone operation).",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6705796,no
An interval nonparametric regression method,"This paper proposes a nonparametric multiple regression method for interval data. Regression smoothing investigates the association between an explanatory variable and a response variable. Here, each interval variable of the input data is represented by its range and center and a smooth function between a pair of vector of interval variables is defined. In order to test the suitability of the proposed model, a simulation study is undertaken and an application using thirteen project data of the NASA repository to estimate interval software size is also considered. These real data represent variability and/or uncertainty innate to the project data. The prediction quality is assessed by a mean magnitude of relative errors calculated from test data.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6706983,no
A trust management system for ad-hoc mobile clouds,"Most current cloud provisioning involves a data center model, in which clusters of machines are dedicated to running cloud infrastructure software. Ad-hoc mobile clouds, in which infrastructure software is distributed over resources harvested from machines already existed and used for other purposes, are gaining popularity. In this paper, a trust management system (TMC) for mobile ad-hoc clouds is proposed. This system considers availability, neighbors' evaluation and response quality and task completeness in calculating the trust value for a node. The trust management system is built over PlanetCloud which introduced the term of ubiquitous computing. EigenTrust algorithm is used to calculate the reputation trust value for nodes. Finally, performance tests were executed to prove the efficiency of the proposed TMC in term of execution time, and detecting node behavior.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6707167,no
Software applications integrated in the management of the patient with chronic liver disease,"The focal point in the debate on the management of the patient with chronic liver disease is the healthcare system and the health care services, with an accent on the health care provided at home, which can be discussed only when there is a connection integrated with the medical assistance, providing useful information. This paper discusses in theoretical synthesis (definitions, terms, classification), accompanied by empirical evidence, matters regarding the chronic diseases, followed by comments on the integration of the software applications in the management of the patient with chronic liver disease and the ethical concerns it involves. We believe that all the activities implicated illustrate the complexity of the management of the patient with a chronic disease, which concentrates on the minimization of the probability for complications and strives to offer a quality of life as best as possible.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6707324,no
Development of machine vision solution for grading of Tasar silk yarn,"Quality of Tasar fabric demands uniform coloured silk yarn during weaving. But, the variation of yarn colour depends on various natural factors like eco-race and feeding of silk worms, weather conditions etc and other production factors. So, silk yarns need to be sorted after production. At present, yarns are sorted manually by a group of experts which is subjective in nature. Again, due to lustrous nature of silk yarn, it reflects light and therefore it is difficult to ascertain the exact colour manually. Slight variation in colour is difficult to detect manually but the market demands lots with perfectly uniformly coloured yarns within the lot though the inter-lot variation in colour is encouraged. So, there is need to develop a solution which can grade the silk yarn objectively, reliably and mimic the human perception. This paper proposes a new machine vision solution for automatic grading of silk yarn based on its colour. The system consists of an enclosed cabinet which encompasses of a low cost digital camera, uniform illumination arrangement, weighing module, mechanical arrangement for sample holding and a grading software which applies image analysis technique using CIELab colour model with rotational invariant statistical feature based hierarchical grading algorithm for colour characterization. Performance of the system has been validated with the human experts and accuracy has been calculated as 91%.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6707547,no
The Application of Fuzzing in Web Software Security Vulnerabilities Test,"Web applications need for extensive testing before deployment and use, for early detecting security vulnerabilities to improve the quality of the safety of the software, the purpose of this paper is to research the fuzzing applications in security vulnerabilities. This article first introduces the common Web software security vulnerabilities, and then provide a comprehensive overview of the fuzzing technology, and using fuzzing tools Web fuzz to execute a software vulnerability testing, test whether there is a software security hole. Test results prove that fuzzing is suitable for software security vulnerabilities testing, but this methodology applies only to security research field, and in the aspect of software security vulnerabilities detection is still insufficient.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6709952,no
A toolset for easy development of test and repair infrastructure for embedded memories,"The development of a modern System-on-Chip (SOC) requires usage of embedded IP blocks from different vendors. One of widely used IP blocks in SOC is an embedded memory that usually occupies an essential die area. All IP blocks can have manufacturing defects. Meanwhile, in difference to other SOC components embedded memories are more defect-prone. STAR Hierarchical System (SHS) is an infrastructural IP solution for built-in test and repair engines of IP blocks. It is widely adopted now by a variety of customers which development flows essentially differ from each other. To cover the diversity of requests for user maintenance implying from difference in development flows we suggest a new approach basing on a library of SHS standard use flows implemented in a form of templates and a special toolset for their modification and verification. The implemented library of templates assists to design new flows quickly through retrieving and customizing specific examples. User can extend the library via insertion of new templates. A formal verification approach used already for business processes is successfully applied to the built library. The application is illustrated on some use flow examples.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6710328,no
A novel fault-tolerant task scheduling algorithm for computational grids,"A computational grid is a hardware and software infrastructure that provides consistent, dependable, pervasive and expensive access to high-end computational capabilities in a multi-institutional virtual organization. Computational grids provide computing power needed for execution of tasks. Scheduling the task in computing grid is an important problem. To select and assign the best resources for task, we need a good scheduling algorithm in grids. As grids typically consist of strongly varying and geographically distributed resources, choosing a fault-tolerant computational resource is an important issue. The main scheduling strategy of most fault-tolerant scheduling algorithms depends on the response time and fault indicator when selecting a resource to execute a task. In this paper, a scheduling algorithm is proposed to select the resource, which depends on a new factor called Scheduling Success indicator (SSI). This factor consists of the response time, success rate and the predicted Experience of grid resources. Whenever a grid scheduler has tasks to schedule on grid resources, it uses the Scheduling Success indicator to generate the scheduling decisions. The main scheduling strategy of the Fault-tolerant algorithm is to select resources that have lowest tendency to fail and having more experience in task execution. Extensive experiment simulations are conducted to quantify the performance of the proposed algorithm on GridSim. GridSim is a Java based discrete-event Grid simulation toolkit. Experiments have shown that the proposed algorithm can considerably improve grid performance in terms of throughput, failure tendency and worth.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6710529,no
"Fetal heart rate discovery: Algorithm for detection of fetal heart rate from noisy, noninvasive fetal ECG recordings","Fetal heart rate variability is known to be of a great meaning in assessing fetal health status. The simplest way of measuring fetal heart rate is to the non-invasive fetal ECG (fECG). A novel and efficient algorithm for detection of fetal ECG is needed. We analyzed 75 FECG recordings from the PhysioNet Challenge 2013 database. The detected RR interval peaks were compared with fetal scalp electrode measurements. Our algorithm focuses on detecting the most prominent part of the fetal QRS complex i.e. the RS slope. First, we remove long-range trends and find the two channels with the best quality fetal ECG. Then, we localize the repolarisations having the required characteristics (adequate amplitude and slope). Note, that the algorithm is adaptive and finds by itself the optimal RS slope characteristics for every recording. These steps allowed us to obtain accurate and reliable results of fetal R peak detection, even in the case of very noisy data. The preliminary test score of the PhysioNet Challenge were 132.664 (event 4) and 11.961 (event 5). The phase 3 score of the PhysioNet Challenge were 118.221 (event 4) and 10.663 (event 5). This is an opensource algorithm available at the PhysioNet library.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6712479,no
ECGlab: User friendly ECG/VCG analysis tool for research environments,"We present ECGlab, a cross-platform, user friendly, graphical user interface for assessing results from automated analysis of ECGs in research environments. ECGlab allows visual inspection and adjudication of ECGs. It is part of our recently developed framework to automatically analyze ECGs from clinical studies, including those in the US Food and Drug Administration (FDA) ECG Warehouse. ECGlab is written in C++ using open-source libraries. Supported ECG formats include Physionet, ISHNE and FDA XML HL7. ECG processing and automated analysis is done with ECGlib (ECG analysis library). ECGs can be loaded individually or grouped using ECGlib index format and information such as demographics or signal quality metrics can be loaded from metafiles to navigate through the ECGs and guide their review. The user can graphically adjudicate the ECGs in a semi-automatic or manual fashion. Vectorcardiograms can be assessed as well. A prototype for automatic extraction, based on heart rate stability and signal quality, of 10 seconds ECGs from continuous Holter recordings is also available. ECGlab, which has been successfully tested in Linux and Microsoft Windows, is currently being used to assess ECGs from clinical studies. We are working on making ECGlab open-source in order to facilitate ECG research.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6713492,no
3D analysis of myocardial perfusion from vasodilator stress computed tomography: Can accuracy be improved by iterative reconstruction?,"Computed tomography (CT) is an emerging tool to detect stress-induced myocardial perfusion abnormalities. We hypothesized that iterative reconstruction (IR) could improve the accuracy of the detection of significant coronary artery disease using quantitative 3D analysis of myocardial perfusion during vasodilator stress. We studied 39 patients referred for CT coronary angiography (CTCA) who agreed to undergo additional imaging with regadenoson (Astelias). Images were acquired using 256-channel scanner (Philips) and reconstructed using 2 different algorithms: filtered backprojection (FEP) and IR (iDose 7, Philips). Custom software was used to analyze both FEP and IR images. An index of severity and extent of perfusion abnormality was calculated for each 3D myocardial segment and compared to perfusion defects predicted by coronary stenosis > 50% on CTCA. Five patients with image artifacts were excluded. Ten patients with normal coronaries were used to obtain reference values, which were used to correct for x-ray attenuation differences among normal myocardial segments. Compared to the conventional FEP images, IR images had considerably lower noise levels, resulting in tighter histograms of x-ray attenuation. In the remaining 24 patients, IR improved the detection of perfusion abnormalities. Quantitative 3D analysis of MDCT images allows objective detection of stress-induced perfusion abnormalities, the accuracy of which is improved by IR.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6713503,no
An improved scheme for minimizing handoff failure due to poor signal quality,"There is a growing demand on the mobile wireless operators to provide continuous, satisfactory, and reliable quality of service to their teeming subscribers. Handoff failure which is one of the major causes of call drops is a major challenge prevalent in the mobile systems worldwide. Mobile users are more sensitive to handoff failure than new call failure. In this paper, an improved scheme for minimizing handoff failure due to poor signal quality was presented. The improved scheme was based on the following parameters: call signal quality, channel availability and the direction of movement of the mobile terminal to the base station. Through the use of MatLab software, the performance of the improved handoff scheme was compared with an existing handoff scheme. The comparison was based on the handoff failure probability and new call blocking probability for each of the schemes. The results obtained from simulation showed that the new scheme has a reduced handoff failure probability than the existing scheme.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6715637,no
Performance test and bottle analysis based on scientific research management platform,"The performance and service quality of a Web system become more and more important along with the development of Web application technology and popularization of Web application rapidly. There are many particularities and difficulties in the testing of web applications as to traditional application, especially in performance testing, such as unpredictable load, reality of designing scenario and veracity of analysis bottleneck. This paper which based on traditional Web system performance testing theory and used the testing tool named LoadRunner to analyze how to detect the shortage of Web system performance precisely. The method has been implemented in System of scientific research management platform, and has been obtained anticipative result. This paper has divide the web system method into six processes based on the Web system performance testing: Making performance testing plan, build performance testing environment, record and develop testing script, foundation testing scene, play the monitor scene and analysis testing result. And also gives Web performance test the general step.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6716635,no
Implementation and characterization of a reconfigurable time domain reflectometry system,"A practical architecture for pulsed radar and time domain reflectometry (TDR) is presented in this paper. Incorporating the software-defined radio paradigm, the prototype features a reconfigurable transceiver. Reconfigurability is achieved by implementing an arbitrary waveform generator (AWG) in a Field Programmable Gate Array (FPGA) and suitable digital-to-analog converters (DAC). The AWG allows for changes in the width and shape of a transmitted pulse on-the-fly, i.e. without the need for reprogramming. In the current implementation, the transmitter is able to achieve a minimum pulse width of 6.25ns, which result in a 62.5 cm range resolution for non-dispersive medium with 0.67 velocity factor. The resolution was verified by testing several cable setups with two differently-spaced discontinuities. The receiver, on the other hand, employs equivalent time sampling (ETS) through on-board analog-to-digital converters (ADC) and a custom delay generator. The ETS receiver was able to attain 0.357ns equivalent time sampling interval, which is equivalent to a 2.8 GHz sampling rate for periodic signals. This allows the transceiver to locate a discontinuity with 3.57cm accuracy in a non-dispersive medium with a velocity factor of 0.67, which was verified through experiments performed on open circuit-terminated cables with varying length. The system is intended to be used in detecting faults on a TDR cable buried underground to detect slope movement.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6717971,no
Study of led power fault online avoidance control strategy,"This paper study the power fault online avoidance strategy to improve the reliability of power source. According to real-time monitoring of the power running status, confirm the property and urgency degree. Through adaptive adjusting the stress on the component and executing the sound software shut off method, targeted protect the power source into different levels. The results show the occurrence probability of power source faults decrease and time between failures increase. This is especially applicable to severe environments and fields where power source should have high reliability.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6718833,no
Fault prediction by utilizing self-organizing Map and Threshold,"Predicting parts of the programs that are more defects prone could ease up the software testing process, which leads to testing cost and testing time reduction. Fault prediction models use software metrics and defect data of earlier or similar versions of the project in order to improve software quality and exploit available resources. However, some issues such as cost, experience, and time, limit the availability of faulty data for modules or classes. In such cases, researchers focus on unsupervised techniques such as clustering and they use experts or thresholds for labeling modules as faulty or not faulty. In this paper, we propose a prediction model by utilizing self-organizing map (SOM) with threshold to build a better prediction model that could help testers in labeling process and does not need experts to label the modules any more. Data sets obtained from three Turkish white-goods controller software are used in our empirical investigation. The results based on the proposed technique is shown to aid the testers in making better estimation in most of the cases in terms of overall error rate, false positive rate (FPR), and false negative rate (FNR).",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6720010,no
A study of comparative analysis of regression algorithms for reusability evaluation of object oriented based software components,"Reusability of software is found to be a key feature of quality. The most obvious outcomes of software reuse are overcoming the software crisis, advancing in software quality and improving productivity. The issue of spotting reusable software components from given existing system is very important but yet it is not much cultivated. For identification and evaluation of reusable software we use an approach that has foundation of software models and metrics. Idea of this study is to examine the competence and effectiveness of machine learning regression techniques which are experimented here to build precise and constructive evaluation model that can assess the reusability of Object Oriented based software components based on the values of five metrics of metrics suite presented by Shyam R. Chaidmber and Chris F. Kemerer. By setting different values of parameters of these algorithms, it is also concluded that which specific algorithm or class of algorithms is appropriate for reusability evaluation and with which parameter's values. For this comparative analysis we have used Weka and experimented different regression techniques as Multi-linear regression, Model Tree M5P, Standard instance-based learning scheme IBk and Meta-learning scheme Additive Regression. As the result of this analysis and experimentation Standard instance-based learning IBk with no distance weighting is found to be the best regression algorithm for reusability evaluation of Object Oriented software components using CK metrics.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6720609,no
Bootstrap Interval Estimation Methods for Cost-Optimal Software Release Planning,"We discuss interval estimation methods for cost-optimal software release time based on a discretized software reliability growth model. In our approach, we use a bootstrap method, in which we do not need to derive probability distributions of model parameters and optimal software release time analytically by using an asymptotic theory assuming a large number of samples. Then we estimate bootstrap confidence intervals of cost-optimal software release time based on two kinds of bootstrap confidence interval methods. Our numerical examples confirm that our bootstrap approach yields a simulation-based probability distribution of cost-optimal software release time from software fault-count data.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6721864,no
EucaBomber: Experimental Evaluation of Availability in Eucalyptus Private Clouds,"Cloud computing is a computational paradigm with increasing adoption because it offers resources as services in a dynamically scalable way through the Internet. The constant concern in providing cloud computing services in a reliable and uninterrupted manner inspires availability and reliability studies. A feasible method of performing such studies is through automated fault injection, enabling to observe the behavior of the cloud architecture under many conditions. This paper presents a fault injection tool, named EucaBomber, for reliability and availability studies in the Eucalyptus cloud computing platform. EucaBomber allows to define the probability distribution associated to the time between generated events. The efficiency of EucaBomber is verified through testbed scenarios where faults and repairs are injected in a private Eucalyptus cloud. The experimental results are cross-checked with results estimated from a Reliability Block Diagram, using the same input parameters of the experimental testbed. The test scenarios also illustrate how the tool may assist cloud systems administrators and planners to evaluate the system's availability and maintenance policies.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6722449,no
Optimization of test suite-test case in regression test,"Exhaustive product evolution and testing is required to ensure the quality of product. Regression testing is crucial to ensure software excellence. Regression test cases are applied to assure that new or adapted features do not relapse the existing features. As innovative features are included, new test cases are generated to assess the new functionality, and then included in the existing pool of test cases, thus escalating the cost and the time required in performing regression test and this unswervingly impacts the release, laid plan and the quality of the product. Hence there is a need to select minimal test cases that will test all the functionalities of the engineered product and it must rigorously test the functionalities that have high risk exposure. Test Suite-Test Case Refinement Technique will reduce regression test case pool size, reduce regression testing time, cost & effort and also ensure the quality of the engineered product. This technique is a regression test case optimization technique that is a hybrid of Test Case Minimization based on specifications and Test Case Prioritization based on risk exposure. This approach will facilitate achievement of quality product with decreased regression testing time and cost yet uncover same amount of errors as the original test cases.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6724206,no
Enhancing the Accuracy of Case-Based Estimation Model through Early Prediction of Error Patterns,"The paper tries to explore the importance of software fault prediction and to minimize them thoroughly with the advanced knowledge of the error-prone modules, so as to enhance the software quality. For estimating a new project effort, case-based reasoning is used to predict software quality of the system by examining a software module and predicting whether it is faulty or non faulty. In this research we have proposed a model with the help of past data which is used for prediction. Two different similarity measures namely, Euclidean and Manhattan are used for retrieving the matching case from the knowledge base. These measures are used to calculate the distance of the new record set or case from each record set stored in the knowledge base. The matching case(s) are those that have the minimum distance from the new record set. This can be extended to variety of system like web based applications, real time system etc. In this paper we have used the terms errors and faults, and no explicit distinction made between errors and faults. In order to obtain results we have used MATLAB 7.10.0 version as an analyzing tool.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6724354,no
Acoustic imaging of bump defects in flip-chip devices using split spectrum analysis,"In this paper the performance of multi-narrow-band spectral analysis was evaluated concerning defect detection in microelectronic components with flip-chip contacts. Today, flip-chip technology is widely applied for interconnecting silicon dies to a substrate within high-end semiconductor packaging technologies. The integrity of the bump solder interconnection is of major concern for the reliability in this technology. Non-destructive defect localization and analysis of the flip-chip interconnections operating in a semi-automated mode is strongly desired. Scanning acoustic microscopy (SAM) combined with subsequent signal analysis has high potential for non-destructive localization of defective flip-chip interconnects. Analyzing multiple narrow spectral bands of signals acquired by a scanning acoustic microscope enabled the identification and localization of defective flip-chip interconnects. In the current study a 180 MHz transducer with 8 mm focal length was employed for acoustic data acquisition by SAM. Those data were then analyzed off-line by discrete Fourier transformation, chirp z-transform and cosine transform using custom made MATLAB software. Through multi-narrow band spectral analysis, defective flip-chip interconnects that have not been revealed by standard acoustical imaging methods have been detected successfully. Acoustically found defects have been confirmed by subsequent FIB-cross sectioning and SEM imaging. The high resolution SEM imaging revealed complete and partial delamination at the interface between the die and the bump.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6725260,no
Towards a safety case for runtime risk and uncertainty management in safety-critical systems,"Many safety-critical systems have a human-in-the-loop for some part of their operation, and rely on the higher cognitive abilities of the human operator for fault diagnosis and risk-management decision-making. Although these operators are often experts on the processes being controlled, they still sometimes misjudge situations or make poor decisions. There is thus potential for Safety Decision Support Systems (SDSS) to help operators, building on past successes with Clinical Decision Support Systems in the health care industry. Such SDSS could help operators more accurately assess the system's state along with any associated risk and uncertainty. However, such a system supporting a safety critical operation inevitably attracts its own safety assurance obligations. This paper will outline those challenges and suggest an initial safety case architecture for SDSS.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6725802,no
Notice of Violation of IEEE Publication Principles<BR>The Right Thing to Do: Automating Support for Assisted Living with Dynamic Decision Networks,"Notice of Violation of IEEE Publication Principles<BR> ???The Right Thing To Do: Automating Support for Assisted Living with Dynamic Decision Networks???<BR> by Nayyab Zia Naqvi, Davy Preuveneers, Wannes Meert, Yolande Berbers in the Proceedings of the IEEE 10th International Conference on Ubiquitous Intelligence and Computing, and 10th International Conference on Autonomic and Trusted Computing (UIC/ATC), December 2013, pp. 262-269<BR><BR> After careful and considered review of the content and authorship of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE???s Publication Principles.<BR><BR> This paper contains significant portions of original text from the paper cited below. The original text was copied without attribution (including appropriate references to the original author(s) and/or paper title) and without permission.<BR><BR> ???Dynamic Decision Networks for Decision-Making in Self-Adaptive Systems: A Case Study???<BR> by Nelly Bencomo, Amel Belaggoun, Valerie Issarny in the Proceedings of the 8th International Symposium on Software Engineering for Adaptive and Self-Managing Systems (SEAMS), May 2013, pp. 113-122<BR><BR>In an era where ubiquitous systems will be mainstream, users will take a more passive role and these systems will have to make smart decisions on behalf of their users. Automating these decisions in a continuously evolving dynamic context can be challenging. First of all, the right thing to do usually depends on the circumstances and context at hand. What might be a good decision today, could be a bad one tomorrow. Secondly, the system should be made aware of the impact of its decisions over time so that it can learn from its mistakes as humans do. In this paper, we formulate a technique for decision support systems to mitigate runtime uncertainty in the observed context, and demonstrate our context-driven probabilistic framework for ubiquitous systems that addresses the above mentioned - hallenges. Our framework incorporates end-to-end Quality of Context (QoC) as a key ingredient to make well-informed decisions. It leverages Dynamic Decision Networks (DDN) to deal with the presence of uncertainty and the partial observability of context information, as well as the temporal effects of the decisions. Our experiments with the framework demonstrate the feasibility of our approach and potential benefits to automatically make the best decision in the presence of a changing environment.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6726218,no
A Smart Diagnostic Model for an Autonomic Service Bus Based on a Probabilistic Reasoning Approach,"The growing complexity and scale of systems implies challenges to include Autonomic Computing capabilities that help to maintain or improve the performance, availability and reliability characteristics. The autonomic management of a system can be defined deterministically based on experiment observations on the system and possible results of associated plans. However in dynamic environments with changing conditions and requirements, a better technique to diagnose observations and learn about the functioning conditions of the managed system is needed to guide the autonomic management. In the case of medical diagnostic, tests have included statistical and probabilistic models to aid and improve the results and select better medical treatments. In this paper we also adopt a probabilistic approach to define a Bayesian network from monitored data of an Enterprise Service Bus under different workload conditions. This model is used by the Autonomic Service Bus as a knowledge base to diagnose the cause of degradation problems and repair them. Experimental results assess the effectiveness of our approach.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6726238,no
Agents Based Monitoring of Heterogeneous Cloud Infrastructures,"Monitoring of resources is one among the major challenges that the virtualization brings with it within the Cloud environment since the user applications are often distributed on several nodes whose location is unknown a priori and can dynamically change. Consumers need to monitor their resources for checking that service levels are continuously compliant with the agreed SLA and for detecting under-utilization or overloading conditions of their resources. Furthermore the monitoring of service levels becomes critical because of the conflicts of interest that might occur between provider and customer in case of outage. In this paper a framework that supports the monitoring of Cloud infrastructure is presented. It provides to the user the possibility to check the state of his/her resources, even if they have been acquired from heterogeneous vendors. The proposed environment will offer high elasticity and extensibility by the provisioning of an high level of customization of the performance indexes and metrics.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6726254,no
The Nondestructive Testing Approach of Acoustic Emission for Environmentally Hazardous Objects,"Classical method of frequency distortion influence exclusion consists of FRF calculation with subsequent adjustment of received signals spectral characteristics. In the article, plane shape objects FRF can be calculated theoretically. Let us do FRF calculations for a long rod. The obtained results confirm high rate of AE signals emission irregularity and large fluctuations of spectrum components. This conclusion is valid for all ceramic materials under test. Acoustic emission method allows detecting and registering of only developing defects, prompting to classify them not by the size but by the danger level.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6726378,no
Using Jason to Develop Refactoring Agents,"Refactoring is one of the main techniques used when maintaining and evolving the software. It works by changing the software in such way that improves its internal structure without changing its external behavioral. This paper focuses on issues around software refactoring, such as: (i) figure out where the software should be refactored, (ii) define which refactoring(s) should be applied, (iii) ensure that the external behavior of the software will be preserved after applying the refactoring, (iv) evaluate the gains and losses on the software quality resulting of the refactoring, (v) apply the refactoring, and, (vi) maintain the consistence between the refactored program code and other artifacts. Given the amount of issues to be considered, the refactoring activities when done in a manual way are an error-prone and extremely expensive. This paper provides an extension of the Jason platform to enable the development of refactoring agents able to perform software refactoring in an autonomic way. Such approach accelerates the process of executing a refactoring and reduces the probability of introducing defects.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6726424,no
Automatic verification of test oracles in functional testing,"Functional testing of the applications becomes essential to detect the faults. Nowadays machine learning techniques have been implemented in the software engineering particularly in software testing field. But machine learning algorithms are difficult to detect the faults in certain applications because it is difficult to find the test oracle which is used to verify computed outputs. Here in this paper, a novel functional testing approach to verify the test oracles is attempted. The expected execution result for a given application is generated and verified with the oracle whether the application under test has or has not behaved correctly and issues a pass/fail verdict.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6726656,no
Unifying clone analysis and refactoring activity advancement towards C# applications,"Refactoring involves improve the quality of software and reduce the software complexity without affecting its external behavior. The research focuses code clones is a vital target of refactoring and code clones increase the internal complexity, maintenance effort and reduce the quality of software. A clone consists of two or more segments of code that duplicates with each other on the basis of distinct type of similar measurements. The developed algorithm insist a new semantic based clone management and refactoring system to detect and manage as well as refactor both exact and near-miss code clones. The major goal has to remove the clones in source code fragments by unifying the process of clone detection and refactoring. The implemented clone refactoring technique detects and fixes the clones in multiple classes using graph structure and methods. The code analyzer analyzes the user typed code by separating the auto generated code. Based on a graph structure, a new Abstract Semantic Graph Refactoring algorithm for detecting the clones in multiple classes of source code fragments, have been experimented in this research.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6726742,no
Performance analysis of circular patch antenna for breast cancer detection,"Now a day wireless communication systems has becoming the master for the world. The reason behind is the major component called Antenna. Even though peoples have sophisticated life, they are suffered by several diseases. Especially women are suffered by breast cancer. In earlier days X-ray mammography technique has been applied to detect the breast cancer. But the major drawback in this technique is the ionizing radiation level from X-rays which leads to cell death, cell mutation and fetal damage within the body. So the Microwave Breast Imaging (MBI) technique has been implemented to get the information about breast tissues. It uses low power when compared to X-ray technique. But this technique also has the impediments which gets high reflection from the breast tissue. Hence to overcome this problem, a circular patch antenna has been designed using flexible FR-4 substrate with the radius of 14.5 mm. The proposed antenna is also gets combined with the skin model to get the cancer level within the body. Both the models get designed in Ansoft HFSS software over an operating frequency range of 2.5GHz. The results of the proposed antenna gets analyzed such as return loss, VSWR, gain, directivity which is achieved of about -21dB, 1.2, 4.404dB, 4.48dB. The current density parameter plays a wide role here since it only tells about the cancer level. Higher the current density parameter increases the visibility of the cancer. Here it is achieved of about 127 A/m<sup>2</sup>. For this proposed antenna a miniaturization technique called Defected Ground Structure has been applied in the ground to improve the performance of the system.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6726791,no
Agent based tool for topologically sorting badsmells and refactoring by analyzing complexities in source code,"Code smells are smells found in source code. As the source code becomes larger and larger we find bad smells in the source code. These bad smells are removed using Refactoring. Hence experts say the method of removing bad smells without changing the quality of code is called as Refactoring[1]. But this Refactoring if not done properly is risky, and can take time (i.e.) might be days or weeks. Hence here we provide a technique to arrange these Bad smells analyze the complexities found in the source code and then Refactor them. These Bad smell detection and scheduling has been done manually or semi automatically. This paper provides a method of automatically detecting theses Bad smells. This Automatic detection of Bad smells are done with the help of Java Agent DEvelopment.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6726851,no
Verification of multi decisional reactive agent using SMV model checker,"On account of the evolution of technology, more complicated software arrives with the need to be verified to prevent the errors occurrence in a system which could generate fatal accidents and economic loss. These errors must be detected in an early stage during the development process to reduce redesign costs and faults. To ensure the correctness of software systems, formal verification provides an alternative approach to verify that an implementation of the expected system fulfills its specification. This paper focuses on the verification of reactive system behaviors specified by the Multi Decisional Reactive Agent (MDRA) and modeled using MDRA Profile. The objective in this paper is to use the Model Checking technique for MDRA Profile verification through the Model Checker SMV (Symbolic Model Verifier) to automatically verify the system properties expressed in temporal logic. The SMV mainly focusing on reactive systems provides a modular hierarchical descriptions and definition of reusable components. Besides, the expression of system properties is more described through both Computational Tree Logic (CTL) and Linear Temporal Logic (LTL).",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6727075,no
Fault tolerance on multicore processors using deterministic multithreading,"This paper describes a software based fault tolerance approach for multithreaded programs running on multicore processors. Redundant multithreaded processes are used to detect soft errors and recover from them. Our scheme makes sure that the execution of the redundant processes is identical even in the presence of non-determinism due to shared memory accesses. This is done by making sure that the redundant processes acquire the locks for accessing the shared memory in the same order. Instead of using record/replay technique to do that, our scheme is based on deterministic multithreading, meaning that for the same input, a multithreaded program always have the same lock interleaving. Unlike record/replay systems, this eliminates the requirement for communication between the redundant processes. Moreover, our scheme is implemented totally in software, requiring no special hardware, making it very portable. Furthermore, our scheme is totally implemented at user-level, requiring no modification of the kernel. For selected benchmarks, our scheme adds an average overhead of 49% for 4 threads.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6727107,no
Refinement of Adaptivity by Reflection,"Adaptivity is a system's ability to respond flexibly to dynamically changing needs. Adaptivity to human needs, wishes and desires-even to those that might be unconsciously present-is a particularly ambitious task. A digital system which is expected to behave adaptively has to learn about the needs and desires to which it shall adapt. Advanced adaptivity requires learning on the system's side. Under realistic application conditions, information about a human user available to a computerized system usually is highly incomplete. Therefore, the system's learning process is unavoidably error-prone and the knowledge on which the system's adaptive behavior has to rely is hypothetical by nature. Adaptive system behavior is improved by the system's ability to reflect on the reliability of its current hypothetical knowledge.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6727207,no
Intellectus: Multi-hop fault detection methodology evaluation,"Wireless Sensor Networks (WSNs) can experience problems (anomalies) during deployment, due to dynamic environmental factors or node hardware and software failures. These anomalies demand reliable detection strategies for supporting long term and/or large scale WSN deployments. Several strategies have been proposed for detecting specific WSN anomaly, yet there is still a need for more comprehensive anomaly detection strategies that jointly address network and node level anomalies. Intellectus methodology [23], [24], [25] build a tool that detected a new limited set of faults: sensor nodes may dynamically fail, be isolate and reboot and local topology control. These bugs are difficult to diagnose because the only externally visible characteristic is that no data is seen at the sink, from one or more nodes. This paper evaluate Intellectus methodology by different experiment in a Testbed network. In fact, Intellectus is be able to detect the injected fault and assess different scenarios of topology change.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6727621,no
Automated source code extension for debugging of OpenFlow based networks,"Software-Defined Networks using OpenFlow have to provide a reliable way to to detect network faults in operational environments. Since the functionality of such networks is mainly based on the installed software, tools are required in order to determine software bugs. Moreover, network debugging might be necessary in order to detect faults that occurred on the network devices. To determine such activities, existing controller programs must be extended with the relevant functionality. In this paper we propose a framework that can modify controller programs transparently by using graph transformation, making possible online fault management through logging of network parameters in a NoSQL database. Latter acts as a storage system for flow entries and respective parameters, that can be leveraged to detect network anomalies or to perform forensic analysis.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6727816,no
Lane marking aided vehicle localization,"A localization system that exploits L1-GPS estimates, vehicle data, and features from a video camera as well as lane markings embedded in digital navigation maps is presented. A sensitivity analysis of the detected lane markings is proposed in order to quantify both the lateral and longitudinal errors caused by 2D-world hypothesis violation. From this, a camera observation model for vehicle localization is proposed. The paper presents also a method to build a map of the lane markings in a first stage. The solver is based on dynamical Kalman filtering with a two-stage map-matching process which is described in details. This is a software-based solution using existing automotive components. Experimental results in urban conditions demonstrate an significant increase in the positioning quality.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6728444,no
A high-resolution imaging method based on broadband excitation and warped frequency transform,"Lamb wave has received widely attention in structural health monitoring (SHM). However, due to its multi-mode character and dispersion effect, the damage positioning and imaging resolution are limited. Besides Narrowband wave, which is usually adopted as excitation in Lamb wave detecting, broadband signal can also be chosen as excitation, with which plenty of signals can be obtained in one test to strengthen the brightness of damages by superposition. Warped frequency transform (WFT) is a new method based on group velocity dispersion curves, which can directly used to the received signals to suppress the dispersion and turn the signal to distance domain. In this paper, a new high-resolution method is proposed base on warped frequency transform and broadband excitation. The propagation of Lamb waves in damaged aluminum plate is simulated by finite element software ABAQUS, results show that high resolution images can be obtained with the proposed method.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6729719,no
Detection and Root Cause Analysis of Memory-Related Software Aging Defects by Automated Tests,"Memory-related software defects manifest after a long incubation time and are usually discovered in a production scenario. As a consequence, this frequently encountered class of so-called software aging problems incur severe follow-up costs, including performance and reliability degradation, need for workarounds (usually controlled restarts) and effort for localizing the causes. While many excellent tools for identifying memory leaks exist, they are inappropriate for automated leak detection or isolation as they require developer involvement or slow down execution considerably. In this work we propose a lightweight approach which allows for automated leak detection during the standardized unit or integration tests. The core idea is to compare at the byte-code level the memory allocation behavior of related development versions of the same software. We evaluate our approach by injecting memory leaks into the YARN component of the popular Hadoop framework and comparing the accuracy of detection and isolation in various scenarios. The results show that the approach can detect and isolate such defects with high precision, even if multiple leaks are injected at once.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6730788,no
Reliability analysis using fault tree method and its prediction using neural networks,"An electric power system is a network of electrical components used to supply, transmit and distribute electric power. It is an interconnected and complex system. It consist of many components like buses, substations, transformers, generators etc. The main function of the power system is to provide energy to the customers adequately and efficiently. In the normal situation, the power system is demanded to be highly efficient and safe. If any part within the system has failed, the amount of delivered power can be affected and huge economic losses can be induced. Consequently, reliability evaluation of the power system is of significant importance. Here reliability evaluation is done using fault tree method and it is done for 220 kV Kerala Power System. The numerical probability of failure is found from Open FTA software. Single line diagram of the 220kv substation in Kerala is simulated using ETAP software. Reliability indices are determined using this software. Reliability prediction is done using neural networks. Neural lab is used for the reliability prediction.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6731635,no
Loopy  An open-source TCP/IP rapid prototyping and validation framework,"Setting up host-to-board connections for hardware validation or hybrid simulation purposes is a time-consuming and error-prone process. In this paper we present a novel approach to automatically generate host-to-board connections, called the Loopy framework. The generated drivers enable blocking and non-blocking access to the hardware from high-level languages like C++ through an intuitive, object-based model of the hardware implementation. The framework itself is written in Java, and offers cross-platform support. It is open-source, well-documented, and can be enhanced with new supported languages, boards, tools, and features easily. Loopy combines several approaches presented in the past to an all-embracing helper toolkit for hardware designers, verification engineers, or people who want to use hardware accelerators in a software context. We have evaluated Loopy with real-life examples and present a case study with a complex MIMO system hardware-in-the-Ioop setup.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6732305,no
StEERING: A software-defined networking for inline service chaining,"Network operators are faced with the challenge of deploying and managing middleboxes (also called inline services) such as firewalls within their broadband access, datacenter or enterprise networks. Due to the lack of available protocols to route traffic through middleboxes, operators still rely on error-prone and complex low-level configurations to coerce traffic through the desired set of middleboxes. Built upon the recent software-defined networking (SDN) architecture and OpenFlow protocol, this paper proposes StEERING, short for SDN inlinE sERvices and forwardlNG. It is a scalable framework for dynamically routing traffic through any sequence of middleboxes. With simple centralized configuration, StEERING can explicitly steer different types of flows through the desired set of middleboxes, scaling at the level of per-subscriber and per-application policies. With its capability to support flexible routing, we further propose an algorithm to select the best locations for placing services, such that the performance is optimized. Overall, StEERING allows network operators to monetize their middlebox deployment in new ways by allowing subscribers flexibly to select available network services.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6733615,no
Sensor fault detection for a repetitive controller based D-FACT device,"This paper proposes a sensor fault detection system for a two-level DVR, controlled by a repetitive controller. The system compensates key voltage-quality disturbances namely; voltage sags, harmonic voltages, voltage imbalances, and control current during downstream fault, additionally detect any fault in sensor measurements as well. All the control actions of controller depend on the availability and quality of sensor measurement. However, measurements are inevitably subjected to faults caused by sensor failure, broken or bad connections, bad communication, or malfunction of some hardware or software. Therefore an auto-associative neural network based system is used here to detect any fault in sensor measurement. MATLAB/SIMULINK is used to carry out all modeling aspects of test system.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6733757,no
Situational requirement engineering: A systematic literature review protocol,"Requirements Engineering (RE) is known to be one of the critical phases in software development. Lots of work related to RE is already published. Field of RE is maturing day by day, leading to exploration at its deeper level. It is argued that RE is subject to situational characteristics. This exposure becomes even more when RE is performed in global software development environment. There is a need to identify these situational characteristics based on RE literature. We plan to systematically explore situational RE based studies to distinguish and account state of the art in situational RE based reported research. This paper objective is to provide the systematic literature review (SLR) protocol to illustrate a process for combining the situational RE work that will ultimately present a state of the art of the field in global software development environment. This SLR aims to not only summarize the data related to situational RE in form of situational characteristics but will also be useful for RE practitioners specifically working in global software development environment by providing a check list base upon situational characteristics. It will also assist RE researchers to discover knowledge gaps to distinguish needs and probability for future research directions in the field of situational RE in global software development environment.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6735060,no
Reliability analysis of an on-chip watchdog for embedded systems exposed to radiation and EMI,"Due to stringent constraints such as battery-powered, high-speed, low-voltage power supply and noise-exposed operation, safety-critical real-time embedded systems are often subject to transient faults originated from a large spectrum of noisy sources; among them, conducted and radiated Electromagnetic Interference (EMI). As the major consequence, the system's reliability degrades. In this paper, we present the most recent results involving the reliability analysis of a hardware-based intellectual property (IP) core, namely Real-Time Operating System - Guardian (RTOS-G). This is an on-chip watchdog that monitors the RTOS' activity in order to detect faults that corrupt tasks' execution flow in embedded systems running preemptive RTOS. Experimental results based on the Plasma processor IP core running different test programs that exploit several RTOS resources have been developed. During test execution, the proposed system was aged by means of total ionizing dose (TID) radiation and then, exposed to radiated EMI according to the international standard IEC 62.132-2 (TEM Cell Test Method). The obtained results demonstrate the proposed approach provides higher fault coverage and reduced fault latency when compared to the native (software) fault detection mechanisms embedded in the kernel of the RTOS.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6735179,no
Workload analysis and efficient OpenCL-based implementation of SIFT algorithm on a smartphone,"Feature detection and extraction are essential in computer vision applications such as image matching and object recognition. The Scale-Invariant Feature Transform (SIFT) algorithm is one of the most robust approaches to detect and extract distinctive invariant features from images. However, high computational complexity makes it difficult to apply the SIFT algorithm to mobile applications. Recent developments in mobile processors have enabled heterogeneous computing on mobile devices, such as smartphones and tablets. In this paper, we present an OpenCL-based implementation of the SIFT algorithm on a smartphone, taking advantage of the mobile GPU. We carefully analyze the SIFT workloads and identify the parallelism. We implemented major steps of the SIFT algorithm using both serial C++ code and OpenCL kernels targeting mobile processors, to compare the performance of different workflows. Based on the profiling results, we partition the SIFT algorithm between the CPU and GPU in a way that best exploits the parallelism and minimizes the buffer transferring time to achieve better performance. The experimental results show that we are able to achieve 8.5 FPS for keypoints detection and 19 FPS for descriptor generation without reducing the number and the quality of the keypoints. Moreover, the heterogeneous implementation can reduce energy consumption by 41% compared to an optimized CPU-only implementation.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6737002,no
Priority classification based fast intra mode decision for High Efficiency Video Coding,"The latest High-Efficiency Video Coding (HEVC) video coding standard version 1 offers 50% bit rate reduction against the H.264/AVC at the same visual quality. However, HEVC encoder complexity is tremendously increased. It is therefore important to develop efficient encoding algorithms for the success of HEVC based applications. In this paper, we propose a priority classification based fast intra mode decision to speed up the HEVC intra encoder. Each prediction unit (PU) is given a priority label out of four based on its spatial and temporal neighbor PU information as well as the predicted PU depth. Different processing strategy will be applied to different priority class, under the assumption that more computing resource should be allocated to the high priority class since its corresponding PU has the high potential to be chosen as the optima. Experiments are performed using all the common test sequences, and results show that, the encoder complexity is significantly reduced by about 46% for All Intra configuration with BD-Rate (Bjontegaard Delta Rate) increase less than 0.9% for luma component. Meanwhile, compared with several recent works, our proposed solution demonstrates the well trade-off between the coding efficiency and complexity reduction.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6737739,no
Motion blur compensation in scalable HEVC hybrid video coding,"One main element of modern hybrid video coders consists of motion compensated prediction. It employs spatial or temporal neighborhood to predict the current sample or block of samples, respectively. The quality of motion compensated prediction largely depends on the similarity of the reference picture block used for prediction and the current picture block. In case of varying blur in the scene, e.g. caused by accelerated motion between the camera and objects in the focal plane, the picture prediction is degraded. Since motion blur is a common characteristic in several application scenarios like action and sport movies we suggest the in-loop compensation of motion blur in hybrid video coding. Former approaches applied motion blur compensation in single layer coding with the drawback of needing additional signaling. In contrast to that we employ a scalable video coding framework. Thus, we can derive strength as well as the direction of motion of any block for the high quality enhancement layer by base-layer information. Hence, there is no additional signaling necessary neither for predefined filters nor for current filter coefficients. We implemented our approach in a scalable extension of the High Efficiency Video Coding (HEVC) reference software HM 8.1 and are able to provide up to 1% BD-Rate gain in the enhancement layer compared to the reference at the same PSNR-quality for JCT-VC test sequences and up to 2.5% for self-recorded sequences containing lots of varying motion blur.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6737746,no
A rule-based instantaneous denoising method for impulsive noise removal in range images,"To improve comprehensive performance of denoising range images, A rule-based instantaneous denoising method for impulsive noise removal (RID-INR) is proposed in this paper. Based on silhouette features analysis for two typical impulsive noise (IN), dropouts and outliers, a few new coefficients are defined to describe their exclusive features. Founded on several discriminant criteria, the principles of dropout IN detection and outlier IN detection are detailed demonstrated. Subsequently, IN denoising is performed by an Index Distance Weighted Mean filter after a nearest non-IN neighbors searching process. Originated from a theoretical model of invader occlusion, variable window technique is presented for enhancing adaptability of our method, accompanying with practical criteria of adaptive variable window size determination. A complete algorithm has been implemented as embedded modules in two self-developed software. A series of experiments on real range images of single scan line are carried out with comprehensive evaluations in terms of computational complexity, time expenditure and denoising quality. It is indicated that the proposed method can not only detect the impulsive noises with high accuracy, but also denoise them with outstanding efficiency, quality, and adaptability. The proposed method is inherently invariant to translation and rotation transformations, since all the coefficients are established based on distances between the points or their ratio. Therefore, RID-INR is qualified for industrial applications with stringent requirements due to its practicality.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6737806,no
Exposing fake bitrate video and its original bitrate,"Video bitrate, as one of the important factors that reflect the video quality, can be easily manipulated via some video editing softwares. In some forensic scenarios, for example, video uploaders of video-sharing websites may increase video bitrate for seeking more commercial profits. In this paper, we try to detect those fake high bitrate videos, and then further to estimate their original bitrates. The proposed method is mainly based on the fact that if the video bitrate has been increased with the help of video editing software, its essential video quality will not increase at all. By analyzing the quality of the questionable video and a series of its re-encoded versions with different lower bitrates, we can obtain a feature curve to measure the change of the video quality, and then we propose a compact feature vector (3-D) to expose fake bitrate videos and their original bitrates. The experimental results evaluated on both CIF and QCIF raw sequences have shown the effectiveness of the proposed method.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6738925,no
SAFe: A Secure and Fast Auto Filling Form System,"Current practices in government and private offices to register a service are time-consuming and prone to fraud. As a consequence, government offices are unable to provide high-quality services to citizens; while private offices make less productivity and profits from their services. This paper presents an innovative registration system for Malaysian when applying services at government and private offices. Referred to as Secure and Fast Auto Filling Form System (SAFe), the proposed system retrieves the customers' information from their MyKad and transfers the information to a digital application form. For security measures, the fingerprint verification of the authentic customers is required before their formation can be retrieved from MyKad. The proposed system is developed using the Visual Basic software and a commercial smartcard reader. Results of the performance evaluation show that SAFe shortens the time to complete the registration securely. Therefore, SAFe can improve the service quality and productivity of government and private offices.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6738995,no
A combined analysis method of FMEA and FTA for improving the safety analysis quality of safety-critical software,"Software safety analysis methods are used broadly in safety-critical systems to secure software safety and to recognize potential errors during software development, particularly at the early stage. FMEA and FTA are two traditional safety analysis methods, both of which provide a complementary way of identifying errors and tracking their possible influences. They have already been widely adopted in safety-critical industries. However, the effectiveness of FMEA and FTA depends on a complete understanding of the software being analyzed. Unlike hardware safety analysis, software safety analysis is usually a process of iteration. It is more difficult to get a comprehensive understanding of the software being analyzed at the early stage of software life cycle. A combined analysis method of FMEA and FTA was presented in this paper, which could detect more potential errors of software at the early stage. An analysis process which can convert and verify between FMEA and FTA was created. A semi-auto analyzing tool was developed to carry the process. Comparison experiments were carried out to testify the effectiveness of this method, which showed that the combined method proposed by this paper achieved better results.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6740435,no
Autonomous control and simulation of the VideoRay Pro III vehicle using MOOS and IvP Helm,Most underwater vehicles are controlled by human operators through tethered cables which inhibit range and affect the hydrodynamics. The overall performance of these promising vehicles would improve through the employment of higher levels of autonomy. Implementation of open source autonomous guidance software in an off the shelf underwater vehicle is explored here as a solution. Development and implementation of this autonomous guidance and vehicle control is greatly facilitated through the use of an accurate vehicle simulator. Running real world tests of an underwater vehicle is extremely time intensive and error prone. The analysis of the vehicle performance underwater is extremely challenging with limited accurate positioning sensing e.g. GPS. A vehicle simulator allows for faster development of the system by providing vehicle performance information prior to expensive real world missions. This research presents a method for simulation and testing of autonomous guidance and control in a vehicle accurate simulator for the VideoRay Pro III underwater vehicle and demonstrates the capability through simulated examples and analysis.,2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6741205,no
Performance analysis of a fault-tolerant exact motif mining algorithm on the cloud,"In this paper, we present the performance analysis and design challenges of implementing a fault-tolerant parallel exact motif mining algorithm leveraging the services provided by the underlying cloud storage platform (e.g., data replication, node failure detection). More specifically, first, we present the design of the intermediate data structures and data models that are needed for effective parallelization of the motif mining algorithm on the cloud. Second, we present the design and implementation of a fault-tolerant parallel motif mining algorithm that enables the data analytic system to recover from arbitrary node failures in the cloud environment by detecting node failures and redistributing remaining computational tasks in real-time. We also present a data caching scheme to improve the system performance even further. We evaluated the impact of various factors such as the replication factor and random node failures on the performance of our system using two different datasets, namely, an EOG dataset and an image dataset. In both cases, our algorithm exhibits superior performance over the existing algorithms, thus demonstrating the effectiveness of our presented system.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6742786,no
A reconfigurable AC excitation control system for impulse hydroelectric generating unit based on fault-tolerance,"Aiming at the AC excitation control of high-head impulse hydroelectric generating unit, and in order to improve system reliability, a reconfigurable AC excitation control system for impulse hydroelectric generating unit based on fault-tolerance control is introduced. The reconfigurable scheme and structure of hardware system for AC excitation system is proposed. On the basis of the analysis of main system faults, an intelligent control strategy is proposed, with the construction of reconfigurable software modules, including basic structure of software platform, reconfiguration of detecting algorithm, and reconfiguration of control algorithm for digital valves. The experimental results show that the reconfigurable AC excitation control system can realize intelligent diagnosis of system fault, reconfiguration of system structure, and active fault-tolerance control, with the improvement of system reliability.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6743257,no
Visual Quality and File Size Prediction of H.264 Videos and Its Application to Video Transcoding for the Multimedia Messaging Service and Video on Demand,"In this paper, we address the problem of adapting video files to meet terminal file size and resolution constraints while maximizing visual quality. First, two new quality estimation models are proposed, which predict quality as function of resolution, quantization step size, and frame rate parameters. The first model is generic and the second takes video motion into account. Then, we propose a video file size estimation model. Simulation results show a Pearson correlation coefficient (PCC) of 0.956 between the mean opinion score and our generic quality model (0.959 for the motion-conscious model). We obtain a PCC of 0.98 between actual and estimated file sizes. Using these models, we estimate the combination of parameters that yields the best video quality while meeting the target terminal's constraints. We obtain an average quality difference of 4.39% (generic model) and of 3.22% (motion-conscious model) when compared with the best theoretical transcoding possible. The proposed models can be applied to video transcoding for the Multimedia Messaging Service and for video on demand services such as YouTube and Netflix.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6746814,no
Tutorial: Digital microfluidic biochips: Towards hardware/software co-design and cyber-physical system integration,"This tutorial will first provide an overview of typical bio-molecular applications (market drivers) such as immunoassays, DNA sequencing, clinical chemistry, etc. Next, microarrays and various microfluidic platforms will be discussed. The next part of the tutorial will focus on electro-wetting-based digital micro-fluidic biochips. The key idea here is to manipulate liquids as discrete droplets. A number of case studies based on representative assays and laboratory procedures will be interspersed in appropriate places throughout the tutorial. Basic concepts in micro-fabrication techniques will also be discussed. Attendees will next learn about CAD and reconfiguration aspects of digital microfluidic biochips. Synthesis tools will be described to map assay protocols from the lab bench to a droplet-based microfluidic platform and generate an optimized schedule of bioassay operations, the binding of assay operations to functional units, and the layout and droplet-flow paths for the biochip. The role of the digital microfluidic platform as a programmable and reconfigurable processor for biochemical applications will be highlighted. Cyber-physical integration using low-cost sensors and adaptive control, software will be highlighted. Cost-effective testing techniques will be described to detect faults after manufacture and during field operation. On-line and off-line reconfiguration techniques will be presented to easily bypass faults once they are detected. The problem of mapping a small number of chip pins to a large number of array electrodes will also be covered. With the availability of these tools, chip users and chip designers will be able to concentrate on the development and chip-level adaptation of nano-scale bioassays (higher productivity), leaving implementation details to CAD tools.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6749708,no
A control strategy for inverter-interfaced microgrids under symmetrical and asymmetrical faults,"The increase in the renewable energy penetration level imposes the microgrid concept, which can be consisted of several inverter-interfaced distributed resources (DERs) and loads, operating in dual state; either connected with the utility grid or isolated in island mode. The power sharing among the connected DERs is carried out by the combination of the droop characteristics of each DER according to the active and reactive power demand of the loads. When a fault occurs within the microgrid operating in island mode, it is very difficult to be detected due to the lack of large current production capacity. The fault situation can be further complicated, if the fault takes place between two phases or between a single phase and the earth. This paper proposes a fault detection method for the symmetrical and asymmetrical faults, which is independent of any further communication means. The three-phase faults can be detected through the impedance variation of the islanded microgrid, while the asymmetrical ones by the negative sequence components of the output voltage of each DER. A significant contribution is the voltage recovery after the fault clearance with a seamless transient effect. After the fault clearance, the microgrid will continue feeding its loads, through the implementation of a positive- and negative-sequence control strategy. The effectiveness of the proposed control strategy is evaluated through a set of simulation tests, conducted in PSIM software environment.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6749752,no
Real-time model base fault diagnosis of PV panels using statistical signal processing,"This paper proposes new method of monitoring and fault detection in photovoltaic systems, based mainly on the analysis of the power losses of the photovoltaic system (PV) by using statistical signal processing. Firstly, real time new universal circuit based model of photovoltaic panels is presented. Then, the development of software fault detection on a real installation is performed under the MATLAB/Simulink environment. With model based fault diagnosis analysis, residual signal from comparing Simulink and real model is generated. To observe clear alarm signal from arbitrary data captured, Wald test technic is applied on residual signal. A model residual based on Sequential Probability Ratio Test (WSPRT) framework for electrical fault diagnosis in PV system is introduced.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6749826,no
AptStore: Dynamic Storage Management for Hadoop,"Typical Hadoop setups employ Direct Attached Storage (DAS) with compute nodes and uniform replication of data to sustain high I/O throughput and fault tolerance. However, not all data is accessed at the same time or rate. Thus, if a large replication factor is used to support higher throughput for popular data, it wastes storage by unnecessarily replicating unpopular data as well. Conversely, if less replication is used to conserve storage for the unpopular data, it means fewer replicas for even popular data and thus lower I/O throughput. We present Apt Store, a dynamic data management system for Hadoop, which aims to improve overall I/O throughput while reducing storage cost. We design a tiered storage that uses the standard DAS for popular data to sustain high I/O throughput, and network-attached enterprise filers for cost-effective, fault-tolerant, but lower-throughput storage for unpopular data. We design a file Popularity Prediction Algorithm (PPA) that analyzes file system audit logs and predicts the appropriate storage policy of each file, as well as use the information for transparent data movement between tiers. Our evaluation of Apt Store on a real cluster shows 21.3% improvement in application execution time over standard Hadoop, while trace driven simulations show 23.7% increase in read throughput and 43.4% reduction in the storage capacity requirement of the system.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6753775,no
Dynamic Workflow Reconfigurations for Recovering from Faulty Cloud Services,"The workflow paradigm is a well established approach to deal with application complexity by supporting the application development by composition of multiple activities. Furthermore workflows allow encapsulating parts of a problem inside an activity that can be reused in different workflow application scenarios for instance long-running experiments such as the ones involving data streaming. These workflows are characterized by multiple, eventually infinite, iterations processing datasets in multiple activities according to the workflow graph. Some of these activities can invoke Cloud services often unreliably or with limitations on quality of service provoking faults. After a fault the most common approach requires restarting of the entire workflow which can lead to a waste of execution time due to unnecessarily repeating of computations. This paper discuss how the AWARD (Autonomic Workflow Activities Reconfigurable and Dynamic) framework supports recovery from activity faults using dynamic reconfigurations. This is illustrated through an experimental scenario based on a long-running workflow where an activity fails when invoking a Cloud-hosted Web service with a variable level of availability. On detecting this, the AWARD framework allows the dynamic reconfiguration of the corresponding activity to access a new Web service, and avoiding restarting the complete workflow.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6753782,no
Improvement on ABDOM-Qd and Its Application in Open-Source Community Software Defect Discovery Process,"ABDOM-Q<sub>d</sub> is a model to describe the characteristics of software defect discovery time-ordered process, such as periodicity, attenuation, oscillation, incremental and discrete. It can help testing participants to evaluate testing quality and to predict testing process with the time-ordered software defect discovery amounts in a good organized software testing process. Due to the poor organization in open-source software community, software defect discovery data show their obvious uncertainties such as mutations and randomness. In the early study of ABDOM/ABDOM-Q<sub>d</sub> this kind of process was excluded from the discussion. So it becomes an issue that whether ABDOM-Q<sub>d</sub> model can be applied to open-source community software defect discovery process and reveal new natures and characteristics. In order to answer the question, the normalization of b in ABDOM-Q<sub>d</sub> were discussed firstly under the curves paradigms and theirs actual significances discussion in conditions b >0 and b <; 0. With the discussion results, a software defect discovery cycle stability coefficient B was proposed and the improved model ABDOM-QB<sub>d</sub> with stronger describing capability was established. Then ABDOM-QB<sub>d</sub> was applied in a NASA's open-source project, which is a typical representative of open-source projects, to fit the software defect data and a good fitting result was obtained. Finally with the application results the model's applicability and the characteristics of open-source community software defects discovery process were preliminarily discussed.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6754268,no
Patch Reviewer Recommendation in OSS Projects,"In an Open Source Software (OSS) project, many developers contribute by submitting source code patches. To maintain the quality of the code, certain experienced developers review each patch before it can be applied or committed. Ideally, within a short amount of time after its submission, a patch is assigned to a reviewer and reviewed. In the real world, however, many large and active OSS projects evolve at a rapid pace and the core developers can get swamped with a large number of patches to review. Furthermore, since these core members may not always be available or may choose to leave the project, it can be challenging, at times, to find a good reviewer for a patch. In this paper, we propose a graph-based method to automatically recommend the most suitable reviewers for a patch. To evaluate our method, we conducted experiments to predict the developers who will apply new changes to the source code in the Eclipse project. Our method achieved an average recall of 0.84 for top-5 predictions and a recall of 0.94 for top-10 predictions.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6754342,no
A Practical Study of Debugging Using Model Checking,"Debugging is one of the most time-consuming tasks in software development. The application of a model-checking technique in debugging has strong potential to solve this problem. Here, lessons learned through our practical experiences with POM/MC are discussed. The aim of this proposed hypothesis-based method of debugging is not only to reproduce a failure as counterexamples, but also to obtain a counterexample that is useful for detecting the fault or the cause of the failure. One of the characteristics of the proposed approach is that it degenerates a source code in order to clarify the fault. An example of this degeneration shows that the method is useful for fault analysis and avoidance of the """"state-explosion"""" problem. Furthermore, the characteristics of debugging using POM/MC are explained from the viewpoint of debugging hypotheses.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6754367,no
Comparison of stamp classification using SVM and random ferns,"In distributed software systems and processes that use large amounts of documents there is an essential need for data mining and document classification algorithms. These algorithms are aimed at optimizing the process, making it less error prone. In this paper we deal with the problem of document classification using two machine learning algorithms. Both algorithms use stamp images in documents to classify the document itself. The idea is to classify the document stamp and then, using known information about the stamp owner, search the rest of the document for relevant data. Our results are based on actual documents used in the process of debt collection and our training and test datasets are randomly picked from an existing database with over three million documents. The mentioned machine learning classification algorithms are implemented and compared in terms of classification accurateness, robustness and speed.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6755055,no
A Byzantine Fault Tolerance Model for a Multi-cloud Computing,"Data security has become an important requirement for clients when dealing with clouds that may fail due to faults in the software or hardware, or attacks from malicious insiders. Hence, building a highly dependable and reliable cloud system has become a critical research problem. This paper presents BFT-MCDB (Byzantine Fault Tolerance Multi-Clouds Database), a practical model for building a system with Byzantine fault tolerance in a multi-cloud environment. The model relies on a novel approach that combines Byzantine Agreement protocols and Shamir's secret sharing approach to detect Byzantine failure in a multi-cloud computing environment as well as ensuring the security of the stored data within the cloud. Using qualitative analysis, we show that adopting the Byzantine Agreement protocols in the proposed BFT-MCDB model increases system reliability and enables gains in regard to the three security dimensions (data integrity, data confidentiality, and service availability). We also carry out experiments to determine the overheads of using the Agreement protocols.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6755208,no
Memorization of Materialization Points,"Data streaming frameworks, constructed to work on large numbers of processing nodes in order to analyze big data, are fault-prone. Not only the large amount of nodes and network components that could fail are a source of errors. Development of data analyzing jobs has the disadvantage that errors or wrong assumptions about the input data may only be detected in productive processing. This usually leads to a re-execution of the entire job and re-computing all input data. This can be a tremendous profuseness of computing time if most of the job's tasks are not affected by these changes and therefore process and produce the same exact data again. This paper describes an approach to use materialized intermediate data from previous job executions to reduce the number of tasks that have to be re-executed in case of an updated job. Saving intermediate data to disk is a common technique to achieve fault tolerance in data streaming systems. These intermediate results can be used for memoization to avoid needless re-execution of tasks. We show that memoization can decrease the runtime of an updated job distinctly.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6755368,no
Substitute Eyes for Blind with Navigator Using Android,Our aim is to develop an affordable technology which is cheap and can be a substitute eyes for blind people. As a first step to achieve this goal we decided to make a Navigation System for the Blind. Our device consists of the following 2 parts: 1) Embedded Device: can be used to detect local obstacles such as walls/cars/etc. using 2 ultrasonic sensors to detect the obstacles and vibrator motors to give tactile feedback to the blind. 2) Android App: will give the navigation directions. Can be installed on any android device: cellphone/tablet/etc.,2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6757112,no
Road Accident Prevention Unit (R.A.P.U) (A Prototyping Approach to Mitigate an Omnipresent Threat),"Road accidents claim a staggeringly high number of lives every year. From drunk driving, rash driving and driver distraction to visual impairment, over speeding and over-crowding of vehicles, the majority of road accidents occur because of some fault or the other of the driver/occupants of the vehicle. According to the report on Road Accidents in India, 2011 by the Ministry of Transport and Highways, Government of India, approximately every 11th person out of 100,000 died in a road accident and further, every 37th person was injured in one, making it an alarming situation for a completely unnecessary cause of death. The above survey also concluded that in 77.5 percent of the cases, the driver of the vehicle was at fault. The situation makes it a necessity to target the root cause of road accidents in order to avoid them. While car manufacturers include a system for avoiding damages to the driver and the vehicle, no real steps have been taken to actually avoid accidents. Road Accident Prevention Unit is a step forward in this stead. This design monitors the driver's state using multiple sensors and looks for triggers that can cause accidents, such as alcohol in the driver's breath and driver fatigue or distraction. When an alert situation is detected, the system informs the driver and tries to alert him. If the driver does not respond within a stipulated time, the system turns on a distress signal outside the vehicle to inform nearby drivers and sends a text message to the driver's next of kin about the situation. A marketable design would also shut down power to the vehicle, thus providing maximum probability for avoiding road accidents and extending a crucial window for preventive and mitigation measures to be taken.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6757115,no
Judgement of ball mill working condition in combined grinding system,"In the cement grinding system, the quantity of raw material in the ball mill has an important effect on the cement production. Based on the quantity, the working conditions of the ball mill are divided into three states, the full state, the normal state, the empty state. Normally, it's hard to predict the working station of the ball mill and the precision is low. This paper, least square method and its improved algorithm is used as a new method to judge the working condition of ball mill. A parameter  is added to the finite impulse response model. Through the value of the ball mill working condition can be judged so the operator can change the parameters of the equipment in time and an on-line software is programmed by VC++. As a result, both the quality of the cement and the stability of the equipment are improved.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6758068,no
The research of natural gas pipeline leak detection based on adaptive filter technology,"This paper expounds the potential safety hazard of natural gas pipeline leakage, and briefly summarizes the means and defects of pipeline leak detection recently. A acoustic detection method of natural gas pipeline leak based on adaptive filtering algorithm is proposed so that the situation the natural gas pipeline leakage is difficult to find and detect can be solved. The sensor structure and flow diagram are elaborated, and pipeline leakage simulation test system was designed. This experiment processes and discriminates of leakage characteristics through the upper machine software Labview. Eventually the leak can be detected and located in fundamental.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6758181,no
Study on sensors using in wall climbing robot for motion controller,"The storage tank anticorrosion quality seeming to be particularly important with the rapid development of the domestic oil. This design is used to detect the coating's thickness of storage tanks, PC and the console computer control the wall-climbing robot's motion. The goal of this paper is to work on study of sensors using in wall climbing robot for motion controller, which include IR detector sensor and Ultrasonic PING Sensor. The system design includes hardware and software systems: the hardware control panel using STC12C5A60S2 master chip, receive the host computer's instructions, drive wall-climbing robot to walk; the host computer's software system is developed by Visual C# and the consol computer's software is developed by Keil C. Ultimately it achieves coating thickness of the oil storage tank.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6758183,no
Closed-loop subspace projection based state-space model-plant mismatch detection and isolation for MIMO MPC performance monitoring,"In multivariate model predictive control (MPC) systems, the quality of multi-input multi-output (MIMO) plant models has significant impact on the controller performance in different aspects. Though re-identification of plant models can improve model quality and prediction accuracy, it is very time consuming and economically expensive in industrial practice. Therefore, the automatic detection and isolation of the model-plant mismatch is highly desirable to monitor and improve MPC performance. In this paper, a new closed-loop MPC performance monitoring approach is proposed to detect model-plant mismatch within state-space formulations through subspace projections and statistical hypothesis testing. A monitoring framework consisting of three quadratic indices is developed to capture model-plant mismatches precisely. The validity and effectiveness of the proposed method is demonstrated through a paper machine headbox control example.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6760860,no
Impact of refactoring on external code quality improvement: An empirical evaluation,"Refactoring is the process of improving the design of the existing code by changing its internal structure without affecting its external behaviour, with the main aims of improving the quality of software product. Therefore, there is belief that refactoring improves quality factors such as understandability, flexibility, and reusability. Moreover, there are also claims that refactoring yields higher development productivity. However, there is limited empirical evidence to support such assumptions. The objective of this study is to validate/invalidate the claims that refactoring improves software quality. Experimental research approach was used to achieve the objective and ten selected refactoring techniques were used for the analysis. The impact of each refactoring technique was assessed based on external measures namely; analysability, changeability, time behaviour and resource utilization. After analysing the experimental results, among the tested ten refactoring techniques, Replace Conditional with Polymorphism ranked in the highest as having high percentage of improvement in code quality. Introduce Null Object was ranked as worst which is having highest percentage of deteriorate of code quality.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6761156,no
Let's talk together: Understanding concurrent transmission in wireless sensor networks,"Wireless sensor networks (WSNs) are increasingly being applied to scenarios that simultaneously demand for high packet reliability and short delay. A promising technique to achieve this goal is concurrent transmission, i.e. multiple nodes transmit identical or different packets in parallel. Practical implementations of concurrent transmission exist, yet its performance is not well understood due to the lack of expressive models that accurately predict the success of packet reception. We experimentally investigate the two phenomena that can occur during concurrent transmission depending on the transmission timing and signal strength, i.e. constructive interference and capture effect. Equipped with the thorough understanding of these two phenomena, we propose an accurate prediction model for the reception of concurrent transmission. The extensive measurements carried out with varying number of transmitters, packet length and signal strength verify the excellent quality of our model, which provides a valuable tool for protocol design and simulation of concurrent transmission in WSNs.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6761237,no
Fault isolation by test scheduling for embeded systems using probabilistics approach,This paper deals with the isolation of the failed components in the system. Each component can be affected in a random way by failures. The detection of the state of a component or a subsystem is carried out using tests. The objective of this research is to exploit the techniques of built in test and available knowledge to generate the sequence of tests which makes it possible to locate quickly the whole of the components responsible for the failure of the system. One considers an operative system according to a series structure for which one knows the cost of tests and the conditional probability that a component is responsible for the failure. The various strategies of diagnosis are analyzed. The treated algorithms call upon the probabilistic analysis of the systems.,2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6761523,no
Real-time mobility aware shoe  Analyzing dynamics of pressure variations at important foot points,"Data collected from pressure sensors attached to shoe insole is a rich source of information about the dynamics of the varying pressure exerted at different points while a person is in motion. Depending on the accuracy and the density of the points of data collection, this could be applied for different uses. Analyzing the time series data of the pressure, it is possible (1) to detect faults in walking and balancing problems for old people, (2) to design personalized foot orthoses, (3) to calculate the calorie burnt, even when walking and jogging are mixed, and the road slope changes, (4) to find subtle faults in sprinters or tennis players, (5) for person identification, (6) even for initiating alarm arising from mishandling of machines (like accelerator pedal of a car). In this work, we look for an efficient, real-time, yet cheap solution. We use a few thin, cheap, resistive pressure sensors, placed at critical points on the insole of the shoe to collect dynamic pressure data, preprocess it and extract features to identify the mobility speed. Nearly 100% classification accuracy was achieved. Thus, the target to classify whether the person is walking or jogging or climbing up or down the stairs was found to be possible, even with very simple gadget. From the time duration and the speed, the distance travel could be calculated. If, in addition, this signal could tell us the body-weight, we could accurately calculate the calorie burnt at the end of the day. The analysis method, and results from real experiments are discussed.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6765414,no
Research on converting CAD model to MCNP model based on STEP file,"MCNP input file has the characteristics of complicated form and is error-prone in describing geometry model. Therefore we need design and implement an algorithm of conversion general CAD model to MCNP model to solve the existing problems in MCNP aided modeling software. And it can convert the CAD model to MCNP input file. In order to achieve the above goal, this paper concentrates on converting CAD model to MCNP model, after analyzing STEP neutral File and MCNP INP file, we designed an algorithm to achieve converting STEP file to INP file. The result of experiment shows that it has better applicability than the other converting algorithm after getting geometry information of the STEP file. And this algorithm can be widely used and make the communication between CAD systems and MCNP models.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6765499,no
Development of a new modeling circuit for the Remote Terminal Unit (RTU) with GSM communication,"This paper introduces the design and development of Intelligent Remote Terminal Unit (RTU) which is to be applied as an automation technique for operating and controlling the low voltage (LV) downstream of 415/240V to enhance reliability of power for the consumers. The design proposed based on Global System for Mobile (GSM) communication and this paper also presents an efficient design for distribution automation system and its implementation in remote/automatic monitoring and controlling of the relays (circuit breaker) by means of GSM Short Message Service (SMS) services, automatic decision making and continuous monitoring of distribution system components in real time [1]. The systems has been equipped with microcontroller as a main component which act as an RTU programmed using Microcontroller PRO compiler software. The RTU provides monitoring fault operation, controlling functions and data collection for analysis. RTU will initiate the transaction with the digital and output modules. The master of this system is RTU and the slaves are digital and output modules. RTU plays an important role in detecting fault and assigned to serve message immediately in the control room. This system involves the detection of fault connected to the microcontroller (PIC18F77A) and GSM modem. When the fault occurs, the sensor will send the signals to the PIC16F77A. The PIC is programmed to process the data and send the signals to the GSM modem. Once received the data, GSM will send the message to the control room operators or other authorized personnel to alert them on the current situation through cellular phone. The results are then communicated between hardware circuit and simulation circuit for the final conclusion with the properly functional algorithm.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6775685,no
Digital image tampering detection and localization using singular value decomposition technique,"Recent years have witnessed an exponential growth in the use of digital images due to development of high quality digital cameras and multimedia technology. Easy availability of image editing software has made digital image processing very popular. Ready to use software are available on internet which can be easily used to manipulate the images. In such an environment, the integrity of the image can not be taken for granted. Malicious tampering has serious implication for legal documents, copyright issues and forensic cases. Researchers have come forward with large number of methods to detect image tampering. The proposed method is based on hash generation technique using singular value decomposition. Design of an efficient hash vector as proposed will help in detection and localization of image tampering. The proposed method shows that it is robust against content preserving manipulation but extremely sensitive to even very minute structural tampering.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6776160,no
Enhancement of camera captured text images with specular reflection,"Specular reflection of light degrades the quality of scene images. Whenever specular reflection affects the text portion of such an image, its readability is reduced significantly. Consequently, it becomes difficult for an OCR software to detect and recognize similar texts. In the present work, we propose a novel but simple technique to enhance the region of the image with specular reflection. The pixels with specular reflection were identified in YUV color plane. In the next step, it enhances the region by interpolating possible pixel values in YUV space. The proposed method has been compared against a few existing general purpose image enhancement techniques which include (i) histogram equalization, (ii) gamma correction and (iii) Laplacian filter based enhancement method. The proposed approach has been tested on some images from ICDAR 2003 Robust Reading Competition image database. We computed a Mean Opinion Score based measure to show that the proposed method outperforms the existing enhancement techniques for enhancement of readability of texts in images affected by specular reflection.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6776189,no
A Density Grid-Based Clustering Algorithm for Uncertain Data Streams,"This paper proposes a grid-based clustering algorithm Clu-US which is competent to find clusters of non-convex shapes on uncertain data stream. Clu-US maps the uncertain data tuples to the grid space which could store and update the summary information of stream. The uncertainty of data is taken into account for calculating the probability center of a grid. Then, the distance between the probability centers of two adjacent grids is adopted for measuring whether they are """"close enough"""" in grids merging process. Furthermore, a dynamic outlier deletion mechanism is developed to improve clustering performance. The experimental results show that Clu-US outperforms other algorithms in terms of clustering quality and speed.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6778662,no
Genetic Algorithms Applied to Discrete Distribution Fitting,"A common problem when dealing with preprocessing of real world data for a large variety of applications, such as classification and outliers detection, consists in fitting a probability distribution to a set of observations. Traditional approaches often require the resolution of complex equations systems or the use of specialized software for numerical resolution. This paper proposes an approach to discrete distribution fitting based on Genetic Algorithms which is easy to use and has a large variety of potential applications. This algorithm is able not only to identify the discrete distribution function type but also to simultaneously find the optimal value of its parameters. The proposed approach has been applied to an industrial problem concerning surface quality monitoring in flat steel products. The results of the tests, which have been developed using real world data coming from three different industries, demonstrate the effectiveness of the method.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6779817,no
Perceptual Evaluation of Voice Quality and Its Correlation with Acoustic Measurement,"The GRBAS scale is a widely used subjective measure of voice quality. The aim of this paper is to investigate the correlation between the 'grade', 'roughness', 'breathiness', 'asthenia' and 'strain' dimensions of this scale and the objective measurements provided by the 'Analysis of Dysphonia in speech and Voice' (ADSV) software package. To do this, voice recordings of 107 samples were collected in a quiet room, and each voice was perceptually evaluated on the GRBAS scale by three experienced speech and language therapists. The same recordings were also acoustically analysed using ADSV. Statistical analysis using the Spearman's rank correlation coefficient model identified a degree of moderate correlation between the result of cepstral based analysis and the GRBAS scale. A classifier such as a decision tree may then be applied to the ADSV cepstral measurement for the objective prediction of GRBAS scores. The accuracy of the classifier in predicting the score of each therapist is given in the paper.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6779860,no
Binomial distribution approach for efficient detection of service level violations in cloud,"Optimum usage of hardware devices, network devices, software resources and consistent quality of service is the aim of cloud computing. Thus, cloud computing is reducing the cost and mounting the revenue for cloud provider. Cloud computing gives the choice and freedom to cloud consumer for the use of dynamic and elastic services as pay-per-use model. In this context, to justify the cost and to verify the quality of services, active Service Level Management is required. Moreover to overcome unrealistic demands from customers and to surpass the occurring violations, a strong and straight forward approach is needed, through which SLA violations can be noticed, detected and distinguished. In this Paper, we have focused on fundamental elements for effective SLA violations detection system. We also propose an approach for binomial distribution for Service level agreement violation detection. This approach gives probability of success and system reliability of cloud service which may be used by service providers to minimize and prevent different violations.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6780089,no
8 channel vibration monitoring and analyzing system using LabVIEW,"The prime objective of this paper is to present prompt 8 channel vibration monitoring and analyzing system using prevailing LabVIEW tools. Vibration measurement is very needful in mechanical and electrical industries to check the machine health & to take predictive maintenance steps before failure or major fault occur. A system based on virtual instrument is introduced that can measure vibration signals. The hardware-developing of the system includes vibration sensors, a signal conditioning circuit cum sensor exciter, a data acquisition device and a PC. Here for research, we are implementing vibration sensor on cantilever beam type arrangement and analyzing vibration signal using signal processing technique in LabVIEW. This methodology relies on the use of advanced methods for machine vibration analysis and health monitoring. Because of some issues encountered regarding traditional methods, for Fourier analysis of non stationary rotating machines the use of more advanced method using powerful software is required. FFT is a very powerful tool in frequency domain for analysis of vibration signal and it provides the information at which frequency the fault occurs? And according to that information we also predict the speed of rotating machine. For FFT analysis; it is necessary to convert vibration data (dynamic data 100mv/g) into EUs (Engineering Units) like acceleration, Velocity and displacement. Here we did the same. The main advantage of vibration monitoring with propose system is that it enables simultaneous monitoring of a number of machines of industries located at different places from a common point. The paper presents a scientific direction and development of vibration signals measurement.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6780181,no
Fault tolerant Lu factorization in transactional memory system,"With the popularization of multi-core processors, transaction memory, as a concurrent control mechanism with easy programing and high scalability, has attracted more and more attention. As a result, the reliability problems of transactional memory become a concerning issue. This paper addresses a transactional implementation of the Lu benchmark of SPLASH-2, and proposes a fault-tolerant Lu algorithm for this transactionalize Lu algorithm. The fault-tolerant Lu uses the data-versioning mechanism of the transactional memory system, detects errors based on transactions and recovers the error by rolling back the error transaction. The experiments show that the fault-tolerant Lu can get a better fault tolerance effect under a smaller cost.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6784788,no
A method of locating fault based on software monitoring,"Software testing can detect most faults, but it can not find the dynamic fault with space and time, especially the dead halt fault. In order to solve this problem, this paper proposes a method of fault location based on software monitoring. First, a method of generating the extended control flow graph is studied, and then a method of generating running record and the process of locating faults is researched. Finally, an example is taken to test validity of this method.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6784878,no
Path coverage assessment for software architecture testing,"Software architecture is receiving increasingly attention as a critical design level for software systems, architecture testing and assessment is key issues to improve and assure software quality. The development of techniques and tools to support architectural understanding, testing, reengineering, maintenance, and reuse will become an important issue. This paper introduces a testing technology to aid architectural testing. Proposes a family of structural criteria for software architecture specification, sketch and proof the subsume relation between coverage criteria by CW system. The common coverage criteria are assessed against the CW system. The assessment provides a feasible comparison of the effectiveness of test criteria on software architecture. The assessment can help to design more testable software architecture.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6784979,no
Software state monitoring model studies based on multivariate HPM,"Hardware Performance Monitor counters (HPM) are an emerging analysis technology in the area of software performance analysis. This paper proposes a method of software state monitoring based on HPM from the perspective of software fault diagnosis. Compared with traditional methods, the method does not depend on test case and expected result, and it can detect abnormal behavior in real-time based on software performance data. By the use of Performance API (PAPI), the method can gather CPU performance data. These data are recorded in HPM and can reflect software state at the running time of software. With Hidden Markov Model (HMM), the method can learn prior probability of software state and conditional probability of performance data readings in each interval. Finally, based on the above parameters, the method classifies the follow-up multivariate observations by Naive Bayesian classifier (NBC) so as to monitor software state in real-time. The experiment shows that based on predefined monitoring event set, our method can effectively identify abnormal behavior which may occur in the running time of software.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6784995,no
Analysis of the reputation system and user contributions on a question answering website: StackOverflow,"Question answering (Q&A) communities have been gaining popularity in the past few years. The success of such sites depends mainly on the contribution of a small number of expert users who provide a significant portion of the helpful answers, and so identifying users that have the potential of becoming strong contributers is an important task for owners of such communities. We present a study of the popular Q&A website StackOverflow (SO), in which users ask and answer questions about software development, algorithms, math and other technical topics. The dataset includes information on 3.5 million questions and 6.9 million answers created by 1.3 million users in the years 2008-2012. Participation in activities on the site (such as asking and answering questions) earns users reputation, which is an indicator of the value of that user to the site. We describe an analysis of the SO reputation system, and the participation patterns of high and low reputation users. The contributions of very high reputation users to the site indicate that they are the primary source of answers, and especially of high quality answers. Interestingly, we find that while the majority of questions on the site are asked by low reputation users, on average a high reputation user asks more questions than a user with low reputation. We consider a number of graph analysis methods for detecting influential and anomalous users in the underlying user interaction network, and find they are effective in detecting extreme behaviors such as those of spam users. Lastly, we show an application of our analysis: by considering user contributions over first months of activity on the site, we predict who will become influential long-term contributors.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6785805,no
An Empirical Study on Wrapper-Based Feature Selection for Software Engineering Data,"Software metrics give valuable information for understanding and predicting the quality of software modules, and thus it is important to select the right software metrics for building software quality classification models. In this paper we focus on wrapper-based feature (metric) selection techniques, which evaluate the merit of feature subsets based on the performance of classification models. We seek to understand the relationship between the internal learner used inside wrappers and the external learner for building the final classification model. We perform experiments using four consecutive releases of a very large telecommunications system, which include 42 software metrics (and with defect data collected for every program module). Our results demonstrate that (1) the best performance is never found when the internal and external learner match, (2)the best performance is usually found by using NB (Naive Bayes) inside the wrapper unless SVM (Support Vector Machine) is external learner, (3) LR (Logistic Regression) is often the best learner to use for building classification models regardless of which learner was used inside the wrapper.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6786086,no
Touch screen based TETRA vehicle radio: Preliminary results of multi-methodology usability testing prototype,"A modern emergency vehicle is the combination of different technologies and single vehicle can contain dozens of user interfaces (UI). Laurea University of Applied Sciences launched on 2010 Mobile Object Bus Interaction-project (MOBI) that defines user requirements for designing emergency vehicles, finding solutions to decrease power consumption, experimenting possibilities to create common ICT-architecture and reduce number of UIs in emergency vehicles. MOBI-project equipped a demo vehicle with the latest technology and a possibility was offered to test new prototype of touch screen based TETRA vehicle radio integrated with the onboard computer. This paper presents a multi-methodology approach for usability testing for touch screen based TETRA vehicle radio where heuristic evaluation was combined with field testing and real users in a moving vehicle. Research result confirms that multi-methodology approach is able to detect key usability problems in early stage of the product development cycle and enables to improve software quality in prototyping phase.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6799938,no
Functional Validation Driven by Automated Tests,"The functional quality of any software system can be evaluated by how well it conforms to its functional requirements. These requirements are often described as use cases and are verified by functional tests that check whether the system under test (SUT) runs as specified. There is a need for software tools to make these tests less laborious and more economical to create, maintain and execute. This paper presents a fully automated process for the generation, execution, and analysis of functional tests based on use cases within software systems. A software tool called Fun Tester has been created to perform this process and detect any discrepancies from the SUT. Also while performing this process it generates conditions to cause failures which can be analyzed and fixed.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6800180,no
A Method for Model Checking Context-Aware Exception Handling,"The context-aware exception handling (CAEH) is an error recovery technique employed to improve the ubiquitous software robustness. In the design of CAEH, context conditions are specified to characterize abnormal situations and used to select the proper handlers. The erroneous specification of such conditions represents a critical design fault that can lead the CAEH mechanism to behave erroneously or improperly at runtime (e.g., abnormal situations may not be recognized and the system's reaction may deviate from what is expected). Thus, in order to improve the CAEH reliability this kind of design faults must be rigorously identified and eliminated from the design in the early stages of development. However, despite the existence of formal approaches to verify context-aware ubiquitous systems, such approaches lack specific support to verify the CAEH behavior. This work proposes a method for model checking CAEH. This method provides a set of modeling abstractions and 3 (three) properties formally defined that can be used to identify exiting design faults in the CAEH design. In order to assess the method feasibility: (i) a support tool was developed, and (ii) fault scenarios that are recurring in the CAEH was injected in a correct model and verified using the proposed approach.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6800182,no
Are Domain-Specific Detection Strategies for Code Anomalies Reusable? An Industry Multi-project Study,"To prevent the quality decay, detection strategies are reused to identify symptoms of maintainability problems in the entire program. A detection strategy is a heuristic composed by the following elements: software metrics, thresholds, and logical operators combining them. The adoption of detection strategies is largely dependent on their reuse across the portfolio of the organizations software projects. If developers need to define or tailor those strategy elements to each project, their use will become time-consuming and neglected. Nevertheless, there is no evidence about efficient reuse of detection strategies across multiple software projects. Therefore, we conduct an industry multi-project study to evaluate the reusability of detection strategies in a critical domain. We assessed the degree of accurate reuse of previously-proposed detection strategies based on the judgment of domain specialists. The study revealed that even though the reuse of strategies in a specific domain should be encouraged, their accuracy is still limited when holistically applied to all the modules of a program. However, the accuracy and reuse were both significantly improved when the metrics, thresholds and logical operators were tailored to each recurring concern of the domain.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6800184,no
An Extended Assessment of Data-Driven Bayesian Networks in Software Effort Prediction,"Software prediction unveils itself as a difficult but important task which can aid the manager on decision making, possibly allowing for time and resources sparing, achieving higher software quality among other benefits. Bayesian Networks are one of the machine learning techniques proposed to perform this task. However, the data pre-processing procedures related to their application remain scarcely investigated in this field. In this context, this study extends a previously published paper, benchmarking data-driven Bayesian Networks against mean and median baseline models and also against ordinary least squares regression with a logarithmic transformation across three public datasets. The results were obtained through a 10-fold cross validation procedure and measured by five accuracy metrics. Some current limitations of Bayesian Networks are highlighted and possible improvements are discussed. Furthermore, we assess the effectiveness of some pre-processing procedures and bring forward some guidelines on the exploration of data prior to Bayesian Networks' model learning. These guidelines can be useful to any Bayesian Networks that use data for model learning. Finally, this study also confirms the potential benefits of feature selection in software effort prediction.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6800192,no
Modeling of transient processes at ground faults in the electrical network with a high content of harmonics,"The paper presents analytical investigations on determination of influence of higher harmonics existing in a ground fault current on the characteristics of transient processes at arcing ground faults. As initial data, results of real oscillography of ground fault currents in the operational 6 kV distribution network are used. It is shown that there are harmonics of high amplitudes in a ground fault current which are 2.6 times greater than a residual 50 Hz reactive current at a single-phase fault location. Calculations using the detailed mathematical model of the 6 kV network confirm the fact that negative influence of arcing overvoltages on equipment insulation increases due to the high content of harmonics in a fault current which are not compensated by Petersen coils and worsen conditions for successful arc quenching. It is noted that a ground fault current may cross zero many times without current extinction due to harmonic distortion. Due to high-frequency overvoltages, probability for restrikes increases. It results in expanding of a fault area in cable insulation and reducing of time for transition of single phase-to-ground fault into stable arcing with possible breakdowns of phase-to-phase insulation. To prove these statements, resistances in a fault place taking into account harmonic distortions of an arc current are calculated.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6804342,no
On the Use of Software Quality Standard ISO/IEC9126 in Mobile Environments,"The capabilities and resources offered by mobile technologies are still far from those provided by fixed environments, and this poses serious challenges, in terms of evaluating the quality of applications operating in mobile environments. This article presents a study to help quality managers apply the ISO 9126 standard on software quality, particularly the External Quality model, to mobile environments. The influence of the limitations of mobile technologies are evaluated for each software quality characteristic, based on the coverage rates of its external metrics, which are themselves influenced by these limitations. The degrees of this influence are discussed and aggregated to provide useful recommendations to quality managers for their evaluation of quality characteristics in mobile environments. These recommendations are intended for mobile software in general and aren't targeted a specific ones. The External Quality model is especially valuable for assessing the Reliability, Usability, and Efficiency characteristics, and illustrates very well the conclusive nature of the recommendations of this study. However, more study is needed on the other quality characteristics, in order to determine the relevance of evaluating them in mobile environments.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6805383,no
Fault-Prone Module Prediction Using a Prediction Model and Manual Inspection,"This paper proposes a fault-prone prediction approach that combines a fault-prone prediction model and manual inspection. Manual inspection is conducted by a predefined checklist that consists of questions and scoring procedures. The questions capture the fault signs or indications that are difficult to be captured by source code metrics used as input by prediction models. Our approach consists of two steps. In the first, the modules are prioritized by a fault-prone prediction model. In the second step, an inspector inspects and scores  percent of the prioritized modules. We conducted a case study of source code modules in commercial software that had been maintained and evolved over ten years and compared AUC (Area Under the Curve) values of Alberg Diagram among three prediction models: (A) support vector machines, (B) lines of code, and (C) random predictor with four prioritization orders. Our results indicated that the maximum AUC values under appropriate  and the coefficient of the inspection score were larger than the AUC values of the prediction models without manual inspection in each of the four combinations and the three models in our context. In two combinations, our approach increased the AUC values to 0.860 from 0.774 and 0.724. Our results also indicated that one of the combinations monotonically increased the AUC values with the numbers of manually inspected modules. This might lead to flexible inspection; the number of manually inspected modules has not been preliminary determined, and the inspectors can inspect as many modules as possible, depending on the available effort.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6805396,no
Evaluating Performance of Network Metrics for Bug Prediction in Software,"Code-based metrics and network analysis based metrics are widely used to predict defects in software. However, their effectiveness in predicting bugs either individually or together is still actively researched. In this paper, we evaluate the performance of these metrics using three different techniques, namely, Logistic regression, Support vector machines and Random forests. We analysed the performance of these techniques under three different scenarios on a large dataset. The results show that code metrics outperform network metrics and also no considerable advantage in using both of them together. Further, an analysis on the influence of individual metrics for prediction of bugs shows that network metrics (except out-degree) are uninfluential.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6805398,no
A Controlled Experiment to Assess the Effectiveness of Eight Use Case Templates,"Use case models, that include use case diagrams along with their documentations, are typically used to specify the functional requirements of the software systems. Use cases are usually semi-structured and documented using some natural language hence issues like ambiguity, inconsistency, and incompleteness are inevitably introduced in the specifications. There have been many efforts to formalize the use case template that make use of certain grammatical construction to guide the structure or style of the description. This paper describes an empirical work to assess the usefulness of eight such use case templates against a set of five judging criteria, namely, completeness, consistency, understandability, redundancy and fault proneness. We conducted a controlled experiment where a group of postgraduate students applied these use case templates on multiple problem specifications. In our results, Yue's template was found to be more consistent and less fault prone, Cockburn's template was found to be more complete and more understandable and, Tiwari's template was found to be less redundant as compared to the other use case templates, though the results were not statistically significant.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6805408,no
Mining Attribute Lifecycle to Predict Faults and Incompleteness in Database Applications,"In a database application, for each attribute, a value is created initially via insertion. Then, the value can be referenced or updated via selection and updating respectively. Eventually, when the record is deleted, the values of the attributes are also deleted. These occurrences of events are associated with the states to constitute the attribute lifecycle. Our empirical studies discover that faults and incompleteness in database applications are highly associated with the attribute lifecycle. Consequently, we propose a novel approach to automatically extract the attribute lifecycle out of a database application from its source code through inter-procedural static program analysis. Data mining methods are applied to predict faults and incompleteness in database applications. Experiments on PHP systems give evidence to support applicability and accuracy of the proposed method.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6805410,no
On Detecting Concurrency Defects Automatically at the Design Level,"We describe an automated approach for detecting concurrency defects from design diagrams of a software, in particular, sequence diagrams. From a given sequence diagram, we automatically infer a formal, parallel specification that generalizes the communication behavior that is designed informally and incompletely in the diagram. We model-check the parallel specification against generic concurrency defect patterns. No additional specification of the software is needed. We present several case-studies to evaluate our approach. The results show that our approach is technically feasible, and effective in detecting nasty concurrency defects at the design level.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6805415,no
Data-Race-Freedom of Concurrent Programs,"Reasoning about access isolation in a program that uses locks, transactions or both to coordinate accesses to shared memory is complex and error-prone. The programmer must understand when accesses issued to the same memory by distinct threads, under possibly different coordination semantics, are isolated, otherwise, data races are introduced. We present a program analysis that guarantees a program is data-race-free irrespective of whether locks, transactions or both are used to coordinate accesses to memory. Our framework entails two main steps: (i) a program is statically executed to determine its memory space and the types of accesses it issues to that memory, then (ii) our isolation algorithm checks that the accesses issued by the program do not result in a data race. To the best of our knowledge our work is the first to guarantee the data-race-freedom of concurrent programs that use locks, transactions or both to coordinate accesses to mutable memory.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6805416,no
Quality-Aware Refactoring for Early Detection and Resolution of Energy Deficiencies,"Software development processes usually target requirements regarding particular qualities in late iteration phases. The developed system is optimised in terms of quality issues, such as, e.g., energy efficiency, without altering the software's behaviour. Bad structures in terms of specific qualities can be considered as bad smells and refactorings can be used to resolve them to preserve its semantics. The problem is that no explicit relationship between smells, qualities and refactorings exists. Without such a relation it is not possible to give evidence about which quality requirements are not satisfied by detected smells. It cannot be specified which smells are resolved by particular refactorings. Thus, developers are not supported in focusing specific qualities and cannot detect and resolve badly structured code in combination. In this paper we present an approach for correlating smells, qualities and refactorings explicitly which supports to focus on specific qualities in early development phases already. We introduce the new term quality smell and come up with a metamodel and architecture enabling developers to establish such relations. A small evaluation regarding energy efficiency in Java code and discussion completes this paper.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6809426,no
A Software Defined Self-Aware Network: The Cognitive Packet Network,"This article is a summary description of the Cognitive Packet Network (CPN) which is an example both of a completely software defined network (SDN) and of a self-aware computer network (SAN) which has been completely implemented and used in numerous experiments. CPN is able to observe its own internal performance as well as the interfaces of the external systems that it interacts with, in order to modify its behaviour so as to adaptively achieve objectives, such as discovering services for its users, improving their Quality of Service (QoS), reduce its own energy consumption, compensate for components which fail or malfunction, detect and react to intrusions, and defend itself against attacks.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6816577,no
REE: Exploiting idempotent property of applications for fault detection and recovery,"As semiconductor technologies scale down to deep sub-micron dimensions, transient faults will soon become a critical reliability concern. This paper presents the Reliability Enhancement Exploiting (REE) technique, a software-implemented fault tolerance solution which employs idempotent property of applications. An idempotent region of code is simply one that can be re-executed multiple times and still produces the same, correct result. By instrumenting extra instructions in an idempotent region to re-execute the region, REE can detect the transient faults occurring during the execution of the idempotent region. Once a fault is detected, REE can recover from the fault by executing the idempotent region again. To the best of our knowledge, this is the first to exploit idempotent property for fault detection. With similar fault coverage to a classic solution, the memory overhead and the performance overhead have been reduced by 71.8% and 31.3%, respectively.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6818241,no
Research on optimization scheme of regression testing,"Regression testing is an important process during software development. In order to reduce costs of regression testing, research on optimization of scheme of regression testing have been done in this paper. For the purpose of reducing the number of test cases and detecting faults of programs early, this paper proposed to combine test case selection with test case prioritization. Regression testing process has been designed and optimization of testing scheme has been implemented. The criterion of test case selection is modify impact of programs, finding programs which are impacted by program modification according to modify information of programs and dependencies between programs. Test cases would be selected during test case selection. The criterion of test case prioritization is coverage ability and troubleshooting capabilities of test case. Test cases which have been selected during test case selection would be ordering in test case prioritization. Finally, the effectiveness of the new method is discussed.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6818242,no
Low power consumption scheduling based on software Fault-tolerance,"The space computer puts forward high demands on the performance. Therefore, the high-performance digital signal processors are increasingly used in the space computer. However, the single particle effects caused by the cosmic radiation make the reliability of the space computer become a huge challenge. The COTS DSP chip has a huge advantage compared to the antiradiation DSP chip in performance, price, size and weight. The software implemented fault-tolerance technique can protect the program, but degrade the system performance and increase the power consumption. According to the DSP structural characteristics and in the premise of not reducing the detecting error ratio, this paper proposes an instruction scheduling method for the low power consumption, to reduce the overheads in terms of the performance and the energy incurred by the fault-tolerance technique.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6818273,no
Predict fault-prone classes using the complexity of UML class diagram,"Complexity is an important attribute to determine the software quality. Software complexity can be measured during the design phase before implementation of system. At the design phase, UML class diagram is the important diagram to show the relationships among the classes of objects in the system. In this paper, we measure the complexity of object-oriented software at design phase to predict the fault-prone classes. The ability to predict the fault-prone classes can provide guidance for software testing and improve the effectiveness of development process. We constructed the Naive Bayesian and k-Nearest Neighbors model to find the relationship between the design complexity and fault-proneness. The proposed models are empirically evaluated using four version of JEdit. The models had been validated using 10-fold cross validation. The performance of prediction models were evaluated by goodness-of-fit criteria and Receiver Operating Characteristic (ROC) analysis. Results obtained from our case study showed the average of models developed by design complexity can predict up to 70% fault-prone classes in object oriented software. It is a better an early indicator of software quality.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6819188,no
Perceived QoS assessment for Voip networks,"As the Internet evolves into a ubiquitous communications, VoIP network becomes more important and popular. It will be expected to meet the quality standards for VoIP networks. The aim of this paper is to undertake a fundamental investigation to quantify the impact of network impairment and speech relates parameters on perceived QoS in VoIP networks. Our contribution is threefold. First, a new VoIP simulation platform is established. The network simulation software is WANem, the voice communication protocol is implemented by OpenPhone. Secondly, we analyze the factors that affect the perceive QoS of VoIP networks. Thirdly, we use the newest NPESQ (New Perceptual Evaluation of Speech Quality) algorithm to assess the perceived QoS value under different IP impairment parameters for VoIP networks.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6820466,no
Improving Reliability of Real-Time Systems through Value and Time Voting,"Critical systems often use N-modular redundancy to tolerate faults in subsystems. Traditional approaches to N-modular redundancy in distributed, loosely-synchronised, real-time systems handle time and value errors separately: a voter detects value errors, while watchdog-based health monitoring detects timing errors. In prior work, we proposed the integrated Voting on Time and Value (VTV) strategy, which allows both timing and value errors to be detected simultaneously. In this paper, we show how VTV can be harnessed as part of an overall fault tolerance strategy and evaluate its performance using a well-known control application, the Inverted Pendulum. Through extensive simulations, we compare the performance of Inverted Pendulum systems which employs VTV and alternative voting strategies to demonstrate that VTV better tolerates well-recognised faults in this realistically complex control problem.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6820837,no
Towards Formal Approaches to System Resilience,"Technology scaling and techniques such as dynamic voltage/frequency scaling are predicted to increase the number of transient faults in future processors. Error detectors implemented in hardware are often energy inefficient, as they are """"always on."""" While software-level error detection can augment hardware-level detectors, creating detectors in software that are highly effective remains a challenge. In this paper, we first present anew LLVM-level fault injector called KULFI that helps simulate faults occurring within CPU state elements in a versatile manner. Second, using KULFI, we study the behavior of a family of well-known and simple algorithms under error injection. (We choose a family of sorting algorithms for this study.) We then propose a promising way to interpret our empirical results using a formal model that builds on the idea of predicate state transition diagrams. After introducing the basic abstraction underlying our predicate transition diagrams, we draw connections to the level of resilience empirically observed during fault injection studies. Building on the observed connections, we develop a simple, and yet effective, predicate-abstraction-based fault detector. While in its initial stages, ours is believed to be the first study that offers a formal way to interpret and compare fault injection results obtained from algorithms from within one family. Given the absolutely unpredictable nature of what a fault can do to a computation in general, our approach may help designers choose amongst a class of algorithms one that behaves most resilient of all.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6820839,no
Probabilistic Modeling of Failure Dependencies Using Markov Logic Networks,"We present a methodology for the probabilistic modeling of failure dependencies in large, complex systems using Markov Logic Networks (MLNs), a state-of-the-art probabilistic relational modeling technique in machine learning. We illustrate this modeling methodology on example system architectures, and show how the the Probabilistic Consistency Engine (PCE) tool can create and analyze failure-dependency models. We compare MLN-based analysis with analytical symbolic analysis to validate our approach. The latter method yields bounds on the expected system behaviors for different component-failure probabilities, but it requires closed-form representations and is therefore often an impractical approach for complex system analysis. The MLN-based method facilitates techniques of early design analysis for reliability (e.g., probabilistic sensitivity analysis). We analyze two examples - a portion of the Time-Triggered Ethernet (TTEthernet) communication platform used in space, and an architecture based on Honeywell's Cabin Air Compressor(CAC) - that highlight the value of the MLN-based approach for analyzing failure dependencies in complex cyber-physical systems.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6820861,no
Exploring Time and Frequency Domains for Accurate and Automated Anomaly Detection in Cloud Computing Systems,"Cloud computing has become increasingly popular by obviating the need for users to own and maintain complex computing infrastructures. However, due to their inherent complexity and large scale, production cloud computing systems are prone to various runtime problems caused by hardware and software faults and environmental factors. Autonomic anomaly detection is crucial for understanding emergent, cloud-wide phenomena and self-managing cloud resources for system-level dependability assurance. To detect anomalous cloud behaviors, we need to monitor the cloud execution and collect runtime cloud performance data. For different types of failures, the data display different correlations with the performance metrics. In this paper, we present a wavelet-based multi-scale anomaly identification mechanism, that can analyze profiled cloud performance metrics in both time and frequency domains and identify anomalous cloud behaviors. Learning technologies are exploited to adapt the selection of mother wavelets and a sliding detection window is employed to handle cloud dynamicity and improve anomaly detection accuracy. We have implemented a prototype of the anomaly identification system and conducted experiments on an on-campus cloud computing environment. Experimental results show the proposed mechanism can achieve 93.3% detection sensitivity while keeping the false positive rate as low as 6.1% while outperforming other tested anomaly detection schemes.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6820866,no
Applying Reduced Precision Arithmetic to Detect Errors in Floating Point Multiplication,"Prior work developed an efficient technique, called reduced precision checking, for detecting errors in floating point addition. In this work, we extend reduced precision checking (RPC) to multiplication. Our results show that RPC can successfully detect errors in floating point multiplication at relatively low cost.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6820870,no
Generalized Cox Proportional Hazards Regression-Based Software Reliability Modeling with Metrics Data,"Multifactor software reliability modeling with software test metrics data is well known to be useful for predicting the software reliability with higher accuracy, because it utilizes not only software fault count data but also software testing metrics data observed in the development process. In this paper we generalize the existing Cox proportional hazards regression-based software reliability model by introducing more generalized hazards representation, and improve the goodness-of-fit and predictive performances. In numerical examples with real software development project data, we show that our generalized model can significantly outperform several logistic regression-based models as well as the existing Cox proportional hazards regression-based model.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6820881,no
Should We Beware the Exceptions? An Empirical Study on the Eclipse Project,"Exception handling is a mechanism that highlights exceptional functionality of software systems. Currently there are empirical studies pointing out that design entities (classes) that use exceptions are more defect prone than the other classes and sometimes developers neglect exceptional functionality, minimizing its importance. In this paper we investigate if classes that use exceptions are the most complex classes from software systems and, consequently, have an increased likelihood to exhibit defects. We also detect two types of improper usages of exceptions in three releases of Eclipse and investigate the relations between classes that handle/do not handle properly exceptions and the defects those classes exhibit. The results show that (i) classes that use exceptions are more complex than the other classes and (ii) classes that handle improperly the exceptions in the source code exhibit an increased likelihood of exhibiting defects than classes which handle them properly. Based on the provided evidence, practitioners get knowledge about the correlations between exceptions and complexity and are advised once again about the negative impact deviations from best programming practices have at a source code level.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6821157,no
Network coding to enhance standard routing protocols in wireless mesh networks,"This paper introduces a design and simulation of a locally optimized network coding protocol, called PlayNCool, for wireless mesh networks. PlayN-Cool is easy to implement and compatible with existing routing protocols and devices. This allows the system to gain from network coding capabilities implemented in software without the need for new hardware. PlayNCool enhances performance by (i) choosing a local helper between nodes in the path to strengthen the quality of each link, (ii) using local information to decide when and how many transmissions to allow from the helper, and (iii) using random linear network coding to increase the usefulness of each transmission from the helpers. This paper focuses on the design details needed to make the system operate in reality and evaluating performance using ns-3 in multi-hop topologies. Our results show that the PlayNCool protocol increases the end-to-end throughput by more than two-fold and up to four-fold in our settings.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6825055,no
Design and implementation QoS system based on OpenFlow,"In this paper, we design an architecture to implement a QoS system based on OpenFlow. The system can filter malicious packets that have no permission to gain quality of service, and can guarantee scheduling the fastest path for QoS packets in working network. By predicting and estimating current network flows, we implement to assign optimal path for QoS flow without affecting the rest of the normal packet transmission. We can also implement to transferring the packet even in congested networks by simply flushing all flow-tables in the switches and rescheduling the path for different packets. All the provisioning of our system can be flexible and real-time updated. We implement the system based on POX controller, verify and evaluate our design goals.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6825283,no
OB-STM: An Optimistic Approach for Byzantine Fault Tolerance in Software Transactional Memory,"Recently, researchers have shown an increased interest in concurrency control using distributed Software Transactional Memory (STM). However, there has been little discussion about certain types of fault tolerance, such as Byzantine Fault Tolerance (BFT), for kind of systems. The focus of this paper is on tolerating byzantine faults on optimistic processing of transactions using STM. The result is an algorithm named OB-STM. The processing of a transaction runs with an optimistic approach, benefiting from the high probability of messages being delivered in order when using Reliable Multicast on a local network (LAN). The protocol has a better performs when messages are delivered ordered. In case of a malicious replica or out-of-order messages, the Byzantine protocol is initiated. In smaller scenarios and using an optimistic approach, the protocol has a better throughput than Tazio.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6825335,no
Automatic test platform for photovoltaic grid-connected inverters,"Photovoltaic (PV) solar inverter is equipment that converts the DC output of solar batteries to the AC power which meets the requirements of the gird, its performance and quality are directly related to the photovoltaic effect on the public grid. Current national standard specifies only the requirements for protection and did not develop appropriate testing rules and procedures. This paper researched and developed the PV grid-connected inverter detects platform, analyzed the PV grid-connected inverter protective function and testing methods and procedures. We realized the PC integration of the system and the automatic test of the inverter by using Kingview software, to ensure the reliability and accuracy of test results, in addition, the host computer system has proved ease of use, stability and scalability.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6828203,no
An incentive scheme based on heterogeneous belief values for crowd sensing in mobile social networks,"Crowd sensing is a new paradigm which exploits pervasive mobile devices to provide complex sensing services in mobile social networks (MSNs). To achieve good service quality for crowd sensing applications, incentive mechanisms are indispensable to attract more participants. Most of existing research apply only for the offline sensing data collections, where all participants' information are known a priori. In contrast, we focus on a more real scenario requiring a continuous crowd sensing. We model the problem as a restless multi-armed bandit process rather than a regular auction, where users submit their bids to the server over time, and the server choose a subset of users to collect sensing data. In this paper, to maximize the social welfare for the infinite horizonal continuous sensing, we design an incentive scheme based on heterogeneous belief values for joint social states and realtime throughput. Analysis results indicate that our algorithm is not only near optimal and stable, but truthful, individually rational, profitable, and computationally efficient.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6831321,no
Design and Optimization of a Big Data Computing Framework Based on CPU/GPU Cluster,"Big data processing is receiving significant amount of interest as an important technology to reveal the information behind the data, such as trends, characteristics, etc. MapReduce is one of the most popular distributed parallel data processing framework. However, some high-end applications, especially some scientific analyses have both data-intensive and computation intensive features. Therefore, we have designed and implemented a high performance big data process framework called Lit, which leverages the power of Hadoop and GPUs. In this paper, we presented the basic design and architecture of Lit. More importantly, we spent a lot of effort on optimizing the communications between CPU and GPU. Lit integrated GPU with Hadoop to improve the computational power of each node in the cluster. To simplify the parallel programming, Lit provided an annotation based approach to automatically generate CUDA codes from Hadoop codes. Lit hid the complexity of programming on CPU/GPU cluster by providing extended compiler and optimizer. To utilize the simplified programming, scalability and fault tolerance benefits of Hadoop and combine them with the high performance computation power of GPU, Lit extended the Hadoop by applying a GPUClassloader to detect the GPU, generate and compile CUDA codes, and invoke the shared library. For all CPU-GPU co-processing systems, the communication with the GPU is the well-known performance bottleneck. We introduced data flow optimization approach to reduce unnecessary memory copies. Our experimental results show that Lit can achieve an average speedup of 1 to 3 on three typical applications over Hadoop, and the data flow optimization approach for the Lit can achieve about 16% performance gain.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6832029,no
Towards Energy-Aware Resource Scheduling to Maximize Reliability in Cloud Computing Systems,"Cloud computing has become increasingly popular due to deployment of cloud solutions that will enable enterprises to cost reduction and more operational flexibility. Reliability is a key metric for assessing performance in such systems. Fault tolerance methods are extensively used to enhance reliability in Cloud Computing Systems (CCS). However, these methods impose extra hardware and/or software cost. Proper resource allocation is an alternative approach which can significantly improve system reliability without any extra overhead. On the other hand, contemplating reliability irrespective of energy consumption and Quality of Service (QoS) requirements is not desirable in CCSs. In this paper, an analytical model to analyze system reliability besides energy consumption and QoS requirements is introduced. Based on the proposed model, a new online resource allocation algorithm to find the right compromise between system reliability and energy consumption while satisfying QoS requirements is suggested. The algorithm is a new swarm intelligence technique based on imperialist competition which elaborately combines the strengths of some well-known meta-heuristic algorithms with an effective fast local search. A wide range of simulation results, based on real data, clearly demonstrate high efficiency of the proposed algorithm.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6832090,no
Defects prediction of early phases of Software Development Life Cycle using fuzzy logic,"In this paper, a model is proposed to predict the software Defects indicator of early phases of Software Development Life Cycle (SDLC) using the top most reliability relevant metrics at each artifacts. Failure data is not available in the early phases of SDLC. Therefore qualitative values of software metrics are used in this model. Defect indicator predicated in the requirement analysis, Design and Coding phases are very helpful for testing and maintenance of the software. Requirement analysis phase Defect Indicator value is relatively greater than that of the design and coding artifacts. Model is validated with the existing literature. Validation result are satisfactory.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6832299,no
Software quality modeling using metrics of early artifacts,"Software industries require reliability prediction for quality evaluation and resource planning. In early phase of software development, failure data is not accessible to conclude the reliability of software. However, early software fault prediction procedure provides a flexibility to predict the faults in early stage. In this paper, a software faults prediction model is proposed using BBN that focus on the structure of the software development process explicitly representing complex relationship of five influencing parameters (Techno-complexity, Practitioner Level, Creation Effort, Review Effort, and Urgency). In order to assess the constructed model, an empirical experiment has been performed, based on the data collected from software development projects used by an organization. The predicted fault ware found very near to the actual fault detected during testing.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6832300,no
Aspect Oriented Software metrics based maintainability assessment: Framework and model,"This paper emphasize on a new framework to access the Aspect Oriented Software's (AOS) using software metrics. Software metrics for the qualitative and quantitative assessment is the combination of static and dynamic metrics for software's. It is found from the literature survey that till date most the framework only considered the static metrics based assessment for aspect oriented software's. In our work we have mainly considered the set of static metrics along with dynamic software metrics specific to AspectJ. This framework may provide a new research direction while predicting the software attributes because earlier dynamic metrics were neglected while evaluating the quality attributes like maintainability, reliability, understandability for AO software's. Based on basic fundamentals of software engineering dynamic metrics are equally important as well as static metrics for software analysis. A similar concept is borrowed to apply on aspect oriented software development by adding dynamic software metrics. Presently we have only proposed a framework and model using the static and dynamic metrics for the assessment of aspect oriented system but still the proposed approach need to be validated.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6832305,no
Software defect prediction using supervised learning algorithm and unsupervised learning algorithm,"Software defect prediction has recently attracted attention of many software quality researchers. One of the major areas in current project management software is to effectively utilize resources to make meaningful impact on time and cost. A pragmatic assessment of metrics is essential in order to comprehend the quality of software and to ensure corrective measures. Software defect prediction methods are majorly used to study the impact areas in software using different techniques which comprises of neural network (NN) techniques, clustering techniques, statistical method and machine learning methods. These techniques of Data mining are applied in building software defect prediction models which improve the software quality. The aim of this paper is to propose various classification and clustering methods with an objective to predict software defect. To predict software defect we analyzed classification and clustering techniques. The performance of three data mining classifier algorithms named J48, Random Forest, and Naive Bayesian Classifier (NBC) are evaluated based on various criteria like ROC, Precision, MAE, RAE etc. Clustering technique is then applied on the data set using k-means, Hierarchical Clustering and Make Density Based Clustering algorithm. Evaluation of results for clustering is based on criteria like Time Taken, Cluster Instance, Number of Iterations, Incorrectly Clustered Instance and Log Likelihood etc. A thorough exploration of ten real time defect datasets of NASA[1] software project, followed by various applications on them finally results in defect prediction.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6832328,no
Table of contents,The following topics are dealt with: MIMO systems; MAP probability decoders; OFDM channel estimation; quantum key distribution; noiseless linear amplifier; transform domain de-noising; digital image retrieval; parallel executing command scheduler; NAND flash storage system; transmitted-reference ultra-wideband cooperative communication system; multisensor data fusion; wireless sensor network; target detection; user-priority based virtual network embedding model; homomorphic encryption; image segmentation; memristors; LLOP localization algorithm; NLOS wireless propagation; routing algorithm; schismatic communication network; UAV; LDPC layered decoder; priority scheme; spectrum partition; multiuser opportunistic spectrum access; cognitive radio networks; shadow detection method; improved Gaussian mixture model; multithreaded coprocessor IP core; embedded SoC chip; WCDMA network quality; arterial highways; autocorrelation OFDM chirp waveform; file system; self-computing; high speed QSPI memory controller; shot boundary detection; video retrieval; reversible quantum n-to-2n decoder; quadratic interleaved codes; wireless Doppler environments; cross-site scripting attack; encoding; smart home system; remote heart sound monitoring system; LZSS lossless compression algorithm; Virtex-7 FPGA-based high-speed signal processing hardware platform design; mine detecting robot; wireless communication; irregular mesh NoC; JAVA blueprint procedure pattern; condition monitoring system; hydroelectric generating unit; HPP; information-focused model SoS framework; software-based GPS receiver; chosen plaintext attacking; hyper-chaos image encryption algorithm; mobile ECG monitoring system; CMMB; precession parameters extraction; midcourse target; HRRP sequence; refactoring techniques usage; code changing; greedy algorithms; ballistic target recognition; micromotion characteristics; sequential HRRP; high speed motion compensation; fractional Fourier transform; active distribution network- smart grid; PTS scheme; IFFT; PAPR reduction; SISO/MIMO OFDM; fuzzy PID control; passive lower extremity exoskeleton; adaptive self-tuning PID control; submarine periscope; high resolution SAR imagery; PMSM control system; target jamming; wideband linear frequency modulated signal; adaptive facet subdivision scheme; shadowing culling; multi-objective optimization problem; unmanned aerial vehicle image denoising; RBF neural network; energy efficient cooperative multicast transmission scheme; clustered WSN; circular arc detection algorithm; Freeman chain code; 3D model building; fault diagnosis; power transformer; improved differential evolution-neural network; subway station; combined social force model; infrared thermal imaging diagnosis technology; power equipment; electronic thermometer; Bluetooth low energy; deformation measurement; formation satellites; collision avoidance constraints; LQG controller; antenna control system; block-PEG construction method; TPM signal digital notch filters; statistical model; text segmentation; BFD-triggered OAM mechanisms; IP RAN network; collaborative filtering; social networks; network coding; multicast routing; vehicle management system; ubiquitous network; mobile application development; Adobe AIR; in vehicle network; backstepping active disturbance rejection controller; helicopter; SOC algorithm; lithium-ion batteries; electric vehicles; hardware Trojan detection; malicious circuit properties; fuzzy-sliding mode control algorithm; servo system; antenna sub-reflector; vehicle context-aware system; optimal power allocation scheme; opportunistic cooperative multicast transmission; cognitive OFDMA networks; text mining model; and density clustering algorithm.,2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6835522,no
Field studies for transient stability in continuous operation and contingency condition during 3   short circuit at PCC for grid connected 10MW Kastina wind farm,"As a result of world's energy resources crisis and environment pollution increasingly aggravating, distributed generation (DG) based on renewable energy has become a developmental trend for electric power industry in 21 century. However, DG's are usually affected by natural conditions being not able to output power continuously and steadily, so when large scale wind turbine generators are incorporated into the grid there is likely attendance impact it would bring on electric power system stability. In this investigation, the wind farm connection was done at 33/1 1kV level at Kastina substation located at Northern part of Nigeria. This paper is to establish voltage instability and frequency fluctuation that might be associated with the connection of the wind farm to the existing grid. The DigSILENT Power Factory software was used for modeling and simulation analysis of the studies and the results shows that when wind farm is connected to the grid, large clearance time is required for the fault to be cleared for voltage and frequency to become normalize again as compare with normal grid system. The studies has verified transient stability associated problem due to voltage and frequency stability variation phenomenon during the disconection of both windfarm and the load from the existing grid and is easier to predict in future the setting of protective devices and clearance time.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6837109,no
A Novel Fault Localization Method with Fault Propagation Context Analysis,"A variety of Graph-based fault localization methods are always applied to abstract the relationships between program entities, and thereby facilitate program debugging and understanding. They assess the fault suspiciousness of individual program entities and rank the statements in descending order according to their suspiciousness scores to help identify faults in programs. However, many of these methods focus on assessing the suspiciousness of individual program entities while ignore the propagation of infected program states among them. They could not locate the true fault. In this paper, we consider the fault in a statement may be propagated to its subsequent statements via its data flow edges. We propose a novel CDN-based fault localization method. It includes two steps. First is fault-related statements localization and second is fault comprehension. It calculates the combined dependence probability of each statement to find the fault-related statements and then analyzes the propagation contexts of the statements to locate the true fault. The experimental results show that our approach is superior to other fault localization methods on both localization effectiveness and stability.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6840655,no
Estimating the regression test case selection probability using fuzzy rules,"Software maintenance is performed regularly for enhancing and adapting the functionalities of the existing software, which modifies the software and breaks the previously verified functionalities. This sets a requirement for software regression testing, making it a necessary maintenance activity. As the evolution of software takes place the size of the test suite tends to grow, which makes it difficult to execute the entire test suite in a time constrained environment. There are many existing techniques for regression test case selection. Some are based on dataflow analysis technique, slicing-based technique, bio-inspired techniques, and genetic algorithm based techniques. This paper gives a regression test case selection technique based on fuzzy model, which reduces the size of the test suite by selecting test cases from existing test suite. The test cases, which are necessary for validating the recent changes in the software and have the ability to find the faults and cover maximum coding under testing in minimum time, are selected. A fuzzy model is designed which takes three parameters namely code covered, execution time and faults covered as input and produces the estimation for the test case selection probability as very low, low, medium, high and very high.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6844270,no
Software components prioritization using OCL formal specification for effective testing,"In soft real time system development, testing effort minimization is a challenging task. Earlier research has shown that often a small percentage of components are responsible for most of the faults reported at the later stages of software development. Due to the time and other resource constraints, fault-prone components are ignored during testing activity which leads to compromises on software quality. Thus there is a need to identify fault-prone components of the system based on the data collected at the early stages of software development. The major focus of the proposed methodology is to identify and prioritize fault-prone components of the system using its OCL formal specifications. This approach enables testers to distribute more effort on fault-prone components than non fault-prone components of the system. The proposed methodology is illustrated based on three case study applications.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6844288,no
Design of 1553B Bus Testing and Simulating System,"With the appearance of different manufacturers and types of 1553B bus devices, the problem of the lack of real-time analysis and evaluation of effective tools for bus test method becomes severe. So a set of general configurable and flexible testing and simulating verification system for 1553B bus. In this thesis, a set of real-time hardware and software systems solution is proposed, a novel mechanism of inject fault testing and detect fault is adopted. Besides, the proposed system could receive good performance in practical applications.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6846601,no
Design and Implementation of Waterworks Operation Data Real-Time Detection and Processing System,"As people's water consumption become large and large in china, people begin to expect high requirements of water quality. However, some waterworks' monitoring system cannot automatically detect and process water quality parameters and system operation status. Therefore, staffs have to manual meter reading. The paper first discusses the design of the whole system, including the server side and the client side. It then goes on to discuss, in the server side, how to realize the communication between server and OPC client by the OPC communication technology. Finally, the paper focuses on the client side that the user interface is designed to export of automatic report and to draw the graph. Running results show that the software can detect and process parameters in real-time, and the interface is friendly, with good reliability, and can meet the demands of customers.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6846662,no
Improvement in productionrate for 3 phase energy meter terminal block by choosing an optimum gate location and reducing the defects of the tool,"Due to heavy demand in plastic products, plastic industries are growing in a fastest rate. Plastic injection moulding begins with mould making and in manufacturing of critical shapes. The optimum gate location is one of the most important criterions in mould design. Mould Flow analysis is a powerful simulation tool to optimize the best gate location and to predict the production time required at the lowest possible cost. Verification using simulation requires much less time to achieve a quality result, and with no material costs, as compared with the conventional trial-and-error methods on the production floor. In this paper, an attempt has been made in analysis by taking four gate locations for a rectangular - shaped plastic component. Mould Flow Plastic simulation software is used for the analysis and the optimum gate location is found with least defects. The placement of a gate in an injection mould is one of the most important variables of the total mould design. The tool is of single cavity mould. The component has got 12 circular holes, rectangular slots, flat openings which requires two side cores and two core inserts. This asks for a finger cam in tool construction. Analysis for filling, flow, best gate location and cooling is carried out using mould flow software.The quality of the moulded part is greatly affected by the gate location, because it influences the manner in which the plastic flows into the mould cavity. The mould flow analysis helps in reducing costs and time and also prevents other defects occurring in the process.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6851568,no
Vibration measurement of dental CBCT based on 3D accelerometer,"Currently, dental CBCT(Cone-Beam Computed Tomography) is wildly used in the tooth implantation, maxillofacial surgery and the treatment of temporomandibular joint disorders. The three-dimensional images of Dental CBCT are reconstructed based on the projection images, which are obtained at each degree of 360. The projected image should have a precise correspondence with the angle of rotation of arm. If vibration occurs during the rotation, artifacts will be caused from reconstruction, thus affecting the image quality. It has a very important significance on the image reconstruction if vibration information of CBCT arm can be detected and evaluated. This paper presented a vibration measuring device based 3D accelerometer, which can measure and evaluate vibration of CBCT during operation. With this device, vibration information can be sensed by 3D accelerometer. The analog signals are sampled by Microprocessor with internal ADC converter. Then, all data are sent to PC via wireless communication way. Software written by MATLAB will acquire, display, save and analyze data from measurement device, which makes it possible that evaluates vibration information from CBCT.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6864512,no
A cool way of improving the reliability of HPC machines,"Soaring energy consumption, accompanied by declining reliability, together loom as the biggest hurdles for the next generation of supercomputers. Recent reports have expressed concern that reliability at exascale level could degrade to the point where failures become a norm rather than an exception. HPC researchers are focusing on improving existing fault tolerance protocols to address these concerns. Research on improving hardware reliability, i.e., machine component reliability, has also been making progress independently. In this paper, we try to bridge this gap and explore the potential of combining both software and hardware aspects towards improving reliability of HPC machines. Fault rates are known to double for every 10C rise in core temperature. We leverage this notion to experimentally demonstrate the potential of restraining core temperatures and load balancing to achieve two-fold benefits: improving reliability of parallel machines and reducing total execution time required by applications. Our experimental results show that we can improve the reliability of a machine by a factor of 2.3 and reduce the execution time by 12%. In addition, our scheme can also reduce machine energy consumption by as much as 25%. For a 350K socket machine, regular checkpoint/restart fails to make progress (less than 1% efficiency), whereas our validated model predicts an efficiency of 20% by improving the machine reliability by a factor of up to 2.29.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877491,no
Optimization of cloud task processing with checkpoint-restart mechanism,"In this paper, we aim at optimizing fault-tolerance techniques based on a checkpointing/restart mechanism, in the context of cloud computing. Our contribution is three-fold. (1) We derive a fresh formula to compute the optimal number of checkpoints for cloud jobs with varied distributions of failure events. Our analysis is not only generic with no assumption on failure probability distribution, but also attractively simple to apply in practice. (2) We design an adaptive algorithm to optimize the impact of checkpointing regarding various costs like checkpointing/restart overhead. (3) We evaluate our optimized solution in a real cluster environment with hundreds of virtual machines and Berkeley Lab Checkpoint/Restart tool. Task failure events are emulated via a production trace produced on a large-scale Google data center. Experiments confirm that our solution is fairly suitable for Google systems. Our optimized formula outperforms Young's formula by 3-10 percent, reducing wall-clock lengths by 50-100 seconds per job on average.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877497,no
A clustering method for pruning false positive of clonde code detection,"There are some false positives when detect syntax similar cloned code with clone code technology based on token. In this paper, we propose a novel algorithm to automatically prune false positives of clone code detection by performing clustering with different attribute and weights. First, closely related statements are grouped into a cluster by performing clustering. Second, compare the hash values of the statements in the two clusters to prune false positives. The experimental results show that our method can effectively prune clone code false positives caused by switching the orders of same structure statements. It not only improves the accuracy of cloned code detection and cloned code related defects detection but also contribute to the following study of cloned code refactorings.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6885366,no
A high reliability on-board parallel system based on multiple DSPs,"Data volumes produced by various space missions have increased significantly, creating an urgent need for on-board systems with higher performance and reliability. A 2 parallel + 1 standby system based on multiple DSPs is proposed in this paper. This system consists of three DSPs and four FPGAs. Two DSPs work in parallel and the rest one is standby. The standby DSP replaces the fault DSP when an unrecoverable error is detected by radiation hardening modules. We applied a 2048-point FFT on this system to evaluate the performance. Then we injected some errors into the system and repeated the FFT application. The experimental results show that the system reaches a speedup of 1.63 and could recover from errors in a short time.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6885376,no
A metamodel for tracing requirements of real-time systems,"Modeling and tracing requirements are difficult, error-prone activities which have great impact on the overall software development process. Most techniques for modeling requirements present a number of problems and limitations, including modeling requirements at a single level of abstraction, and being specific to model functional requirements. In addition, non-functional requirements are frequently overlooked. Without the proper modeling of requirements, the activity of tracing requirements is impaired. This article aims to perform a study on modeling requirements of Real-Time Systems through an extension of the SysML Requirements Diagram focusing on the traceability of non-functional and functional requirements. The SysML metamodel is extended with new stereotypes and relationships, and the proposed metamodel is applied to a set of requirements for the specification of a Road Traffic Control System. The proposed approach has demonstrated to be effective for representing software requirements of real-time systems at multiple levels of abstraction and classification. The proposed metamodel represents concisely the traceability of requirements at a high level of abstraction.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6913189,no
F6COM: A component model for resource-constrained and dynamic space-based computing environments,"Component-based programming models are well-suited to the design of large-scale, distributed applications because of the ease with which distributed functionality can be developed, deployed, and validated using the models' compositional properties. Existing component models supported by standardized technologies, such as the OMG's CORBA Component Model (CCM), however, incur a number of limitations in the context of cyber physical systems (CPS) that operate in highly dynamic, resource-constrained, and uncertain environments, such as space environments, yet require multiple quality of service (QoS) assurances, such as timeliness, reliability, and security. To overcome these limitations, this paper presents the design of a novel component model called F6COM that is developed for applications operating in the context of a cluster of fractionated spacecraft. Although F6COM leverages the compositional capabilities and port abstractions of existing component models, it provides several new features. Specifically, F6COM abstracts the component operations as tasks, which are scheduled sequentially based on a specified scheduling policy. The infrastructure ensures that at any time at most one task of a component can be active - eliminating race conditions and deadlocks without requiring complicated and error-prone synchronization logic to be written by the component developer. These tasks can be initiated due to (a) interactions with other components, (b) expiration of timers, both sporadic and periodic, and (c) interactions with input/output devices. Interactions with other components are facilitated by ports. To ensure secure information flows, every port of an F6COM component is associated with a security label such that all interactions are executed within a security context. Thus, all component interactions can be subjected to Mandatory Access Control checks by a Trusted Computing Base that facilitates the interactions. Finally, F6COM provides capabilities to monitor - ask execution deadlines and to configure component-specific fault mitigation actions.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6913199,no
An evaluation framework for assessing the dependability of Dynamic Binding in Service-Oriented Computing,"Service-Oriented Computing (SOC) provides a flexible framework in which applications may be built up from services, often distributed across a network. One of the promises of SOC is that of Dynamic Binding where abstract consumer requests are bound to concrete service instances at runtime, thereby offering a high level of flexibility and adaptability. Existing research has so far focused mostly on the design and implementation of dynamic binding operations and there is little research into a comprehensive evaluation of dynamic binding systems, especially in terms of system failure and dependability. In this paper, we present a novel, extensible evaluation framework that allows for the testing and assessment of a Dynamic Binding System (DBS). Based on a fault model specially built for DBS's, we are able to insert selectively the types of fault that would affect a DBS and observe its behavior. By treating the DBS as a black box and distributing the components of the evaluation framework we are not restricted to the implementing technologies of the DBS, nor do we need to be co-located in the same environment as the DBS under test. We present the results of a series of experiments, with a focus on the interactions between a real-life DBS and the services it employs. The results on the NECTISE Software Demonstrator (NSD) system show that our proposed method and testing framework is able to trigger abnormal behavior of the NSD due to interaction faults and generate important information for improving both dependability and performance of the system under test.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6913206,no
Acoustic characteristics concerning construction and drive of axial-flux motors for electric bicycles,"Ride quality, including perceptible noise and tactile vibration, is one of key considerations for electric bicycles. Featured with high torque density and slim shape, axial-flux permanent magnet (AFPM) motors fulfill most of the integration requirements for electric bicycles. Such a pancake shape construction, however, is prone to structural vibration since large axial force exerts on the stator by the rotor magnets. In this study, two conventional bicycles were modified to equip with either inrunner or outrunner AFPM motors, and induced noise concerns during riding. Measured data of phase currents, vibration and noise were analyzed by time signature, spectrum or cepstrum for both motors. Additional modal testing was performed for the outrunner motor as structural resonances occurred. Through investigations both on the motor structure and the motor drive, the major vibration and noise peaks were correlated to their excitation sources. In this study, the torque ripple induced by current control scheme was the root cause of the inrunner motor noise. The outrunner motor noise was mainly caused by the stator slotting effect and the coincidence with structural resonance. Moreover, the perceptibility of switching noise was highly linked to pulse-width-modulation switching frequency. After the comprehensive cause-effect analysis and effective remedies to refine the drive scheme or the controller's software, we obtained a satisfactory impression of motor noise with remarkable noise reductions, 16 dB and 6 dB for the inrunner motor and outrunner motor, respectively. As a result, the operating noise at rider's ear location was below 60 dB and fulfilled the expectations of most cyclists.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6914782,no
A comprehensive compiler-assisted thread abstraction for resource-constrained systems,"While size and complexity of sensor networks software has increased significantly in recent years, the hardware capabilities of sensor nodes have been remaining very constrained. The predominant event-based programming paradigm addresses these hardware constraints, but does not scale well with the growing software complexity, often leading to software that is hard-to-manage and error-prone. Thread abstractions could remedy this situation, but existing solutions in sensor networks either provide incomplete thread semantics or introduce a significant resource overhead. This reflects the common understanding that one has to trade expressiveness for efficiency and vice versa. Our work, however, shows that this trade-off is not inherent to resource-constrained systems. We propose a comprehensive compiler-assisted cooperative threading abstraction, where full-fledged thread-based C code is translated to efficient event-based C code that runs atop an event-based operating system such as Contiki or TinyOS. Our evaluation shows that our approach outperforms thread libraries and generates code that is almost as efficient as hand-written event-based code with overheads of 1 % RAM, 2 % CPU, and 3 % ROM.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6917583,no
Evolutionary Search Algorithms for Test Case Prioritization,"To improve the effectiveness of certain performance goals, test case prioritization techniques are used. These technique schedule the test cases in particular order for execution so as to increase the efficacy in meeting the performance goals. For every change in the program it is considered inefficient to re-execute each and every test case. Test case prioritization techniques arrange the test cases within a test suite in such a way that the most important test case is executed first. This process enhances the effectiveness of testing. This algorithm during time constraint execution has been shown to have detected maximum number fault while including the sever test cases.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6918806,no
Measuring the Gain of Automatic Debug,"The purpose of regression testing is to quickly catch any deterioration in quality of a product under development. The more frequently tests are run, the earlier new issues can be detected resulting in a larger burden for the engineers who need to manually debug all test failures, many of which are failing due to the same underlying bug. However, there are software tools that automatically debug the test failures back to the faulty change and notifies the engineer who made this change. By analyzing data from a real commercial ASIC project we aimed to measure whether bugs are fixed faster when using automatic debug tools compared to manual debugging. All bugs in an ASIC development project were analyzed over a period of 3 months in order to determine the time it took the bug to be fixed and to compare the results from both automatic and manual debug. By measuring the time from when the bug report was sent out by the automatic debug tool until the bug was fixed, we can show that bugs are fixed 4 times faster with automatic debug enabled. Bug fixing time was on average 5.7 hours with automatic debug and 23.0 hours for manual debug. The result was achieved by comparing bugs that were automatically debugged to those issues that could not be debugged by the tool, because those issues were outside the defined scope of the device under test. Such issues are still reported by the automatic debug tool but marked as requiring manual debug and is consequently a good point of comparison. A 4 times quicker bug fixing process is significant and can ultimately contribute to a shortening of a development project as the bug turnaround time is one of the key aspects defining the length of a project, especially in the later phase just before release.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6926095,no
Efficient mitigation of data and control flow errors in microprocessors,"The use of microprocessor-based systems is gaining importance in application domains where safety is a must. For this reason, there is a growing concern about the mitigation of SEU and SET effects. This paper presents a new hybrid technique aimed to protect both the data and the control-flow of embedded applications running on microprocessors. On one hand, the approach is based on software redundancy techniques for correcting errors produced in the data. On the other hand, control-flow errors can be detected by reusing the on-chip debug interface, existing in most modern microprocessors. Experimental results show an important increase in the system reliability even superior to two orders of magnitude, in terms of mitigation of both SEUs and SETs. Furthermore, the overheads incurred by our technique can be perfectly assumable in low-cost systems.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6937381,no
Reliability of different fault detection algorithms under high impedance faults,This paper proposes a comparative study on high impedance faults (HIF) using different fault detection techniques in distance relaying. The original signal under fault consist two parts namely normal and disturbance part. The fault detection is easily achieved as the disturbance part of the signal produces an irregular shape compared to the shape produced from the normal part of the signal. By selecting suitable threshold value the starting point of irregular part can be found. But in case of HIF the disturbance part is closely equal to normal signal and the detection of these types of faults are difficult. The Detection algorithms are so effective when compared to other algorithms if they detect HIFs. Results are carried out in MATLAB/SIMULINK software.,2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6938708,no
An efficient and shortest path selection primary-segmented backup algorithm for real - time communication in multi-hop networks,"The Development of high-speed networking has introduced opportunities for new applications such as real-time distributed computation, remote control systems, video conferencing, medical imaging, digital continuous media (audio and motion video), and scientific visualization. Several distributed real-time applications (e.g., medical imaging, video conferencing and air traffic control) demand hard guarantees on the message delivery latency and the recovery delay from component failures. As these generally demands cannot be met in old or traditional datagram services, special type of schemes have been proposed to provide timely as well as efficiently recovery for real-time communications in multihop or multi node networks. These schemes reserve additional network resources (spare resources) a priori along a backup channel that is disjoint with the primary.Such distributed real-time applications demand quality-of-service (QoS) guarantees on timeliness of message delivery and failure-recovery delay. These guarantees are agreed upon before setting up the communication channel and must be met even in the case of bursty network traffic, hardware failure (router and switch crashes, physical cable cuts, etc.), or software bugs. Applications using traditional best effort datagram services like IP experience varying delays due to varying queue sizes and packet drops at the routers. As we know that in distributed system all application required guarantees of the message delivery in short time along with shortest path. To deliver message from source to destination node, we have used a primary path.Since we know that the primary path is the shortest path and very advantageous too.But it can break down due to numerous reasons as any communication network is prone to faults due to hardware failure or software bugs.If the primary path",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6950850,no
Mutation testing tools- An empirical study,"Prevailing code coverage techniques in software testing such as condition coverage, branch coverage are thoroughness indicators, rather than the test suites capabilities to detect the fault. Mutation testing can be prospected as a fault based technique that extent the effectiveness of test suites for localization of faults. Generating and running vast number of mutants against the test cases is arduous and time- consuming. Therefore, the use of mutation testing in software industry is uncommon. Hence, an automated, fast and reliable tool for the same is required to perform mutation testing. Various Mutation testing tools exists for the software industry. In this paper, various available tools are studied and based on the study; a comparison is made between these Mutation testing tools. An inference is made that all the available tools are language dependent or need to be configured differently for different languages to generate and run test cases. Comparative study reveals that most of available Mutation testing tools are in Java language and possess many important features while less number of tools is available for other languages like C, C++, C# and FORTRAN.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6950880,no
An enhancement for single sampling plan method,"Acceptance sampling has been widely used as a quality control technique in industry. A standard single sampling plan consists of 3 switchable inspection plans: Tightened, Normal and Reduced [1]. It means the 3 inspection schemes can be switched from one to another based on predetermined successive products' quality. Theoretically, higher quality of successive lots or batches has high probability of acceptance, and vice versa, lower quality of products will suffer from high reject ratio. However, how about the switch rule works? How to adjust the switch rule to meet requirements of manufacturing and cost? In this paper, we simulate the acceptance probability with different class of defects or defectives in a standard single sampling plan using a self-developed program. And using this program, we investigate the actual inspection cost and both producers' and consumers' risk with a variety of switch rule. An optimized single sampling plan is proposed based on the results of design of experiment (DOE). The sampling plan is more economic and effective to manufactories.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6962596,no
A novel resource related faults detecting approach,"In order to detect resource related faults in operating system, an approach based on path-insensitive analysis is proposed. The approach, which can detect a wider variety of resources issues, is context-sensitive. Test cases to identify errors are automatically generated. The models for the C code and resource related faults are set up. Furthermore a platform for resource faults detection is developed. We evaluate our approach by applying it to Linux 2.6.34 kernel. The results show that most resources related faults are successfully detected and located, with lower rate of false positive and false negative. Test cases generated by the platform greatly improve the efficiency of identifying real defects from the results of static analysis.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6967109,no
ChangeChecker: A tool for defect prediction in source code changes based on incremental learning method,"In software development process, software developers may introduce defects as they make changes to software projects. Being aware of introduced defects immediately upon the completion of the change would allow software developers or testers to allocate more resources of testing and inspecting on the current risky change timely, which can shorten the process of defect finding and fixing effectively. In this paper, we propose a software tool called ChangeChecker to help software developers predict whether current source code change has any defects or not during the software development process. This tool infers the existence of defect by dynamically mining patterns of the source code changes in the revision history of the software project. It mainly consists of three components: (1) incremental feature collection and transformation, (2) real-time defect prediction for source code changes, and (3) dynamic update of the learning model. The tool has been evaluated in a large famous open source project Eclipse and applied to a real software development scenario.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6967127,no
An Improvement of the Slotted CSMA/CA Algorithm with Multi-level Priority Strategy and Service Differentiation Mechanisms,"It is the fact that IEEE 802.15.4 protocol does not support any service of priority scheduling mechanism and there are some shortages existing in the slotted CSMA/CA algorithm. In view of the different types of priority and the defects of the original CSMA/CA algorithm, a kind of CSMA/CA algorithm with multi-level priority strategy and service differentiation mechanisms is proposed in this paper. What's more, in order to provide multi-level differentiation service, different BE and CW are used. Four types of priority are assumed, there are high, medium, low and normal priority and they are assigned depend on current state of the network by CSMA/CA algorithm. In the end, it is proved that the proposed algorithm performs better than the original one in the aspects of the throughput, network delay and the probability of successfully access to channel using network simulator OPNET.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6973572,no
Using process modeling and analysis techniques to reduce errors in healthcare,"Summary form only given. As has been widely reported in the news lately, healthcare errors are a major cause of death and suffering. In the University of Massachusetts Medical Safety Project, we are exploring the use of process modeling and analysis technologies to help reduce medical errors and improve efficiency. Specifically, we are modeling healthcare processes using a process definition language and then analyzing these processes using model checking, fault-tree analysis, discrete event simulation, and other techniques. Working with the UMASS School of Nursing and the Baystate Medical Center, we are undertaking in-depth case studies on error-prone and life-critical healthcare processes. In many ways, these processes are similar to complex, distributed systems with many interacting, concurrent threads and numerous exceptional conditions that must be handled carefully. This talk describes the technologies we are using, discusses case studies, and presents our observations and findings to date. Although presented in terms of the healthcare domain, the described approach could be applied to human-intensive processes in other domains to provide a technology-driven approach to process improvement.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7035522,no
High-Speed Format Converter with Intelligent Quality Checker for File-Based System,"Japan Broadcasting Corporation is shifting to file-based systems for its television production and playout systems, including videotape recorders and editing machines. A variety of codecs and formats based on the material exchange format for broadcast equipment have been adopted. These include Motion Picture Experts Group 2 (MPEG-2) or advanced video coding and operational pattern 1a or atom. Video files need to be converted into the selected codec and format to operate efficiently. The quality of video and audio must be checked during this conversion process, because degradation and noise may occur. This paper describes equipment that can quickly convert files to multiple formats, as well as intelligently check the quality of video and audio during the conversion. The equipment automatically adjusts thresholds to detect anomalies in the video quality check, depending on the type of codec and the spatial frequency of each area. This can be done in less time than the actual video duration by optimizing the processing software.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7308356,no
A structured team building method for collaborative crowdsourcing,"The traditional crowdsourcing approach consists in open calls that give the access to a worldwide crowd potentially able to solve particular problems or perform small tasks. However, over the years crowdsourcing platforms have started to select narrower groups of skilled solvers basing on their expertise, in order to ensure quality and effectiveness of the final result. As a consequence, the selection and allocation of the most appropriate team for the resolution of different types of problems have become a critical process. The present research aims to highlight the main variables to assess solvers capabilities and provides a skills-based methodology for advanced team building in collaborative crowdsourcing contexts. The method focuses on selecting the most suitable team to face a determined problem as well as on tracking the evolution of individuals skills over the performed challenges. A case study conducted within a self-developed platform is proposed to support the description.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7352708,no
Key Issues Regarding Digital Libraries:Evaluation and Integration,"This is the second book based on the 5S (Societies, Scenarios, Spaces, Structures, Streams) approach to digital libraries (DLs). Leveraging the first volume, on Theoretical Foundations, we focus on the key issues of evaluation and integration. These cross-cutting issues serve as a bridge for those interested in DLs, connecting the introduction and formal discussion in the first book, with the coverage of key technologies in the third book, and of illustrative applications in the fourth book. These two topics have central importance in the DL field, allowing it to be treated scientifically as well as practically. In the scholarly world, we only really understand something if we know how to measure and evaluate it. In the Internet era of distributed information systems, we only can be practical at scale if we integrate across both systems and their associated content. Evaluation of DLs must take place atmultiple levels,so we can address the different entities and their associated measur s. Thus, for digital objects, we assess accessibility, pertinence, preservability, relevance, significance, similarity, and timeliness. Other measures are specific to higher-level constructs like metadata, collections, catalogs, repositories, and services.We tie these together through a case study of the 5SQual tool, which we designed and implemented to perform an automatic quantitative evaluation of DLs. Thus, across the Information Life Cycle, we describe metrics and software useful to assess the quality of DLs, and demonstrate utility with regard to representative application areas: archaeology and education. Though integration has been a challenge since the earliest work on DLs, we provide the first comprehensive 5S-based formal description of the DL integration problem, cast in the context of related work. Since archaeology is a fundamentally distributed enterprise, we describe ETANADL, for integrating Near Eastern Archeology sites and information. Thus, we show how 5S-based mode ing can lead to integrated services and content. While the first book adopts a minimalist and formal approach to DLs, and provides a systematic and functional method to design and implement DL exploring services, here we broaden to practical DLs with richer metamodels, demonstrating the power of 5S for integration and evaluation.",2013,http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6813145.pdf&bkn=6813144&pdfType=book,no
The design of polynomial function-based neural network predictors for detection of software defects,"In this study, we introduce a design methodology of polynomial function-based Neural Network (pf-NN) classifiers (predictors). The essential design components include Fuzzy C-Means (FCM) regarded as a generic clustering algorithm and polynomials providing all required nonlinear capabilities of the model. The learning method uses a weighted cost function (objective function) while to analyze the performance of the system we engage a standard receiver operating characteristics (ROC) analysis. The proposed networks are used to detect software defects. From the conceptual standpoint, the classifier of this form can be expressed as a collection of ''if-then'' rules. Fuzzy clustering (Fuzzy C-Means, FCM) is aimed at the development of premise layer of the rules while the corresponding consequences of the rules are formed by some local polynomials. A detailed learning algorithm for the pf-NNs is presented with particular provisions made for dealing with imbalanced classes encountered quite commonly in software quality problems. The use of simple measures such as accuracy of classification becomes questionable. In the assessment of quality of classifiers, we confine ourselves to the use of the area under curve (AUC) in the receiver operating characteristics (ROCs) analysis. AUC comes as a sound classifier metric capturing a tradeoff between the high true positive rate (TP) and the low false positive rate (FP). The performance of the proposed classifier is contrasted with the results produced by some ''standard'' Radial Basis Function (RBF) neural networks.",2013,https://www.researchgate.net/profile/Byoung-Jun_Park/publication/251730276_The_design_of_polynomial_function-based_neural_network_predictors_for_detection_of_software_defects/links/55b1f57908ae092e9650089d.pdf,yes
Balancing Privacy and Utility in Cross-Company Defect Prediction,"Background: Cross-company defect prediction (CCDP) is a field of study where an organization lacking enough local data can use data from other organizations for building defect predictors. To support CCDP, data must be shared. Such shared data must be privatized, but that privatization could severely damage the utility of the data. Aim: To enable effective defect prediction from shared data while preserving privacy. Method: We explore privatization algorithms that maintain class boundaries in a dataset. CLIFF is an instance pruner that deletes irrelevant examples. MORPH is a data mutator that moves the data a random distance, taking care not to cross class boundaries. CLIFF+MORPH are tested in a CCDP study among 10 defect datasets from the PROMISE data repository. Results: We find: 1) The CLIFFed+MORPHed algorithms provide more privacy than the state-of-the-art privacy algorithms; 2) in terms of utility measured by defect prediction, we find that CLIFF+MORPH performs significantly better. Conclusions: For the OO defect data studied here, data can be privatized and shared without a significant degradation in utility. To the best of our knowledge, this is the first published result where privatization does not compromise defect prediction.",2013,http://ieeexplore.ieee.org/document/6419712/,yes
Software fault prediction metrics: A systematic literature review,ContextSoftware metrics may be used in fault prediction models to improve software quality by predicting fault location. ObjectiveThis paper aims to identify software metrics and to assess their applicability in software fault prediction. We investigated the influence of context on metrics' selection and performance. MethodThis systematic literature review includes 106 papers published between 1991 and 2011. The selected papers are classified according to metrics and context properties. ResultsObject-oriented metrics (49%) were used nearly twice as often compared to traditional source code metrics (27%) or process metrics (24%). Chidamber and Kemerer's (CK) object-oriented metrics were most frequently used. According to the selected studies there are significant differences between the metrics used in fault prediction performance. Object-oriented and process metrics have been reported to be more successful in finding faults compared to traditional size and complexity metrics. Process metrics seem to be better at predicting post-release faults compared to any static code metrics. ConclusionMore studies should be performed on large industrial software systems to find metrics more relevant for the industry and to answer the question as to which metrics should be used in a given context.,2013,http://romisatriawahono.net/lecture/rm/survey/software%20engineering/Software%20Fault%20Defect%20Prediction/Radjenovic%20-%20Software%20fault%20prediction%20metrics%20-%202013.pdf,yes